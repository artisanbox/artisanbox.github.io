<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>编译原理之美 on Gen 的学习笔记</title><link>https://artisanbox.github.io/6/</link><description>Recent content in 编译原理之美 on Gen 的学习笔记</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 08 Mar 2022 18:37:53 +0800</lastBuildDate><atom:link href="https://artisanbox.github.io/6/index.xml" rel="self" type="application/rss+xml"/><item><title>01_理解代码：编译器的前端技术</title><link>https://artisanbox.github.io/6/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/1/</guid><description>在开篇词里，我分享了一些使用编译技术的场景。其中有的场景，你只要掌握编译器的前端技术就能解决。比如文本分析场景，软件需要用户自定义功能的场景以及前端编程语言的翻译场景等。而且咱们大学讲的编译原理，也是侧重讲解前端技术，可见编译器的前端技术有多么重要。
当然了，这里的“前端（Front End）”指的是编译器对程序代码的分析和理解过程。它通常只跟语言的语法有关，跟目标机器无关。而与之对应的“后端（Back End）”则是生成目标代码的过程，跟目标机器有关。为了方便你理解，我用一张图直观地展现了编译器的整个编译过程。
你可以看到，编译器的“前端”技术分为词法分析、语法分析和语义分析三个部分。而它主要涉及自动机和形式语言方面的基础的计算理论。
这些抽象的理论也许会让你“撞墙”，不过不用担心，我今天会把难懂的理论放到一边，用你听得懂的大白话，联系实际使用的场景，带你直观地理解它们，让你学完本节课之后，实现以下目标：
对编译过程以及其中的技术点有个宏观、概要的了解。 能够在大脑里绘制一张清晰的知识地图，以应对工作需要。比如分析一个日志文件时，你能知道所对应的技术点，从而针对性地解决问题。 好了，接下来让我们正式进入今天的课程吧！
词法分析（Lexical Analysis）通常，编译器的第一项工作叫做词法分析。就像阅读文章一样，文章是由一个个的中文单词组成的。程序处理也一样，只不过这里不叫单词，而是叫做“词法记号”，英文叫Token。我嫌“词法记号”这个词太长，后面直接将它称作Token吧。
举个例子，看看下面这段代码，如果我们要读懂它，首先要怎么做呢？
#include &amp;lt;stdio.h&amp;gt; int main(int argc, char* argv[]){ int age = 45; if (age &amp;gt;= 17+8+20) { printf(&amp;quot;Hello old man!\\n&amp;quot;); } else{ printf(&amp;quot;Hello young man!\\n&amp;quot;); } return 0; } 我们会识别出if、else、int这样的关键字，main、printf、age这样的标识符，+、-、=这样的操作符号，还有花括号、圆括号、分号这样的符号，以及数字字面量、字符串字面量等。这些都是Token。
那么，如何写一个程序来识别Token呢？可以看到，英文内容中通常用空格和标点把单词分开，方便读者阅读和理解。但在计算机程序中，仅仅用空格和标点分割是不行的。比如“age &amp;gt;= 45”应该分成“age”“&amp;gt;=”和“45”这三个Token，但在代码里它们可以是连在一起的，中间不用非得有空格。
这和汉语有点儿像，汉语里每个词之间也是没有空格的。但我们会下意识地把句子里的词语正确地拆解出来。比如把“我学习编程”这个句子拆解成“我”“学习”“编程”，这个过程叫做“分词”。如果你要研发一款支持中文的全文检索引擎，需要有分词的功能。
其实，我们可以通过制定一些规则来区分每个不同的Token，我举了几个例子，你可以看一下。
识别age这样的标识符。它以字母开头，后面可以是字母或数字，直到遇到第一个既不是字母又不是数字的字符时结束。
识别&amp;gt;=这样的操作符。 当扫描到一个&amp;gt;字符的时候，就要注意，它可能是一个GT（Greater Than，大于）操作符。但由于GE（Greater Equal，大于等于）也是以&amp;gt;开头的，所以再往下再看一位，如果是=，那么这个Token就是GE，否则就是GT。
识别45这样的数字字面量。当扫描到一个数字字符的时候，就开始把它看做数字，直到遇到非数字的字符。
这些规则可以通过手写程序来实现。事实上，很多编译器的词法分析器都是手写实现的，例如GNU的C语言编译器。
如果嫌手写麻烦，或者你想花更多时间陪恋人或家人，也可以偷点儿懒，用词法分析器的生成工具来生成，比如Lex（或其GNU版本，Flex）。这些生成工具是基于一些规则来工作的，这些规则用“正则文法”表达，符合正则文法的表达式称为“正则表达式”。生成工具可以读入正则表达式，生成一种叫“有限自动机”的算法，来完成具体的词法分析工作。
不要被“正则文法（Regular Grammar）”和“有限自动机（Finite-state Automaton，FSA，or Finite Automaton）”吓到。正则文法是一种最普通、最常见的规则，写正则表达式的时候用的就是正则文法。我们前面描述的几个规则，都可以看成口语化的正则文法。
有限自动机是有限个状态的自动机器。我们可以拿抽水马桶举例，它分为两个状态：“注水”和“水满”。摁下冲马桶的按钮，它转到“注水”的状态，而浮球上升到一定高度，就会把注水阀门关闭，它转到“水满”状态。
词法分析器也是一样，它分析整个程序的字符串，当遇到不同的字符时，会驱使它迁移到不同的状态。例如，词法分析程序在扫描age的时候，处于“标识符”状态，等它遇到一个&amp;gt;符号，就切换到“比较操作符”的状态。词法分析过程，就是这样一个个状态迁移的过程。</description></item><item><title>02_正则文法和有限自动机：纯手工打造词法分析器</title><link>https://artisanbox.github.io/6/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/2/</guid><description>上一讲，我提到词法分析的工作是将一个长长的字符串识别出一个个的单词，这一个个单词就是Token。而且词法分析的工作是一边读取一边识别字符串的，不是把字符串都读到内存再识别。你在听一位朋友讲话的时候，其实也是同样的过程，一边听，一边提取信息。
那么问题来了，字符串是一连串的字符形成的，怎么把它断开成一个个的Token呢？分割的依据是什么呢？本节课，我会通过讲解正则表达式（Regular Expression）和有限自动机的知识带你解决这个问题。
其实，我们手工打造词法分析器的过程，就是写出正则表达式，画出有限自动机的图形，然后根据图形直观地写出解析代码的过程。而我今天带你写的词法分析器，能够分析以下3个程序语句：
age &amp;gt;= 45 int age = 40 2+3*5 它们分别是关系表达式、变量声明和初始化语句，以及算术表达式。
接下来，我们先来解析一下“age &amp;gt;= 45”这个关系表达式，这样你就能理解有限自动机的概念，知道它是做词法解析的核心机制了。
解析 age &amp;gt;= 45在“01 | 理解代码：编译器的前端技术”里，我举了一个词法分析的例子，并且提出词法分析要用到有限自动机。当时，我画了这样一个示意图：
我们来描述一下标识符、比较操作符和数字字面量这三种Token的词法规则。
标识符：第一个字符必须是字母，后面的字符可以是字母或数字。 比较操作符：&amp;gt;和&amp;gt;=（其他比较操作符暂时忽略）。 数字字面量：全部由数字构成（像带小数点的浮点数，暂时不管它）。 我们就是依据这样的规则，来构造有限自动机的。这样，词法分析程序在遇到age、&amp;gt;=和45时，会分别识别成标识符、比较操作符和数字字面量。不过上面的图只是一个简化的示意图，一个严格意义上的有限自动机是下面这种画法：
我来解释一下上图的5种状态。
1.初始状态：刚开始启动词法分析的时候，程序所处的状态。
2.标识符状态：在初始状态时，当第一个字符是字母的时候，迁移到状态2。当后续字符是字母和数字时，保留在状态2。如果不是，就离开状态2，写下该Token，回到初始状态。
3.大于操作符（GT）：在初始状态时，当第一个字符是&amp;gt;时，进入这个状态。它是比较操作符的一种情况。
4.大于等于操作符（GE）：如果状态3的下一个字符是=，就进入状态4，变成&amp;gt;=。它也是比较操作符的一种情况。
5.数字字面量：在初始状态时，下一个字符是数字，进入这个状态。如果后续仍是数字，就保持在状态5。
这里我想补充一下，你能看到上图中的圆圈有单线的也有双线的。双线的意思是这个状态已经是一个合法的Token了，单线的意思是这个状态还是临时状态。
按照这5种状态迁移过程，你很容易编成程序（我用Java写了代码示例，你可以用自己熟悉的语言编写）。我们先从状态1开始，在遇到不同的字符时，分别进入2、3、5三个状态：
DfaState newState = DfaState.Initial; if (isAlpha(ch)) { //第一个字符是字母 newState = DfaState.Id; //进入Id状态 token.type = TokenType.Identifier; tokenText.append(ch); } else if (isDigit(ch)) { //第一个字符是数字 newState = DfaState.IntLiteral; token.type = TokenType.IntLiteral; tokenText.append(ch); } else if (ch == '&amp;gt;') { //第一个字符是&amp;gt; newState = DfaState.</description></item><item><title>03_语法分析（一）：纯手工打造公式计算器</title><link>https://artisanbox.github.io/6/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/3/</guid><description>我想你应该知道，公式是Excel电子表格软件的灵魂和核心。除此之外，在HR软件中，可以用公式自定义工资。而且，如果你要开发一款通用报表软件，也会大量用到自定义公式来计算报表上显示的数据。总而言之，很多高级一点儿的软件，都会用到自定义公式功能。
既然公式功能如此常见和重要，我们不妨实现一个公式计算器，给自己的软件添加自定义公式功能吧！
本节课将继续“手工打造”之旅，让你纯手工实现一个公式计算器，借此掌握语法分析的原理和递归下降算法（Recursive Descent Parsing），并初步了解上下文无关文法（Context-free Grammar，CFG）。
我所举例的公式计算器支持加减乘除算术运算，比如支持“2 + 3 * 5”的运算。
在学习语法分析时，我们习惯把上面的公式称为表达式。这个表达式看上去很简单，但你能借此学到很多语法分析的原理，例如左递归、优先级和结合性等问题。
当然了，要实现上面的表达式，你必须能分析它的语法。不过在此之前，我想先带你解析一下变量声明语句的语法，以便让你循序渐进地掌握语法分析。
解析变量声明语句：理解“下降”的含义在“01 | 理解代码：编译器的前端技术”里，我提到语法分析的结果是生成AST。算法分为自顶向下和自底向上算法，其中，递归下降算法是一种常见的自顶向下算法。
与此同时，我给出了一个简单的代码示例，也针对“int age = 45”这个语句，画了一个语法分析算法的示意图：
我们首先把变量声明语句的规则，用形式化的方法表达一下。它的左边是一个非终结符（Non-terminal）。右边是它的产生式（Production Rule）。在语法解析的过程中，左边会被右边替代。如果替代之后还有非终结符，那么继续这个替代过程，直到最后全部都是终结符（Terminal），也就是Token。只有终结符才可以成为AST的叶子节点。这个过程，也叫做推导（Derivation）过程：
intDeclaration : Int Identifier ('=' additiveExpression)?; 你可以看到，int类型变量的声明，需要有一个Int型的Token，加一个变量标识符，后面跟一个可选的赋值表达式。我们把上面的文法翻译成程序语句，伪代码如下：
//伪代码 MatchIntDeclare(){ MatchToken(Int)； //匹配Int关键字 MatchIdentifier(); //匹配标识符 MatchToken(equal); //匹配等号 MatchExpression(); //匹配表达式 } 实际代码在SimpleCalculator.java类的IntDeclare()方法中：
SimpleASTNode node = null; Token token = tokens.peek(); //预读 if (token != null &amp;amp;&amp;amp; token.getType() == TokenType.Int) { //匹配Int token = tokens.read(); //消耗掉int if (tokens.peek().getType() == TokenType.Identifier) { //匹配标识符 token = tokens.</description></item><item><title>04_语法分析（二）：解决二元表达式中的难点</title><link>https://artisanbox.github.io/6/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/4/</guid><description>在“03 | 语法分析（一）：纯手工打造公式计算器”中，我们已经初步实现了一个公式计算器。而且你还在这个过程中，直观地获得了写语法分析程序的体验，在一定程度上破除了对语法分析算法的神秘感。
当然了，你也遇到了一些问题，比如怎么消除左递归，怎么确保正确的优先级和结合性。所以本节课的主要目的就是解决这几个问题，让你掌握像算术运算这样的二元表达式（Binary Expression）。
不过在课程开始之前，我想先带你简单地温习一下什么是左递归（Left Recursive）、优先级（Priority）和结合性（Associativity）。
在二元表达式的语法规则中，如果产生式的第一个元素是它自身，那么程序就会无限地递归下去，这种情况就叫做左递归。比如加法表达式的产生式“加法表达式 + 乘法表达式”，就是左递归的。而优先级和结合性则是计算机语言中与表达式有关的核心概念。它们都涉及了语法规则的设计问题。
我们要想深入探讨语法规则设计，需要像在词法分析环节一样，先了解如何用形式化的方法表达语法规则。“工欲善其事必先利其器”。熟练地阅读和书写语法规则，是我们在语法分析环节需要掌握的一项基本功。
所以本节课我会先带你了解如何写语法规则，然后在此基础上，带你解决上面提到的三个问题。
书写语法规则，并进行推导我们已经知道，语法规则是由上下文无关文法表示的，而上下文无关文法是由一组替换规则（又叫产生式）组成的，比如算术表达式的文法规则可以表达成下面这种形式：
add -&amp;gt; mul | add + mul mul -&amp;gt; pri | mul * pri pri -&amp;gt; Id | Num | (add) 按照上面的产生式，add可以替换成mul，或者add + mul。这样的替换过程又叫做“推导”。以“2+3*5” 和 “2+3+4”这两个算术表达式为例，这两个算术表达式的推导过程分别如下图所示：
通过上图的推导过程，你可以清楚地看到这两个表达式是怎样生成的。而分析过程中形成的这棵树，其实就是AST。只不过我们手写的算法在生成AST的时候，通常会做一些简化，省略掉中间一些不必要的节点。比如，“add-add-mul-pri-Num”这一条分支，实际手写时会被简化成“add-Num”。其实，简化AST也是优化编译过程的一种手段，如果不做简化，呈现的效果就是上图的样子。
那么，上图中两颗树的叶子节点有哪些呢？Num、+和*都是终结符，终结符都是词法分析中产生的Token。而那些非叶子节点，就是非终结符。文法的推导过程，就是把非终结符不断替换的过程，让最后的结果没有非终结符，只有终结符。
而在实际应用中，语法规则经常写成下面这种形式：
add ::= mul | add + mul mul ::= pri | mul * pri pri ::= Id | Num | (add) 这种写法叫做“巴科斯范式”，简称BNF。Antlr和Yacc这两个工具都用这种写法。为了简化书写，我有时会在课程中把“::=”简化成一个冒号。你看到的时候，知道是什么意思就可以了。
你有时还会听到一个术语，叫做扩展巴科斯范式(EBNF)。它跟普通的BNF表达式最大的区别，就是里面会用到类似正则表达式的一些写法。比如下面这个规则中运用了*号，来表示这个部分可以重复0到多次：
add -&amp;gt; mul (+ mul)* 其实这种写法跟标准的BNF写法是等价的，但是更简洁。为什么是等价的呢？因为一个项多次重复，就等价于通过递归来推导。从这里我们还可以得到一个推论：就是上下文无关文法包含了正则文法，比正则文法能做更多的事情。</description></item><item><title>05_语法分析（三）：实现一门简单的脚本语言</title><link>https://artisanbox.github.io/6/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/5/</guid><description>前两节课结束后，我们已经掌握了表达式的解析，并通过一个简单的解释器实现了公式的计算。但这个解释器还是比较简单的，看上去还不大像一门语言。那么如何让它支持更多的功能，更像一门脚本语言呢？本节课，我会带你寻找答案。
我将继续带你实现一些功能，比如：
支持变量声明和初始化语句，就像“int age” “int age = 45”和“int age = 17+8+20”； 支持赋值语句“age = 45”； 在表达式中可以使用变量，例如“age + 10 *2”； 实现一个命令行终端，能够读取输入的语句并输出结果。 实现这些功能之后，我们的成果会更像一个脚本解释器。而且在这个过程中，我还会带你巩固语法分析中的递归下降算法，和你一起讨论“回溯”这个特征，让你对递归下降算法的特征理解得更加全面。
不过，为了实现这些新的语法，我们首先要把它们用语法规则描述出来。
增加所需要的语法规则首先，一门脚本语言是要支持语句的，比如变量声明语句、赋值语句等等。单独一个表达式，也可以视为语句，叫做“表达式语句”。你在终端里输入2+3；，就能回显出5来，这就是表达式作为一个语句在执行。按照我们的语法，无非是在表达式后面多了个分号而已。C语言和Java都会采用分号作为语句结尾的标识，我们也可以这样写。
我们用扩展巴科斯范式（EBNF）写出下面的语法规则：
programm: statement+; statement intDeclaration | expressionStatement | assignmentStatement ; 变量声明语句以int开头，后面跟标识符，然后有可选的初始化部分，也就是一个等号和一个表达式，最后再加分号：
intDeclaration : &amp;lsquo;int&amp;rsquo; Id ( &amp;lsquo;=&amp;rsquo; additiveExpression)? &amp;lsquo;;&amp;rsquo;; 表达式语句目前只支持加法表达式，未来可以加其他的表达式，比如条件表达式，它后面同样加分号：
expressionStatement : additiveExpression &amp;lsquo;;&amp;rsquo;; 赋值语句是标识符后面跟着等号和一个表达式，再加分号：
assignmentStatement : Identifier &amp;lsquo;=&amp;rsquo; additiveExpression &amp;lsquo;;&amp;rsquo;; 为了在表达式中可以使用变量，我们还需要把primaryExpression改写，除了包含整型字面量以外，还要包含标识符和用括号括起来的表达式：
primaryExpression : Identifier| IntLiteral | &amp;lsquo;(&amp;rsquo; additiveExpression &amp;lsquo;)&amp;rsquo;; 这样，我们就把想实现的语法特性，都用语法规则表达出来了。接下来，我们就一步一步实现这些特性。
让脚本语言支持变量之前实现的公式计算器只支持了数字字面量的运算，如果能在表达式中用上变量，会更有用，比如能够执行下面两句：
int age = 45; age + 10 * 2; 这两个语句里面的语法特性包含了变量声明、给变量赋值，以及在表达式里引用变量。为了给变量赋值，我们必须在脚本语言的解释器中开辟一个存储区，记录不同的变量和它们的值：</description></item><item><title>06_编译器前端工具（一）：用Antlr生成词法、语法分析器</title><link>https://artisanbox.github.io/6/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/6/</guid><description>前面的课程中，我重点讲解了词法分析和语法分析，在例子中提到的词法和语法规则也是高度简化的。虽然这些内容便于理解原理，也能实现一个简单的原型，在实际应用中却远远不够。实际应用中，一个完善的编译程序还要在词法方面以及语法方面实现很多工作，我这里特意画了一张图，你可以直观地看一下。
如果让编译程序实现上面这么多工作，完全手写效率会有点儿低，那么我们有什么方法可以提升效率呢？答案是借助工具。
编译器前端工具有很多，比如Lex（以及GNU的版本Flex）、Yacc（以及GNU的版本Bison）、JavaCC等等。你可能会问了：“那为什么我们这节课只讲Antlr，不选别的工具呢？”主要有两个原因。
第一个原因是Antlr能支持更广泛的目标语言，包括Java、C#、JavaScript、Python、Go、C++、Swift。无论你用上面哪种语言，都可以用它生成词法和语法分析的功能。而我们就使用它生成了Java语言和C++语言两个版本的代码。
第二个原因是Antlr的语法更加简单。它能把类似左递归的一些常见难点在工具中解决，对提升工作效率有很大的帮助。这一点，你会在后面的课程中直观地感受到。
而我们今天的目标就是了解Antlr，然后能够使用Antlr生成词法分析器与语法分析器。在这个过程中，我还会带你借鉴成熟的词法和语法规则，让你快速成长。
接下来，我们先来了解一下Antlr这个工具。
初识AntlrAntlr是一个开源的工具，支持根据规则文件生成词法分析器和语法分析器，它自身是用Java实现的。
你可以下载Antlr工具，并根据说明做好配置。同时，你还需要配置好机器上的Java环境（可以在Oracle官网找到最新版本的JDK）。
因为我用的是Mac，所以我用macOS平台下的软件包管理工具Homebrew安装了Antlr，它可以自动设置好antlr和grun两个命令（antlr和grun分别是java org.antlr.v4.Tool和java org.antlr.v4.gui.TestRig这两个命令的别名）。这里需要注意的是，你要把Antlr的JAR文件设置到CLASSPATH环境变量中，以便顺利编译所生成的Java源代码。
GitHub上还有很多供参考的语法规则，你可以下载到本地硬盘随时查阅。
现在你已经对Antlr有了初步的了解，也知道如何安装它了。接下来，我带你实际用一用Antlr，让你用更轻松的方式生成词法分析器和语法分析器。
用Antlr生成词法分析器你可能对Antlr还不怎么熟悉，所以我会先带你使用前面课程中，你已经比较熟悉的那些词法规则，让Antlr生成一个新的词法分析器，然后再借鉴一些成熟的规则文件，把词法分析器提升到更加专业、实用的级别。
Antlr通过解析规则文件来生成编译器。规则文件以.g4结尾，词法规则和语法规则可以放在同一个文件里。不过为了清晰起见，我们还是把它们分成两个文件，先用一个文件编写词法规则。
为了让你快速进入状态，我们先做一个简单的练习预热一下。我们创建一个Hello.g4文件，用于保存词法规则，然后把之前用过的一些词法规则写进去。
lexer grammar Hello; //lexer关键字意味着这是一个词法规则文件，名称是Hello，要与文件名相同 //关键字 If : &amp;lsquo;if&amp;rsquo;; Int : &amp;lsquo;int&amp;rsquo;;
//字面量 IntLiteral: [0-9]+; StringLiteral: &amp;lsquo;&amp;quot;&amp;rsquo; .*? &amp;lsquo;&amp;quot;&amp;rsquo; ; //字符串字面量
//操作符 AssignmentOP: &amp;lsquo;=&amp;rsquo; ; RelationalOP: &amp;lsquo;&amp;gt;&amp;rsquo;|&amp;rsquo;&amp;gt;=&amp;rsquo;|&amp;rsquo;&amp;lt;&amp;rsquo; |&amp;rsquo;&amp;lt;=&amp;rsquo; ; Star: &amp;lsquo;*&amp;rsquo;; Plus: &amp;lsquo;+&amp;rsquo;; Sharp: &amp;lsquo;#&amp;rsquo;; SemiColon: &amp;lsquo;;&amp;rsquo;; Dot: &amp;lsquo;.&amp;rsquo;; Comm: &amp;lsquo;,&amp;rsquo;; LeftBracket : &amp;lsquo;[&amp;rsquo;; RightBracket: &amp;lsquo;]&amp;rsquo;; LeftBrace: &amp;lsquo;{&amp;rsquo;; RightBrace: &amp;lsquo;}&amp;rsquo;; LeftParen: &amp;lsquo;(&amp;rsquo;; RightParen: &amp;lsquo;)&amp;rsquo;;</description></item><item><title>07_编译器前端工具（二）：用Antlr重构脚本语言</title><link>https://artisanbox.github.io/6/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/7/</guid><description>上一讲，我带你用Antlr生成了词法分析器和语法分析器，也带你分析了，跟一门成熟的语言相比，在词法规则和语法规则方面要做的一些工作。
在词法方面，我们参考Java的词法规则文件，形成了一个CommonLexer.g4词法文件。在这个过程中，我们研究了更完善的字符串字面量的词法规则，还讲到要通过规则声明的前后顺序来解决优先级问题，比如关键字的规则一定要在标识符的前面。
目前来讲，我们已经完善了词法规则，所以今天我们来补充和完善一下语法规则，看一看怎样用最高效的速度，完善语法功能。比如一天之内，我们是否能为某个需要编译技术的项目实现一个可行性原型？
而且，我还会带你熟悉一下常见语法设计的最佳实践。这样当后面的项目需要编译技术做支撑时，你就会很快上手，做出成绩了！
接下来，我们先把表达式的语法规则梳理一遍，让它达到成熟语言的级别，然后再把语句梳理一遍，包括前面几乎没有讲过的流程控制语句。最后再升级解释器，用Visitor模式实现对AST的访问，这样我们的代码会更清晰，更容易维护了。
好了，让我们正式进入课程，先将表达式的语法完善一下吧！
完善表达式（Expression）的语法在“06 | 编译器前端工具（一）：用Antlr生成词法、语法分析器”中，我提到Antlr能自动处理左递归的问题，所以在写表达式时，我们可以大胆地写成左递归的形式，节省时间。
但这样，我们还是要为每个运算写一个规则，逻辑运算写完了要写加法运算，加法运算写完了写乘法运算，这样才能实现对优先级的支持，还是有些麻烦。
其实，Antlr能进一步地帮助我们。我们可以把所有的运算都用一个语法规则来涵盖，然后用最简洁的方式支持表达式的优先级和结合性。在我建立的PlayScript.g4语法规则文件中，只用了一小段代码就将所有的表达式规则描述完了：
expression : primary | expression bop='.' ( IDENTIFIER | functionCall | THIS ) | expression '[' expression ']' | functionCall | expression postfix=('++' | '--') | prefix=('+'|'-'|'++'|'--') expression | prefix=('~'|'!') expression | expression bop=('*'|'/'|'%') expression | expression bop=('+'|'-') expression | expression ('&amp;lt;' '&amp;lt;' | '&amp;gt;' '&amp;gt;' '&amp;gt;' | '&amp;gt;' '&amp;gt;') expression | expression bop=('&amp;lt;=' | '&amp;gt;=' | '&amp;gt;' | '&amp;lt;') expression | expression bop=INSTANCEOF typeType | expression bop=('==' | '!</description></item><item><title>08_作用域和生存期：实现块作用域和函数</title><link>https://artisanbox.github.io/6/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/8/</guid><description>目前，我们已经用Antlr重构了脚本解释器，有了工具的帮助，我们可以实现更高级的功能，比如函数功能、面向对象功能。当然了，在这个过程中，我们还要克服一些挑战，比如：
如果要实现函数功能，要升级变量管理机制； 引入作用域机制，来保证变量的引用指向正确的变量定义； 提升变量存储机制，不能只把变量和它的值简单地扔到一个HashMap里，要管理它的生存期，减少对内存的占用。 本节课，我将借实现块作用域和函数功能，带你探讨作用域和生存期及其实现机制，并升级变量管理机制。那么什么是作用域和生存期，它们的重要性又体现在哪儿呢？
“作用域”和“生存期”是计算机语言中更加基础的概念，它们可以帮你深入地理解函数、块、闭包、面向对象、静态成员、本地变量和全局变量等概念。
而且一旦你深入理解，了解作用域与生存期在编译期和运行期的机制之后，就能解决在学习过程中可能遇到的一些问题，比如：
闭包的机理到底是什么？ 为什么需要栈和堆两种机制来管理内存？它们的区别又是什么？ 一个静态的内部类和普通的内部类有什么区别？ 了解上面这些内容之后，接下来，我们来具体看看什么是作用域。
作用域（Scope）作用域是指计算机语言中变量、函数、类等起作用的范围，我们来看一个具体的例子。
下面这段代码是用C语言写的，我们在全局以及函数fun中分别声明了a和b两个变量，然后在代码里对这些变量做了赋值操作：
/* scope.c 测试作用域。 */ #include &amp;lt;stdio.h&amp;gt; int a = 1;
void fun() { a = 2; //b = 3; //出错，不知道b是谁 int a = 3; //允许声明一个同名的变量吗？ int b = a; //这里的a是哪个？ printf(&amp;quot;in fun: a=%d b=%d \n&amp;quot;, a, b); }
int b = 4; //b的作用域从这里开始
int main(int argc, char **argv){ printf(&amp;quot;main&amp;ndash;1: a=%d b=%d \n&amp;quot;, a, b);</description></item><item><title>09_面向对象：实现数据和方法的封装</title><link>https://artisanbox.github.io/6/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/9/</guid><description>在现代计算机语言中，面向对象是非常重要的特性，似乎常用的语言都支持面向对象特性，比如Swift、C++、Java……不支持的反倒是异类了。
而它重要的特点就是封装。也就是说，对象可以把数据和对数据的操作封装在一起，构成一个不可分割的整体，尽可能地隐藏内部的细节，只保留一些接口与外部发生联系。 在对象的外部只能通过这些接口与对象进行交互，无需知道对象内部的细节。这样能降低系统的耦合，实现内部机制的隐藏，不用担心对外界的影响。那么它们是怎样实现的呢？
本节课，我将从语义设计和运行时机制的角度剖析面向对象的特性，带你深入理解面向对象的实现机制，让你能在日常编程工作中更好地运用面向对象的特性。比如，在学完这讲之后，你会对对象的作用域和生存期、对象初始化过程等有更清晰的了解。而且你不会因为学习了Java或C++的面向对象机制，在学习JavaScript和Ruby的面向对象机制时觉得别扭，因为它们的本质是一样的。
接下来，我们先简单地聊一下什么是面向对象。
面向对象的语义特征我的一个朋友，在10多年前做过培训师，为了吸引学员的注意力，他在讲“什么是面向对象”时说：“面向对象是世界观，是方法论。”
虽然有点儿语不惊人死不休的意思，但我必须承认，所有的计算机语言都是对世界进行建模的方式，只不过建模的视角不同罢了。面向对象的设计思想，在上世纪90年代被推崇，几乎被视为最好的编程模式。实际上，各种不同的编程思想，都会表现为这门语言的语义特征，所以，我就从语义角度，利用类型、作用域、生存期这样的概念带你深入剖析一下面向对象的封装特性，其他特性在后面的课程中再去讨论。
从类型角度 类型处理是语义分析时的重要工作。现代计算机语言可以用自定义的类来声明变量，这是一个巨大的进步。因为早期的计算机语言只支持一些基础的数据类型，比如各种长短不一的整型和浮点型，像字符串这种我们编程时离不开的类型，往往是在基础数据类型上封装和抽象出来的。所以，我们要扩展语言的类型机制，让程序员可以创建自己的类型。
从作用域角度 首先是类的可见性。作为一种类型，它通常在整个程序的范围内都是可见的，可以用它声明变量。当然，一些像Java的语言，也能限制某些类型的使用范围，比如只能在某个命名空间内，或者在某个类内部。
对象的成员的作用域是怎样的呢？我们知道，对象的属性（“属性”这里指的是类的成员变量）可以在整个对象内部访问，无论在哪个位置声明。也就是说，对象属性的作用域是整个对象的内部，方法也是一样。这跟函数和块中的本地变量不一样，它们对声明顺序有要求，像C和Java这样的语言，在使用变量之前必须声明它。
从生存期的角度 对象的成员变量的生存期，一般跟对象的生存期是一样的。在创建对象的时候，就对所有成员变量做初始化，在销毁对象的时候，所有成员变量也随着一起销毁。当然，如果某个成员引用了从堆中申请的内存，这些内存需要手动释放，或者由垃圾收集机制释放。
但还有一些成员，不是与对象绑定的，而是与类型绑定的，比如Java中的静态成员。静态成员跟普通成员的区别，就是作用域和生存期不同，它的作用域是类型的所有对象实例，被所有实例共享。生存期是在任何一个对象实例创建之前就存在，在最后一个对象销毁之前不会消失。
你看，我们用这三个语义概念，就把面向对象的封装特性解释清楚了，无论语言在顶层怎么设计，在底层都是这么实现的。
了解了面向对象在语义上的原理之后，我们来实际动手解析一下代码中的类，这样能更深刻地体会这些原理。
设计类的语法，并解析它我们要在语言中支持类的定义，在PlayScript.g4中，可以这样定义类的语法规则：
classDeclaration : CLASS IDENTIFIER (EXTENDS typeType)? (IMPLEMENTS typeList)? classBody ; classBody : &amp;lsquo;{&amp;rsquo; classBodyDeclaration* &amp;lsquo;}&amp;rsquo; ;
classBodyDeclaration : &amp;lsquo;;&amp;rsquo; | memberDeclaration ;
memberDeclaration : functionDeclaration | fieldDeclaration ;
functionDeclaration : typeTypeOrVoid IDENTIFIER formalParameters (&amp;rsquo;[&amp;rsquo; &amp;lsquo;]&amp;rsquo;)* (THROWS qualifiedNameList)? functionBody ; 我来简单地讲一下这个语法规则：
类声明以class关键字开头，有一个标识符是类型名称，后面跟着类的主体。 类的主体里要声明类的成员。在简化的情况下，可以只关注类的属性和方法两种成员。我们故意把类的方法也叫做function，而不是method，是想把对象方法和函数做一些统一的设计。 函数声明现在的角色是类的方法。 类的成员变量的声明和普通变量声明在语法上没什么区别。 你能看到，我们构造像class这样高级别的结构时，越来越得心应手了，之前形成的一些基础的语法模块都可以复用，比如变量声明、代码块（block）等。
用上面的语法写出来的playscript脚本的效果如下，在示例代码里也有，你可以运行它：
/* ClassTest.</description></item><item><title>10_闭包：理解了原理，它就不反直觉了</title><link>https://artisanbox.github.io/6/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/10/</guid><description>在讲作用域和生存期时，我提到函数里的本地变量只能在函数内部访问，函数退出之后，作用域就没用了，它对应的栈桢被弹出，作用域中的所有变量所占用的内存也会被收回。
但偏偏跑出来闭包（Closure）这个怪物。
在JavaScript中，用外层函数返回一个内层函数之后，这个内层函数能一直访问外层函数中的本地变量。按理说，这个时候外层函数已经退出了，它里面的变量也该作废了。可闭包却非常执着，即使外层函数已经退出，但内层函数仿佛不知道这个事实一样，还继续访问外层函数中声明的变量，并且还真的能够正常访问。
不过，闭包是很有用的，对库的编写者来讲，它能隐藏内部实现细节；对面试者来讲，它几乎是前端面试必问的一个问题，比如如何用闭包特性实现面向对象编程？等等。
本节课，我会带你研究闭包的实现机制，让你深入理解作用域和生存期，更好地使用闭包特性。为此，要解决两个问题：
函数要变成playscript的一等公民。也就是要能把函数像普通数值一样赋值给变量，可以作为参数传递给其他函数，可以作为函数的返回值。 要让内层函数一直访问它环境中的变量，不管外层函数退出与否。 我们先通过一个例子，研究一下闭包的特性，看看它另类在哪里。
闭包的内在矛盾来测试一下JavaScript的闭包特性：
/** * clojure.js * 测试闭包特性 * 作者：宫文学 */ var a = 0; var fun1 = function(){ var b = 0; // 函数内的局部变量
var inner = function(){ // 内部的一个函数 a = a+1; b = b+1; return b; // 返回内部的成员 } return inner; // 返回一个函数 }
console.log(&amp;quot;outside: a=%d&amp;quot;, a);
var fun2 = fun1(); // 生成闭包 for (var i = 0; i&amp;lt; 2; i++){ console.</description></item><item><title>11_语义分析（上）：如何建立一个完善的类型系统？</title><link>https://artisanbox.github.io/6/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/11/</guid><description>在做语法分析时我们可以得到一棵语法树，而基于这棵树能做什么，是语义的事情。比如，+号的含义是让两个数值相加，并且通常还能进行缺省的类型转换。所以，如果要区分不同语言的差异，不能光看语言的语法。比如Java语言和JavaScript在代码块的语法上是一样的，都是用花括号，但在语义上是不同的，一个有块作用域，一个没有。
这样看来，相比词法和语法的设计与处理，语义设计和分析似乎要复杂很多。虽然我们借作用域、生存期、函数等特性的实现涉猎了很多语义分析的场景，但离系统地掌握语义分析，还差一点儿火候。所以，为了帮你攻破语义分析这个阶段，我会用两节课的时间，再梳理一下语义分析中的重要知识，让你更好地建立起相关的知识脉络。
今天这节课，我们把注意力集中在类型系统这个话题上。
围绕类型系统产生过一些争论，有的程序员会拥护动态类型语言，有的会觉得静态类型语言好。要想探究这个问题，我们需要对类型系统有个清晰的了解，最直接的方式，就是建立一个完善的类型系统。
那么什么是类型系统？我们又该怎样建立一个完善的类型系统呢？
其实，类型系统是一门语言所有的类型的集合，操作这些类型的规则，以及类型之间怎么相互作用的（比如一个类型能否转换成另一个类型）。如果要建立一个完善的类型系统，形成对类型系统比较完整的认知，需要从两个方面出发：
根据领域的需求，设计自己的类型系统的特征。 在编译器中支持类型检查、类型推导和类型转换。 先从第一个方面出发看一下。
设计类型系统的特征在进入这个话题之前，我想先问你一个有意义的问题：类型到底是什么？我们说一个类型的时候，究竟在说什么？
要知道，在机器代码这个层面，其实是分不出什么数据类型的。在机器指令眼里，那就是0101，它并不对类型做任何要求，不需要知道哪儿是一个整数，哪儿代表着一个字符，哪儿又是内存地址。你让它做什么操作都可以，即使这个操作没有意义，比如把一个指针值跟一个字符相加。
那么高级语言为什么要增加类型这种机制呢？
对类型做定义很难，但大家公认的有一个说法：类型是针对一组数值，以及在这组数值之上的一组操作。比如，对于数字类型，你可以对它进行加减乘除算术运算，对于字符串就不行。
所以，类型是高级语言赋予的一种语义，有了类型这种机制，就相当于定了规矩，可以检查施加在数据上的操作是否合法。因此类型系统最大的好处，就是可以通过类型检查降低计算出错的概率。所以，现代计算机语言都会精心设计一个类型系统，而不是像汇编语言那样完全不区分类型。
不过，类型系统的设计有很多需要取舍和权衡的方面，比如：
面向对象的拥护者希望所有的类型都是对象，而重视数据计算性能的人认为应该支持非对象化的基础数据类型； 你想把字符串作为原生数据类型，还是像Java那样只是一个普通的类？ 是静态类型语言好还是动态类型语言好？ …… 虽然类型系统的设计有很多需要取舍和权衡的方面，但它最需要考虑的是，是否符合这门语言想解决的问题，我们用静态类型语言和动态类型语言分析一下。
根据类型检查是在编译期还是在运行期进行的，我们可以把计算机语言分为两类：
静态类型语言（全部或者几乎全部的类型检查是在编译期进行的）。 动态类型语言（类型的检查是在运行期进行的）。 静态类型语言的拥护者说：
因为编译期做了类型检查，所以程序错误较少，运行期不用再检查类型，性能更高。像C、Java和Go语言，在编译时就对类型做很多处理，包括检查类型是否匹配，以及进行缺省的类型转换，大大降低了程序出错的可能性，还能让程序运行效率更高，因为不需要在运行时再去做类型检查和转换。
而动态类型语言的拥护者说：
静态语言太严格，还要一遍遍编译，编程效率低，用动态类型语言方便进行快速开发。JavaScript、Python、PHP等都是动态类型的。
客观地讲，这些说法都有道理。目前的趋势是，某些动态类型语言在想办法增加一些机制，在编译期就能做类型检查，比如用TypeScript代替JavaScript编写程序，做完检查后再输出成JavaScript。而某些静态语言呢，却又发明出一些办法，部分地绕过类型检查，从而提供动态类型语言的灵活性。
再延伸一下，跟静态类型和动态类型概念相关联的，还有强类型和弱类型。强类型语言中，变量的类型一旦声明就不能改变，弱类型语言中，变量类型在运行期时可以改变。二者的本质区别是，强类型语言不允许违法操作，因为能够被检查出来，弱类型语言则从机制上就无法禁止违法操作，所以是不安全的。比如你写了一个表达式a*b。如果a和b这两个变量是数值，这个操作就没有问题，但如果a或b不是数值，那就没有意义了，弱类型语言可能就检查不出这类问题。
也就是，静态类型和动态类型说的是什么时候检查的问题，强类型和弱类型说的是就算检查，也检查不出来，或者没法检查的问题，这两组概念经常会被搞混，所以我在这里带你了解一下。
接着说回来。关于类型特征的取舍，是根据领域问题而定的。举例来说，很多人可能都觉得强类型更好，但对于儿童编程启蒙来说，他们最好尽可能地做各种尝试，如果必须遵守与类型有关的规则，程序总是跑不起来，可能会打击到他们。
对于playscript而言，因为目前是用来做教学演示的，所以我们尽可能地多涉及与类型处理有关的情况，供大家体会算法，或者在自己的工作中借鉴。
首先，playscript是静态类型和强类型的，所以几乎要做各种类型检查，你可以参考看看这些都是怎么做的。
第二，我们既支持对象，也支持原生的基础数据类型。这两种类型的处理特点不一样，你也可以借鉴一下。后面面向对象的一讲，我会再讲与之相关的子类型（Subtyping）和运行时类型信息（Run Time Type Information, RTTI）的概念，这里就不展开了。
第三，我们还支持函数作为一等公民，也就是支持函数的类型。函数的类型是它的原型，包括返回值和参数，原型一样的函数，就看做是同样类型的，可以进行赋值。这样，你也就可以了解实现函数式编程特性时，要处理哪些额外的类型问题。
接下来，我们来说一说如何做类型检查、类型推导和类型转换。
如何做类型检查、类型推导和类型转换先来看一看，如果编写一个编译器，我们在做类型分析时会遇到哪些问题。以下面这个最简单的表达式为例，这个表达式在不同的情况下会有不同的运行结果：
a = b + 10 如果b是一个浮点型，b+10的结果也是浮点型。如果b是字符串型的，有些语言也是允许执行+号运算的，实际的结果是字符串的连接。这个分析过程，就是类型推导（Type Inference）。 当右边的值计算完，赋值给a的时候，要检查左右两边的类型是否匹配。这个过程，就是类型检查（Type Checking）。 如果a的类型是浮点型，而右边传过来的是整型，那么一般就要进行缺省的类型转换（Type Conversion）。 类型的检查、推导和转换是三个工作，可是采用的技术手段差不多，所以我们放在一起讲，先来看看类型的推导。
在早期的playscript的实现中，是假设运算符两边的类型都是整型的，并做了强制转换。
这在实际应用中，当然不够用，因为我们还需要用到其他的数据类型。那怎么办呢？在运行时再去判断和转换吗？当然可以，但我们还有更好的选择，就是在编译期先判断出表达式的类型来。比如下面这段代码，是在RefResolve.java中，推导表达式的类型：
case PlayScriptParser.ADD: if (type1 == PrimitiveType.String || type2 == PrimitiveType.</description></item><item><title>12_语义分析（下）：如何做上下文相关情况的处理？</title><link>https://artisanbox.github.io/6/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/12/</guid><description>我们知道，词法分析和语法分析阶段，进行的处理都是上下文无关的。可仅凭上下文无关的处理，是不能完成一门强大的语言的。比如先声明变量，再用变量，这是典型的上下文相关的情况，我们肯定不能用上下文无关文法表达这种情况，所以语法分析阶段处理不了这个问题，只能在语义分析阶段处理。语义分析的本质，就是针对上下文相关的情况做处理。
我们之前讲到的作用域，是一种上下文相关的情况，因为如果作用域不同，能使用的变量也是不同的。类型系统也是一种上下文相关的情况，类型推导和类型检查都要基于上下文中相关的AST节点。
本节课，我们再讲两个这样的场景：引用的消解、左值和右值，然后再介绍上下文相关情况分析的一种方法：属性计算。这样，你会把语义分析就是上下文处理的本质掌握得更清楚，并掌握属性计算这个强大的方法。
我们先来说说引用的消解这个场景。
语义分析场景：引用的消解在程序里使用变量、函数、类等符号时，我们需要知道它们指的是谁，要能对应到定义它们的地方。下面的例子中，当使用变量a时，我们需要知道它是全局变量a，还是fun()函数中的本地变量a。因为不同作用域里可能有相同名称的变量，所以必须找到正确的那个。这个过程，可以叫引用消解。
/* scope.c 测试作用域 */ #include &amp;lt;stdio.h&amp;gt; int a = 1;
void fun() { a = 2; //这是指全局变量a int a = 3; //声明一个本地变量 int b = a; //这个a指的是本地变量 printf(&amp;quot;in func: a=%d b=%d \n&amp;quot;, a, b); } 在集成开发环境中，当我们点击一个变量、函数或类，可以跳到定义它的地方。另一方面，当我们重构一个变量名称、方法名称或类名称的时候，所有引用它的地方都会同步修改。这是因为IDE分析了符号之间的交叉引用关系。
函数的引用消解比变量的引用消解还要更复杂一些。
它不仅要比对函数名称，还要比较参数和返回值（可以叫函数原型，又或者叫函数的类型）。我们在把函数提升为一等公民的时候，提到函数类型（FunctionType）的概念。两个函数的类型相同，需要返回值、参数个数、每个参数的类型都能匹配得上才行。
在面向对象编程语言中，函数引用的消解也很复杂。
当一个参数需要一个对象的时候，程序中提供其子类的一个实例也是可以的，也就是子类可以用在所有需要父类的地方，例如下面的代码：
class MyClass1{} //父类 class MyClass2 extends MyClass1{} //子类
MyClass1 obj1; MyClass2 obj2;
function fun(MyClass1 obj){} //参数需要父类的实例
fun(obj2); //提供子类的实例 在C++语言中，引用的消解还要更加复杂。
它还要考虑某个实参是否能够被自动转换成形参所要求的类型，比如在一个需要double类型的地方，你给它传一个int也是可以的。
命名空间也是做引用消解的时候需要考虑的因素。
像Java、C++都支持命名空间。如果在代码前头引入了某个命名空间，我们就可以直接引用里面的符号，否则需要冠以命名空间。例如：
play.PlayScriptCompiler.Compile() //Java语言 play::PlayScriptCompiler.</description></item><item><title>13_继承和多态：面向对象运行期的动态特性</title><link>https://artisanbox.github.io/6/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/13/</guid><description>面向对象是一个比较大的话题。在“09 | 面向对象：实现数据和方法的封装”中，我们了解了面向对象的封装特性，也探讨了对象成员的作用域和生存期特征等内容。本节课，我们再来了解一下面向对象的另外两个重要特征：继承和多态。
你也许会问，为什么没有在封装特性之后，马上讲继承和多态呢？那是因为继承和多态涉及的语义分析阶段的知识点比较多，特别是它对类型系统提出了新的概念和挑战，所以我们先掌握语义分析，再了解这部分内容，才是最好的选择。
继承和多态对类型系统提出的新概念，就是子类型。我们之前接触的类型往往是并列关系，你是整型，我是字符串型，都是平等的。而现在，一个类型可以是另一个类型的子类型，比如我是一只羊，又属于哺乳动物。这会导致我们在编译期无法准确计算出所有的类型，从而无法对方法和属性的调用做完全正确的消解（或者说绑定）。这部分工作要留到运行期去做，也因此，面向对象编程会具备非常好的优势，因为它会导致多态性。这个特性会让面向对象语言在处理某些类型的问题时，更加优雅。
而我们要想深刻理解面向对象的特征，就必须了解子类型的原理和运行期的机制。所以，接下来，我们从类型体系的角度理解继承和多态，然后看看在编译期需要做哪些语义分析，再考察继承和多态的运行期特征。
从类型体系的角度理解继承和多态继承的意思是一个类的子类，自动具备了父类的属性和方法，除非被父类声明为私有的。比如一个类是哺乳动物，它有体重（weight）的属性，还会做叫(speak)的操作。如果基于哺乳动物这个父类创建牛和羊两个子类，那么牛和羊就自动继承了哺乳动物的属性，有体重，还会叫。
所以继承的强大之处，就在于重用。也就是有些逻辑，如果在父类中实现，在子类中就不必重复实现。
多态的意思是同一个类的不同子类，在调用同一个方法时会执行不同的动作。这是因为每个子类都可以重载掉父类的某个方法，提供一个不同的实现。哺乳动物会“叫”，而牛和羊重载了这个方法，发出“哞~”和“咩~”的声音。这似乎很普通，但如果创建一个哺乳动物的数组，并在里面存了各种动物对象，遍历这个数组并调用每个对象“叫”的方法时，就会发出“哞~”“咩~”“喵~”等各种声音，这就有点儿意思了。
下面这段示例代码，演示了继承和多态的特性，a的speak()方法和b的speak()方法会分别打印出牛叫和羊叫，调用的是子类的方法，而不是父类的方法：
/** mammal.play 演示面向对象编程：继承和多态。 */ class Mammal{ int weight = 20; boolean canSpeak(){ return true; } void speak(){ println(&amp;amp;quot;mammal speaking...&amp;amp;quot;); } }
class Cow extends Mammal{ void speak(){ println(&amp;quot;moo~~ moo~~&amp;quot;); } }
class Sheep extends Mammal{ void speak(){ println(&amp;quot;mee~~ mee~~&amp;quot;); println(&amp;quot;My weight is: &amp;quot; + weight); //weight的作用域覆盖子类 } }
//将子类的实例赋给父类的变量 Mammal a = Cow(); Mammal b = Sheep();
//canSpeak()方法是继承的 println(&amp;quot;a.</description></item><item><title>14_前端技术应用（一）：如何透明地支持数据库分库分表？</title><link>https://artisanbox.github.io/6/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/14/</guid><description>从今天开始，我们正式进入了应用篇，我会用两节课的时间，带你应用编译器的前端技术。这样，你会把学到的编译技术和应用领域更好地结合起来，学以致用，让技术发挥应有的价值。还能通过实践加深对原理的理解，形成一个良好的循环。
这节课，我们主要讨论，一个分布式数据库领域的需求。我会带你设计一个中间层，让应用逻辑不必关心数据库的物理分布。这样，无论把数据库拆成多少个分库，编程时都会像面对一个物理库似的没什么区别。
接下来，我们先了解一下分布式数据库的需求和带来的挑战。
分布式数据库解决了什么问题，又带来了哪些挑战随着技术的进步，我们编写的应用所采集、处理的数据越来越多，处理的访问请求也越来越多。而单一数据库服务器的处理能力是有限的，当数据量和访问量超过一定级别以后，就要开始做分库分表的操作。比如，把一个数据库的大表拆成几张表，把一个库拆成几个库，把读和写的操作分离开等等。我们把这类技术统称为分布式数据库技术。
分库分表（Sharding）有时也翻译成“数据库分片”。分片可以依据各种不同的策略，比如我开发过一个与社区有关的应用系统，这个系统的很多业务逻辑都是围绕小区展开的。对于这样的系统，按照地理分布的维度来分片就很合适，因为每次对数据库的操作基本上只会涉及其中一个分库。
假设我们有一个订单表，那么就可以依据一定的规则对订单或客户进行编号，编号中就包含地理编码。比如SDYT代表山东烟台，BJHD代表北京海淀，不同区域的数据放在不同的分库中：
通过数据库分片，我们可以提高数据库服务的性能和可伸缩性。当数据量和访问量增加时，增加数据库节点的数量就行了。不过，虽然数据库的分片带来了性能和伸缩性的好处，但它也带来了一些挑战。
最明显的一个挑战，是数据库分片逻辑侵入到业务逻辑中。过去，应用逻辑只访问一个数据库，现在需要根据分片的规则，判断要去访问哪个数据库，再去跟这个数据库服务器连接。如果增加数据库分片，或者对分片策略进行调整，访问数据库的所有应用模块都要修改。这会让软件的维护变得更复杂，显然也不太符合软件工程中模块低耦合、把变化隔离的理念。
所以如果有一种技术，能让我们访问很多数据库分片时，像访问一个数据库那样就好了。数据库的物理分布，对应用是透明的。
可是，“理想很吸引人，现实很骨感”。要实现这个技术，需要解决很多问题：
首先是跨库查询的难题。如果SQL操作都针对一个库还好，但如果某个业务需求恰好要跨多个库，比如上面的例子中，如果要查询多个小区的住户信息，那么就要在多个库中都执行查询，然后把查询结果合并，一般还要排序。
如果我们前端显示的时候需要分页，每页显示一百行，那就更麻烦了。我们不可能从10个分库中各查出10行，合并成100行，这100行不一定排在前面，最差的情况，可能这100行恰好都在其中一个分库里。所以，你可能要从每个分库查出100行来，合并、排序后，再取出前100行。如果涉及数据库表跨库做连接，你想象一下，那就更麻烦了。
其次就是跨库做写入的难题。如果对数据库写入时遇到了跨库的情况，那么就必须实现分布式事务。所以，虽然分布式数据库的愿景很吸引人，但我们必须解决一系列技术问题。
这一讲，我们先解决最简单的问题，也就是当每次数据操作仅针对一个分库的时候，能否自动确定是哪个分库的问题。解决这个问题我们不需要依据别的信息，只需要提供SQL就行了。这就涉及对SQL语句的解析了，自然要用到编译技术。
解析SQL语句，判断访问哪个数据库我画了一张简化版的示意图：假设有两张表，分别是订单表和客户表，它们的主键是order_id和cust_id：
我们采用的分片策略，是依据这两个主键的前4位的编码来确定数据库分片的逻辑，比如：前四位是SDYT，那就使用山东烟台的分片，如果是BJHD，就使用北京海淀的分片，等等。
在我们的应用中，会对订单表进行一些增删改查的操作，比如会执行下面的SQL语句：
//查询 select * from orders where order_id = 'SDYT20190805XXXX' select * from orders where cust_id = 'SDYT987645' //插入 insert into orders (order_id，&amp;hellip;其他字段) values( &amp;quot;BJHD20190805XXXX&amp;quot;,&amp;hellip;)
//修改 update orders set price=298.00 where order_id=&amp;lsquo;FJXM20190805XXXX&amp;rsquo;
//删除 delete from orders where order_id=&amp;lsquo;SZLG20190805XXXX&amp;rsquo; 我们要能够解析这样的SQL语句，根据主键字段的值，决定去访问哪个分库或者分表。这就需要用到编译器前端技术，包括词法分析、语法分析和语义分析。
听到这儿，你可能会质疑：“解析SQL语句？是在开玩笑吗？”你可能觉得这个任务太棘手，犹豫着是否要忍受业务逻辑和技术逻辑混杂的缺陷，把判断分片的逻辑写到应用代码里，或者想解决这个问题，又或者想自己写一个开源项目，帮到更多的人。
无论你的内心活动如何，应用编译技术，能让你有更强的信心解决这个问题。那么如何去做呢？要想完成解析SQL的任务，在词法分析和语法分析这两个阶段，我建议你采用工具快速落地，比如Antlr。你要找一个现成的SQL语句的语法规则文件。
GitHub中，那个收集了很多示例Antlr规则文件的项目里，有两个可以参考的规则：一个是PLSQL的（它是Oracle数据库的SQL语法）；一个是SQLite的（这是一个嵌入式数据库）。
实际上，我还找到MySQL workbench所使用的一个产品级的规则文件。MySQL workbench是一个图形化工具，用于管理和访问MySQL。这个规则文件还是很靠谱的，不过它里面嵌了很多属性计算规则，而且是C++语言写的，我嫌处理起来麻烦，就先弃之不用，暂且采用SQLite的规则文件来做示范。
先来看一下这个文件里的一些规则，例如select语句相关的语法：
factored_select_stmt : ( K_WITH K_RECURSIVE? common_table_expression ( &amp;lsquo;,&amp;rsquo; common_table_expression )* )?</description></item><item><title>15_前端技术应用（二）：如何设计一个报表工具？</title><link>https://artisanbox.github.io/6/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/15/</guid><description>众所周知，很多软件都需要面向开发者甚至最终用户提供自定义功能，在开篇词里，我提到自己曾经做过工作流软件和电子表单软件，它们都需要提供自定义功能，报表软件也是其中的典型代表。
在每个应用系统中，我们对数据的处理大致会分成两类：一类是在线交易，叫做OLTP，比如在网上下订单；一类是在线分析，叫做OLAP，它是对应用中积累的数据进行进一步分析利用。而报表工具就是最简单，但也是最常用的数据分析和利用的工具。
本节课，我们就来分析一下，如果我们要做一个通用的报表工具，需要用到哪些编译技术，又该怎样去实现。
报表工具所需要的编译技术如果要做一个报表软件，我们要想清楚软件面对的用户是谁。有一类报表工具面向的用户是程序员，那么这种软件可以暴露更多技术细节。比如，如果报表要从数据库获取数据，你可以写一个SQL语句作为数据源。
还有一类软件是给业务级的用户使用的，很多BI软件包都是这种类型。带有IT背景的顾问给用户做一些基础配置，然后用户就可以用这个软件包了。Excel可以看做是这种报表工具，IT人员建立Excel与数据库之间的连接，剩下的就是业务人员自己去操作了。
这些业务人员可以采用一个图形化的界面设计报表，对数据进行加工处理。我们来看看几个场景。
第一个场景是计算字段。计算字段的意思是，原始数据里没有这个数据，我们需要基于原始数据，通过一个自定义的公式来把它计算出来。比如在某个CRM系统中保存着销售数据，我们有每个部门的总销售额，也有每个部门的人数，要想在报表中展示每个部门的人均销售额，这个时候就可以用到计算公式功能，计算公式如下：
人均销售额=部门销售额/部门人数 得到的结果如下图所示：
进一步，我们可以在计算字段中支持函数。比如我们可以把各个部门按照人均销售额排名次。这可以用一个函数来计算：
=rank(人均销售额) rank就是排名次的意思，其他统计函数还包括：
min()，求最小值。 max()，求最大值。 avg()，求平均值。 sum()，求和。 还有一些更有意思的函数，比如：
runningsum()，累计汇总值。 runningavg()，累计平均值。 这些有意思的函数是什么意思呢？因为很多明细性的报表，都是逐行显示的，累计汇总值和累计平均值，就是累计到当前行的计算结果。当然了，我们还可以支持更多的函数，比如当前日期、当前页数等等。更有意思的是，上述字段也好、函数也好，都可以用来组合成计算字段的公式，比如：
=部门销售额/sum(部门销售额) //本部门的销售额在全部销售额的占比 =max(部门销售额)-部门销售额 //本部门的销售额与最高部门的差距 =max(部门销售额/部门人数)-部门销售额/部门人数 //本部门人均销售额与最高的那个部门的差 =sum(部门销售额)/sum(人数)-部门销售额/部门人数 //本部门的人均销售额与全公司人均销售额的差 怎么样，是不是越来越有意思了呢？现在你已经知道了在报表中会用到普通字段和各种各样的计算公式，那么，我们如何用这样的字段和公式来定义一张报表呢？
如何设计报表假设我们的报表是一行一行地展现数据，也就是最简单的那种。那我们将报表的定义做成一个XML文件，可能是下面这样的，它定义了表格中每一列的标题和所采用字段或公式：
&amp;lt;playreport title=&amp;quot;Report 1&amp;quot;&amp;gt; &amp;lt;section&amp;gt; &amp;lt;column&amp;gt; &amp;lt;title&amp;gt;部门&amp;lt;/title&amp;gt; &amp;lt;field&amp;gt;dept&amp;lt;/field&amp;gt; &amp;lt;/column&amp;gt; &amp;lt;column&amp;gt; &amp;lt;title&amp;gt;人数&amp;lt;/title&amp;gt; &amp;lt;field&amp;gt;num_person&amp;lt;/field&amp;gt; &amp;lt;/column&amp;gt; &amp;lt;column&amp;gt; &amp;lt;title&amp;gt;销售额&amp;lt;/title&amp;gt; &amp;lt;field&amp;gt;sales_amount&amp;lt;/field&amp;gt; &amp;lt;/column&amp;gt; &amp;lt;column&amp;gt; &amp;lt;title&amp;gt;人均销售额&amp;lt;/title&amp;gt; &amp;lt;field&amp;gt;sales_amount/num_person&amp;lt;/field&amp;gt; &amp;lt;/column&amp;gt; &amp;lt;/section&amp;gt; &amp;lt;datasource&amp;gt; &amp;lt;connection&amp;gt;数据库连接信息...&amp;lt;/connection&amp;gt; &amp;lt;sql&amp;gt;select dept, num_person, sales_amount from sales&amp;lt;/sql&amp;gt; &amp;lt;/datasource&amp;gt; &amp;lt;/playreport&amp;gt; 这个报表定义文件还是蛮简单的，它主要表达的是数据逻辑，忽略了表现层的信息。如果我们想要优先表达表现层的信息，例如字体大小、界面布局等，可以采用HTML模板的方式来定义报表，其实就是在一个HTML中嵌入了公式，比如：
&amp;lt;html&amp;gt; &amp;lt;body&amp;gt; &amp;lt;div class=&amp;quot;report&amp;quot; datasource=&amp;quot;这里放入数据源信息&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;table_header&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;column_header&amp;quot;&amp;gt;部门&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;column_header&amp;quot;&amp;gt;人数&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;column_header&amp;quot;&amp;gt;销售额&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;column_header&amp;quot;&amp;gt;人均销售额&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;table_body&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;field&amp;quot;&amp;gt;{=dept}&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;field&amp;quot;&amp;gt;{=num_person}&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;field&amp;quot;&amp;gt;{=sales_amount}&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;field&amp;quot;&amp;gt;{=sales_amount/num_person}&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 这样的HTML模板看上去是不是很熟悉？其实在很多语言里，比如PHP，都提供模板引擎功能，实现界面设计和应用代码的分离。这样一个模板，可以直接解释执行，或者先翻译成PHP或Java代码，然后再执行。只要运用我们学到的编译技术，这些都可以实现。</description></item><item><title>16_NFA和DFA：如何自己实现一个正则表达式工具？</title><link>https://artisanbox.github.io/6/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/16/</guid><description>回顾之前讲的内容，原理篇重在建立直观理解，帮你建立信心，这是第一轮的认知迭代。应用篇帮你涉足应用领域，在解决领域问题时发挥编译技术的威力，积累运用编译技术的一手经验，也启发你用编译技术去解决更多的领域问题，这是第二轮的认知迭代。而为时三节课的算法篇将你是第三轮的认知迭代。
在第三轮的认知迭代中，我会带你掌握前端技术中的核心算法。而本节课，我就借“怎样实现正则表达式工具？”这个问题，探讨第一组算法：与正则表达式处理有关的算法。
在词法分析阶段，我们可以手工构造有限自动机（FSA，或FSM）实现词法解析，过程比较简单。现在我们不再手工构造词法分析器，而是直接用正则表达式解析词法。
你会发现，我们只要写一些规则，就能基于这些规则分析和处理文本。这种能够理解正则表达式的功能，除了能生成词法分析器，还有很多用途。比如Linux的三个超级命令，又称三剑客（grep、awk和sed），都是因为能够直接支持正则表达式，功能才变得强大的。
接下来，我就带你完成编写正则表达式工具的任务，与此同时，你就能用正则文法生成词法分析器了：
首先，把正则表达式翻译成非确定的有限自动机（Nondeterministic Finite Automaton，NFA）。
其次，基于NFA处理字符串，看看它有什么特点。
然后，把非确定的有限自动机转换成确定的有限自动机（Deterministic Finite Automaton，DFA）
最后，运行DFA，看看它有什么特点。
强调一下，不要被非确定的有限自动机、确定的有限自动机这些概念吓倒，我肯定让你学明白。
认识DFA和NFA在讲词法分析时，我提到有限自动机（FSA）有有限个状态。识别Token的过程，就是FSA状态迁移的过程。其中，FSA分为确定的有限自动机（DFA）和非确定的有限自动机（NFA）。
DFA的特点是，在任何一个状态，我们基于输入的字符串，都能做一个确定的转换，比如：
NFA的特点是，它存在某些状态，针对某些输入，不能做一个确定的转换，这又细分成两种情况：
对于一个输入，它有两个状态可以转换。 存在ε转换。也就是没有任何输入的情况下，也可以从一个状态迁移到另一个状态。 比如，“a[a-zA-Z0-9]*bc”这个正则表达式对字符串的要求是以a开头，以bc结尾，a和bc之间可以有任意多个字母或数字。在图中状态1的节点输入b时，这个状态是有两条路径可以选择的，所以这个有限自动机是一个NFA。
这个NFA还有引入ε转换的画法，它们是等价的。实际上，第二个NFA可以用我们今天讲的算法，通过正则表达式自动生成出来。
需要注意的是，无论是NFA还是DFA，都等价于正则表达式。也就是，所有的正则表达式都能转换成NFA或DFA，所有的NFA或DFA，也都能转换成正则表达式。
理解了NFA和DFA之后，来看看我们如何从正则表达式生成NFA。
从正则表达式生成NFA我们需要把它分为两个子任务：
第一个子任务，是把正则表达式解析成一个内部的数据结构，便于后续的程序使用。因为正则表达式也是个字符串，所以要先做一个小的编译器，去理解代表正则表达式的字符串。我们可以偷个懒，直接针对示例的正则表达式生成相应的数据结构，不需要做出这个编译器。
用来测试的正则表达式可以是int关键字、标识符，或者数字字面量：
int | [a-zA-Z][a-zA-Z0-9]* | [0-9]+ 我用下面这段代码创建了一个树状的数据结构，来代表用来测试的正则表达式：
private static GrammarNode sampleGrammar1() { GrammarNode node = new GrammarNode(&amp;quot;regex1&amp;quot;,GrammarNodeType.Or); //int关键字 GrammarNode intNode = node.createChild(GrammarNodeType.And); intNode.createChild(new CharSet('i')); intNode.createChild(new CharSet('n')); intNode.createChild(new CharSet('t')); //标识符 GrammarNode idNode = node.createChild(GrammarNodeType.And); GrammarNode firstLetter = idNode.createChild(CharSet.letter); GrammarNode letterOrDigit = idNode.createChild(CharSet.letterOrDigit); letterOrDigit.setRepeatTimes(0, -1); //数字字面量 GrammarNode literalNode = node.</description></item><item><title>17_First和Follow集合：用LL算法推演一个实例</title><link>https://artisanbox.github.io/6/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/17/</guid><description>在前面的课程中，我讲了递归下降算法。这个算法很常用，但会有回溯的现象，在性能上会有损失。所以我们要把算法升级一下，实现带有预测能力的自顶向下分析算法，避免回溯。而要做到这一点，就需要对自顶向下算法有更全面的了解。
另外，在留言区，有几个同学问到了一些问题，涉及到对一些基本知识点的理解，比如：
基于某个语法规则做解析的时候，什么情况下算是成功，什么情况下算是失败？ 使用深度优先的递归下降算法时，会跟广度优先的思路搞混。 要搞清这些问题，也需要全面了解自顶向下算法。比如，了解Follow集合和$符号的用法，能帮你解决第一个问题；了解广度优先算法能帮你解决第二个问题。
所以，本节课，我先把自顶向下分析的算法体系梳理一下，让你先建立更加清晰的全景图，然后我再深入剖析LL算法的原理，讲清楚First集合与Follow集合这对核心概念，最终让你把自顶向下的算法体系吃透。
自顶向下分析算法概述自顶向下分析的算法是一大类算法。总体来说，它是从一个非终结符出发，逐步推导出跟被解析的程序相同的Token串。
这个过程可以看做是一张图的搜索过程，这张图非常大，因为针对每一次推导，都可能产生一个新节点。下面这张图只是它的一个小角落。
算法的任务，就是在大图中，找到一条路径，能产生某个句子（Token串）。比如，我们找到了三条橘色的路径，都能产生“2+3*5”这个表达式。
根据搜索的策略，有深度优先（Depth First）和广度优先（Breadth First）两种，这两种策略的推导过程是不同的。
深度优先是沿着一条分支把所有可能性探索完。以“add-&amp;gt;mul+add”产生式为例，它会先把mul这个非终结符展开，比如替换成pri，然后再把它的第一个非终结符pri展开。只有把这条分支都向下展开之后，才会回到上一级节点，去展开它的兄弟节点。
递归下降算法就是深度优先的，这也是它不能处理左递归的原因，因为左边的分支永远也不能展开完毕。
而针对“add-&amp;gt;add+mul”这个产生式，广度优先会把add和mul这两个都先展开，这样就形成了四条搜索路径，分别是mul+mul、add+mul+mul、add+pri和add+mul*pri。接着，把它们的每个非终结符再一次展开，会形成18条新的搜索路径。
所以，广度优先遍历，需要探索的路径数量会迅速爆炸，成指数级上升。哪怕用下面这个最简单的语法，去匹配“2+3”表达式，都需要尝试20多次，更别提针对更复杂的表达式或者采用更加复杂的语法规则了。
//一个很简单的语法 add -&amp;gt; pri //1 add -&amp;gt; add + pri //2 pri -&amp;gt; Int //3 pri -&amp;gt; (add) //4 这样看来，指数级上升的内存消耗和计算量，使得广度优先根本没有实用价值。虽然上面的算法有优化空间，但无法从根本上降低算法复杂度。当然了，它也有可以使用左递归文法的优点，不过我们不会为了这个优点去忍受算法的性能。
而深度优先算法在内存占用上是线性增长的。考虑到回溯的情况，在最坏的情况下，它的计算量也会指数式增长，但我们可以通过优化，让复杂度降为线性增长。
了解广度优先算法，你的思路会得到拓展，对自顶向下算法的本质有更全面的理解。另外，在写算法时，你也不会一会儿用深度优先，一会儿用广度优先了。
针对深度优先算法的优化方向是减少甚至避免回溯，思路就是给算法加上预测能力。比如，我在解析statement的时候，看到一个if，就知道肯定这是一个条件语句，不用再去尝试其他产生式了。
LL算法就属于这类预测性的算法。第一个L，是Left-to-right，代表从左向右处理程序代码。第二个L，是Leftmost，意思是最左推导。
按照语法规则，一个非终结符展开后，会形成多个子节点，其中包含终结符和非终结符。最左推导是指，从左到右依次推导展开这些非终结符。采用Leftmost的方法，在推导过程中，句子的左边逐步都会被替换成终结符，只有右边的才可能包含非终结符。
以“2+3*5”为例，它的推导顺序从左到右，非终结符逐步替换成了终结符：
下图是上述推导过程建立起来的AST，“1、2、3……”等编号是AST节点创建的顺序：
好了，我们把自顶向下分析算法做了总体概述，并讲清楚了最左推导的含义，现在来看看LL算法到底是怎么回事。
计算和使用First集合LL算法是带有预测能力的自顶向下算法。在推导的时候，我们希望当存在多个候选的产生式时，瞄一眼下一个（或多个）Token，就知道采用哪个产生式。如果只需要预看一个Token，就是LL(1)算法。
拿statement的语法举例子，它有好几个产生式，分别产生if语句、while语句、switch语句……
statement : block | IF parExpression statement (ELSE statement)? | FOR '(' forControl ')' statement | WHILE parExpression statement | DO statement WHILE parExpression ';' | SWITCH parExpression '{' switchBlockStatementGroup* switchLabel* | RETURN expression?</description></item><item><title>18_移进和规约：用LR算法推演一个实例</title><link>https://artisanbox.github.io/6/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/18/</guid><description>到目前为止，我们所讨论的语法分析算法，都是自顶向下的。与之相对应的，是自底向上的算法，比如本节课要探讨的LR算法家族。
LR算法是一种自底向上的算法，它能够支持更多的语法，而且没有左递归的问题。第一个字母L，与LL算法的第一个L一样，代表从左向右读入程序。第二个字母R，指的是RightMost（最右推导），也就是在使用产生式的时候，是从右往左依次展开非终结符。例如，对于“add-&amp;gt;add+mul”这样一个产生式，是优先把mul展开，然后再是add。在接下来的讲解过程中，你会看到这个过程。
自顶向下的算法，是递归地做模式匹配，从而逐步地构造出AST。那么自底向上的算法是如何构造出AST的呢？答案是用移进-规约的算法。
本节课，我就带你通过移进-规约方法，自底向上地构造AST，完成语法的解析。接下来，我们先通过一个例子看看自底向上语法分析的过程。
通过实例了解自底向上语法分析的过程我们选择熟悉的语法规则：
add -&amp;gt; mul add -&amp;gt; add + mul mul -&amp;gt; pri mul -&amp;gt; mul * pri pri -&amp;gt; Int | (add) 然后来解析“2+3*5”这个表达式，AST如下：
我们分步骤看一下解析的具体过程。
第1步，看到第一个Token，是Int，2。我们把它作为AST的第一个节点，同时把它放到一个栈里（就是图中红线左边的部分）。这个栈代表着正在处理的一些AST节点，把Token移到栈里的动作叫做移进（Shift）。
第2步，根据语法规则，Int是从pri推导出来的（pri-&amp;gt;Int），那么它的上级AST肯定是pri，所以，我们给它加了一个父节点pri，同时，也把栈里的Int替换成了pri。这个过程是语法推导的逆过程，叫做规约（Reduce）。
Reduce这个词你在学Map-Reduce时可能接触过，它相当于我们口语化的“倒推”。具体来讲，它是从工作区里倒着取出1到n个元素，根据某个产生式，组合出上一级的非终结符，也就是AST的上级节点，然后再放进工作区（也就是竖线的左边）。
这个时候，栈里可能有非终结符，也可能有终结符，它仿佛是我们组装AST的一个工作区。竖线的右边全都是Token（也就是终结符），它们在等待处理。
第3步，与第2步一样，因为pri只能是mul推导出来的，产生式是“mul-&amp;gt;pri”，所以我们又做了一次规约。
第4步，我们根据“add-&amp;gt;mul”产生式，将mul规约成add。至此，我们对第一个Token做了3次规约，已经到头了。这里为什么做规约，而不是停在mul上，移进+号，是有原因的。因为没有一个产生式，是mul后面跟+号，而add后面却可以跟+号。
第5步，移进+号。现在栈里有两个元素了，分别是add和+。
第6步，移进Int，也就是数字3。栈里现在有3个元素。
第7到第8步，Int规约到pri，再规约到mul。
到目前为止，我们做规约的方式都比较简单，就是对着栈顶的元素，把它反向推导回去。
第9步，我们面临3个选择，比较难。
第一个选择是继续把mul规约成add，第二个选择是把“add+mul”规约成add。这两个选择都是错误的，因为它们最终无法形成正确的AST。
第三个选择，也就是按照“mul-&amp;gt;mul*pri”，继续移进 *号 ，而不是做规约。只有这样，才能形成正确的AST，就像图中的虚线。
第10步，移进Int，也就是数字5。
第11步，Int规约成pri。
第12步，mul*pri规约成mul。
注意，这里也有两个选择，比如把pri继续规约成mul。但它显然也是错误的选择。
第13步，add+mul规约成add。
至此，我们就构建完成了一棵正确的AST，并且，栈里也只剩下了一个元素，就是根节点。
整个语法解析过程，实质是反向最右推导（Reverse RightMost Derivation）。什么意思呢？如果把AST节点根据创建顺序编号，就是下面这张图呈现的样子，根节点编号最大是13：
但这是规约的过程，如果是从根节点开始的推导过程，顺序恰好是反过来的，先是13号，再是右子节点12号，再是12号的右子节点11号，以此类推。我们把这个最右推导过程写在下面：
在语法解析的时候，我们是从底下反推回去，所以叫做反向的最右推导过程。从这个意义上讲，LR算法中的R，带有反向（Reverse）和最右（Reightmost）这两层含义。
在最右推导过程中，我加了下划线的部分，叫做一个句柄（Handle）。句柄是一个产生式的右边部分，以及它在一个右句型（最右推导可以得到的句型）中的位置。以最底下一行为例，这个句柄“Int”是产生式“pri-&amp;gt;Int”的右边部分，它的位置是句型“Int + Int * Int”的第一个位置。
简单来说，句柄，就是产生式是在这个位置上做推导的，如果需要做反向推导的话，也是从这个位置去做规约。
针对这个简单的例子，我们可以用肉眼进行判断，找到正确的句柄，做出正确的选择。不过，要把这种判断过程变成严密的算法，做到在每一步都采取正确的行动，知道该做移进还是规约，做规约的话，按照哪个产生式，这就是LR算法要解决的核心问题了。
那么，如何找到正确的句柄呢？
找到正确的句柄我们知道，最右推导是从最开始的产生式出发，经过多步推导（多步推导记做-&amp;gt;*），一步步形成当前的局面 （也就是左边栈里有一些非终结符和终结符，右边还可以预看1到k个Token）。
add -&amp;gt;* 栈 | Token 我们要像侦探一样，根据手头掌握的信息，反向推导出这个多步推导的路径，从而获得正确的句柄。我们依据的是左边栈里的信息，以及右边的Token串。对于LR(0)算法来说，我们只依据左边的栈，就能找到正确的句柄，对于LR(1)算法来说，我们可以从右边预看一个Token。</description></item><item><title>19_案例总结与热点问题答疑：对于左递归的语法，为什么我的推导不是左递归的？</title><link>https://artisanbox.github.io/6/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/19/</guid><description>目前为止，“编译原理”的前端部分已经讲完了，你学到现在，感受如何呢？
不得不说，订阅这门课程的同学，都是很有追求的。因为编译原理这门课，肯定给你的学习生涯多多少少地带来过“伤害”，你现在有勇气重拾“编译原理”，下决心将它攻克，本身就是一种有追求的表现。
在课程开始之初，很多同学当场立下（入）了Flag（坑），比如：
@andylo25：立下Flag，想写一个解释性语言。
@陈越 ：许诺会跟着学完。
@许。：强调自己因为面试华为来学习编译原理。
……
还有同学认为自己半路出家，为了长远的发展，一定要补好基本功。要我说，乔布斯还是辍学加半路出家的呢，终生学习是互联网时代的常态：
@一只豪猪 ：半路出家的野路子码农来补课了。
……
在准备课程的过程中，我努力把晦涩的知识点变得通俗易懂，希望得到你的认可。当我在留言区看到一些留言时，我的内心是欣慰的，也是欣喜的：
@许童童：之前看到词法分析什么的就是一脸蒙，看了老师的文章，醍醐灌顶。
@VVK：老师讲的太好了，十几年没搞懂的概念终于整理明白了。
……
与此同时，我也在不断优化课程，力求将内容做到深入浅出，比如，在策划算法篇的内容时，我吸取一些同学的建议，尽可能画成可视化的图形，并且让整个算法的推导过程很直观地呈现。
但是我不能回避一个事实，就是即便这些内容你认为很好，但你要想学好编译原理，还是要花费不少精力将这些内容反复地看上几遍。你需要认真跟上课程的思路和进程，用心思考和实践，才会有所得，单看内容不动手尝试是没办法学为所用的。所以，在这里，我想表扬一些有耐心，愿意尝试的同学，比如@曾经瘦过@Unrestrained@周小明@Sam 当然，还有很多同学在一直坚持，我为你们点赞！
而且，我发现，很多同学还有探知和质疑精神，比如，@沉淀的梦想 发现我在示例代码里用的都是左值，也跟我讨论在实现闭包的时候，如何仍然正常访问全局变量。@mcuking 指出JavaScript的ES6版本已经支持块作用域 @李梁|东大 也与我讨论了关于C++ auto变量的类型推导等等。
我知道大部分同学的时间很紧，但我感谢你们的坚持，感谢你们在努力抽时间动手实践，比如@Smallfly 自己动手写规则；@曾经瘦过 再次动手跟着敲代码。
还有很多同学花了很多时间，用自己熟悉的语言，参照课程的示例代码重写了词法分析器、语法分析器，并分享了代码：
@（——_ ——)：写了一晚上，终于用C语言模仿实现了第二节课的内容。
@windpiaoxue：也做了一个C语言实现。
……
其他有Go语言的（@catplanet）、Swift语言的（@Smallfly@Rockbean@贾献华）、C++语言的（@阿尔伯特@中年男子@蛋黄儿）、TypeScript的（@缺个豆饼吗@好吃的呆梨）、PHP的（@吴军旗）等等，我通常都会编译并运行一下。
@catplanet 甚至提供了一个界面，可以通过浏览器调用自己写的编译程序，运行并显示结果。
@京京beaver 还分享了在Windows环境下如何做Antlr的配置，让其他同学可以更顺畅地运行Antlr。
@knull 建议我在写BNF的时候，用到+号Token要带上引号，避免跟原来BNF表示重复1到多次的+号冲突。
@kaixiao7 提醒我在Windows下，EOF是用Ctl+z输入。
我对你们取得的成果以及建议感到由衷的高兴和感谢，我相信，你们的分享也激励了其他同学克服困难，继续前进！
当然了，你在学习的过程中，还会遇到一些问题，我很感谢提问题的同学。其中一些问题，我认为是比较典型，有通用意义的，所以选了4个典型的问题，再带你详细地探究一下。
问题一：对于左递归的语法，为什么我的推导不是左递归的？
这个问题本身反映了，进行递归下降分析的时候，如何保持清晰的思路，值得讲一讲。
在03讲，我们刚开始接触到语法分析，也刚开始接触递归下降算法。这时，我介绍了左递归的概念，但你可能在实际推导的过程中，觉得不是左递归，比如用下面这个语法，来推导“2+3”这个简单的表达式：
//简化的左递归文法 add-&amp;gt;Int add-&amp;gt;add + Int 你可能会拿第一个产生式做推导：
add-&amp;gt;2
成功返回
因为没有采用第二条产生式，所以不会触发递归调用。但这里的问题是，“2+3”是一个加法表达式，2也是一个合法的加法表达式，但仅仅解析出2是不行的，我们必须完整地解析出“2+3”来。
在17讲，我提到，任何自顶向下的算法，都是在一个大的图里找到一条搜索路径的过程。最后的结果，是经过多次推导，生成跟输入的Token串相同的结果，解析完毕以后，所有Token也耗光。
如果只匹配上2，那就证明这条搜索路径是错误的，我们必须尝试另一种可能性，也就是第二个产生式。
要找到正确的搜索路径，在递归下降算法或者LL算法时，我们都是采用“贪婪”策略，这个策略在16讲关于正则表达式时讲过。也就是要匹配尽量多的Token才可以。就算是换成右递归的文法，也不能采用第一个产生式。因为解析完Int以后，接下来的Token是+号，还可以尝试用第二个产生式，那我们就要启动贪婪策略，用第二个，而不是第一个。
//简化的右递归文法 add-&amp;gt;Int add-&amp;gt;Int + add 以上是第一种情况。</description></item><item><title>20_高效运行：编译器的后端技术</title><link>https://artisanbox.github.io/6/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/20/</guid><description>前18节课，我们主要探讨了编译器的前端技术，它的重点，是让编译器能够读懂程序。无结构的代码文本，经过前端的处理以后，就变成了Token、AST和语义属性、符号表等结构化的信息。基于这些信息，我们可以实现简单的脚本解释器，这也从另一个角度证明了我们的前端处理工作确实理解了程序代码，否则程序不可能正确执行嘛。
实际上，学完前端技术以后，我们已经能做很多事情了，比如让软件有自定义功能，就像我们在15讲中提到的报表系统，这时，不需要涉及编译器后端技术。
但很多情况下，我们需要继续把程序编译成机器能读懂的代码，并高效运行。这时，我们就面临了三个问题：
1.我们必须了解计算机运行一个程序的原理（也就是运行期机制），只有这样，才知道如何生成这样的程序。
2.要能利用前端生成的AST和属性信息，将其正确翻译成目标代码。
3.需要对程序做尽可能多的优化，比如让程序执行效率更高，占空间更少等等。
弄清这三个问题，是顺利完成编译器后端工作的关键，本节课，我会让你对程序运行机制、生成代码和优化代码有个直观的了解，然后再在接下来的课程中，将这些问题逐一击破。
弄清程序的运行机制总的来说，编译器后端要解决的问题是：现在给你一台计算机，你怎么生成一个可以运行的程序，然后还能让这个程序在计算机上正确和高效地运行？
我画了一个模型：
基本上，我们需要面对的是两个硬件：
一个是CPU，它能接受机器指令和数据，并进行计算。它里面有寄存器、高速缓存和运算单元，充分利用寄存器和高速缓存会让系统的性能大大提升。
另一个是内存。我们要在内存里保存编译好的代码和数据，还要设计一套机制，让程序最高效地利用这些内存。
通常情况下，我们的程序要受某个操作系统的管理，所以也要符合操作系统的一些约定。但有时候我们的程序也可能直接跑在硬件上，单片机和很多物联网设备采用这样的结构，甚至一些服务端系统，也可以不跑在操作系统上。
你可以看出，编译器后端技术跟计算机体系结构的关系很密切。我们必须清楚地理解计算机程序是怎么运行的，有了这个基础，才能探讨如何编译生成这样的程序。
所以，我会在下一节课，也就是21讲，将运行期的机制讲清楚，比如内存空间如何划分和组织；程序是如何启动、跳转和退出的；执行过程中指令和数据如何传递到CPU；整个过程中需要如何跟操作系统配合，等等。
也有的时候，我们的面对的机器是虚拟机，Java的运行环境就是一个虚拟机（JVM），那我们需要就了解这个虚拟机的特点，以便生成可以在这个虚拟机上运行的代码，比如Java的字节码。同时，字节码有时仍然需要编译成机器码。
在对运行期机制有了一定的了解之后，我们就有底气来进行下一步了，生成符合运行期机制的代码。
生成代码编译器后端的最终结果，就是生成目标代码。如果目标是在计算机上直接运行，就像C语言程序那样，那这个目标代码指的是汇编代码。而如果运行目标是Java虚拟机，那这个目标代码就是指JVM的字节码。
基于我们在编译器前端所生成的成果，我们其实可以直接生成汇编代码，在后面的课程中，我会带你做一个这样的尝试。
你可能惧怕汇编代码，觉得它肯定很难，能写汇编的人一定很牛。在我看来，这是一个偏见，因为汇编代码并不难写，为什么呢？
其实汇编没有类型，也没有那么多的语法结构，它要做的通常就是把数据拷贝到寄存器，处理一下，再保存回内存。所以，从汇编语言的特性看，就决定了它不可能复杂到哪儿去。
你如果问问硬件工程师就知道了，因为他们经常拿汇编语言操作寄存器、调用中断，也没多难。但另一方面，正是因为汇编的基础机制太简单，而且不太安全，用它编写程序的效率太低，所以现在直接用汇编写的程序，都是处理很小、很单一的问题，我们不会再像阿波罗登月计划那样，用汇编写整个系统，这个项目的代码最近已经开源了，如果现在用高级语言去做这项工作，会容易得多，还可以像现在的汽车自动驾驶系统一样实现更多的功能。
所以，在22和23讲，我会带你从AST直接翻译成汇编代码，并编译成可执行文件，这样你就会看到这个过程没有你想象的那么困难，你对汇编代码的恐惧感，也会就此消失了。
当然，写汇编跟使用高级语言有很多不同，其中一点就是要关心CPU和内存这样具体的硬件。比如，你需要了解不同的CPU指令集的差别，你还需要知道CPU是64位的还是32位的，有几个寄存器，每个寄存器可以用于什么指令，等等。但这样导致的问题是，每种语言，针对每种不同的硬件，都要生成不同的汇编代码。你想想看，一般我们设计一门语言要支持尽可能多的硬件平台，这样的工作量是不是很庞大？
所以，为了降低后端工作量，提高软件复用度，就需要引入中间代码（Intermediate Representation，IR）的机制，它是独立于具体硬件的一种代码格式。各个语言的前端可以先翻译成IR，然后再从IR翻译成不同硬件架构的汇编代码。如果有n个前端语言，m个后端架构，本来需要做m*n个翻译程序，现在只需要m+n个了。这就大大降低了总体的工作量。
甚至，很多语言主要做好前端就行了，后端可以尽量重用已有的库和工具，这也是现在推出新语言越来越快的原因之一。像Rust就充分利用了LLVM，GCC的各种语言，如C、C++、Object C等，也是充分共享了后端技术。
IR可以有多种格式，在第24讲，我们会介绍三地址代码、静态单赋值码等不同的IR。比如，“x + y * z”翻译成三地址代码是下面的样子，每行代码最多涉及三个地址，其中t1和t2是临时变量：
t1 := y * z t2 := x + t1 Java语言生成的字节码也是一种IR，我们还会介绍LLVM的IR，并且基于LLVM这个工具来加速我们后端的开发。
其实，IR这个词直译成中文，是“中间表示方式”的意思，不一定非是像汇编代码那样的一条条的指令。所以，AST其实也可以看做一种IR。我们在前端部分实现的脚本语言，就是基于AST这个IR来运行的。
每种IR的目的和用途是不一样的：
AST主要用于前端的工作。 Java的字节码，是设计用来在虚拟机上运行的。 LLVM的中间代码，主要是用于做代码翻译和编译优化的。 …… 总的来说，我们可以把各种语言翻译成中间代码，再针对每一种目标架构，通过一个程序将中间代码翻译成相应的汇编代码就可以了。然而事情真的这么简单吗？答案是否定的，因为我们还必须对代码进行优化。
代码分析和优化生成正确的、能够执行的代码比较简单，可这样的代码执行效率很低，因为直接翻译生成的代码往往不够简洁，比如会生成大量的临时变量，指令数量也较多。因为翻译程序首先照顾的是正确性，很难同时兼顾是否足够优化，这是一方面。另一方面，由于高级语言本身的限制和程序员的编程习惯，也会导致代码不够优化，不能充分发挥计算机的性能。所以我们一定要对代码做优化。程序员在比较各种语言的时候，一定会比较它们的性能差异。一个语言的性能太差，就会影响它的使用和普及。
实际上，就算是现在常见的脚本语言，如Python和JavaScript，也做了很多后端优化的工作，包括编译成字节码、支持即时编译等，这些都是为了进一步提高性能。从谷歌支持的开源项目V8开始，JavaScript的性能获得了巨大的提高，这才导致了JavaScript再一次的繁荣，包括支持体验更好的前端应用和基于Node.js的后端应用。
优化工作又分为“独立于机器的优化”和“依赖于机器的优化”两种。
独立于机器的优化，是基于IR进行的。它可以通过对代码的分析，用更加高效的代码代替原来的代码。比如下面这段代码中的foo()函数，里面有多个地方可以优化。甚至，我们连整个对foo()函数的调用，也可以省略，因为foo()的值一定是101。这些优化工作在编译期都可以去做。
int foo(){ int a = 10*10; //这里在编译时可以直接计算出100这个值 int b = 20; //这个变量没有用到，可以在代码中删除 if (a&amp;amp;gt;0){ //因为a一定大于0，所以判断条件和else语句都可以去掉 return a+1; //这里可以在编译器就计算出是101 } else{ return a-1; } } int a = foo(); //这里可以直接地换成 a=101; 上面的代码，通过优化，可以消除很多冗余的逻辑。这就好比你正在旅行，先从北京飞到了上海，然后又飞到厦门，最后飞回北京。然后你朋友问你现在在哪时，你告诉他在北京。那么他虽然知道你在北京，但并没有意识到你已经在几个城市折腾了一圈，因为他只关心你现在在哪儿，并不关心你的中间过程。 我们在给a赋值的时候，只需要知道这个值是101就行了。完全不需要在运行时去兜一大圈来计算。</description></item><item><title>21_运行时机制：突破现象看本质，透过语法看运行时</title><link>https://artisanbox.github.io/6/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/21/</guid><description>编译器的任务，是要生成能够在计算机上运行的代码，但要生成代码，我们必须对程序的运行环境和运行机制有比较透彻的了解。
你要知道，大型的、复杂一点儿的系统，比如像淘宝一样的电商系统、搜索引擎系统等等，都存在一些技术任务，是需要你深入了解底层机制才能解决的。比如淘宝的基础技术团队就曾经贡献过，Java虚拟机即时编译功能中的一个补丁。
这反映出掌握底层技术能力的重要性，所以，如果你想进阶成为这个层次的工程师，不能只学学上层的语法，而是要把计算机语言从上层的语法到底层的运行机制都了解透彻。
本节课，我会对计算机程序如何运行，做一个解密，话题分成两个部分：
1.了解程序运行的环境，包括CPU、内存和操作系统，探知它们跟程序到底有什么关系。
2.了解程序运行的过程。比如，一个程序是怎么跑起来的，代码是怎样执行和跳转的，又是如何管理内存的。
首先，我们先来了解一下程序运行的环境。
程序运行的环境程序运行的过程中，主要是跟两个硬件（CPU和内存）以及一个软件（操作系统）打交道。
本质上，我们的程序只关心CPU和内存这两个硬件。你可能说：“不对啊，计算机还有其他硬件，比如显示器和硬盘啊。”但对我们的程序来说，操作这些硬件，也只是执行某些特定的驱动代码，跟执行其他代码并没有什么差异。
1.关注CPU和内存CPU的内部有很多组成部分，对于本课程来说，我们重点关注的是寄存器以及高速缓存，它们跟程序的执行机制和优化密切相关。
寄存器是CPU指令在进行计算的时候，临时数据存储的地方。CPU指令一般都会用到寄存器，比如，典型的一个加法计算（c=a+b）的过程是这样的：
指令1（mov）：从内存取a的值放到寄存器中；
指令2（add）：再把内存中b的值取出来与这个寄存器中的值相加，仍然保存在寄存器中；
指令3（mov）：最后再把寄存器中的数据写回内存中c的地址。
寄存器的速度也很快，所以能用寄存器就别用内存。尽量充分利用寄存器，是编译器做优化的内容之一。
而高速缓存可以弥补CPU的处理速度和内存访问速度之间的差距。所以，我们的指令在内存读一个数据的时候，它不是老老实实地只读进当前指令所需要的数据，而是把跟这个数据相邻的一组数据都读进高速缓存了。这就相当于外卖小哥送餐的时候，不会为每一单来回跑一趟，而是一次取一批，如果这一批外卖恰好都是同一个写字楼里的，那小哥的送餐效率就会很高。
内存和高速缓存的速度差异差不多是两个数量级，也就是一百倍。比如，高速缓存的读取时间可能是0.5ns，而内存的访问时间可能是50ns。不同硬件的参数可能有差异，但总体来说是几十倍到上百倍的差异。
你写程序时，尽量把某个操作所需的数据都放在内存中的连续区域中，不要零零散散地到处放，这样有利于充分利用高速缓存。这种优化思路，叫做数据的局部性。
这里提一句，在写系统级的程序时，你要对各种IO的时间有基本的概念，比如高速缓存、内存、磁盘、网络的IO大致都是什么数量级的。因为这都影响到系统的整体性能，也影响到你如何做程序优化。如果你需要对程序做更多的优化，还需要了解更多的CPU运行机制，包括流水线机制、并行机制等等，这里就不展开了。
讲完CPU之后，还有内存这个硬件。
程序在运行时，操作系统会给它分配一块虚拟的内存空间，让它在运行期可以使用。我们目前使用的都是64位的机器，你可以用一个64位的长整型来表示内存地址，它能够表示的所有地址，我们叫做寻址空间。
64位机器的寻址空间就有2的64次方那么大，也就是有很多很多个TB（Terabyte），大到你的程序根本用不完。不过，操作系统一般会给予一定的限制，不会给你这么大的寻址空间，比如给到100来个G，这对一般的程序，也足够用了。
在存在操作系统的情况下，程序逻辑上可使用的内存一般大于实际的物理内存。程序在使用内存的时候，操作系统会把程序使用的逻辑地址映射到真实的物理内存地址。有的物理内存区域会映射进多个进程的地址空间。
对于不太常用的内存数据，操作系统会写到磁盘上，以便腾出更多可用的物理内存。
当然，也存在没有操作系统的情况，这个时候你的程序所使用的内存就是物理内存，我们必须自己做好内存的管理。
对于这个内存，该怎么用呢？
本质上来说，你想怎么用就怎么用，并没有什么特别的限制。一个编译器的作者，可以决定在哪儿放代码，在哪儿放数据，当然了，别的作者也可能采用其他的策略。实际上，C语言和Java虚拟机对内存的管理和使用策略就是不同的。
尽管如此，大多数语言还是会采用一些通用的内存管理模式。以C语言为例，会把内存划分为代码区、静态数据区、栈和堆。
一般来讲，代码区是在最低的地址区域，然后是静态数据区，然后是堆。而栈传统上是从高地址向低地址延伸，栈的最顶部有一块区域，用来保存环境变量。
代码区（也叫文本段）存放编译完成以后的机器码。这个内存区域是只读的，不会再修改，但也不绝对。现代语言的运行时已经越来越动态化，除了保存机器码，还可以存放中间代码，并且还可以在运行时把中间代码编译成机器码，写入代码区。
静态数据区保存程序中全局的变量和常量。它的地址在编译期就是确定的，在生成的代码里直接使用这个地址就可以访问它们，它们的生存期是从程序启动一直到程序结束。它又可以细分为Data和BSS两个段。Data段中的变量是在编译期就初始化好的，直接从程序装在进内存。BSS段中是那些没有声明初始化值的变量，都会被初始化成0。
堆适合管理生存期较长的一些数据，这些数据在退出作用域以后也不会消失。比如，我们在某个方法里创建了一个对象并返回，并希望代表这个对象的数据在退出函数后仍然可以访问。
而栈适合保存生存期比较短的数据，比如函数和方法里的本地变量。它们在进入某个作用域的时候申请内存，退出这个作用域的时候就可以释放掉。
讲完了CPU和内存之后，我们再来看看跟程序打交道的操作系统。
2.程序和操作系统的关系程序跟操作系统的关系比较微妙：
一方面我们的程序可以编译成不需要操作系统也能运行，就像一些物联网应用那样，完全跑在裸设备上。
另一方面，有了操作系统的帮助，可以为程序提供便利，比如可以使用超过物理内存的存储空间，操作系统负责进行虚拟内存的管理。
在存在操作系统的情况下，因为很多进程共享计算机资源，所以就要遵循一些约定。这就仿佛办公室是所有同事共享的，那么大家就都要遵守一些约定，如果一个人大声喧哗，就会影响到其他人。
程序需要遵守的约定包括：程序文件的二进制格式约定，这样操作系统才能程序正确地加载进来，并为同一个程序的多个进程共享代码区。在使用寄存器和栈的时候也要遵守一些约定，便于操作系统在不同的进程之间切换的时候、在做系统调用的时候，做好上下文的保护。
所以，我们编译程序的时候，要知道需要遵守哪些约定。因为就算是使用同样的CPU，针对不同的操作系统，编译的结果也是非常不同的。
好了，我们了解了程序运行时的硬件和操作系统环境。接下来，我们看看程序运行时，是怎么跟它们互动的。
程序运行的过程你天天运行程序，可对于程序运行的细节，真的清楚吗？
1.程序运行的细节首先，可运行的程序一般是由操作系统加载到内存的，并且定位到代码区里程序的入口开始执行。比如，C语言的main函数的第一行代码。
每次加载一条代码，程序都会顺序执行，碰到跳转语句，才会跳到另一个地址执行。CPU里有一个指令寄存器，里面保存了下一条指令的地址。
假设我们运行这样一段代码编译后形成的程序：
int main(){ int a = 1; foo(3); bar(); } int foo(int c){ int b = 2; return b+c; }</description></item><item><title>22_生成汇编代码（一）：汇编语言其实不难学</title><link>https://artisanbox.github.io/6/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/22/</guid><description>敲黑板：课程用的是GNU汇编器，macOS和Linux已内置，本文的汇编语言的写法是GNU汇编器规定的写法。Windows系统可安装MinGW或Linux虚拟机。
对于静态编译型语言，比如C语言和Go语言，编译器后端的任务就是生成汇编代码，然后再由汇编器生成机器码，生成的文件叫目标文件，最后再使用链接器就能生成可执行文件或库文件了。
就算像JavaScript这样的解释执行的语言，也要在运行时利用类似的机制生成机器码，以便调高执行的速度。Java的字节码，在运行时通常也会通过JIT机制编译成机器码。而汇编语言是完成这些工作的基础。
对你来说，掌握汇编语言是十分有益的，因为哪怕掌握一小点儿汇编技能，就能应用到某项工作中，比如，在C语言里嵌入汇编，实现某个特殊功能；或者读懂某些底层类库或驱动程序的代码，因为它可能是用汇编写的。
本节课，我先带你了解一下汇编语言，来个破冰之旅。然后在接下来的课程中再带你基于AST手工生成汇编代码，破除你对汇编代码的恐惧，了解编译期后端生成汇编代码的原理。
以后，当你看到高级语言的代码，以及IR时，就可以想象出来它对应的汇编代码是什么样子，实现从上层到底层认知的贯通。
了解汇编语言机器语言都是0101的二进制的数据，不适合我们阅读。而汇编语言，简单来说，是可读性更好的机器语言，基本上每条指令都可以直接翻译成一条机器码。
跟你日常使用的高级语言相比，汇编语言的语法特别简单，但它要跟硬件（CPU和内存）打交道，我们来体会一下。
计算机的处理器有很多不同的架构，比如x86-64、ARM、Power等，每种处理器的指令集都不相同，那也就意味着汇编语言不同。我们目前用的电脑，CPU一般是x86-64架构，是64位机。（如不做特别说明，本课程都是以x86-64架构作为例子的）。
说了半天，汇编代码长什么样子呢？我用C语言写的例子来生成一下汇编代码。
#include &amp;lt;stdio.h&amp;gt; int main(int argc, char* argv[]){ printf(&amp;quot;Hello %s!\n&amp;quot;, &amp;quot;Richard&amp;quot;); return 0; } 在macOS中输入下面的命令，其中的-S参数就是告诉编译器把源代码编译成汇编代码，而-O2参数告诉编译器进行2级优化，这样生成的汇编代码会短一些：
clang -S -O2 hello.c -o hello.s 或者： gcc -S -O2 hello.c -o hello.s 生成的汇编代码是下面的样子：
.section __TEXT,__text,regular,pure_instructions .build_version macos, 10, 14 sdk_version 10, 14 .globl _main ## -- Begin function main .p2align 4, 0x90 _main: ## @main .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .</description></item><item><title>23_生成汇编代码（二）：把脚本编译成可执行文件</title><link>https://artisanbox.github.io/6/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/23/</guid><description>学完两节课之后，对于后端编译过程，你可能还会产生一些疑问，比如：
1.大致知道汇编程序怎么写，却不知道如何从AST生成汇编代码，中间有什么挑战。
2.编译成汇编代码之后需要做什么，才能生成可执行文件。
本节课，我会带你真正动手，基于AST把playscript翻译成正确的汇编代码，并将汇编代码编译成可执行程序。
通过这样一个过程，可以实现从编译器前端到后端的完整贯通，帮你对编译器后端工作建立比较清晰的认识。这样一来，你在日常工作中进行大型项目的编译管理的时候，或者需要重用别人的类库的时候，思路会更加清晰。
从playscript生成汇编代码先来看看如何从playscript生成汇编代码。
我会带你把playscript的几个有代表性的功能，而不是全部的功能翻译成汇编代码，一来工作量少一些，二来方便做代码优化。这几个有代表性的功能如下：
1.支持函数调用和传参（这个功能可以回顾加餐）。
2.支持整数的加法运算（在这个过程中要充分利用寄存器提高性能）。
3.支持变量声明和初始化。
具体来说，要能够把下面的示例程序正确生成汇编代码：
//asm.play int fun1(int x1, int x2, int x3, int x4, int x5, int x6, int x7, int x8){ int c = 10; return x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + c; } println(&amp;quot;fun1:&amp;quot; + fun1(1,2,3,4,5,6,7,8)); 在加餐中，我提供了一段手写的汇编代码，功能等价于这段playscript代码，并讲述了如何在多于6个参数的情况下传参，观察栈帧的变化过程，你可以看看下面的图片和代码，回忆一下：
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;# function-call2-craft.s 函数调用和参数传递 # 文本段,纯代码 .section __TEXT,__text,regular,pure_instructions
_fun1: # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来</description></item><item><title>24_中间代码：兼容不同的语言和硬件</title><link>https://artisanbox.github.io/6/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/24/</guid><description>前几节课，我带你尝试不通过IR，直接生成汇编代码，这是为了帮你快速破冰，建立直觉。在这个过程中，你也遇到了一些挑战，比如：
你要对生成的代码进行优化，才有可能更好地使用寄存器和内存，同时也能减少代码量；
另外，针对不同的CPU和操作系统，你需要调整生成汇编代码的逻辑。
这些实际体验，都进一步验证了20讲中，IR的作用：我们能基于IR对接不同语言的前端，也能对接不同的硬件架构，还能做很多的优化。
既然IR有这些作用，那你可能会问，IR都是什么样子的呢？有什么特点？如何生成IR呢？
本节课，我就带你了解IR的特点，认识常见的三地址代码，学会如何把高级语言的代码翻译成IR。然后，我还会特别介绍LLVM的IR，以便后面使用LLVM这个工具。
首先，来看看IR的特征。
介于中间的语言IR的意思是中间表达方式，它在高级语言和汇编语言的中间，这意味着，它的特征也是处于二者之间的。
与高级语言相比，IR丢弃了大部分高级语言的语法特征和语义特征，比如循环语句、if语句、作用域、面向对象等等，它更像高层次的汇编语言；而相比真正的汇编语言，它又不会有那么多琐碎的、与具体硬件相关的细节。
相信你在学习汇编语言的时候，会发现汇编语言的细节特别多。比如，你要知道很多指令的名字和用法，还要记住很多不同的寄存器。在22讲，我提到，如果你想完整地掌握x86-64架构，还需要接触很多指令集，以及调用约定的细节、内存使用的细节等等（参见Intel的手册）。
仅仅拿指令的数量来说，据有人统计，Intel指令的助记符有981个之多！都记住怎么可能啊。所以说，汇编语言并不难，而是麻烦。
IR不会像x86-64汇编语言那么繁琐，但它却包含了足够的细节信息，能方便我们实现优化算法，以及生成针对目标机器的汇编代码。
另外，我在20讲提到，IR有很多种类（AST也是一种IR），每种IR都有不同的特点和用途，有的编译器，甚至要用到几种不同的IR。
我们在后端部分所讲的IR，目的是方便执行各种优化算法，并有利于生成汇编。这种IR，可以看做是一种高层次的汇编语言，主要体现在：
它可以使用寄存器，但寄存器的数量没有限制； 控制结构也跟汇编语言比较像，比如有跳转语句，分成多个程序块，用标签来标识程序块等； 使用相当于汇编指令的操作码。这些操作码可以一对一地翻译成汇编代码，但有时一个操作码会对应多个汇编指令。 下面来看看一个典型IR：三地址代码，简称TAC。
认识典型的IR：三地址代码（TAC）下面是一种常见的IR的格式，它叫做三地址代码（Three Address Code, TAC），它的优点是很简洁，所以适合用来讨论算法：
x := y op z //二元操作 x := op y //一元操作 每条三地址代码最多有三个地址，其中两个是源地址（比如第一行代码的y和z），一个是目的地址（也就是x），每条代码最多有一个操作（op）。
我来举几个例子，带你熟悉一下三地址代码，这样，你能掌握三地址代码的特点，从高级语言的代码转换生成三地址代码。
1.基本的算术运算：
int a, b, c, d; a = b + c * d; TAC：
t1 := c * d a := b + t1 t1是新产生的临时变量。当源代码的表达式中包含一个以上的操作符时，就需要引入临时变量，并把原来的一条代码拆成多条代码。
2.布尔值的计算：
int a, b; bool x, y; x = a * 2 &amp;lt; b; y = a + 3 == b; TAC：</description></item><item><title>25_后端技术的重用：LLVM不仅仅让你高效</title><link>https://artisanbox.github.io/6/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/25/</guid><description>在编译器后端，做代码优化和为每个目标平台生成汇编代码，工作量是很大的。那么，有什么办法能降低这方面的工作量，提高我们的工作效率呢？答案就是利用现成的工具。
在前端部分，我就带你使用Antlr生成了词法分析器和语法分析器。那么在后端部分，我们也可以获得类似的帮助，比如利用LLVM和GCC这两个后端框架。
相比前端的编译器工具，如Lex（Flex）、Yacc（Bison）和Antlr等，对于后端工具，了解的人比较少，资料也更稀缺，如果你是初学者，那么上手的确有一些难度。不过我们已经用20～24讲，铺垫了必要的基础知识，也尝试了手写汇编代码，这些知识足够你学习和掌握后端工具了。
本节课，我想先让你了解一些背景信息，所以会先概要地介绍一下LLVM和GCC这两个有代表性的框架的情况，这样，当我再更加详细地讲解LLVM，带你实际使用一下它的时候，你接受起来就会更加容易了。
两个编译器后端框架：LLVM和GCCLLVM是一个开源的编译器基础设施项目，主要聚焦于编译器的后端功能（代码生成、代码优化、JIT……）。它最早是美国伊利诺伊大学的一个研究性项目，核心主持人员是Chris Lattner（克里斯·拉特纳）。
LLVM的出名是由于苹果公司全面采用了这个框架。苹果系统上的C语言、C++、Objective-C的编译器Clang就是基于LLVM的，最新的Swift编程语言也是基于LLVM，支撑了无数的移动应用和桌面应用。无独有偶，在Android平台上最新的开发语言Kotlin，也支持基于LLVM编译成本地代码。
另外，由Mozilla公司（Firefox就是这个公司的产品）开发的系统级编程语言RUST，也是基于LLVM开发的。还有一门相对小众的科学计算领域的语言，叫做Julia，它既能像脚本语言一样灵活易用，又可以具有C语言一样的速度，在数据计算方面又有特别的优化，它的背后也有LLVM的支撑。
OpenGL和一些图像处理领域也在用LLVM，我还看到一个资料，说阿里云的工程师实现了一个Cava脚本语言，用于配合其搜索引擎系统HA3。
LLVM的logo，一只漂亮的龙：
还有，在人工智能领域炙手可热的TensorFlow框架，在后端也是用LLVM来编译。它把机器学习的IR翻译成LLVM的IR，然后再翻译成支持CPU、GPU和TPU的程序。
所以这样看起来，你所使用的很多语言和工具，背后都有LLVM的影子，只不过你可能没有留意罢了。所以在我看来，要了解编译器的后端技术，就不能不了解LLVM。
与LLVM起到类似作用的后端编译框架是GCC（GNU Compiler Collection，GNU编译器套件）。它支持了GNU Linux上的很多语言，例如C、C++、Objective-C、Fortran、Go语言和Java语言等。其实，它最初只是一个C语言的编译器，后来把公共的后端功能也提炼了出来，形成了框架，支持多种前端语言和后端平台。最近华为发布的方舟编译器，据说也是建立在GCC基础上的。
LLVM和GCC很难比较优劣，因为这两个项目都取得了很大的成功。
在本课程中，我们主要采用LLVM，但其中学到的一些知识，比如IR的设计、代码优化算法、适配不同硬件的策略，在学习GCC或其他编译器后端的时候，也是有用的，从而大大提升学习效率。
接下来，我们先来看看LLVM的构成和特点，让你对它有个宏观的认识。
了解LLVM的特点LLVM能够支持多种语言的前端、多种后端CPU架构。在LLVM内部，使用类型化的和SSA特点的IR进行各种分析、优化和转换：
LLVM项目包含了很多组成部分：
LLVM核心（core）。就是上图中的优化和分析工具，还包括了为各种CPU生成目标代码的功能；这些库采用的是LLVM IR，一个良好定义的中间语言，在上一讲，我们已经初步了解它了。
Clang前端（是基于LLVM的C、C++、Objective-C编译器）。
LLDB（一个调试工具）。
LLVM版本的C++标准类库。
其他一些子项目。
我个人很喜欢LLVM，想了想，主要有几点原因：
首先，LLVM有良好的模块化设计和接口。以前的编译器后端技术很难复用，而LLVM具备定义了良好接口的库，方便使用者选择在什么时候，复用哪些后端功能。比如，针对代码优化，LLVM提供了很多算法，语言的设计者可以自己选择合适的算法，或者实现自己特殊的算法，具有很好的灵活性。
第二，LLVM同时支持JIT（即时编译）和AOT（提前编译）两种模式。过去的语言要么是解释型的，要么编译后运行。习惯了使用解释型语言的程序员，很难习惯必须等待一段编译时间才能看到运行效果。很多科学工作者，习惯在一个REPL界面中一边写脚本，一边实时看到反馈。LLVM既可以通过JIT技术支持解释执行，又可以完全编译后才执行，这对于语言的设计者很有吸引力。
第三，有很多可以学习借鉴的项目。Swift、Rust、Julia这些新生代的语言，实现了很多吸引人的特性，还有很多其他的开源项目，而我们可以研究、借鉴它们是如何充分利用LLVM的。
第四，全过程优化的设计思想。LLVM在设计上支持全过程的优化。Lattner和Adve最早关于LLVM设计思想的文章《LLVM: 一个全生命周期分析和转换的编译框架》，就提出计算机语言可以在各个阶段进行优化，包括编译时、链接时、安装时，甚至是运行时。
以运行时优化为例，基于LLVM我们能够在运行时，收集一些性能相关的数据对代码编译优化，可以是实时优化的、动态修改内存中的机器码；也可以收集这些性能数据，然后做离线的优化，重新生成可执行文件，然后再加载执行，这一点非常吸引我，因为在现代计算环境下，每种功能的计算特点都不相同，确实需要针对不同的场景做不同的优化。下图展现了这个过程（图片来源《 LLVM: A Compilation Framework for Lifelong Program Analysis &amp;amp; Transformation》）：
我建议你读一读Lattner和Adve的这篇论文（另外强调一下，当你深入学习编译技术的时候，阅读领域内的论文就是必不可少的一项功课了）。
第五，LLVM的授权更友好。GNU的很多软件都是采用GPL协议的，所以如果用GCC的后端工具来编写你的语言，你可能必须要按照GPL协议开源。而LLVM则更友好一些，你基于LLVM所做的工作，完全可以是闭源的软件产品。
而我之所以说：“LLVM不仅仅让你更高效”，就是因为上面它的这些特点。
现在，你已经对LLVM的构成和特点有一定的了解了，接下来，我带你亲自动手操作和体验一下LLVM的功能，这样你就可以迅速消除对它的陌生感，快速上手了。
体验一下LLVM的功能首先你需要安装一下LLVM（参照官方网站上的相关介绍下载安装）。因为我使用的是macOS，所以用brew就可以安装。
brew install llvm 因为LLVM里面带了一个版本的Clang和C++的标准库，与本机原来的工具链可能会有冲突，所以brew安装的时候并没有在/usr/local下建立符号链接。你在用LLVM工具的时候，要配置好相关的环境变量。
# 可执行文件的路径 export PATH=&amp;quot;/usr/local/opt/llvm/bin:$PATH&amp;quot; # 让编译器能够找到LLVM export LDFLAGS=&amp;quot;-L/usr/local/opt/llvm/lib&amp;quot; export CPPFLAGS=&amp;quot;-I/usr/local/opt/llvm/include” 安装完毕之后，我们使用一下LLVM自带的命令行工具，分几步体验一下LLVM的功能：</description></item><item><title>26_生成IR：实现静态编译的语言</title><link>https://artisanbox.github.io/6/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/26/</guid><description>目前来讲，你已经初步了解了LLVM和它的IR，也能够使用它的命令行工具。不过，我们还是要通过程序生成LLVM的IR，这样才能复用LLVM的功能，从而实现一门完整的语言。
不过，如果我们要像前面生成汇编语言那样，通过字符串拼接来生成LLVM的IR，除了要了解LLVM IR的很多细节之外，代码一定比较啰嗦和复杂，因为字符串拼接不是结构化的方法，所以，最好用一个定义良好的数据结构来表示IR。
好在LLVM项目已经帮我们考虑到了这一点，它提供了代表LLVM IR的一组对象模型，我们只要生成这些对象，就相当于生成了IR，这个难度就低多了。而且，LLVM还提供了一个工具类，IRBuilder，我们可以利用它，进一步提升创建LLVM IR的对象模型的效率，让生成IR的过程变得更加简单！
接下来，就让我们先来了解LLVM IR的对象模型。
LLVM IR的对象模型LLVM在内部有用C++实现的对象模型，能够完整表示LLVM IR，当我们把字节码读入内存时，LLVM就会在内存中构建出这个模型。只有基于这个对象模型，我们才可以做进一步的工作，包括代码优化，实现即时编译和运行，以及静态编译生成目标文件。所以说，这个对象模型是LLVM运行时的核心。
IR对象模型的头文件在include/llvm/IR目录下，其中最重要的类包括：
Module（模块） Module类聚合了一个模块中的所有数据，它可以包含多个函数。你可以通过Model::iterator来遍历模块中所有的函数。它也包含了一个模块的全局变量。
Function（函数） Function包含了与函数定义（definition）或声明（declaration）有关的所有对象。函数定义包含了函数体，而函数声明，则仅仅包含了函数的原型，它是在其他模块中定义的，在本模块中使用。
你可以通过getArgumentList()方法来获得函数参数的列表，也可以遍历函数体中的所有基本块，这些基本块会形成一个CFG（控制流图）。
//函数声明，没有函数体。这个函数是在其他模块中定义的，在本模块中使用 declare void @foo(i32) //函数定义，包含函数体 define i32 @fun3(i32 %a) { %calltmp1 = call void @foo(i32 %a) //调用外部函数 ret i32 10 }
BasicBlock（基本块） BasicBlock封装了一系列的LLVM指令，你可以借助bigin()/end()模式遍历这些指令，还可以通过getTerminator()方法获得最后一条指令（也就是终结指令）。你还可以用到几个辅助方法在CFG中导航，比如获得某个基本块的前序基本块。
Instruction（指令） Instruction类代表了LLVM IR的原子操作（也就是一条指令），你可以通过getOpcode()来获得它代表的操作码，它是一个llvm::Instruction枚举值，你可以通过op_begin()和op_end()方法对获得这个指令的操作数。
Value（值） Value类代表一个值。在LLVM的内存IR中，如果一个类是从Value继承的，意味着它定义了一个值，其他方可以去使用。函数、基本块和指令都继承了Value。
LLVMContext（上下文） 这个类代表了LLVM做编译工作时的一个上下文，包含了编译工作中的一些全局数据，比如各个模块用到的常量和类型。
这些内容是LLVM IR对象模型的主要部分，我们生成IR的过程，就是跟这些类打交道，其他一些次要的类，你可以在阅读和编写代码的过程中逐渐熟悉起来。
接下来，就让我们用程序来生成LLVM的IR。
尝试生成LLVM IR我刚刚提到的每个LLVM IR类，都可以通过程序来构建。那么，为下面这个fun1()函数生成IR，应该怎么办呢？
int fun1(int a, int b){ return a+b; } 第一步，我们可以来生成一个LLVM模块，也就是顶层的IR对象。
Module *mod = new Module(&amp;quot;fun1.</description></item><item><title>27_代码优化：为什么你的代码比他的更高效？</title><link>https://artisanbox.github.io/6/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/27/</guid><description>在使用LLVM的过程中，你应该觉察到了，优化之后和优化之前的代码相差很大。代码优化之后，数量变少了，性能也更高了。而针对这个看起来很神秘的代码优化，我想问你一些问题：
代码优化的目标是什么？除了性能上的优化，还有什么优化？ 代码优化可以在多大的范围内执行？是在一个函数内，还是可以针对整个应用程序？ 常见的代码优化场景有哪些？ 这些问题是代码优化的基本问题，很重要，我会用两节课的时间带你了解和掌握。
当然了，代码优化是编译器后端的两大工作之一（另一个是代码生成），弄懂它，你就掌握了一大块后端技术。而学习代码优化的原理，然后通过LLVM实践一下，这样原理与实践相结合，会帮你早日弄懂代码优化。
接下来，我带你概要地了解一下代码优化的目标、对象、范围和策略等内容。
了解代码优化的目标、对象、范围和策略 代码优化的目标 代码优化的目标，是优化程序对计算机资源的使用。我们平常最关心的就是CPU资源，最大效率地利用CPU资源可以提高程序的性能。代码优化有时候还会有其他目标，比如代码大小、内存占用大小、磁盘访问次数、网络通讯次数等等。
代码优化的对象 从代码优化的对象看，大多数的代码优化都是在IR上做的，而不是在前一阶段的AST和后一阶段汇编代码上进行的，为什么呢？
其实，在AST上也能做一些优化，比如在讲前端内容的时候，我们曾经会把一些不必要的AST层次削减掉（例如add-&amp;gt;mul-&amp;gt;pri-&amp;gt;Int，每个父节点只有一个子节点，可以直接简化为一个Int节点），但它抽象层次太高，含有的硬件架构信息太少，难以执行很多优化算法。 在汇编代码上进行优化会让算法跟机器相关，当换一个目标机器的时候，还要重新编写优化代码。所以，在IR上是最合适的，它能尽量做到机器独立，同时又暴露出很多的优化机会。
代码优化的范围 从优化的范围看，分为本地优化、全局优化和过程间优化。
优化通常针对一组指令，最常用也是最重要的指令组，就是基本块。基本块的特点是：每个基本块只能从入口进入，从最后一条指令退出，每条指令都会被顺序执行。因着这个特点，我们在做某些优化时会比较方便。比如，针对下面的基本块，我们可以很安全地把第3行的“y:=t+x”改成“y:= 3 * x”，因为t的赋值一定是在y的前面：
BB1: t:=2 * x y:=t + x Goto BB2 这种针对基本块的优化，我们叫做本地优化（Local Optimization）。
那么另一个问题来了：我们能否把第二行的“t:=2 * x”也优化删掉呢？这取决于是否有别的代码会引用t。所以，我们需要进行更大范围的分析，才能决定是否把第二行优化掉。
超越基本块的范围进行分析，我们需要用到控制流图（Control Flow Graph，CFG）。CFG是一种有向图，它体现了基本块之前的指令流转关系。如果从BB1的最后一条指令是跳转到BB2，那么从BB1到BB2就有一条边。一个函数（或过程）里如果包含多个基本块，可以表达为一个CFG。
如果通过分析CFG，我们发现t在其他地方没有被使用，就可以把第二行删掉。这种针对一个函数、基于CFG的优化，叫做全局优化（Global Optimization）。
比全局优化更大范围的优化，叫做过程间优化（Inter-procedural Optimization），它能跨越函数的边界，对多个函数之间的关系进行优化，而不是仅针对一个函数做优化。
代码优化的策略 最后，你不需要每次都把代码优化做彻底，因为做代码优化本身也需要消耗计算机的资源。所以，你需要权衡代码优化带来的好处和优化本身的开支这两个方面，然后确定做多少优化。比如，在浏览器里加载JavaScript的时候，JavaScript引擎一定会对JavaScript做优化，但如果优化消耗的时间太长，界面的响应会变慢，反倒影响用户使用页面的体验，所以JavaScript引擎做优化时要掌握合适的度或调整优化时机。
接下来，我带你认识一些常见的代码优化的场景，这样可以让你对代码优化的认识更加直观，然后我们也可以将这部分知识作为后面讨论算法的基础。
一些优化的场景 代数优化（Algebraic Optimazation） 代数优化是最简单的一种优化，当操作符是代数运算的时候，你可以根据学过的数学知识进行优化。
比如“x:=x+0 ”这行代码，操作前后x没有任何变化，所以这样的代码可以删掉；又比如“x:=x*0” 可以简化成“x:=0”；对某些机器来说，移位运算的速度比乘法的快，那么“x:=x*8”可以优化成“x:=x&amp;lt;&amp;lt;3”。
常数折叠（Constant Folding） 它是指，对常数的运算可以在编译时计算，比如 “x:= 20 * 3 ”可以优化成“x:=60”。另外，在if条件中，如果条件是一个常量，那就可以确定地取某个分支。比如：“If 2&amp;gt;0 Goto BB2” 可以简化成“Goto BB2”就好了。</description></item><item><title>28_数据流分析：你写的程序，它更懂</title><link>https://artisanbox.github.io/6/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/28/</guid><description>上一讲，我提到了删除公共子表达式、拷贝传播等本地优化能做的工作，其实，这几个工作也可以在全局优化中进行。
只不过，全局优化中的算法，不会像在本地优化中一样，只针对一个基本块。而是更复杂一些，因为要覆盖多个基本块。这些基本块构成了一个CFG，代码在运行时有多种可能的执行路径，这会造成多路径下，值的计算问题，比如活跃变量集合的计算。
当然了，还有些优化只能在全局优化中做，在本地优化中做不了，比如：
代码移动（code motion）能够将代码从一个基本块挪到另一个基本块，比如从循环内部挪到循环外部，来减少不必要的计算。 部分冗余删除（Partial Redundancy Elimination），它能把一个基本块都删掉。 总之，全局优化比本地优化能做的工作更多，分析算法也更复杂，因为CFG中可能存在多条执行路径。不过，我们可以在上一节课提到的本地优化的算法思路上，解决掉多路径情况下，V值的计算问题。而这种基于CFG做优化分析的方法框架，就叫做数据流分析。
本节课，我会把全局优化的算法思路讲解清楚，借此引入数据流分析的完整框架。而且在解决多路径情况下，V值的计算问题时，我还会带你学习一个数学工具：半格理论。这样，你会对基于数据流分析的代码优化思路建立清晰的认识，从而有能力根据需要编写自己的优化算法。
数据流分析的场景：活跃性分析上一讲，我已经讲了本地优化时的活跃性分析，那时，情况比较简单，你不需要考虑多路径问题。而在做全局优化时，情况就要复杂一些：代码不是在一个基本块里简单地顺序执行，而可能经过控制流图（CFG）中的多条路径。我们来看一个例子（例子由if语句形成了两条分支语句）：
基于这个CFG，我们可以做全局的活跃性分析，从最底下的基本块开始，倒着向前计算活跃变量的集合（也就是从基本块5倒着向基本块1计算）。
这里需要注意，对基本块1进行计算的时候，它的输入是基本块2的输出，也就是{a, b, c}，和基本块3的输出，也就是{a, c}，计算结果是这两个集合的并集{a, b, c}。也就是说，基本块1的后序基本块，有可能用到这三个变量。这里就是与本地优化不同的地方，我们要基于多条路径来计算。
基于这个分析图，我们马上发现y变量可以被删掉（因为它前面的活变量集合{x}不包括y，也就是不被后面的代码所使用），并且影响到了活跃变量的集合。
删掉y变量以后，再继续优化一轮，会发现d也可以删掉。
d删掉以后，2号基本块里面已经没有代码了，也可以被删掉，最后的CFG是下面这样：
到目前为止，我们发现：全局优化总体来说跟本地优化很相似，唯一的不同，就是要基于多个分支计算集合的内容（也就是V值）。在进入基本块1时，2和3两个分支相遇（meet），我们取了2和3V值的并集。这就是数据流分析的基本特征，你可以记住这个例子，建立直观印象。
但是，上面这个CFG还是比较简单的，因为它没有循环，属于有向无环图。这种图的特点是：针对图中的每一个节点，我们总能找到它的前序节点和后序节点，所以我们只需要按照顺序计算就好了。但是如果加上了环路，就不那么简单了，来看一看下面这张图：
基本块4有两个后序节点，分别是5和1，所以要计算4的活跃变量，就需要知道5和1的输出是什么。5的输出好说，但1的呢？还没计算出来呢。因为要计算1，就要依赖2和3，从而间接地又依赖了4。这样一来，1和4是循环依赖的。再进一步探究的话，你发现其实1、2、3、4四个节点之间，都是循环依赖的。
所以说，一旦在CFG中引入循环回路，严格的前后计算顺序就不存在了。那你要怎么办呢？
其实，我们不是第一次面对这个处境了。在前端部分，我们计算First和Follow集合的时候，就会遇到循环依赖的情况，只不过那时候没有像这样展开，细细地分析。不过，你可以回顾一下17讲和18讲，那个时候你是用什么算法来破解僵局的呢？是不动点法。在这里，我们还是要运用不动点法，具体操作是：给每个基本块的V值都分配初始值，也就是空集合。
然后对所有节点进行多次计算，直到所有集合都稳定为止。第一遍的时候，我们按照5-4-3-2-1的顺序计算（实际上，采取任何顺序都可以），计算结果如下：
如果现在计算就结束，我们实际上可以把基本块2中的d变量删掉。但如果我们再按照5-4-3-2-1的顺序计算一遍，就会往集合里增加一些新的元素（在图中标的是橙色）。这是因为，在计算基本块4的时候，基本块1的输出{b, c, d}也会变成4的输入。这时，我们发现，进入基本块2时，活变量集合里是含有d的，所以d是不能删除的。
你再仔细看看，这个d是哪里需要的呢？是基本块3需要的：它会跟1去要，1会跟4要，4跟2要。所以，再次证明，1、2、3、4四个节点是互相依赖的。
我们再来看一下，对于活变量集合的计算，当两个分支相遇的情况下，最终的结果我们取了两个分支的并集。
在上一讲，我们说一个本地优化分析包含四个元素：方向（D）、值（V）、转换函数（F）和初始值（I）。在做全局优化的时候，我们需要再多加一个元素，就是两个分支相遇的时候，要做一个运算，计算他们相交的值，这个运算我们可以用大写的希腊字母Λ（lambda）表示。包含了D、V、F、I和Λ的分析框架，就叫做数据流分析。
那么Λ怎么计算呢？研究者们用了一个数学工具，叫做“半格”（Semilattice），帮助做Λ运算。
直观地理解半格理论如果要从数学理论角度完全把“半格”这个概念说清楚，需要依次介绍清楚“格”（Lattice）、“半格”（Semilattice）和“偏序集”（Partially Ordered Set）等概念。我想这个可以作为爱好数学的同学的一个研究题目，或者去向离散数学的老师求教。在我们的课程里，我只是通过举例子，让你对它有直观的认识。
首先，半格是一种偏序集。偏序集就是集合中只有部分成员能够互相比较大小。举例来说会比较直观。在做全局活跃性分析的时候，{a, b, c}和{a, c}相遇，产生的新值是{a, b, c}。我们形式化地写成{a, b, c} Λ {a, c} = {a, b, c}。
这时候我们说{a, b, c}是可以跟{a, c}比较大小的。那么哪个大哪个小呢？
如果XΛY=X，我们说X&amp;lt;=Y。
所以，{a, b, c}是比较小的，{a, c}是比较大的。
当然，{a, b, c}也可以跟{a, b}比较大小，但它没有办法跟{c, d}比较大小。所以把包含了{{a, b, c}、{a, c}、{a, b}、{c, d}…}这样的一个集合，叫做偏序集，它们中只有部分成员之间可以比较大小。哪些成员可以比较呢？就是下面的半格图中，可以通过有方向的线连起来的。</description></item><item><title>29_目标代码的生成和优化（一）：如何适应各种硬件架构？</title><link>https://artisanbox.github.io/6/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/29/</guid><description>在编译器的后端，我们要能够针对不同的计算机硬件，生成优化的代码。在23讲，我曾带你试着生成过汇编代码，但当时生成汇编代码的逻辑是比较幼稚的，一个正式的编译器后端，代码生成部分需要考虑得更加严密才可以。
那么具体要考虑哪些问题呢？其实主要有三点：
指令的选择。同样一个功能，可以用不同的指令或指令序列来完成，而我们需要选择比较优化的方案。
寄存器分配。每款CPU的寄存器都是有限的，我们要有效地利用它。
指令重排序。计算执行的次序会影响所生成的代码的效率。在不影响运行结果的情况下，我们要通过代码重排序获得更高的效率。
我会用两节课的时间，带你对这三点问题建立直观认识，然后，我还会介绍LLVM的实现策略。这样一来，你会对目标代码的生成，建立比较清晰的顶层认知，甚至可以尝试去实现自己的算法。
接下来，我们针对第一个问题，聊一聊为什么需要选择指令，以及如何选择指令。
选择正确的指令你可能会问：我们为什么非要关注指令的选择呢？我来做个假设。
如果我们不考虑目标代码的性能，可以按照非常机械的方式翻译代码。比如，我们可以制定一个代码翻译的模板，把形如“a := b + c”的代码都翻译成下面的汇编代码：
mov b, r0 //把b装入寄存器r0 add c, r0 //把c加到r0上 mov r0, a //把r0存入a 那么，下面两句代码：
a := b + c d := a + e 将被机械地翻译成：
mov b, r0 add c, r0 mov r0, a mov a, r0 add e, r0 mov r0, d 你可以从上面这段代码中看到，第4行其实是多余的，因为r0的值就是a，不用再装载一遍了。另外，如果后面的代码不会用到a（也就是说a只是个临时变量），那么第3行也是多余的。
这种算法很幼稚，正确性没有问题，但代码量太大，代价太高。所以我们最好用聪明一点儿的算法来生成更加优化的代码。这是我们要做指令选择的原因之一。
做指令选择的第二个原因是，实现同一种功能可以使用多种指令，特别是CISC指令集（可替代的选择很多，但各自有适用的场景）。
对于某个CPU来说，完成同样的任务可以采用不同的指令。比如，实现“a := a + 1”，可以生成三条代码：</description></item><item><title>30_目标代码的生成和优化（二）：如何适应各种硬件架构？</title><link>https://artisanbox.github.io/6/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/30/</guid><description>前一讲，我带你了解了指令选择和寄存器分配，本节课我们继续讲解目标代码生成的，第三个需要考虑的因素：指令重排序（Instruction Scheduling）。
我们可以通过重新排列指令，让代码的整体执行效率加快。那你可能会问了：就算重新排序了，每一条指令还是要执行啊？怎么就会变快了呢？
别着急，本节课我就带你探究其中的原理和算法，来了解这个问题。而且，我还会带你了解LLVM是怎么把指令选择、寄存器分配、指令重排序这三项工作组织成一个完整流程，完成目标代码生成的任务的。这样，你会对编译器后端的代码生成过程形成完整的认知，为正式做一些后端工作打下良好的基础。
首先，我们来看看指令重排序的问题。
指令重排序如果你有上面的疑问，其实是很正常的。因为我们通常会把CPU看做一个整体，把CPU执行指令的过程想象成，依此检票进站的过程，改变不同乘客的次序，并不会加快检票的速度。所以，我们会自然而然地认为改变顺序并不会改变总时间。
但当我们进入CPU内部，会看到CPU是由多个功能部件构成的。下图是Ice Lake微架构的CPU的内部构成（从Intel公司的技术手册中获取）：
在这个结构中，一条指令执行时，要依次用到多个功能部件，分成多个阶段，虽然每条指令是顺序执行的，但每个部件的工作完成以后，就可以服务于下一条指令，从而达到并行执行的效果。这种结构叫做流水线（pipeline）结构。我举例子说明一下，比如典型的RISC指令在执行过程会分成前后共5个阶段。
IF：获取指令； ID（或RF）：指令解码和获取寄存器的值； EX：执行指令； ME（或MEM）：内存访问（如果指令不涉及内存访问，这个阶段可以省略）； WB：写回寄存器。 对于CISC指令，CPU的流水线会根据指令的不同，分成更多个阶段，比如7个、10个甚至更多。
在执行指令的阶段，不同的指令也会由不同的单元负责，我们可以把这些单元叫做执行单元，比如，Intel的Ice Lake架构的CPU有下面这些执行单元：
其他执行单元还有：BM、Vec ALU、Vec SHFT、Vec Add、Vec Mul、Shuffle等。
因为CPU内部存在着多个功能单元，所以在同一时刻，不同的功能单元其实可以服务于不同的指令，看看下面这个图；
这样的话，多条指令实质上是并行执行的，从而减少了总的执行时间，这种并行叫做指令级并行：
如果没有这种并行结构，或者由于指令之间存在依赖关系，无法并行，那么执行周期就会大大加长：
我们来看一个实际的例子。
为了举例子方便，我们做个假设：假设load和store指令需要3个时钟周期来读写数据，add指令需要1个时钟周期，mul指令需要2个时钟周期。
图中橙色的编号是原来的指令顺序，绿色的数字是每条指令开始时的时钟周期，你把每条指令的时钟周期累计一下就能算出来。最后一条指令开始的时钟周期是20，该条指令运行需要3个时钟周期，所以在第22个时钟周期执行完所有的指令。右边是重新排序后的指令，一共花了13个时钟周期。这个优化力度还是很大的！
仔细看一下左边前两条指令，这两条指令的意思是：先加载数据到寄存器，然后做一个加法。但加载需要3个时钟周期，所以add指令无法执行，只能干等着。
右列的前三条都是load指令，它们之间没有数据依赖关系，我们可以每个时钟周期启动一个，到了第四个时钟周期，每一条指令的数据已经加载完毕，所以就可以执行加法运算了。
我们可以把右边的内容画成下面的样子，你能看到，很多指令在时钟周期上是重叠的，这就是指令级并行的特点。
当然了，不是所有的指令都可以并行，最后的3条指令就是顺序执行的，导致无法并行的原因有几个：
数据依赖约束 如果后一条指令要用到前一条指令的结果，那必须排在它后面，比如下面两条指令：add和mul。
对于第二条指令来说，除了获取指令的阶段（IF）可以和第一条指令并行以外，其他阶段需要等第一条指令的结果写入r1，第二条指令才可以使用r1的值继续运行。
add r2, r1 mul r3, r1 功能部件约束 如果只有一个乘法计算器，那么一次只能执行一条乘法运算。
指令流出约束 指令流出部件一次流出n条指令。
寄存器约束 寄存器数量有限，指令并行时使用的寄存器不可以超标。
后三者也可以合并成为一类，称作资源约束。
在数据依赖约束中，如果有因为使用同一个存储位置，而导致不能并行的，可以用重命名变量的方式消除，这类约束被叫做伪约束。而先写再写，以及先读后写是伪约束的两种呈现方式：
先写再写：如果指令A写一个寄存器或内存位置，B也写同一个位置，就不能改变A和B的执行顺序，不过我们可以修改程序，让A和B写不同的位置。
先读后写：如果A必须在B写某个位置之前读某个位置，那么不能改变A和B的执行顺序。除非能够通过重命名让它们使用不同的位置。
以上就是指令重排序的原理，掌握这个原理你就明白为什么重排序可以提升性能了，不过明白原理之后，我们还有能够用算法实现出来才行。
用算法排序的关键点，是要找出代码之间的数据依赖关系。下图展现了示例中各行代码之间的数据依赖，可以叫做数据的依赖图（dependence graph）。它的边代表了值的流动，比如a行加载了一个数据到r1，b行利用r1来做计算，所以b行依赖a行，这个图也可以叫做优先图（precedence graph），因为a比b优先，b比d优先。
我们可以给图中的每个节点再加上两个属性，利用这两个属性，就可以对指令进行排序了：
一是操作类型，因为这涉及它所需要的功能单元。 二是时延属性，也就是每条指令所需的时钟周期。 图中的a、c、e、g是叶子，它们没有依赖任何其他的节点，所以尽量排在前面。b、d、f、h必须出现在各自所依赖的节点后面。而根节点i，总是要排在最后面。</description></item><item><title>31_内存计算：对海量数据做计算，到底可以有多快？</title><link>https://artisanbox.github.io/6/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/31/</guid><description>内存计算是近十几年来，在数据库和大数据领域的一个热点。随着内存越来越便宜，CPU的架构越来越先进，整个数据库都可以放在内存中，并通过SIMD和并行计算技术，来提升数据处理的性能。
我问你一个问题：做1.6亿条数据的汇总计算，需要花费多少时间呢？几秒？几十秒？还是几分钟？如果你经常使用数据库，肯定会知道，我们不会在数据库的一张表中保存上亿条的数据，因为处理速度会很慢。
但今天，我会带你采用内存计算技术，提高海量数据处理工作的性能。与此同时，我还会介绍SIMD指令、高速缓存和局部性、动态优化等知识点。这些知识点与编译器后端技术息息相关，掌握这些内容，会对你从事基础软件研发工作，有很大的帮助。
了解SIMD本节课所采用的CPU，支持一类叫做SIMD（Single Instruction Multiple Data）的指令，它的字面意思是：单条指令能处理多个数据。相应的，你可以把每次只处理一个数据的指令，叫做SISD（Single Instruction Single Data）。
SISD使用普通的寄存器进行操作，比如加法：
addl $10, %eax 这行代码是把一个32位的整型数字，加到%eax寄存器上（在x86-64架构下，这个寄存器一共有64位，但这个指令只用它的低32位，高32位是闲置的）。
这种一次只处理一个数据的计算，叫做标量计算；一次可以同时处理多个数据的计算，叫做矢量计算。它在一个寄存器里可以并排摆下4个、8个甚至更多标量，构成一个矢量。图中ymm寄存器是256位的，可以支持同时做4个64位数的计算（xmm寄存器是它的低128位）。
如果不做64位整数，而做32位整数计算，一次能计算8个，如果做单字节（8位）数字的计算，一次可以算32个！
1997年，Intel公司推出了奔腾处理器，带有MMX指令集，意思是多媒体扩展。当时，让计算机能够播放多媒体（比如播放视频），是一个巨大的进步。但播放视频需要大量的浮点计算，依靠原来CPU的浮点运算功能并不够。
所以，Intel公司就引入了MMX指令集，和容量更大的寄存器来支持一条指令，同时计算多个数据，这是在PC上最早的SIMD指令集。后来，SIMD又继续发展，陆续产生了SSE（流式SIMD扩展）、AVX（高级矢量扩展）指令集，处理能力越来越强大。
2017年，Intel公司发布了一款至强处理器，支持AVX-512指令（也就是它的一个寄存器有512位）。每次能处理8个64位整数，或16个32位整数，或者32个双精度数、64个单精度数。你想想，一条指令顶64条指令，几十倍的性能提升，是不是很厉害！
那么你的电脑是否支持SIMD指令？又支持哪些指令集呢？在命令行终端，打下面的命令，你可以查看CPU所支持的指令集。
sysctl -a | grep features | grep cpu //macOs操作系统 cat /proc/cpuinfo //Linux操作系统 现在，想必你已经知道了SIMD指令的强大之处了。而它的实际作用主要有以下几点：
SIMD有助于多媒体的处理，比如在电脑上流畅地播放视频，或者开视频会议；
在游戏领域，图形渲染主要靠GPU，但如果你没有强大的GPU，还是要靠CPU的SIMD指令来帮忙；
在商业领域，数据库系统会采用SIMD来快速处理海量的数据；
人工智能领域，机器学习需要消耗大量的计算量，SIMD指令可以提升机器学习的速度。
你平常写的程序，编译器也会优化成，尽量使用SIMD指令来提高性能。
所以，我们所用到的程序，其实天天在都在执行SIMD指令。
接下来，我来演示一下如何使用SIMD指令，与传统的数据处理技术做性能上的对比，并探讨如何在编译器中生成SIMD指令，这样你可以针对自己的项目充分发挥SIMD指令的优势。
Intel公司为SIMD指令提供了一个标准的库，可以生成SIMD的汇编指令。我们写一个简单的程序（参考simd1.c）来对两组数据做加法运算，每组8个整数：
#include &amp;lt;stdio.h&amp;gt; #include &amp;quot;immintrin.h&amp;quot; void sum(){ //初始化两个矢量 ，8个32位整数 __m256i a=_mm256_set_epi32(20,30,40,60,342,34523,474,123); __m256i b=_mm256_set_epi32(234,234,456,78,2345,213,76,88);
//矢量加法 __m256i sum=_mm256_add_epi32(a, b);</description></item><item><title>32_字节码生成：为什么Spring技术很强大？</title><link>https://artisanbox.github.io/6/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/32/</guid><description>Java程序员几乎都了解Spring。它的IoC（依赖反转）和AOP（面向切面编程）功能非常强大、易用。而它背后的字节码生成技术（在运行时，根据需要修改和生成Java字节码的技术）就是一项重要的支撑技术。
Java字节码能够在JVM（Java虚拟机）上解释执行，或即时编译执行。其实，除了Java，JVM上的Groovy、Kotlin、Closure、Scala等很多语言，也都需要生成字节码。另外，playscript也可以生成字节码，从而在JVM上高效地运行！
而且，字节码生成技术很有用。你可以用它将高级语言编译成字节码，还可以向原来的代码中注入新代码，来实现对性能的监测等功能。
目前，我就有一个实际项目的需求。我们的一个产品，需要一个规则引擎，解析自定义的DSL，进行规则的计算。这个规则引擎处理的数据量比较大，所以它的性能越高越好。因此，如果把DSL编译成字节码就最理想了。
既然字节码生成技术有很强的实用价值，那么本节课，我就带你掌握它。
我会先带你了解Java的虚拟机和字节码的指令，然后借助ASM这个工具，生成字节码，最后，再实现从AST编译成字节码。通过这样一个过程，你会加深对Java虚拟机的了解，掌握字节码生成技术，从而更加了解Spring的运行机制，甚至有能力编写这样的工具！
Java虚拟机和字节码字节码是一种二进制格式的中间代码，它不是物理机器的目标代码，而是运行在Java虚拟机上，可以被解释执行和即时编译执行。
在讲后端技术时，我强调的都是，如何生成直接在计算机上运行的二进制代码，这比较符合C、C++、Go等静态编译型语言。但如果想要解释执行，除了直接解释执行AST以外，我没有讲其他解释执行技术。
而目前更常见的解释执行的语言，是采用虚拟机，其中最典型的就是JVM，它能够解释执行Java字节码。
而虚拟机的设计又有两种技术：一是基于栈的虚拟机；二是基于寄存器的虚拟机。
标准的JVM是基于栈的虚拟机（后面简称“栈机”）。
每一个线程都有一个JVM栈，每次调用一个方法都会生成一个栈桢，来支持这个方法的运行。栈桢里面又包含了本地变量数组（包括方法的参数和本地变量）、操作数栈和这个方法所用到的常数。这种栈桢的设计跟之前我们学过C语言的栈桢的结构，其实有很大的相似性，你可以通过21讲回顾一下。
栈机是基于操作数栈做计算的。以“2+3”的计算为例，只要把它转化成逆波兰表达式，“2 3 +”，然后按照顺序执行就可以了。也就是：先把2入栈，再把3入栈，再执行加法指令，这时，要从栈里弹出2个操作数做加法计算，再把结果压入栈。
你可以看出，栈机的加法指令，是不需要带操作数的，就是简单的“iadd”就行，这跟你之前学过的IR都不一样。为什么呢？因为操作数都在栈里，加法操作需要2个操作数，从栈里弹出2个元素就行了。
也就是说，指令的操作数是由栈确定的，我们不需要为每个操作数显式地指定存储位置，所以指令可以比较短，这是栈机的一个优点。
接下来，我们聊聊字节码的特点。
字节码是什么样子的呢？我编写了一个简单的类MyClass.java，其中的foo()方法实现了一个简单的加法计算，你可以看看它对应的字节码是怎样的：
public class MyClass { public int foo(int a){ return a + 3; } } 在命令行终端敲入下面两行命令，生成文本格式的字节码文件：
javac MyClass.java javap -v MyClass &amp;gt; MyClass.bc 打开MyClass.bc文件，你会看到下面的内容片段：
public int foo(int); Code: 0: iload_1 //把下标为1的本地变量入栈 1: iconst_3 //把常数3入栈 2: iadd //执行加法操作 3: ireturn //返回 其中，foo()方法一共有四条指令，前三条指令是计算一个加法表达式a+3。这完全是按照逆波兰表达式的顺序来执行的：先把一个本地变量入栈，再把常数3入栈，再执行加法运算。
如果你细心的话，应该会发现：把参数a入栈的第一条指令，用的下标是1，而不是0。这是因为，每个方法的第一个参数（下标为0）是当前对象实例的引用（this）。
我提供了字节码中，一些常用的指令，增加你对字节码特点的直观认识，完整的指令集可以参见JVM的规格书：
其中，每个指令都是8位的，占一个字节，而且iload_0、iconst_0这种指令，甚至把操作数（变量的下标、常数的值）压缩进了操作码里，可以看出，字节码的设计很注重节省空间。
根据这些指令所对应的操作码的数值，MyClass.bc文件中，你所看到的那四行代码，变成二进制格式，就是下面的样子：
你可以用“hexdump MyClass.class”显示字节码文件的内容，从中可以发现这个片段（就是橙色框里的内容）：
现在，你已经初步了解了基于栈的虚拟机，与此对应的是基于寄存器的虚拟机。这类虚拟机的运行机制跟机器码的运行机制是差不多的，它的指令要显式地指出操作数的位置（寄存器或内存地址）。它的优势是：可以更充分地利用寄存器来保存中间值，从而可以进行更多的优化。
例如，当存在公共子表达式时，这个表达式的计算结果可以保存在某个寄存器中，另一个用到该公共子表达式的指令，就可以直接访问这个寄存器，不用再计算了。在栈机里是做不到这样的优化的，所以基于寄存器的虚拟机，性能可以更高。而它的典型代表，是Google公司为Android开发的Dalvik虚拟机和Lua语言的虚拟机。
这里你需要注意，栈机并不是不用寄存器，实际上，操作数栈是可以基于寄存器实现的，寄存器放不下的再溢出到内存里。只不过栈机的每条指令，只能操作栈顶部的几个操作数，所以也就没有办法访问其它寄存器，实现更多的优化。
现在，你应该对虚拟机以及字节码有了一定的了解了。那么，如何借助工具生成字节码呢？你可能会问了：为什么不纯手工生成字节码呢？当然可以，只不过借助工具会更快一些。</description></item><item><title>33_垃圾收集：能否不停下整个世界？</title><link>https://artisanbox.github.io/6/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/33/</guid><description>对于内存的管理，我们已经了解了栈和栈桢，在编译器和操作系统的配合下，栈里的内存可以实现自动管理。
不过，如果你熟悉C和C++，那么肯定熟悉在堆中申请内存，也知道要小心维护所申请的内存，否则很容易引起内存泄漏或奇怪的Bug。
其实，现代计算机语言大多数都带有自动内存管理功能，也就是垃圾收集（GC）。程序可以使用堆中的内存，但我们没必要手工去释放。垃圾收集器可以知道哪些内存是垃圾，然后归还给操作系统。
那么这里会有几个问题，也是本节课关注的重点：
自动内存管理有哪些不同的策略？这些策略各自有什么优缺点？ 为什么垃圾收集会造成系统停顿？工程师们又为什么特别在意这一点？ 相信学完这节课之后，你对垃圾收集的机制理解得会更加深刻，从而在使用Java、Go等带有垃圾收集功能的语言时，可以更好地提升回收效率，减少停顿，提高程序的运行效率。
当然，想要达到这个目的，你首先需要了解什么是内存垃圾，如何发现哪些内存是没用的？
什么是内存垃圾内存垃圾是一些保存在堆里的对象，但从程序里已经无法访问。
在堆中申请一块内存时（比如Java中的对象实例），我们会用一个变量指向这块内存。这个变量可能是：全局变量、常量、栈里的变量、寄存器里的变量。我们把这些变量叫做GC根节点。它指向的对象中，可能还包含指向其他对象的指针。
但是，如果给变量赋予一个新的地址，或者当栈桢弹出，该栈桢的变量全部失效，这时，变量所指向的内存就无用了（如图中的灰色块）。
另外，如果A对象有一个成员变量指向C对象，那么如果A不可达，C也会不可达，也就失效了。但D对象除了被A引用，还被B引用，仍然是可达的。
所以，所有可达的内存就不是垃圾，而计算可达性，重点在于知道哪些是根节点。在一个活动记录（栈桢）里，有些位置放的是指向堆中内存的指针，有的位置不是，比如，可能存放的是返回地址，或者是一个整数值。如果我们能够知道活动记录的布局，就可以找出所有的指针，然后就能计算寻找垃圾内存。
现在，你应该知道了内存垃圾的特点了，接下来，只要用算法找出哪些内存是不可达的，就能进行垃圾收集了。
标记和清除（Mark and Sweep）标记和清除算法是最为经典的垃圾收集算法，它分为标记阶段和清除阶段。
在标记阶段中，GC跟踪所有可达的对象并做标记。每个对象上有一个标记位，一开始置为0，如果发现这个对象是可达的，就置为1。这个过程其实就是图的遍历算法，我们把这个算法细化一下，写成伪代码如下：
把所有的根节点加入todo列表 只要todo列表不为空，就循环处理： 从todo列表里移走一个变量v 如果v的标记为0，那么 把v的标记置为1 假设v1...vn是v中包含的指针 那么把v1...vn加入todo列表(去除重复成员) 下面的示例图中，x和y是GC根节点，标记完毕以后，A、C和D是可达的，B、E和F是可收集的（我用不同的颜色做了标注）。
在清除阶段中，GC遍历所有从堆里申请的对象，把标记为0的对象收回，把标记为1的内存重新置为0，等待下次垃圾收集再做标记。
这个算法虽然看上去简单清晰，但存在一个潜在的问题。
在标记阶段，也就是遍历图的时候，必须要有一个列表作为辅助的数据结构，来保存所有待检查的对象。但这个列表要多大，只有运行时才清楚，所以没有办法提前预留出一块内存，用于清除算法。而一旦开始垃圾收集，那说明系统的内存已经比较紧张了，所以剩下的内存是否够这个辅助的数据结构用，是不确定的。
可能你会说：那我可以改成递归算法，递归地查找下级节点并做标记。这是不行的，因为每次递归调用都会增加一个栈桢，来保存递归调用的参数等信息，内存消耗有可能更大。
不过，方法总比问题多，针对算法的内存占用问题，你可以用指针逆转（pointer reversal）来解决。这个技术的思想是：把算法所需要的辅助数据，记录在内存对象自身的存储空间。具体做法是：顺着指针方向从A到达B时，我们把从A到B的指针逆转过来，改成从B到A。把B以及B的子节点标记完以后，再顺着这个指针找到回去的路，回到A，然后再把指针逆转回来。
整个标记过程的直观示意图如下：
关于这个技术，你需要注意其中一个技术细节：内存对象中，可能没有空间来存一个指针信息。比如下图中，B对象原来就有一个变量，用来保存指向C的指针。现在用这个变量的位置保存逆转指针，来指向A就行了。但到C的时候，发现C没有空间来存逆转到B的指针。
这时，借助寄存器就可以了。在设置从B到A的指针之前，要把B和C的地址，临时保存在寄存器里，避免地址的丢失。进入C以后，如果C没有存指针的空间，就证明C是个叶子节点，这时，用寄存器里保存的地址返回给B就行了。
采用标记和清除算法，你会记住所有收集了的内存（通常是存在一个双向列表里），在下次申请内存的时候，可以从中寻找大小合适的内存块。不过，这会导致一个问题：随着我们多次申请和释放内存，内存会变得碎片化。所以，在申请内存的时候，要寻找合适的内存块，算法会有点儿复杂。而且就算你努力去寻找，当申请稍微大一点儿的内存时，也会失败。
为了避免内存碎片，你可以采用变化后的算法，标记-整理算法：在做完标记以后，做一下内存的整理，让存活的对象都移动到一边，消除掉内存碎片。
除此之外，停止和拷贝算法，也能够避免内存碎片化。
停止和拷贝（Stop and Copy）采用这个算法后，内存被分成两块：
一块是旧空间，用于分配内存。 一块是新空间，用于垃圾收集。 停止和拷贝算法也可以叫做复制式收集（Coping Collection）。
你需要保持一个堆指针，指向自由空间开始的位置。申请内存时，把堆指针往右移动就行了，比标记-清除算法申请内存更简单。
这里需要注意，旧空间里有一些对象可能已经不可达了（图中的灰色块），但你不用管。当旧空间变满时，就把所有可达的对象，拷贝到新空间，并且把新旧空间互换。这时，新空间里所有对象整齐排列，没有内存碎片。
停止-拷贝算法被认为是最快的垃圾收集算法，有两点原因：
分配内存比较简单，只需要移动堆指针就可以了。 垃圾收集的代价也比较低，因为它只拷贝可达的对象。当垃圾对象所占比例较高的时候，这种算法的优势就更大。 不过，停止-拷贝算法还有缺陷：
有些语言不允许修改指针地址。 在拷贝内存之后，你需要修改所有指向这块内存的指针。像C、C++这样的语言，因为内存地址是对编程者可见的，所以没法采用停止和拷贝算法。
始终有一半内存是闲置的，所以内存利用率不高。 最后，它一次垃圾收集的工作量比较大，会导致系统停顿时间比较长，对于一些关键系统来说，这种较长时间的停顿是不可接受的。但这两个算法都是基础的算法，它们可以被组合进更复杂的算法中，比如分代和增量的算法中，就能避免这个问题。 引用计数（Reference Counting）引用计数支持增量的垃圾收集，可以避免较长时间的停顿。
它的原理是：在每个对象中，保存引用本对象的指针数量，每次做赋值操作时，都要修改这个引用计数。如果x和y分别指向A和B，当执行“x=y”这样的赋值语句时，要把A的引用计数减少，把B的引用计数增加。如果某个对象的引用计数变成了0，那就可以把它收集掉。
所以，引用计数算法非常容易实现，只需要在赋值时修改引用计数就可以了。
不过，引用计数方法也有缺陷：
首先，是不能收集循环引用的结构。比如图中的A、B、C和D的引用计数都是1，但它们只是互相引用，没有其他变量指向它们。而循环引用在面向对象编程里很常见，比如一棵树的结构中，父节点保存了子节点的引用，子节点也保存了父节点的引用，这会让整棵树都没有办法被收集。
如果你有C++工作经验，应该思考过，怎么自动管理内存。有一个思路是：实现智能指针，对指针的引用做计数。这种思路也有循环引用的问题，所以要用其他算法辅助，来解决这个问题。
其次，在每次赋值时，都要修改引用计数，开销大。何况修改引用计数涉及写内存的操作，而写内存是比较慢的，会导致性能的降低。
其实，这三个算法都是比较单一的算法，实际上，它们可以作为更复杂、更实用算法的组成部分，比如分代收集算法。</description></item><item><title>34_运行时优化：即时编译的原理和作用</title><link>https://artisanbox.github.io/6/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/34/</guid><description>前面所讲的编译过程，都存在一个明确的编译期，编译成可执行文件后，再执行，这种编译方式叫做提前编译（AOT）。 与之对应的，另一个编译方式是即时编译（JIT），也就是，在需要运行某段代码的时候，再去编译。其实，Java、JavaScript等语言，都是通过即时编译来提高性能的。
那么问题来了：
什么时候用AOT，什么时候用JIT呢？ 在讲运行期原理时，我提到程序编译后，会生成二进制的可执行文件，加载到内存以后，目标代码会放到代码区，然后开始执行。那么即时编译时，对应的过程是什么？目标代码会存放到哪里呢？ 在什么情况下，我们可以利用即时编译技术，获得运行时的优化效果，又该如何实现呢？ 本节课，我会带你掌握，即时编译技术的特点，和它的实现机理，并通过一个实际的案例，探讨如何充分利用即时编译技术，让系统获得更好的优化。这样一来，你对即时编译技术的理解会更透彻，也会更清楚怎样利用即时编译技术，来优化自己的软件。
首先，来了解一下，即时编译的特点和原理。
了解即时编译的特点及原理根据计算机程序运行的机制，我们把，不需要编译成机器码的执行方式，叫做解释执行。解释执行，通常都会基于虚拟机来实现，比如，基于栈的虚拟机，和基于寄存器的虚拟机（在32讲中，我带你了解过）。
与解释执行对应的，是编译成目标代码，直接在CPU上运行。而根据编译时机的不同，又分为AOT和JIT。那么，JIT的特点，和使用场景是什么呢？
一般来说，一个稍微大点儿的程序，静态编译一次，花费的时间很长，而这个等待时间是很煎熬的。如果采用JIT机制，你的程序就可以像，解释执行的程序一样，马上运行，得到反馈结果。
其次，JIT能保证代码的可移植性。在某些场景下，我们没法提前知道，程序运行的目标机器，所以，也就没有办法提前编译。Java语言，先编译成字节码，到了具体运行的平台上，再即时编译成，目标代码来运行，这种机制，使Java程序具有很好的可移植性。
再比如，很多程序会用到GPU的功能，加速图像的渲染，但针对不同的GPU，需要编译成不同的目标代码，这里也会用到即时编译功能。
最后，JIT是编译成机器码的，在这个过程中，可以进行深度的优化，因此程序的性能要比解释执行高很多。
这样看来，JIT非常有优势。
而从实际应用来看，原来一些解释执行的语言，后来也采用JIT技术，优化其运行机制，在保留即时运行、可移植的优点的同时，又提升了运行效率，JavaScript就是其中的典型代表。基于谷歌推出的V8开源项目，JavaScript的性能获得了极大的提升，使基于Web的前端应用的体验，越来越好，这其中就有JIT的功劳。
而且据我了解，R语言也加入了JIT功能，从而提升了性能；Python的数据计算模块numba也采用了JIT。
在了解JIT的特点，和使用场景之后，你有必要对JIT和AOT在技术上的不同之处有所了解，以便掌握JIT的技术原理。
静态编译的时候，首先要把程序，编译成二进制目标文件，再链接形成可执行文件，然后加载到内存中运行。JIT也需要编译成二进制的目标代码，但是目标代码的加载和链接过程，就不太一样了。
首先说说目标代码的加载。
在静态编译的情况下，应用程序会被操作系统加载，目标代码被放到了代码区。从安全角度出发，操作系统给每个内存区域，设置了不同的权限，代码区具备可执行权限，所以我们才能运行程序。
在JIT的情况下，我们需要为这种动态生成的目标代码，申请内存，并给内存设置可执行权限。我写个实际的C语言程序，让你直观地理解一下这个过程。
我们在一个数组里，存一段小小的机器码，只有9个字节。这段代码的功能，相当于一个C语言函数的功能，也就是把输入的参数加上2，并返回。
/* * 机器码，对应下面函数的功能： * int foo(int a){ * return a + 2; * } */ uint8_t machine_code[] = { 0x55, 0x48, 0x89, 0xe5, 0x8d, 0x47, 0x02, 0x5d, 0xc3 }; 你可能问了：你怎么知道这个函数，对应的机器码是这9个字节呢？
这不难，你把foo.c编译成目标文件，然后查看里面的机器码就行了。
clang -c -O2 foo.c -o foo.o 或者 gcc -c -O2 foo.c -o foo.o objdump -d foo.</description></item><item><title>35_案例总结与热点问题答疑：后端部分真的比前端部分难吗？</title><link>https://artisanbox.github.io/6/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/35/</guid><description>本节课，我会继续剖析一些，你们提出的，有代表性的问题（以后端问题为主），主要包括以下几个方面：
后端技术部分真的比前端技术部分难吗？ 怎样更好地理解栈和栈桢（有几个同学提出的问题很好，有必要在这里探究一下）？这样，你对栈桢的理解会更加扎实。 有关数据流分析框架。数据流分析是后端技术的几个重点之一，需要再细化一下。 关于Java的两个知识点：泛型和反射。我会从编译技术的角度讲一讲。 接下来，进入第一个问题：后端技术真的难吗？正确的学习路径是什么？
后端技术真的难吗？该怎么学？有同学觉得，一进到后端，难度马上加大了，你是不是也有这样的感觉？我承认，前端部分和后端部分确实不太相同。
前端部分偏纯逻辑，你只要把算法琢磨透就行了。而后端部分，开始用到计算机组成原理的知识，要考虑CPU、寄存器、内存和指令集，甚至还要深入到CPU内部，去看它的流水线结构，以便理解指令排序。当然，我们还要说清楚与操作系统的关系，操作系统是如何加载代码并运行的，如何帮你管理内存等等。另外，还涉及ABI和调用约定，NP完全的算法等等。看上去复杂了很多。
虽然比较复杂，但我认为，这并不意味着后端更难，只意味着知识点更多。可这些知识，往往你熟悉了就不难了。
比如，@风同学见到了汇编代码说：总算遇到了自己熟悉的内容了，不用天天看Java代码了。
我觉得，从算法的角度出发，后端部分的算法，至少没比前端的语法分析算法难。而且有些知识点，别的课程里应该讲过，如果你从以下三个方面多多积累，会更容易掌握后端内容：
计算机组成原理：CPU的运行原理、汇编指令等等。 数据结构和算法，特别是与树和图有关的算法：如果你之前了解过，与图有关的算法，了解旅行商问题，那么会发现，指令选择等算法似曾相识。自然会理解，我提到某些算法是NP完全的，是什么意思。 操作系统：大部分情况下，程序是在操作系统中运行的，所以，要搞清楚我们编译的程序是如何跟操作系统互动的。 @沉淀的梦想就对这些内容，发表过感触：感觉学编译原理，真的能够帮助我们贯通整个计算机科学，涉及到的东西好多。
确实如他所说，那么我也希望《编译原理之美》这门课，能促使你去学习另外几门基础课，把基础夯实。
后端技术的另一个特点，是它比较偏工程性，不像前端部分有很强的理论性，对于每个问题有清晰的答案。而后端技术部分，往往对同一个问题有多种解决思路和算法，不一定有统一的答案，甚至算法和术语的名称都不统一。
后端技术的工程性特点，还体现在它会涉及很多技术细节，这些细节信息往往在教科书上是找不到的，必须去查厂商（比如Intel）的手册，有时要到社区里问，有时要看论文，甚至有时候要看源代码。
总的来说，如何学好后端，我的建议主要有三个方面：
学习关联的基础课程，比如《数据结构与算法》，互相印证； 理解编译原理工程性的特点，接受术语、算法等信息的不一致，并从多渠道获得前沿信息，比如源代码、厂商的手册等等。 注重实操，亲自动手。比如，你在学优化算法时，即使没时间写算法，也要尽可能用LLVM的算法做做实验。 按照上面三条建议，你应该可以充分掌握后端技术了。当然，如果你只是想做一个概要的了解，那么阅读文稿也会有不错的收获，因为我已经把主线梳理出来了，能避免你摸不着头脑，不知如何入手。
接下来，我们进入第二个问题：再次审视一下栈桢。
再次认识栈桢@刘强同学问：操作系统在栈的管理中到底起不起作用？
这是操作系统方面的知识点，但可以跟编译技术中栈的管理联系在一起看。
我们应用程序能够访问很大的地址空间，但操作系统不会慷慨地，一下子分配很多真实的物理内存。操作系统会把内存分成很多页，一页一页地按需分配给应用程序。那么什么时候分配呢？
当应用访问自己内存空间中的一个地址，但实际上没有对应的物理内存时，就会导致CPU产生一个PageFault（在Intel手册中可以查到），这是一种异常（Exception）。
对异常的处理跟中断的处理很相似，会调用注册好的一个操作系统的例程，在内核态运行，来处理这个异常。这时候，操作系统就会实际分配物理内存。之后，回到用户态，继续执行你的程序，比如，一个push指令等等。整个过程对应用程序是透明的，其实背后有CPU和操作系统的参与。
@风提出了关于栈桢的第二个问题：看到汇编代码里管理栈桢的时候，用了rbp和rsp两个寄存器。是不是有点儿浪费？一个寄存器就够了啊。
确实是这样，用这种写法是习惯形成的，其实可以省略。而我在34讲里，用到的那个foo函数，根本没有使用栈，仅仅用寄存器就完成了工作。这时，可以把下面三行指令全部省掉：
pushq %rbp movq %rsp, %rbp popq %rbp 从而让产生的机器码少5个字节。最重要的是，还省掉两次内存读写操作（相比对寄存器的操作，对内存的操作是很费时间的）。
实际上，如果你用GCC编译的话，可以使用-fomit-frame-pointer参数来优化，会产生同样的效果，也就是不再使用rbp。在访问栈中的地址时，会采用4(%rsp)、8(%rsp)的方式，在rsp的基础上加某个值，来访问内存。
@沉淀的梦想提出了第三个问题：栈顶（也就是rsp的值）为什么要16字节对齐？
这其实是一个调用约定。是在GCC发展的过程中，形成的一个事实上的标准。不过，它也有一些好处，比如内存对齐后，某些指令读取数据的速度会更快，这会让你产生一个清晰的印象，每次用到栈桢，至少要占16个字节，也就是4个32位的整数的空间。那么，如果把一些尾递归转化为循环来执行，确实会降低系统的开销，包括内存开销和保存前一个桢的bsp、返回地址、寄存器的运行时间开销。
而@不的问了第四个问题： 为什么要设计成区分调用者、被调用者保护的寄存器，统一由被调用者或者调用者保护，有什么问题么？
这个问题是关于保护寄存器的，我没有仔细去研究它的根源。不过我想，这种策略是最经济的。
如果全部都是调用者保护，那么你调用的对象不会破坏你的寄存器的话，你也要保护起来，那就增加了成本；如果全部都是被调用者保护，也是一样的逻辑。如果调用者用了很少几个寄存器，被调用者却要保护很多，也不划算。
所以最优的方法，其实是比较中庸主义的，两边各负责保护一部分，不过，我觉得这可以用概率的方法做比较严谨的证明。
关于栈桢，我最后再补充一点。有的教材用活动记录这个术语，有的教材叫做栈桢。你要知道这两个概念的联系和区别。活动记录是比较抽象的概念，它可以表现为多种实际的实现方式。在我们的课程中，栈桢加上函数调用中所使用的寄存器，就相当于一个活动记录。
讲完栈桢之后，再来说说与数据流分析框架有关的问题。
细化数据流分析框架数据流分析本身，理解起来并不难，就算不引入半格这个数学工具，你也完全可以理解。
对于数据流分析方法，不同的文献也有不同的描述，有的说是3个要素，有的说是4个要素。而我在文稿里说的是5个要素：方向（D）、值（V）、转换函数（F）、相遇运算（meet operation, Λ）和初始值（I）。你只要把这几个问题弄清楚，就可以了。
引入半格理论，主要是进一步规范相遇运算，这也是近些年研究界的一个倾向。用数学做形式化地描述虽然简洁清晰，但会不小心提升学习门槛。如果你只是为了写算法，完全可以不理半格理论，但如果为了方便看这方面算法的论文，了解半格理论会更好。
首先，半格是一种偏序集。偏序集里，某些元素是可以比较大小的。但怎么比较大小呢？其实，有时是人为定的，比如，{a, b}和{a, b, c}的大小，就是人为定的。
那么，既然能比较大小，就有上界（Upper Bound）和下界（Lower Bound）的说法。给定偏序集P的一个子集A，如果A中的每个元素a，都小于等于一个值x（x属于P），那么x就是A的一个上界。反过来，你也知道什么是下界。
半格是偏序集中，一种特殊的类型，它要求偏序集中，每个非空有限的子集，要么有最小上界（并半格，join-semilattice），要么有最大下界（交半格，meet-semilattice）。
其实，如果你把一个偏序集排序的含义反过来，它就会从交半格转换成并半格，或者并半格转换成交半格。我们还定义了两个特殊值：Top、Bottom。在不同的文献里，Top和Bottom有时刚好是反着的，那是因为排序的方向是反着的。
因为交半格和并半格是可以相互转化的，所以有的研究者采用的框架，就只用交半格。交半格中，集合{x, y}的最大下界，就记做x Λ y。在做活跃性分析的时候，我们就规定{a, b} &amp;gt; {a, b, c}就行了，这样就是个交半格。如果按照这个规矩，我在28讲中举的那个常数传播的例子，应该把大小反过来，也做成个交半格。文稿中的写法，实际是个并半格，不过也不影响写算法。</description></item><item><title>36_当前技术的发展趋势以及其对编译技术的影响</title><link>https://artisanbox.github.io/6/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/36/</guid><description>在IT领域，技术一直在飞速的进步，而每次进步，都会带来新的业态和新的发展机遇。
退回到10年前，移动互联网刚兴起不久，谁也没想到它会催生现在这么多的业态。而云计算还在酝酿期，腾讯和百度的创始人都觉得它走不远，现在竟然这么普及。
退回到20年前，互联网刚兴起，上网都要拨号。互联网的几个巨头，像阿里巴巴、百度、腾讯、新浪，还有网易，都是在那个时代展露头角的。毫不夸张地说，如果你在那个时代搞技术，懂Web编程的话，那绝对是人人争抢的“香饽饽”，毕竟那时，Web编程是前沿技术，懂这个领域的人，凤毛麟角。
退回到30年前，微软等公司才刚开始展露头角，雷军、求伯君等老一代程序员也正在发力，WPS的第一个版本运行在DOS操作系统上。我还记得，95年的时候，我在大学的阶梯教室里，看了比尔盖茨曾发表的，关于未来技术方向的演讲。当时，他预测了未来的科技成果，比如移动智能设备，听上去像天方夜谭，但现在移动互联网、人工智能和5G的发展，早已超出了他当时的想象。
那么你有理由相信，未来10年、20年、30年，会发生同样天翻地覆的变化。这种变化所造成的的影响，你我哪怕大开“脑洞”都无法预料。而你在这种趋势下，所能做的就是，把握当下，并为未来的职业生涯做好准备。这是一件认真且严肃的事情，值得你用心对待。
当然，洞悉未来很难，但你可以根据当前了解到的信息，捕捉一些发展趋势，看看这些发展趋势，让编译技术的发展方向有了哪些不同，跟你又有什么关系。
本节课，我想与你分享3个方面的技术发展趋势，以及它们对编译技术的影响：
人工智能，以及如何让编程和编译技术变得更智能？ 云计算，以及是否需要云原生的语言？ 前端技术，以及能否出现统一各个平台的大前端技术？ 期望这些内容，能让你看到一些不同的思考视角，获得一些新的信息。
趋势1：让编程更智能人工智能是当前发展最迅速的技术之一了。这几年，它的发展速度超过了人们的预期。那么你知道，它对编译技术和计算机语言的影响是什么吗？
首先，它需要编译器能够支撑，机器学习对庞大计算力的需求，同时兼容越来越多新的硬件架构。
由于机器学习的过程需要大量的计算，仅仅采用CPU效率很低，所以GPU和TPU等新的硬件架构得到了迅速的发展。对于编译技术来说，首要的任务，是要充分发挥这些新硬件的能力；因为AI的算法要能跑在各种后端架构上，包括CPU、GPU和TPU，也包括仍然要采用SIMD等技术，所以后端技术就会变得比较复杂。同时，前端也有不同的技术框架，比如谷歌的TensorFlow、Facebooke的PyTorch等。那么编译器怎样更好地支持多种前端和多种后端呢？
根据在24讲学到的知识，你应该会想到要借助中间代码。所以，MLIR应运而生。这里要注意，ML是Multi-Level（多层次）的意思，而不是Machine Learning的缩写。我还想告诉你，MLIR的作者，也是LLVM的核心作者、Swift语言的发明人，Chris Lattner（他目前在谷歌TensorFlow项目中）。而当你看到MLIR的格式，也许会觉得跟LLVM的IR很像，那么你其实可以用更短的学习周期来掌握这个IR。
其次，AI还可能让现有的编译技术发生较大的改变。
实际上，把AI和编译技术更好地结合，是已经持续研究了20年左右的，一个研究方向。不过，没有很大的发展。因为之前，人工智能技术的进步不像这几年这么快。近几年，随着人工智能技术快速进步，在人脸识别、自动驾驶等各个领域产生了相当实用的成果，人们对人工智能可能给编译技术带来的改变，产生了更大的兴趣。这给了研究者们研究的动力，他们开始从各个角度探索变革的可能性。
比如说，在后端技术部分，很多算法都是NP完全的。这就是说，如果你用穷举的方法找出最优解，成本非常高。这个时候，就会用启发式（heuristic）的算法，也就是凭借直观或经验构造的算法，能够在可接受的花费下找出一个可行的解。那么采用人工智能技术，通过大数据的训练，有可能找出更好的启发式算法，供我们选择。这是人工智能和编译技术结合的一个方向。
Milepost GCC项目早在2009年就发布了，它是一款开源的，人工智能编译器。它能够通过自动学习来确定去优化哪些代码，以便让程序的性能更高。据IBM的测试数据，某些嵌入式软件的性能因此提升了18%。
另一个讨论度比较高的方向就是人工智能编程（或者叫自动编程）。从某种意义上看，从计算机诞生到现在，我们编写程序的方式一直没有太大的进步。最早，是通过在纸带或卡片上打孔，来写程序；后来产生了汇编语言和高级语言。但是，写程序的本质没有变化，我们只是在用更高级的方式打孔。
讽刺的是，在计算机语言的帮助下，很多领域都出现了非常好的工具，比如CAD完全改变了建筑设计行业。但计算机行业本身用的工具仍然是比较原始的，还是在一个编辑器中，用文本的方式输入代码。
而人工智能技术可能让我们习惯已久的编程模式发生改变。比如，现在的编译器只是检查错误并生成代码，带有AI功能的编译器呢，有可能不仅检查出比较明显的错误，甚至还会对你的编码方式提出建议。假设你用一个循环去做某个数组的计算，带有AI功能的编译器会告诉你，用函数式编程做向量计算性能更高，并提供一键式替换功能。
这里延伸一下，有可能，未来写程序的方式都会改变。微软收购GitHub以后，运用大量的代码作为训练数据，正在改进IDE，提供智能提示功能。而这还只是开始。目前，AI其实已经能帮你做UI的设计：你画一个草图，AI给你生成对应的Web页面。
而且在AI辅助设计领域，算法还能根据你的需要，帮你生成平面或三维的设计方案。我能想象，未来，你告诉AI给你生成一个电商APP，它就能给你生成出来。你再告诉它做什么样的修改，它都会立即修改。在未来，应用开发中，最令人头疼的需求变化的问题，在AI看来根本不是事。
那么，如果这个前景是真实的，对于你而言，需要掌握什么技能呢？
我建议你了解，编译技术和人工智能这两个领域的知识。那些计算机的基础知识会一直有用，你可以参与到编程范式迁移，这样一个伟大的进程中。现有程序开发中的那些简单枯燥，又不需要多少创造力的工作，也就是大家通常所说的“搬砖”工作，可能会被AI代替。而我猜测，未来的机会可能会留给两类人：
一类是具备更加深入的计算机基础技能，能应对未来挑战的，计算机技术人才，他们为新的计算基础设施的发展演化，贡献自己的力量。
另一类是应用领域的专家和人才。他们通过更富有创造力的工作，利用新的编程技术实现各种应用。编写应用程序的重点，可能不再是写代码，而是通过人工智能，训练出能够反映领域特点的模型。
当然，向自动编程转移的过程肯定是逐步实现的：AI先是能帮一些小忙，解放我们一部分工作量，比如辅助做界面设计、智能提示；接着是能够自动生成某些小的、常用的模块；最后是能够生成和管理复杂的系统。
总而言之，AI技术给编译技术，和编程模式带来了各种可能性，而你会见证这种转变。除此之外，云计算技术的普及和深化，也可能给编译技术和编程模式带来改变。
趋势2：云原生的开发语言云计算技术现在的普及度很广，所有应用的后端部分，缺省情况下都是跑在云平台上的，云就是应用的运行环境。
在课程里，我带你了解过程序的运行环境。那时，我们的关注点，还是在一个单机的环境上，包括CPU和内存这些硬件，以及操作系统这个软件，弄清楚程序跟它们互动的关系。比如，操作系统能够加载程序，能够帮程序管理内存，能够为程序提供一些系统功能（把数据写到磁盘上等等）。
然而，在云计算时代，云就是应用的运行环境。一个应用程序不是仅仅加载到一台物理机上，而是要能够根据需要，加载很多实例到很多机器上，实现横向扩展。当然了，云也给应用程序提供各种系统功能，比如云存储功能，它就像一台单独的服务器，会给程序提供读写磁盘的能力一样。
除此之外，在单机环境下，传统的应用程序，是通过函数或方法，来调用另一个模块的功能，函数调用的层次体现为栈里一个个栈桢的叠加，编译器要能够形成正确的栈桢，实现自动的内存管理。而在云环境下，软件模块以服务的形式存在，也就是说，一个模块通过RESTful接口或各种RPC协议，调用另外的模块的功能。程序还需要处理通讯失败的情况，甚至要在调用多个微服务时，保证分布式事务特性。而我们却没从编译技术的角度，帮助程序员减轻这个负担。
导致的结果是：现在后端的程序特别复杂。你要写很多代码，来处理RPC、消息、分布式事务、数据库分片等逻辑，还要跟各种库、框架、通讯协议等等打交道。更糟糕的是，这些技术性的逻辑跟应用逻辑，混杂在一起，让系统的复杂度迅速提高，开发成本迅速提升，维护难度也增加。很多试图采用微服务架构的项目因此受到挫折，甚至回到单一应用的架构。
所以，一个有意义的问题是：能否在语言设计的时候，就充分利用云的基础设施，实现云原生（Cloud Native）的应用？也就是说，我们的应用，能够透明地利用好云计算的能力，并能兼容各种不同厂商的云计算平台，就像传统应用程序，能够编译成，不同操作系统的可执行文件一样。
好消息是，云计算的服务商在不断地升级技术，希望能帮助应用程序，更好地利用云计算的能力。而无服务器（Serverless）架构就是最新的成果之一。采用无服务器架构，你的程序都不需要知道容器的存在，也不需要关心虚拟机和物理机器，你只需要写一个个的函数，来完成功能就可以了。至于这个函数所需要的计算能力、存储能力，想要多少就有多少。
但是，云计算厂商提供的服务和接口缺少标准化，当你把大量应用都部署到某个云平台的时候，可能会被厂商锁定。如果有一门面向云原生应用的编程语言，和相应的开发平台，能帮助人们简化云应用的开发，同时又具备跨不同云平台的能力，那就最理想了。
实际上，已经有几个创业项目在做这个方向做探索了，比如 Ballerina、Pulumi和Dark，你可以看一下。
当然了，云计算和编程结合起来，就是另一个有趣的话题：云编程。我会在下一讲，与你进一步讨论这个话题。
趋势3：大前端技术栈上面所讲的云计算，针对的是后端编程，而与此对应的，是前端编程工作。
后端工作的特点，是越来越云化，让工程师们头疼的问题，是处理分布式计算环境下，软件系统的复杂性。当然，前端的挑战也不少。
我们开发一款应用，通常需要支持Web、IOS和Android三种平台，有时候，甚至需要提供Windows和macOS的桌面客户端。不同的平台会需要不同的技术栈，从而导致一款应用的开发成本很高，这也是前端工程师们不太满意的地方。
所以，前端工程师们一直希望能用一套技术栈，搞定多个平台。比如，尝试用Web开发的技术栈完成Android、IOS和桌面应用的开发。React Native、Electron等框架是这个方面的有益探索；Flutter项目也做了一些更大胆的尝试。
Flutter采用Dart开发语言，可以在Android和IOS上生成高质量的原生界面，甚至还可以支持macOS、Windows和Linux上的原生界面。另外，它还能编译成Web应用。所以，本质上，你可以用同一套代码，给移动端、桌面端和Web端开发UI。
你可以把这种技术思路叫做大前端：同一套代码，支持多个平台。
从Flutter框架中，你可以看出编译技术起到的作用。首先，Dart语言也是基于虚拟机的，编译方式支持AOT和JIT，能够运行在移动端和桌面端，能够调用本地操作系统的功能。对于Web应用则编译成JavaScript、CSS和HTML。这个项目的创新力度已经超过了React Native这些项目，工程师们已经不满足于，在现有的语言（JavaScript）基础上编写框架，而是用一门新的语言去整合多个技术栈。
当然，提到前端技术，就不能不提Web Assembly（WASM）。WASM是一种二进制的字节码，也就是一种新的IR，能够在浏览器里运行。相比JavaScript，它有以下特点：
静态类型； 性能更高； 支持C/C++/Rust等各种语言生成WASM，LLVM也给了WASM很好的支持； 字节码尺寸比较少，减少了下载时间； 因为提前编译成字节码，因此相比JavaScript减少了代码解析的时间。 由于这些特点，WASM可以在浏览器里，更高效地运行，比如可以支持更复杂的游戏效果。我猜想，未来可能出现，基于浏览器的、性能堪比本地应用的字处理软件、电子表格软件。基于云的文档软件（比如Google Doc）会得到再一次升级，使用者也将获得更好的体验。</description></item><item><title>37_云编程：云计算会如何改变编程模式？</title><link>https://artisanbox.github.io/6/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/37/</guid><description>上一讲中，我分享了当前3个技术发展趋势，以及其对编译技术的影响。今天我们把其中的云计算和编程模式、编译技术的之间的关系、前景再展开探讨一下。
总的来说，现在编写程序是越来越云化了，所以，我们简单地称作云编程就好了。
关于云编程，有很多有趣的问题：
1.编程本身是否也能上云？在云上编程会跟本地开发有什么不同？
2.如何编写云应用，来充分发挥云平台的能力？分为哪些不同的模式？
3.为什么编写云应用那么复杂？如何降低这些复杂度？云原生应用的开发平台，能否解决这些问题？
本节课，我就带你深入讨论这些问题，希望借此帮助你对编程和云计算技术的关系做一个梳理，促使你更好地利用云计算技术。
首先，来看看如何实现云上编程。
实现云上编程90年代初，我在大学学习编程，宿舍几个人合买了一台386电脑。那个时候，我记得自己不太喜欢微软提供的MFC编程框架，这和386电脑没有浮点运算器，编译起来比较慢有关，编译一次使用MFC框架的，C++程序的时间，足够我看一页报纸的了。
喜欢编程的人，为了获得流畅的性能，电脑配置总是很高，虽然这足以满足C/C++时代的编程需要，但进入Java时代后，因为应用结构越来越复杂，工程师们有时需要在笔记本或桌面电脑上，安装各种复杂的中间件，甚至还要安装数据库软件，这时，电脑的配置即便再高，也很难安装和配置好这么复杂的环境。那么到了云计算时代，挑战就更大了，比如，你能想象在电脑上安装Hadoop等软件，来做大数据功能的开发吗？
其实，编写一个小的应用还好，但现在的应用越来越复杂，所需的服务端资源越来越多。以我最近参与的一个项目为例，这个项目是采用微服务架构的一个企业应用，要想实现可扩展的性能、更好的功能复用，就要用到数据库、消息队列、容器服务、RPC服务、分布式事务服务、API服务等等很多基础设施，在自己的电脑上配置所有这些环境，是不大可能的。
因此，工程师们已经习惯于，在云上搭建开发和测试环境，这样，可以随需获取各种云端资源。
因为编程跟云的关系越发紧密，有些开发工具已经跟云平台有了一定的整合，方便开发者按需获取云端资源。比如，微软的Visual Studio支持直接使用Azure云上的资源。
再进一步，IDE本身也可以云化，我们可以把它叫做“云IDE”。你的电脑只负责代码编辑的工作，代码本身放在云上，编译过程以及所需的类库也放在云上。Visual Studio Code就具备UI和服务端分离的能力。还有一些服务商提供基于浏览器的IDE，也是实现了前后端的分离。
我认为，未来的IDE可能会越来越云化，因为云IDE有很多优势，能给你带来很多好处。
1.易于管理的编程环境
编程环境完全配置在云上，不用在本地配置各种依赖项。
这一点，会给编程教育这个领域，提供很大的帮助。因为，学习编程的人能够根据需要，打开不同的编程环境，立即投入学习。反之，如果要先做很多复杂的配置才能开始学习，学习热情就会减退，一些人也就因此止步了。
其实，在软件开发团队中，你经常会看到这样一个现象：新加入项目组的成员，要花很长的时间，才能把开发环境搭建起来。因为他们需要安装各种软件，开通各种账号等等。那么，如果是基于云IDE开发的，这些麻烦都可以省掉。
2.支持跨平台编程
有些编程所需要的环境，在本地很难配置，在云中开发就很简单。比如，可以用Windows电脑为Linux平台开发程序，甚至你可以在云上，为你的无人机开发程序，并下载到无人机上。
在为手机编程时，比较复杂的一项工作是，适配各种不同型号的手机。这时，你只需要通过云IDE，整合同样基于云的移动应用测试环境，就可以在成百上千种型号的手机上测试你的应用了。
3.更强的计算能力
有些软件的编译非常消耗CPU，比如，完整编译LLVM可能需要一两个小时，而充分利用服务器的资源可以让编译速度更快。如果你从事AI方面的开发，体会会更深，AI需要大量的算力，并且GPU和TPU都很昂贵，我们很难自己去搭建这样的开发环境。而基于云开发，你可以按需使用云上的GPU、TPU和CPU的计算能力。
4.有利于开发过程的管理
开发活动集中到云上以后，会有利于各种管理工作。比如，很多软件项目是外包开发的，那么你可以想象，基于云编程的平台，甲乙双方的项目管理者，都可以获得更多关于开发过程的大数据，也更容易做好源代码的保护。
5.更好的团队协作
越来越多的人已经习惯在网上编写文档，平心而论，线上文档工具并没有本地的Office软件功能强大，是什么因素让我们更加偏爱线上文档工具呢？就是它的协作功能。团队中的成员可以同时编辑一个文档，还可以方便地将这个文档在团队中分享。
而我比较希望见到这样的场景，那就是，程序员们可以基于同一个代码文件，进行点评和交互式的修改，这相当于基于云的结对编程，对于加强团队的知识分享、提升软件质量都会有好处。
基于上述几点，我个人猜测：编程这项工作，会越来越与云紧密结合。这样一来，不仅仅能方便地调取云端的资源，越来越多的编程环境也会迁移到云上。
既然提到了在云上编程的方式，那么接下来，我们从编译技术的视角，来探讨一下，如何编写能充分运用云计算强大威力的应用，这样，你会对云计算有一个更加全面的认知。
如何编写云应用？学习编译原理，你可能会有一个感受，那就是编程可以在不同的抽象层次上进行。也就是说，你可以通过抽象，把底层复杂的技术细节转换成上层简单的语义。
程序员最早是直接编写机器码，指令和寄存器都要直接用0101来表示。后来，冯·诺依曼的一个学生，发明了用助记符的方法（也就是汇编语言）简化机器码的编写。用汇编语言编程的时候，你仍然要使用指令和寄存器，但可以通过名称来引用，比如34讲中，用pushq %rbp这样的汇编指令来表示机器码0x55。这就增加了一个抽象层次，用名称代替了指令和寄存器的编码。
而高级语言出现后，我们不再直接访问寄存器，而是使用变量、过程和作用域，抽象程度进一步增加。
总结起来，就是我们使用的语言抽象程度越来越高，每一次抽象对下一层的复杂性做了屏蔽，因此使用起来越来越友好。而编译技术，则帮你一层层地还原这个抽象过程，重新转换成复杂的底层实现。
云计算的发展过程跟编译技术也很类似。云计算服务商们希望通过层层的抽象，来屏蔽底层的复杂性，让云计算变得更易用。
而且，通常来说，在较低的抽象层次上，你可以有更大的掌控力，而在更高的抽象层次上，则会获得更好的方便性。
虚拟机是人们最早使用云资源的方式，一台物理服务器可以分割成多个虚拟机。在需要的时候，可以创建同一个虚拟机镜像的多个实例，形成集群。因为虚拟机包含了一套完整的操作系统，所以占据空间比较大，启动一个实例的速度比较慢。
我们一般是通过编写脚本来管理软件的部署，每种软件的安装部署方式都不相同，系统管理的负担比较重。
最近几年，容器技术变得流行起来。容器技术可以用更轻量级的方式，分配和管理计算资源。一台物理服务器可以运行几十、上百个容器，启动新容器的速度也比虚拟机快了很多。
跟虚拟机模式相比，容器部署和管理软件模块的方式标准化了，我们通过Kubernetes这样的软件，编写配置文件来管理容器。从编译原理的角度出发，这些配置文件就是容器管理的DSL，它用标准化的方式，取代了原来对软件配置进行管理的各种脚本。
无服务器（Serverless）架构，或者叫做FaaS（Function as a Service），做了进一步的抽象。你只要把一个个功能写成函数，就能被平台调用，来完成Web服务、消息队列处理等工作。这些函数可能是运行在容器中的，通过Kubernetes管理的，并且按照一定的架构来协调各种服务功能。
但这些技术细节都不需要你关心，你会因此丧失一些掌控力，比如，你不能自己去生成很多个线程做并行计算。不过，也因为需要你关心的技术细节变少了，编程效率会提高很多。
上面三个层次，每一级都比上一级的抽象层次更高。就像编译技术中，高级语言比汇编语言简单一样，使用无服务架构要比直接使用虚拟机和容器更简单、更方便。
但即使到了FaaS这个层次，编写一个云应用仍然不是一件简单的事情，你还是要面临很多复杂性，比如，处理应用程序与大容量数据库的关系，实现跨公有云和私有云的应用等等。那么能否再进一步抽象并简化云应用的开发？是否能通过针对云原生应用的编程平台，来实现这个目标呢？
为了探究这个问题，我们需要进一步审视一下，现在云编程仍然有哪些，需要被新的抽象层次消除掉的复杂性。
对云原生编程平台的需求：能否解决云应用的复杂性？在《人月神话》里，作者把复杂性分为两种：
一种叫做本质复杂性（Essential Complexity），指的是你要解决的问题本身的复杂性，是无法避免的。 一种叫做附属复杂性（Accidental Complexity），是指我们在解决本质问题时，所采用的解决方案而引入的复杂性。在我们现在的系统中，90%的工作量都是用来解决附属复杂性的。 我经常会被问到这样的问题：做一个电商系统，成本是多少？而我给出的回答是：可能几千块，也可能很多亿。
如果你理解我的答案，那意味着比较理解当前软件编程的复杂性问题。因为软件系统的复杂性会随着规模急剧上升。
像阿里那样的电商系统，需要成千上万位工程师来维护。它在双11的时候，一天的成交量要达到几千亿，接受几亿用户的访问，在性能、可靠性、安全性、数据一致性等维度，都面临巨大的挑战。最重要的是，复杂性不是线性叠加的，可能是相乘的。
比如，当一个软件服务1万个用户的时候，增加一个功能可能需要100人天的话；针对服务于1百万用户的系统，增加同样的功能，可能需要几千到上万人天。同样的，如果功能不变，只是用户规模增加，你同样要花费很多人天来修改系统。那么你可以看出，整体的复杂性是多个因素相乘的结果，而不是简单相加。
这跟云计算的初衷是相悖的。云计算最早承诺，当我们需要更多计算资源的时候，简单增加一下就行了。然而，现有软件的架构，其实离这个目标还很远。那有没有可能把这些复杂性解耦，使得复杂性的增长变成线性或多项式级别（这里是借助算法复杂性的理论）的呢？
我再带你细化地看一下附属复杂性的一些构成，以便加深你对造成复杂性的根源的理解。
1.基础设施的复杂性
编写一个简单的程序，你只需要写写业务逻辑、处理少量数据，采用很简单的架构就行了。但是编写大型应用，你必须关心软件运行的基础设施，比如，你是用虚拟机还是容器？你还要关心很多技术构成部分，比如Kubernetes、队列、负载均衡器、网络、防火墙、服务发现、系统监控、安全、数据库、分片、各种优化，等等。
这些基础设施产生的复杂性，要花费你很多时间。像无服务器架构这样的技术，已经能够帮你屏蔽部分的复杂性，但还不够，仍然有很多复杂性因素需要找到解决方案。举个例子。
大多数商业应用都要很小心地处理跟数据库的关系，因为一旦数据出错（比如电商平台上的商品价格出错），就意味着重大的商业损失。你要根据应用需求设计数据库结构；要根据容量设计数据库分片的方案；要根据数据分析的需求设计数据仓库方案，以及对应的ETL程序。</description></item><item><title>38_元编程：一边写程序，一边写语言</title><link>https://artisanbox.github.io/6/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/38/</guid><description>今天，我再带你讨论一个很有趣的话题：元编程。把这个话题放在这一篇的压轴位置，也暗示了这个话题的重要性。
我估计很多同学会觉得元编程（Meta Programming）很神秘。编程，你不陌生，但什么是元编程呢？
元编程是这样一种技术：你可以让计算机程序来操纵程序，也就是说，用程序修改或生成程序。另一种说法是，具有元编程能力的语言，能够把程序当做数据来处理，从而让程序产生程序。
而元编程也有传统编程所不具备的好处：比如，可以用更简单的编码来实现某个功能，以及可以按需产生、完成某个功能的代码，从而让系统更有灵活性。
某种意义上，元编程让程序员拥有了语言设计者的一些权力。是不是很酷？你甚至可以说，普通程序员自己写程序，文艺程序员让程序写程序。
那么本节课，我会带你通过实际的例子，详细地来理解什么是元编程，然后探讨带有元编程能力的语言的特性，以及与编译技术的关系。通过这样的讨论，我希望你能理解元编程的思维，并利用编译技术和元编程的思维，提升自己的编程水平。
从Lisp语言了解元编程说起元编程，追溯源头，应该追到Lisp语言。这门语言其实没有复杂的语法结构，仅有的语法结构就是一个个函数嵌套的调用，就像下面的表达式，其中“+”和“*”也是函数，并不是其他语言中的操作符：
(+ 2 (* 3 5)) //对2和3求和，这里+是一个函数，并不是操作符 你会发现，如果解析Lisp语言形成AST，是特别简单的事情，基本上括号嵌套的结构，就是AST的树状结构（其实，你让Antlr打印输出AST的时候，它缺省就是按照Lisp的格式输出的，括号嵌套括号）。这也是Lisp容易支持元编程的根本原因，你实际上可以通过程序来生成，或修改AST。
我采用了Common Lisp的一个实现，叫做SBCL。在macOS下，你可以用“brew install sbcl”来安装它；而在Windows平台，你需要到sbcl.org去下载安装。在命令行输入sbcl，就可以进入它的REPL，你可以试着输入刚才的代码运行一下。
在Lisp中，你可以把(+ 2 (* 3 5))看做一段代码，也可以看做是一个列表数据。所以，你可以生成这样一组数据，然后作为代码执行。这就是Lisp的宏功能。
我们通过一个例子来看一下，宏跟普通的函数有什么不同。下面两段代码分别是用Java和Common Lisp写的，都是求一组数据的最大值。
Java版本：
public static int max(int[] numbers) { int rtn = numbers[0]; for (int i = 1;i &amp;lt; numbers.length; i++){ if (numbers[i] &amp;gt; rtn) rtn = numbers[i]; } return rtn; } Common Lisp版本：
(defun mymax1 (list) (let ((rtn (first list))) ;让rtn等于list的第一个元素 (do ((i 1 (1+ i))) ;做一个循环，让i从1开始，每次加1 ((&amp;gt;= i (length list)) rtn) ;循环终止条件：i&amp;gt;=list的长度 (when (&amp;gt; (nth i list) rtn) ;如果list的第i个元素 &amp;gt; rtn (setf rtn (nth i list)))))) ;让rtn等于list的第i个元素 那么，如果写一个函数，去求一组数据的最小值，你该怎么做呢？采用普通的编程方法，你会重写一个函数，里面大部分代码都跟求最大值的代码一样，只是把其中的一个“&amp;gt;”改为"</description></item><item><title>加餐_汇编代码编程与栈帧管理</title><link>https://artisanbox.github.io/6/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/40/</guid><description>在22讲中，我们侧重讲解了汇编语言的基础知识，包括构成元素、汇编指令和汇编语言中常用的寄存器。学习完基础知识之后，你要做的就是多加练习，和汇编语言“混熟”。小窍门是查看编译器所生成的汇编代码，跟着学习体会。
不过，可能你是初次使用汇编语言，对很多知识点还会存在疑问，比如：
在汇编语言里调用函数（过程）时，传参和返回值是怎么实现的呢？ 21讲中运行期机制所讲的栈帧，如何通过汇编语言实现？ 条件语句和循环语句如何实现？ …… 为此，我策划了一期加餐，针对性地讲解这样几个实际场景，希望帮你加深对汇编语言的理解。
示例1：过程调用和栈帧这个例子涉及了一个过程调用（相当于C语言的函数调用）。过程调用是汇编程序中的基础结构，它涉及到栈帧的管理、参数的传递这两个很重要的知识点。
假设我们要写一个汇编程序，实现下面C语言的功能：
/*function-call1.c */ #include &amp;lt;stdio.h&amp;gt; int fun1(int a, int b){ int c = 10; return a+b+c; } int main(int argc, char *argv[]){ printf(&amp;quot;fun1: %d\n&amp;quot;, fun1(1,2)); return 0; } fun1函数接受两个整型的参数：a和b，来看看这两个参数是怎样被传递过去的，手写的汇编代码如下：
# function-call1-craft.s 函数调用和参数传递 # 文本段,纯代码 .section __TEXT,__text,regular,pure_instructions
_fun1: # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来
movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部
subq $4, %rsp # 扩展栈 movl $10, -4(%rbp) # 变量c赋值为10，也可以写成 movl $10, (%rsp) # 做加法 movl %edi, %eax # 第一个参数放进%eax addl %esi, %eax # 把第二个参数加到%eax,%eax同时也是存放返回值的寄存器 addl -4(%rbp), %eax # 加上c的值 addq $4, %rsp # 缩小栈 # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 .</description></item><item><title>开篇词_为什么你要学习编译原理？</title><link>https://artisanbox.github.io/6/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/43/</guid><description>你好，我是宫文学，一名技术创业者。我曾经参与过几个公司的创业过程，在开源技术社区也做过一些工作，现在是北京物演科技CEO。
我喜欢做平台性的软件，而编译技术就是产品取得优势的关键。我是国内最早一拨做BPM的，也就是流程管理平台，也是最早一拨做BI平台的，现在流行叫大数据。当时我们只有3个人，用编译技术做了一些硬核的产品原型，跟联想集团签订了战略级合作协议。之后我又做过电子表单和快速开发平台，而它们的核心就是编译技术。
我参与的第一个公司卖给了上市公司，第二个在新三板上市，这些成果在一定程度上受益于编译技术。而我呢，对编译技术一直很重视，也一直保持着兴趣。所以很高兴能在“极客时间”上分享与编译技术有关的原理和经验，希望我的分享能帮助你在编译技术这个领域获得实实在在的进步。
众所周知，编译技术是计算机科学皇冠上的明珠之一。历史上各门计算机语言的发明人，总是被当作英雄膜拜。比尔·盖茨早期最主要的成就，就是写了一个Basic的解释器。当年Brendan Eich设计的JavaScript，虽然语言略微有点儿糙，但却顽强地生存到了现在。
很多国外厂商的软件，普遍都具备二次编程能力，比如Office、CAD、GIS、Mathematica等等。德国SAP公司的企业应用软件也是用自己的业务级语言编写的。目前来看，谷歌也好，苹果也好，微软也好，这些技术巨头们的核心能力，都是拥有自己的语言和生态。可见编译技术有多么重要！
编译技术，与你的工作息息相关但也有一些程序员认为：“我不可能自己去写一门新的语言，还有必要学习编译原理吗？”
这种想法是把编译原理的用途简单化了。编译原理不是只能用于炫耀的屠龙技。 别的不说，作为程序员，在实际工作中你经常会碰到需要编译技术的场景。
Java程序员想必很熟悉Hibernate和Spring，前者用到了编译技术做HQL的解析，后者对注解的支持和字节码动态生成也属于编译技术。所以，如果你要深入理解和用好这类工具，甚至想写这种类型的工具，会需要编译技术。
而PHP程序员在写程序的时候，一般会用到模板引擎实现界面设计与代码的分离。模板引擎对模板进行编译，形成可执行的PHP代码。模板引擎可以很强大，支持条件分支、循环等语法。如果你了解编译技术，会更容易掌握这些模板引擎，甚至写出更符合领域需求的模板引擎。
我们2001年开发了一款工作流软件，里面有依据自定义公式判断流转方向的功能。像这类需要用户自定义功能的软件，比如报表软件、工资管理软件等，都需要编译技术。
如果你要参与编写一个基础设施类的软件，比如数据库软件、ETL软件、大数据平台等，很多需要采用编译技术提供软件自带的语言功能，比如SQL。这种功能无法由外部通用语言实现。
除此之外，解析用户输入，防止代码注入，为前端工程师提供像React那样的DSL，像TypeScript那样把一门语言翻译成另一门语言，像CMake和Maven那样通过配置文件来灵活工作，以及运维工程师分析日志文件等等高级别的需求，都要用到编译技术。
除了丰富的应用场景，学习编译技术对于提升程序员的竞争力也很重要。现在一些大公司在招聘程序员时，有难度的面试题都是涉及底层机制的。因为理解了底层机制，才能有更深入思考问题，以及深层次解决问题的能力，而不是只能盲目地搜索答案，从表面解决问题。而学习编译原理能让你从前端的语法维度、代码优化的维度、与硬件结合的维度几个方面，加深对计算机技术的理解，提升自己的竞争力。
所以，无论你是前端工程师、后端工程师，还是运维工程师，不论你是初级工程师还是职场老手，编译技术都能给你帮助，甚至让你提升一个级别。
编译技术并不难学但问题来了，你可能会说：“我知道编译技术很重要，我也很想把它啃下，可是我每次鼓起勇气拿起《编译原理》，啃不了多少页就放下了。编译原理已经成了我的心魔……”
在我看来，你之所以遇到困难，很大一个原因在于市面上讲述编译原理的内容往往过于抽象和理论化。学习，说到底是一个学和练，以及学以致用的过程。所以在和朋友们沟通了解之后，我想用下面的思路组织课程内容，帮你克服畏难情绪，更好地理解和学习编译原理。
我会通过具体的案例带你理解抽象的原理。比如语义分析阶段有个I属性和S属性，传统课本里只专注I属性和S属性的特点和计算过程，很抽象。那么我会分析常用语言做语义分析时，哪些属性是I属性，哪些是S属性，以及如何进一步运用这些属性，来让你更直观地了解它们。
我也会重视过程，带你一步步趟过雷区。我写了示例程序，带你逐渐迭代出一门脚本语言和一门编译型语言。当然了，我们会遇到一些挑战和问题，而在解决问题的过程中，你会切切实实体会到某个技术在哪个环节会发挥什么作用。最重要的是，你会因此逐渐战胜畏难情绪，不再担心看不懂、学不会。
我还会让你在工作中真正运用到编译技术。课程里的代码，可以给你的工作提供参考。我介绍的Antlr和LLVM工具，前者能帮你做编译器前端的工作，后者能帮你完成编译器后端的工作。在课程中，你能真正运用编译技术解决报表设计等实际问题。
为了帮你迅速了解课程的知识结构体系，我画了一张思维导图。课程从三方面展开，包括实现一门脚本语言、实现一门编译型语言和面向未来的编程语言。
课程的第一部分主要聚焦编译器前端技术，也就是通常说的词法分析、语法分析和语义分析。我会带你了解它们的原理，实现一门脚本语言。我也会教你用工具提升编译工作的效率，还会在几个应用场景中检验我们的学习成果。
第二部分主要聚焦编译器后端技术，也就是如何生成目标代码和对代码进行优化的过程。我会带你纯手工生成汇编代码，然后引入中间代码和后端工具LLVM，最后生成可执行的文件能支持即时编译，并经过了多层优化。
第三部分是对编译技术发展趋势的一些分析。这些分析会帮助你更好地把握未来技术发展的脉搏。比如人工智能与编译技术结合是否会出现人工智能编程？云计算与编译技术结合是否会催生云编程的新模式？等等。
写在后面课程虽然只有30多节，但每节课绝对是干货满满。我希望这个课程能让所有有志于提升自己技术的工程师，顺利攻下编译技术这重要的一关，能够在工作中应用它见到实效，并且对编程理解更上一层。
最后，我希望你在留言区立下Flag，写下自己的计划，在“极客时间”与志同道合的朋友互相监督，一起学习，一起进步！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>期中考试_来赴一场100分的约定吧！</title><link>https://artisanbox.github.io/6/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/44/</guid><description>你好，我是宫文学。
时间过得真快，从8月14日课程上线，到现在已经有一个半月的时间了。这一个半月里，有很多同学反馈说，自己学了这个课程特别有收获，而运行同学们写的编译器，我也觉得很有成就感，当然了，有的时候，我也会比较焦虑，因为一遍遍改文稿和写示例程序都需要投入大量的精力，不夸张地说，有几次晚上做梦的内容，都与咱们的课程有关……
但是，我觉得把编译原理中，看似高不可攀的一个个知识点，变成一篇篇得到你们肯定的文章是一件十分有趣的事情。动手写示例程序时，也往往让我废寝忘食，比如，报表系统的示例程序就是在飞机和火车上写出来的，一边写，一边灵感不断涌现，那时，我先写了一个版本，后来又改成了基于向量计算的版本，因为总是想给你们呈现最优质的内容，所以一直在不断地思考，优化。
在准备算法篇的示例程序时，我也有了很多新的灵感，比如对于元编程的理念，我又有了一些创新的想法。这些内容，我会在课程的第三部分与你分享。
在互联网时代，廉价的快乐随处可得，而努力拼搏才能获得的乐趣，从来都只属于少数人。这门课的目标是让尽可能多的人，有机会享受这种乐趣，当我看到你们进入编译技术的美丽花园中徜徉流连，我的内心是十分欣慰的。
我相信你们是真心喜欢计算机技术，所以想要努力搞懂这个学科的基础原理。而且，你们还能够静下心来，真正坐下来动手尝试。
有的同学会跟“这个推导过程我看过去怎么不会无限递归啊？”这样的问题较劲，而我是很感动的，因为他知道不把手弄脏（get hands dirty），是学不会手艺的。
在别人觉得没有问题的地方提问，本身就需要一定的勇气。其实，那个问题不像表面上那么简单。我在19讲里花了很大的篇幅解答了这个问题。而从这个问题，可以引出很多问题，比如，有多个产生式的时候，到底该如何选择？深度优先和广度优先有什么区别？等等。
你可以把在学习过程中发现的这些问题看做是花园的入口，而不是障碍。对于在学习编译原理时遇到了困难的同学，我要说，你至少找到了一个入口。
从这个角度来说，通过这次期中考试的20道题目，你又获得了20个新的入口。我亲自出的这20道题目，可以让你对之前学过的内容查漏补缺。你有一周的时间去回顾内容，弥补不足。在你答题的过程中，分值其实是不重要的，能引起你的思考最为重要，这可能是你又一轮的认知迭代！
接下来，我们聊一个轻松的话题，国庆将至，如果你想趁机好好休息一下，不妨找一个小众的城市，远离人群。而且还可以尝试带着电脑，在古镇上，在流水边，在星空下，在一切美景的围绕下，安安静静地写个编译器。这种尝试，难道不是一件美事吗？
当然了，也欢迎你在留言区，将你去过的城市美景分享给大家，给我们的课程增添不一样的色彩。
最后，来挑战一下，开启你的期中考试之旅吧！
编辑角：答题不限次数，但分值以第一次为准，答题前三名，有惊喜哦。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>用户故事_因为热爱，所以坚持</title><link>https://artisanbox.github.io/6/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/45/</guid><description>你好，我是宫文学。
很高兴能够看到你分享自己的学习故事。
通过你的留言和故事分享，我能深刻地感受到你对编译原理的热爱，感谢你能与我一起，坚持学习，努力进步，把编译原理这门硬骨头一点一点、一步一步地消化掉，学有所用。
你好，我是雲至，今年38岁，现在在电力公司做信息运维工作。
虽然我的工作与编译技术并不相关，学习编译原理似乎没有用武之地，但是，编译原理于我而言有着特殊的意义，它伴随了我整个大学时代，“啃”下它，攻破它，成了我多年后的目标。
大学时，我学的是信息与计算科学，那时，接触了很多计算机的数学原理，出于好奇心，我尝试去了解编译原理教材，却觉得像天书一样，整整看了50多遍就是看不懂。虽然不服输，但因为各种客观原因，只好放弃。
其实，我特别想知道计算机语言是怎么样变成能被计算机执行的语言的，步入中年后，我开始计划学习编程语言和计算机基础课，可在学习编程语言时发现如果不懂编译原理的话，自己的认识水平根本无法提高，而那时，我心里那股不服输的劲儿又燃了起来，所以当“极客时间”开设《编译原理之美》课程时，我马上就报了名。现在，学到18讲，我想把自己的感受分享给大家，可文笔不佳，还望大家不要见怪。
感受一：宫老师讲解思路特别清晰，课程设计比较巧妙。
在原理上，老师讲了很多书本上看不到的编程思想，比如清晰化，简单化和好维护。并用清晰的AST还原了程序代码从代码变成可执行的代码的过程。
在学习方法上，宫老师提供了一个比较高效的学习方法，先帮助我建立了对编译器前端技术的直观理解，在“01 | 理解代码：编译器的前端技术”里，让我真正理解了词法分析、语法分析和语义分析到底是什么意思。然后宫老师由浅入深，展开解析，帮助我理清了编译原理的知识体系。
感受二：原理和实践并行， 让我通过动手提升认知。
在学习完词法分析讲之后，我认识到有限状态机的编程思路可以大大简化编码的难度，老师通过计算器的例子，特别清楚地讲明白了这个方法。
而 “08 | 作用域和生存期：实现块作用域和函数”则解决了我很多年的困惑，让我明白了变量的作用域的概念具体是怎么一回事。与此同时，课后老师及时提供了示例代码的链接，我通过动手演练，明白了一些没有搞懂的内容，比如函数、作用域等等。
随着课程不断深入，我的困惑也多了起来，当我在留言区提出自己的疑问时，宫老师总是能不厌其烦地讲解，十分认真负责！十分感谢宫老师带来这个课程，我也会继续努力学习的。
你好，我是沁园，是一名软件工程专业的研二学生。
研一时，我曾学过编译原理的课，但课上老师只讲了一些理论，没有结合实例，学的不明所以。而我自己一直对编译原理非常好奇，好奇编程语言底层到底是怎么实现的，也一直想要探知，本想啃下“龙书”和“虎书”，却因其厚重、难懂而搁置了。
后来，宫老师在极客时间上开设了《编译原理之美》的课程，三个月讲完编译的前端与后端技术，我毫不犹豫地入了坑，并从第一讲一直坚持，学到了现在。在这个过程中，我有一些学习的心得，所以想借此分享给大家，也向宫老师表达感谢之情。
心得一：在我看来，这门课不能只利用碎片的时间，而是需要课下动手和思考的。
因为编译原理本身就比较有难度，外加篇幅所限，只看文本的话，还是会产生困惑。我记得自己在“08 | 作用域和生存期：实现块作用域和函数”时，走入了死胡同，后面的连续几讲都看不明白，几乎快要放弃。
不过，宫老师贴心地在GitHub上提供了全部的源码，而且用到了我比较擅长的Java语言。我相信Java语言的程序利用IDEA的调试器就没有什么看不明白的，于是利用一个周末，把老师提供的示例脚本全部放到解析器中跑，并把解释器用IDEA的调试功能单步执行一遍，观察解释器都是怎么处理类、对象、函数以及闭包的。
调试的过程中，我边调试，边思考，边在笔记上总结，最后才恍然大悟，真正明白了老师讲的内容。真正搞懂之后，一直以来，编译器在我心中的神秘色彩也就消失了，编译中的类型推导，引用消解等高大上的概念也不过是由判断，循环等简单逻辑组成，只不过需要考虑的东西相对多些，如果几十年前让我来创立第一门编程语言，我肯定也这么搞。（目前只学了前端，学完后端以后可能观念还会有所改变）。
心得二：除此之外，这门课非常注重实战，先帮助我们建立直观认识，再去讲细节的算法。
一开始我还很奇怪，课程怎么这么“水”？编译原理不应该先把DFA、NFA、NFA转DFA、LL、LR这些经典算法作为开场吗？这个课程怎么用前三讲就把这些东西“水”过去了，然后开始讲Antlr以及语义分析了呢？
后来我发现，这些内容放在了“算法篇”中讲解。在将老师写的解释器源代码过了一遍之后，我越发地感觉到老师用心良苦。对于实现一门编程语言，实现语义才是最重要的，之前学编译的时候完全陷进了NFA、DFA、LL、LR等算法的细节中，完全没有意识到语义才是一门编程语言的灵魂。
宫老师一开始就教我们如何复用现有的成熟的Antlr规则，然后基于这些规则实现自己的语义，在学习算法篇之前，我就已经有了“如果哪天有需要，我可以徒手写一个编程语言解释器”的自信，之后虽然算法学起来也很吃力，但是不会因为陷进去而感到慌张，因为已经对解释器有了全局的把握。
一年前，我也像很多人一样，觉得编译原理是没有用的屠龙技，后来，我越发感觉编译原理在工作和学习中无处不在，比如Java程序员都会深入学习的JVM，不懂编译很多概念就只是听听，完全不理解。当你需要深入研究某个DB的时候，SQL解析优化器也绕不过去的一个坎，只有懂编译才能搞明白。所以，我庆幸自己能够接触到宫老师的《编译原理之美》，也感谢老师的良苦用心，我会一直坚持学下去，趁年轻，趁热爱，趁一起都还来得及。
编辑角：9月30日～10月6日是期中考试周，宫老师亲自出题，为你策划了20道期中测试题，帮你回顾前端技术要点内容，9月30日来挑战一下吧，不见不散！
编译器的后端技术开篇，也就是第20讲会在10月7日00:00更新。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>第二季回归_这次，我们一起实战解析真实世界的编译器</title><link>https://artisanbox.github.io/6/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/39/</guid><description>你好，我是宫文学，这次我带着一门全新的课程《编译原理实战课》回来了。
我在《编译原理之美》的开篇词中就说过，编译原理与你的工作息息相关，无论你是前端工程师、后端工程师，还是运维工程师，不论你是初级工程师还是职场老手，编译技术都能给你帮助，甚至让你提升一个级别。
在第一季，我带你一起梳理了编译技术最核心的概念、理论和算法，帮你构建出了一条相对平坦的学习曲线，让你能够理解大多数技术人都很畏惧的编译原理核心知识。在课程更新的过程中，我发现有很多同学都会有这样一个疑问，那就是：“我确实理解了编译技术的相关原理、概念、算法等，但是有没有更直接的方式，能让我更加深入地把知识与实践相结合呢？”
所以，在第二季，我会以实战的方式带你挑战编译原理这个领域，也就是带你一起解析真实世界中的编译器。在课程中，我会带你研究不同语言的编译器的源代码，一起跟踪它们的运行过程，分析编译过程的每一步是如何实现的，并会对有特点的编译技术点加以分析和点评。在这个过程中，你会获得对编译器的第一手的理解。
另外，我还会带你分析和总结前面已经研究过的编译器，让你对现代语言的编译器的结构、所采用的算法以及设计上的权衡，都获得比较真切的认识。
下面是专栏的目录：
为了感谢老同学， 我还准备了一个「专属福利」：
6月1日课程上线，我会送你一张15元专属优惠券，可与限时优惠同享，有效期48小时，建议尽早使用。
点击下方图片，立即免费试读新专栏。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>结束语_用程序语言，推动这个世界的演化</title><link>https://artisanbox.github.io/6/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/42/</guid><description>据说，第二次世界大战期间，图灵和同事破译的情报，在盟军诺曼底登陆等重大军事行动中发挥了重要作用。历史学家认为，他让二战提早了2年结束，至少拯救了2000万人的生命。也据说，苹果公司的Logo就是用来纪念图灵的。
图灵的故事我不再赘述，你上网随便搜个关键词都能找到。不过，通过这个故事，我们能得到两点启示：
对信息的处理能力至关重要，从此信息技术成为了科技进步的主角，一直到现在。 科技永远关乎人性，科技是客观的，而推动科技发展的人，是有温度、有故事的。 所以，在《编译原理之美》这个课程结束的今天，除了想跟你好好地说声再见之外，我更多地是想分享作为一个程序员，我们的挣扎、骄傲，以及跟这个社会的关系，跟时代洪流的关系。我有一些感受分享一下。
学习技术的过程，是跟大师对话的过程，是融入科技发展这条历史河流的过程，是一个有温度的心路历程。
有同学在留言区说，这门课，串联了计算机领域的很多基础课程。的确如他所说，当然，我也认为编译原理这门课，串联着整个计算机发展的历史，以及做出重要贡献的一代代大师。
什么是大师？这么说吧。比如你针对某方面的问题琢磨了很多年，有所心得。刚想进一步梳理头绪，就发现有人在多年前，已经针对这方面的问题发表了一个理论，并且论述得很完整，很严密。这个人，就可以叫做大师。
我的一个朋友，某上市公司的副总，原来是在大学教物理的，闲暇时间还会琢磨物理学的理论。有时候，他在琢磨一个点的时候，觉得很有心得，刚想整理出来，再一查文献，发现某个人已经在这个方向发表了成果。他形象地比喻说，刚想写《红楼梦》呢，发现一个叫曹雪芹的已经写了。
计算机领域也有很多大师。我们在学编译原理的时候，其实一直在跟各位大师邂逅。
比如，当讨论有限自动机的时候，你知道那是一个最简单的图灵机（Turling Machine）。你再去阅读这方面的资料，会发现图灵那时在思考什么是计算，这种根本性的问题。
当我们探讨到程序运行环境、汇编语言、机器语言的时候，你会感觉似乎跟冯·诺依曼（John Von Neumann）走近了。你会感受到第一代程序员，用机器码写程序是什么感觉。
第一代程序员的人数只有个位数，他们甚至当时都没有考虑到，还可以用别的方式写程序。所以，当冯·诺依曼的一个学生发明汇编的写法时，这位老师甚至觉得那不叫写程序。
而只有你自己动手写了汇编代码，你才能体会到，第二代程序员是怎样写程序的，其中包括比尔·盖茨（Bill Gates）。显然，比尔·盖茨认为普通程序员应该用更简单的语言，于是他写了一个Basic语言的解释器。其他熟练使用汇编语言的程序员，还包括为阿波罗登月计划，编写程序的传奇女程序员，玛格丽特·希菲尔德·汉密尔顿（Margaret Heafield Hamilton）。以及中国的雷军等等。题外话，我看过一个图表，早期程序员中，女性的比例很高，希望未来更多的女性回归这个行业。
接下来，你会遇到C语言的发明人丹尼斯·里奇（Dennis Ritchie）， 他的工作是基于肯.汤普森（Ken Thompson）的B语言。这俩人还是Unix操作系统的发明者。目前，肯.汤普森仍在Go语言项目组中工作。
我们使用的Java、JavaScript、Go语言等的语法风格，都是一路受到C语言的影响。我们做编译器的时候，要考虑调用约定、二进制接口，也能从这里找到源头。
在前端部分，我们讨论过面向对象的语义特征，和类型系统。而面向对象的编程思想，在60年代就被提出了，经由80年代的C++和90年代的Java才开始盛行。
我们同样简单实现过一等公民的函数和高阶函数，它们是函数式编程的特征。最近几年函数式编程的思想开始热起来，但它的起源更早，可以追溯到30年代阿隆佐·邱奇（Alonzo Church）提出的Lambda演算理论中。
邱奇用一种与图灵不同的方式，探讨了什么叫做计算，这个根本问题。他的思想于50年代体现在Lisp语言上。Lisp的发明人是人工智能的先驱约翰·麦卡锡（John McCarthy），这门语言成了计算机语言一些重要基因的来源，JavaScript、Ruby、Clojure、Scala、Julia等语言都从中汲取营养。我最近在研究云计算环境下的分布式数据库问题，发现可能还是要借鉴函数式编程的思想。
再有，编译原理中的属性语法和很多算法，不能不提高德纳（Donald Ervin Knuth）的贡献。他的著作应该成为你的必读。
当我们讨论Java的一些特征时，你可以试着体会Java语言之父詹姆斯·高斯林（James Gosling）当初设计字节码时在想什么。你还可以体会一下 布兰登·艾奇（Brendan Eich） 用很短的时间发明JavaScript时，是汲取了前人的哪些思想，以及是如何做出那些重要的决定的，这些决定使得JavaScript在元编程能力、函数式编程等方面，直到现在都焕发出勃勃生机。
当你学会编译原理的一个个知识点的时候，就是一步步走近大师们的过程。他们的名字不再是教科书上抽象的符号，你已经能够逐渐欣赏他们的思想，感受到他们的感受，和他们隔着时空交流。而当你凭着自己的经验，探索到了跟他们相同的方向上，你会更觉得有成就感，会觉得自己真正融入了科技演化的洪流中，算是开了窍了，算是其中的一份子了。
我想，真正在科技领域做出重大成绩的人，都会有这样一种，摸到了科技发展脉搏的感觉。据说，张小龙曾经说过，读懂了《失控》这本书的人，可以直接去他的团队上班。我猜，他对复杂系统科学情有独钟，产生了很多的心得。而任正非先生则对热力学中熵的理论感触很深，并把它深刻地融入到了华为的价值观和管理体系中。
除此之外，我们还要感谢Antlr工具的作者特恩斯·帕尔（Terence Parr）以及LLVM的核心作者 克里斯·拉特纳（Chris Lattner） 。通过阅读他们的文章和代码，以及其他研究者的论文，你会感受到这个领域最前沿的脉搏。
而通过编译原理中的一些应用课程，我们还可以更好地理解Spring等工具的设计者的思维。并且思考，是否自己也有能力驾驭这样的项目，从而成为技术进步洪流中的博浪者。
我相信，如果你不想学习编译原理，可以轻松找到一百个理由。比如：
这个课程太难，我恐怕学不会； 这个课程跟我现在的工作关系不大； 我没有时间； 连谁谁谁都没有学，我就不凑这个热闹了； … 但如果你想下定决心学会它的话，只要有一个理由就行了，那就是，你也可以成为技术进步洪流中的博浪者，而不是岸边的旁观者。这时，你的自我意识会觉醒：我来了，我要参与。在信息技术成为社会进步关键推动力的今天，这是作为一名程序员的傲骨。
更为重要的是，越来越多的中国程序员已经登上了舞台。越来越多高质量的开源项目，背后是一个个中国名字。我查阅自动化编程这个最前沿领域的文献时，发现文献上也不乏中国名字！
整个世界的目光也开始投向中国，因为他们越来越相信中国的创新能力。我们也确实有能力，因为我们已经有了云计算、人工智能和5G技术的积淀，我们正在芯片领域奋起直追，完全自主的操作系统已经开始萌芽。而在这些领域，编译技术都能大展身手。最重要的是，中国作为全球最大的市场之一，拥有最丰富的应用场景，也拥有越来越相信中国创新能力的消费者。
我相信，学习这门课的学员中，不管是大学生，还是已经很有工作经验的大侠，会有相当一批人，在下一个10年，使用编译技术做出一番成绩。
对我来说，我很高兴有机会专心致志地梳理编译原理相关的知识体系。而在梳理到每个知识点的时候，我都会迸发出很多灵感。这些灵感将会融入到我正在开发的一个软件和后续的工作中。
在这个过程中，我还有一个额外的收获，就是感觉自己的写作水平和普通话水平都提高了。原因很简单：因为每篇文稿都要改好几遍，录音有时也要录几遍。而把陡峭的学习曲线，变成一个让你缓缓爬坡的过程，也促使我必须竭尽全力！
我也觉得用仅仅40讲左右的课程，涵盖整个编译原理的知识体系，恐怕会显得不足。虽然涵盖了主要的知识点和脉络，但我在进入每个技术点的时候，发现要把这个点完全展开，可能都需要好几讲才行。不过没关系，我和极客时间还有进一步的计划，你可以等待好消息！
总的来说，信息技术的进步史，也是一代代大师的人文故事史。而编译技术让我们有机会走近这些大师，与他们对话，并加入他们。中国的程序员面临着历史的机遇，而抓住机遇的关键，是自我意识的觉醒，是敢于成为科技进步历史洪流中的博浪者的决心。
希望与你共勉，一起进步！
最后，我为你准备了一份结课问卷，题目不多，两三分钟就可以完成。希望你能畅所欲言，把自己真实的学习感受和意见表达出来，我一定会认真看，期待你的反馈。当然，如果你对专栏内容还有什么问题，也欢迎你在留言区继续提问，我会持续回复你的留言，我们江湖再见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>结课测试_编译原理的这些知识，你都掌握了吗？</title><link>https://artisanbox.github.io/6/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/41/</guid><description>你好，我是宫文学。
不知道你学完这些编译原理的相关知识以后，掌握得怎么样呢？
为了帮助你检测自己的学习成果，我特别准备了一套结课测试题。这套测试题共有3道单选题，17道多选题，满分100分。
题目中涉及到的知识点，我在这个系列课程中都讲过。
希望你能认真完成这次测试，如果你发现有些知识还没有掌握，可以回顾相应的内容，再去加深一下理解。也欢迎你在留言区与我互动交流。
好，点击下面的图片，开始测试吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item></channel></rss>