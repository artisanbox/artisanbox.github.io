<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gen 的学习笔记</title><link>https://artisanbox.github.io/</link><description>Recent content on Gen 的学习笔记</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 08 Mar 2022 18:37:53 +0800</lastBuildDate><atom:link href="https://artisanbox.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>01_为什么要学习数据结构和算法？</title><link>https://artisanbox.github.io/2/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/2/</guid><description>你是不是觉得数据结构和算法，跟操作系统、计算机网络一样，是脱离实际工作的知识？可能除了面试，这辈子也用不着？
尽管计算机相关专业的同学在大学都学过这门课程，甚至很多培训机构也会培训这方面的知识，但是据我了解，很多程序员对数据结构和算法依旧一窍不通。还有一些人也只听说过数组、链表、快排这些最最基本的数据结构和算法，稍微复杂一点的就完全没概念。
当然，也有很多人说，自己实际工作中根本用不到数据结构和算法。所以，就算不懂这块知识，只要Java API、开发框架用得熟练，照样可以把代码写得“飞”起来。事实真的是这样吗？
今天我们就来详细聊一聊，为什么要学习数据结构和算法。
想要通关大厂面试，千万别让数据结构和算法拖了后腿很多大公司，比如BAT、Google、Facebook，面试的时候都喜欢考算法、让人现场写代码。有些人虽然技术不错，但每次去面试都会“跪”在算法上，很是可惜。那你有没有想过，为什么这些大公司都喜欢考算法呢？
校招的时候，参加面试的学生通常没有实际项目经验，公司只能考察他们的基础知识是否牢固。社招就更不用说了，越是厉害的公司，越是注重考察数据结构与算法这类基础知识。相比短期能力，他们更看中你的长期潜力。
你可能要说了，我不懂数据结构与算法，照样找到了好工作啊。那我是不是就不用学数据结构和算法呢？当然不是，你别忘了，我们学任何知识都是为了“用”的，是为了解决实际工作问题的，学习数据结构和算法自然也不例外。
业务开发工程师，你真的愿意做一辈子CRUD boy吗？如果你是一名业务开发工程师，你可能要说，我整天就是做数据库CRUD（增删改查），哪里用得到数据结构和算法啊？
是的，对于大部分业务开发来说，我们平时可能更多的是利用已经封装好的现成的接口、类库来堆砌、翻译业务逻辑，很少需要自己实现数据结构和算法。但是，不需要自己实现，并不代表什么都不需要了解。
如果不知道这些类库背后的原理，不懂得时间、空间复杂度分析，你如何能用好、用对它们？存储某个业务数据的时候，你如何知道应该用ArrayList，还是Linked List呢？调用了某个函数之后，你又该如何评估代码的性能和资源的消耗呢？
作为业务开发，我们会用到各种框架、中间件和底层系统，比如Spring、RPC框架、消息中间件、Redis等等。在这些基础框架中，一般都揉和了很多基础数据结构和算法的设计思想。
比如，我们常用的Key-Value数据库Redis中，里面的有序集合是用什么数据结构来实现的呢？为什么要用跳表来实现呢？为什么不用二叉树呢？
如果你能弄明白这些底层原理，你就能更好地使用它们。即便出现问题，也很容易就能定位。因此，掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。
在平时的工作中，数据结构和算法的应用到处可见。我来举一个你非常熟悉的例子：如何实时地统计业务接口的99%响应时间？
你可能最先想到，每次查询时，从小到大排序所有的响应时间，如果总共有1200个数据，那第1188个数据就是99%的响应时间。很显然，每次用这个方法查询的话都要排序，效率是非常低的。但是，如果你知道“堆”这个数据结构，用两个堆可以非常高效地解决这个问题。
基础架构研发工程师，写出达到开源水平的框架才是你的目标！现在互联网上的技术文章、架构分享、开源项目满天飞，照猫画虎做一套基础框架并不难。我就拿RPC框架举例。
不同的公司、不同的人做出的RPC框架，架构设计思路都差不多，最后实现的功能也都差不多。但是有的人做出来的框架，Bug很多、性能一般、扩展性也不好，只能在自己公司仅有的几个项目里面用一下。而有的人做的框架可以开源到GitHub上给很多人用，甚至被Apache收录。为什么会有这么大的差距呢？
我觉得，高手之间的竞争其实就在细节。这些细节包括：你用的算法是不是够优化，数据存取的效率是不是够高，内存是不是够节省等等。这些累积起来，决定了一个框架是不是优秀。所以，如果你还不懂数据结构和算法，没听说过大O复杂度分析，不知道怎么分析代码的时间复杂度和空间复杂度，那肯定说不过去了，赶紧来补一补吧！
对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！何为编程能力强？是代码的可读性好、健壮？还是扩展性好？我觉得没法列，也列不全。但是，在我看来，性能好坏起码是其中一个非常重要的评判标准。但是，如果你连代码的时间复杂度、空间复杂度都不知道怎么分析，怎么写出高性能的代码呢？
你可能会说，我在小公司工作，用户量很少，需要处理的数据量也很少，开发中不需要考虑那么多性能的问题，完成功能就可以，用什么数据结构和算法，差别根本不大。但是你真的想“十年如一日”地做一样的工作吗？
经常有人说，程序员35岁之后很容易陷入瓶颈，被行业淘汰，我觉得原因其实就在此。有的人写代码的时候，从来都不考虑非功能性的需求，只是完成功能，凑合能用就好；做事情的时候，也从来没有长远规划，只把眼前事情做好就满足了。
我曾经面试过很多大龄候选人，简历能写十几页，经历的项目有几十个，但是细看下来，每个项目都是重复地堆砌业务逻辑而已，完全没有难度递进，看不出有能力提升。久而久之，十年的积累可能跟一年的积累没有任何区别。这样的人，怎么不会被行业淘汰呢？
如果你在一家成熟的公司，或者BAT这样的大公司，面对的是千万级甚至亿级的用户，开发的是TB、PB级别数据的处理系统。性能几乎是开发过程中时刻都要考虑的问题。一个简单的ArrayList、Linked List的选择问题，就可能会产生成千上万倍的性能差别。这个时候，数据结构和算法的意义就完全凸显出来了。
其实，我觉得，数据结构和算法这个东西，如果你不去学，可能真的这辈子都用不到，也感受不到它的好。但是一旦掌握，你就会常常被它的强大威力所折服。之前你可能需要费很大劲儿来优化的代码，需要花很多心思来设计的架构，用了数据结构和算法之后，很容易就可以解决了。
内容小结我们学习数据结构和算法，并不是为了死记硬背几个知识点。我们的目的是建立时间复杂度、空间复杂度意识，写出高质量的代码，能够设计基础架构，提升编程技能，训练逻辑思维，积攒人生经验，以此获得工作回报，实现你的价值，完善你的人生。
所以，不管你是业务开发工程师，还是基础架构工程师；不管你是初入职场的初级工程师，还是工作多年的资深架构师，又或者是想转人工智能、区块链这些热门领域的程序员，数据结构与算法作为计算机的基础知识、核心知识，都是必须要掌握的。
掌握了数据结构与算法，你看待问题的深度，解决问题的角度就会完全不一样。因为这样的你，就像是站在巨人的肩膀上，拿着生存利器行走世界。数据结构与算法，会为你的编程之路，甚至人生之路打开一扇通往新世界的大门。
课后思考你为什么要学习数据结构和算法呢？在过去的软件开发中，数据结构和算法在哪些地方帮到了你？
欢迎留言和我分享，我会第一时间给你反馈。如果你的朋友也在学习算法这个问题上犹豫不决，欢迎你把这篇文章分享给他！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>01_冯·诺依曼体系结构：计算机组成的金字塔</title><link>https://artisanbox.github.io/4/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/1/</guid><description>学习计算机组成原理，到底是在学些什么呢？这个事儿，一两句话还真说不清楚。不过没关系，我们先从“装电脑”这个看起来没有什么技术含量的事情说起，来弄清楚计算机到底是由什么组成的。
不知道你有没有自己搞过“装机”这回事儿。在2019年的今天，大部分人用的计算机，应该都已经是组装好的“品牌机”。如果我们把时钟拨回到上世纪八九十年代，不少早期的电脑爱好者，都是自己采购各种电脑配件，来装一台自己的计算机的。
计算机的基本硬件组成早年，要自己组装一台计算机，要先有三大件，CPU、内存和主板。
在这三大件中，我们首先要说的是CPU，它是计算机最重要的核心配件，全名你肯定知道，叫中央处理器（Central Processing Unit）。为什么说CPU是“最重要”的呢？因为计算机的所有“计算”都是由CPU来进行的。自然，CPU也是整台计算机中造价最昂贵的部分之一。
第二个重要的配件，就是内存（Memory）。你撰写的程序、打开的浏览器、运行的游戏，都要加载到内存里才能运行。程序读取的数据、计算得到的结果，也都要放在内存里。内存越大，能加载的东西自然也就越多。
存放在内存里的程序和数据，需要被CPU读取，CPU计算完之后，还要把数据写回到内存。然而CPU不能直接插到内存上，反之亦然。于是，就带来了最后一个大件——主板（Motherboard）。
主板是一个有着各种各样，有时候多达数十乃至上百个插槽的配件。我们的CPU要插在主板上，内存也要插在主板上。主板的芯片组（Chipset）和总线（Bus）解决了CPU和内存之间如何通信的问题。芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题。总线则是实际数据传输的高速公路。因此，总线速度（Bus Speed）决定了数据能传输得多快。
有了三大件，只要配上电源供电，计算机差不多就可以跑起来了。但是现在还缺少各类输入（Input）/输出（Output）设备，也就是我们常说的I/O设备。如果你用的是自己的个人电脑，那显示器肯定必不可少，只有有了显示器我们才能看到计算机输出的各种图像、文字，这也就是所谓的输出设备。
同样的，鼠标和键盘也都是必不可少的配件。这样我才能输入文本，写下这篇文章。它们也就是所谓的输入设备。
最后，你自己配的个人计算机，还要配上一个硬盘。这样各种数据才能持久地保存下来。绝大部分人都会给自己的机器装上一个机箱，配上风扇，解决灰尘和散热的问题。不过机箱和风扇，算不上是计算机的必备硬件，我们拿个纸板或者外面放个电风扇，也一样能用。
说了这么多，其实你应该有感觉了，显示器、鼠标、键盘和硬盘这些东西并不是一台计算机必须的部分。你想一想，我们其实只需要有I/O设备，能让我们从计算机里输入和输出信息，是不是就可以了？答案当然是肯定的。
你肯定去过网吧吧？不知道你注意到没有，很多网吧的计算机就没有硬盘，而是直接通过局域网，读写远程网络硬盘里面的数据。我们日常用的各类云服务器，只要让计算机能通过网络，SSH远程登陆访问就好了，因此也没必要配显示器、鼠标、键盘这些东西。这样不仅能够节约成本，还更方便维护。
还有一个很特殊的设备，就是显卡（Graphics Card）。现在，使用图形界面操作系统的计算机，无论是Windows、Mac OS还是Linux，显卡都是必不可少的。有人可能要说了，我装机的时候没有买显卡，计算机一样可以正常跑起来啊！那是因为，现在的主板都带了内置的显卡。如果你用计算机玩游戏，做图形渲染或者跑深度学习应用，你多半就需要买一张单独的显卡，插在主板上。显卡之所以特殊，是因为显卡里有除了CPU之外的另一个“处理器”，也就是GPU（Graphics Processing Unit，图形处理器），GPU一样可以做各种“计算”的工作。
鼠标、键盘以及硬盘，这些都是插在主板上的。作为外部I/O设备，它们是通过主板上的南桥（SouthBridge）芯片组，来控制和CPU之间的通信的。“南桥”芯片的名字很直观，一方面，它在主板上的位置，通常在主板的“南面”。另一方面，它的作用就是作为“桥”，来连接鼠标、键盘以及硬盘这些外部设备和CPU之间的通信。
有了南桥，自然对应着也有“北桥”。是的，以前的主板上通常也有“北桥”芯片，用来作为“桥”，连接CPU和内存、显卡之间的通信。不过，随着时间的变迁，现在的主板上的“北桥”芯片的工作，已经被移到了CPU的内部，所以你在主板上，已经看不到北桥芯片了。
冯·诺依曼体系结构刚才我们讲了一台计算机的硬件组成，这说的是我们平时用的个人电脑或者服务器。那我们平时最常用的智能手机的组成，也是这样吗？
我们手机里只有SD卡（Secure Digital Memory Card）这样类似硬盘功能的存储卡插槽，并没有内存插槽、CPU插槽这些东西。没错，因为手机尺寸的原因，手机制造商们选择把CPU、内存、网络通信，乃至摄像头芯片，都封装到一个芯片，然后再嵌入到手机主板上。这种方式叫SoC，也就是System on a Chip（系统芯片）。
这样看起来，个人电脑和智能手机的硬件组成方式不太一样。可是，我们写智能手机上的App，和写个人电脑的客户端应用似乎没有什么差别，都是通过“高级语言”这样的编程语言撰写、编译之后，一样是把代码和数据加载到内存里来执行。这是为什么呢？因为，无论是个人电脑、服务器、智能手机，还是Raspberry Pi这样的微型卡片机，都遵循着同一个“计算机”的抽象概念。这是怎么样一个“计算机”呢？这其实就是，计算机祖师爷之一冯·诺依曼（John von Neumann）提出的冯·诺依曼体系结构（Von Neumann architecture），也叫存储程序计算机。
什么是存储程序计算机呢？这里面其实暗含了两个概念，一个是“可编程”计算机，一个是“存储”计算机。
说到“可编程”，估计你会有点懵，你可以先想想，什么是“不可编程”。计算机是由各种门电路组合而成的，然后通过组装出一个固定的电路板，来完成一个特定的计算程序。一旦需要修改功能，就要重新组装电路。这样的话，计算机就是“不可编程”的，因为程序在计算机硬件层面是“写死”的。最常见的就是老式计算器，电路板设好了加减乘除，做不了任何计算逻辑固定之外的事情。
我们再来看“存储”计算机。这其实是说，程序本身是存储在计算机的内存里，可以通过加载不同的程序来解决不同的问题。有“存储程序计算机”，自然也有不能存储程序的计算机。典型的就是早年的“Plugboard”这样的插线板式的计算机。整个计算机就是一个巨大的插线板，通过在板子上不同的插头或者接口的位置插入线路，来实现不同的功能。这样的计算机自然是“可编程”的，但是编写好的程序不能存储下来供下一次加载使用，不得不每次要用到和当前不同的“程序”的时候，重新插板子，重新“编程”。
可以看到，无论是“不可编程”还是“不可存储”，都会让使用计算机的效率大大下降。而这个对于效率的追求，也就是“存储程序计算机”的由来。
于是我们的冯祖师爷，基于当时在秘密开发的EDVAC写了一篇报告First Draft of a Report on the EDVAC，描述了他心目中的一台计算机应该长什么样。这篇报告在历史上有个很特殊的简称，叫First Draft，翻译成中文，其实就是《第一份草案》。这样，现代计算机的发展就从祖师爷写的一份草案开始了。
First Draft里面说了一台计算机应该有哪些部分组成，我们一起来看看。
首先是一个包含算术逻辑单元（Arithmetic Logic Unit，ALU）和处理器寄存器（Processor Register）的处理器单元（Processing Unit），用来完成各种算术和逻辑运算。因为它能够完成各种数据的处理或者计算工作，因此也有人把这个叫作数据通路（Datapath）或者运算器。
然后是一个包含指令寄存器（Instruction Register）和程序计数器（Program Counter）的控制器单元（Control Unit/CU），用来控制程序的流程，通常就是不同条件下的分支和跳转。在现在的计算机里，上面的算术逻辑单元和这里的控制器单元，共同组成了我们说的CPU。
接着是用来存储数据（Data）和指令（Instruction）的内存。以及更大容量的外部存储，在过去，可能是磁带、磁鼓这样的设备，现在通常就是硬盘。
最后就是各种输入和输出设备，以及对应的输入和输出机制。我们现在无论是使用什么样的计算机，其实都是和输入输出设备在打交道。个人电脑的鼠标键盘是输入设备，显示器是输出设备。我们用的智能手机，触摸屏既是输入设备，又是输出设备。而跑在各种云上的服务器，则是通过网络来进行输入和输出。这个时候，网卡既是输入设备又是输出设备。
任何一台计算机的任何一个部件都可以归到运算器、控制器、存储器、输入设备和输出设备中，而所有的现代计算机也都是基于这个基础架构来设计开发的。
而所有的计算机程序，也都可以抽象为从输入设备读取输入信息，通过运算器和控制器来执行存储在存储器里的程序，最终把结果输出到输出设备中。而我们所有撰写的无论高级还是低级语言的程序，也都是基于这样一个抽象框架来进行运作的。
总结延伸可以说，冯·诺依曼体系结构确立了我们现在每天使用的计算机硬件的基础架构。因此，学习计算机组成原理，其实就是学习和拆解冯·诺依曼体系结构。
具体来说，学习组成原理，其实就是学习控制器、运算器的工作原理，也就是CPU是怎么工作的，以及为何这样设计；学习内存的工作原理，从最基本的电路，到上层抽象给到CPU乃至应用程序的接口是怎样的；学习CPU是怎么和输入设备、输出设备打交道的。
学习组成原理，就是在理解从控制器、运算器、存储器、输入设备以及输出设备，从电路这样的硬件，到最终开放给软件的接口，是怎么运作的，为什么要设计成这样，以及在软件开发层面怎么尽可能用好它。
好了，这一讲说到这儿就结束了。你应该已经理解了计算机的硬件是由哪些设备组成的，以及冯·诺依曼体系结构是什么样的了。下一讲，我会带你看一张地图，也是计算机组成原理的知识地图。我们一起来看一看怎么样才是学习组成原理的好方法。
推荐阅读我一直认为，读读经典的论文，是从一个普通工程师迈向优秀工程师必经的一步。如果你有时间，不妨去读一读First Draft of a Report on the EDVAC。对于工程师来说，直接读取英文论文的原文，既可以搞清楚、弄明白对应的设计及其背后的思路来源，还可以帮你破除对于论文或者核心技术的恐惧心理。</description></item><item><title>01_基础架构：一条SQL查询语句是如何执行的？</title><link>https://artisanbox.github.io/1/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/1/</guid><description>你好，我是林晓斌。
这是专栏的第一篇文章，我想来跟你聊聊MySQL的基础架构。我们经常说，看一个事儿千万不要直接陷入细节里，你应该先鸟瞰其全貌，这样能够帮助你从高维度理解问题。同样，对于MySQL的学习也是这样。平时我们使用数据库，看到的通常都是一个整体。比如，你有个最简单的表，表里只有一个ID字段，在执行下面这个查询语句时：
mysql&amp;gt; select * from T where ID=10； 我们看到的只是输入一条语句，返回一个结果，却不知道这条语句在MySQL内部的执行过程。
所以今天我想和你一起把MySQL拆解一下，看看里面都有哪些“零件”，希望借由这个拆解过程，让你对MySQL有更深入的理解。这样当我们碰到MySQL的一些异常或者问题时，就能够直戳本质，更为快速地定位并解决问题。
下面我给出的是MySQL的基本架构示意图，从中你可以清楚地看到SQL语句在MySQL的各个功能模块中的执行过程。
MySQL的逻辑架构图大体来说，MySQL可以分为Server层和存储引擎层两部分。
Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。
而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。
也就是说，你执行create table建表的时候，如果不指定引擎类型，默认使用的就是InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在create table语句中使用engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同，在后面的文章中，我们会讨论到引擎的选择。
从图中不难看出，不同的存储引擎共用一个Server层，也就是从连接器到执行器的部分。你可以先对每个组件的名字有个印象，接下来我会结合开头提到的那条SQL语句，带你走一遍整个执行流程，依次看下每个组件的作用。
连接器第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：
mysql -h$ip -P$port -u$user -p 输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在-p后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。
连接命令中的mysql是客户端工具，用来跟服务端建立连接。在完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。
如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。
连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在show processlist命令中看到它。文本中这个图是show processlist的结果，其中的Command列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。
客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时。
如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。
数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。
建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。
但是全部使用长连接后，你可能会发现，有些时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。
怎么解决这个问题呢？你可以考虑以下两种方案。
定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。
查询缓存连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步：查询缓存。
MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。
如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。
但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。
查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。
好在MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：
mysql&amp;gt; select SQL_CACHE * from T where ID=10； 需要注意的是，MySQL 8.</description></item><item><title>01_存储：一个完整的数据存储过程是怎样的？</title><link>https://artisanbox.github.io/8/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/1/</guid><description>你好，我是朱晓峰。今天，我想跟你聊一聊MySQL是怎么存储数据的。
存储数据是处理数据的第一步。在咱们的超市项目中，每天都要处理大量的商品，比如说进货、卖货、盘点库存，商品的种类很多，而且数量也比较大。只有正确地把数据存储起来，我们才能进行有效的处理和分析，进而对经营情况进行科学的评估，超市负责人在做决策时，就能够拿到数据支持。否则，只能是一团乱麻，没有头绪，也无从着手。
那么，怎样才能把用户各种经营相关的、纷繁复杂的数据，有序和高效地存储起来呢？
在MySQL中，一个完整的数据存储过程总共有4步，分别是创建数据库、确认字段、创建数据表、插入数据。
接下来，我就给你详细讲解一下这个过程的每一步，帮你掌握MySQL的数据存储机制。
先提醒你一句，这节课最后有一个视频，我在视频里演示了今天讲到的所有操作。我建议你学完文字以后，跟着视频实操一下。
好了，话不多说，我们现在开始。
创建MySQL数据库数据存储的第一步，就是创建数据库。
你可能会问，为啥我们要先创建一个数据库，而不是直接创建数据表呢？
这是个很好的问题。其实啊，这是因为，从系统架构的层次上看，MySQL数据库系统从大到小依次是数据库服务器、数据库、数据表、数据表的行与列。
安装程序已经帮我们安装了MySQL数据库服务器，所以，我们必须从创建数据库开始。
数据库是MySQL里面最大的存储单元。数据表、数据表里的数据，以及我们以后会学到的表与表之间的关系，还有在它们的基础上衍生出来的各种工具，都存储在数据库里面。没有数据库，数据表就没有载体，也就无法存储数据。
下面我就来给你具体介绍下，怎么在我们安装的MySQL服务器里面创建、删除和查看数据库。
1.如何创建数据库？创建数据库，我们已经在上节课介绍过了，你可以在Workbench的工作区，通过下面的SQL语句创建数据库“demo”：
CREATE DATABASE demo； 2.如何查看数据库？下面我们来看一下，如何查看数据库。
在Workbench的导航栏，我们可以看到数据库服务器里的所有数据库，如下图所示：
你也可以在Workbench右边的工作区，通过查询语句，查看所有的数据库：
mysql&amp;gt; SHOW DATABASES; +--------------------+ | Database | +--------------------+ | demo | | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.00 sec) 看到这儿，你是不是觉得很奇怪，为什么Workbench导航栏里面的数据库只有两个（我们创建的数据库“demo”和安装完MySQL就有的数据库“sys”）呢？
换句话说，为什么有的数据库我们可以在Workbench里面看到，有的数据库却必须通过查询语句才可以看到呢？要弄明白这个问题，你必须要知道这些数据库都是干什么的。
“demo”是我们通过SQL语句创建的数据库，是我们用来存储用户数据的，也是我们使用的主要数据库。 “information_schema”是MySQL系统自带的数据库，主要保存MySQL数据库服务器的系统信息，比如数据库的名称、数据表的名称、字段名称、存取权限、数据文件所在的文件夹和系统使用的文件夹，等等。 “performance_schema”是MySQL系统自带的数据库，可以用来监控MySQL的各类性能指标。 “sys”数据库是MySQL系统自带的数据库，主要作用是，以一种更容易被理解的方式展示MySQL数据库服务器的各类性能指标，帮助系统管理员和开发人员监控MySQL的技术性能。 “mysql”数据库保存了MySQL数据库服务器运行时需要的系统信息，比如数据文件夹、当前使用的字符集、约束检查信息，等等。 如果你是DBA，或者是MySQL数据库程序员，想深入了解MySQL数据库系统的相关信息，可以看下官方文档。
话说回来，为什么Workbench里面我们只能看到“demo”和“sys”这2个数据库呢？其实啊，这是因为，Workbench是图形化的管理工具，主要面向开发人员，“demo”和“sys”这2个数据库已经够用了。如果有特殊需求，比如，需要监控MySQL数据库各项性能指标、直接操作MySQL数据库系统文件等，可以由DBA通过SQL语句，查看其它的系统数据库。
确认字段数据存储流程的第二步是确认表的字段。
创建好数据库之后，我们选择要导入的Excel数据文件，MySQL会让我们确认新表中有哪些列，以及它们的数据类型。这些列就是MySQL数据表的字段。
MySQL数据表由行与列组成，一行就是一条数据记录，每一条数据记录都被分成许多列，一列就叫一个字段。每个字段都需要定义数据类型，这个数据类型叫做字段类型。
这样一来，每一条数据记录的每一个片段，就按照字段的定义被严格地管理起来了，从而使数据有序而且可靠。MySQL支持多种字段类型，字段的定义会影响数据的取值范围、精度，以及系统的可靠性，下节课我会重点给你讲一讲字段的定义。这里你只要选择系统默认的字段类型，就可以了。
创建数据表数据存储流程的第三步，是创建数据表。
当我们确认好了表的字段，点击下一步，Workbench就帮助我们创建了一张表。
MySQL中的数据表是什么呢？你可以把它看成用来存储数据的最主要工具。数据表对存储在里面的数据进行组织和管理，使数据变得有序，并且能够实现高效查询和处理。
虽然Workbench帮助我们创建了一个表，但大多数情况下，我们是不会先准备一个Excel文件，再通过Workbench的数据导入来创建表的，这样太麻烦了。我们可以通过SQL语句，自己来创建表。
具体咋做呢？我来介绍一下。</description></item><item><title>01_理解代码：编译器的前端技术</title><link>https://artisanbox.github.io/6/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/1/</guid><description>在开篇词里，我分享了一些使用编译技术的场景。其中有的场景，你只要掌握编译器的前端技术就能解决。比如文本分析场景，软件需要用户自定义功能的场景以及前端编程语言的翻译场景等。而且咱们大学讲的编译原理，也是侧重讲解前端技术，可见编译器的前端技术有多么重要。
当然了，这里的“前端（Front End）”指的是编译器对程序代码的分析和理解过程。它通常只跟语言的语法有关，跟目标机器无关。而与之对应的“后端（Back End）”则是生成目标代码的过程，跟目标机器有关。为了方便你理解，我用一张图直观地展现了编译器的整个编译过程。
你可以看到，编译器的“前端”技术分为词法分析、语法分析和语义分析三个部分。而它主要涉及自动机和形式语言方面的基础的计算理论。
这些抽象的理论也许会让你“撞墙”，不过不用担心，我今天会把难懂的理论放到一边，用你听得懂的大白话，联系实际使用的场景，带你直观地理解它们，让你学完本节课之后，实现以下目标：
对编译过程以及其中的技术点有个宏观、概要的了解。 能够在大脑里绘制一张清晰的知识地图，以应对工作需要。比如分析一个日志文件时，你能知道所对应的技术点，从而针对性地解决问题。 好了，接下来让我们正式进入今天的课程吧！
词法分析（Lexical Analysis）通常，编译器的第一项工作叫做词法分析。就像阅读文章一样，文章是由一个个的中文单词组成的。程序处理也一样，只不过这里不叫单词，而是叫做“词法记号”，英文叫Token。我嫌“词法记号”这个词太长，后面直接将它称作Token吧。
举个例子，看看下面这段代码，如果我们要读懂它，首先要怎么做呢？
#include &amp;lt;stdio.h&amp;gt; int main(int argc, char* argv[]){ int age = 45; if (age &amp;gt;= 17+8+20) { printf(&amp;quot;Hello old man!\\n&amp;quot;); } else{ printf(&amp;quot;Hello young man!\\n&amp;quot;); } return 0; } 我们会识别出if、else、int这样的关键字，main、printf、age这样的标识符，+、-、=这样的操作符号，还有花括号、圆括号、分号这样的符号，以及数字字面量、字符串字面量等。这些都是Token。
那么，如何写一个程序来识别Token呢？可以看到，英文内容中通常用空格和标点把单词分开，方便读者阅读和理解。但在计算机程序中，仅仅用空格和标点分割是不行的。比如“age &amp;gt;= 45”应该分成“age”“&amp;gt;=”和“45”这三个Token，但在代码里它们可以是连在一起的，中间不用非得有空格。
这和汉语有点儿像，汉语里每个词之间也是没有空格的。但我们会下意识地把句子里的词语正确地拆解出来。比如把“我学习编程”这个句子拆解成“我”“学习”“编程”，这个过程叫做“分词”。如果你要研发一款支持中文的全文检索引擎，需要有分词的功能。
其实，我们可以通过制定一些规则来区分每个不同的Token，我举了几个例子，你可以看一下。
识别age这样的标识符。它以字母开头，后面可以是字母或数字，直到遇到第一个既不是字母又不是数字的字符时结束。
识别&amp;gt;=这样的操作符。 当扫描到一个&amp;gt;字符的时候，就要注意，它可能是一个GT（Greater Than，大于）操作符。但由于GE（Greater Equal，大于等于）也是以&amp;gt;开头的，所以再往下再看一位，如果是=，那么这个Token就是GE，否则就是GT。
识别45这样的数字字面量。当扫描到一个数字字符的时候，就开始把它看做数字，直到遇到非数字的字符。
这些规则可以通过手写程序来实现。事实上，很多编译器的词法分析器都是手写实现的，例如GNU的C语言编译器。
如果嫌手写麻烦，或者你想花更多时间陪恋人或家人，也可以偷点儿懒，用词法分析器的生成工具来生成，比如Lex（或其GNU版本，Flex）。这些生成工具是基于一些规则来工作的，这些规则用“正则文法”表达，符合正则文法的表达式称为“正则表达式”。生成工具可以读入正则表达式，生成一种叫“有限自动机”的算法，来完成具体的词法分析工作。
不要被“正则文法（Regular Grammar）”和“有限自动机（Finite-state Automaton，FSA，or Finite Automaton）”吓到。正则文法是一种最普通、最常见的规则，写正则表达式的时候用的就是正则文法。我们前面描述的几个规则，都可以看成口语化的正则文法。
有限自动机是有限个状态的自动机器。我们可以拿抽水马桶举例，它分为两个状态：“注水”和“水满”。摁下冲马桶的按钮，它转到“注水”的状态，而浮球上升到一定高度，就会把注水阀门关闭，它转到“水满”状态。
词法分析器也是一样，它分析整个程序的字符串，当遇到不同的字符时，会驱使它迁移到不同的状态。例如，词法分析程序在扫描age的时候，处于“标识符”状态，等它遇到一个&amp;gt;符号，就切换到“比较操作符”的状态。词法分析过程，就是这样一个个状态迁移的过程。</description></item><item><title>01_程序的运行过程：从代码到机器运行</title><link>https://artisanbox.github.io/9/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/1/</guid><description>你好，我是LMOS。
欢迎来到操作系统第一课。在真正打造操作系统前，有一条必经之路：你知道程序是如何运行的吗？
一个熟练的编程老手只需肉眼看着代码，就能对其运行的过程了如指掌。但对于初学者来说，这常常是很困难的事，这需要好几年的程序开发经验，和在长期的程序开发过程中对编程基本功的积累。
我记得自己最初学习操作系统的时候，面对逻辑稍微复杂的一些程序，在编写、调试代码时，就会陷入代码的迷宫，找不到东南西北。
不知道你现在处在什么阶段，是否曾有同样的感受？我常常说，扎实的基本功就像手里的指南针，你可以一步步强大到不依赖它，但是不能没有。
因此今天，我将带领你从“Hello World”起，扎实基本功，探索程序如何运行的所有细节和原理。这节课的配套代码，你可以从这里下载。
一切要从牛人做的牛逼事说起第一位牛人，是世界级计算机大佬的传奇——Unix之父Ken Thompson。
在上世纪60年代的一个夏天，Ken Thompson的妻子要回娘家一个月。呆在贝尔实验室的他，竟然利用这极为孤独的一个月，开发出了UNiplexed Information and Computing System（UNICS）——即UNIX的雏形，一个全新的操作系统。
要知道，在当时C语言并没有诞生，从严格意义上说，他是用B语言和汇编语言在PDP-7的机器上完成的。
牛人的朋友也是牛人，他的朋友Dennis Ritchie也随之加入其中，共同创造了大名鼎鼎的C语言，并用C语言写出了UNIX和后来的类UNIX体系的几十种操作系统，也写出了对后世影响深远的第一版“Hello World”：
#include &amp;quot;stdio.h&amp;quot; int main(int argc, char const *argv[]) { printf(&amp;quot;Hello World!\n&amp;quot;); return 0; } 计算机硬件是无法直接运行这个C语言文本程序代码的，需要C语言编译器，把这个代码编译成具体硬件平台的二进制代码。再由具体操作系统建立进程，把这个二进制文件装进其进程的内存空间中，才能运行。
听起来很复杂？别急，接着往下看。
程序编译过程我们暂且不急着摸清操作系统所做的工作，先来研究一下编译过程和硬件执行程序的过程，约定使用GCC相关的工具链。
那么使用命令：gcc HelloWorld.c -o HelloWorld 或者 gcc ./HelloWorld.c -o ./HelloWorld ，就可以编译这段代码。其实，GCC只是完成编译工作的驱动程序，它会根据编译流程分别调用预处理程序、编译程序、汇编程序、链接程序来完成具体工作。
下图就是编译这段代码的过程：
其实，我们也可以手动控制以上这个编译流程，从而留下中间文件方便研究：
gcc HelloWorld.c -E -o HelloWorld.i预处理：加入头文件，替换宏。 gcc HelloWorld.c -S -c -o HelloWorld.s编译：包含预处理，将C程序转换成汇编程序。 gcc HelloWorld.c -c -o HelloWorld.o汇编：包含预处理和编译，将汇编程序转换成可链接的二进制程序。 gcc HelloWorld.c -o HelloWorld链接：包含以上所有操作，将可链接的二进制程序和其它别的库链接在一起，形成可执行的程序文件。 程序装载执行对运行内容有了了解后，我们开始程序的装载执行。</description></item><item><title>01_编译的全过程都悄悄做了哪些事情？</title><link>https://artisanbox.github.io/7/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/1/</guid><description>你好，我是宫文学。
正如我在开篇词中所说的，这一季课程的设计，是要带你去考察实际编译器的代码，把你带到编译技术的第一现场，让你以最直观、最接地气的方式理解编译器是怎么做出来的。
但是，毕竟编译领域还是有很多基本概念的。对于编译原理基础不太扎实的同学来说，在跟随我出发探险之前，最好还是做一点准备工作，磨刀不误砍柴工嘛。所以，在正式开始本课程之前，我会先花8讲的时间，用通俗的语言，帮你把编译原理的知识体系梳理一遍。
当然，对于已经学过编译原理的同学来说，这几讲可以帮助你复习以前学过的知识，把相关的知识点从遥远的记忆里再调出来，重温一下，以便更好地进入状态。
今天这一讲，我首先带你从宏观上理解一下整个编译过程。后面几讲中，我再针对编译过程中的每个阶段做细化讲解。
好了，让我们开始吧。
编译，其实就是把源代码变成目标代码的过程。如果源代码编译后要在操作系统上运行，那目标代码就是汇编代码，我们再通过汇编和链接的过程形成可执行文件，然后通过加载器加载到操作系统里执行。如果编译后是在解释器里执行，那目标代码就可以不是汇编代码，而是一种解释器可以理解的中间形式的代码即可。
我举一个很简单的例子。这里有一段C语言的程序，我们一起来看看它的编译过程。
int foo(int a){ int b = a + 3; return b; } 这段源代码，如果把它编译成汇编代码，大致是下面这个样子：
.section __TEXT,__text,regular,pure_instructions .globl _foo ## -- Begin function foo _foo: ## @foo pushq %rbp movq %rsp, %rbp movl %edi, -4(%rbp) movl -4(%rbp), %eax addl $3, %eax movl %eax, -8(%rbp) movl -8(%rbp), %eax popq %rbp retq 你可以看出，源代码和目标代码之间的差异还是很大的。那么，我们怎么实现这个翻译呢？
其实，编译和把英语翻译成汉语的大逻辑是一样的。前提是你要懂这两门语言，这样你看到一篇英语文章，在脑子里理解以后，就可以把它翻译成汉语。编译器也是一样，你首先需要让编译器理解源代码的意思，然后再把它翻译成另一种语言。
表面上看，好像从英语到汉语，一下子就能翻译过去。但实际上，大脑一瞬间做了很多个步骤的处理，包括识别一个个单词，理解语法结构，然后弄明白它的意思。同样，编译器翻译源代码，也需要经过多个处理步骤，如下图所示。
图1：编译的各个阶段我来解释一下各个步骤。
词法分析（Lexical Analysis）首先，编译器要读入源代码。
在编译之前，源代码只是一长串字符而已，这显然不利于编译器理解程序的含义。所以，编译的第一步，就是要像读文章一样，先把里面的单词和标点符号识别出来。程序里面的单词叫做Token，它可以分成关键字、标识符、字面量、操作符号等多个种类。把字符串转换为Token的这个过程，就叫做词法分析。
图2：把字符串转换为Token（注意：其中的空白字符，代表空格、tab、回车和换行符，EOF是文件结束符）语法分析（Syntactic Analysis）识别出Token以后，离编译器明白源代码的含义仍然有很长一段距离。下一步，我们需要让编译器像理解自然语言一样，理解它的语法结构。这就是第二步，语法分析。
上语文课的时候，老师都会让你给一个句子划分语法结构。比如说：“我喜欢又聪明又勇敢的你”，它的语法结构可以表示成下面这样的树状结构。
图3：把一个句子变成语法树那么在编译器里，语法分析阶段也会把Token串，转换成一个体现语法规则的、树状的数据结构，这个数据结构叫做抽象语法树（AST，Abstract Syntax Tree）。我们前面的示例程序转换为AST以后，大概是下面这个样子：</description></item><item><title>01｜实现一门超简单的语言最快需要多久？</title><link>https://artisanbox.github.io/3/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/3/</guid><description>你好，我是宫文学。
说到实现一门计算机语言，你肯定觉得这是一个庞大又复杂的工程，工作量巨大！
这个理解，我只能说部分正确。其实，有的时候，实现一门语言的速度也可以很快。比如，当年兰登·艾克（Brendan Eich）只花了10天时间就把JavaScript语言设计出来了。当然，语言跟其他软件一样，也需要不断迭代，至今JS的标准和实现仍在不停的演化。
如果我说，你也完全可以在这么短的时间内实现一门语言，甚至都不需要那么长时间，你一定会觉得我是在哗众取宠、标题党。
别急，我再补充说明一下，你马上就会认可我的说法了。这个让你一开始实现的版本，只是为了去探索计算机语言的原理，是高度简化的版本，并不要求马上能实用。你可以把它看做是一个原型系统，仅此而已，实现起来不会太复杂。
好吧，我知道你肯定还在心里打鼓：再简单的计算机语言，那也是一门语言呀，难度又能低到哪里去？
这样，先保留你的疑虑，我们进入今天的课程。今天我就要带你挑战，仅仅只用一节课时间，就实现一门超简洁的语言。我会暂时忽略很多的技术细节，带你抓住实现一门计算机语言的骨干部分，掌握其核心原理。在这节课中，你会快速获得两个技能：
如何通过编译器来理解某个程序； 如何解释执行这个程序。 这两个点，分别是编译时的核心和运行时的核心。理解了这两个知识点，你就大致理解计算机语言是如何工作的了！
我们的任务这节课，我们要让下面这个程序运行起来：
//一个函数的声明，这个函数很简单，只打印"Hello World!" function sayHello(){ println("Hello World!"); } //调用刚才声明的函数 sayHello(); 这个程序做了两件事：第一件是声明了一个函数，叫做sayHello；第二件事，就是调用sayHello()函数。运行这个程序的时候，我们期待它会输出“Hello World！”。
这个程序看上去还挺像那么回事的，但其实为降低难度，我们对JavaScript/TypeScript做了极度的简化：它只支持声明函数和调用函数，在我们的sayHello()函数里，它只能调用函数。你可以调用一个自己声明的函数，如foo，也可以调用语言内置的函数，如示例中的println()。
这还不够，为了进一步降低难度，我们的编译器是从一个数组里读取程序，而不是读一个文本文件。这个数组的每一个元素是一个单词，分别是function、sayHello、左括号、右括号等等，这些单词，我们叫它Token，它们是程序的最小构成单位。注意，最后一个Token比较特殊，它叫做EOF，有时会记做$，表示程序的结尾。
好了，现在任务清楚了，那我们开始第一步，解析这个程序。
解析这个程序解析，英文叫做Parse，是指读入程序，并形成一个计算机可以理解的数据结构的过程。能够完成解析工作的程序，就叫做解析器（Parser），它是编译器的组成部分之一。
那么，什么数据结构是计算机能够理解的呢？很简单，其实就是一棵树，这棵树叫做抽象语法树，英文缩写是AST（Abstract Syntax Tree）。针对我们的例子，这棵AST是下面的样子：
你仔细看一下这棵树，你会发现它跟我们程序想表达的思想是一样的：
根节点代表了整个程序； 根节点有两个子节点，分别代表一个函数声明和一个函数调用语句； 函数声明节点，又包含了两个子节点，一个是函数名称，一个是函数体； 函数体中又包含一个函数调用语句； 而函数调用语句呢，则是由函数名称和参数列表构成的； …… 通过这样自顶向下的层层分析，你会发现这棵树确实体现了我们原来的程序想表达的意思。其实，这就跟我们自己在阅读文章的时候是一样的，我们的大脑也是把段落分解成句子，再把句子分解成主语、谓语、宾语等一个个语法单元，最终形成一棵树型的结构，我们的大脑是能够理解这种树型结构的。
总结起来，解析器的工作，就是要读取一个Token串，然后把它转换成一棵AST。
好了，知道了解析的工作目标后，我们就来实现这个解析器吧！
可是，这怎么下手呢？
你可以琢磨一下，你的大脑是如何理解这些程序，并且把它们在不知不觉之间转化成一棵树的。我们假设，人类的大脑采用了一种自顶向下的分析方式，也就是把一个大的解析任务逐步分解成小的任务，落实到解析器的实现上也是如此。
首先，我们的目的是识别Token串的特征，并把它转换成AST。我们暂时忽略细节，假设我们能够成功地完成这个解析，那么把这个解析动作写成代码，就是：
prog = parseProg()； 我们再具体一点，看看实现parseProg()需要做什么事情。
parseProg()需要建立程序的子节点，也就是函数声明或者函数调用。我们规定一个程序可以有零到多个函数声明或函数调用。我们把这个语法规则用比较严谨的方法表达出来，是这样的：
prog = (functionDecl | functionCall)* ; 咦？这个表达方式看上去有点熟悉呀？这个式子的格式叫做EBNF格式（扩展巴科斯范式）。你可以看到，它的左边是一个语法成份的名称，右边说的是这个语法成份是由哪些子成分构成的。这样，整个式子就构成了一条语法规则。
不过，编译原理的教科书里，有时会用产生式的格式。EBNF格式和产生式是等价的，区别是产生式不允许使用？、*和+号，而是用递归来表示多个元素的重复，用ε来表示不生成任何字符串。如果我们把上述语法规则用产生式来表示的话，相当于下面四条：
prog -&amp;gt; statement prog prog -&amp;gt; ε statement -&amp;gt; functionDecl statement -&amp;gt; functionCall 你马上就能看出来，还是EBNF格式更简洁吧？</description></item><item><title>02_几行汇编几行C：实现一个最简单的内核</title><link>https://artisanbox.github.io/9/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/2/</guid><description>你好，我是LMOS。
我们知道，在学习许多编程语言一开始的时候，都有一段用其语言编写的经典程序——Hello World。这不过是某一操作系统平台之上的应用程序，却心高气傲地问候世界。
而我们学习操作系统的时候，那么也不妨撇开其它现有的操作系统，基于硬件，写一个最小的操作系统——Hello OS，先练练手、热热身，直观感受一下。
本节课的配套代码，你可以从这里下载。
请注意，这节课主要是演示思路，不要求你马上动手实现。详细的环境安装、配置我们到第十节课再详细展开。有兴趣上手的同学，可以参考留言区置顶的实验笔记探索。
PC机的引导流程看标题就知道，写操作系统要用汇编和C语言，尽管这个Hello OS很小，但也要用到两种编程语言。其实，现有的商业操作系统都是用这两种语言开发出来的。
先不用害怕，Hello OS的代码量很少。
其实，我们也不打算从PC的引导程序开始写起，原因是目前我们的知识储备还不够，所以先借用一下GRUB引导程序，只要我们的PC机上安装了Ubuntu Linux操作系统，GRUB就已经存在了。这会大大降低我们开始的难度，也不至于打消你的热情。
那在写Hello OS之前，我们先要搞清楚Hello OS的引导流程，如下图所示：
简单解释一下，PC机BIOS固件是固化在PC机主板上的ROM芯片中的，掉电也能保存，PC机上电后的第一条指令就是BIOS固件中的，它负责检测和初始化CPU、内存及主板平台，然后加载引导设备（大概率是硬盘）中的第一个扇区数据，到0x7c00地址开始的内存空间，再接着跳转到0x7c00处执行指令，在我们这里的情况下就是GRUB引导程序。
当然，更先进的UEFI BIOS则不同，这里就不深入其中了，你可以通过链接自行了解。
Hello OS引导汇编代码明白了PC机的启动流程，下面只剩下我们的Hello OS了，我们马上就去写好它。
我们先来写一段汇编代码。这里我要特别说明一个问题：为什么不能直接用C？
C作为通用的高级语言，不能直接操作特定的硬件，而且C语言的函数调用、函数传参，都需要用栈。
栈简单来说就是一块内存空间，其中数据满足后进先出的特性，它由CPU特定的栈寄存器指向，所以我们要先用汇编代码处理好这些C语言的工作环境。
;彭东 @ 2021.01.09 MBT_HDR_FLAGS EQU 0x00010003 MBT_HDR_MAGIC EQU 0x1BADB002 ;多引导协议头魔数 MBT_HDR2_MAGIC EQU 0xe85250d6 ;第二版多引导协议头魔数 global _start ;导出_start符号 extern main ;导入外部的main函数符号 [section .start.text] ;定义.start.text代码节 [bits 32] ;汇编成32位代码 _start: jmp _entry ALIGN 8 mbt_hdr: dd MBT_HDR_MAGIC dd MBT_HDR_FLAGS dd -(MBT_HDR_MAGIC+MBT_HDR_FLAGS) dd mbt_hdr dd _start dd 0 dd 0 dd _entry ;以上是GRUB所需要的头 ALIGN 8 mbt2_hdr: DD MBT_HDR2_MAGIC DD 0 DD mbt2_hdr_end - mbt2_hdr DD -(MBT_HDR2_MAGIC + 0 + (mbt2_hdr_end - mbt2_hdr)) DW 2, 0 DD 24 DD mbt2_hdr DD _start DD 0 DD 0 DW 3, 0 DD 12 DD _entry DD 0 DW 0, 0 DD 8 mbt2_hdr_end: ;以上是GRUB2所需要的头 ;包含两个头是为了同时兼容GRUB、GRUB2 ALIGN 8 _entry: ;关中断 cli ;关不可屏蔽中断 in al, 0x70 or al, 0x80 out 0x70,al ;重新加载GDT lgdt [GDT_PTR] jmp dword 0x8 :_32bits_mode _32bits_mode: ;下面初始化C语言可能会用到的寄存器 mov ax, 0x10 mov ds, ax mov ss, ax mov es, ax mov fs, ax mov gs, ax xor eax,eax xor ebx,ebx xor ecx,ecx xor edx,edx xor edi,edi xor esi,esi xor ebp,ebp xor esp,esp ;初始化栈，C语言需要栈才能工作 mov esp,0x9000 ;调用C语言函数main call main ;让CPU停止执行指令 halt_step: halt jmp halt_step GDT_START: knull_dsc: dq 0 kcode_dsc: dq 0x00cf9e000000ffff kdata_dsc: dq 0x00cf92000000ffff k16cd_dsc: dq 0x00009e000000ffff k16da_dsc: dq 0x000092000000ffff GDT_END: GDT_PTR: GDTLEN dw GDT_END-GDT_START-1 GDTBASE dd GDT_START 以上的汇编代码（/lesson02/HelloOS/entry.</description></item><item><title>02_如何抓住重点，系统高效地学习数据结构与算法？</title><link>https://artisanbox.github.io/2/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/3/</guid><description>你是否曾跟我一样，因为看不懂数据结构和算法，而一度怀疑是自己太笨？实际上，很多人在第一次接触这门课时，都会有这种感觉，觉得数据结构和算法很抽象，晦涩难懂，宛如天书。正是这个原因，让很多初学者对这门课望而却步。
我个人觉得，其实真正的原因是你没有找到好的学习方法，没有抓住学习的重点。实际上，数据结构和算法的东西并不多，常用的、基础的知识点更是屈指可数。只要掌握了正确的学习方法，学起来并没有看上去那么难，更不需要什么高智商、厚底子。
还记得大学里每次考前老师都要划重点吗？今天，我就给你划划我们这门课的重点，再告诉你一些我总结的学习小窍门。相信有了这些之后，你学起来就会有的放矢、事半功倍了。
什么是数据结构？什么是算法？大部分数据结构和算法教材，在开篇都会给这两个概念下一个明确的定义。但是，这些定义都很抽象，对理解这两个概念并没有实质性的帮助，反倒会让你陷入死抠定义的误区。毕竟，我们现在学习，并不是为了考试，所以，概念背得再牢，不会用也就没什么用。
虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。 下面我就从广义和狭义两个层面，来帮你理解数据结构与算法这两个概念。
从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。
图书馆储藏书籍你肯定见过吧？为了方便查找，图书管理员一般会将书籍分门别类进行“存储”。按照一定规律编号，就是书籍这种“数据”的存储结构。
那我们如何来查找一本书呢？有很多种办法，你当然可以一本一本地找，也可以先根据书籍类别的编号，是人文，还是科学、计算机，来定位书架，然后再依次查找。笼统地说，这些查找方法都是算法。
从狭义上讲，也就是我们专栏要讲的，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。这些都是前人智慧的结晶，我们可以直接拿来用。我们要讲的这些经典数据结构和算法，都是前人从很多实际操作场景中抽象出来的，经过非常多的求证和检验，可以高效地帮助我们解决很多实际的开发问题。
那数据结构和算法有什么关系呢？为什么大部分书都把这两个东西放到一块儿来讲呢？
这是因为，数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。 因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。
比如，因为数组具有随机访问的特点，常用的二分查找算法需要用数组来存储数据。但如果我们选择链表这种数据结构，二分查找算法就无法工作了，因为链表并不支持随机访问。
数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。
现在你对数据结构与算法是不是有了比较清晰的理解了呢？有了这些储备，下面我们来看看，究竟该怎么学数据结构与算法。
学习这个专栏需要什么基础？看到数据结构和算法里的“算法”两个字，很多人就会联想到“数学”，觉得算法会涉及到很多深奥的数学知识。那我数学基础不是很好，学起来会不会很吃力啊？
数据结构和算法课程确实会涉及一些数学方面的推理、证明，尤其是在分析某个算法的时间、空间复杂度的时候，但是这个你完全不需要担心。
这个专栏不会像《算法导论》那样，里面有非常复杂的数学证明和推理。我会由浅入深，从概念到应用，一点一点给你解释清楚。你只要有高中数学水平，就完全可以学习。
当然，我希望你最好有些编程基础，如果有项目经验就更好了。这样我给你讲数据结构和算法如何提高效率、如何节省存储空间，你就会有很直观的感受。因为，对于每个概念和实现过程，我都会从实际场景出发，不仅教你“是什么”，还会教你“为什么”，并且告诉你遇到同类型问题应该“怎么做”。
学习的重点在什么地方？提到数据结构和算法，很多人就很头疼，因为这里面的内容实在是太多了。这里，我就帮你梳理一下，应该先学什么，后学什么。你可以对照看看，你属于哪个阶段，然后有针对性地进行学习。
想要学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。
这个概念究竟有多重要呢？可以这么说，它几乎占了数据结构和算法这门课的半壁江山，是数据结构和算法学习的精髓。
数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无招胜有招！
所以，复杂度分析这个内容，我会用很大篇幅给你讲透。你也一定要花大力气来啃，必须要拿下，并且要搞得非常熟练。否则，后面的数据结构和算法也很难学好。
搞定复杂度分析，下面就要进入数据结构与算法的正文内容了。
为了让你对数据结构和算法能有个全面的认识，我画了一张图，里面几乎涵盖了所有数据结构和算法书籍中都会讲到的知识点。
（图谱内容较多，建议长按保存后浏览）
但是，作为初学者，或者一个非算法工程师来说，你并不需要掌握图里面的所有知识点。很多高级的数据结构与算法，比如二分图、最大流等，这些在我们平常的开发中很少会用到。所以，你暂时可以不用看。我还是那句话，咱们学习要学会找重点。如果不分重点地学习，眉毛胡子一把抓，学起来肯定会比较吃力。
所以，结合我自己的学习心得，还有这些年的面试、开发经验，我总结了20个最常用的、最基础数据结构与算法，不管是应付面试还是工作需要，只要集中精力逐一攻克这20个知识点就足够了。
这里面有10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树；10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。
掌握了这些基础的数据结构和算法，再学更加复杂的数据结构和算法，就会非常容易、非常快。
在学习数据结构和算法的过程中，你也要注意，不要只是死记硬背，不要为了学习而学习，而是要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”。对于每一种数据结构或算法，我都会从这几个方面进行详细讲解。只要你掌握了我每节课里讲的内容，就能在开发中灵活应用。
学习数据结构和算法的过程，是非常好的思维训练的过程，所以，千万不要被动地记忆，要多辩证地思考，多问为什么。如果你一直这么坚持做，你会发现，等你学完之后，写代码的时候就会不由自主地考虑到很多性能方面的事情，时间复杂度、空间复杂度非常高的垃圾代码出现的次数就会越来越少。你的编程内功就真正得到了修炼。
一些可以让你事半功倍的学习技巧前面我给你划了学习的重点，也讲了学习这门课需要具备的基础。作为一个过来人，现在我就给你分享一下，专栏学习的一些技巧。掌握了这些技巧，可以让你化被动为主动，学起来更加轻松，更加有动力！
1.边学边练，适度刷题“边学边练”这一招非常有用。建议你每周花1～2个小时的时间，集中把这周的三节内容涉及的数据结构和算法，全都自己写出来，用代码实现一遍。这样一定会比单纯地看或者听的效果要好很多！
有面试需求的同学，可能会问了，那我还要不要去刷题呢？
我个人的观点是可以“适度”刷题，但一定不要浪费太多时间在刷题上。我们学习的目的还是掌握，然后应用。除非你要面试Google、Facebook这样的公司，它们的算法题目非常非常难，必须大量刷题，才能在短期内提升应试正确率。如果是应对国内公司的技术面试，即便是BAT这样的公司，你只要彻底掌握这个专栏的内容，就足以应对。
2.多问、多思考、多互动学习最好的方法是，找到几个人一起学习，一块儿讨论切磋，有问题及时寻求老师答疑。 但是，离开大学之后，既没有同学也没有老师，这个条件就比较难具备了。
不过，这也就是咱们专栏学习的优势。专栏里有很多跟你一样的学习者。你可以多在留言区写下自己的疑问、思考和总结，也可以经常看看别人的留言，和他们进行互动。
除此之外，如果你有疑问，你可以随时在留言区给我留言，我只要有空就会及时回复你。你不要担心问的问题太小白。因为我初学的时候，也常常会被一些小白问题困扰。不懂一点都不丢人，只要你勇敢提出来，我们一起解决了就可以了。
我也会力争每节课都最大限度地给你讲透，帮你扫除知识盲点，而你要做的就是，避免一知半解，要想尽一切办法去搞懂我讲的所有内容。
3.打怪升级学习法学习的过程中，我们碰到最大的问题就是，坚持不下来。 是的，很多基础课程学起来都非常枯燥。为此，我自己总结了一套“打怪升级学习法”。
游戏你肯定玩过吧？为什么很多看起来非常简单又没有乐趣的游戏，你会玩得不亦乐乎呢？这是因为，当你努力打到一定级别之后，每天看着自己的经验值、战斗力在慢慢提高，那种每天都在一点一点成长的成就感就不由自主地产生了。
所以，我们在枯燥的学习过程中，也可以给自己设立一个切实可行的目标，就像打怪升级一样。
比如，针对这个专栏，你就可以设立这样一个目标：每节课后的思考题都认真思考，并且回复到留言区。当你看到很多人给你点赞之后，你就会为了每次都能发一个漂亮的留言，而更加认真地学习。
当然，还有很多其他的目标，比如，每节课后都写一篇学习笔记或者学习心得；或者你还可以每节课都找一下我讲得不对、不合理的地方……诸如此类，你可以总结一个适合你的“打怪升级攻略”。
如果你能这样学习一段时间，不仅能收获到知识，你还会有意想不到的成就感。因为，这其实帮你改掉了一点学习的坏习惯。这个习惯一旦改掉了，你的人生也会变得不一样。
4.知识需要沉淀，不要想试图一下子掌握所有在学习的过程中，一定会碰到“拦路虎”。如果哪个知识点没有怎么学懂，不要着急，这是正常的。因为，想听一遍、看一遍就把所有知识掌握，这肯定是不可能的。学习知识的过程是反复迭代、不断沉淀的过程。
如果碰到“拦路虎”，你可以尽情地在留言区问我，也可以先沉淀一下，过几天再重新学一遍。所谓，书读百遍其义自见，我觉得是很有道理的！
我讲的这些学习方法，不仅仅针对咱们这一个课程的学习，其实完全适用任何知识的学习过程。你可以通过这个专栏的学习，实践一下这些方法。如果效果不错，再推广到之后的学习过程中。
内容小结今天，我带你划了划数据结构和算法的学习重点，复杂度分析，以及10个数据结构和10个算法。
这些内容是我根据平时的学习和工作、面试经验积累，精心筛选出来的。只要掌握这些内容，应付日常的面试、工作，基本不会有问题。
除此之外，我还给你分享了我总结的一些学习技巧，比如边学边练、多问、多思考，还有两个比较通用的学习方法，打怪升级法和沉淀法。掌握了这些学习技巧，可以让你学习过程中事半功倍。所以，你一定要好好实践哦！
课后思考今天的内容是一个准备课，从下节开始，我们就要正式开始学习精心筛选出的这20个数据结构和算法了。所以，今天给你布置一个任务，对照我上面讲的“打怪升级学习法”，请思考一下你自己学习这个专栏的方法，让我们一起在留言区立下Flag，相互鼓励！
另外，你在之前学习数据结构和算法的过程中，遇到过什么样的困难或者疑惑吗？
欢迎留言和我分享，我会第一时间给你反馈。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>02_字段：这么多字段类型，该怎么定义？</title><link>https://artisanbox.github.io/8/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/2/</guid><description>你好，我是朱晓峰。
MySQL中有很多字段类型，比如整数、文本、浮点数，等等。如果类型定义合理，就能节省存储空间，提升数据查询和处理的速度，相反，如果数据类型定义不合理，就有可能会导致数据超出取值范围，引发系统报错，甚至可能会出现计算错误的情况，进而影响到整个系统。
之前，我们就遇到过这样一个问题：在销售流水表中，需要定义商品销售的数量。由于有称重商品，不能用整数，我们想当然地用了浮点数，为了确保精度，我们还用了DOUBLE类型。结果却造成了在没有找零的情况下，客人无法结账的重大错误。经过排查，我们才发现，原来DOUBLE类型是不精准的，不能使用。
你看，准确地定义字段类型，不但关系到数据存储的效率，而且会影响整个信息系统的可靠性。所以，我们必须要掌握不同字段的类型，包括它们的适用场景、定义方法，这节课，我们就聊一聊这个问题。
首先，我要说的是MySQL中最简单的数据类型：整数类型。
整数类型整数类型一共有5种，包括TINYINT、SMALLINT、MEDIUMINT、INT（INTEGER）和BIGINT，它们的区别如下表所示：
这么多整数类型，咱们该怎么选择呢？
其实，在评估用哪种整数类型的时候，你需要考虑存储空间和可靠性的平衡问题：一方面，用占用字节数少的整数类型可以节省存储空间；另一方面，要是为了节省存储空间，使用的整数类型取值范围太小，一旦遇到超出取值范围的情况，就可能引起系统错误，影响可靠性。
举个例子，在我们的项目中，商品编号采用的数据类型是INT。
我们之所以没有采用占用字节更少的SMALLINT类型整数，原因就在于，客户门店中流通的商品种类较多，而且，每天都有旧商品下架，新商品上架，这样不断迭代，日积月累。如果使用SMALLINT类型，虽然占用字节数比INT类型的整数少，但是却不能保证数据不会超出范围65535。相反，使用INT，就能确保有足够大的取值范围，不用担心数据超出范围影响可靠性的问题。
你要注意的是，在实际工作中，系统故障产生的成本远远超过增加几个字段存储空间所产生的成本。因此，我建议你首先确保数据不会超过取值范围，在这个前提之下，再去考虑如何节省存储空间。
接下来，我再给你介绍下浮点数类型和定点数类型。
浮点数类型和定点数类型浮点数和定点数类型的特点是可以处理小数，你可以把整数看成小数的一个特例。因此，浮点数和定点数的使用场景，就比整数大多了。
我们先来了解下MySQL支持的浮点数类型，分别是FLOAT、DOUBLE、REAL。
FLOAT表示单精度浮点数； DOUBLE表示双精度浮点数； REAL默认就是DOUBLE。如果你把SQL模式设定为启用“REAL_AS_FLOAT”，那么，MySQL就认为REAL是FLOAT。如果要启用“REAL_AS_FLOAT”，就可以通过以下SQL语句实现： SET sql_mode = “REAL_AS_FLOAT”; FLOAT和DOUBLE这两种数据类型的区别是啥呢？其实就是，FLOAT占用字节数少，取值范围小；DOUBLE占用字节数多，取值范围也大。
看到这儿，你有没有发现一个问题：为什么浮点数类型的无符号数取值范围，只相当于有符号数取值范围的一半，也就是只相当于有符号数取值范围大于等于零的部分呢？
其实，这里的原因是，MySQL是按照这个格式存储浮点数的：符号（S）、尾数（M）和阶码（E）。因此，无论有没有符号，MySQL的浮点数都会存储表示符号的部分。因此，所谓的无符号数取值范围，其实就是有符号数取值范围大于等于零的部分。
不过，我要提醒你的是，浮点数类型有个缺陷，就是不精准。因此，在一些对精确度要求较高的项目中，千万不要使用浮点数，不然会导致结果错误，甚至是造成不可挽回的损失。下面我来重点解释一下为什么MySQL的浮点数不够精准。
为了方便你理解，我来借助一个实际的例子演示下。
我们先创建一个表，如下所示：
CREATE TABLE demo.goodsmaster ( barcode TEXT, goodsname TEXT, price DOUBLE, itemnumber INT PRIMARY KEY AUTO_INCREMENT ); 运行这个语句，我们就创建了一个表，其中的字段“price”就是浮点数类型。我们再通过下面的SQL语句，给这个表插入几条数据：
-- 第一条 INSERT INTO demo.goodsmaster ( barcode, goodsname, price ) VALUES ( '0001', '书', 0.47 ); -- 第二条 INSERT INTO demo.goodsmaster ( barcode, goodsname, price ) VALUES ( '0002', '笔', 0.</description></item><item><title>02_日志系统：一条SQL更新语句是如何执行的？</title><link>https://artisanbox.github.io/1/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/2/</guid><description>前面我们系统了解了一个查询语句的执行流程，并介绍了执行过程中涉及的处理模块。相信你还记得，一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。
那么，一条更新语句的执行流程又是怎样的呢？
之前你可能经常听DBA同事说，MySQL可以恢复到半个月内任意一秒的状态，惊叹的同时，你是不是心中也会不免会好奇，这是怎样做到的呢？
我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键ID和一个整型字段c：
mysql&amp;gt; create table T(ID int primary key, c int); 如果要将ID=2这一行的值加1，SQL语句就会这么写：
mysql&amp;gt; update T set c=c+1 where ID=2; 前面我有跟你介绍过SQL语句基本的执行链路，这里我再把那张图拿过来，你也可以先简单看看这个图回顾下。首先，可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。
MySQL的逻辑架构图你执行语句前要先连接数据库，这是连接器的工作。
前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。
接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引。然后，执行器负责具体执行，找到这一行，然后更新。
与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。如果接触MySQL，那这两个词肯定是绕不过的，我后面的内容里也会不断地和你强调。不过话说回来，redo log和binlog在设计上有很多有意思的地方，这些设计思路也可以用到你自己的程序里。
重要的日志模块：redo log不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。
如果有人要赊账或者还账的话，掌柜一般有两种做法：
一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉； 另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算。 在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。
这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的帮助，每次记账都得翻账本，效率是不是低得让人难以忍受？
同样，在MySQL里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。为了解决这个问题，MySQL的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。
而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。
具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。
如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。
与此类似，InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。
write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。
write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。
有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。
要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。
重要的日志模块：binlog前面我们讲过，MySQL整体来看，其实就有两块：一块是Server层，它主要做的是MySQL功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。
我想你肯定会问，为什么会有两份日志呢？
因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。
这两种日志有以下三点不同。
redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。
redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</description></item><item><title>02_正则文法和有限自动机：纯手工打造词法分析器</title><link>https://artisanbox.github.io/6/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/2/</guid><description>上一讲，我提到词法分析的工作是将一个长长的字符串识别出一个个的单词，这一个个单词就是Token。而且词法分析的工作是一边读取一边识别字符串的，不是把字符串都读到内存再识别。你在听一位朋友讲话的时候，其实也是同样的过程，一边听，一边提取信息。
那么问题来了，字符串是一连串的字符形成的，怎么把它断开成一个个的Token呢？分割的依据是什么呢？本节课，我会通过讲解正则表达式（Regular Expression）和有限自动机的知识带你解决这个问题。
其实，我们手工打造词法分析器的过程，就是写出正则表达式，画出有限自动机的图形，然后根据图形直观地写出解析代码的过程。而我今天带你写的词法分析器，能够分析以下3个程序语句：
age &amp;gt;= 45 int age = 40 2+3*5 它们分别是关系表达式、变量声明和初始化语句，以及算术表达式。
接下来，我们先来解析一下“age &amp;gt;= 45”这个关系表达式，这样你就能理解有限自动机的概念，知道它是做词法解析的核心机制了。
解析 age &amp;gt;= 45在“01 | 理解代码：编译器的前端技术”里，我举了一个词法分析的例子，并且提出词法分析要用到有限自动机。当时，我画了这样一个示意图：
我们来描述一下标识符、比较操作符和数字字面量这三种Token的词法规则。
标识符：第一个字符必须是字母，后面的字符可以是字母或数字。 比较操作符：&amp;gt;和&amp;gt;=（其他比较操作符暂时忽略）。 数字字面量：全部由数字构成（像带小数点的浮点数，暂时不管它）。 我们就是依据这样的规则，来构造有限自动机的。这样，词法分析程序在遇到age、&amp;gt;=和45时，会分别识别成标识符、比较操作符和数字字面量。不过上面的图只是一个简化的示意图，一个严格意义上的有限自动机是下面这种画法：
我来解释一下上图的5种状态。
1.初始状态：刚开始启动词法分析的时候，程序所处的状态。
2.标识符状态：在初始状态时，当第一个字符是字母的时候，迁移到状态2。当后续字符是字母和数字时，保留在状态2。如果不是，就离开状态2，写下该Token，回到初始状态。
3.大于操作符（GT）：在初始状态时，当第一个字符是&amp;gt;时，进入这个状态。它是比较操作符的一种情况。
4.大于等于操作符（GE）：如果状态3的下一个字符是=，就进入状态4，变成&amp;gt;=。它也是比较操作符的一种情况。
5.数字字面量：在初始状态时，下一个字符是数字，进入这个状态。如果后续仍是数字，就保持在状态5。
这里我想补充一下，你能看到上图中的圆圈有单线的也有双线的。双线的意思是这个状态已经是一个合法的Token了，单线的意思是这个状态还是临时状态。
按照这5种状态迁移过程，你很容易编成程序（我用Java写了代码示例，你可以用自己熟悉的语言编写）。我们先从状态1开始，在遇到不同的字符时，分别进入2、3、5三个状态：
DfaState newState = DfaState.Initial; if (isAlpha(ch)) { //第一个字符是字母 newState = DfaState.Id; //进入Id状态 token.type = TokenType.Identifier; tokenText.append(ch); } else if (isDigit(ch)) { //第一个字符是数字 newState = DfaState.IntLiteral; token.type = TokenType.IntLiteral; tokenText.append(ch); } else if (ch == '&amp;gt;') { //第一个字符是&amp;gt; newState = DfaState.</description></item><item><title>02_给你一张知识地图，计算机组成原理应该这么学</title><link>https://artisanbox.github.io/4/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/2/</guid><description>了解了现代计算机的基本硬件组成和背后最基本的冯·诺依曼体系结构，我们就可以正式进入计算机组成原理的学习了。在学习一个一个零散的知识点之前，我整理了一份学习地图，好让你对将要学习的内容有一个总纲层面的了解。
从这张图可以看出来，整个计算机组成原理，就是围绕着计算机是如何组织运作展开的。
计算机组成原理知识地图计算机组成原理的英文叫Computer Organization。这里的Organization是“组织机构”的意思。计算机由很多个不同的部件放在一起，变成了一个“组织机构”。这个组织机构最终能够进行各种计算、控制、读取输入，进行输出，达成各种强大的功能。
在这张图里面，我们把整个计算机组成原理的知识点拆分成了四大部分，分别是计算机的基本组成、计算机的指令和计算、处理器设计，以及存储器和I/O设备。
首先，我们来看计算机的基本组成。
这一部分，你需要学习计算机是由哪些硬件组成的。这些硬件，又是怎么对应到经典的冯·诺依曼体系结构中的，也就是运算器、控制器、存储器、输入设备和输出设备这五大基本组件。除此之外，你还需要了解计算机的两个核心指标，性能和功耗。性能和功耗也是我们在应用和设计五大基本组件中需要重点考虑的因素。
了解了组成部分，接下来你需要掌握计算机的指令和计算。
在计算机指令部分，你需要搞明白，我们每天撰写的一行行C、Java、PHP程序，是怎么在计算机里面跑起来的。这里面，你既需要了解我们的程序是怎么通过编译器和汇编器，变成一条条机器指令这样的编译过程（如果把编译过程展开的话，可以变成一门完整的编译原理课程），还需要知道我们的操作系统是怎么链接、装载、执行这些程序的（这部分知识如果再深入学习，又可以变成一门操作系统课程）。而这一条条指令执行的控制过程，就是由计算机五大组件之一的控制器来控制的。
在计算机的计算部分，你要从二进制和编码开始，理解我们的数据在计算机里的表示，以及我们是怎么从数字电路层面，实现加法、乘法这些基本的运算功能的。实现这些运算功能的ALU（Arithmetic Logic Unit/ALU），也就是算术逻辑单元，其实就是我们计算机五大组件之一的运算器。
这里面有一个在今天看起来特别重要的知识点，就是浮点数（Floating Point）。浮点数是我们在日常运用中非常容易用错的一种数据表示形式。掌握浮点数能让你对数据的编码、存储和计算能够有一个从表到里的深入理解。尤其在AI火热的今天，浮点数是机器学习中重度使用的数据表示形式，掌握它更是非常有必要。
明白计算机指令和计算是如何运转的，我们就可以深入到CPU的设计中去一探究竟了。
CPU时钟可以用来构造寄存器和内存的锁存器和触发器，因此，CPU时钟应该是我们学习CPU的前导知识。搞明白我们为什么需要CPU时钟（CPU Clock），以及寄存器和内存是用什么样的硬件组成的之后，我们可以再来看看，整个计算机的数据通路是如何构造出来的。
数据通路，其实就是连接了整个运算器和控制器，并最终组成了CPU。而出于对于性能和功耗的考虑，你要进一步理解和掌握面向流水线设计的CPU、数据和控制冒险，以及分支预测的相关技术。
既然CPU作为控制器要和输入输出设备通信，那么我们就要知道异常和中断发生的机制。在CPU设计部分的最后，我会讲一讲指令的并行执行，看看如何直接在CPU层面，通过SIMD来支持并行计算。
最后，我们需要看一看，计算机五大组成部分之一，存储器的原理。通过存储器的层次结构作为基础的框架引导，你需要掌握从上到下的CPU高速缓存、内存、SSD硬盘和机械硬盘的工作原理，它们之间的性能差异，以及实际应用中利用这些设备会遇到的挑战。存储器其实很多时候又扮演了输入输出设备的角色，所以你需要进一步了解，CPU和这些存储器之间是如何进行通信的，以及我们最重视的性能问题是怎么一回事；理解什么是IO_WAIT，如何通过DMA来提升程序性能。
对于存储器，我们不仅需要它们能够正常工作，还要确保里面的数据不能丢失。于是你要掌握我们是如何通过RAID、Erasure Code、ECC以及分布式HDFS，这些不同的技术，来确保数据的完整性和访问性能。
学习计算机组成原理，究竟有没有好办法？相信这个学习地图，应该让你对计算机组成这门课要学些什么，有了一些了解。不过这个地图上的知识点繁多，应该也给你带来了不小的挑战。
我上一节也说过，相较于整个计算机科学中的其他科目，计算机组成原理更像是整个计算机学科里的“纲要”。这门课里任何一个知识点深入挖下去，都可以变成计算机科学里的一门核心课程。
比如说，程序怎样从高级代码变成指令在计算机里面运行，对应着“编译原理”和“操作系统”这两门课程；计算实现背后则是“数字电路”；如果要深入CPU和存储器系统的优化，必然要深入了解“计算机体系结构”。
因此，为了帮你更快更好地学计算机组成，我为你总结了三个学习方法，帮你更好地掌握这些知识点，并且能够学为所用，让你在工作中能够用得上。
首先，学会提问自己来串联知识点。学完一个知识点之后，你可以从下面两个方面，问一下自己。
我写的程序，是怎样从输入的代码，变成运行的程序，并得到最终结果的？
整个过程中，计算器层面到底经历了哪些步骤，有哪些地方是可以优化的？
无论是程序的编译、链接、装载和执行，以及计算时需要用到的逻辑电路、ALU，乃至CPU自发为你做的流水线、指令级并行和分支预测，还有对应访问到的硬盘、内存，以及加载到高速缓存中的数据，这些都对应着我们学习中的一个个知识点。建议你自己脑子里过一遍，最好是口头表述一遍或者写下来，这样对你彻底掌握这些知识点都会非常有帮助。
其次，写一些示例程序来验证知识点。计算机科学是一门实践的学科。计算机组成中的大量原理和设计，都对应着“性能”这个词。因此，通过把对应的知识点，变成一个个性能对比的示例代码程序记录下来，是把这些知识点融汇贯通的好方法。因为，相比于强记硬背知识点，一个有着明确性能对比的示例程序，会在你脑海里留下更深刻的印象。当你想要回顾这些知识点的时候，一个程序也更容易提示你把它从脑海深处里面找出来。
最后，通过和计算机硬件发展的历史做对照。计算机的发展并不是一蹴而就的。从第一台电子计算机ENIAC（Electronic Numerical Integrator And Computer，电子数值积分计算机）的发明到现在，已经有70多年了。现代计算机用的各个技术，都是跟随实际应用中遇到的挑战，一个个发明、打磨，最后保留下来的。这当中不仅仅有学术层面的碰撞，更有大量商业层面的交锋。通过了解充满戏剧性和故事性的计算机硬件发展史，让你更容易理解计算机组成中各种原理的由来。
比如说，奔腾4和SPARC的失败，以及ARM的成功，能让我们记住CPU指令集的繁与简、权衡性能和功耗的重要性，而现今高速发展的机器学习和边缘计算，又给计算机硬件设计带来了新的挑战。
给松鼠症患者的学习资料学习总是要花点笨功夫的。最有效的办法还是“读书百遍，其义自见”。对于不够明白的知识点，多搜索，多看不同来源的资料，多和朋友、同事、老师一起交流，一定能够帮你掌握好想要学习的知识点。
在这个专栏之前，计算机组成原理，已经有很多优秀的图书和课程珠玉在前了。为了覆盖更多知识点的细节，这些书通常都有点厚，课程都会有点长。不过作为专栏的补充阅读材料，却是最合适不过了。
因此，每一讲里，我都会留下一些“补充阅读”的材料。如果你想更进一步理解更多深入的计算机组成原理的知识，乃至更多相关的其他核心课程的知识，多用一些业余时间来看一看，读一读这些“补充阅读”也一定不会让你对花在上面的时间后悔的。
下面给你推荐一些我自己看过、读过的内容。我在之后的文章里推荐的“补充阅读”，大部分都是来自这些资料。你可以根据自己的情况来选择学习。
入门书籍我知道，订阅这个专栏的同学，有很多是非计算机科班出身，我建议你先对计算机组成原理这门课有个基本概念。建立这个概念，有两种方法，第一，你可以把我上面那张地图的核心内容记下来，对这些内容之间的关系先有个大致的了解。
第二，我推荐你阅读两本书，准确地说，这其实是两本小册子，因为它们非常轻薄、好读，而且图文并茂，非常适合初学者和想要入门组成原理的同学。一本是《计算机是怎样跑起来的》，另一本是《程序是怎样跑起来的》。我要特别说一下后面这本，它可以说是一个入门微缩版本的“计算机组成原理”。
除此之外，计算机组成中，硬件层面的基础实现，比如寄存器、ALU这些电路是怎么回事，你可以去看一看Coursera上的北京大学免费公开课《Computer Organization》。这个视频课程的视频部分也就10多个小时。在学习专栏相应章节的前后去浏览一遍，相信对你了解程序在电路层面会变成什么样子有所帮助。
深入学习书籍对于想要深入掌握计算机组成的同学，我推荐你去读一读《计算机组成与设计：硬件/软件接口》和经典的《深入理解计算机系统》这两本书。后面这本被称为CSAPP的经典教材，网上也有配套的视频课程。我在这里给你推荐两个不同版本的链接（Bilibili版和Youtube版 ）。不过这两本都在500页以上，坚持啃下来需要不少实践经验。
计算机组成原理还有一本的经典教材，就是来自操作系统大神塔能鲍姆（Andrew S. Tanenbaum）的《计算机组成：结构化方法》。这本书的组织结构和其他教材都不太一样，适合作为一个辅助的参考书来使用。
如果在学习这个专栏的过程中，引发了你对于计算机体系结构的兴趣，你还可以深入读一读《计算机体系结构：量化研究方法》。
课外阅读在上面这些教材之外，对于资深程序员来说，来自Redhat的What Every Programmer Should Know About Memory是写出高性能程序不可不读的经典材料。而LMAX开源的Disruptor，则是通过实际应用程序，来理解计算机组成原理中各个知识点的最好范例了。
《编码：隐匿在计算机软硬件背后的语言》和《程序员的自我修养：链接、装载和库》是理解计算机硬件和操作系统层面代码执行的优秀阅读材料。
总结延伸学习不是死记硬背，学习材料也不是越多越好。到了这里，希望你不要因为我给出了太多可以学习的材料，结果成了“松鼠症”患者，光囤积材料，却没有花足够多的时间去学习这些知识。
我工作之后一直在持续学习，在这个过程中，我发现最有效的办法，不是短时间冲刺，而是有节奏地坚持，希望你能够和专栏的发布节奏同步推进，做好思考题，并且多在留言区和其他朋友一起交流，就更容易能够“积小步而至千里”，在程序员这个职业上有更长足的发展。
好了，对于学习资料的介绍就到这里了。希望在接下来的几个月里，你能和我一起走完这趟“计算机组成”之旅，从中收获到知识和成长。
课后思考今天我为你梳理计算机组成的知识地图，也讲了我认为学习这个专栏的一些方法，听了这么多，那么你打算怎么学习这个专栏呢？
欢迎你在留言区写下你的学习目标和学习计划，和大家一起交流，也欢迎你把今天的文章分享给你的朋友，互相督促，共同成长。</description></item><item><title>02_词法分析：用两种方式构造有限自动机</title><link>https://artisanbox.github.io/7/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/2/</guid><description>你好，我是宫文学。
上一讲，我带你把整个编译过程走了一遍。这样，你就知道了编译过程的整体步骤，每一步是做什么的，以及为什么要这么做。
进一步地，你就可以研究一下每个环节具体是如何实现的、有哪些难点、有哪些理论和算法。通过这个过程，你不仅可以了解每个环节的原理，还能熟悉一些专有词汇。这样一来，你在读编译原理领域的相关资料时，就会更加顺畅了。
不过，编译过程中涉及的算法和原理有些枯燥，所以我会用尽量通俗、直观的方式来给你解读，让你更容易接受。
本讲，我主要跟你讨论一下词法分析（Lexical Analysis）这个环节。通过这节课，你可以掌握词法分析这个阶段是如何把字符串识别成一个个Token的。进而，你还会学到如何实现一个正则表达式工具，从而实现任意的词法解析。
词法分析的原理首先，我们来了解一下词法分析的原理。
通过上一讲，你已经很熟悉词法分析的任务了：输入的是字符串，输出的是Token串。所以，词法分析器在英文中一般叫做Tokenizer。
图1：把字符串转换为Token（注意：其中的空白字符，代表空格、tab、回车和换行符，EOF是文件结束符）但具体如何实现呢？这里要有一个计算模型，叫做有限自动机（Finite-state Automaton，FSA），或者叫做有限状态自动机（Finite-state Machine，FSM）。
有限自动机这个名字，听上去可能比较陌生。但大多数程序员，肯定都接触过另一个词：状态机。假设你要做一个电商系统，那么订单状态的迁移，就是一个状态机。
图2：状态机的例子（订单的状态和迁移过程）有限自动机就是这样的状态机，它的状态数量是有限的。当它收到一个新字符的时候，会导致状态的迁移。比如说，下面的这个状态机能够区分标识符和数字字面量。
图3：一个能够识别标识符和数字字面量的有限自动机在这样一个状态机里，我用单线圆圈表示临时状态，双线圆圈表示接受状态。接受状态就是一个合格的Token，比如图3中的状态1（数字字面量）和状态2（标识符）。当这两个状态遇到空白字符的时候，就可以记下一个Token，并回到初始态（状态0），开始识别其他Token。
可以看出，词法分析的过程，其实就是对一个字符串进行模式匹配的过程。说起字符串的模式匹配，你能想到什么工具吗？对的，正则表达式工具。
大多数语言，以及一些操作系统的命令，都带有正则表达式工具，来帮助你匹配合适的字符串。比如下面的这个Linux命令，可以用来匹配所有包含“sa”“sb” … “sh”字符串的进程。
ps -ef | grep 's[a-h]' 在这个命令里，“s[a-h]”是用来描述匹配规则的，我们把它叫做一个正则表达式。
同样地，正则表达式也可以用来描述词法规则。这种描述方法，我们叫做正则文法（Regular Grammar）。比如，数字字面量和标识符的正则文法描述是这样的：
IntLiteral : [0-9]+; //至少有一个数字 Id : [A-Za-z][A-Za-z0-9]*; //以字母开头，后面可以是字符或数字 与普通的正则表达式工具不同的是，词法分析器要用到很多个词法规则，每个词法规则都采用“Token类型: 正则表达式”这样一种格式，用于匹配一种Token。
然而，当我们采用了多条词法规则的时候，有可能会出现词法规则冲突的情况。比如说，int关键字其实也是符合标识符的词法规则的。
Int : int; //int关键字 For : for; //for关键字 Id : [A-Za-z][A-Za-z0-9]*; //以字母开头，后面可以是字符或数字 所以，词法规则里面要有优先级，比如排在前面的词法规则优先级更高。这样的话，我们就能够设计出区分int关键字和标识符的有限自动机了，可以画成下面的样子。其中，状态1、2和3都是标识符，而状态4则是int关键字。
图4：一个能够识别int关键字和标识符的有限自动机从正则表达式生成有限自动机现在，你已经了解了如何构造有限自动机，以及如何处理词法规则的冲突。基本上，你就可以按照上面的思路来手写词法分析器了。但你可能觉得，这样手写词法分析器的步骤太繁琐了，我们能否只写出词法规则，就自动生成相对应的有限自动机呢？
当然是可以的，实际上，正则表达式工具就是这么做的。此外，词法分析器生成工具lex（及GNU版本的flex）也能够基于规则自动生成词法分析器。
它的具体实现思路是这样的：把一个正则表达式翻译成NFA，然后把NFA转换成DFA。对不起，我这里又引入了两个新的术语：NFA和DFA。
先说说DFA，它是“Deterministic Finite Automaton”的缩写，即确定的有限自动机。它的特点是：该状态机在任何一个状态，基于输入的字符，都能做一个确定的状态转换。前面例子中的有限自动机，都属于DFA。
再说说NFA，它是“Nondeterministic Finite Automaton”的缩写，即不确定的有限自动机。它的特点是：该状态机中存在某些状态，针对某些输入，不能做一个确定的转换。
这又细分成两种情况：
对于一个输入，它有两个状态可以转换。 存在ε转换的情况，也就是没有任何字符输入的情况下，NFA也可以从一个状态迁移到另一个状态。 比如，“a[a-zA-Z0-9]*bc”这个正则表达式，对字符串的要求是以a开头，以bc结尾，a和bc之间可以有任意多个字母或数字。可以看到，在图5中，状态1的节点输入b时，这个状态是有两条路径可以选择的：一条是迁移到状态2，另一条是仍然保持在状态1。所以，这个有限自动机是一个NFA。
图5：一个NFA的例子，识别“a[a-zA-Z0-9]*bc”的自动机这个NFA还有引入ε转换的画法，如图6所示，它跟图5的画法是等价的。实际上，图6表示的NFA可以用我们下面马上要讲到的算法，通过正则表达式自动生成出来。
图6：另一个NFA的例子，同样能识别“a[a-zA-Z0-9]*bc”，其中有ε转换需要注意的是，无论是NFA还是DFA，都等价于正则表达式。也就是说，所有的正则表达式都能转换成NFA或DFA；而所有的NFA或DFA，也都能转换成正则表达式。
理解了NFA和DFA以后，接下来我再大致说一下算法。
首先，一个正则表达式可以机械地翻译成一个NFA。它的翻译方法如下：
识别字符i的NFA。 当接受字符i的时候，引发一个转换，状态图的边上标注i。其中，第一个状态（i，initial）是初始状态，第二个状态(f，final)是接受状态。</description></item><item><title>02｜词法分析：识别Token也可以很简单吗？</title><link>https://artisanbox.github.io/3/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/4/</guid><description>你好，我是宫文学。
上一节课，我们用了很简单的方法就实现了语法分析。但当时，我们省略了词法分析的任务，使用了一个Token的列表作为语法分析阶段的输入，而这个Token列表呢，就是词法分析的结果。
其实，编译器的第一项工作就是词法分析，也是你实现一门计算机语言的头一项基本功。今天，我们就来补补课，学习一下怎么实现词法分析功能，词法分析也就是把程序从字符串转换成Token串的过程。
词法分析难不难呢？我们来对比一下，语法分析的结果是一棵AST树，而词法分析的结果是一个列表。直观上看，列表就要比树结构简单一些，所以你大概会猜想到，词法分析应该会更简单一些。
那么，具体来说，词法分析要用什么算法呢？词法是不是也像语法一样有规则？词法规则又是如何表达的？这一节课，我会带着你实现一个词法分析器，来帮你掌握这些技能。
在这里，我有个好消息告诉你。你在上一节课学到的语法分析的技能，很多可以用在词法分析中，这会大大降低你的学习难度。好了，我们开始了。
词法分析的任务你已经知道，词法分析的任务就是把程序从字符串转变成Token串，那它该怎么实现呢？我们这里先不讲具体的算法，先来看看下面这张示意图，分析一下，我们人类的大脑是如何把这个字符串断成一个个Token的？
你可能首先会想到，借助字符串中的空白字符（包括空格、回车、换行），把这个字符串截成一段段的，每一段作为一个Token，行不行？
按照这个方法，function关键字可以被单独识别出来。但是你看，我们还有一些圆括号、花括号等等，这些符号跟前一个单词之间并没有空格或回车，我们怎么把它们断开呢？
OK，你可以说，凡是遇到圆括号、花括号、加号、减号、点号等这些符号，我们把它们单独作为Token识别出来就好了。比如，对于cat.weight这样的对象属性访问的场景，点符号就是一个单独的Token。
但是，你马上会发现这个规则仍然不能处理所有的情况，例如，对于一个浮点数的字面量“3.14”的情况，点符号是浮点数的一部分，不能作为单独的Token。我稍微解释一下，这里的字面量（Literal），是指写在程序中的常量值，包括整数值、浮点数值、字符串等。
此外，还有一些难处理的情况，比如像“==”、“+=”、“-=”、“- -”、“&amp;amp;&amp;amp;”这些由两个或两个以上字符构成的运算符，程序处理时是要跟“=”、“+”、“-”等区分开的。
再比如，在JavaScript/TypeScript中，十六进制的字面量用“0x”开头，里面有a到f的字母，比如0x1F4；八进制的字面量用“0”开头，后面跟0~7的数字；而二进制的字面量用“0b”开头，后面跟着0或1。
所以，你可以看到，做词法分析需要考虑的情况还挺多，不是用简单的一两个规则就能解决的，我们必须寻找一种系统性的解决方法。
在这里，为了让你对词法分析的任务有更全面的了解，我梳理了各种不同的处理工作，你可以看看下面这张表：
那么，如何用系统性的方法进行词法分析呢？
借助这节课一开头的提示，我们试一下能否用语法分析的方法来处理词法，也就是说像做语法分析一样，我们要先用一个规则来描述每个Token的词法，然后让程序基于词法规则来做处理。
词法规则同语法规则一样，我们可以用正则表达式来描述词法。在这里，标识符的规则是用字母开头，后面的字符可以是字母、数字或下划线，标识符的词法规则可以写成下面这样：
Identifier: [a-zA-Z_][a-zA-Z0-9_]* ; 实际上，JavaScript的标识符是允许使用合法的Unicode字符的，我们这里做了简化。
看上去，词法规则跟上一节学过的语法规则没什么不同嘛，只不过词法的构成要素是字符，而语法的构成要素是Token。
表面上是这样，其实这里还是有一点不同的。实际上，词法规则使用的是正则文法（Formal Grammar），而语法规则使用的是上下文无关文法（Context-free Gammar，CFG）。正则文法是上下文无关文法的一个子集，至于对这两者差别的深入分析，我们还是放到后面的课上，这里我们先专注于完成词法分析功能。
我们再写一下前面讨论过的浮点数字面量的词法规则，这里同样是精简的版本，省略了指数部分、二进制、八进制以及十六进制的内容：
DecimalLiteral: IntegerLiteral '.' [0-9]* | '.' [0-9]+ | IntegerLiteral ; IntegerLiteral: '0' | [1-9] [0-9]* ; 对于上面这个DecimalLiteral词法规则，我们总结一下有这几个特点：
一个合法的浮点数可以是好几种格式，3.14、.14、3等等都行； 整数部分要么只有一个0，要么是1~9开头的数字； 可以没有小数点前的整数部分，但这时候小数点后至少要有一位数字，否则这就只剩了一个点号了； 也可以完全没有小数部分，只有整数部分。 好了，目前我们已经知道如何用词法规则来描述不同的Token了。接下来，我们要做的就是用程序实现这些词法规则，然后生成Token。
用程序实现词法分析上节课我们在讲语法分析的时候，提到了递归下降算法。这个算法比较让人喜欢的一点是，程序结构基本上就是对语法规则的一对一翻译。
其实词法分析程序也是一样的，比如我们要识别一个浮点数，我们照样可以根据上述DecimalLiteral的几条规则一条条地匹配过去。
首先，我们匹配第一条规则，就是既有整数部分又有小数部分的情况；如果匹配不上，就尝试第二条规则，也就是以小数点开头的情况；如果还匹配不上，就尝试第三条，即只有整数部分的情况。只要这三条匹配里有一条成功，就意味着我们匹配浮点数成功。
我们来看看具体的程序实现：
if (this.isDigit(ch)){ this.stream.next(); let ch1 = this.stream.peek(); let literal:string = ''; //首先解析整数部分 if(ch == '0'){//暂不支持八进制、二进制、十六进制 if (!</description></item><item><title>03_事务隔离：为什么你改了我还看不见？</title><link>https://artisanbox.github.io/1/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/3/</guid><description>提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转账，你要给朋友小王转100块钱，而此时你的银行卡只有100块钱。
转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这100块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。
简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事务支持是在引擎层实现的。你现在知道，MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如MySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代的重要原因之一。
今天的文章里，我将会以InnoDB为例，剖析MySQL在事务支持方面的特定实现，并基于原理给出相应的实践建议，希望这些案例能加深你对MySQL事务原理的理解。
隔离性与隔离级别提到事务，你肯定会想到ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中I，也就是“隔离性”。
当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。
在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面我逐一为你解释：
读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表T中只有一列，其中一行的值为1，下面是按照时间顺序执行两个事务的行为。
mysql&amp;gt; create table T(c int) engine=InnoDB; insert into T(c) values(1); 我们来看看在不同的隔离级别下，事务A会有哪些不同的返回结果，也就是图里面V1、V2、V3的返回值分别是什么。
若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。
我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致，你一定要记得将MySQL的隔离级别设置为“读提交”。
配置的方式是，将启动参数transaction-isolation的值设置成READ-COMMITTED。你可以用show variables来查看当前的值。
mysql&amp;gt; show variables like 'transaction_isolation'; +-----------------------+----------------+ | Variable_name | Value | +-----------------------+----------------+ | transaction_isolation | READ-COMMITTED | +-----------------------+----------------+ 总结来说，存在即合理，每种隔离级别都有自己的使用场景，你要根据自己的业务情况来定。我想你可能会问那什么时候需要“可重复读”的场景呢？我们来看一个数据校对逻辑的案例。
假设你在管理一个个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。
这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。
事务隔离的实现理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复读”。
在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。
假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。
当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-view A，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。</description></item><item><title>03_复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？</title><link>https://artisanbox.github.io/2/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/4/</guid><description>我们都知道，数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行得更快，如何让代码更省存储空间。所以，执行效率是算法一个非常重要的考量指标。那如何来衡量你编写的算法代码的执行效率呢？这里就要用到我们今天要讲的内容：时间、空间复杂度分析。
其实，只要讲到数据结构与算法，就一定离不开时间、空间复杂度分析。而且，我个人认为，复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。
复杂度分析实在太重要了，因此我准备用两节内容来讲。希望你学完这个内容之后，无论在任何场景下，面对任何代码的复杂度分析，你都能做到“庖丁解牛”般游刃有余。
为什么需要复杂度分析？你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？
首先，我可以肯定地说，你这种评估算法执行效率的方法是正确的。很多数据结构和算法书籍还给这种方法起了一个名字，叫事后统计法。但是，这种统计方法有非常大的局限性。
1. 测试结果非常依赖测试环境
测试环境中硬件的不同会对测试结果有很大的影响。比如，我们拿同样一段代码，分别用Intel Core i9处理器和Intel Core i3处理器来运行，不用说，i9处理器要比i3处理器执行的速度快很多。还有，比如原本在这台机器上a代码执行的速度比b代码要快，等我们换到另一台机器上时，可能会有截然相反的结果。
2.测试结果受数据规模的影响很大
后面我们会讲排序算法，我们先拿它举个例子。对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别。极端情况下，如果数据已经是有序的，那排序算法不需要做任何操作，执行时间就会非常短。除此之外，如果测试数据规模太小，测试结果可能无法真实地反映算法的性能。比如，对于小规模的数据排序，插入排序可能反倒会比快速排序要快！
所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是我们今天要讲的时间、空间复杂度分析方法。
大O复杂度表示法算法的执行效率，粗略地讲，就是算法代码执行的时间。但是，如何在不运行代码的情况下，用“肉眼”得到一段代码的执行时间呢？
这里有段非常简单的代码，求1,2,3...n的累加和。现在，我就带你一块来估算一下这段代码的执行时间。
int cal(int n) { int sum = 0; int i = 1; for (; i &amp;lt;= n; ++i) { sum = sum + i; } return sum; } 从CPU的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的CPU执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？
第2、3行代码分别需要1个unit_time的执行时间，第4、5行都运行了n遍，所以需要2n*unit_time的执行时间，所以这段代码总的执行时间就是(2n+2)*unit_time。可以看出来，所有代码的执行时间T(n)与每行代码的执行次数成正比。
按照这个分析思路，我们再来看这段代码。
int cal(int n) { int sum = 0; int i = 1; int j = 1; for (; i &amp;lt;= n; ++i) { j = 1; for (; j &amp;lt;= n; ++j) { sum = sum + i * j; } } } 我们依旧假设每个语句的执行时间是unit_time。那这段代码的总执行时间T(n)是多少呢？</description></item><item><title>03_表：怎么创建和修改数据表？</title><link>https://artisanbox.github.io/8/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/3/</guid><description>你好，我是朱晓峰。今天，我们来聊一聊怎么创建和修改数据表。
创建和修改数据表，是数据存储过程中的重要一环。我们不仅需要把表创建出来，还需要正确地设置限定条件，这样才能确保数据的一致性和完整性。同时，表中的数据会随着业务需求的变化而变化，添加和修改相应的字段也是常见的操作。这节课，我们就来学习下具体的方法。
在我们的超市项目里，客户经常需要进货，这就需要在MySQL数据库里面创建一个表，来管理进货相关的数据。我们先看看这个表里有什么内容。
假设这个表叫做进货单头表（importhead），如下图所示：
这里的1、2、3表示门店的3种进货方式，分别是配送中心配送、门店采买和供货商直供。
其中，“1（配送中心配送）”是标准进货方式。因为超市是连锁经营，为了确保商品质量和品类一致，超过9成的门店进货，是通过配送中心进行配送的。因此，我们希望这个字段的值能够默认是1，这样一来，除非有特别的指定，否则，门店进货单的进货方式，就自动设置成“1”了。
现在，客户需要一个类似的表来存储进货数据，而且进货方式还有3个可能的取值范围，需要设置默认值，那么，应该怎么创建这个表呢？另外，创建好表以后，又该怎么进行修改呢？
如何创建数据表？首先，我们要知道MySQL创建表的语法结构：
CREATE TABLE &amp;lt;表名&amp;gt; ( 字段名1 数据类型 [字段级别约束] [默认值]， 字段名2 数据类型 [字段级别约束] [默认值]， ...... [表级别约束] ); 在这里，我们通过定义表名、表中的字段、表的属性等，把一张表创建出来。
你可能注意到了，在MySQL创建表的语法结构里面，有一个词叫做“约束”。“约束”限定了表中数据应该满足的条件。MySQL会根据这些限定条件，对表的操作进行监控，阻止破坏约束条件的操作执行，并提示错误，从而确保表中数据的唯一性、合法性和完整性。这是创建表时不可缺少的一部分。
下面我来带你创建刚刚提到的进货单表。需要注意的是，这里我们需要定义默认值，也就是要定义默认值约束，除此之外，还有很多种约束，一会儿我再细讲。
我们先来看基本的数据表创建流程，创建代码如下：
CREATE TABLE demo.importhead ( listnumber INT, supplierid INT, stocknumber INT, --我们在字段importype定义为INT类型的后面，按照MySQL创建表的语法，加了默认值1。 importtype INT DEFAULT 1, quantity DECIMAL(10,3), importvalue DECIMAL(10,2), recorder INT, recordingdate DATETIME ); 运行这个SQL语句，表demo.importhead就按照我们的要求被创建出来了。
在创建表的时候，字段名称要避开MySQL的系统关键字，原因是MySQL系统保留的关键字都有特定的含义，如果作为字段名称出现在SQL语句中，MySQL会把这个字段名称理解为系统关键字，从而导致SQL语句无法正常运行。比如，刚刚我们把进货金额设置为“importvalue”，而不是“value”，就是因为，“value”是MySQL的系统关键字。
好了，现在我们尝试往刚刚创建的表里插入一条记录，来验证一下对字段“importtype”定义的默认值约束是否起了作用。
INSERT INTO demo.importhead ( listnumber, supplierid, stocknumber, -- 这里我们没有插入字段importtype的值 quantity, importvalue, recorder, recordingdate ) VALUES ( 3456, 1, 1, 10, 100, 1, '2020-12-10' ); 插入完成后，我们来查询一下表的内容：</description></item><item><title>03_语法分析（一）：纯手工打造公式计算器</title><link>https://artisanbox.github.io/6/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/3/</guid><description>我想你应该知道，公式是Excel电子表格软件的灵魂和核心。除此之外，在HR软件中，可以用公式自定义工资。而且，如果你要开发一款通用报表软件，也会大量用到自定义公式来计算报表上显示的数据。总而言之，很多高级一点儿的软件，都会用到自定义公式功能。
既然公式功能如此常见和重要，我们不妨实现一个公式计算器，给自己的软件添加自定义公式功能吧！
本节课将继续“手工打造”之旅，让你纯手工实现一个公式计算器，借此掌握语法分析的原理和递归下降算法（Recursive Descent Parsing），并初步了解上下文无关文法（Context-free Grammar，CFG）。
我所举例的公式计算器支持加减乘除算术运算，比如支持“2 + 3 * 5”的运算。
在学习语法分析时，我们习惯把上面的公式称为表达式。这个表达式看上去很简单，但你能借此学到很多语法分析的原理，例如左递归、优先级和结合性等问题。
当然了，要实现上面的表达式，你必须能分析它的语法。不过在此之前，我想先带你解析一下变量声明语句的语法，以便让你循序渐进地掌握语法分析。
解析变量声明语句：理解“下降”的含义在“01 | 理解代码：编译器的前端技术”里，我提到语法分析的结果是生成AST。算法分为自顶向下和自底向上算法，其中，递归下降算法是一种常见的自顶向下算法。
与此同时，我给出了一个简单的代码示例，也针对“int age = 45”这个语句，画了一个语法分析算法的示意图：
我们首先把变量声明语句的规则，用形式化的方法表达一下。它的左边是一个非终结符（Non-terminal）。右边是它的产生式（Production Rule）。在语法解析的过程中，左边会被右边替代。如果替代之后还有非终结符，那么继续这个替代过程，直到最后全部都是终结符（Terminal），也就是Token。只有终结符才可以成为AST的叶子节点。这个过程，也叫做推导（Derivation）过程：
intDeclaration : Int Identifier ('=' additiveExpression)?; 你可以看到，int类型变量的声明，需要有一个Int型的Token，加一个变量标识符，后面跟一个可选的赋值表达式。我们把上面的文法翻译成程序语句，伪代码如下：
//伪代码 MatchIntDeclare(){ MatchToken(Int)； //匹配Int关键字 MatchIdentifier(); //匹配标识符 MatchToken(equal); //匹配等号 MatchExpression(); //匹配表达式 } 实际代码在SimpleCalculator.java类的IntDeclare()方法中：
SimpleASTNode node = null; Token token = tokens.peek(); //预读 if (token != null &amp;amp;&amp;amp; token.getType() == TokenType.Int) { //匹配Int token = tokens.read(); //消耗掉int if (tokens.peek().getType() == TokenType.Identifier) { //匹配标识符 token = tokens.</description></item><item><title>03_语法分析：两个基本功和两种算法思路</title><link>https://artisanbox.github.io/7/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/3/</guid><description>你好，我是宫文学。
通过第1讲的学习，现在你已经清楚了语法分析阶段的任务：依据语法规则，把Token串转化成AST。
今天，我就带你来掌握语法分析阶段的核心知识点，也就是两个基本功和两种算法思路。理解了这些重要的知识点，对于语法分析，你就不是外行了。
两个基本功：第一，必须能够阅读和书写语法规则，也就是掌握上下文无关文法；第二，必须要掌握递归下降算法。 两种算法思路：一种是自顶向下的语法分析，另一种则是自底向上的语法分析。 上下文无关文法（Context-Free Grammar）在开始语法分析之前，我们要解决的第一个问题，就是如何表达语法规则。在上一讲中，你已经了解了，我们可以用正则表达式来表达词法规则，语法规则其实也差不多。
我还是以下面这个示例程序为例，里面用到了变量声明语句、加法表达式，我们看看语法规则应该怎么写：
int a = 2; int b = a + 3; return b; 第一种写法是下面这个样子，它看起来跟上一讲的词法规则差不多，都是左边是规则名称，右边是正则表达式。
start：blockStmts ; //起始 block : '{' blockStmts '}' ; //语句块 blockStmts : stmt* ; //语句块中的语句 stmt = varDecl | expStmt | returnStmt | block; //语句 varDecl : type Id varInitializer？ ';' ; //变量声明 type : Int | Long ; //类型 varInitializer : '=' exp ; //变量初始化 expStmt : exp ';' ; //表达式语句 returnStmt : Return exp ';' ; //return语句 exp : add ; //表达式 add : add '+' mul | mul; //加法表达式 mul : mul '*' pri | pri; //乘法表达式 pri : IntLiteral | Id | '(' exp ')' ; //基础表达式 在语法规则里，我们把冒号左边的叫做非终结符（Non-terminal），又叫变元（Variable）。非终结符可以按照右边的正则表达式来逐步展开，直到最后都变成标识符、字面量、运算符这些不可再展开的符号，也就是终结符（Terminal）。终结符其实也是词法分析过程中形成的Token。</description></item><item><title>03_通过你的CPU主频，我们来谈谈“性能”究竟是什么？</title><link>https://artisanbox.github.io/4/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/3/</guid><description>“性能”这个词，不管是在日常生活还是写程序的时候，都经常被提到。比方说，买新电脑的时候，我们会说“原来的电脑性能跟不上了”；写程序的时候，我们会说，“这个程序性能需要优化一下”。那么，你有没有想过，我们常常挂在嘴边的“性能”到底指的是什么呢？我们能不能给性能下一个明确的定义，然后来进行准确的比较呢？
在计算机组成原理乃至体系结构中，“性能”都是最重要的一个主题。我在前面说过，学习和研究计算机组成原理，就是在理解计算机是怎么运作的，以及为什么要这么运作。“为什么”所要解决的事情，很多时候就是提升“性能”。
什么是性能？时间的倒数计算机的性能，其实和我们干体力劳动很像，好比是我们要搬东西。对于计算机的性能，我们需要有个标准来衡量。这个标准中主要有两个指标。
第一个是响应时间（Response time）或者叫执行时间（Execution time）。想要提升响应时间这个性能指标，你可以理解为让计算机“跑得更快”。
第二个是吞吐率（Throughput）或者带宽（Bandwidth），想要提升这个指标，你可以理解为让计算机“搬得更多”。
服务器使用的网络带宽，通常就是一个吞吐率性能指标所以说，响应时间指的就是，我们执行一个程序，到底需要花多少时间。花的时间越少，自然性能就越好。
而吞吐率是指我们在一定的时间范围内，到底能处理多少事情。这里的“事情”，在计算机里就是处理的数据或者执行的程序指令。
和搬东西来做对比，如果我们的响应时间短，跑得快，我们可以来回多跑几趟多搬几趟。所以说，缩短程序的响应时间，一般来说都会提升吞吐率。
除了缩短响应时间，我们还有别的方法吗？当然有，比如说，我们还可以多找几个人一起来搬，这就类似现代的服务器都是8核、16核的。人多力量大，同时处理数据，在单位时间内就可以处理更多数据，吞吐率自然也就上去了。
提升吞吐率的办法有很多。大部分时候，我们只要多加一些机器，多堆一些硬件就好了。但是响应时间的提升却没有那么容易，因为CPU的性能提升其实在10年前就处于“挤牙膏”的状态了，所以我们得慎重地来分析对待。下面我们具体来看。
我们一般把性能，定义成响应时间的倒数，也就是：
性能 = 1/响应时间这样一来，响应时间越短，性能的数值就越大。同样一个程序，在Intel最新的CPU Coffee Lake上，只需要30s就能运行完成，而在5年前CPU Sandy Bridge上，需要1min才能完成。那么我们自然可以算出来，Coffee Lake的性能是1/30，Sandy Bridge的性能是1/60，两个的性能比为2。于是，我们就可以说，Coffee Lake的性能是Sandy Bridge的2倍。
过去几年流行的手机跑分软件，就是把多个预设好的程序在手机上运行，然后根据运行需要的时间，算出一个分数来给出手机的性能评估。而在业界，各大CPU和服务器厂商组织了一个叫作SPEC（Standard Performance Evaluation Corporation）的第三方机构，专门用来指定各种“跑分”的规则。
SPEC提供的CPU基准测试程序，就好像CPU届的“高考”，通过数十个不同的计算程序，对于CPU的性能给出一个最终评分。这些程序丰富多彩，有编译器、解释器、视频压缩、人工智能国际象棋等等，涵盖了方方面面的应用场景。感兴趣的话，你可以点击这个链接看看。
计算机的计时单位：CPU时钟虽然时间是一个很自然的用来衡量性能的指标，但是用时间来衡量时，有两个问题。
第一个就是时间不“准”。如果用你自己随便写的一个程序，来统计程序运行的时间，每一次统计结果不会完全一样。有可能这一次花了45ms，下一次变成了53ms。
为什么会不准呢？这里面有好几个原因。首先，我们统计时间是用类似于“掐秒表”一样，记录程序运行结束的时间减去程序开始运行的时间。这个时间也叫Wall Clock Time或者Elapsed Time，就是在运行程序期间，挂在墙上的钟走掉的时间。
但是，计算机可能同时运行着好多个程序，CPU实际上不停地在各个程序之间进行切换。在这些走掉的时间里面，很可能CPU切换去运行别的程序了。而且，有些程序在运行的时候，可能要从网络、硬盘去读取数据，要等网络和硬盘把数据读出来，给到内存和CPU。所以说，要想准确统计某个程序运行时间，进而去比较两个程序的实际性能，我们得把这些时间给刨除掉。
那这件事怎么实现呢？Linux下有一个叫time的命令，可以帮我们统计出来，同样的Wall Clock Time下，程序实际在CPU上到底花了多少时间。
我们简单运行一下time命令。它会返回三个值，第一个是real time，也就是我们说的Wall Clock Time，也就是运行程序整个过程中流逝掉的时间；第二个是user time，也就是CPU在运行你的程序，在用户态运行指令的时间；第三个是sys time，是CPU在运行你的程序，在操作系统内核里运行指令的时间。而程序实际花费的CPU执行时间（CPU Time），就是user time加上sys time。
$ time seq 1000000 | wc -l 1000000 real 0m0.101s user 0m0.031s sys 0m0.016s 在我给的这个例子里，你可以看到，实际上程序用了0.101s，但是CPU time只有0.031+0.016 = 0.047s。运行程序的时间里，只有不到一半是实际花在这个程序上的。
备注：你最好在云平台上，找一台1 CPU的机器来跑这个命令，在多CPU的机器上，seq和wc两个命令可能分配到不同的CPU上，我们拿到的user time和sys time是两个CPU上花费的时间之和，可能会导致real time可能会小于user time+sys time。</description></item><item><title>03_黑盒之中有什么：内核结构与设计</title><link>https://artisanbox.github.io/9/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/3/</guid><description>你好，我是LMOS。
在上节课中，我们写了一个极简的操作系统——Hello OS，并成功运行，直观地感受了一下自己控制计算机的乐趣，或许你正沉浸在这种乐趣之中，但我不得不提醒你赶快从这种快乐中走出来。
因为我们的Hello OS虽然能使计算机运行起来，但其实没有任何实际的功能。
什么？没有实际功能，我们往里增加功能不就好了吗？
你可能会这样想，但是这样想就草率了，开发操作系统内核（以下简称内核）就像建房子一样，房子要建得好，就先要设计。比如用什么结构，什么材料，房间怎么布局，电路、水路等，最后画出设计图纸，依据图纸按部就班地进行建造。
而一个内核的复杂程度要比房子的复杂程度高出几个数量级，所以在开发内核之前先要对其进行设计。
下面我们就先搞清楚内核之中有些什么东西，然后探讨一下怎么组织它们、用什么架构来组织、并对比成熟的架构，最后设计出我们想要的内核架构。
黑盒之中有什么从用户和应用程序的角度来看，内核之中有什么并不重要，能提供什么服务才是重要的，所以内核在用户和上层应用眼里，就像一个大黑盒，至于黑盒里面有什么，怎么实现的，就不用管了。
不过，作为内核这个黑盒的开发者，我们要实现它，就必先设计它，而要设计它，就必先搞清楚内核中有什么。
从抽象角度来看，内核就是计算机资源的管理者，当然管理资源是为了让应用使用资源。既然内核是资源的管理者，我们先来看看计算机中有哪些资源，然后通过资源的归纳，就能推导出内核这个大黑盒中应该有什么。
计算机中资源大致可以分为两类资源，一种是硬件资源，一种是软件资源。先来看看硬件资源有哪些，如下：
1.总线，负责连接各种其它设备，是其它设备工作的基础。
2.CPU，即中央处理器，负责执行程序和处理数据运算。
3.内存，负责储存运行时的代码和数据。
4.硬盘，负责长久储存用户文件数据。
5.网卡，负责计算机与计算机之间的通信。
6.显卡，负责显示工作。
7.各种I/O设备，如显示器，打印机，键盘，鼠标等。
下面给出一幅经典的计算机内部结构图，如下：
而计算机中的软件资源，则可表示为计算机中的各种形式的数据。如各种文件、软件程序等。
内核作为硬件资源和软件资源的管理者，其内部组成在逻辑上大致如下：
1.管理CPU，由于CPU是执行程序的，而内核把运行时的程序抽象成进程，所以又称为进程管理。
2.管理内存，由于程序和数据都要占用内存，内存是非常宝贵的资源，所以内核要非常小心地分配、释放内存。
3.管理硬盘，而硬盘主要存放用户数据，而内核把用户数据抽象成文件，即管理文件，文件需要合理地组织，方便用户查找和读写，所以形成了文件系统。
4.管理显卡，负责显示信息，而现在操作系统都是支持GUI（图形用户接口）的，管理显卡自然而然地就成了内核中的图形系统。
5.管理网卡，网卡主要完成网络通信，网络通信需要各种通信协议，最后在内核中就形成了网络协议栈，又称网络组件。
6.管理各种I/O设备，我们经常把键盘、鼠标、打印机、显示器等统称为I/O（输入输出）设备，在内核中抽象成I/O管理器。
内核除了这些必要组件之外，根据功能不同还有安全组件等，最值得一提的是，各种计算机硬件的性能不同，硬件型号不同，硬件种类不同，硬件厂商不同，内核要想管理和控制这些硬件就要编写对应的代码，通常这样的代码我们称之为驱动程序。
硬件厂商就可以根据自己不同的硬件编写不同的驱动，加入到内核之中。
以上我们已经大致知道了内核之中有哪些组件，但是另一个问题又出现了，即如何组织这些组件，让系统更加稳定和高效，这就需要我们从现有的一些经典内核结构里找灵感了。
宏内核结构其实看这名字，就已经能猜到了，宏即大也，这种最简单适用，也是最早的一种内核结构。
宏内核就是把以上诸如管理进程的代码、管理内存的代码、管理各种I/O设备的代码、文件系统的代码、图形系统代码以及其它功能模块的代码，把这些所有的代码经过编译，最后链接在一起，形成一个大的可执行程序。
这个大程序里有实现支持这些功能的所有代码，向用户应用软件提供一些接口，这些接口就是常说的系统API函数。而这个大程序会在处理器的特权模式下运行，这个模式通常被称为宏内核模式。结构如下图所示。
尽管图中一层一层的，这并不是它们有层次关系，仅仅表示它们链接在一起。
为了理解宏内核的工作原理，我们来看一个例子，宏内核提供内存分配功能的服务过程，具体如下：
1.应用程序调用内存分配的API（应用程序接口）函数。
2.处理器切换到特权模式，开始运行内核代码。
3.内核里的内存管理代码按照特定的算法，分配一块内存。
4.把分配的内存块的首地址，返回给内存分配的API函数。
5.内存分配的API函数返回，处理器开始运行用户模式下的应用程序，应用程序就得到了一块内存的首地址，并且可以使用这块内存了。
上面这个过程和一个实际的操作系统中的运行过程，可能有差异，但大同小异。当然，系统API和应用程序之间可能还有库函数，也可能只是分配了一个虚拟地址空间，但是我们关注的只是这个过程。
上图的宏内核结构有明显的缺点，因为它没有模块化，没有扩展性、没有移植性，高度耦合在一起，一旦其中一个组件有漏洞，内核中所有的组件可能都会出问题。
开发一个新的功能也得重新编译、链接、安装内核。其实现在这种原始的宏内核结构已经没有人用了。这种宏内核唯一的优点是性能很好，因为在内核中，这些组件可以互相调用，性能极高。
为了方便我们了解不同内核架构间的优缺点，下面我们看一个和宏内核结构对应的反例。
微内核结构微内核架构正好与宏内核架构相反，它提倡内核功能尽可能少：仅仅只有进程调度、处理中断、内存空间映射、进程间通信等功能（目前不懂没事，这是属于管理进程和管理内存的功能模块，后面课程里还会专门探讨的）。
这样的内核是不能完成什么实际功能的，开发者们把实际的进程管理、内存管理、设备管理、文件管理等服务功能，做成一个个服务进程。和用户应用进程一样，只是它们很特殊，宏内核提供的功能，在微内核架构里由这些服务进程专门负责完成。
微内核定义了一种良好的进程间通信的机制——消息。应用程序要请求相关服务，就向微内核发送一条与此服务对应的消息，微内核再把这条消息转发给相关的服务进程，接着服务进程会完成相关的服务。服务进程的编程模型就是循环处理来自其它进程的消息，完成相关的服务功能。其结构如下所示：
为了理解微内核的工程原理，我们来看看微内核提供内存分配功能的服务过程，具体如下：
1.应用程序发送内存分配的消息，这个发送消息的函数是微内核提供的，相当于系统API，微内核的API（应用程序接口）相当少，极端情况下仅需要两个，一个接收消息的API和一个发送消息的API。
2.处理器切换到特权模式，开始运行内核代码。
3.微内核代码让当前进程停止运行，并根据消息包中的数据，确定消息发送给谁，分配内存的消息当然是发送给内存管理服务进程。
4.内存管理服务进程收到消息，分配一块内存。
5.内存管理服务进程，也会通过消息的形式返回分配内存块的地址给内核，然后继续等待下一条消息。
6.微内核把包含内存块地址的消息返回给发送内存分配消息的应用程序。
7.处理器开始运行用户模式下的应用程序，应用程序就得到了一块内存的首地址，并且可以使用这块内存了。
微内核的架构实现虽然不同，但是大致过程和上面一样。同样是分配内存，在微内核下拐了几个弯，一来一去的消息带来了非常大的开销，当然各个服务进程的切换开销也不小。这样系统性能就大打折扣。
但是微内核有很多优点，首先，系统结构相当清晰利于协作开发。其次，系统有良好的移植性，微内核代码量非常少，就算重写整个内核也不是难事。最后，微内核有相当好的伸缩性、扩展性，因为那些系统功能只是一个进程，可以随时拿掉一个服务进程以减少系统功能，或者增加几个服务进程以增强系统功能。
微内核的代表作有MACH、MINIX、L4系统，这些系统都是微内核，但是它们不是商业级的系统，商业级的系统不采用微内核主要还是因为性能差。
好了，粗略了解了宏内核和微内核两大系统内核架构的优、缺点，以后设计我们自己的系统内核时，心里也就有了底了，到时就可以扬长避短了，下面我们先学习一点其它的东西，即分离硬件相关性，为设计出我们自己的内核架构打下基础。
分离硬件的相关性我们会经常听说，Windows内核有什么HAL层、Linux内核有什么arch层。这些xx层就是Windows和Linux内核设计者，给他们的系统内核分的第一个层。
今天如此庞杂的计算机，其实也是一层一层地构建起来的，从硬件层到操作系统层再到应用软件层这样构建。分层的主要目的和好处在于屏蔽底层细节，使上层开发更加简单。
计算机领域的一个基本方法是增加一个抽象层，从而使得抽象层的上下两层独立地发展，所以在内核内部再分若干层也不足为怪。
分离硬件的相关性，就是要把操作硬件和处理硬件功能差异的代码抽离出来，形成一个独立的软件抽象层，对外提供相应的接口，方便上层开发。
为了让你更好理解，我们举进程管理中的一个模块实现细节的例子：进程调度模块。通过这个例子，来看看分层对系统内核的设计与开发有什么影响。
一般操作系统理论课程都会花大量篇幅去讲进程相关的概念，其实说到底，进程是操作系统开发者为了实现多任务而提出的，并让每个进程在CPU上运行一小段时间，这样就能实现多任务同时运行的假象。
当然，这种假象十分奏效。要实现这种假象，就要实现下面这两种机制：
1.进程调度，它的目的是要从众多进程中选择一个将要运行的进程，当然有各种选择的算法，例如，轮转算法、优先级算法等。</description></item><item><title>03｜支持表达式：解析表达式和解析语句有什么不同？</title><link>https://artisanbox.github.io/3/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/5/</guid><description>你好，我是宫文学。
到目前为止，我们已经学习了一些语法分析的算法。不过，我们主要是分析了如何来解析语句，比如函数声明、函数调用，没有把重点放在解析表达式上。
其实我是刻意为之的，故意把表达式的解析往后推迟一下。原因是表达式解析，特别是像“2+3*5”这样的看似特别简单的二元运算的表达式解析，涉及的语法分析技术反而是比较复杂的。所以，从循序渐进的角度来说，我们要把它们放在后面。
表达式的解析复杂在哪里呢？是这样，我们在解析二元表达式的时候，会遇到递归下降算法最大的短板，也就是不支持左递归的文法。如果遇到左递归的文法，会出现无限循环的情况。
在这一节里，我会给你分析这种左递归的困境，借此加深你对递归下降算法运算过程的理解。
同时，我也要给出避免左递归问题的方法。这里，我没有采用教科书上经常推荐的改写文法的方法，而是使用了业界实际编译器中更常用的算法：运算符优先级解析器（Operator-precedence parser）。JDK的Java编译器、V8的JaveScript编译器和Go语言的GC编译器，都毫无例外地采用了这个算法，所以这个算法非常值得我们掌握。
好了，那我们首先来了解一下用递归下降算法解析算术表达式会出现的这个左递归问题。
左递归问题我们先给出一种简化的加法表达式的语法规则：
add : add '+' IntLiteral | IntLiteral ; 对这个规则的解读是这样的：一个加法表达式，它要么是一个整型字面量，要么是另一个加法表达式再加上一个整型字面量。在这个规则下，2、2+3、2+3+4都是合格的加法表达式。
那如果用递归下降算法去解析2+3，我们会采用“add ‘+’ IntLiteral”的规则。而这个规则呢，又要求匹配出一个add来，从而算法又会递归地再次调用“add ‘+’ IntLiteral”规则，导致无限递归下去。
2+3是不是一个add表达式？ -&amp;gt;先匹配出一个add表达式来，再是+号，再是整型字面量 -&amp;gt;先匹配出一个add表达式来，再是+号，再是整型字面量 -&amp;gt;先匹配出一个add表达式来，再是+号，再是整型字面量 -&amp;gt;无限递归... 这就是著名的左递归问题，是递归下降算法或者LL算法都无法解决的。
你可能会问，如果把产生式的写法换一下，把add放在后面，不就会避免左递归了吗？
add : IntLiteral '+' add | IntLiteral ; 这个也是不行的，因为这样会导致运算的结合性出错。如果执意按照这个语法解析，解析2+3+4这个表达式所形成的AST会是右结合的：
你会看到，基于使用右递归文法生成的AST，实际是先计算3+4，再跟2相加。这违背了加法运算的结合性的规定。正确的运算顺序，应该是先计算2+3，然后再加上4，是左结合，对应的AST应该是右边的那个。这种结合性的错误，看上去对于加法影响不大，但如果换成减法或者除法，那计算结果就完全错误了。
好了，现在你已经理解了左递归问题了。那我们要如何解决这个问题呢？一个可行的解决方法就是改写文法，并且要在解析算法上做一些特殊的处理，你可以参考《编译原理之美》课程04讲。除了改写文法的方法以外，还有一些研究者提出了其他一些算法，也能解决左递归问题。
不过，针对二元表达式的解析，今天我要采用的是被实际编译器广泛采用的运算符优先级算法。
运算符优先级算法这是个怎么样的算法呢？我想先用简单的方式帮你理解运算符优先级算法的原理，然后再一步步深化。
在01讲介绍递归下降算法的时候，我提到，它对应的是人类的一种思维方式，也就是从顶向下逐步分解。但人类还有另一种思维方式：自底向上逐步归纳。而运算符优先级算法，对应的就是自底向上的一种思维方式。
首先我们来看2+3+4这个表达式，如果我们用自底向上归纳的思路做语法分析是怎么样一个思考过程呢？
第1步，首先看到2。你心里想，这里有一个整数了，那它是不是一个算术表达式的组成部分呀？是一个加法表达式的，还是乘法表达式的一部分呢？我们再往下看一看就知道。
第2步，看到一个+号。噢，你说，原来是一个加法表达式呀。这时候，我们知道，2肯定是要参与加法运算的，所以是加法的左子树。但加法后面可以跟很多东西的，比如另一个整数，或者是一个乘法表达式什么的，都有可能。那我们继续向下看。
第3步，看到整数3。奥，你心里想，原来是2+3呀。那我现在根据这三个Token是不是可以先凑出一棵AST的子树来呢？先等一等，我们现在还不知道3后面跟的是什么。
如果3后面跟的是+号或者-号，那没问题，3是先参与前面这个+号的计算，再把2+3的结果一起，去参与后面的计算的。
但如果3后面遇到的是 * 号呢？那么3就要先参与乘法运算，计算完的才参与前面的加法的。这两个不同的计算顺序，导致AST的结构是不一样的，而影响AST结构的，其实就是3前后的两个运算符的优先级。
第4步，看到第2个+号。这个时候，你心里知道了，原来3后面的运算符的优先级跟前面的是一样的呀，那么按照结合性的规定，应该先算前面的加法，再算后面的加法，所以3应该跟前面的2和+号一起凑成一个AST子树。并且，这棵子树会作为一个稳定的单元，参与后面的AST的构建。
第5步，看到整数4。现在的情况跟第3步是一样的，我们不知道4后面跟着的是什么。如果4后面跟着一个 * 号，那么4还要先参与后面的计算，然后再跟前面这一堆做加法。如果4后面也是一个加法运算符，那4就要先参与前面的计算，4在AST中的位置也就会变得确定。
第6步，再往下看，发现后面的Token既不是+号，也不是 * 号，而是EOF，也就是Token串的结尾。这样的话，整个AST就可以确定下来了。
好了，这是一个比较简单的算法运行的场景。你可以多读几遍，借此找找自底向上分析的直观感觉。
接下来，我们再换一个任务，分析一下2+3*5。你会发现，跟前一个例子相比，一直到第5步的时候，也就是读入了5以后，仍然没有形成一棵稳定的AST子树：
这是为什么呢？
因为根据5后面读入的Token的不同，形成的AST的结构会有很大的区别。这里我们展示3种情形：
情形1，第6个Token是+号：它的优先级不高于最后一个运算符 * 号，所以3 * 5这棵子树的结构就是确定的；进一步看，它也不高于第一个运算符的优先级，所以整个2+3 * 5这棵子树的结构都可以确定下来，并且肯定是最后一个+号的左子树。</description></item><item><title>04_增删改查：如何操作表中的数据？</title><link>https://artisanbox.github.io/8/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/4/</guid><description>你好，我是朱晓峰。今天，我们来聊一聊如何操作数据表里的数据。
在咱们的超市项目中，我们给用户设计好了一个数据表 demo.goodsmaster，定义好了里面的字段，以及各种约束，如下所示：
mysql&amp;gt; DESCRIBE demo.goodsmaster; +---------------+--------------+------+-----+---------+--+ | Field | Type | Null | Key | Default |Extra | +---------------+------------+------+-----+---------+------------+ | itemnumber | int | NO | PRI | NULL |auto_increment | | barcode | text | NO | | NULL | | | goodsname | text | NO | | NULL | | | specification | text | YES | | NULL | | | unit | text | YES | | NULL | | | price | decimal(10,2)| NO | | NULL | | +---------------+------------+------+-----+---------+----------------+ 6 rows in set (0.</description></item><item><title>04_复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度</title><link>https://artisanbox.github.io/2/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/5/</guid><description>上一节，我们讲了复杂度的大O表示法和几个分析技巧，还举了一些常见复杂度分析的例子，比如O(1)、O(logn)、O(n)、O(nlogn)复杂度分析。掌握了这些内容，对于复杂度分析这个知识点，你已经可以到及格线了。但是，我想你肯定不会满足于此。
今天我会继续给你讲四个复杂度分析方面的知识点，最好情况时间复杂度（best case time complexity）、最坏情况时间复杂度（worst case time complexity）、平均情况时间复杂度（average case time complexity）、均摊时间复杂度（amortized time complexity）。如果这几个概念你都能掌握，那对你来说，复杂度分析这部分内容就没什么大问题了。
最好、最坏情况时间复杂度上一节我举的分析复杂度的例子都很简单，今天我们来看一个稍微复杂的。你可以用我上节教你的分析技巧，自己先试着分析一下这段代码的时间复杂度。
// n表示数组array的长度 int find(int[] array, int n, int x) { int i = 0; int pos = -1; for (; i &amp;lt; n; ++i) { if (array[i] == x) pos = i; } return pos; } 你应该可以看出来，这段代码要实现的功能是，在一个无序的数组（array）中，查找变量x出现的位置。如果没有找到，就返回-1。按照上节课讲的分析方法，这段代码的复杂度是O(n)，其中，n代表数组的长度。
我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。但是，这段代码写得不够高效。我们可以这样优化一下这段查找代码。
// n表示数组array的长度 int find(int[] array, int n, int x) { int i = 0; int pos = -1; for (; i &amp;lt; n; ++i) { if (array[i] == x) { pos = i; break; } } return pos; } 这个时候，问题就来了。我们优化完之后，这段代码的时间复杂度还是O(n)吗？很显然，咱们上一节讲的分析方法，解决不了这个问题。</description></item><item><title>04_深入浅出索引（上）</title><link>https://artisanbox.github.io/1/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/4/</guid><description>提到数据库索引，我想你并不陌生，在日常工作中会经常接触到。比如某一个SQL查询比较慢，分析完原因之后，你可能就会说“给某个字段加个索引吧”之类的解决方案。但到底什么是索引，索引又是如何工作的呢？今天就让我们一起来聊聊这个话题吧。
数据库索引的内容比较多，我分成了上下两篇文章。索引是数据库系统里面最重要的概念之一，所以我希望你能够耐心看完。在后面的实战文章中，我也会经常引用这两篇文章中提到的知识点，加深你对数据库索引的理解。
一句话简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。一本500页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。
索引的常见模型索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了索引模型的概念。可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是哈希表、有序数组和搜索树。
下面我主要从使用的角度，为你简单分析一下这三种模型的区别。
哈希表是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的键即key，就可以找到其对应的值即Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。
不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。
假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示：
图1 哈希表示意图图中，User2和User4根据身份证号算出来的值都是N，但没关系，后面还跟了一个链表。假设，这时候你要查ID_card_n2对应的名字是什么，处理步骤就是：首先，将ID_card_n2通过哈希函数算出N；然后，按顺序遍历，找到User2。
需要注意的是，图中四个ID_card_n的值并不是递增的，这样做的好处是增加新的User时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。
你可以设想下，如果你现在要找身份证号在[ID_card_X, ID_card_Y]这个区间的所有用户，就必须全部扫描一遍了。
所以，哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。
而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示：
图2 有序数组示意图这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查ID_card_n2对应的名字，用二分法就可以快速得到，这个时间复杂度是O(log(N))。
同时很显然，这个索引结构支持范围查询。你要查身份证号在[ID_card_X, ID_card_Y]区间的User，可以先用二分法找到ID_card_X（如果不存在ID_card_X，就找到大于ID_card_X的第一个User），然后向右遍历，直到查到第一个大于ID_card_Y的身份证号，退出循环。
如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。
所以，有序数组索引只适用于静态存储引擎，比如你要保存的是2017年某个城市的所有人口信息，这类不会再修改的数据。
二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示：
图3 二叉搜索树示意图二叉搜索树的特点是：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。这样如果你要查ID_card_n2的话，按照图中的搜索顺序就是按照UserA -&amp;gt; UserC -&amp;gt; UserF -&amp;gt; User2这个路径得到。这个时间复杂度是O(log(N))。
当然为了维持O(log(N))的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是O(log(N))。
树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。
你可以想象一下一棵100万节点的平衡二叉树，树高20。一次查询可能需要访问20个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的。
为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小。
以InnoDB的一个整数字段索引为例，这个N差不多是1200。这棵树高是4的时候，就可以存1200的3次方个值，这已经17亿了。考虑到树根的数据块总是在内存中的，一个10亿行的表上一个整数字段的索引，查找一个值最多只需要访问3次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。
N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。
不管是哈希还是有序数组，或者N叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM树等数据结构也被用于引擎设计中，这里我就不再一一展开了。
你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。
截止到这里，我用了半篇文章的篇幅和你介绍了不同的数据结构，以及它们的适用场景，你可能会觉得有些枯燥。但是，我建议你还是要多花一些时间来理解这部分内容，毕竟这是数据库处理数据的核心概念之一，在分析问题的时候会经常用到。当你理解了索引的模型后，就会发现在分析问题的时候会有一个更清晰的视角，体会到引擎设计的精妙之处。
现在，我们一起进入相对偏实战的内容吧。
在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于InnoDB存储引擎在MySQL数据库中使用最为广泛，所以下面我就以InnoDB为例，和你分析一下其中的索引模型。
InnoDB 的索引模型在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。
每一个索引在InnoDB里面对应一棵B+树。
假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。
这个表的建表语句是：
mysql&amp;gt; create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 表中R1~R5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意图如下。
图4 InnoDB的索引组织结构从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。
主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。</description></item><item><title>04_穿越功耗墙，我们该从哪些方面提升“性能”？</title><link>https://artisanbox.github.io/4/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/4/</guid><description>上一讲，在讲CPU的性能时，我们提到了这样一个公式：
程序的CPU执行时间 = 指令数×CPI×Clock Cycle Time这么来看，如果要提升计算机的性能，我们可以从指令数、CPI以及CPU主频这三个地方入手。要搞定指令数或者CPI，乍一看都不太容易。于是，研发CPU的硬件工程师们，从80年代开始，就挑上了CPU这个“软柿子”。在CPU上多放一点晶体管，不断提升CPU的时钟频率，这样就能让CPU变得更快，程序的执行时间就会缩短。
于是，从1978年Intel发布的8086 CPU开始，计算机的主频从5MHz开始，不断提升。1980年代中期的80386能够跑到40MHz，1989年的486能够跑到100MHz，直到2000年的奔腾4处理器，主频已经到达了1.4GHz。而消费者也在这20年里养成了“看主频”买电脑的习惯。当时已经基本垄断了桌面CPU市场的Intel更是夸下了海口，表示奔腾4所使用的CPU结构可以做到10GHz，颇有一点“大力出奇迹”的意思。
功耗：CPU的“人体极限”然而，计算机科学界从来不相信“大力出奇迹”。奔腾4的CPU主频从来没有达到过10GHz，最终它的主频上限定格在3.8GHz。这还不是最糟的，更糟糕的事情是，大家发现，奔腾4的主频虽然高，但是它的实际性能却配不上同样的主频。想要用在笔记本上的奔腾4 2.4GHz处理器，其性能只和基于奔腾3架构的奔腾M 1.6GHz处理器差不多。
于是，这一次的“大力出悲剧”，不仅让Intel的对手AMD获得了喘息之机，更是代表着“主频时代”的终结。后面几代Intel CPU主频不但没有上升，反而下降了。到如今，2019年的最高配置Intel i9 CPU，主频也只不过是5GHz而已。相较于1978年到2000年，这20年里300倍的主频提升，从2000年到现在的这19年，CPU的主频大概提高了3倍。
奔腾4的主频为什么没能超过3.8GHz的障碍呢？答案就是功耗问题。什么是功耗问题呢？我们先看一个直观的例子。
一个3.8GHz的奔腾4处理器，满载功率是130瓦。这个130瓦是什么概念呢？机场允许带上飞机的充电宝的容量上限是100瓦时。如果我们把这个CPU安在手机里面，不考虑屏幕内存之类的耗电，这个CPU满载运行45分钟，充电宝里面就没电了。而iPhone X使用ARM架构的CPU，功率则只有4.5瓦左右。
我们的CPU，一般都被叫作超大规模集成电路（Very-Large-Scale Integration，VLSI）。这些电路，实际上都是一个个晶体管组合而成的。CPU在计算，其实就是让晶体管里面的“开关”不断地去“打开”和“关闭”，来组合完成各种运算和功能。
想要计算得快，一方面，我们要在CPU里，同样的面积里面，多放一些晶体管，也就是增加密度；另一方面，我们要让晶体管“打开”和“关闭”得更快一点，也就是提升主频。而这两者，都会增加功耗，带来耗电和散热的问题。
这么说可能还是有点抽象，我还是给你举一个例子。你可以把一个计算机CPU想象成一个巨大的工厂，里面有很多工人，相当于CPU上面的晶体管，互相之间协同工作。
为了工作得快一点，我们要在工厂里多塞一点人。你可能会问，为什么不把工厂造得大一点呢？这是因为，人和人之间如果离得远了，互相之间走过去需要花的时间就会变长，这也会导致性能下降。这就好像如果CPU的面积大，晶体管之间的距离变大，电信号传输的时间就会变长，运算速度自然就慢了。
除了多塞一点人，我们还希望每个人的动作都快一点，这样同样的时间里就可以多干一点活儿了。这就相当于提升CPU主频，但是动作快，每个人就要出汗散热。要是太热了，对工厂里面的人来说会中暑生病，对CPU来说就会崩溃出错。
我们会在CPU上面抹硅脂、装风扇，乃至用上水冷或者其他更好的散热设备，就好像在工厂里面装风扇、空调，发冷饮一样。但是同样的空间下，装上风扇空调能够带来的散热效果也是有极限的。
因此，在CPU里面，能够放下的晶体管数量和晶体管的“开关”频率也都是有限的。一个CPU的功率，可以用这样一个公式来表示：
功耗 ~= 1/2 ×负载电容×电压的平方×开关频率×晶体管数量那么，为了要提升性能，我们需要不断地增加晶体管数量。同样的面积下，我们想要多放一点晶体管，就要把晶体管造得小一点。这个就是平时我们所说的提升“制程”。从28nm到7nm，相当于晶体管本身变成了原来的1/4大小。这个就相当于我们在工厂里，同样的活儿，我们要找瘦小一点的工人，这样一个工厂里面就可以多一些人。我们还要提升主频，让开关的频率变快，也就是要找手脚更快的工人。
但是，功耗增加太多，就会导致CPU散热跟不上，这时，我们就需要降低电压。这里有一点非常关键，在整个功耗的公式里面，功耗和电压的平方是成正比的。这意味着电压下降到原来的1/5，整个的功耗会变成原来的1/25。
事实上，从5MHz主频的8086到5GHz主频的Intel i9，CPU的电压已经从5V左右下降到了1V左右。这也是为什么我们CPU的主频提升了1000倍，但是功耗只增长了40倍。比如说，我写这篇文章用的是Surface Go，在这样的轻薄笔记本上，微软就是选择了把电压下降到0.25V的低电压CPU，使得笔记本能有更长的续航时间。
并行优化，理解阿姆达尔定律虽然制程的优化和电压的下降，在过去的20年里，让我们的CPU性能有所提升。但是从上世纪九十年代到本世纪初，软件工程师们所用的“面向摩尔定律编程”的套路越来越用不下去了。“写程序不考虑性能，等明年CPU性能提升一倍，到时候性能自然就不成问题了”，这种想法已经不可行了。
于是，从奔腾4开始，Intel意识到通过提升主频比较“难”去实现性能提升，边开始推出Core Duo这样的多核CPU，通过提升“吞吐率”而不是“响应时间”，来达到目的。
提升响应时间，就好比提升你用的交通工具的速度，比如原本你是开汽车，现在变成了火车乃至飞机。本来开车从上海到北京要20个小时，换成飞机就只要2个小时了，但是，在此之上，再想要提升速度就不太容易了。我们的CPU在奔腾4的年代，就好比已经到了飞机这个速度极限。
那你可能要问了，接下来该怎么办呢？相比于给飞机提速，工程师们又想到了新的办法，可以一次同时开2架、4架乃至8架飞机，这就好像我们现在用的2核、4核，乃至8核的CPU。
虽然从上海到北京的时间没有变，但是一次飞8架飞机能够运的东西自然就变多了，也就是所谓的“吞吐率”变大了。所以，不管你有没有需要，现在CPU的性能就是提升了2倍乃至8倍、16倍。这也是一个最常见的提升性能的方式，通过并行提高性能。
这个思想在很多地方都可以使用。举个例子，我们做机器学习程序的时候，需要计算向量的点积，比如向量$W = [W_0, W_1, W_2, …, W_{15}]$和向量 $X = [X_0, X_1, X_2, …, X_{15}]$，$W·X = W_0 * X_0 + W_1 * X_1 +$ $W_2 * X_2 + … + W_{15} * X_{15}$。这些式子由16个乘法和1个连加组成。如果你自己一个人用笔来算的话，需要一步一步算16次乘法和15次加法。如果这个时候我们把这个任务分配给4个人，同时去算$W_0～W_3$, $W_4～W_7$, $W_8～W_{11}$, $W_{12}～W_{15}$这样四个部分的结果，再由一个人进行汇总，需要的时间就会缩短。</description></item><item><title>04_语义分析：让程序符合语义规则</title><link>https://artisanbox.github.io/7/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/4/</guid><description>你好，我是宫文学。这一讲，我们进入到语义分析阶段。
对计算机程序语义的研究，是一个专门的学科。要想很简单地把它讲清楚，着实不是太容易的事情。但我们可以退而求其次，只要能直观地去理解什么是语义就可以了。语义，就是程序要表达的意思。
因为计算机最终是用来做计算的，那么理解程序表达的意思，就是要知道让计算机去执行什么计算动作，这样才好翻译成目标代码。
那具体来说，语义分析要做什么工作呢？我们在第1讲中说过，每门计算机语言的标准中，都会定义很多语义规则，比如对加法运算要执行哪些操作。而在语义分析阶段，就是去检查程序是否符合这些语义规则，并为后续的编译工作收集一些语义信息，比如类型信息。
再具体一点，这些语义规则可以分为两大类。
第一类规则与上下文有关。因为我们说了，语法分析只能处理与上下文无关的工作。而与上下文有关的工作呢，自然就放到了语义分析阶段。
第二类规则与类型有关。在计算机语言中，类型是语义的重要载体。所以，语义分析阶段要处理与类型有关的工作。比如，声明新类型、类型检查、类型推断等。在做类型分析的时候，我们会用到一个工具，就是属性计算，也是需要你了解和掌握的。
补充：某些与类型有关的处理工作，还必须到运行期才能去做。比如，在多态的情况，调用一个方法时，到底要采用哪个子类的实现，只有在运行时才会知道。这叫做动态绑定。
在语义分析过程中，会使用两个数据结构。一个还是AST，但我们会把语义分析时获得的一些信息标注在AST上，形成带有标注的AST。另一个是符号表，用来记录程序中声明的各种标识符，并用于后续各个编译阶段。
那今天这一讲，我就会带你看看如何完成与上下文有关的分析、与类型有关的处理，并带你认识符号表和属性计算。
首先，我们来学习如何处理与上下文有关的工作。
上下文相关的分析那什么是与上下文有关的工作呢？在解析一个程序时，会有非常多的分析工作要结合上下文来进行。接下来，我就以控制流检查、闭包分析和引用消解这三个场景和你具体分析下。
场景1：控制流检查
像return、break和continue等语句，都与程序的控制流有关，它们必须符合控制流方面的规则。在Java这样的语言中，语义规则会规定：如果返回值不是void，那么在退出函数体之前，一定要执行一个return语句，那么就要检查所有的控制流分支，是否都以return语句结尾。
场景2：闭包分析
很多语言都支持闭包。而要正确地使用闭包，就必须在编译期知道哪些变量是自由变量。这里的自由变量是指在本函数外面定义的变量，但被这个函数中的代码所使用。这样，在运行期，编译器就会用特殊的内存管理机制来管理这些变量。所以，对闭包的分析，也是上下文敏感的。
场景3：引用消解
我们重点说一下引用消解，以及相关的作用域问题。
引用消解（Reference Resolution），有时也被称作名称消解（Name Resolution）或者标签消解（Label Resolution）。对变量名称、常量名称、函数名称、类型名称、包名称等的消解，都属于引用消解。因此，引用消解是一种非常重要的上下文相关的语义规则，我来重点讲解下。
在高级语言里，我们会做变量、函数（或方法）和类型的声明，然后在其他地方使用它们。这个时候，我们要找到定义和使用之间的正确引用关系。
我们来看一个例子。在语法分析阶段，对于“int b = a + 3”这样一条语句，无论a是否提前声明过，在语法上都是正确的。而在实际的计算机语言中，如果引用某个变量，这个变量就必须是已经声明过的。同时，当前这行代码，要处于变量a的作用域中才行。
图1：变量引用的消解对于变量来说，为了找到正确的引用，就需要用到作用域（Scope）这个概念。在编译技术里面，作用域这个词，有两个稍微有所差异的使用场景。
作用域的第一个使用场景，指的是变量、函数等标识符可以起作用的范围。下图列出了三个变量的作用域，每个变量声明完毕以后，它的下一句就可以引用它。
图2：变量的作用域作用域的第二个使用场景，是词法作用域（Lexical Scope），也就是程序中的不同文本区域。比如，一个语句块、参数列表、类定义的主体、函数（方法）的主体、模块主体、整个程序等。
到这里，咱们来总结下这两个使用场景。标识符和词法的作用域的差异在于：一个本地变量（标识符）的作用域，虽然属于某个词法作用域（如某个函数体），但其作用范围只是在变量声明之后的语句。而类的成员变量（标识符）的作用域，跟词法作用域是一致的，也就是整个类的范围，跟声明的位置无关。如果这个成员变量不是私有的，它的作用域还会覆盖到子类。
那具体到不同的编程语言，它们的作用域规则是不同的。比如，C语言里允许你在一个if语句块里定义一个变量，覆盖外部的变量，而Java语言就不允许这样。所以，在给Java做语义分析时，我们要检查出这种错误。
void foo(){ int a = 2; if (...){ int a = 3; //在C语言里允许，在Java里不允许 ... } } 在做引用消解的时候，为了更好地查找变量、类型等定义信息，编译器会使用一个辅助的数据结构：符号表。
符号表（Symbol Table）在写程序的时候，我们会定义很多标识符，比如常量名称、变量名称、函数名称、类名称，等等。在编译器里，我们又把这些标识符叫做符号（Symbol）。用来保存这些符号的数据结构，就叫做符号表。
比如，对于变量a来说，符号表中的基本信息可以包括：
名称：a 分类：变量 类型：int 作用域：foo函数体 其他必要的信息。 符号表的具体实现，每个编译器可能都不同。比如，它可能是一张线性的表格，也可能是按照作用域形成的一种有层次的表格。以下面这个程序为例，它包含了两个函数，每个函数里面都定义了多个变量：
void foo(){ int a； int b； if (a&amp;gt;0){ int c; int d; } else{ int e; int f; } } void bar(){ int g; { int h; int i; } } 它的符号表可能是下面这样的，分成了多个层次，每个层次对应了一个作用域。在全局作用域，符号表里包含foo和bar两个函数。在foo函数体里，有两个变量a和b，还有两个内部块，每个块里各有两个变量。</description></item><item><title>04_语法分析（二）：解决二元表达式中的难点</title><link>https://artisanbox.github.io/6/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/4/</guid><description>在“03 | 语法分析（一）：纯手工打造公式计算器”中，我们已经初步实现了一个公式计算器。而且你还在这个过程中，直观地获得了写语法分析程序的体验，在一定程度上破除了对语法分析算法的神秘感。
当然了，你也遇到了一些问题，比如怎么消除左递归，怎么确保正确的优先级和结合性。所以本节课的主要目的就是解决这几个问题，让你掌握像算术运算这样的二元表达式（Binary Expression）。
不过在课程开始之前，我想先带你简单地温习一下什么是左递归（Left Recursive）、优先级（Priority）和结合性（Associativity）。
在二元表达式的语法规则中，如果产生式的第一个元素是它自身，那么程序就会无限地递归下去，这种情况就叫做左递归。比如加法表达式的产生式“加法表达式 + 乘法表达式”，就是左递归的。而优先级和结合性则是计算机语言中与表达式有关的核心概念。它们都涉及了语法规则的设计问题。
我们要想深入探讨语法规则设计，需要像在词法分析环节一样，先了解如何用形式化的方法表达语法规则。“工欲善其事必先利其器”。熟练地阅读和书写语法规则，是我们在语法分析环节需要掌握的一项基本功。
所以本节课我会先带你了解如何写语法规则，然后在此基础上，带你解决上面提到的三个问题。
书写语法规则，并进行推导我们已经知道，语法规则是由上下文无关文法表示的，而上下文无关文法是由一组替换规则（又叫产生式）组成的，比如算术表达式的文法规则可以表达成下面这种形式：
add -&amp;gt; mul | add + mul mul -&amp;gt; pri | mul * pri pri -&amp;gt; Id | Num | (add) 按照上面的产生式，add可以替换成mul，或者add + mul。这样的替换过程又叫做“推导”。以“2+3*5” 和 “2+3+4”这两个算术表达式为例，这两个算术表达式的推导过程分别如下图所示：
通过上图的推导过程，你可以清楚地看到这两个表达式是怎样生成的。而分析过程中形成的这棵树，其实就是AST。只不过我们手写的算法在生成AST的时候，通常会做一些简化，省略掉中间一些不必要的节点。比如，“add-add-mul-pri-Num”这一条分支，实际手写时会被简化成“add-Num”。其实，简化AST也是优化编译过程的一种手段，如果不做简化，呈现的效果就是上图的样子。
那么，上图中两颗树的叶子节点有哪些呢？Num、+和*都是终结符，终结符都是词法分析中产生的Token。而那些非叶子节点，就是非终结符。文法的推导过程，就是把非终结符不断替换的过程，让最后的结果没有非终结符，只有终结符。
而在实际应用中，语法规则经常写成下面这种形式：
add ::= mul | add + mul mul ::= pri | mul * pri pri ::= Id | Num | (add) 这种写法叫做“巴科斯范式”，简称BNF。Antlr和Yacc这两个工具都用这种写法。为了简化书写，我有时会在课程中把“::=”简化成一个冒号。你看到的时候，知道是什么意思就可以了。
你有时还会听到一个术语，叫做扩展巴科斯范式(EBNF)。它跟普通的BNF表达式最大的区别，就是里面会用到类似正则表达式的一些写法。比如下面这个规则中运用了*号，来表示这个部分可以重复0到多次：
add -&amp;gt; mul (+ mul)* 其实这种写法跟标准的BNF写法是等价的，但是更简洁。为什么是等价的呢？因为一个项多次重复，就等价于通过递归来推导。从这里我们还可以得到一个推论：就是上下文无关文法包含了正则文法，比正则文法能做更多的事情。</description></item><item><title>04_震撼的Linux全景图：业界成熟的内核架构长什么样？</title><link>https://artisanbox.github.io/9/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/4/</guid><description>你好，我是LMOS。
什么？你想成为计算机黑客？
梦想坐在计算机前敲敲键盘，银行账号里的数字就会自己往上涨。拜托，估计明天你就该被警察逮捕了。真正的黑客是对计算机技术有近乎极致的追求，而不是干坏事。
下面我就带你认识这样一个计算机黑客，看看他是怎样创造出影响世界的Linux，然后进一步了解一下Linux的内部结构。
同时，我也会带你看看Windows NT和Darwin的内部结构，三者形成对比，你能更好地了解它们之间的差异和共同点，这对我们后面写操作系统会很有帮助。
关于LinusLinus Benedict Torvalds，这个名字很长，下面简称Linus，他1969年12月28日出生在芬兰的赫尔辛基市，并不是美国人。Linus在赫尔辛基大学学的就是计算机，妻子还是空手道高手，一个“码林高手”和一个“武林高手”真的是绝配啊。
Linus在小时候就对各种事情充满好奇，这点非常具有黑客精神，后来有了自己的计算机更是痴迷其中，开始自己控制计算机做一些事情，并深挖其背后的原理。就是这种黑客精神促使他后来写出了颠覆世界的软件——Linux，也因此登上了美国《时代》周刊。
你是否对很多垃圾软件感到愤慨，但自己又无法改变。Linus就不一样，他为了方便访问大学服务器中的资源 ，而在自己的机器上写了一个文件系统和硬盘驱动，这样就可以把自己需要的资源下载到自己的机器中。
再后来，这成为了Linux的第一个版本。看看，牛人之所以为牛人就是敢于对现有的规则说不，并勇于改变。
如果仅仅如此，那么也不会有后来的Linux内核。Linus随后做了一个重要决定，他把这款操作系统雏形开源，并加入到自由软件运动，以GPL协议授权，允许用户自由复制或者改动程序代码，但用户必须公开自己的修改并传播。
无疑，正是Linus的这一重要决定使得Linux和他自己名声大振。短短几年时间，就已经聚集了成千上万的狂热分子，大家不计得失的为Linux添砖加瓦，很多程序员更是对Linus像神明一样顶礼膜拜。
Linux内核好了回到正题，回到Linux。Linus也不是什么神明，现有的Linux，99.9%的代码都不是Linus所写，而且他的代码，也不一定比你我的代码写得更好。
Linux，全称GNU/Linux，是一套免费使用和自由传播的操作系统，支持类UNIX、POSIX标准接口，也支持多用户、多进程、多线程，可以在多CPU的机器上运行。由于互联网的发展，Linux吸引了来自全世界各地软件爱好者、科技公司的支持，它已经从大型机到服务器蔓延至个人电脑、嵌入式系统等领域。
Linux系统性能稳定且开源。在很多公司企业网络中被当作服务器来使用，这是Linux的一大亮点，也是它得以壮大的关键。
Linux的基本思想是一切都是文件：每个文件都有确定的用途，包括用户数据、命令、配置参数、硬件设备等对于操作系统内核而言，都被视为各种类型的文件。Linux支持多用户，各个用户对于自己的文件有自己特殊的权利，保证了各用户之间互不影响。多任务则是现代操作系统最重要的一个特点，Linux可以使多个程序同时并独立地运行。
Linux发展到今天，不是哪一个人能做到的，更不是一群计算机黑客能做到的，而是由很多世界级的顶尖科技公司联合开发，如IBM、甲骨文、红帽、英特尔、微软，它们开发Linux并向Linux社区提供补丁，使Linux工作在它们的服务器上，向客户出售业务服务。
Linux发展到今天其代码量近2000万行，可以用浩如烟海来形容，没人能在短时间内弄清楚。但是你也不用害怕，我们可以先看看Linux内部的全景图，从全局了解一下Linux的内部结构，如下图。
啊哈！是不是感觉壮观之后一阵头晕目眩，头晕目眩就对了，因为Linux太大了，别怕，下面我们来分解一下。但这里我要先解释一下，上图仍然不足于描述Linux的全部，只是展示了重要且显而易见的部分。
上图中大致分为五大重要组件，每个组件又分成许多模块从上到下贯穿各个层次，每个模块中有重要的函数和数据结构。具体每个模块的主要功能，我都给你列在了文稿里，你可以详细看看后面这张图。
不要着急，不要心慌，因为现在我们不需要搞清楚这些Linux模块的全部实现细节，只要在心里默念Linux的模块真多啊，大概有五大组件，有好几十个模块，每个模块主要完成什么功能就行了。
是不是松了口气，先定定神，然后我们就能发现Linux这么多模块挤在一起，之间的通信主要是函数调用，而且函数间的调用没有一定的层次关系，更加没有左右边界的限定。函数的调用路径是纵横交错的，从图中的线条可以得到印证。
继续深入思考你就会发现，这些纵横交错的路径上有一个函数出现了问题，就麻烦大了，它会波及到全部组件，导致整个系统崩溃。当然调试解决这个问题，也是相当困难的。同样，模块之间没有隔离，安全隐患也是巨大的。
当然，这种结构不是一无是处，它的性能极高，而性能是衡量操作系统的一个重要指标。这种结构就是传统的内核结构，也称为宏内核架构。
想要评判一个产品好不好，最直接的方法就是用相似的产品对比。你说Linux很好，但是什么为好呢？我说Linux很差，它又差在什么地方呢？
下面我们就拿出Windows和macOS进行对比，注意我们只是对比它们的内核架构。
Darwin-XNU内核我们先来看看Darwin，Darwin是由苹果公司在2000年开发的一个开放源代码的操作系统。
一个经久不衰的公司，必然有自己的核心竞争力，也许是商业策略，也许是技术产品，又或是这两者的结合。而作为苹果公司各种产品和强大的应用生态系统的支撑者——Darwin，更是公司核心竞争力中的核心。
苹果公司有台式计算机、笔记本、平板、手机，台式计算机、笔记本使用了macOS操作系统，平板和手机则使用了iOS操作系统。Darwin作为macOS与iOS操作系统的核心，从技术实现角度说，它必然要支持PowerPC、x86、ARM架构的处理器。
Darwin 使用了一种微内核（Mach）和相应的固件来支持不同的处理器平台，并提供操作系统原始的基础服务，上层的功能性系统服务和工具则是整合了BSD系统所提供的。苹果公司还为其开发了大量的库、框架和服务，不过它们都工作在用户态且闭源。
下面我们先从整体看一下Darwin的架构。
什么？两套内核？惊不惊喜？由于我们是研究Darwin内核，所以上图中我们只需要关注内核-用户转换层以下的部分即可。显然它有两个内核层——Mach层与BSD层。
Mach内核是卡耐基梅隆大学开发的经典微内核，意在提供最基本的操作系统服务，从而达到高性能、安全、可扩展的目的，而BSD则是伯克利大学开发的类UNIX操作系统，提供一整套操作系统服务。
那为什么两套内核会同时存在呢？
MAC OS X（2011年之前的称呼）的发展经过了不同时期，随着时代的进步，产品功能需求增加，单纯的Mach之上实现出现了性能瓶颈，但是为了兼容之前为Mach开发的应用和设备驱动，就保留了Mach内核，同时加入了BSD内核。
Mach内核仍然提供十分简单的进程、线程、IPC通信、虚拟内存设备驱动相关的功能服务，BSD则提供强大的安全特性，完善的网络服务，各种文件系统的支持，同时对Mach的进程、线程、IPC、虚拟内核组件进行细化、扩展延伸。
那么应用如何使用Darwin系统的服务呢？应用会通过用户层的框架和库来请求Darwin系统的服务，即调用Darwin系统API。
在调用Darwin系统API时，会传入一个API号码，用这个号码去索引Mach陷入中断服务表中的函数。此时，API号码如果小于0，则表明请求的是Mach内核的服务，API号码如果大于0，则表明请求的是BSD内核的服务，它提供一整套标准的POSIX接口。
就这样，Mach和BSD就同时存在了。
Mach中还有一个重要的组件Libkern，它是一个库，提供了很多底层的操作函数，同时支持C++运行环境。
依赖这个库的还有IOKit，IOKit管理所有的设备驱动和内核功能扩展模块。驱动程序开发人员则可以使用C++面向对象的方式开发驱动，这个方式很优雅，你完全可以找一个成熟的驱动程序作为父类继承它，要特别实现某个功能就重载其中的函数，也可以同时继承其它驱动程序，这大大节省了内存，也大大降低了出现BUG的可能。
如果你要详细了解Darwin内核的话，可以自行阅读相应的代码。而在这里，你只要从全局认识一下它的结构就行了。
Windows NT内核接下来我们再看下 NT 内核。现代Windows的内核就是NT，我们不妨先看看NT的历史。
如果你是90后，大概没有接触过MS-DOS，它的交互方式是你在键盘上输入相应的功能命令，它完成相应的功能后给用户返回相应的操作信息，没有图形界面。
在MS-DOS内核的实现上，也没有应用现代硬件的保护机制，这导致后来微软基于它开发的图形界面的操作系统，如Windows 3.1、Windows95/98/ME，极其不稳定，且容易死机。
加上类UNIX操作系统在互联网领域大行其道，所以微软急需一款全新的操作系统来与之竞争。所以，Windows NT诞生了。
Windows NT是微软于1993年推出的面向工作站、网络服务器和大型计算机的网络操作系统，也可做PC操作系统。它是一款全新从零开始开发的新操作系统，并应用了现代硬件的所有特性，“NT”所指的便是“新技术”（New Technology）。
而普通用户第一次接触基于NT内核的Windows是Windows 2000，一开始用户其实是不愿意接受的，因为Windows 2000对用户的硬件和应用存在兼容性问题。
随着硬件厂商和应用厂商对程序的升级，这个兼容性问题被缓解了，加之Windows 2000的高性能、高稳定性、高安全性，用户很快便接受了这个操作系统。这可以从Windows 2000的迭代者Windows XP的巨大成功，得到验证。
现在，NT内核在设计上层次非常清晰明了，各组件之间界限耦合程度很低。下面我们就来看看NT内核架构图，了解一下NT内核是如何“庄严宏伟”。如下图：
这样看NT内核架构，是不是就清晰了很多？但这并不是我画图画得清晰，事实上的NT确实如此。</description></item><item><title>04｜如何让我们的语言支持变量和类型？</title><link>https://artisanbox.github.io/3/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/6/</guid><description>你好，我是宫文学。
到目前为止，我们的语言已经能够处理语句，也能够处理表达式，并且都能够解释执行了。不过，我们目前程序能够处理的数据，还都只是字面量而已。接下来，我们要增加一个重要的能力：支持变量。
在程序中声明变量、对变量赋值、基于变量进行计算，是计算机语言的基本功能。只有支持了变量，我们才能实现那些更加强大的功能，比如，你可以用程序写一个计算物体下落的速度和位置，如何随时间变化的公式。这里的时间就是变量，通过给时间变量赋予不同的值，我们可以知道任意时间的物体速度和位置。
这一节，我就带你让我们手头上的语言能够支持变量。在这个过程中，你还会掌握语义分析的更多技能，比如类型处理等。
好了，我们已经知道了这一节的任务。那么第一步要做什么呢？你可以想到，我们首先要了解与处理变量声明和初始化有关的语法规则。
与变量有关的语法分析功能在TypeScript中，我们在声明变量的时候，可以指定类型，这样有助于在编译期做类型检查：
let myAge : number = 18; 如果编译成JavaScript，那么类型信息就会被抹掉：
var myAge = 18; 不过，因为我们的目标是教给你做类型分析的方法，以后还要静态编译成二进制的机器码，所以我们选择的是TypeScript的语法。
此外，在上面的例子中，变量在声明的时候就已经做了初始化。你还可以把这个过程拆成两步。第一步的时候，只是声明变量，之后再给它赋值：
let myAge : number; myAge = 18; 知道了如何声明变量以后，你就可以试着写出相关的语法规则。我这里给出一个示范的版本：
variableDecl : 'let' Identifier typeAnnotation？ ('=' singleExpression)?; typeAnnotation : ':' typeName; 学完了前面3节课，我相信你现在应该对阅读语法规则越来越熟悉了。接下来，就要修改我们的语法分析程序，让它能够处理变量声明语句。这里有什么关键点呢？
这里你要注意的是，我们采用的语法分析的算法是LL算法。而在02讲中，我们知道LL算法的关键是计算First和Follow集合。
首先是First集合。在变量声明语句的上一级语法规则（也就是statement）中，要通过First集合中的不同元素，准确地确定应该采用哪一条语法规则。由于变量声明语句是用let开头的，这就使它非常容易辨别。只要预读的Token是let，那就按照变量声明的语法规则来做解析就对了。
接下来是Follow集合。在上面的语法规则中你能看出，变量的类型注解和初始化部分都是可选的，它们都使用了?号。
由于类型注解是可选的，那么解析器在处理了变量名称后，就要看一下后面的Token是什么。如果是冒号，由于冒号是在typeAnnotation的First集合中，那就去解析一个类型注解；如果这个Token不是冒号，而是typeAnnotation的Follow集合中的元素，就说明当前语句里没有typeAnnotation，所以可以直接略过。
那typeAnnotation的Follow集合有哪些元素呢？我就不直说了，你自己来分析一下吧。
再往后，由于变量的初始化部分也是可选的，还要计算一下它的Follow集合。你能看出，这个Follow集合只有;号这一个元素。所以，在解析到变量声明部分的时候，我们可以通过预读准确地判断接下来该采取什么动作：
如果预读的Token是=号，那就是继续做变量初始化部分的解析； 如果预读的Token是;号，那就证明该语句没有变量初始化部分，因此可以结束变量声明语句的解析了； 如果读到的是=号和;号之外的任何Token呢，那就触发语法错误了。 相关的实现很简单，你参考一下这个示例代码：
let t1 = this.scanner.peek(); //可选的类型标注 if (t1.text == ':'){ this.scanner.next(); t1 = this.scanner.peek(); if (t1.kind == TokenKind.Identifier){ this.scanner.next(); varType = t1.</description></item><item><title>05_CPU工作模式：执行程序的三种模式</title><link>https://artisanbox.github.io/9/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/5/</guid><description>你好，我是LMOS。
我们在前面已经设计了我们的OS架构，你也许正在考虑怎么写代码实现它。恕我直言，现在我们还有很多东西没搞清楚。
由于OS内核直接运行在硬件之上，所以我们要对运行我们代码的硬件平台有一定的了解。接下来，我会通过三节课，带你搞懂硬件平台的关键内容。
今天我们先来学习CPU的工作模式，硬件中最重要的就是CPU，它就是执行程序的核心部件。而我们常用的电脑就是x86平台，所以我们要对x86 CPU有一些基本的了解。
按照CPU功能升级迭代的顺序，CPU的工作模式有实模式、保护模式、长模式，这几种工作模式下CPU执行程序的方式截然不同，下面我们一起来探讨这几种工作模式。
从一段死循环的代码说起请思考一下，如果下面这段应用程序代码能够成功运行，会有什么后果？
int main() { int* addr = (int*)0; cli(); //关中断 while(1) { *addr = 0; addr++; } return 0; } 上述代码首先关掉了CPU中断，让CPU停止响应中断信号，然后进入死循环，最后从内存0地址开始写入0。你马上就会想到，这段代码只做了两件事：一是锁住了CPU，二是清空了内存，你也许会觉得如果这样的代码能正常运行，那简直太可怕了。
不过如果是在实模式下，这样的代码确实是能正常运行。因为在很久以前，计算机资源太少，内存太小，都是单道程序执行，程序大多是由专业人员编写调试好了，才能预约到一个时间去上机运行，没有现代操作系统的概念。
后来有DOS操作系统，也是单道程序系统，不具备执行多道程序的能力，所以CPU这种模式也能很好地工作。
下面我们就从最简单，也是最原始的实模式开始讲起。
实模式实模式又称实地址模式，实，即真实，这个真实分为两个方面，一个方面是运行真实的指令，对指令的动作不作区分，直接执行指令的真实功能，另一方面是发往内存的地址是真实的，对任何地址不加限制地发往内存。
实模式寄存器由于CPU是根据指令完成相应的功能，举个例子：ADD AX,CX；这条指令完成加法操作，AX、CX为ADD指令的操作数，可以理解为ADD函数的两个参数，其功能就是把AX、CX中的数据相加。
指令的操作数，可以是寄存器、内存地址、常数，其实通常情况下是寄存器，AX、CX就是x86 CPU中的寄存器。
下面我们就去看看x86 CPU在实模式下的寄存器。表中每个寄存器都是16位的。
实模式下访问内存虽然有了寄存器，但是数据和指令都是存放在内存中的。通常情况下，需要把数据装载进寄存器中才能操作，还要有获取指令的动作，这些都要访问内存才行，而我们知道访问内存靠的是地址值。
那问题来了，这个值是如何计算的呢？计算过程如下图。
结合上图可以发现，所有的内存地址都是由段寄存器左移4位，再加上一个通用寄存器中的值或者常数形成地址，然后由这个地址去访问内存。这就是大名鼎鼎的分段内存管理模型。
只不过这里要特别注意的是，代码段是由CS和IP确定的，而栈段是由SS和SP段确定的。
下面我们写一个DOS下的Hello World应用程序，这是一个工作在实模式下的汇编代码程序，一共16位，具体代码如下：
data SEGMENT ;定义一个数据段存放Hello World! hello DB 'Hello World!$' ;注意要以$结束 data ENDS code SEGMENT ;定义一个代码段存放程序指令 ASSUME CS:CODE,DS:DATA ;告诉汇编程序，DS指向数据段，CS指向代码段 start: MOV AX,data ;将data段首地址赋值给AX MOV DS,AX ;将AX赋值给DS，使DS指向data段 LEA DX,hello ;使DX指向hello首地址 MOV AH,09h ;给AH设置参数09H，AH是AX高8位，AL是AX低8位，其它类似 INT 21h ;执行DOS中断输出DS指向的DX指向的字符串hello MOV AX,4C00h ;给AX设置参数4C00h INT 21h ;调用4C00h号功能，结束程序 code ENDS END start 上述代码中的结构模型，也是符合CPU实模式下分段内存管理模式的，它们被汇编器转换成二进制数据后，也是以段的形式存在的。</description></item><item><title>05_主键：如何正确设置主键？</title><link>https://artisanbox.github.io/8/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/5/</guid><description>你好，我是朱晓峰，今天我们来聊一聊如何用好MySQL的主键。
前面在讲存储的时候，我提到过主键，它可以唯一标识表中的某一条记录，对数据表来说非常重要。当我们需要查询和引用表中的一条记录的时候，最好的办法就是通过主键。只有合理地设置主键，才能确保我们准确、快速地找到所需要的数据记录。今天我就借助咱们的超市项目的实际需求，来给你讲一讲怎么正确设置主键。
在我们的项目中，客户要进行会员营销，相应的，我们就需要处理会员信息。会员信息表（demo.membermaster）的设计大体如下：
为了能够唯一地标识一个会员的信息，我们需要为会员信息表设置一个主键。那么，怎么为这个表设置主键，才能达到我们理想的目标呢？
今天，我就带你在解决这个实际问题的过程中，学习下三种设置主键的思路：业务字段做主键、自增字段做主键和手动赋值字段做主键。
业务字段做主键针对这个需求，我们最容易想到的，是选择表中已有的字段，也就是跟业务相关的字段做主键。那么，在这个表里，哪个字段比较合适呢？我们来分析一下。
会员卡号（cardno）看起来比较合适，因为会员卡号不能为空，而且有唯一性，可以用来标识一条会员记录。我们来尝试一下用会员卡号做主键。
我们可以用下面的代码，在创建表的时候，设置字段cardno为主键：
mysql&amp;gt; CREATE TABLE demo.membermaster -&amp;gt; ( -&amp;gt; cardno CHAR(8) PRIMARY KEY, -- 会员卡号为主键 -&amp;gt; membername TEXT, -&amp;gt; memberphone TEXT, -&amp;gt; memberpid TEXT, -&amp;gt; memberaddress TEXT, -&amp;gt; sex TEXT, -&amp;gt; birthday DATETIME -&amp;gt; ); Query OK, 0 rows affected (0.06 sec) 我们来查询一下表的结构，确认下主键是否创建成功了：
mysql&amp;gt; DESCRIBE demo.membermaster; +---------------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------------+----------+------+-----+---------+-------+ | cardno | char(8) | NO | PRI | NULL | | | membername | text | YES | | NULL | | | memberphone | text | YES | | NULL | | | memberpid | text | YES | | NULL | | | memberaddress | text | YES | | NULL | | | sex | text | YES | | NULL | | | birthday | datetime | YES | | NULL | | +---------------+----------+------+-----+---------+-------+ 7 rows in set (0.</description></item><item><title>05_数组：为什么很多编程语言中数组都从0开始编号？</title><link>https://artisanbox.github.io/2/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/6/</guid><description>提到数组，我想你肯定不陌生，甚至还会自信地说，它很简单啊。
是的，在每一种编程语言中，基本都会有数组这种数据类型。不过，它不仅仅是一种编程语言中的数据类型，还是一种最基础的数据结构。尽管数组看起来非常基础、简单，但是我估计很多人都并没有理解这个基础数据结构的精髓。
在大部分编程语言中，数组都是从0开始编号的，但你是否下意识地想过，为什么数组要从0开始编号，而不是从1开始呢？ 从1开始不是更符合人类的思维习惯吗？
你可以带着这个问题来学习接下来的内容。
如何实现随机访问？什么是数组？我估计你心中已经有了答案。不过，我还是想用专业的话来给你做下解释。数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。
这个定义里有几个关键词，理解了这几个关键词，我想你就能彻底掌握数组的概念了。下面就从我的角度分别给你“点拨”一下。
第一是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。
而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。
第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。
说到数据的访问，那你知道数组是如何实现根据下标随机访问数组元素的吗？
我们拿一个长度为10的int类型的数组int[] a = new int[10]来举例。在我画的这个图中，计算机给数组a[10]，分配了一块连续内存空间1000～1039，其中，内存块的首地址为base_address = 1000。
我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：
a[i]_address = base_address + i * data_type_size 其中data_type_size表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是int类型数据，所以data_type_size就为4个字节。这个公式非常简单，我就不多做解释了。
这里我要特别纠正一个“错误”。我在面试的时候，常常会问数组和链表的区别，很多人都回答说，“链表适合插入、删除，时间复杂度O(1)；数组适合查找，查找时间复杂度为O(1)”。
实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为O(1)。
低效的“插入”和“删除”前面概念部分我们提到，数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。现在我们就来详细说一下，究竟为什么会导致低效？又有哪些改进方法呢？
我们先来看插入操作。
假设数组的长度为n，现在，如果我们需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，给新来的数据，我们需要将第k～n这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？你可以自己先试着分析一下。
如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为(1+2+...n)/n=O(n)。
如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移k之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第k个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。
为了更好地理解，我们举一个例子。假设数组a[10]中存储了如下5个元素：a，b，c，d，e。
我们现在需要将元素x插入到第3个位置。我们只需要将c放入到a[5]，将a[2]赋值为x即可。最后，数组中的元素如下： a，b，x，d，e，c。
利用这种处理技巧，在特定场景下，在第k个位置插入一个元素的时间复杂度就会降为O(1)。这个处理思想在快排中也会用到，我会在排序那一节具体来讲，这里就说到这儿。
我们再来看删除操作。
跟插入数据类似，如果我们要删除第k个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。
和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为O(1)；如果删除开头的数据，则最坏情况时间复杂度为O(n)；平均情况时间复杂度也为O(n)。
实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？
我们继续来看例子。数组a[10]中存储了8个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除a，b，c三个元素。
为了避免d，e，f，g，h这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。
如果你了解JVM，你会发现，这不就是JVM标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。
警惕数组的访问越界问题了解了数组的几个基本操作后，我们来聊聊数组访问越界的问题。
首先，我请你来分析一下这段C语言代码的运行结果：
int main(int argc, char* argv[]){ int i = 0; int arr[3] = {0}; for(; i&amp;lt;=3; i++){ arr[i] = 0; printf(&amp;quot;hello world\n&amp;quot;); } return 0; } 你发现问题了吗？这段代码的运行结果并非是打印三行“hello word”，而是会无限打印“hello world”，这是为什么呢？</description></item><item><title>05_深入浅出索引（下）</title><link>https://artisanbox.github.io/1/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/5/</guid><description>在上一篇文章中，我和你介绍了InnoDB索引的数据结构模型，今天我们再继续聊聊跟MySQL索引有关的概念。
在开始这篇文章之前，我们先来看一下这个问题：
在下面这个表T中，如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？
下面是这个表的初始化语句。
mysql&amp;gt; create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; insert into T values(100,1, &amp;lsquo;aa&amp;rsquo;),(200,2,&amp;lsquo;bb&amp;rsquo;),(300,3,&amp;lsquo;cc&amp;rsquo;),(500,5,&amp;rsquo;ee&amp;rsquo;),(600,6,&amp;lsquo;ff&amp;rsquo;),(700,7,&amp;lsquo;gg&amp;rsquo;); 图1 InnoDB的索引组织结构现在，我们一起来看看这条SQL查询语句的执行流程：
在k索引树上找到k=3的记录，取得 ID = 300；
再到ID索引树查到ID=300对应的R3；
在k索引树取下一个值k=5，取得ID=500；
再回到ID索引树查到ID=500对应的R4；
在k索引树取下一个值k=6，不满足条件，循环结束。
在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了k索引树的3条记录（步骤1、3和5），回表了两次（步骤2和4）。
在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？
覆盖索引如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。</description></item><item><title>05_计算机指令：让我们试试用纸带编程</title><link>https://artisanbox.github.io/4/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/5/</guid><description>你在学写程序的时候，有没有想过，古老年代的计算机程序是怎么写出来的？
上大学的时候，我们系里教C语言程序设计的老师说，他们当年学写程序的时候，不像现在这样，都是用一种古老的物理设备，叫作“打孔卡（Punched Card）”。用这种设备写程序，可没法像今天这样，掏出键盘就能打字，而是要先在脑海里或者在纸上写出程序，然后在纸带或者卡片上打洞。这样，要写的程序、要处理的数据，就变成一条条纸带或者一张张卡片，之后再交给当时的计算机去处理。
你看这个穿孔纸带是不是有点儿像我们现在考试用的答题卡？那个时候，人们在特定的位置上打洞或者不打洞，来代表“0”或者“1”。
为什么早期的计算机程序要使用打孔卡，而不能像我们现在一样，用C或者Python这样的高级语言来写呢？原因很简单，因为计算机或者说CPU本身，并没有能力理解这些高级语言。即使在2019年的今天，我们使用的现代个人计算机，仍然只能处理所谓的“机器码”，也就是一连串的“0”和“1”这样的数字。
那么，我们每天用高级语言的程序，最终是怎么变成一串串“0”和“1”的？这一串串“0”和“1”又是怎么在CPU中处理的？今天，我们就来仔细介绍一下，“机器码”和“计算机指令”到底是怎么回事。
在软硬件接口中，CPU帮我们做了什么事？我们常说，CPU就是计算机的大脑。CPU的全称是Central Processing Unit，中文是中央处理器。
我们上一节说了，从硬件的角度来看，CPU就是一个超大规模集成电路，通过电路实现了加法、乘法乃至各种各样的处理逻辑。
如果我们从软件工程师的角度来讲，CPU就是一个执行各种计算机指令（Instruction Code）的逻辑机器。这里的计算机指令，就好比一门CPU能够听得懂的语言，我们也可以把它叫作机器语言（Machine Language）。
不同的CPU能够听懂的语言不太一样。比如，我们的个人电脑用的是Intel的CPU，苹果手机用的是ARM的CPU。这两者能听懂的语言就不太一样。类似这样两种CPU各自支持的语言，就是两组不同的计算机指令集，英文叫Instruction Set。这里面的“Set”，其实就是数学上的集合，代表不同的单词、语法。
所以，如果我们在自己电脑上写一个程序，然后把这个程序复制一下，装到自己的手机上，肯定是没办法正常运行的，因为这两者语言不通。而一台电脑上的程序，简单复制一下到另外一台电脑上，通常就能正常运行，因为这两台CPU有着相同的指令集，也就是说，它们的语言相通的。
一个计算机程序，不可能只有一条指令，而是由成千上万条指令组成的。但是CPU里不能一直放着所有指令，所以计算机程序平时是存储在存储器中的。这种程序指令存储在存储器里面的计算机，我们就叫作存储程序型计算机（Stored-program Computer）。
说到这里，你可能要问了，难道还有不是存储程序型的计算机么？其实，在没有现代计算机之前，有着聪明才智的工程师们，早就发明了一种叫Plugboard Computer的计算设备。我把它直译成“插线板计算机”。在一个布满了各种插口和插座的板子上，工程师们用不同的电线来连接不同的插口和插座，从而来完成各种计算任务。下面这个图就是一台IBM的Plugboard，看起来是不是有一股满满的蒸汽朋克范儿？
从编译到汇编，代码怎么变成机器码？了解了计算机指令和计算机指令集，接下来我们来看看，平时编写的代码，到底是怎么变成一条条计算机指令，最后被CPU执行的呢？我们拿一小段真实的C语言程序来看看。
// test.c int main() { int a = 1; int b = 2; a = a + b; } 这是一段再简单不过的C语言程序，即便你不了解C语言，应该也可以看懂。我们给两个变量 a、b分别赋值1、2，然后再将a、b两个变量中的值加在一起，重新赋值给了a这个变量。
要让这段程序在一个Linux操作系统上跑起来，我们需要把整个程序翻译成一个汇编语言（ASM，Assembly Language）的程序，这个过程我们一般叫编译（Compile）成汇编代码。
针对汇编代码，我们可以再用汇编器（Assembler）翻译成机器码（Machine Code）。这些机器码由“0”和“1”组成的机器语言表示。这一条条机器码，就是一条条的计算机指令。这样一串串的16进制数字，就是我们CPU能够真正认识的计算机指令。
在一个Linux操作系统上，我们可以简单地使用gcc和objdump这样两条命令，把对应的汇编代码和机器码都打印出来。
$ gcc -g -c test.c $ objdump -d -M intel -S test.o 可以看到，左侧有一堆数字，这些就是一条条机器码；右边有一系列的push、mov、add、pop等，这些就是对应的汇编代码。一行C语言代码，有时候只对应一条机器码和汇编代码，有时候则是对应两条机器码和汇编代码。汇编代码和机器码之间是一一对应的。
test.o: file format elf64-x86-64 Disassembly of section .text: 0000000000000000 &amp;lt;main&amp;gt;: int main() { 0: 55 push rbp 1: 48 89 e5 mov rbp,rsp int a = 1; 4: c7 45 fc 01 00 00 00 mov DWORD PTR [rbp-0x4],0x1 int b = 2; b: c7 45 f8 02 00 00 00 mov DWORD PTR [rbp-0x8],0x2 a = a + b; 12: 8b 45 f8 mov eax,DWORD PTR [rbp-0x8] 15: 01 45 fc add DWORD PTR [rbp-0x4],eax } 18: 5d pop rbp 19: c3 ret 这个时候你可能又要问了，我们实际在用GCC（GUC编译器套装，GNU Compiler Collectipon）编译器的时候，可以直接把代码编译成机器码呀，为什么还需要汇编代码呢？原因很简单，你看着那一串数字表示的机器码，是不是摸不着头脑？但是即使你没有学过汇编代码，看的时候多少也能“猜”出一些这些代码的含义。</description></item><item><title>05_语法分析（三）：实现一门简单的脚本语言</title><link>https://artisanbox.github.io/6/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/5/</guid><description>前两节课结束后，我们已经掌握了表达式的解析，并通过一个简单的解释器实现了公式的计算。但这个解释器还是比较简单的，看上去还不大像一门语言。那么如何让它支持更多的功能，更像一门脚本语言呢？本节课，我会带你寻找答案。
我将继续带你实现一些功能，比如：
支持变量声明和初始化语句，就像“int age” “int age = 45”和“int age = 17+8+20”； 支持赋值语句“age = 45”； 在表达式中可以使用变量，例如“age + 10 *2”； 实现一个命令行终端，能够读取输入的语句并输出结果。 实现这些功能之后，我们的成果会更像一个脚本解释器。而且在这个过程中，我还会带你巩固语法分析中的递归下降算法，和你一起讨论“回溯”这个特征，让你对递归下降算法的特征理解得更加全面。
不过，为了实现这些新的语法，我们首先要把它们用语法规则描述出来。
增加所需要的语法规则首先，一门脚本语言是要支持语句的，比如变量声明语句、赋值语句等等。单独一个表达式，也可以视为语句，叫做“表达式语句”。你在终端里输入2+3；，就能回显出5来，这就是表达式作为一个语句在执行。按照我们的语法，无非是在表达式后面多了个分号而已。C语言和Java都会采用分号作为语句结尾的标识，我们也可以这样写。
我们用扩展巴科斯范式（EBNF）写出下面的语法规则：
programm: statement+; statement intDeclaration | expressionStatement | assignmentStatement ; 变量声明语句以int开头，后面跟标识符，然后有可选的初始化部分，也就是一个等号和一个表达式，最后再加分号：
intDeclaration : &amp;lsquo;int&amp;rsquo; Id ( &amp;lsquo;=&amp;rsquo; additiveExpression)? &amp;lsquo;;&amp;rsquo;; 表达式语句目前只支持加法表达式，未来可以加其他的表达式，比如条件表达式，它后面同样加分号：
expressionStatement : additiveExpression &amp;lsquo;;&amp;rsquo;; 赋值语句是标识符后面跟着等号和一个表达式，再加分号：
assignmentStatement : Identifier &amp;lsquo;=&amp;rsquo; additiveExpression &amp;lsquo;;&amp;rsquo;; 为了在表达式中可以使用变量，我们还需要把primaryExpression改写，除了包含整型字面量以外，还要包含标识符和用括号括起来的表达式：
primaryExpression : Identifier| IntLiteral | &amp;lsquo;(&amp;rsquo; additiveExpression &amp;lsquo;)&amp;rsquo;; 这样，我们就把想实现的语法特性，都用语法规则表达出来了。接下来，我们就一步一步实现这些特性。
让脚本语言支持变量之前实现的公式计算器只支持了数字字面量的运算，如果能在表达式中用上变量，会更有用，比如能够执行下面两句：
int age = 45; age + 10 * 2; 这两个语句里面的语法特性包含了变量声明、给变量赋值，以及在表达式里引用变量。为了给变量赋值，我们必须在脚本语言的解释器中开辟一个存储区，记录不同的变量和它们的值：</description></item><item><title>05_运行时机制：程序如何运行，你有发言权</title><link>https://artisanbox.github.io/7/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/5/</guid><description>你好，我是宫文学。在语义分析之后，编译过程就开始进入中后端了。
经过前端阶段的处理分析，编译器已经充分理解了源代码的含义，准备好把前端处理的结果（带有标注信息的AST、符号表）翻译成目标代码了。
我在第1讲也说过，如果想做好翻译工作，编译器必须理解目标代码。而要理解目标代码，它就必须要理解目标代码是如何被执行的。通常情况下，程序有两种执行模式。
第一种执行模式是在物理机上运行。针对的是C、C++、Go这样的语言，编译器直接将源代码编译成汇编代码（或直接生成机器码），然后生成能够在操作系统上运行的可执行程序。为了实现它们的后端，编译器需要理解程序在底层的运行环境，包括CPU、内存、操作系统跟程序的互动关系，并要能理解汇编代码。
第二种执行模式是在虚拟机上运行。针对的是Java、Python、Erlang和Lua等语言，它们能够在虚拟机上解释执行。这时候，编译器要理解该语言的虚拟机的运行机制，并生成能够被执行的IR。
理解了这两种执行模式的特点，我们也就能弄清楚用高级语言编写的程序是如何运行的，进而也就理解了编译器在中后端的任务是什么。接下来，我们就从最基础的物理机模式开始学习吧。
在物理机上运行在计算机发展的早期，科学家们确立了计算机的结构，并一直延续至今，这种结构就是冯·诺依曼结构。它的主要特点是：数据和指令不加区别，混合存储在同一个存储器中（即主存，或叫做内存）；用一个指令指针指向内存中指令的位置，CPU就能自动加载这个位置的指令并执行。
在x86架构下，这个指针是eip寄存器（32位模式）或rip寄存器（64位模式）。一条指令执行完毕，指令指针自动增加，并执行下一条指令。如果遇到跳转指令，则跳转到另一个地址去执行。
图1：计算机的运行机制这其实就是计算机最基本的运行原理。这样，你就可以在大脑中建立起像图1那样的直观结构。
通过图1，你会看到，计算机指令的执行基本上只跟两个硬件相关：一个是CPU，一个是内存。
CPUCPU是计算机的核心。从硬件构成方面，我们需要知道它的三个信息：
第一，CPU上面有寄存器，并且可以直接由指令访问。寄存器的读写速度非常快，大约是内存的100倍。所以我们编译后的代码，要尽量充分利用寄存器，而不是频繁地去访问内存。 第二，CPU有高速缓存，并且可能是多级的。高速缓存也比内存快。CPU在读取指令和数据的时候，不是一次读取一条，而是读取相邻的一批数据，放到高速缓存里。接下来要读取的数据，很可能已经在高速缓存里了，通过这种机制来提高运行性能。因此，编译器要尽量提高缓存的命中率。 第三，CPU内部有多个功能单元，有的负责计算，有的负责解码，等等。所以，一个指令可以被切分成多个执行阶段，每个阶段在不同的功能单元上运行，这为实现指令级并行提供了硬件基础。在第8讲，我还会和你详细解释这个话题。 好了，掌握了这个知识点，我们可以继续往下学习了。我们说，CPU是运行指令的地方，那指令到底是什么样子的呢？
我们知道，CPU有多种不同的架构，比如x86架构、ARM架构等。不同架构的CPU，它的指令是不一样的。不过它们的共性之处在于，指令都是01这样的机器码。为了便于理解，我们通常会用汇编代码来表示机器指令。比如，b=a+2指令对应的汇编码可能是这样的：
movl -4(%rbp), %eax #把%rbp-4内存地址的值拷贝到%eax寄存器 addl $2, %eax #把2加到%eax寄存器 movl %eax, -8(%rbp) #把%eax寄存器的值保存回内存，地址是%rbp-8 上面的汇编代码采用的是GNU汇编器规定的格式。每条指令都包含了两部分：操作码（opcode）和操作数（oprand）。
图2：汇编代码示例操作码是让CPU执行的动作。这段示例代码中，movl、addl是助记符（Assembly Mnemonic），其中的mov和add是指令，l是后缀，表示操作数的位数。
而操作数是指令的操作对象，它可以是常数、寄存器和某个内存地址。图2示例的汇编代码中，“$2”就是个常数，在指令里我们把它叫做立即数；而“%eax”是访问一个寄存器，其中eax是寄存器的名称；而带有括号的“-4(%rbp)”，则是对内存的访问方式，这个内存的地址是在rbp寄存器的值的基础上减去4。
如果你还想对指令、汇编代码有更多的了解，可以再去查阅些资料学习，比如去参考下我的《编译原理之美》中的第22、23、31这几讲。
这里要提一下，虽然程序觉得自己一直在使用CPU，但实际上，背后有操作系统在做调度。操作系统是管理系统资源的，而CPU是计算机的核心资源，操作系统会把CPU的时间划分成多个时间片，分配给不同的程序使用，每个程序实际上都是在“断断续续”地使用CPU，这就是操作系统的分时调度机制。在后面课程里讨论并发的时候，我们会更加深入地探讨这个机制。
内存好了，接下来我说说执行指令相关的另一个硬件：内存。
程序在运行时，操作系统会给它分配一块虚拟的内存空间，让它可以在运行期内使用。内存中的每个位置都有一个地址，地址的长度决定了能够表示多大空间，这叫做寻址空间。我们目前使用的都是64位的机器，理论上，你可以用一个64位的长整型来表示内存地址。
不过，由于我们根本用不了这么大的内存，所以AMD64架构的寻址空间只使用了48位。但这也有256TB，远远超出了一般情况下的需求。所以，像Windows这样的操作系统还会给予进一步的限制，缩小程序的寻址空间。
图3：48位寻址空间有多大但即使是在加了限制的情况下，程序在逻辑上可使用的内存一般也会大于实际的物理内存。不过进程不会一下子使用那么多的内存，只有在向操作系统申请内存的时候，操作系统才会把一块物理内存，映射成进程寻址空间内的一块内存。对应到图4中，中间一条是物理内存，上下两条是两个进程的寻址空间，它们要比物理内存大。
对于有些物理内存的内容，还可以映射进多个进程的地址空间，以减少内存的使用。比如说，如果进程1和进程2运行的是同一个可执行文件，那么程序的代码段是可以在两个进程之间共享的。你在图中可以看到这种情况。
图4：物理内存和逻辑内存的关系另外，对于已经分配给进程的内存，如果进程很长时间不用，操作系统会把它写到磁盘上，以便腾出更多可用的物理内存。在需要的时候，再把这块空间的数据从磁盘中读回来。这就是操作系统的虚拟内存机制。
当然，也存在没有操作系统的情况，这个时候你的程序所使用的内存就是物理内存，我们必须自己做好内存的管理。
那么从程序角度来说，我们应该怎样使用内存呢？
本质上来说，你想怎么用就怎么用，并没有什么特别的限制。一个编译器的作者，可以决定在哪儿放代码，在哪儿放数据。当然了，别的作者也可能采用其他的策略。比如，C语言和Java虚拟机对内存的管理和使用策略就是不同的。
不过尽管如此，大多数语言还是会采用一些通用的内存管理模式。以C语言为例，会把内存划分为代码区、静态数据区、栈和堆，如下所示。
图5：C语言的内存布局方式其中，代码区（也叫做文本段），主要存放编译完成后的机器码，也就是CPU指令；静态数据区会保存程序中的全局变量和常量。这些内存是静态的、固定大小的，在编译完毕以后就能确定清楚所占用空间的大小、代码区每个函数的地址，以及静态数据区每个变量和常量的地址。这些内存在程序运行期间会一直被占用。
而堆和栈，属于程序动态、按需获取的内存。我来和你分析下这两种内存。
我们先看看栈（Stack）。使用栈的一个好处是，操作系统会根据程序使用内存的需求，自动地增加或减少栈的空间。通常来说，操作系统会用一个寄存器保存栈顶的地址，程序可以修改这个寄存器的值，来获取或者释放空间。有的CPU，还有专门的指令来管理栈，比如x86架构，会使用push和pop指令，把数据写入栈或弹出栈，并自动修改栈顶指针。
在程序里使用栈的场景是这样的，程序的运行可以看做是在逐级调用函数（或者叫过程）。像下面的示例程序，存在着main-&amp;gt;bar-&amp;gt;foo的调用结构，这也就是控制流转移的过程。
int main(){ int a = 1; foo(3); bar(); } int foo(int c){ int b = 2; return b+c; }
int bar(){ return foo(4) + 1; 图6：程序逐级调用的过程每次调用函数的过程中，都需要一些空间来保存一些信息，比如参数、需要保护的寄存器的值、返回地址、本地变量等，这些信息叫做这个过程的活动记录（Activation Record）。</description></item><item><title>05｜函数实现：是时候让我们的语言支持函数和返回值了</title><link>https://artisanbox.github.io/3/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/7/</guid><description>你好，我是宫文学。
不知道你还记不记得，我们在第一节课就支持了函数功能。不过那个版本的函数功能是被高度简化了的，比如，它不支持声明函数的参数，也不支持函数的返回值。
在上一节课实现了对变量的支持以后，我们终于可以进一步升级我们的函数功能了。为什么要等到这个时候呢？因为其实函数的参数的实现机制跟变量是很类似的。
为了升级我们的函数功能，我们需要完成几项任务：
参考变量的机制实现函数的参数机制； 支持在函数内部声明和使用本地变量，这个时候，我们需要能够区分函数作用域和全局作用域，还要能够在退出函数的时候，让本地变量的生命期随之结束； 要支持函数的返回值。 你可以想象到，在实现了这节课的功能以后，我们的语言就越来像样了。你甚至可以用这个语言来实现一点复杂的功能了，比如设计个函数，用来计算圆的周长、面积什么的。
好吧，让我们赶紧动手吧。首先，像上节课一样，我们还是要增强一下语法分析功能，以便解析函数的参数和返回值，并支持在函数内部声明本地变量。
增强语法分析功能我们原来的函数声明的语法比较简陋，现在我们采用一下TypeScript完整的函数声明语法。采用这个语法，函数可以有0到多个参数，每个参数都可以指定类型，就像变量一样，还可以指定函数返回值的类型。
//函数声明，由'function'关键字、函数名、函数签名和函数体构成。 functionDeclaration : 'function' Identifier callSignature '{' functionBody '}'; //函数签名，也就是参数数量和类型正确，以及函数的返回值类型正确 callSignature : &amp;lsquo;(&amp;rsquo; parameterList? &amp;lsquo;)&amp;rsquo; typeAnnotation? ;
//参数列表，由1到多个参数声明构成。 parameterList : parameter (&amp;rsquo;,&amp;rsquo; parameter)* ;
//参数，由参数名称和可选的类型标注构成 parameter : Identifier typeAnnotation? ;
//返回语句 returnStatement: &amp;lsquo;return&amp;rsquo; expression? &amp;lsquo;;&amp;rsquo; ; 采用该规则以后，你可以声明一个像下面的函数，比如，你给这个函数传入圆的半径的值，它会给你计算出圆的面积：
//计算圆的面积 function circleArea(r : number):number{ let area : number = 3.14rr; return area; } let r:number =4; println(&amp;ldquo;r=&amp;rdquo; + r +&amp;quot;, area=&amp;quot;+circleArea(r)); r = 5; println(&amp;ldquo;r=&amp;rdquo; + r +&amp;quot;, area=&amp;quot;+circleArea(r)); 好了，修改好语法规则以后，我们就按照该语法规则来升级一下语法分析程序，跟04讲一样，我们同样需要计算一下相关元素的First和Follow集合。在这里，我就不再演示计算First集合和Follow集合了，而是把它们留到了思考题的部分，让你自己来计算一个语法成分的Follow集合，这样能让你对LL算法理解得更加深入。</description></item><item><title>06_中间代码：不是只有一副面孔</title><link>https://artisanbox.github.io/7/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/6/</guid><description>你好，我是宫文学。今天这一讲，我来带你认识一下中间代码（IR）。
IR，也就是中间代码（Intermediate Representation，有时也称Intermediate Code，IC），它是编译器中很重要的一种数据结构。编译器在做完前端工作以后，首先就是生成IR，并在此基础上执行各种优化算法，最后再生成目标代码。
所以说，编译技术的IR非常重要，它是运行各种优化算法、代码生成算法的基础。不过，鉴于IR的设计一般与编译器密切相关，而一些教科书可能更侧重于讲理论，所以对IR的介绍就不那么具体。这就导致我们对IR有非常多的疑问，比如：
IR都有哪些不同的设计，可以分成什么类型？ IR有像高级语言和汇编代码那样的标准书写格式吗？ IR可以采用什么数据结构来实现？ 为了帮助你把对IR的认识从抽象变得具体，我今天就从全局的视角和你一起梳理下IR有关的认知。
首先，我们来了解一下IR的用途，并一起看看由于用途不同导致IR分成的多个层次。
IR的用途和层次设计IR的目的，是要满足编译器中的各种需求。需求的不同，就会导致IR的设计不同。通常情况下，IR有两种用途，一种是用来做分析和变换的，一种是直接用于解释执行的。我们先来看第一种。
编译器中，基于IR的分析和处理工作，一开始可以基于一些抽象层次比较高的语义，这时所需要的IR更接近源代码。而在后面，则会使用低层次的、更加接近目标代码的语义。
基于这种从高到低的抽象层次，IR可以归结为HIR、MIR和LIR三类。
HIR：基于源语言做一些分析和变换假设你要开发一款IDE，那最主要的功能包括：发现语法错误、分析符号之间的依赖关系（以便进行跳转、判断方法的重载等）、根据需要自动生成或修改一些代码（提供重构能力）。
这个时候，你对IR的需求，是能够准确表达源语言的语义就行了。这种类型的IR，可以叫做High IR，简称HIR。
其实，AST和符号表就可以满足这个需求。也就是说，AST也可以算作一种IR。如果你要开发IDE、代码翻译工具（从一门语言翻译到另一门语言）、代码生成工具、代码统计工具等，使用AST（加上符号表）就够了。
当然，有些HIR并不是树状结构（比如可以采用线性结构），但一般会保留诸如条件判断、循环、数组等抽象层次比较高的语法结构。
基于HIR，可以做一些高层次的代码优化，比如常数折叠、内联等。在Java和Go的编译器中，你可以看到不少基于AST做的优化工作。
MIR：独立于源语言和CPU架构做分析和优化大量的优化算法是可以通用的，没有必要依赖源语言的语法和语义，也没有必要依赖具体的CPU架构。
这些优化包括部分算术优化、常量和变量传播、死代码删除等，我会在下一讲和你介绍。实现这类分析和优化功能的IR可以叫做Middle IR，简称MIR。
因为MIR跟源代码和目标代码都无关，所以在讲解优化算法时，通常是基于MIR，比如三地址代码（Three Address Code，TAC）。
TAC的特点是，最多有三个地址（也就是变量），其中赋值符号的左边是用来写入的，而右边最多可以有两个地址和一个操作符，用于读取数据并计算。
我们来看一个例子，示例函数foo：
int foo (int a){ int b = 0; if (a &amp;gt; 10) b = a; else b = 10; return b; } 对应的TAC可能是：
BB1: b := 0 if a&amp;gt;10 goto BB3 //如果t是false(0),转到BB3 BB2: b := 10 goto BB4 BB3: b := a BB4: return b 可以看到，TAC用goto语句取代了if语句、循环语句这种比较高级的语句，当然也不会有类、继承这些高层的语言结构。但是，它又没有涉及数据如何在内存读写等细节，书写格式也不像汇编代码，与具体的目标代码也是独立的。</description></item><item><title>06_全局锁和表锁：给表加个字段怎么有这么多阻碍？</title><link>https://artisanbox.github.io/1/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/6/</guid><description>今天我要跟你聊聊MySQL的锁。数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。
根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类。今天这篇文章，我会和你分享全局锁和表级锁。而关于行锁的内容，我会留着在下一篇文章中再和你详细介绍。
这里需要说明的是，锁的设计比较复杂，这两篇文章不会涉及锁的具体实现细节，主要介绍的是碰到锁时的现象和其背后的原理。
全局锁顾名思义，全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。
全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。
以前有一种做法，是通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。
但是让整库都只读，听上去就很危险：
如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。 看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？我们来看一下不加锁会有什么问题。
假设你现在要维护“极客时间”的购买系统，关注的是用户账户余额表和用户课程表。
现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉他的余额，然后往已购课程里面加上一门课。
如果时间顺序上是先备份账户余额表(u_account)，然后用户购买，然后备份用户课程表(u_course)，会怎么样呢？你可以看一下这个图：
图1 业务和备份状态图可以看到，这个备份结果里，用户A的数据状态是“账户余额没扣，但是用户课程表里面已经多了一门课”。如果后面用这个备份来恢复数据的话，用户A就发现，自己赚了。
作为用户可别觉得这样可真好啊，你可以试想一下：如果备份表的顺序反过来，先备份用户课程表再备份账户余额表，又可能会出现什么结果？
也就是说，不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。
说到视图你肯定想起来了，我们在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视图的，对吧？
是的，就是在可重复读隔离级别下开启一个事务。
备注：如果你对事务隔离级别的概念不是很清晰的话，可以再回顾一下第3篇文章《事务隔离：为什么你改了我还看不见？》中的相关内容。
官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。
你一定在疑惑，有了这个功能，为什么还需要FTWRL呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。
所以，single-transaction方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。
你也许会问，既然要全库只读，为什么不使用set global readonly=true的方式呢？确实readonly方式也可以让全库进入只读状态，但我还是会建议你用FTWRL方式，主要有两个原因：
一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大，我不建议你使用。 二是，在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。 业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。
但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍的表级锁。
表级锁MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。
表锁的语法是 lock tables … read/write。与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
举个例子, 如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。
在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。
另一类表级的锁是MDL（metadata lock)。MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。
因此，在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。
读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。</description></item><item><title>06_外键和连接：如何做关联查询？</title><link>https://artisanbox.github.io/8/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/6/</guid><description>你好，我是朱晓峰。今天我来和你聊一聊关联查询的问题。
在实际的数据库应用开发过程中，我们经常需要把2个或2个以上的表进行关联，以获取需要的数据。这是因为，为了提高存取效率，我们会把不同业务模块的信息分别存放在不同的表里面。但是，从业务层面上看，我们需要完整全面的信息为经营决策提供数据支撑。
就拿咱们的超市项目来说，数据库里面的销售流水表一般只保存销售必需的信息（比如商品编号、数量、价格、金额和会员卡号等）。但是，在呈现给超市经营者的统计报表里面，只包括这些信息是不够的，比如商品编号、会员卡号，这些数字经营者就看不懂。因此，必须要从商品信息表提取出商品信息，从会员表中提取出会员的相关信息，这样才能形成一个完整的报表。这种把分散在多个不同的表里的数据查询出来的操作，就是多表查询。
不过，这种查询可不简单，我们需要建立起多个表之间的关联，然后才能去查询，同时还需要规避关联表查询中的常见错误。具体怎么做呢？我来借助实际的项目给你讲一讲。
在我们项目的进货模块，有这样2个数据表，分别是进货单头表（importhead）和进货单明细表（importdetails），我们每天都要对这两个表进行增删改查的操作。
进货单头表记录的是整个进货单的总体信息：
进货单明细表记录了每次进货的商品明细信息。一条进货单头数据记录，对应多条进货商品的明细数据，也就是所谓的一对多的关系。具体信息如下表所示：
现在我们需要查询一次进货的所有相关数据，包括进货单的总体信息和进货商品的明细，这样一来，我们就需要把2个表关联起来，那么，该怎么操作呢？
在MySQL中，为了把2个表关联起来，会用到2个重要的功能：外键（FOREIGN KEY）和连接（JOIN）。外键需要在创建表的阶段就定义；连接可以通过相同意义的字段把2个表连接起来，用在查询阶段。
接下来，我就先和你聊聊外键。
如何创建外键？我先来解释一下什么是外键。
假设我们有2个表，分别是表A和表B，它们通过一个公共字段“id”发生关联关系，我们把这个关联关系叫做R。如果“id”在表A中是主键，那么，表A就是这个关系R中的主表。相应的，表B就是这个关系中的从表，表B中的“id”，就是表B用来引用表A中数据的，叫外键。所以，外键就是从表中用来引用主表中数据的那个公共字段。
为了方便你理解，我画了一张图来展示：
如图所示，在关联关系R中，公众字段（字段A）是表A的主键，所以表A是主表，表B是从表。表B中的公共字段（字段A）是外键。
在MySQL中，外键是通过外键约束来定义的。外键约束就是约束的一种，它必须在从表中定义，包括指明哪个是外键字段，以及外键字段所引用的主表中的主键字段是什么。MySQL系统会根据外键约束的定义，监控对主表中数据的删除操作。如果发现要删除的主表记录，正在被从表中某条记录的外键字段所引用，MySQL就会提示错误，从而确保了关联数据不会缺失。
外键约束可以在创建表的时候定义，也可以通过修改表来定义。我们先来看看外键约束定义的语法结构：
[CONSTRAINT &amp;lt;外键约束名称&amp;gt;] FOREIGN KEY 字段名 REFERENCES &amp;lt;主表名&amp;gt; 字段名 你可以在创建表的时候定义外键约束：
CREATE TABLE 从表名 ( 字段名 类型, ... -- 定义外键约束，指出外键字段和参照的主表字段 CONSTRAINT 外键约束名 FOREIGN KEY (字段名) REFERENCES 主表名 (字段名) ) 当然，你也可以通过修改表来定义外键约束：
ALTER TABLE 从表名 ADD CONSTRAINT 约束名 FOREIGN KEY 字段名 REFERENCES 主表名 （字段名）; 一般情况下，表与表的关联都是提前设计好了的，因此，会在创建表的时候就把外键约束定义好。不过，如果需要修改表的设计（比如添加新的字段，增加新的关联关系），但没有预先定义外键约束，那么，就要用修改表的方式来补充定义。
下面，我就来讲一讲怎么创建外键约束。
先创建主表demo.importhead：
CREATE TABLE demo.importhead ( listnumber INT PRIMARY KEY, supplierid INT, stocknumber INT, importtype INT, importquantity DECIMAL(10 , 3 ), importvalue DECIMAL(10 , 2 ), recorder INT, recordingdate DATETIME ); 然后创建从表demo.</description></item><item><title>06_指令跳转：原来if...else就是goto</title><link>https://artisanbox.github.io/4/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/6/</guid><description>上一讲，我们讲解了一行代码是怎么变成计算机指令的。你平时写的程序中，肯定不只有int a = 1这样最最简单的代码或者指令。我们总是要用到if…else这样的条件判断语句、while和for这样的循环语句，还有函数或者过程调用。
对应的，CPU执行的也不只是一条指令，一般一个程序包含很多条指令。因为有if…else、for这样的条件和循环存在，这些指令也不会一路平铺直叙地执行下去。
今天我们就在上一节的基础上来看看，一个计算机程序是怎么被分解成一条条指令来执行的。
CPU是如何执行指令的？拿我们用的Intel CPU来说，里面差不多有几百亿个晶体管。实际上，一条条计算机指令执行起来非常复杂。好在CPU在软件层面已经为我们做好了封装。对于我们这些做软件的程序员来说，我们只要知道，写好的代码变成了指令之后，是一条一条顺序执行的就可以了。
我们先不管几百亿的晶体管的背后是怎么通过电路运转起来的，逻辑上，我们可以认为，CPU其实就是由一堆寄存器组成的。而寄存器就是CPU内部，由多个触发器（Flip-Flop）或者锁存器（Latches）组成的简单电路。
触发器和锁存器，其实就是两种不同原理的数字电路组成的逻辑门。这块内容并不是我们这节课的重点，所以你只要了解就好。如果想要深入学习的话，你可以学习数字电路的相关课程，这里我们不深入探讨。
好了，现在我们接着前面说。N个触发器或者锁存器，就可以组成一个N位（Bit）的寄存器，能够保存N位的数据。比方说，我们用的64位Intel服务器，寄存器就是64位的。
一个CPU里面会有很多种不同功能的寄存器。我这里给你介绍三种比较特殊的。
一个是PC寄存器（Program Counter Register），我们也叫指令地址寄存器（Instruction Address Register）。顾名思义，它就是用来存放下一条需要执行的计算机指令的内存地址。
第二个是指令寄存器（Instruction Register），用来存放当前正在执行的指令。
第三个是条件码寄存器（Status Register），用里面的一个一个标记位（Flag），存放CPU进行算术或者逻辑计算的结果。
除了这些特殊的寄存器，CPU里面还有更多用来存储数据和内存地址的寄存器。这样的寄存器通常一类里面不止一个。我们通常根据存放的数据内容来给它们取名字，比如整数寄存器、浮点数寄存器、向量寄存器和地址寄存器等等。有些寄存器既可以存放数据，又能存放地址，我们就叫它通用寄存器。
实际上，一个程序执行的时候，CPU会根据PC寄存器里的地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。可以看到，一个程序的一条条指令，在内存里面是连续保存的，也会一条条顺序加载。
而有些特殊指令，比如上一讲我们讲到J类指令，也就是跳转指令，会修改PC寄存器里面的地址值。这样，下一条要执行的指令就不是从内存里面顺序加载的了。事实上，这些跳转指令的存在，也是我们可以在写程序的时候，使用if…else条件语句和while/for循环语句的原因。
从if…else来看程序的执行和跳转我们现在就来看一个包含if…else的简单程序。
// test.c #include &amp;lt;time.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt;
int main() { srand(time(NULL)); int r = rand() % 2; int a = 10; if (r == 0) { a = 1; } else { a = 2; } 我们用rand生成了一个随机数r，r要么是0，要么是1。当r是0的时候，我们把之前定义的变量a设成1，不然就设成2。
$ gcc -g -c test.c $ objdump -d -M intel -S test.</description></item><item><title>06_编译器前端工具（一）：用Antlr生成词法、语法分析器</title><link>https://artisanbox.github.io/6/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/6/</guid><description>前面的课程中，我重点讲解了词法分析和语法分析，在例子中提到的词法和语法规则也是高度简化的。虽然这些内容便于理解原理，也能实现一个简单的原型，在实际应用中却远远不够。实际应用中，一个完善的编译程序还要在词法方面以及语法方面实现很多工作，我这里特意画了一张图，你可以直观地看一下。
如果让编译程序实现上面这么多工作，完全手写效率会有点儿低，那么我们有什么方法可以提升效率呢？答案是借助工具。
编译器前端工具有很多，比如Lex（以及GNU的版本Flex）、Yacc（以及GNU的版本Bison）、JavaCC等等。你可能会问了：“那为什么我们这节课只讲Antlr，不选别的工具呢？”主要有两个原因。
第一个原因是Antlr能支持更广泛的目标语言，包括Java、C#、JavaScript、Python、Go、C++、Swift。无论你用上面哪种语言，都可以用它生成词法和语法分析的功能。而我们就使用它生成了Java语言和C++语言两个版本的代码。
第二个原因是Antlr的语法更加简单。它能把类似左递归的一些常见难点在工具中解决，对提升工作效率有很大的帮助。这一点，你会在后面的课程中直观地感受到。
而我们今天的目标就是了解Antlr，然后能够使用Antlr生成词法分析器与语法分析器。在这个过程中，我还会带你借鉴成熟的词法和语法规则，让你快速成长。
接下来，我们先来了解一下Antlr这个工具。
初识AntlrAntlr是一个开源的工具，支持根据规则文件生成词法分析器和语法分析器，它自身是用Java实现的。
你可以下载Antlr工具，并根据说明做好配置。同时，你还需要配置好机器上的Java环境（可以在Oracle官网找到最新版本的JDK）。
因为我用的是Mac，所以我用macOS平台下的软件包管理工具Homebrew安装了Antlr，它可以自动设置好antlr和grun两个命令（antlr和grun分别是java org.antlr.v4.Tool和java org.antlr.v4.gui.TestRig这两个命令的别名）。这里需要注意的是，你要把Antlr的JAR文件设置到CLASSPATH环境变量中，以便顺利编译所生成的Java源代码。
GitHub上还有很多供参考的语法规则，你可以下载到本地硬盘随时查阅。
现在你已经对Antlr有了初步的了解，也知道如何安装它了。接下来，我带你实际用一用Antlr，让你用更轻松的方式生成词法分析器和语法分析器。
用Antlr生成词法分析器你可能对Antlr还不怎么熟悉，所以我会先带你使用前面课程中，你已经比较熟悉的那些词法规则，让Antlr生成一个新的词法分析器，然后再借鉴一些成熟的规则文件，把词法分析器提升到更加专业、实用的级别。
Antlr通过解析规则文件来生成编译器。规则文件以.g4结尾，词法规则和语法规则可以放在同一个文件里。不过为了清晰起见，我们还是把它们分成两个文件，先用一个文件编写词法规则。
为了让你快速进入状态，我们先做一个简单的练习预热一下。我们创建一个Hello.g4文件，用于保存词法规则，然后把之前用过的一些词法规则写进去。
lexer grammar Hello; //lexer关键字意味着这是一个词法规则文件，名称是Hello，要与文件名相同 //关键字 If : &amp;lsquo;if&amp;rsquo;; Int : &amp;lsquo;int&amp;rsquo;;
//字面量 IntLiteral: [0-9]+; StringLiteral: &amp;lsquo;&amp;quot;&amp;rsquo; .*? &amp;lsquo;&amp;quot;&amp;rsquo; ; //字符串字面量
//操作符 AssignmentOP: &amp;lsquo;=&amp;rsquo; ; RelationalOP: &amp;lsquo;&amp;gt;&amp;rsquo;|&amp;rsquo;&amp;gt;=&amp;rsquo;|&amp;rsquo;&amp;lt;&amp;rsquo; |&amp;rsquo;&amp;lt;=&amp;rsquo; ; Star: &amp;lsquo;*&amp;rsquo;; Plus: &amp;lsquo;+&amp;rsquo;; Sharp: &amp;lsquo;#&amp;rsquo;; SemiColon: &amp;lsquo;;&amp;rsquo;; Dot: &amp;lsquo;.&amp;rsquo;; Comm: &amp;lsquo;,&amp;rsquo;; LeftBracket : &amp;lsquo;[&amp;rsquo;; RightBracket: &amp;lsquo;]&amp;rsquo;; LeftBrace: &amp;lsquo;{&amp;rsquo;; RightBrace: &amp;lsquo;}&amp;rsquo;; LeftParen: &amp;lsquo;(&amp;rsquo;; RightParen: &amp;lsquo;)&amp;rsquo;;</description></item><item><title>06_虚幻与真实：程序中的地址如何转换？</title><link>https://artisanbox.github.io/9/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/6/</guid><description>你好，我是LMOS。
从前面的课程我们得知，CPU执行程序、处理数据都要和内存打交道，这个打交道的方式就是内存地址。
读取指令、读写数据都需要首先告诉内存芯片：hi，内存老哥请你把0x10000地址处的数据交给我……hi，内存老哥，我已经计算完成，请让我把结果写回0x200000地址的空间。这些地址存在于代码指令字段后的常数，或者存在于某个寄存器中。
今天，我们就来专门研究一下程序中的地址。说起程序中的地址，不知道你是否好奇过，为啥系统设计者要引入虚拟地址呢？
我会先带你从一个多程序并发的场景热身，一起思考这会导致哪些问题，为什么能用虚拟地址解决这些问题。
搞懂原理之后，我还会带你一起探索虚拟地址和物理地址的关系和转换机制。在后面的课里，你会发现，我们最宝贵的内存资源正是通过这些机制来管理的。
从一个多程序并发的场景说起设想一下，如果一台计算机的内存中只运行一个程序A，这种方式正好用前面CPU的实模式来运行，因为程序A的地址在链接时就可以确定，例如从内存地址0x8000开始，每次运行程序A都装入内存0x8000地址处开始运行，没有其它程序干扰。
现在改变一下，内存中又放一道程序B，程序A和程序B各自运行一秒钟，如此循环，直到其中之一结束。这个新场景下就会产生一些问题，当然这里我们只关心内存相关的这几个核心问题。
1.谁来保证程序A跟程序B 没有内存地址的冲突？换句话说，就是程序A、B各自放在什么内存地址，这个问题是由A、B程序协商，还是由操作系统决定。
2.怎样保证程序A跟程序B 不会互相读写各自的内存空间？这个问题相对简单，用保护模式就能解决。
3.如何解决内存容量问题？程序A和程序B，在不断开发迭代中程序代码占用的空间会越来越大，导致内存装不下。
4.还要考虑一个扩展后的复杂情况，如果不只程序A、B，还可能有程序C、D、E、F、G……它们分别由不同的公司开发，而每台计算机的内存容量不同。这时候，又对我们的内存方案有怎样的影响呢？
要想完美地解决以上最核心的4个问题，一个较好的方案是：让所有的程序都各自享有一个从0开始到最大地址的空间，这个地址空间是独立的，是该程序私有的，其它程序既看不到，也不能访问该地址空间，这个地址空间和其它程序无关，和具体的计算机也无关。
事实上，计算机科学家们早就这么做了，这个方案就是虚拟地址，下面我们就来看看它。
虚拟地址正如其名，这个地址是虚拟的，自然而然地和具体环境进行了解耦，这个环境包括系统软件环境和硬件环境。
虚拟地址是逻辑上存在的一个数据值，比如0~100就有101个整数值，这个0~100的区间就可以说是一个虚拟地址空间，该虚拟地址空间有101个地址。
我们再来看看最开始Hello World的例子，我们用objdump工具反汇编一下Hello World二进制文件，就会得到如下的代码片段：
00000000000004e8 &amp;lt;_init&amp;gt;: 4e8: 48 83 ec 08 sub $0x8,%rsp 4ec: 48 8b 05 f5 0a 20 00 mov 0x200af5(%rip),%rax # 200fe8 &amp;lt;__gmon_start__&amp;gt; 4f3: 48 85 c0 test %rax,%rax 4f6: 74 02 je 4fa &amp;lt;_init+0x12&amp;gt; 4f8: ff d0 callq *%rax 4fa: 48 83 c4 08 add $0x8,%rsp 4fe: c3 retq 上述代码中，左边第一列数据就是虚拟地址，第三列中是程序指令，如：“mov 0x200af5(%rip),%rax，je 4fa，callq *%rax”指令中的数据都是虚拟地址。</description></item><item><title>06_链表（上）：如何实现LRU缓存淘汰算法</title><link>https://artisanbox.github.io/2/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/7/</guid><description>今天我们来聊聊“链表（Linked list）”这个数据结构。学习链表有什么用呢？为了回答这个问题，我们先来讨论一个经典的链表应用场景，那就是LRU缓存淘汰算法。
缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的CPU缓存、数据库缓存、浏览器缓存等等。
缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略FIFO（First In，First Out）、最少使用策略LFU（Least Frequently Used）、最近最少使用策略LRU（Least Recently Used）。
这些策略你不用死记，我打个比方你很容易就明白了。假如说，你买了很多本技术书，但有一天你发现，这些书太多了，太占书房空间了，你要做个大扫除，扔掉一些书籍。那这个时候，你会选择扔掉哪些书呢？对应一下，你的选择标准是不是和上面的三种策略神似呢？
好了，回到正题，我们今天的开篇问题就是：如何用链表来实现LRU缓存淘汰策略呢？ 带着这个问题，我们开始今天的内容吧！
五花八门的链表结构相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍难一些。这两个非常基础、非常常用的数据结构，我们常常会放到一块儿来比较。所以我们先来看，这两者有什么区别。
我们先从底层的存储结构上来看一看。
为了直观地对比，我画了一张图。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个100MB大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于100MB，仍然会申请失败。
而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是100MB大小的链表，根本不会有问题。
链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。我们首先来看最简单、最常用的单链表。
我们刚刚讲到，链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针next。
从我画的单链表图中，你应该可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址NULL，表示这是链表上最后一个结点。
与数组一样，链表也支持数据的查找、插入和删除操作。
我们知道，在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。
为了方便你理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是O(1)。
但是，有利就有弊。链表要想随机访问第k个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。
你可以把链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第k位的人是谁的时候，我们就需要从第一个人开始，一个一个地往下数。所以，链表随机访问的性能没有数组好，需要O(n)的时间复杂度。
好了，单链表我们就简单介绍完了，接着来看另外两个复杂的升级版，循环链表和双向链表。
循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。
和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。
单链表和循环链表是不是都不难？接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。
单向链表只有一个方向，结点只有一个后继指针next指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。
从我画的图中可以看出来，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。那相比单链表，双向链表适合解决哪种问题呢？
从结构上来看，双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。
你可能会说，我刚讲到单链表的插入、删除操作的时间复杂度已经是O(1)了，双向链表还能再怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。我再来带你分析一下链表的两个操作。
我们先来看删除操作。
在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：
删除结点中“值等于某个给定值”的结点；
删除给定指针指向的结点。
对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。
尽管单纯的删除操作时间复杂度是O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。
对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点q需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到p-&amp;gt;next=q，说明p是q的前驱结点。
但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定了！
同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。你可以参照我刚刚讲过的删除操作自己分析一下。
除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。
现在，你有没有觉得双向链表要比单链表更加高效呢？这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉Java语言，你肯定用过LinkedHashMap这个容器。如果你深入研究LinkedHashMap的实现原理，就会发现其中就用到了双向链表这种数据结构。
实际上，这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。
还是开篇缓存的例子。缓存实际上就是利用了空间换时间的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。
所以我总结一下，对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？
了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不用我多讲，你应该知道双向循环链表长什么样子了吧？你可以自己试着在纸上画一画。
链表VS数组性能大比拼通过前面内容的学习，你应该已经知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。
不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。
数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存不友好，没办法有效预读。
数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。
你可能会说，我们Java中的ArrayList容器，也可以支持动态扩容啊？我们上一节课讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。
我举一个稍微极端的例子。如果我们用ArrayList存储了了1GB大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList会申请一个1.5GB大小的存储空间，并且把原来那1GB的数据拷贝到新申请的空间上。听起来是不是就很耗时？
除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是Java语言，就有可能会导致频繁的GC（Garbage Collection，垃圾回收）。
所以，在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。
解答开篇好了，关于链表的知识我们就讲完了。我们现在回过头来看下开篇留给你的思考题。如何基于链表实现LRU缓存淘汰算法？
我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。
1.如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
2.如果此数据没有在缓存链表中，又可以分为两种情况：
如果此时缓存未满，则将此结点直接插入到链表的头部；</description></item><item><title>06｜怎么支持条件语句和循环语句？</title><link>https://artisanbox.github.io/3/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/8/</guid><description>你好，我是宫文学。
我们现在的语言已经支持表达式、变量和函数了。可是你发现没有，到现在为止，我们还没有支持流程控制类的语句，比如条件语句和循环语句。如果再加上这两类语句的话，我们的语言就能做很复杂的事情了，甚至你会觉得它已经是一门比较完整的语言了。
那么今天，我们就来加上条件语句和循环语句。在这个过程中，我们会加深对作用域和栈桢的理解，包括跨作用域的变量引用、词法作用域的概念，以及如何在运行时访问其他作用域的变量。这些知识点会帮助你加深对计算机语言的运行机制的理解。
而这些理解和认知，会有助于我们后面把基于AST的解释器升级成基于字节码的解释器，也有助于我们理解编译成机器码后的运行时机制。
好了，首先我们先从语法层面支持一下这两种语句。
语法分析：支持一元表达式按照惯例，我们首先要写下新的语法规则，然后使用LL算法来升级语法分析程序。新的语法规则如下：
ifStatement &amp;nbsp; &amp;nbsp; : If '(' expression ')' statement (Else statement)? &amp;nbsp; &amp;nbsp; ; forStatement :For '(' expression? ';' expression? ';' expression? ')' statement ; statement: : block | functionDecl | varaibleStatement | expressionStatement | returnStatement | ifStatement | forStatement | emptyStatement ; 你从上面的语法可以得到这几个信息：
首先，if语句中，else部分是可选的。这样，我们在解析完if条件后面的语句以后，要去看看后面跟着的是不是’else’关键字，从而决定是否解析else后面的语句块。更具体的你可以参见parseIfStatement函数的代码。
第二，在for循环语句中，for括号里用分号分割的三个表达式都是可选的，在解析的时候也要根据Follow集合来判断是否需要解析这三个表达式。这点你具体可以参见parseForStatement函数的代码。
最后，从statement的语法规则中，我们也可以发现，我们的语言所支持的语句越来越多了，这也使得语言特性越来越丰富了。
现在，升级我们的语法解析程序，对你来说已经没有太大的困难了，你可以参照我的参考实现动手自己做一下。
不过，为了实现for语句，我们还有一个语言特性需要升级一下，这就是对一元运算的支持。
哪些是一元运算呢？比如，在for语句中，我们经常会使用下面的写法：
for(i = 0; i&amp;lt; 10; i++) 其中i++就是使用了一元运算。在这里，为了方便，我们干脆就让程序支持所有的一元运算！
一元运算符除了++以外，还有–、~、!等。甚至还有更复杂一点的情况，+号和-号除了作为二元运算符以外，还可以作为一元运算符使用，比如下面这个例子：
myAge = +myAge + -10; 你甚至可以将多个一元运算符叠加使用，比如我们把上面的例子修改一下，仍然和原来的计算结果相同：</description></item><item><title>07_Cache与内存：程序放在哪儿？</title><link>https://artisanbox.github.io/9/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/7/</guid><description>你好，我是LMOS。
在前面的课程里，我们已经知道了CPU是如何执行程序的，也研究了程序的地址空间，这里我们终于到了程序的存放地点——内存。
你知道什么是Cache吗？在你心中，真实的内存又是什么样子呢？今天我们就来重新认识一下Cache和内存，这对我们利用Cache写出高性能的程序代码和实现操作系统管理内存，有着巨大的帮助。
通过这节课的内容，我们一起来看看内存到底是啥，它有什么特性。有了这个认识，你就能更加深入地理解我们看似熟悉的局部性原理，从而搞清楚，为啥Cache是解决内存瓶颈的神来之笔。最后，我还会带你分析x86平台上的Cache，规避Cache引发的一致性问题，并让你掌握获取内存视图的方法。
那话不多说，带着刚才的问题，我们正式进入今天的学习吧！
从一段“经典”代码看局部性原理不知道，你还记不记得C语言打印九九乘法表的代码，想不起来也没关系，下面我把它贴出来，代码很短，也很简单，就算你自己写一个也用不了一分钟，如下所示。
#include &amp;lt;stdio.h&amp;gt; int main(){ int i,j; for(i=1;i&amp;lt;=9;i++){ for(j=1;j&amp;lt;=i;j++){ printf(&amp;quot;%d*%d=%2d &amp;quot;,i,j,i*j); } printf(&amp;quot;\n&amp;quot;); } return 0; } 我们当然不是为了研究代码本身，这个代码非常简单，这里我们主要是观察这个结构，代码的结构主要是顺序、分支、循环，这三种结构可以写出现存所有算法的程序。
我们常规情况下写的代码是顺序和循环结构居多。上面的代码中有两重循环，内层循环的次数受到外层循环变量的影响。就是这么简单，但是越简单的东西越容易看到本质。
可以看到，这个代码大数时间在执行一个乘法计算和调用一个printf函数，而程序一旦编译装载进内存中，它的地址就确定了。也就是说，CPU大多数时间在访问相同或者与此相邻的地址，换句话说就是：CPU大多数时间在执行相同的指令或者与此相邻的指令。这就是大名鼎鼎的程序局部性原理。
内存明白了程序的局部性原理之后，我们再来看看内存。你或许感觉这跨越有点大，但是只有明白了内存的结构和特性，你才能明白程序局部性原理的应用场景和它的重要性。
内存也可称为主存，不管硬盘多大、里面存放了多少程序和数据，只要程序运行或者数据要进行计算处理，就必须先将它们装入内存。我们先来看看内存长什么样（你也可以上网自行搜索），如下图所示。
从上图可以看到在PCB板上有内存颗粒芯片，主要是用来存放数据的。SPD芯片用于存放内存自身的容量、频率、厂商等信息。还有最显眼的金手指，用于连接数据总线和地址总线，电源等。
其实从专业角度讲，内存应该叫DRAM，即动态随机存储器。内存储存颗粒芯片中的存储单元是由电容和相关元件做成的，电容存储电荷的多、少代表数字信号0和1。
而随着时间的流逝，电容存在漏电现象，这导致电荷不足，就会让存储单元的数据出错，所以DRAM需要周期性刷新，以保持电荷状态。DRAM结构较简单且集成度很高，通常用于制造内存条中的储存颗粒芯片。
虽然内存技术标准不断更新，但是储存颗粒的内部结构没有本质改变，还是电容存放电荷，标准看似更多，实际上只是提升了位宽、工作频率，以及传输时预取的数据位数。
比如DDR SDRAM，即双倍速率同步动态随机存储器，它使用2.5V的工作电压，数据位宽为64位，核心频率最高为166MHz。下面简称DDR内存，它表示每一个时钟脉冲传输两次数据，分别在时钟脉冲的上升沿和下降沿各传输一次数据，因此称为双倍速率的SDRAM。
后来的DDR2、DDR3、DDR4也都在核心频率和预取位数上做了提升。最新的DDR4采用1.2V工作电压，数据位宽为64位，预取16位数据。DDR4取消了双通道机制，一条内存即为一条通道，工作频率最高可达4266MHz，单根DDR4内存的数据传输带宽最高为34GB/s。
其实我们无需过多关注内存硬件层面的技术规格标准，重点需要关注的是，内存的速度还有逻辑上内存和系统的连接方式和结构，这样你就能意识到内存有多慢，还有是什么原因导致内存慢的。
我们还是画幅图说明吧，如下图所示。
结合图片我们看到，控制内存刷新和内存读写的是内存控制器，而内存控制器集成在北桥芯片中。传统方式下，北桥芯片存在于系统主板上，而现在由于芯片制造工艺的升级，芯片集成度越来越高，所以北桥芯片被就集成到CPU芯片中了，同时这也大大提升了CPU访问内存的性能。
而作为软件开发人员，从逻辑上我们只需要把内存看成一个巨大的字节数组就可以，而内存地址就是这个数组的下标。
CPU到内存的性能瓶颈尽管CPU和内存是同时代发展的，但CPU所使用技术工艺的材料和内存是不同的，侧重点也不同，价格也不同。如果内存使用CPU的工艺和材料制造，那内存条的昂贵程度会超乎想象，没有多少人能买得起。
由于这些不同，导致了CPU和内存条的数据吞吐量天差地别。尽管最新的DDR4内存条带宽高达34GB/s，然而这相比CPU的数据吞吐量要慢上几个数量级。再加上多核心CPU同时访问内存，会导致总线争用问题，数据吞吐量会进一步下降。
CPU要数据，内存一时给不了怎么办？CPU就得等，通常CPU会让总线插入等待时钟周期，直到内存准备好，到这里你就会发现，无论CPU的性能多高都没用，而内存才是决定系统整体性能的关键。显然依靠目前的理论直接提升内存性能，达到CPU的同等水平，这是不可行的，得想其它的办法。
Cache让我们重新回到前面的场景中，回到程序的局部性原理，它告诉我们：CPU大多数时间在访问相同或者与此相邻的地址。那么，我们立马就可以想到用一块小而快的储存器，放在CPU和内存之间，就可以利用程序的局部性原理来缓解CPU和内存之间的性能瓶颈。这块小而快的储存器就是Cache，即高速缓存。
Cache中存放了内存中的一部分数据，CPU在访问内存时要先访问Cache，若Cache中有需要的数据就直接从Cache中取出，若没有则需要从内存中读取数据，并同时把这块数据放入Cache中。但是由于程序的局部性原理，在一段时间内，CPU总是能从Cache中读取到自己想要的数据。
Cache可以集成在CPU内部，也可以做成独立的芯片放在总线上，现在x86 CPU和ARM CPU都是集成在CPU内部的。其逻辑结构如下图所示。
Cache主要由高速的静态储存器、地址转换模块和Cache行替换模块组成。
Cache会把自己的高速静态储存器和内存分成大小相同的行，一行大小通常为32字节或者64字节。Cache和内存交换数据的最小单位是一行，为方便管理，在Cache内部的高速储存器中，多个行又会形成一组。
除了正常的数据空间外，Cache行中还有一些标志位，如脏位、回写位，访问位等，这些位会被Cache的替换模块所使用。
Cache大致的逻辑工作流程如下。
1.CPU发出的地址由Cache的地址转换模块分成3段：组号，行号，行内偏移。
2.Cache会根据组号、行号查找高速静态储存器中对应的行。如果找到即命中，用行内偏移读取并返回数据给CPU，否则就分配一个新行并访问内存，把内存中对应的数据加载到Cache行并返回给CPU。写入操作则比较直接，分为回写和直通写，回写是写入对应的Cache行就结束了，直通写则是在写入Cache行的同时写入内存。
3.如果没有新行了，就要进入行替换逻辑，即找出一个Cache行写回内存，腾出空间，替换行有相关的算法，替换算法是为了让替换的代价最小化。例如，找出一个没有修改的Cache行，这样就不用把它其中的数据回写到内存中了，还有找出存在时间最久远的那个Cache行，因为它大概率不会再访问了。
以上这些逻辑都由Cache硬件独立实现，软件不用做任何工作，对软件是透明的。
Cache带来的问题Cache虽然带来性能方面的提升，但同时也给和硬件和软件开发带来了问题，那就是数据一致性问题。
为了搞清楚这个问题，我们必须先搞清楚Cache在硬件层面的结构，下面我画了x86 CPU的Cache结构图：
这是一颗最简单的双核心CPU，它有三级Cache，第一级Cache是指令和数据分开的，第二级Cache是独立于CPU核心的，第三级Cache是所有CPU核心共享的。
下面来看看Cache的一致性问题，主要包括这三个方面.
1.一个CPU核心中的指令Cache和数据Cache的一致性问题。
2.多个CPU核心各自的2级Cache的一致性问题。
3.CPU的3级Cache与设备内存，如DMA、网卡帧储存，显存之间的一致性问题。这里我们不需要关注这个问题。
我们先来看看CPU核心中的指令Cache和数据Cache的一致性问题，对于程序代码运行而言，指令都是经过指令Cache，而指令中涉及到的数据则会经过数据Cache。
所以，对自修改的代码（即修改运行中代码指令数据，变成新的程序）而言，比如我们修改了内存地址A这个位置的代码（典型的情况是Java运行时编译器），这个时候我们是通过储存的方式去写的地址A，所以新的指令会进入数据Cache。
但是我们接下来去执行地址A处的指令的时候，指令Cache里面可能命中的是修改之前的指令。所以，这个时候软件需要把数据Cache中的数据写入到内存中，然后让指令Cache无效，重新加载内存中的数据。
再来看看多个CPU核心各自的2级Cache的一致性问题。从上图中可以发现，两个CPU核心共享了一个3级Cache。比如第一个CPU核心读取了一个A地址处的变量，第二个CPU也读取A地址处的变量，那么第二个CPU核心是不是需要从内存里面经过第3、2、1级Cache再读一遍，这个显然是没有必要的。
在硬件上Cache相关的控制单元，可以把第一个CPU核心的A地址处Cache内容直接复制到第二个CPU的第2、1级Cache，这样两个CPU核心都得到了A地址的数据。不过如果这时第一个CPU核心改写了A地址处的数据，而第二个CPU核心的2级Cache里面还是原来的值，数据显然就不一致了。
为了解决这些问题，硬件工程师们开发了多种协议，典型的多核心Cache数据同步协议有MESI和MOESI。MOESI和MESI大同小异，下面我们就去研究一下MESI协议。
Cache的MESI协议MESI协议定义了4种基本状态：M、E、S、I，即修改（Modified）、独占（Exclusive）、共享（Shared）和无效（Invalid）。下面我结合示意图，给你解释一下这四种状态。</description></item><item><title>07_代码优化：跟编译器做朋友，让你的代码飞起来</title><link>https://artisanbox.github.io/7/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/7/</guid><description>你好，我是宫文学。
一门语言的性能高低，是它能否成功的关键。拿JavaScript来说，十多年来，它的性能多次得到成倍的提升，这也是前端技术栈如此丰富和强大的根本原因。
因此，编译器会无所不用其极地做优化，而优化工作在编译器的运行时间中，也占据了很大的比例。
不过，对编译技术的初学者来说，通常会搞不清楚编译器到底做了哪些优化，这些优化的实现思路又是怎样的。
所以今天这一讲，我就重点给你普及下编译器所做的优化工作，及其工作原理。在这个过程中，你还会弄明白很多似曾相识的术语，比如在前端必须了解的AST、终结符、非终结符等，在中后端必须熟悉的常数折叠、值编号、公共子表达式消除等。只有这样，你才算是入门了。
首先，我带你认识一些常见的代码优化方法。
常见的代码优化方法对代码做优化的方法有很多。如果要把它们分一下类的话，可以按照下面两个维度：
第一个分类维度，是机器无关的优化与机器相关的优化。机器无关的优化与硬件特征无关，比如把常数值在编译期计算出来（常数折叠）。而机器相关的优化则需要利用某硬件特有的特征，比如SIMD指令可以在一条指令里完成多个数据的计算。 第二个分类维度，是优化的范围。本地优化是针对一个基本块中的代码，全局优化是针对整个函数（或过程），过程间优化则能够跨越多个函数（或过程）做优化。 但优化算法很多，仅仅按照这两个维度分类，仍显粗糙。所以，我就按照优化的实现思路再分分类，让你了解起来更轻松一些。
思路1：把常量提前计算出来程序里的有些表达式，肯定能计算出一个常数值，那就不要等到运行时再去计算，干脆在编译期就计算出来。比如 “x=2*3”可以优化成“x=6”。这种优化方法，叫做常数折叠（Constant Folding）。
而如果你一旦知道x的值其实是一个常量，那你就可以把所有用到x的地方，替换成这个常量，这叫做常数传播（Constant Propagation）。如果有“y=x*2”这样一个语句，那么就能计算出来“y=12”。所以说，常数传播会导致更多的常数折叠。
就算不能引起新的常数折叠，比如说“z=a+x”，替换成“z=a+6”以后，计算速度也会更快。因为对于很多CPU来说，“a+x”和“a+6”对应的指令是不一样的。前者可能要生成两条指令（比如先把a放到寄存器上，再把x加上去），而后者用一条指令就行了，因为常数可以作为操作数。
更有用的是，常数传播可能导致分支判断条件是常量，因此导致一个分支的代码不需要被执行。这种优化叫做稀疏有条件的常数传播（Sparse Conditional Constant Propagation）。
a = 2 b = 3 if(a&amp;lt;b){ //判断语句去掉 ... //直接执行这个代码块 } else{ ... //else分支会去掉 } 思路2：用低代价的方法做计算完成相同的计算，可以用代价更低的方法。比如“x=x+0”这行代码，操作前后x没有任何变化，所以这样的代码可以删掉；又比如“x=x*0” 可以简化成“x=0”。这类利用代数运算的规则所做的简化，叫做代数简化（Algebra Simplification）。
对于很多CPU来说，乘法运算改成移位运算，速度会更快。比如，“x*2”等价于“x&amp;lt;&amp;lt;1”，“x*9”等价于“x&amp;lt;&amp;lt;3+x”。这种采用代价更低的运算的方法，也叫做强度折减（Strength Reduction）。
思路3：消除重复的计算下面的示例代码中，第三行可以被替换成“z:=2*x”， 因为y的值就等于x。这个时候，可能x的值已经在寄存器中，所以直接采用x，运算速度会更快。这种优化叫做拷贝传播（Copy Propagation）。
x := a + b y := x z := 2 * y 值编号（Value Numbering）也能减少重复计算。值编号是把相同的值，在系统里给一个相同的编号，并且只计算一次即可。比如，Wikipedia上的这个案例：
w := 3 x := 3 y := x + 4 z := w + 4 其中w和x的值是一样的，因此编号是相同的。这会进一步导致y和z的编号也是相同的。进而，它们可以简化成：</description></item><item><title>07_函数调用：为什么会发生stackoverflow？</title><link>https://artisanbox.github.io/4/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/7/</guid><description>在开发软件的过程中我们经常会遇到错误，如果你用Google搜过出错信息，那你多少应该都访问过Stack Overflow这个网站。作为全球最大的程序员问答网站，Stack Overflow的名字来自于一个常见的报错，就是栈溢出（stack overflow）。
今天，我们就从程序的函数调用开始，讲讲函数间的相互调用，在计算机指令层面是怎么实现的，以及什么情况下会发生栈溢出这个错误。
为什么我们需要程序栈？和前面几讲一样，我们还是从一个非常简单的C程序function_example.c看起。
// function_example.c #include &amp;lt;stdio.h&amp;gt; int static add(int a, int b) { return a+b; } int main() { int x = 5; int y = 10; int u = add(x, y); } 这个程序定义了一个简单的函数add，接受两个参数a和b，返回值就是a+b。而main函数里则定义了两个变量x和y，然后通过调用这个add函数，来计算u=x+y，最后把u的数值打印出来。
$ gcc -g -c function_example.c $ objdump -d -M intel -S function_example.o 我们把这个程序编译之后，objdump出来。我们来看一看对应的汇编代码。
int static add(int a, int b) { 0: 55 push rbp 1: 48 89 e5 mov rbp,rsp 4: 89 7d fc mov DWORD PTR [rbp-0x4],edi 7: 89 75 f8 mov DWORD PTR [rbp-0x8],esi return a+b; a: 8b 55 fc mov edx,DWORD PTR [rbp-0x4] d: 8b 45 f8 mov eax,DWORD PTR [rbp-0x8] 10: 01 d0 add eax,edx } 12: 5d pop rbp 13: c3 ret 0000000000000014 &amp;lt;main&amp;gt;: int main() { 14: 55 push rbp 15: 48 89 e5 mov rbp,rsp 18: 48 83 ec 10 sub rsp,0x10 int x = 5; 1c: c7 45 fc 05 00 00 00 mov DWORD PTR [rbp-0x4],0x5 int y = 10; 23: c7 45 f8 0a 00 00 00 mov DWORD PTR [rbp-0x8],0xa int u = add(x, y); 2a: 8b 55 f8 mov edx,DWORD PTR [rbp-0x8] 2d: 8b 45 fc mov eax,DWORD PTR [rbp-0x4] 30: 89 d6 mov esi,edx 32: 89 c7 mov edi,eax 34: e8 c7 ff ff ff call 0 &amp;lt;add&amp;gt; 39: 89 45 f4 mov DWORD PTR [rbp-0xc],eax 3c: b8 00 00 00 00 mov eax,0x0 } 41: c9 leave</description></item><item><title>07_条件语句：WHERE与HAVING有什么不同</title><link>https://artisanbox.github.io/8/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/7/</guid><description>你好，我是朱晓峰。
我们在进行查询的时候，经常需要按条件对查询结果进行筛选，这就要用到条件语句WHERE和HAVING了。
WHERE是直接对表中的字段进行限定，来筛选结果；HAVING则需要跟分组关键字GROUP BY一起使用，通过对分组字段或分组计算函数进行限定，来筛选结果。虽然它们都是对查询进行限定，却有着各自的特点和适用场景。很多时候，我们会遇到2个都可以用的情况。一旦用错，就很容易出现执行效率低下、查询结果错误，甚至是查询无法运行的情况。
下面我就借助项目实施过程中的实际需求，给你讲讲WHERE和HAVING分别是如何对查询结果进行筛选的，以及它们各自的优缺点，来帮助你正确地使用它们，使你的查询不仅能够得到正确的结果，还能占用更少的资源，并且速度更快。
一个实际查询需求超市的经营者提出，要查单笔销售金额超过50元的商品。我们来分析一下这个需求：需要查询出一个商品记录集，限定条件是单笔销售金额超过50元。这个时候，我们就需要用到WHERE和HAVING了。
这个问题的条件很明确，查询的结果也只有“商品”一个字段，好像很容易实现。
假设我们有一个这样的商品信息表（demo.goodsmaster），里面有2种商品：书和笔。
mysql&amp;gt; SELECT * -&amp;gt; FROM demo.goodsmaster; +------------+---------+-----------+---------------+------+------------+ | itemnumber | barcode | goodsname | specification | unit | salesprice | +------------+---------+-----------+---------------+------+------------+ | 1 | 0001 | 书 | | 本 | 89.00 | | 2 | 0002 | 笔 | | 支 | 5.00 | +------------+---------+-----------+---------------+------+------------+ 2 rows in set (0.00 sec) 同时，我们还有一个商品销售明细表（demo.transactiondetails），里面有4条销售记录：
mysql&amp;gt; SELECT * -&amp;gt; FROM demo.transactiondetails; +---------------+------------+----------+-------+------------+ | transactionid | itemnumber | quantity | price | salesvalue | +---------------+------------+----------+-------+------------+ | 1 | 1 | 1.</description></item><item><title>07_编译器前端工具（二）：用Antlr重构脚本语言</title><link>https://artisanbox.github.io/6/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/7/</guid><description>上一讲，我带你用Antlr生成了词法分析器和语法分析器，也带你分析了，跟一门成熟的语言相比，在词法规则和语法规则方面要做的一些工作。
在词法方面，我们参考Java的词法规则文件，形成了一个CommonLexer.g4词法文件。在这个过程中，我们研究了更完善的字符串字面量的词法规则，还讲到要通过规则声明的前后顺序来解决优先级问题，比如关键字的规则一定要在标识符的前面。
目前来讲，我们已经完善了词法规则，所以今天我们来补充和完善一下语法规则，看一看怎样用最高效的速度，完善语法功能。比如一天之内，我们是否能为某个需要编译技术的项目实现一个可行性原型？
而且，我还会带你熟悉一下常见语法设计的最佳实践。这样当后面的项目需要编译技术做支撑时，你就会很快上手，做出成绩了！
接下来，我们先把表达式的语法规则梳理一遍，让它达到成熟语言的级别，然后再把语句梳理一遍，包括前面几乎没有讲过的流程控制语句。最后再升级解释器，用Visitor模式实现对AST的访问，这样我们的代码会更清晰，更容易维护了。
好了，让我们正式进入课程，先将表达式的语法完善一下吧！
完善表达式（Expression）的语法在“06 | 编译器前端工具（一）：用Antlr生成词法、语法分析器”中，我提到Antlr能自动处理左递归的问题，所以在写表达式时，我们可以大胆地写成左递归的形式，节省时间。
但这样，我们还是要为每个运算写一个规则，逻辑运算写完了要写加法运算，加法运算写完了写乘法运算，这样才能实现对优先级的支持，还是有些麻烦。
其实，Antlr能进一步地帮助我们。我们可以把所有的运算都用一个语法规则来涵盖，然后用最简洁的方式支持表达式的优先级和结合性。在我建立的PlayScript.g4语法规则文件中，只用了一小段代码就将所有的表达式规则描述完了：
expression : primary | expression bop='.' ( IDENTIFIER | functionCall | THIS ) | expression '[' expression ']' | functionCall | expression postfix=('++' | '--') | prefix=('+'|'-'|'++'|'--') expression | prefix=('~'|'!') expression | expression bop=('*'|'/'|'%') expression | expression bop=('+'|'-') expression | expression ('&amp;lt;' '&amp;lt;' | '&amp;gt;' '&amp;gt;' '&amp;gt;' | '&amp;gt;' '&amp;gt;') expression | expression bop=('&amp;lt;=' | '&amp;gt;=' | '&amp;gt;' | '&amp;lt;') expression | expression bop=INSTANCEOF typeType | expression bop=('==' | '!</description></item><item><title>07_行锁功过：怎么减少行锁对性能的影响？</title><link>https://artisanbox.github.io/1/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/7/</guid><description>在上一篇文章中，我跟你介绍了MySQL的全局锁和表级锁，今天我们就来讲讲MySQL的行锁。
MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。
我们今天就主要来聊聊InnoDB的行锁，以及如何通过减少锁冲突来提升业务并发度。
顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务A更新了一行，而这时候事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。
当然，数据库中还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导致程序出现非预期行为，比如两阶段锁。
从两阶段锁说起我先给你举个例子。在下面的操作序列中，事务B的update语句执行时会是什么现象呢？假设字段id是表t的主键。
这个问题的结论取决于事务A在执行完两条update语句后，持有哪些锁，以及在什么时候释放。你可以验证一下：实际上事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。
知道了这个答案，你一定知道了事务A持有的两个记录的行锁，都是在commit的时候才释放的。
也就是说，在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。
知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。我给你举个例子。
假设你负责实现一个电影票在线交易业务，顾客A要在影院B购买电影票。我们简化一点，这个业务需要涉及到以下操作：
从顾客A账户余额中扣除电影票价；
给影院B的账户余额增加这张电影票价；
记录一条交易日志。
也就是说，要完成这个交易，我们需要update两条记录，并insert一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？
试想如果同时有另外一个顾客C要在影院B买票，那么这两个事务冲突的部分就是语句2了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。
根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。
好了，现在由于你的正确设计，影院余额这一行的行锁在一个事务中不会停留很长时间。但是，这并没有完全解决你的困扰。
如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的MySQL就挂了。你登上服务器一看，CPU消耗接近100%，但整个数据库每秒就执行不到100个事务。这是什么原因呢？
这里，我就要说到死锁和死锁检测了。
死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。
这时候，事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：
一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。 在InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。
但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。
所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。
你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。
那如果是我们上面说到的所有事务都要更新同一行的场景呢？
每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到CPU利用率很高，但是每秒却执行不了几个事务。
根据上面的分析，我们来讨论一下，怎么解决由这种热点行更新导致的性能问题呢？问题的症结在于，死锁检测要耗费大量的CPU资源。
一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。
另一个思路是控制并发度。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有10个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有600个客户端，这样即使每个客户端控制到只有5个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到3000。
因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改MySQL源码的人，也可以做在MySQL里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。
可能你会问，如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？
你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗。
这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成0的时候，代码要有特殊处理。
小结今天，我和你介绍了MySQL的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。
其中，我以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句。这里的原则/我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。
但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事务量。
最后，我给你留下一个问题吧。如果你要删除一个表里面的前10000行数据，有以下三种方法可以做到：
第一种，直接执行delete from T limit 10000; 第二种，在一个连接中循环执行20次 delete from T limit 500; 第三种，在20个连接中同时执行delete from T limit 500。 你会选择哪一种方法呢？为什么呢？
你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾和你讨论这个问题。感谢你的收听，也欢迎你把这篇文章分享给更多的朋友一起阅读。</description></item><item><title>07_链表（下）：如何轻松写出正确的链表代码？</title><link>https://artisanbox.github.io/2/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/8/</guid><description>上一节我讲了链表相关的基础知识。学完之后，我看到有人留言说，基础知识我都掌握了，但是写链表代码还是很费劲。哈哈，的确是这样的！
想要写好链表代码并不是容易的事儿，尤其是那些复杂的链表操作，比如链表反转、有序链表合并等，写的时候非常容易出错。从我上百场面试的经验来看，能把“链表反转”这几行代码写对的人不足10%。
为什么链表代码这么难写？究竟怎样才能比较轻松地写出正确的链表代码呢？
只要愿意投入时间，我觉得大多数人都是可以学会的。比如说，如果你真的能花上一个周末或者一整天的时间，就去写链表反转这一个代码，多写几遍，一直练到能毫不费力地写出Bug free的代码。这个坎还会很难跨吗？
当然，自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要一些方法和技巧。我根据自己的学习经历和工作经验，总结了几个写链表代码技巧。如果你能熟练掌握这几个技巧，加上你的主动和坚持，轻松拿下链表代码完全没有问题。
技巧一：理解指针或引用的含义事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以，要想写对链表代码，首先就要理解好指针。
我们知道，有些语言有“指针”的概念，比如C语言；有些语言没有指针，取而代之的是“引用”，比如Java、Python。不管是“指针”还是“引用”，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。
接下来，我会拿C语言中的“指针”来讲解，如果你用的是Java或者其他没有指针的语言也没关系，你把它理解成“引用”就可以了。
实际上，对于指针的理解，你只需要记住下面这句话就可以了：
将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。
这句话听起来还挺拗口的，你可以先记住。我们回到链表代码的编写过程中，我来慢慢给你解释。
在编写链表代码的时候，我们经常会有这样的代码：p-&amp;gt;next=q。这行代码是说，p结点中的next指针存储了q结点的内存地址。
还有一个更复杂的，也是我们写链表代码经常会用到的：p-&amp;gt;next=p-&amp;gt;next-&amp;gt;next。这行代码表示，p结点的next指针存储了p结点的下下一个结点的内存地址。
掌握了指针或引用的概念，你应该可以很轻松地看懂链表代码。恭喜你，已经离写出链表代码近了一步！
技巧二：警惕指针丢失和内存泄漏不知道你有没有这样的感觉，写链表代码的时候，指针指来指去，一会儿就不知道指到哪里了。所以，我们在写的时候，一定注意不要弄丢了指针。
指针往往都是怎么弄丢的呢？我拿单链表的插入操作为例来给你分析一下。
如图所示，我们希望在结点a和相邻的结点b之间插入结点x，假设当前指针p指向结点a。如果我们将代码实现变成下面这个样子，就会发生指针丢失和内存泄露。
p-&amp;gt;next = x; // 将p的next指针指向x结点； x-&amp;gt;next = p-&amp;gt;next; // 将x的结点的next指针指向b结点； 初学者经常会在这儿犯错。p-&amp;gt;next指针在完成第一步操作之后，已经不再指向结点b了，而是指向结点x。第2行代码相当于将x赋值给x-&amp;gt;next，自己指向自己。因此，整个链表也就断成了两半，从结点b往后的所有结点都无法访问到了。
对于有些语言来说，比如C语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄露。所以，我们插入结点时，一定要注意操作的顺序，要先将结点x的next指针指向结点b，再把结点a的next指针指向结点x，这样才不会丢失指针，导致内存泄漏。所以，对于刚刚的插入代码，我们只需要把第1行和第2行代码的顺序颠倒一下就可以了。
同理，删除链表结点时，也一定要记得手动释放内存空间，否则，也会出现内存泄漏的问题。当然，对于像Java这种虚拟机自动管理内存的编程语言来说，就不需要考虑这么多了。
技巧三：利用哨兵简化实现难度首先，我们先来回顾一下单链表的插入和删除操作。如果我们在结点p后面插入一个新的结点，只需要下面两行代码就可以搞定。
new_node-&amp;gt;next = p-&amp;gt;next; p-&amp;gt;next = new_node; 但是，当我们要向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。我们需要进行下面这样的特殊处理，其中head表示链表的头结点。所以，从这段代码，我们可以发现，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不一样的。
if (head == null) { head = new_node; } 我们再来看单链表结点删除操作。如果要删除结点p的后继结点，我们只需要一行代码就可以搞定。
p-&amp;gt;next = p-&amp;gt;next-&amp;gt;next; 但是，如果我们要删除链表中的最后一个结点，前面的删除代码就不work了。跟插入类似，我们也需要对于这种情况特殊处理。写成代码是这样子的：
if (head-&amp;gt;next == null) { head = null; } 从前面的一步一步分析，我们可以看出，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。如何来解决这个问题呢？
技巧三中提到的哨兵就要登场了。哨兵，解决的是国家之间的边界问题。同理，这里说的哨兵也是解决“边界问题”的，不直接参与业务逻辑。
还记得如何表示一个空链表吗？head=null表示链表中没有结点了。其中head表示头结点指针，指向链表中的第一个结点。
如果我们引入哨兵结点，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。
我画了一个带头链表，你可以发现，哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。
实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。这些内容我们后面才会讲，现在为了让你感受更深，我再举一个非常简单的例子。代码我是用C语言实现的，不涉及语言方面的高级语法，很容易看懂，你可以类比到你熟悉的语言。
代码一：</description></item><item><title>07｜怎么设计属于我们自己的虚拟机和字节码？</title><link>https://artisanbox.github.io/3/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/9/</guid><description>你好，我是宫文学。
到目前为止，我们的语言看上去已经有点像模像样了。但是有一个地方，我还一直是用比较凑合的方式来实现的，这就是解释器，这节课我想带你把它升级一下。
在之前的内容中，我们用的解释器都是基于AST执行的，而实际上，你所能见到的大多数解释执行的脚本语言，比如Python、PHP、JavaScript等，内部都是采用了一个虚拟机，用字节码解释执行的。像Java这样的语言，虽然是编译执行，但编译的结果也是字节码，JVM虚拟机也能够解释执行它。
为什么这些语言都广泛采用虚拟机呢？这主要是由基于AST的解释器的不足导致的。今天这节课，我就带你先来分析一下AST解释器的缺陷，然后了解一下像JVM这样的虚拟机的运行原理，特别是栈机和寄存器机的差别，以便确定我们自己的虚拟机的设计思路。
看上去任务有点多，没关系，我们一步一步来，我们先来分析一下基于AST的解释器。
基于AST的解释器的缺陷其实，我们目前采用的解释器，是一种最简单的解释器。它有时被称为“树遍历解释器”(Tree-walking Interpreter)，或者更简单一点，也被叫做“AST解释器”。
为什么我刚刚会说我们这个基于AST的解释器有点凑合呢？你可能会想通过遍历AST来执行程序不是挺好的吗？
确实，AST解释器虽然简单，但很有用。比如，最常见的就是对一个表达式做求值，实现类似公式计算的功能，这在电子表格等系统里很常见。甚至在MySQL中，也是基于AST做表达式的计算，还有一些计算机语言的早期版本（如Ruby），以及一些产品中自带的脚本功能，也是用AST解释器就足够了！
不过，虽然AST解释器很有用，但它仍然有明显的缺陷。最主要的问题，就是性能差，在运行时需要消耗更多的CPU时间和内存。在上一节课里，你可能使用过我们的函数特性计算过斐波那契数列，在参数值比较大的情况下（比如n大于30以后），你会看到程序的运行速度确实比较慢。
为什么会这样呢？你再来看我们的解释器，会发现它哪怕只是做一个表达式求值，也要层层做很多次的函数调用。比如，简单的计算2+3*5，需要做的调用包括：
visitBlock() visitStatement() visitExpressionStatement() visitBinary() //+号 visitIntegerLiteral() //2 visitBinary() //*号 visitIntegerLiteral() //3 visitIntegerLiteral() //5 从表面上看起来，这只是做了8次的函数调用。其实，如果你仔细看我们代码的细节，就会发现，由于我们的程序采用了Visitor模式，每一次调用还都有一个Visitor.visit(AstNode)和AstNode.accept(Visitor)来回握手的过程，所以加起来一共做了24次函数调用。
这样的函数调用的开销是很大的。在上一节课，你已经知道了，每次函数调用都需要创建一个栈桢，这会导致内存的消耗。调用函数和从函数返回，也都需要耗费额外的CPU时间。在后面的课程里，等我们对程序的运行时机制的细节了解得更清楚以后，你会更加理解这些额外的开销发生在什么地方。
除了性能问题，AST解释器还有其他的问题。比如，我们已经看到，在实现Return语句的时候，需要额外的冗余处理，以便跳过Return后面的语句，类似的情况还发生在Break、Continue等语句中。总的来说，在控制流的跳转方面，用AST都不方便。
还有，我们执行函数调用的时候，需要从函数调用的AST节点跳到函数声明的节点，这让人有点眼花缭乱。如果我们后面支持类、Lambda等更加丰富的特性，需要运行类的构造函数、进行类成员的初始化，查找并执行正确的父类或子类的方法，那么程序的执行过程会更加让人难以理解。
而且，从根本上来说，AST这种数据结构，比较忠实地体现了高级语言的语法特征。而高级语言呢，是设计用来方便人类理解的，并不是适合机器执行的。把计算机语言从适合人类阅读，变成适合机器执行，本来就是编译器要做的事情。不过，把高级语言变成AST，我们叫做解析（Parse），还不能称上是编译（Compile）。要称得上编译，要对程序的表示方式做更多的变换才行。
那接下来呢，我们就探讨一下如何把程序编译成对机器更友好的方式。按照循序渐进的学习原则，我们不会一下子就编译成机器码，而是先编译成字节码，并试着实现一个虚拟机。
初步了解虚拟机说到虚拟机，我们大多数人都不陌生。比如Java语言最显著的特征之一，就是运行在虚拟机上，而不是像C语言或Go语言那样，编译成可执行文件直接运行；.NET也是用了类似的架构。就算现在Java支持AOT（Ahead of Time）编译方式了，它的可执行文件中仍然打包了一个小型的虚拟机，以便支持某些特别的语言特性。
至于其他语言，如Python、JavaScript等，虽然我们不怎么提及它们的虚拟机，但它们通常都是基于虚拟机来运行的。
虚拟机如此流行，不是偶然现象，因为它提供了一些明显的优点。其中最值得注意的，就是程序可以独立于具体的计算机硬件和操作系统而运行。所以，我们在任何设备上，无论是Mac电脑、Windows电脑、安卓手机还是iPad，都可以用浏览器打开一个页面，并运行里面的JavaScript。而如果采用C语言那样的运行方式，那么针对每种不同的CPU和操作系统的组合，都要生成不同的可执行文件，这对于像浏览器一样的很多应用场景，显然都是很麻烦的。
虚拟机还能提供其他好处，比如通过托管运行提供更高的安全性，还有通过标准的方式使用不同计算机上的文件资源、网络资源、线程资源和图形资源等等。
实际上，虚拟化是计算机领域的一个基本思路。比如，云计算平台能够虚拟出很多不同的操作系统，在基于ARM芯的Mac上可以仿真运行基于X86的Windows等等，都是不同角度的虚拟化。
所以说，要实现一门现代计算机语言，就不能忽视虚拟机方面的知识。
那语言的虚拟机都要包含哪些功能呢？又是由哪些部分构成的呢？
介绍虚拟机的文章很多，特别是针对像JVM和安卓的ART这样广泛使用的平台，有专门的书籍和课程来深入剖析。在我们的课程里，由于我们自己要实现一下虚拟机，因此我也会简单介绍一下虚拟机的原理和构成，当然更多的就要靠你动手实践来掌握虚拟机的精髓了。
总的来说，虚拟机可以看做是一台虚拟的计算机，它能像物理计算机一样，给程序提供一个完整的运行环境。
首先，虚拟机像物理计算机一样，会支持一个指令集。我们的程序按照这个指令集编译成目标代码后，就可以在虚拟机上执行了。
第二，虚拟机也像物理计算机一样，提供内存资源。比如在JVM中，你可以使用栈和堆两种内存。根据不同语言的内存管理机制的不同，在虚拟机里通常还要集成垃圾收集机制。
第三，虚拟机要像物理计算机一样，能够加载程序的代码并执行。程序的目标代码文件中，除了可执行的代码（也就是虚拟机指令），还会包含一些静态的数据，比如程序的符号表，这会让你的程序支持元编程功能，比如运行时的类型判断、基于反射来运行代码等等。静态数据还包括程序中使用的一些常量，比如字符串常量、整数常量等等。对于代码和静态数据，会被虚拟机放在特定的区域，并且能够被指令访问。
此外，虚拟机还要在IO、并发等方面提供相应的支持。这样，我们才可以实现像在终端打印字符这样的功能。
好了，我们已经大概了解了虚拟机相关的概念。不过，不同的虚拟机在运行代码的机制方面是有所区别的，这也会影响到字节码的设计和算法的实现，所以我们现在展开介绍一下。
你可能听说过寄存器机和栈机，这就是比较流行两种程序运行机制。
栈机和寄存器机使用栈机的典型代表，就是JVM，它能够运行Java的字节码。Web Assembly是为浏览器设计的字节码，它的设计也是栈机的架构。
使用寄存器机的典型代表是能够运行JavaScript的V8，V8里面有一个基于寄存器机的字节码解释器。而Lua和Erlang内部也是采用了寄存器机作为程序的运行机制，其实我们现在使用物理计算机，也是寄存器机。
栈机和寄存器机的主要区别，是获取操作数的方式不同：栈机是从操作数栈里获取操作数，而寄存器机是从寄存器里获取。比如要计算“a+3”这个表达式，虚拟机通常都提供了一个指令用来做加法。a和3呢，则是加法指令的操作数，其中a是一个本地变量，其值为2，另一个操作数是常量3。那怎么完成这个加法操作呢？
栈机的运行方式，是先把a的值从内存取出来，压到一个叫做操作栈的区域（使用load指令），然后把常量3压到操作数栈里（使用push指令），接着执行add指令。在执行add指令的时候，就从操作数栈里弹出a和3，做完加法以后，再把结果5压到栈里。
而寄存器机的运行方式，是先把a的值加载到寄存器，在执行add指令的时候，从这寄存器取数，加上常量3以后，再把结果5放回到寄存器。
总结起来，栈机和寄存器机的区别有这三个方面：
第一，操作数的存储机制不同。栈机的操作数放在操作栈，操作数栈的大小几乎不受限制；而寄存器机的操作数是放在寄存器里，寄存器的数量是有限的。需要说明的是，对于物理机来说，寄存器指的是物理寄存器；而对于虚拟机来说，寄存器通常只是几个本地变量。但因为这些变量被频繁访问，根据寄存器分配算法，它们有比较大的概率被映射成物理寄存器，从而大大提高运行性能。
第二，指令的格式不同。寄存器机的指令，需要在操作数里指定运算所需的源和目的操作数。而栈机的运算性的指令，比如加减乘除等，是不需要带操作数的，因为操作数就在栈顶；而像push、load这样的指令，是把数据压到栈里，也不需要指定目的地，因为这个数据也一定是存到栈顶的。
第三，生成字节码的难度不同。从AST生成栈机的代码是比较容易的，你在后面就可以体会到。而生成寄存器机的代码的难度就更高一些，因为寄存器的数量是有限的，我们必须要添加寄存器分配算法。
了解了栈机和寄存器机的这些差别以后，我们就可以根据自己的需求做取舍。比如，如果要更关注运行性能，就选用寄存器机；而如果想实现起来简单一点，并且指令数量更少，便于通过网络传输，我们就可以用栈机。
好了，现在我们已经初步了解了两种运行机制和两类指令集的特点了，是时候设计我们自己的虚拟机和字节码了！
设计我们自己的虚拟机设计一个虚拟机是一项挺有挑战的工作，不过，如果我们仍然采取先迈出一小步，然后慢慢迭代的思路，就没那么复杂了。
在实现一个虚拟机之前，有些关键的技术决策是要确定一下的，这些决策影响到虚拟机的特性和我们所采用的技术。
决策1：选择栈机还是寄存器机？
其实，栈机和寄存器都能满足我们对于程序运行的核心需求，因为我们目前对性能、字节码的大小都没有什么特别的要求。不过，经过思考，我最终选择了栈机，主要有这几个考虑：
首先，Java的JVM用的就是栈机，而且讲述JVM的资料也很多，方便我们借鉴和学习它成熟的设计思路。
第二，由于我们最后是要生成面向物理机的机器码，而物理机就是寄存器机，所以我们肯定会学到这方面的知识。从扩大知识面的角度，我们在虚拟机层面熟悉一下栈机就更好了。
第三，Web Assembly是一个很有前途的技术领域，目前各门语言都在添加编译成Web Assembly的工作。而自己动手实现一个栈机的经验，有助于我们理解Web Assembly，为未来支持Web Assembly打下基础。</description></item><item><title>08_ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？</title><link>https://artisanbox.github.io/4/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/8/</guid><description>过去的三节，你和我一起通过一些简单的代码，看到了我们写的程序，是怎么变成一条条计算机指令的；if…else这样的条件跳转是怎么样执行的；for/while这样的循环是怎么执行的；函数间的相互调用是怎么发生的。
我记得以前，我自己在了解完这些知识之后，产生了一个非常大的疑问。那就是，既然我们的程序最终都被变成了一条条机器码去执行，那为什么同一个程序，在同一台计算机上，在Linux下可以运行，而在Windows下却不行呢？反过来，Windows上的程序在Linux上也是一样不能执行的。可是我们的CPU并没有换掉，它应该可以识别同样的指令呀？
如果你和我有同样的疑问，那这一节，我们就一起来解开。
编译、链接和装载：拆解程序执行第5节我们说过，写好的C语言代码，可以通过编译器编译成汇编代码，然后汇编代码再通过汇编器变成CPU可以理解的机器码，于是CPU就可以执行这些机器码了。你现在对这个过程应该不陌生了，但是这个描述把过程大大简化了。下面，我们一起具体来看，C语言程序是如何变成一个可执行程序的。
不知道你注意到没有，过去几节，我们通过gcc生成的文件和objdump获取到的汇编指令都有些小小的问题。我们先把前面的add函数示例，拆分成两个文件add_lib.c和link_example.c。
// add_lib.c int add(int a, int b) { return a+b; } // link_example.c #include &amp;lt;stdio.h&amp;gt; int main() { int a = 10; int b = 5; int c = add(a, b); printf(&amp;quot;c = %d\n&amp;quot;, c); } 我们通过gcc来编译这两个文件，然后通过objdump命令看看它们的汇编代码。
$ gcc -g -c add_lib.c link_example.c $ objdump -d -M intel -S add_lib.o $ objdump -d -M intel -S link_example.o add_lib.o: file format elf64-x86-64 Disassembly of section .</description></item><item><title>08_事务到底是隔离的还是不隔离的？</title><link>https://artisanbox.github.io/1/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/8/</guid><description>你好，我是林晓斌。
你现在看到的这篇文章是我重写过的。在第一版文章发布之后，我发现在介绍事务可见性规则时，由于引入了太多概念，导致理解起来很困难。随后，我索性就重写了这篇文章。
现在的用户留言中，还能看到第一版文章中引入的up_limit_id的概念，为了避免大家产生误解，再此特地和大家事先说明一下。
我在第3篇文章和你讲事务隔离级别的时候提到过，如果是可重复读隔离级别，事务T启动的时候会创建一个视图read-view，之后事务T执行期间，即使有其他事务修改了数据，事务T看到的仍然跟在启动时看到的一样。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。
但是，我在上一篇文章中，和你分享行锁的时候又提到，一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？
我给你举一个例子吧。下面是一个只有两行的表的初始化语句。
mysql&amp;gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 图1 事务A、B、C的执行流程这里，我们需要注意的是事务的启动时机。
begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。
第一种启动方式，一致性视图是在执行第一个快照读语句时创建的；
第二种启动方式，一致性视图是在执行start transaction with consistent snapshot时创建的。
还需要注意的是，在整个专栏里面，我们的例子中如果没有特别说明，都是默认autocommit=1。
在这个例子中，事务C没有显式地使用begin/commit，表示这个update语句本身就是一个事务，语句完成的时候会自动提交。事务B在更新了行之后查询; 事务A在一个只读事务中查询，并且时间顺序上是在事务B的查询之后。
这时，如果我告诉你事务B查到的k的值是3，而事务A查到的k的值是1，你是不是感觉有点晕呢？
所以，今天这篇文章，我其实就是想和你说明白这个问题，希望借由把这个疑惑解开的过程，能够帮助你对InnoDB的事务和锁有更进一步的理解。
在MySQL里，有两个“视图”的概念：
一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view … ，而它的查询方法与表一样。 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。 它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。
在第3篇文章《事务隔离：为什么你改了我还看不见？》中，我跟你解释过一遍MVCC的实现逻辑。今天为了说明查询和更新的区别，我换一个方式来说明，把read view拆开。你可以结合这两篇文章的说明来更深一步地理解MVCC。
“快照”在MVCC里是怎么工作的？在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。
这时，你会说这看上去不太现实啊。如果一个库有100G，那么我启动一个事务，MySQL就要拷贝100G的数据出来，这个过程得多慢啊。可是，我平时的事务执行起来很快啊。
实际上，我们并不需要拷贝出这100G的数据。我们先来看看这个快照是怎么实现的。</description></item><item><title>08_代码生成：如何实现机器相关的优化？</title><link>https://artisanbox.github.io/7/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/8/</guid><description>你好，我是宫文学。我们继续来学习编译器后端的技术。
在编译过程的前几个阶段之后，编译器生成了AST，完成了语义检查，并基于IR运行了各种优化算法。这些工作，基本上都是机器无关的。但编译的最后一步，也就是生成目标代码，则必须是跟特定CPU架构相关的。
这就是编译器的后端。不过，后端不只是简单地生成目标代码，它还要完成与机器相关的一些优化工作，确保生成的目标代码的性能最高。
这一讲，我就从机器相关的优化入手，带你看看编译器是如何通过指令选择、寄存器分配、指令排序和基于机器代码的优化等步骤，完成整个代码生成的任务的。
首先，我们来看看编译器后端的任务：生成针对不同架构的目标代码。
生成针对不同CPU的目标代码我们已经知道，编译器的后端要把IR翻译成目标代码，那么要生成的目标代码是什么样子的呢？
我以foo.c函数为例：
int foo(int a, int b){ return a + b + 10; } 执行“clang -S foo.c -o foo.x86.s”命令，你可以得到对应的x86架构下的汇编代码（为了便于你理解，我进行了简化）：
#序曲 pushq %rbp movq %rsp, %rbp #%rbp是栈底指针 #函数体 movl %edi, -4(%rbp) #把第1个参数写到栈里第一个位置（偏移量为4） movl %esi, -8(%rbp) #把第2个参数写到栈里第二个位置（偏移量为8） movl -4(%rbp), %eax #把第1个参数写到%eax寄存器 addl -8(%rbp), %eax #把第2个参数加到%eax addl $10, %eax #把立即数10加到%eax，%eax同时是放返回值的地方
#尾声 popq %rbp retq 小提示：上述汇编代码采用的是GNU汇编器的代码格式，源操作数在前面，目的操作数在后面。
我在第1讲中说过，要翻译成目标代码，编译器必须要先懂得目标代码，就像做汉译英一样，我们必须要懂得英语。可是，通常情况下，我们会对汇编代码比较畏惧，觉得汇编语言似乎很难学。其实不然。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;补充说明：有些编译器，是先编译成汇编代码，再通过汇编器把汇编代码转变成机器码。而另一些编译器，是直接生成机器码，并写成目标文件，这样编译速度会更快一些。但这样的编译器一般就要带一个反汇编器，在调试等场合把机器码转化成汇编代码，这样我们看起来比较方便。
因此，在本课程中，我有时会不区分机器码和汇编代码。我可能会说，编译器生成了某机器码，但实际写给你看的是汇编代码，因为文本化的汇编代码才方便阅读。你如果看到这样的表述，不要感到困惑。
那为什么我说汇编代码不难学呢？你可以去查阅下各种不同CPU的指令。然后，你就会发现这些指令其实主要就那么几种，一类是做加减乘除的（如add指令），一类是做内存访问的（如mov、lea指令），一类是控制流程的（如jmp、ret指令），等等。说得夸张一点，这就是个复杂的计算器。
只不过，相比于高级语言，汇编语言涉及的细节比较多。它是啰嗦，但并不复杂。那我再分享一个我学习汇编代码的高效方法：让编译器输出高级语言的汇编代码，多看一些各种情况下汇编代码的写法，自然就会对汇编语言越来越熟悉了。
不过，虽然针对某一种CPU的汇编并不难，但问题是不同架构的CPU，其指令是不同的。编译器的后端每支持一种新的架构，就要有一套新的代码。这对写一个编译器来说，就是很大的工作量了。
我来举个例子。我们使用“clang -S -target armv7a-none-eabi foo.</description></item><item><title>08_作用域和生存期：实现块作用域和函数</title><link>https://artisanbox.github.io/6/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/8/</guid><description>目前，我们已经用Antlr重构了脚本解释器，有了工具的帮助，我们可以实现更高级的功能，比如函数功能、面向对象功能。当然了，在这个过程中，我们还要克服一些挑战，比如：
如果要实现函数功能，要升级变量管理机制； 引入作用域机制，来保证变量的引用指向正确的变量定义； 提升变量存储机制，不能只把变量和它的值简单地扔到一个HashMap里，要管理它的生存期，减少对内存的占用。 本节课，我将借实现块作用域和函数功能，带你探讨作用域和生存期及其实现机制，并升级变量管理机制。那么什么是作用域和生存期，它们的重要性又体现在哪儿呢？
“作用域”和“生存期”是计算机语言中更加基础的概念，它们可以帮你深入地理解函数、块、闭包、面向对象、静态成员、本地变量和全局变量等概念。
而且一旦你深入理解，了解作用域与生存期在编译期和运行期的机制之后，就能解决在学习过程中可能遇到的一些问题，比如：
闭包的机理到底是什么？ 为什么需要栈和堆两种机制来管理内存？它们的区别又是什么？ 一个静态的内部类和普通的内部类有什么区别？ 了解上面这些内容之后，接下来，我们来具体看看什么是作用域。
作用域（Scope）作用域是指计算机语言中变量、函数、类等起作用的范围，我们来看一个具体的例子。
下面这段代码是用C语言写的，我们在全局以及函数fun中分别声明了a和b两个变量，然后在代码里对这些变量做了赋值操作：
/* scope.c 测试作用域。 */ #include &amp;lt;stdio.h&amp;gt; int a = 1;
void fun() { a = 2; //b = 3; //出错，不知道b是谁 int a = 3; //允许声明一个同名的变量吗？ int b = a; //这里的a是哪个？ printf(&amp;quot;in fun: a=%d b=%d \n&amp;quot;, a, b); }
int b = 4; //b的作用域从这里开始
int main(int argc, char **argv){ printf(&amp;quot;main&amp;ndash;1: a=%d b=%d \n&amp;quot;, a, b);</description></item><item><title>08_栈：如何实现浏览器的前进和后退功能？</title><link>https://artisanbox.github.io/2/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/9/</guid><description>浏览器的前进、后退功能，我想你肯定很熟悉吧？
当你依次访问完一串页面a-b-c之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面b和a。当你后退到页面a，点击前进按钮，就可以重新查看页面b和c。但是，如果你后退到页面b后，点击了新的页面d，那就无法再通过前进、后退功能查看页面c了。
假设你是Chrome浏览器的开发工程师，你会如何实现这个功能呢？
这就要用到我们今天要讲的“栈”这种数据结构。带着这个问题，我们来学习今天的内容。
如何理解“栈”？关于“栈”，我有一个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出。后进者先出，先进者后出，这就是典型的“栈”结构。
从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。
我第一次接触这种数据结构的时候，就对它存在的意义产生了很大的疑惑。因为我觉得，相比数组和链表，栈带给我的只有限制，并没有任何优势。那我直接使用数组或者链表不就好了吗？为什么还要用这个“操作受限”的“栈”呢？
事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。
当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时我们就应该首选“栈”这种数据结构。
如何实现一个“栈”？从刚才栈的定义里，我们可以看出，栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。理解了栈的定义之后，我们来看一看如何用代码实现一个栈。
实际上，栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。
我这里实现一个基于数组的顺序栈。基于链表实现的链式栈的代码，你可以自己试着写一下。我会将我写好的代码放到GitHub上，你可以去看一下自己写的是否正确。
我这段代码是用Java来实现的，但是不涉及任何高级语法，并且我还用中文做了详细的注释，所以你应该是可以看懂的。
// 基于数组实现的顺序栈 public class ArrayStack { private String[] items; // 数组 private int count; // 栈中元素个数 private int n; //栈的大小 // 初始化数组，申请一个大小为n的数组空间 public ArrayStack(int n) { this.items = new String[n]; this.n = n; this.count = 0; }
// 入栈操作 public boolean push(String item) { // 数组空间不够了，直接返回false，入栈失败。 if (count == n) return false; // 将item放到下标为count的位置，并且count加一 items[count] = item; ++count; return true; }</description></item><item><title>08_聚合函数：怎么高效地进行分组统计？</title><link>https://artisanbox.github.io/8/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/8/</guid><description>你好，我是朱晓峰。今天，我来和你聊一聊聚合函数。
MySQL中有5种聚合函数较为常用，分别是求和函数SUM()、求平均函数AVG()、最大值函数MAX()、最小值函数MIN()和计数函数COUNT()。接下来，我就结合超市项目的真实需求，来带你掌握聚合函数的用法，帮你实现高效的分组统计。
咱们的项目需求是这样的：超市经营者提出，他们需要统计某个门店，每天、每个单品的销售情况，包括销售数量和销售金额等。这里涉及3个数据表，具体信息如下所示：
销售明细表（demo.transactiondetails)：
销售单头表（demo.transactionhead)：
商品信息表（demo.goodsmaster）：
要统计销售，就要用到数据求和，那么我们就先来学习下求和函数SUM()。
SUM（）SUM（）函数可以返回指定字段值的和。我们可以用它来获得用户某个门店，每天，每种商品的销售总计数据：
mysql&amp;gt; SELECT -&amp;gt; LEFT(b.transdate, 10), -- 从关联表获取交易时间，并且通过LEFT函数，获取交易时间字符串的左边10个字符，得到年月日的数据 -&amp;gt; c.goodsname, -- 从关联表获取商品名称 -&amp;gt; SUM(a.quantity), -- 数量求和 -&amp;gt; SUM(a.salesvalue) -- 金额求和 -&amp;gt; FROM -&amp;gt; demo.transactiondetails a -&amp;gt; JOIN -&amp;gt; demo.transactionhead b ON (a.transactionid = b.transactionid) -&amp;gt; JOIN -&amp;gt; demo.goodsmaster c ON (a.itemnumber = c.itemnumber) -&amp;gt; GROUP BY LEFT(b.transdate, 10) , c.goodsname -- 分组 -&amp;gt; ORDER BY LEFT(b.transdate, 10) , c.goodsname; -- 排序 +-----------------------+-----------+-----------------+-------------------+ | LEFT(b.</description></item><item><title>08_锁：并发操作中，解决数据同步的四种方法</title><link>https://artisanbox.github.io/9/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/8/</guid><description>你好，我是LMOS。
我们在前面的课程中探索了，开发操作系统要了解的最核心的硬件——CPU、MMU、Cache、内存，知道了它们的工作原理。在程序运行中，它们起到了至关重要的作用。
在开发我们自己的操作系统以前，还不能一开始就把机器跑起来，而是先要弄清楚数据同步的问题。如果不解决掉数据同步的问题，后面机器跑起来，就会出现很多不可预知的结果。
通过这节课，我会给你讲清楚为什么在并发操作里，很可能得不到预期的访问数据，还会带你分析这个问题的原因以及解决方法。有了这样一个研究、解决问题的过程，对最重要的几种锁（原子变量，关中断，信号量，自旋锁），你就能做到心中有数了。
非预期结果的全局变量来看看下面的代码，描述的是一个线程中的函数和中断处理函数，它们分别对一个全局变量执行加1操作，代码如下。
int a = 0; void interrupt_handle() { a++; } void thread_func() { a++; } 首先我们梳理一下编译器的翻译过程，通常编译器会把a++语句翻译成这3条指令。
1.把a加载某个寄存器中。
2.这个寄存器加1。
3.把这个寄存器写回内存。
那么不难推断，可能导致结果不确定的情况是这样的：thread_func函数还没运行完第2条指令时，中断就来了。
因此，CPU转而处理中断，也就是开始运行interrupt_handle函数，这个函数运行完a=1，CPU还会回去继续运行第3条指令，此时a依然是1，这显然是错的。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;下面来看一下表格，你就明白了。
显然在t2时刻发生了中断，导致了t2到t4运行了interrupt_handle函数，t5时刻thread_func又恢复运行，导致interrupt_handle函数中a的操作丢失，因此出错。
方法一：原子操作 拿下单体变量要解决上述场景中的问题，有这样两种思路。一种是把a++变成原子操作，这里的原子是不可分隔的，也就是说要a++这个操作不可分隔，即a++要么不执行，要么一口气执行完；另一种就是控制中断，比如在执行a++之前关掉中断，执行完了之后打开中断。
我们先来看看原子操作，显然靠编译器自动生成原子操作不太可能。第一，编译器没有这么智能，能检测哪个变量需要原子操作；第二，编译器必须要考虑代码的移植性，例如有些硬件平台支持原子操作的机器指令，有的硬件平台不支持原子操作。
既然实现原子操作无法依赖于具体编译器，那就需要我们自己动手，x86平台支持很多原子指令，我们只需要直接应用这些指令，比如原子加、原子减，原子读写等，用汇编代码写出对应的原子操作函数就行了。
好在现代C语言已经支持嵌入汇编代码，可以在C函数中按照特定的方式嵌入汇编代码了，实现原子操作就更方便了，代码如下。
//定义一个原子类型 typedef struct s_ATOMIC{ volatile s32_t a_count; //在变量前加上volatile，是为了禁止编译器优化，使其每次都从内存中加载变量 }atomic_t; //原子读 static inline s32_t atomic_read(const atomic_t v) { //x86平台取地址处是原子 return ((volatile u32_t*)&amp;amp;(v)-&amp;gt;a_count); } //原子写 static inline void atomic_write(atomic_t *v, int i) { //x86平台把一个值写入一个地址处也是原子的 v-&amp;gt;a_count = i; } //原子加上一个整数 static inline void atomic_add(int i, atomic_t *v) { asm volatile(&amp;quot;lock;&amp;quot; &amp;quot;addl %1,%0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;a_count) : &amp;quot;ir&amp;quot; (i)); } //原子减去一个整数 static inline void atomic_sub(int i, atomic_t *v) { asm volatile(&amp;quot;lock;&amp;quot; &amp;quot;subl %1,%0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;a_count) : &amp;quot;ir&amp;quot; (i)); } //原子加1 static inline void atomic_inc(atomic_t *v) { asm volatile(&amp;quot;lock;&amp;quot; &amp;quot;incl %0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;a_count)); } //原子减1 static inline void atomic_dec(atomic_t *v) { asm volatile(&amp;quot;lock;&amp;quot; &amp;quot;decl %0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;a_count)); }</description></item><item><title>08｜基于TypeScript的虚拟机（一）：实现一个简单的栈机</title><link>https://artisanbox.github.io/3/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/10/</guid><description>你好，我是宫文学。
上一节课，我们已经探讨了设计一个虚拟机所要考虑的那些因素，并做出了一些设计决策。那么今天这一节课，我们就来实现一个初级的虚拟机。
要实现这个初级虚拟机，具体来说，我们要完成下面三方面的工作：
首先，我们要设计一些字节码，来支持第一批语言特性，包括支持函数、变量和整型数据的运算。也就是说，我们的虚拟机要能够支持下面程序的正确运行：
//一个简单的函数，把输入的参数加10，然后返回 function foo(x:number):number{ return x + 10; } //调用foo，并输出结果 println(foo(18)); 第二，我们要做一个字节码生成程序，基于当前的AST生成正确的字节码。
第三，使用TypeScript，实现一个虚拟机的原型系统，验证相关设计概念。
话不多说，开搞，让我们先设计一下字节码吧！
“设计”字节码说是设计，其实我比较懒，更愿意抄袭现成的设计，比如Java的字节码设计。因为Java字节码的资料最充分，比较容易研究，不像V8等的字节码，只有很少的文档资料，探讨的人也很少。另外，学会手工生成Java字节码还有潜在的实用价值，比如你可以把自己的语言编译后直接在JVM上运行。那么我们就先来研究一下Java字节码的特点。
上面的用TypeScript编写的示例代码，如果用Java改写，会变成下面的程序：
//实现同样功能的Java程序。 public class A{ public static int foo(int a){ return a + 10; } public static void main(String args[]){ System.out.println(foo(8)); } } 我们首先把这个Java程序编译成字节码。
javac A.java 这个文件是一个二进制文件。我们可以用hexdump命令查看它的内容。
hexdump -C A.class 从hexdump显示的信息中，你能看到一些可以阅读的字符，比如“java/lang/Object”、"java/lang/System"等等，这些是常量表中的内容。还有一些内容显然不是字符，没法在屏幕上显示，所以hexdump就用一个.号来表示。其中某些字节，代表的是指令，我在图中把代表foo函数、main函数和构造函数的指令标注了出来，这些都是。用于运行的字节码指令，其他都是一些符号表等描述性的信息。
通过上图，你还能得到一个直观的印象：字节码文件并不都是由指令构成的。
没错，指令只是一个程序文件的一部分。除了指令以外，在字节码文件中还要存储不少其他内容，才能保证程序的正常运行，比如类和方法的符号信息、字符串和数字常量，等等。至于字节码文件的格式，是由字节码的规范来规定的，你有兴趣的话，可以按照规范生成这样的字节码文件。这样的话，我们的程序就可以在JVM上运行了。
不过，我现在不想陷入字节码文件格式的细节里，而是想用自己的方式生成字节码文件，够支持现在的语言特性，能够在我们自己的虚拟机上运行就行了。
上面这张图显示的字节码文件不是很容易阅读和理解。所以，我们用javap命令把它转化成文本格式来看看。
javap -v A.class &amp;gt; A.bc 在这个新生成的文件里，我们可以清晰地看到每个函数的定义以及指令，我也在图里标注了主要的指令的含义。
看到这个字节码文件的内容，你可能会直观地觉得：这看上去跟我们的高级语言也没有那么大的区别嘛。程序照样划分成几个函数，只不过每个函数里的语句变成了栈机的指令而已，函数之间照样需要互相调用。
实际上也确实没错。字节码文件里本来就存储了各个类和方法的符号信息，相当于保存了高级语言里的主体框架。当然，每个方法体里的代码就看不出if语句、循环语句这样的结构了，而是变成了字节码的指令。
通过研究这些指令，加上查阅JVM规则中对于字节码的规定，你会发现为了实现上面示例代码中的功能，我们目前只需要这几个指令就够了：
你先花一两分钟看一下这些指令，看上去挺多，其实可以分为几组。
首先是iload系列，这是把指定下标的本地变量入栈。注意，变量的下标是由声明的顺序决定的，参数也算本地变量，并且排在最前面。所以，iload 0的意思，就是把第一个参数入栈。如果没有参数，就是把第一个本地变量入栈。
iload后面的那几个指令，是压缩格式的指令，也就是利用指令末尾富余的位，把操作数和指令压缩在了一起，这样可以少一个字节码，能够缩小最后生成的字节码文件的大小。从这里面，你能借鉴到字节码设计的一些好的实践。所以你看，学习成熟的设计是有好处的吧？
第二组是istore系列，它做的工作刚好跟iload相反，是把栈顶的值存到指定下标的变量里去。
第三组，是对常数做入栈的操作。对于0~5这几个数字，Java字节码也是提供了压缩格式的指令。对于8位整数（-128~127），使用bipush指令。对于16位整数（-32768~32767），使用sipush指令。而对于更大的常数，则要使用ldc指令，从常量池里去取。
第四组，是几个二元运算的指令。它们都是从栈里取两个操作数，计算完毕之后，再压回栈里。</description></item><item><title>09_Java编译器（一）：手写的编译器有什么优势？</title><link>https://artisanbox.github.io/7/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/9/</guid><description>你好，我是宫文学。
从今天开始呢，我会带着你去考察实际编译器的具体实现机制，你可以从中学习和印证编译原理的基础知识，进而加深你对编译原理的理解。
我们探险的第一站，是很多同学都很熟悉的Java语言，我们一起来看看它的编译器里都有什么奥秘。我从97年就开始用它，算是比较早了。当时，我就对它的“一次编译，到处运行”留下了很深的印象，我在Windows下写的程序，编译完毕以后放到Solaris上就能跑。现在看起来这可能不算什么，但在当年，我在Windows和Unix下写程序用的工具可是完全不同的。
到现在，Java已经是一门非常成熟的语言了，而且它也在不断进化，与时俱进，泛型、函数式编程、模块化等特性陆续都增加了进来。在服务端编程领域，它也变得非常普及。
与此同时，Java的编译器和虚拟机中所采用的技术，也比20年前发生了天翻地覆的变化。对于这么一门成熟的、广泛普及的、又不断焕发新生机的语言来说，研究它的编译技术会带来两个好处：一方面，Java编译器所采用的技术肯定是比较成熟的、靠谱的，你在实现自己的编译功能时，完全可以去参考和借鉴；另一方面，你可以借此深入了解Java的编译过程，借此去实现一些高级的功能，比方说，按需生成字节码，就像Spring这类工具一样。
因此，我会花4讲的时间，跟你一起探索Java的前端编译器（javac）。然后再花4讲的时间在Java的JIT编译器上。
那么，针对Java编译器，你可能会提出下面的问题：
Java的编译器是用什么语言编写的？ Java的词法分析器和语法分析器，是工具生成的，还是手工编写的？为什么会这样选择？ 语法分析的算法分为自顶向下和自底向上的。那么Java的选择是什么呢？有什么道理吗？ 如何自己动手修改Java编译器？ 这些问题，在今天的旅程结束后，你都会获得解答。并且，你还会获得一些额外的启发：噢，原来这个功能是可以这样做的呀！这是对你探险精神的奖励。
好吧，让我们开始吧。
第一步，我们先初步了解一下Java的编译器。
初步了解Java的编译器大多数Java工程师是通过javac命令来初次接触Java编译器的。假设你写了一个MyClass类：
public class MyClass { public int a = 2+3; public int foo(){ int b = a + 10; return b; } } 你可以用javac命令把MyClass.java文件编译成字节码文件：
javac MyClass.java 那这个javac的可执行文件就是Java的编译器吗？并不是。javac只是启动了一个Java虚拟机，执行了一个Java程序，跟我们平常用“java”命令运行一个程序是一样的。换句话说，Java编译器本身也是用Java写的。
这就很有趣了。我们知道，计算机语言是用来编写软件的，而编译器也是一种软件。所以，一门语言的编译器，竟然可以用自己来实现。这种现象，叫做“自举”(Bootstrapping)，这就好像一个人抓着自己的头发，要把自己提起来一样，多么神奇！实际上，一门语言的编译器，一开始肯定是要用其他语言来实现的。但等它成熟了以后，就会尝试实现自举。
既然Java编译器是用Java实现的，那意味着你自己也可以写一个程序，来调用Java的编译器。比如，运行下面的示例代码，也同样可以编译MyClass.java文件，生成MyClass.class文件：
import javax.tools.JavaCompiler; import javax.tools.ToolProvider; public class CompileMyClass { public static void main(String[] args) { JavaCompiler compiler = ToolProvider.getSystemJavaCompiler(); int result = compiler.run(null, null, null, &amp;quot;MyClass.java&amp;quot;); System.</description></item><item><title>09_时间函数：时间类数据，MySQL是怎么处理的？</title><link>https://artisanbox.github.io/8/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/9/</guid><description>你好，我是朱晓峰。今天，咱们来聊一聊MySQL的时间函数。
顾名思义，时间函数就是用来处理时间的函数。时间，几乎可以说是各类项目中都会存在的数据，项目需求不同，我们需要的时间函数也不一样，比如：
如果我们要统计一天之中不同时间段的销售情况，就要获取时间值中的小时值，这就会用到函数HOUR()； 要计算与去年同期相比的增长率，这就要计算去年同期的日期时间，会用到函数DATE_ADD()； 要计算今天是周几、有没有优惠活动，这就要用到函数DAYOFWEEK()了； …… 这么多不同类型的时间函数，该怎么选择呢？这节课，我就结合不同的项目需求，来讲一讲不同的时间函数的使用方法，帮助你轻松地处理各类时间数据。
获取日期时间数据中部分信息的函数我先举个小例子。超市的经营者提出，他们希望通过实际的销售数据，了解到一天当中什么时间段卖得好，什么时间段卖得不好，这样他们就可以根据不同时间的销售情况，合理安排商品陈列和人员促销，以实现收益最大化。
要达到这个目标，我们就需要统计一天中每小时的销售数量和销售金额。
这里涉及3组数据，分别是销售单头表（demo.transactionhead)、销售单明细表 (demo.transactiondetails)和商品信息表（demo.goodsmaster）（为了便于你理解，表的结构和表里的记录都是经过简化的）。
销售单头表包含了销售单的整体信息，包括流水单号、交易时间、收款机编号、会员编号和收银员编号等。
销售单明细表中保存的是交易明细数据，包括商品编号、销售数量、价格、销售金额等。
商品信息表主要包括商品编号、条码、商品名称、规格、单位和售价。
需要注意的是，销售单明细表通过流水编号与销售单头表关联，其中流水编号是外键。通过流水编号，销售单明细表引用销售单头表里的交易时间、会员编号等信息，同时，通过商品编号与商品信息表关联，引用商品信息表里的商品名称等信息。
首先，我们来分析一下“统计一天中每小时的销售数量和销售金额”的这个需求。
要统计一天中每小时的销售情况，实际上就是要把销售数据按照小时进行分组统计。那么，解决问题的关键，就是把交易时间的小时部分提取出来。这就要用到MySQL的日期时间处理函数EXTRACT（）和HOUR（）了。
为了获取小时的值，我们要用到EXTRACT()函数。EXTRACT（type FROM date）表示从日期时间数据“date”中抽取“type”指定的部分。
有了这个函数，我们就可以获取到交易时间的小时部分，从而完成一天中每小时的销售数量和销售金额的查询：
mysql&amp;gt; SELECT -&amp;gt; EXTRACT(HOUR FROM b.transdate) AS 时段, -&amp;gt; SUM(a.quantity) AS 数量, -&amp;gt; SUM(a.salesvalue) AS 金额 -&amp;gt; FROM -&amp;gt; demo.transactiondetails a -&amp;gt; JOIN -&amp;gt; demo.transactionhead b ON (a.transactionid = b.transactionid) -&amp;gt; GROUP BY EXTRACT(HOUR FROM b.transdate) -&amp;gt; ORDER BY EXTRACT(HOUR FROM b.transdate); +------+--------+--------+ | 时段 | 数量 | 金额 | +------+--------+--------+ | 9 | 16.</description></item><item><title>09_普通索引和唯一索引，应该怎么选择？</title><link>https://artisanbox.github.io/1/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/9/</guid><description>今天的正文开始前，我要特意感谢一下评论区几位留下高质量留言的同学。
用户名是 @某、人 的同学，对文章的知识点做了梳理，然后提了关于事务可见性的问题，就是先启动但是后提交的事务，对数据可见性的影响。@夏日雨同学也提到了这个问题，我在置顶评论中回复了，今天的文章末尾也会再展开说明。@Justin和@倪大人两位同学提了两个好问题。
对于能够引发更深一步思考的问题，我会在回复的内容中写上“好问题”三个字，方便你搜索，你也可以去看看他们的留言。
非常感谢大家很细致地看文章，并且留下了那么多和很高质量的留言。知道文章有给大家带来一些新理解，对我来说是一个很好的鼓励。同时，也让其他认真看评论区的同学，有机会发现一些自己还没有意识到的、但可能还不清晰的知识点，这也在总体上提高了整个专栏的质量。再次谢谢你们。
好了，现在就回到我们今天的正文内容。
在前面的基础篇文章中，我给你介绍过索引的基本概念，相信你已经了解了唯一索引和普通索引的区别。今天我们就继续来谈谈，在不同的业务场景下，应该选择普通索引，还是唯一索引？
假设你在维护一个市民系统，每个人都有一个唯一的身份证号，而且业务代码已经保证了不会写入两个重复的身份证号。如果市民系统需要按照身份证号查姓名，就会执行类似这样的SQL语句：
select name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz'; 所以，你一定会考虑在id_card字段上建索引。
由于身份证号字段比较大，我不建议你把身份证号当做主键，那么现在你有两个选择，要么给id_card字段创建唯一索引，要么创建一个普通索引。如果业务代码已经保证了不会写入重复的身份证号，那么这两个选择逻辑上都是正确的。
现在我要问你的是，从性能的角度考虑，你选择唯一索引还是普通索引呢？选择的依据是什么呢？
简单起见，我们还是用第4篇文章《深入浅出索引（上）》中的例子来说明，假设字段 k 上的值都不重复。
图1 InnoDB的索引组织结构接下来，我们就从这两种索引对查询语句和更新语句的性能影响来进行分析。
查询过程假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的过程，先是通过B+树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。
对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。
你知道的，InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB。
因为引擎是按页读写的，所以说，当找到k=5的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。
当然，如果k=5这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。
但是，我们之前计算过，对于整型字段，一个数据页可以放近千个key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的CPU来说可以忽略不计。
更新过程为了说明普通索引和唯一索引对更新语句性能的影响这个问题，我需要先跟你介绍一下change buffer。
当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。
需要说明的是，虽然名字叫作change buffer，实际上它是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。
将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。
显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。
那么，什么条件下可以使用change buffer呢？
对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入(4,400)这个记录，就要先判断现在表中是否已经存在k=4的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了。
因此，唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。
change buffer用的是buffer pool里的内存，因此不能无限增大。change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。
现在，你已经理解了change buffer的机制，那么我们再一起来看看如果要在这张表中插入一个新记录(4,400)的话，InnoDB的处理流程是怎样的。
第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB的处理流程如下：
对于唯一索引来说，找到3和5之间的位置，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，找到3和5之间的位置，插入这个值，语句执行结束。 这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的CPU时间。</description></item><item><title>09_瞧一瞧Linux：Linux的自旋锁和信号量如何实现？</title><link>https://artisanbox.github.io/9/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/9/</guid><description>你好，我是LMOS。
上节课，我们学习了解决数据同步问题的思路与方法。Linux作为成熟的操作系统内核，当然也有很多数据同步的机制，它也有原子变量、开启和关闭中断、自旋锁、信号量。
那今天我们就来探讨一下这些机制在Linux中的实现。看看Linux的实现和前面我们自己的实现有什么区别，以及Linux为什么要这么实现，这么实现背后的机理是什么。
Linux的原子变量首先，我们一起来看看Linux下的原子变量的实现，在Linux中，有许多共享的资源可能只是一个简单的整型数值。
例如在文件描述符中，需要包含一个简单的计数器。这个计数器表示有多少个应用程序打开了文件。在文件系统的open函数中，将这个计数器变量加1；在close函数中，将这个计数器变量减1。
如果单个进程执行打开和关闭操作，那么这个计数器变量不会出现问题，但是Linux是支持多进程的系统，如果有多个进程同时打开或者关闭文件，那么就可能导致这个计数器变量多加或者少加，出现错误。
为了避免这个问题，Linux提供了一个原子类型变量atomic_t。该变量的定义如下。
typedef struct { int counter; } atomic_t;//常用的32位的原子变量类型 #ifdef CONFIG_64BIT typedef struct { s64 counter; } atomic64_t;//64位的原子变量类型 #endif 上述代码自然不能用普通的代码去读写加减，而是要用Linux专门提供的接口函数去操作，否则就不能保证原子性了，代码如下。
//原子读取变量中的值 static __always_inline int arch_atomic_read(const atomic_t *v) { return __READ_ONCE((v)-&amp;gt;counter); } //原子写入一个具体的值 static __always_inline void arch_atomic_set(atomic_t *v, int i) { __WRITE_ONCE(v-&amp;gt;counter, i); } //原子加上一个具体的值 static __always_inline void arch_atomic_add(int i, atomic_t *v) { asm volatile(LOCK_PREFIX &amp;quot;addl %1,%0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;counter) : &amp;quot;ir&amp;quot; (i) : &amp;quot;memory&amp;quot;); } //原子减去一个具体的值 static __always_inline void arch_atomic_sub(int i, atomic_t *v) { asm volatile(LOCK_PREFIX &amp;quot;subl %1,%0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;counter) : &amp;quot;ir&amp;quot; (i) : &amp;quot;memory&amp;quot;); } //原子加1 static __always_inline void arch_atomic_inc(atomic_t *v) { asm volatile(LOCK_PREFIX &amp;quot;incl %0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;counter) :: &amp;quot;memory&amp;quot;); } //原子减1 static __always_inline void arch_atomic_dec(atomic_t *v) { asm volatile(LOCK_PREFIX &amp;quot;decl %0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;counter) :: &amp;quot;memory&amp;quot;); } Linux原子类型变量的操作函数有很多，这里我只是介绍了最基础的几个函数，其它的原子类型变量操作也依赖于上述几个基础的函数。</description></item><item><title>09_程序装载：“640K内存”真的不够用么？</title><link>https://artisanbox.github.io/4/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/9/</guid><description>计算机这个行业的历史上有过很多成功的预言，最著名的自然是“摩尔定律”。当然免不了的也有很多“失败”的预测，其中一个最著名的就是，比尔·盖茨在上世纪80年代说的“640K ought to be enough for anyone”，也就是“640K内存对哪个人来说都够用了”。
那个年代，微软开发的还是DOS操作系统，程序员们还在绞尽脑汁，想要用好这极为有限的640K内存。而现在，我手头的开发机已经是16G内存了，上升了一万倍还不止。那比尔·盖茨这句话在当时也是完全的无稽之谈么？有没有哪怕一点点的道理呢？这一讲里，我就和你一起来看一看。
程序装载面临的挑战上一讲，我们看到了如何通过链接器，把多个文件合并成一个最终可执行文件。在运行这些可执行文件的时候，我们其实是通过一个装载器，解析ELF或者PE格式的可执行文件。装载器会把对应的指令和数据加载到内存里面来，让CPU去执行。
说起来只是装载到内存里面这一句话的事儿，实际上装载器需要满足两个要求。
第一，可执行程序加载后占用的内存空间应该是连续的。我们在第6讲讲过，执行指令的时候，程序计数器是顺序地一条一条指令执行下去。这也就意味着，这一条条指令需要连续地存储在一起。
第二，我们需要同时加载很多个程序，并且不能让程序自己规定在内存中加载的位置。虽然编译出来的指令里已经有了对应的各种各样的内存地址，但是实际加载的时候，我们其实没有办法确保，这个程序一定加载在哪一段内存地址上。因为我们现在的计算机通常会同时运行很多个程序，可能你想要的内存地址已经被其他加载了的程序占用了。
要满足这两个基本的要求，我们很容易想到一个办法。那就是我们可以在内存里面，找到一段连续的内存空间，然后分配给装载的程序，然后把这段连续的内存空间地址，和整个程序指令里指定的内存地址做一个映射。
我们把指令里用到的内存地址叫作虚拟内存地址（Virtual Memory Address），实际在内存硬件里面的空间地址，我们叫物理内存地址（Physical Memory Address）。
程序里有指令和各种内存地址，我们只需要关心虚拟内存地址就行了。对于任何一个程序来说，它看到的都是同样的内存地址。我们维护一个虚拟内存到物理内存的映射表，这样实际程序指令执行的时候，会通过虚拟内存地址，找到对应的物理内存地址，然后执行。因为是连续的内存地址空间，所以我们只需要维护映射关系的起始地址和对应的空间大小就可以了。
内存分段这种找出一段连续的物理内存和虚拟内存地址进行映射的方法，我们叫分段（Segmentation）。这里的段，就是指系统分配出来的那个连续的内存空间。
分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处，第一个就是内存碎片（Memory Fragmentation）的问题。
我们来看这样一个例子。我现在手头的这台电脑，有1GB的内存。我们先启动一个图形渲染程序，占用了512MB的内存，接着启动一个Chrome浏览器，占用了128MB内存，再启动一个Python程序，占用了256MB内存。这个时候，我们关掉Chrome，于是空闲内存还有1024 - 512 - 256 = 256MB。按理来说，我们有足够的空间再去装载一个200MB的程序。但是，这256MB的内存空间不是连续的，而是被分成了两段128MB的内存。因此，实际情况是，我们的程序没办法加载进来。
当然，这个我们也有办法解决。解决的办法叫内存交换（Memory Swapping）。
我们可以把Python程序占用的那256MB内存写到硬盘上，然后再从硬盘上读回来到内存里面。不过读回来的时候，我们不再把它加载到原来的位置，而是紧紧跟在那已经被占用了的512MB内存后面。这样，我们就有了连续的256MB内存空间，就可以去加载一个新的200MB的程序。如果你自己安装过Linux操作系统，你应该遇到过分配一个swap硬盘分区的问题。这块分出来的磁盘空间，其实就是专门给Linux操作系统进行内存交换用的。
虚拟内存、分段，再加上内存交换，看起来似乎已经解决了计算机同时装载运行很多个程序的问题。不过，你千万不要大意，这三者的组合仍然会遇到一个性能瓶颈。硬盘的访问速度要比内存慢很多，而每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。所以，如果内存交换的时候，交换的是一个很占内存空间的程序，这样整个机器都会显得卡顿。
内存分页既然问题出在内存碎片和内存交换的空间太大上，那么解决问题的办法就是，少出现一些内存碎片。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决这个问题。这个办法，在现在计算机的内存管理里面，就叫作内存分页（Paging）。
和分段这样分配一整段连续的空间给到程序相比，分页是把整个物理内存空间切成一段段固定尺寸的大小。而对应的程序所需要占用的虚拟内存空间，也会同样切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。从虚拟内存到物理内存的映射，不再是拿整段连续的内存的物理地址，而是按照一个一个页来的。页的尺寸一般远远小于整个程序的大小。在Linux下，我们通常只设置成4KB。你可以通过命令看看你手头的Linux系统设置的页的大小。
$ getconf PAGE_SIZE 由于内存空间都是预先划分好的，也就没有了不能使用的碎片，而只有被释放出来的很多4KB的页。即使内存空间不够，需要让现有的、正在运行的其他程序，通过内存交换释放出一些内存的页出来，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，让整个机器被内存交换的过程给卡住。
更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是只在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。
实际上，我们的操作系统，的确是这么做的。当要读取特定的页，却发现数据并没有加载到物理内存里的时候，就会触发一个来自于CPU的缺页错误（Page Fault）。我们的操作系统会捕捉到这个错误，然后将对应的页，从存放在硬盘上的虚拟内存里读取出来，加载到物理内存里。这种方式，使得我们可以运行那些远大于我们实际物理内存的程序。同时，这样一来，任何程序都不需要一次性加载完所有指令和数据，只需要加载当前需要用到就行了。
通过虚拟内存、内存交换和内存分页这三个技术的组合，我们最终得到了一个让程序不需要考虑实际的物理内存地址、大小和当前分配空间的解决方案。这些技术和方法，对于我们程序的编写、编译和链接过程都是透明的。这也是我们在计算机的软硬件开发中常用的一种方法，就是加入一个间接层。
通过引入虚拟内存、页映射和内存交换，我们的程序本身，就不再需要考虑对应的真实的内存地址、程序加载、内存管理等问题了。任何一个程序，都只需要把内存当成是一块完整而连续的空间来直接使用。
总结延伸现在回到开头我问你的问题，我们的电脑只要640K内存就够了吗？很显然，现在来看，比尔·盖茨的这个判断是不合理的，那为什么他会这么认为呢？因为他也是一个很优秀的程序员啊！
在虚拟内存、内存交换和内存分页这三者结合之下，你会发现，其实要运行一个程序，“必需”的内存是很少的。CPU只需要执行当前的指令，极限情况下，内存也只需要加载一页就好了。再大的程序，也可以分成一页。每次，只在需要用到对应的数据和指令的时候，从硬盘上交换到内存里面来就好了。以我们现在4K内存一页的大小，640K内存也能放下足足160页呢，也无怪乎在比尔·盖茨会说出“640K ought to be enough for anyone”这样的话。
不过呢，硬盘的访问速度比内存慢很多，所以我们现在的计算机，没有个几G的内存都不好意思和人打招呼。
那么，除了程序分页装载这种方式之外，我们还有其他优化内存使用的方式么？下一讲，我们就一起来看看“动态装载”，学习一下让两个不同的应用程序，共用一个共享程序库的办法。
推荐阅读想要更深入地了解代码装载的详细过程，推荐你阅读《程序员的自我修养——链接、装载和库》的第1章和第6章。
课后思考请你想一想，在Java这样使用虚拟机的编程语言里面，我们写的程序是怎么装载到内存里面来的呢？它也和我们讲的一样，是通过内存分页和内存交换的方式加载到内存里面来的么？
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>09_队列：队列在线程池等有限资源池中的应用</title><link>https://artisanbox.github.io/2/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/10/</guid><description>我们知道，CPU资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致CPU频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。
当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？
实际上，这些问题并不复杂，其底层的数据结构就是我们今天要学的内容，队列（queue）。
如何理解“队列”？队列这个概念非常好理解。你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的“队列”。
我们知道，栈只支持两个基本操作：入栈push()和出栈pop()。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队enqueue()，放一个数据到队列尾部；出队dequeue()，从队列头部取一个元素。
所以，队列跟栈一样，也是一种操作受限的线性表数据结构。
队列的概念很好理解，基本操作也很容易掌握。作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列Disruptor、Linux环形缓存，都用到了循环并发队列；Java concurrent并发包利用ArrayBlockingQueue来实现公平锁等。
顺序队列和链式队列我们知道了，队列跟栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持在队尾插入元素，在队头删除元素，那究竟该如何实现一个队列呢？
跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。同样，用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。
我们先来看下基于数组的实现方法。我用Java语言实现了一下，不过并不包含Java语言的高级语法，而且我做了比较详细的注释，你应该可以看懂。
// 用数组实现的队列 public class ArrayQueue { // 数组：items，数组大小：n private String[] items; private int n = 0; // head表示队头下标，tail表示队尾下标 private int head = 0; private int tail = 0; // 申请一个大小为capacity的数组 public ArrayQueue(int capacity) { items = new String[capacity]; n = capacity; }
// 入队 public boolean enqueue(String item) { // 如果tail == n 表示队列已经满了 if (tail == n) return false; items[tail] = item; ++tail; return true; }</description></item><item><title>09_面向对象：实现数据和方法的封装</title><link>https://artisanbox.github.io/6/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/9/</guid><description>在现代计算机语言中，面向对象是非常重要的特性，似乎常用的语言都支持面向对象特性，比如Swift、C++、Java……不支持的反倒是异类了。
而它重要的特点就是封装。也就是说，对象可以把数据和对数据的操作封装在一起，构成一个不可分割的整体，尽可能地隐藏内部的细节，只保留一些接口与外部发生联系。 在对象的外部只能通过这些接口与对象进行交互，无需知道对象内部的细节。这样能降低系统的耦合，实现内部机制的隐藏，不用担心对外界的影响。那么它们是怎样实现的呢？
本节课，我将从语义设计和运行时机制的角度剖析面向对象的特性，带你深入理解面向对象的实现机制，让你能在日常编程工作中更好地运用面向对象的特性。比如，在学完这讲之后，你会对对象的作用域和生存期、对象初始化过程等有更清晰的了解。而且你不会因为学习了Java或C++的面向对象机制，在学习JavaScript和Ruby的面向对象机制时觉得别扭，因为它们的本质是一样的。
接下来，我们先简单地聊一下什么是面向对象。
面向对象的语义特征我的一个朋友，在10多年前做过培训师，为了吸引学员的注意力，他在讲“什么是面向对象”时说：“面向对象是世界观，是方法论。”
虽然有点儿语不惊人死不休的意思，但我必须承认，所有的计算机语言都是对世界进行建模的方式，只不过建模的视角不同罢了。面向对象的设计思想，在上世纪90年代被推崇，几乎被视为最好的编程模式。实际上，各种不同的编程思想，都会表现为这门语言的语义特征，所以，我就从语义角度，利用类型、作用域、生存期这样的概念带你深入剖析一下面向对象的封装特性，其他特性在后面的课程中再去讨论。
从类型角度 类型处理是语义分析时的重要工作。现代计算机语言可以用自定义的类来声明变量，这是一个巨大的进步。因为早期的计算机语言只支持一些基础的数据类型，比如各种长短不一的整型和浮点型，像字符串这种我们编程时离不开的类型，往往是在基础数据类型上封装和抽象出来的。所以，我们要扩展语言的类型机制，让程序员可以创建自己的类型。
从作用域角度 首先是类的可见性。作为一种类型，它通常在整个程序的范围内都是可见的，可以用它声明变量。当然，一些像Java的语言，也能限制某些类型的使用范围，比如只能在某个命名空间内，或者在某个类内部。
对象的成员的作用域是怎样的呢？我们知道，对象的属性（“属性”这里指的是类的成员变量）可以在整个对象内部访问，无论在哪个位置声明。也就是说，对象属性的作用域是整个对象的内部，方法也是一样。这跟函数和块中的本地变量不一样，它们对声明顺序有要求，像C和Java这样的语言，在使用变量之前必须声明它。
从生存期的角度 对象的成员变量的生存期，一般跟对象的生存期是一样的。在创建对象的时候，就对所有成员变量做初始化，在销毁对象的时候，所有成员变量也随着一起销毁。当然，如果某个成员引用了从堆中申请的内存，这些内存需要手动释放，或者由垃圾收集机制释放。
但还有一些成员，不是与对象绑定的，而是与类型绑定的，比如Java中的静态成员。静态成员跟普通成员的区别，就是作用域和生存期不同，它的作用域是类型的所有对象实例，被所有实例共享。生存期是在任何一个对象实例创建之前就存在，在最后一个对象销毁之前不会消失。
你看，我们用这三个语义概念，就把面向对象的封装特性解释清楚了，无论语言在顶层怎么设计，在底层都是这么实现的。
了解了面向对象在语义上的原理之后，我们来实际动手解析一下代码中的类，这样能更深刻地体会这些原理。
设计类的语法，并解析它我们要在语言中支持类的定义，在PlayScript.g4中，可以这样定义类的语法规则：
classDeclaration : CLASS IDENTIFIER (EXTENDS typeType)? (IMPLEMENTS typeList)? classBody ; classBody : &amp;lsquo;{&amp;rsquo; classBodyDeclaration* &amp;lsquo;}&amp;rsquo; ;
classBodyDeclaration : &amp;lsquo;;&amp;rsquo; | memberDeclaration ;
memberDeclaration : functionDeclaration | fieldDeclaration ;
functionDeclaration : typeTypeOrVoid IDENTIFIER formalParameters (&amp;rsquo;[&amp;rsquo; &amp;lsquo;]&amp;rsquo;)* (THROWS qualifiedNameList)? functionBody ; 我来简单地讲一下这个语法规则：
类声明以class关键字开头，有一个标识符是类型名称，后面跟着类的主体。 类的主体里要声明类的成员。在简化的情况下，可以只关注类的属性和方法两种成员。我们故意把类的方法也叫做function，而不是method，是想把对象方法和函数做一些统一的设计。 函数声明现在的角色是类的方法。 类的成员变量的声明和普通变量声明在语法上没什么区别。 你能看到，我们构造像class这样高级别的结构时，越来越得心应手了，之前形成的一些基础的语法模块都可以复用，比如变量声明、代码块（block）等。
用上面的语法写出来的playscript脚本的效果如下，在示例代码里也有，你可以运行它：
/* ClassTest.</description></item><item><title>09｜基于TypeScript的虚拟机（二）：丰富特性，支持跳转语句</title><link>https://artisanbox.github.io/3/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/11/</guid><description>你好，我是宫文学。
在上一节课里，我们已经实现了一个简单的虚拟机。不过，这个虚拟机也太简单了，实在是不够实用啊。
那么，今天这节课，我们就来增强一下当前的虚拟机，让它的特性更丰富一些，也为我们后续的工作做好铺垫，比如用C语言实现一个更强的虚拟机。
我们在这一节课有两项任务要完成：
首先，要支持if语句和for循环语句。这样，我们就能熟悉与程序分支有关的指令，并且还能让虚拟机支持复杂一点的程序，比如我们之前写过的生成斐波那契数列的程序。
第二，做一下性能比拼。既然我们已经完成了字节码虚拟机的开发，那就跟AST解释器做一些性能测试，看看性能到底差多少。
话不多说，开干！首先，我们来实现一下if语句和for循环语句。而实现这两个语句的核心，就是要支持跳转指令。
了解跳转指令if语句和for循环语句，有一个特点，就是让程序根据一定的条件执行不同的代码。这样一个语法，比较适合我们人类阅读，但是对于机器执行并不方便。机器执行的代码，都是一条条指令排成的直线型的代码，但是可以根据需要跳转到不同的指令去执行。
针对这样的差异，编译器就需要把if和for这样结构化编程的代码，转变成通过跳转指令跳转的代码，其中的关键是计算出正确的跳转地址。
我们举个例子来说明一下，下面是我用Java写的一个示例程序，它有一个if语句。
public static int foo3(int a){ int b; if (a &amp;gt; 10){ b = a + 8; } else{ b = a - 8; } return b; } 我们先用javac命令编译成.class文件，然后再用javap命令以文本方式显示生成的字节码：
我主要是想通过这个示例程序，给你展现两条跳转指令。
一条是if_icmple，它是一条有条件的跳转指令。它给栈顶的两个元素做&amp;lt;=（less equal，缩写为le）运算。如果计算结果为真，那么就跳转到分支地址14。
你可能会发现一个问题，为什么我们的源代码是&amp;gt;号，翻译成字节码却变成了&amp;lt;=号了呢？没错，虽然符号变了，但其实我们的语义并没有发生变化。
我给你分析一下，在源代码里面，程序用&amp;gt;号计算为真，就执行if下面的块；那就意味着如果&amp;gt;号计算为假，或者说&amp;lt;=号为真，则跳转到else下面的那个块，这两种说法是等价的。
但现在我们要生成的是跳转指令，所以用&amp;lt;=做判断，然后再跳转，就是比较自然了。具体你可以看看我们下文中为if语句生成代码的逻辑和相关的图，就更容易理解了。
你在字节码中还会看到另一个跳转指令，是goto指令，它是一个无条件跳转指令。
在计算机语言发展的早期，人们用高级语言写程序的时候，也会用很多goto语句，导致程序非常难以阅读，程序的控制流理解起来困难。虽然直到今天，C和C++语言里还保留了goto语句。不过，一般不到迫不得已，你不应该使用goto语句。
这种迫不得已的情况，我指的是使用goto语句实现一些奇特的效果，这些效果是用结构化编程方式（也就是不用goto语句，而是用条件语句和循环语句表达程序分支）无法完成的。比如，采用goto语句能够从一个嵌套很深的语句块，一下子跳到外面，然后还能再跳进去，接着继续执行！这相当于能够暂停一个执行到一半的程序，然后需要时再恢复上下文，接着执行。
我在说什么呢？这可以跟协程的实现机制关联起来，协程要求在应用层把一个程序停止下来，然后在需要的时候再继续执行。那么利用C/C++的goto语句的无条件跳转能力，你其实就可以实现一个协程库，如果你想了解得更具体一些，可以看看我之前的《编译原理实战课》。
总结起来，goto的这种跳转方式，是更加底层的一种机制。所以，在编译程序的过程中，我们会多次变换程序的表达方式，让它越来越接近计算机容易理解的形式，这个过程叫做Lower过程。而Lower到一定程度，就会形成线性代码加跳转语句的代码格式，我们有时候就会把这种格式的IR叫做“goto格式（goto form）”。
好了，刚才聊的关于goto语句的这些知识点，是为了加深大家对它的认识，希望能够对你的编程思想有所启发。
回到正题，现在我们已经对跳转指令有了基本的认识，那么我就把接下来要用到的跳转指令列出来，你可以看看下面这两张表：
如果后面要增加对浮点数和对象引用的比较功能，我们可以再增加一些指令。但由于目前我们还是只处理整数，所以这些指令就够了。
接着，我们就修改一下字节码生成程序和虚拟机中的执行引擎，让它们能够支持if语句和for语句。
为if语句和for循环语句生成字节码让if语句生成字节码的代码你可以参考visitIfStatement方法。在这个方法里，我们首先为if条件、if后面的块、else块分别生成了字节码。
//条件表达式的代码 let code_condition:number[] = this.visit(ifstmt.condition); //if块的代码 let code_ifBlock:number[] = this.visit(ifstmt.stmt); //else块的代码 let code_elseBlock:number[] = (ifstmt.</description></item><item><title>10_Java编译器（二）：语法分析之后，还要做些什么？</title><link>https://artisanbox.github.io/7/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/10/</guid><description>你好，我是宫文学。
上一讲，我带你了解了Java语言编译器的词法分析和语法分析功能，这两项工作是每个编译器都必须要完成的。那么，根据第1讲我对编译过程的介绍，接下来就应该是语义分析和生成IR了。对于javac编译器来说，生成IR，也就是字节码以后，编译器就完成任务了。也就是说，javac编译器基本上都是在实现一些前端的功能。
不过，由于Java的语法特性很丰富，所以即使只是前端，它的编译功能也不少。那么，除了引用消解和类型检查这两项基础工作之外，你是否知道注解是在什么时候处理的呢？泛型呢？还有各种语法糖呢？
所以，今天这一讲，我就带你把Java编译器的总体编译过程了解一遍。然后，我会把重点放在语义分析中的引用消解、符号表的建立和注解的处理上。当你学完以后，你就能真正理解以下这些问题了：
符号表是教科书上提到的一种数据结构，但它在Java编译器里是如何实现的？编译器如何建立符号表？ 引用消解会涉及到作用域，那么作用域在Java编译器里又是怎么实现的？ 在编译期是如何通过注解的方式生成新程序的？ 为了方便你理解Java编译器内部的一些对象结构，我画了一些类图（如果你不习惯看类图的话，可以参考下面的图表说明，比如我用方框表示一个类，用小圆圈表示一个接口，几种线条分别代表继承关系、引用关系和接口实现）。
图1 ：课程中用到的类图的图表说明在课程开始之前，我想提醒几点：建议你在一个良好的学习环境进入今天的学习，因为你需要一步步地，仔细地跟住我的脚步，避免在探索过程中迷路；除此之外，你的手边还需要一个电脑，这样随时可以查看我在文章中提到的源代码。
了解整个编译过程现在，你可以打开jdk.compiler模块中的com.sun.tools.javac.comp包对应的源代码目录。
comp应该是Compile的缩写。这里面有一个com.sun.tools.javac.comp.CompileStates类，它的意思是编译状态。其中有一个枚举类型CompileState，里面列出了所有的编译阶段。
你会看到，词法和语法分析只占了一个环节（PARSE），生成字节码占了一个环节，而剩下的8个环节都可以看作是语义分析工作（建立符号表、处理注解、属性计算、数据流分析、泛型处理、模式匹配处理、Lambda处理和去除其他语法糖）。
public enum CompileState { INIT(0), //初始化 PARSE(1), //词法和语法分析 ENTER(2), //建立符号表 PROCESS(3), //处理注解 ATTR(4), //属性计算 FLOW(5), //数据流分析 TRANSTYPES(6), //去除语法糖：泛型处理 TRANSPATTERNS(7), //去除语法糖：模式匹配处理 UNLAMBDA(8), //去除语法糖：LAMBDA处理(转换成方法) LOWER(9), //去除语法糖：内部类、foreach循环、断言等。 GENERATE(10); //生成字节码 ... } 另外，你还可以打开com.sun.tools.javac.main.JavaCompiler的代码，看看它的compile()方法。去掉一些细节，你会发现这样的代码主干，从中能看出编译处理的步骤：
processAnnotations( //3：处理注解 enterTrees(stopIfError(CompileState.PARSE, //2：建立符号表 initModules(stopIfError(CompileState.PARSE, parseFiles(sourceFileObjects)) //1：词法和语法分析 )) ),classnames); &amp;hellip; case SIMPLE: generate( //10：生成字节码 desugar( //6~9：去除语法糖 flow( //5：数据流分析 attribute(todo)))); //4：属性计算
其中，PARSE阶段的成果就是生成一个AST，后续的语义分析阶段会基于它做进一步的处理：
enterTrees()：对应ENTER，这个阶段的主要工作是建立符号表。 processAnnotations()：对应PROCESS阶段，它的工作是处理注解。 attribute()：对应ATTR阶段，这个阶段是做属性计算，我会在下一讲中给你做详细的介绍。 flow()：对应FLOW阶段，主要是做数据流分析。我在第7讲中就提到过数据流分析，那时候是用它来做代码优化。那么，难道在前端也需要做数据流分析吗？它会起到什么作用？这些问题的答案我也会在下一讲中为你揭晓。 desugar()：去除语法糖，其实这里包括了TRANSTYPES（处理泛型）、TRANSPATTERNS（处理模式匹配）、UNLAMBDA（处理Lambda）和LOWER（处理其他所有的语法糖，比如内部类、foreach循环等）四个阶段，我会在第12讲给你介绍。 generate()：生成字节码，对应了GENERATE阶段，这部分内容我也会在第12讲详细介绍。 在今天这一讲，我会给你介绍前两个阶段的工作：建立符号表和处理注解。</description></item><item><title>10_MySQL为什么有时候会选错索引？</title><link>https://artisanbox.github.io/1/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/10/</guid><description>前面我们介绍过索引，你已经知道了在MySQL中一张表其实是可以支持多个索引的。但是，你写SQL语句的时候，并没有主动指定使用哪个索引。也就是说，使用哪个索引是由MySQL来确定的。
不知道你有没有碰到过这种情况，一条本来可以执行得很快的语句，却由于MySQL选错了索引，而导致执行速度变得很慢？
我们一起来看一个例子吧。
我们先建一个简单的表，表里有a、b两个字段，并分别建上索引：
CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `b` (`b`) ) ENGINE=InnoDB; 然后，我们往表t中插入10万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到(100000,100000,100000)。
我是用存储过程来插入数据的，这里我贴出来方便你复现：
delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=100000)do insert into t values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 接下来，我们分析一条SQL语句：
mysql&amp;gt; select * from t where a between 10000 and 20000; 你一定会说，这个语句还用分析吗，很简单呀，a上有索引，肯定是要使用索引a的。</description></item><item><title>10_动态链接：程序内部的“共享单车”</title><link>https://artisanbox.github.io/4/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/10/</guid><description>我们之前讲过，程序的链接，是把对应的不同文件内的代码段，合并到一起，成为最后的可执行文件。这个链接的方式，让我们在写代码的时候做到了“复用”。同样的功能代码只要写一次，然后提供给很多不同的程序进行链接就行了。
这么说来，“链接”其实有点儿像我们日常生活中的标准化、模块化生产。我们有一个可以生产标准螺帽的生产线，就可以生产很多个不同的螺帽。只要需要螺帽，我们都可以通过链接的方式，去复制一个出来，放到需要的地方去，大到汽车，小到信箱。
但是，如果我们有很多个程序都要通过装载器装载到内存里面，那里面链接好的同样的功能代码，也都需要再装载一遍，再占一遍内存空间。这就好比，假设每个人都有骑自行车的需要，那我们给每个人都生产一辆自行车带在身边，固然大家都有自行车用了，但是马路上肯定会特别拥挤。
链接可以分动、静，共享运行省内存我们上一节解决程序装载到内存的时候，讲了很多方法。说起来，最根本的问题其实就是内存空间不够用。如果我们能够让同样功能的代码，在不同的程序里面，不需要各占一份内存空间，那该有多好啊！就好比，现在马路上的共享单车，我们并不需要给每个人都造一辆自行车，只要马路上有这些单车，谁需要的时候，直接通过手机扫码，都可以解锁骑行。
这个思路就引入一种新的链接方法，叫作动态链接（Dynamic Link）。相应的，我们之前说的合并代码段的方法，就是静态链接（Static Link）。
在动态链接的过程中，我们想要“链接”的，不是存储在硬盘上的目标文件代码，而是加载到内存中的共享库（Shared Libraries）。顾名思义，这里的共享库重在“共享“这两个字。
这个加载到内存中的共享库会被很多个程序的指令调用到。在Windows下，这些共享库文件就是.dll文件，也就是Dynamic-Link Libary（DLL，动态链接库）。在Linux下，这些共享库文件就是.so文件，也就是Shared Object（一般我们也称之为动态链接库）。这两大操作系统下的文件名后缀，一个用了“动态链接”的意思，另一个用了“共享”的意思，正好覆盖了两方面的含义。
地址无关很重要，相对地址解烦恼不过，要想要在程序运行的时候共享代码，也有一定的要求，就是这些机器码必须是“地址无关”的。也就是说，我们编译出来的共享库文件的指令代码，是地址无关码（Position-Independent Code）。换句话说就是，这段代码，无论加载在哪个内存地址，都能够正常执行。如果不是这样的代码，就是地址相关的代码。
如果还不明白，我给你举一个生活中的例子。如果我们有一个骑自行车的程序，要“前进500米，左转进入天安门广场，再前进500米”。它在500米之后要到天安门广场了，这就是地址相关的。如果程序是“前进500米，左转，再前进500米”，无论你在哪里都可以骑车走这1000米，没有具体地点的限制，这就是地址无关的。
你可以想想，大部分函数库其实都可以做到地址无关，因为它们都接受特定的输入，进行确定的操作，然后给出返回结果就好了。无论是实现一个向量加法，还是实现一个打印的函数，这些代码逻辑和输入的数据在内存里面的位置并不重要。
而常见的地址相关的代码，比如绝对地址代码（Absolute Code）、利用重定位表的代码等等，都是地址相关的代码。你回想一下我们之前讲过的重定位表。在程序链接的时候，我们就把函数调用后要跳转访问的地址确定下来了，这意味着，如果这个函数加载到一个不同的内存地址，跳转就会失败。
对于所有动态链接共享库的程序来讲，虽然我们的共享库用的都是同一段物理内存地址，但是在不同的应用程序里，它所在的虚拟内存地址是不同的。我们没办法、也不应该要求动态链接同一个共享库的不同程序，必须把这个共享库所使用的虚拟内存地址变成一致。如果这样的话，我们写的程序就必须明确地知道内部的内存地址分配。
那么问题来了，我们要怎么样才能做到，动态共享库编译出来的代码指令，都是地址无关码呢？
动态代码库内部的变量和函数调用都很容易解决，我们只需要使用相对地址（Relative Address）就好了。各种指令中使用到的内存地址，给出的不是一个绝对的地址空间，而是一个相对于当前指令偏移量的内存地址。因为整个共享库是放在一段连续的虚拟内存地址中的，无论装载到哪一段地址，不同指令之间的相对地址都是不变的。
PLT和GOT，动态链接的解决方案要实现动态链接共享库，也并不困难，和前面的静态链接里的符号表和重定向表类似，还是和前面一样，我们还是拿出一小段代码来看一看。
首先，lib.h 定义了动态链接库的一个函数 show_me_the_money。
// lib.h #ifndef LIB_H #define LIB_H void show_me_the_money(int money);
#endif lib.c包含了lib.h的实际实现。
// lib.c #include &amp;lt;stdio.h&amp;gt;
void show_me_the_money(int money) { printf(&amp;quot;Show me USD %d from lib.c \n&amp;quot;, money); } 然后，show_me_poor.c 调用了 lib 里面的函数。
// show_me_poor.c #include &amp;quot;lib.h&amp;quot; int main() { int money = 5; show_me_the_money(money); } 最后，我们把 lib.</description></item><item><title>10_如何进行数学计算、字符串处理和条件判断？</title><link>https://artisanbox.github.io/8/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/10/</guid><description>你好，我是朱晓峰。
MySQL提供了很多功能强大，而且使用起来非常方便的函数，包括数学函数、字符串处理函数和条件判断函数等。
在很多场景中 ，我们都会用到这些函数，比如说，在超市项目的实际开发过程中，会有这样的需求：
会员积分的规则是一元积一分，不满一元不积分，这就要用到向下取整的数学函数FLOOR()； 在打印小票的时候，收银纸的宽度是固定的，怎么才能让打印的结果清晰而整齐呢？这个时候，就要用到CONCAT()等字符串处理函数； 不同数据的处理方式不同，怎么选择正确的处理方式呢？这就会用到IF(表达式，V1，V2)这样的条件判断函数； …… 这些函数对我们管理数据库、提高数据处理的效率有很大的帮助。接下来，我就带你在解决实际问题的过程中，帮你掌握使用这些函数的方法。
数学函数我们先来学习下数学函数，它主要用来处理数值数据，常用的主要有3类，分别是取整函数ROUND()、CEIL()、FLOOR()，绝对值函数ABS()和求余函数MOD()。
知道了这些函数，我们来看看超市经营者的具体需求。他们提出，为了提升销量，要进行会员营销，主要是给会员积分，并以积分数量为基础，给会员一定的优惠。
积分的规则也很简单，就是消费一元积一分，不满一元不积分，那我们就需要对销售金额的数值进行取整。
这里主要用到四个表，分别是销售单明细表、销售单头表、商品信息表和会员信息表。为了方便你理解，我对表结构和数据进行了简化。
销售单明细表：
销售单头表：
商品信息表：
会员信息表：
这个场景下，可以用到MySQL数学函数中的取整函数，主要有3种。
向上取整CEIL(X)和CEILING(X)：返回大于等于X的最小INT型整数。 向下取整FLOOR(X)：返回小于等于X的最大INT型整数。 舍入函数ROUND(X,D)：X表示要处理的数，D表示保留的小数位数，处理的方式是四舍五入。ROUND(X)表示保留0位小数。 现在积分的规则是一元积一分，不满一元不积分，显然是向下取整，那就可以用FLOOR（）函数。
首先，我们要通过关联查询，获得会员消费的相关信息：
mysql&amp;gt; SELECT -&amp;gt; c.membername AS '会员', -- 从会员表获取会员名称 -&amp;gt; b.transactionno AS '单号',-- 从销售单头表获取单号 -&amp;gt; b.transdate AS '交易时间', -- 从销售单头表获取交易时间 -&amp;gt; d.goodsname AS '商品名称', -- 从商品信息表获取商品名称 -&amp;gt; a.salesvalue AS '交易金额' -&amp;gt; FROM -&amp;gt; demo.transactiondetails a -&amp;gt; JOIN -&amp;gt; demo.transactionhead b ON (a.transactionid = b.transactionid) -&amp;gt; JOIN -&amp;gt; demo.</description></item><item><title>10_设置工作模式与环境（上）：建立计算机</title><link>https://artisanbox.github.io/9/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/10/</guid><description>你好，我是LMOS。
经过前面那么多课程的准备，现在我们距离把我们自己操作系统跑起来，已经是一步之遥了。现在，你是不是很兴奋，很激动？有这些情绪说明你是喜欢这门课程的。
接下来的三节课，我们会一起完成一个壮举，从GRUB老大哥手中接过权柄，让计算机回归到我们的革命路线上来，为我们之后的开发自己的操作系统做好准备。
具体我是这样来安排的，今天这节课，我们先来搭好操作系统的测试环境。第二节课，我们一起实现一个初始化环境的组件——二级引导器，让它真正继承GRUB权力。第三节课，我们正式攻下初始化的第一个山头，对硬件抽象层进行初始化。
好，让我们正式开始今天的学习。首先我们来解决内核文件封装的问题，然后动手一步步建好虚拟机和生产虚拟硬盘。课程配套代码你可以在这里下载。
从内核映像格式说起我们都知道，一个内核工程肯定有多个文件组成，为了不让GRUB老哥加载多个文件，因疲劳过度而产生问题，我们决定让GRUB只加载一个文件。
但是要把多个文件变成一个文件就需要封装，即把多个文件组装在一起形成一个文件。这个文件我们称为内核映像文件，其中包含二级引导器的模块，内核模块，图片和字库文件。为了这映像文件能被GRUB加载，并让它自身能够解析其中的内容，我们就要定义好具体的格式。如下图所示。
上图中的GRUB头有4KB大小，GRUB正是通过这一小段代码，来识别映像文件的。另外，根据映像文件头描述符和文件头描述符里的信息，这一小段代码还可以解析映像文件中的其它文件。
映像文件头描述符和文件描述符是两个C语言结构体，如下所示。
//映像文件头描述符 typedef struct s_mlosrddsc { u64_t mdc_mgic; //映像文件标识 u64_t mdc_sfsum;//未使用 u64_t mdc_sfsoff;//未使用 u64_t mdc_sfeoff;//未使用 u64_t mdc_sfrlsz;//未使用 u64_t mdc_ldrbk_s;//映像文件中二级引导器的开始偏移 u64_t mdc_ldrbk_e;//映像文件中二级引导器的结束偏移 u64_t mdc_ldrbk_rsz;//映像文件中二级引导器的实际大小 u64_t mdc_ldrbk_sum;//映像文件中二级引导器的校验和 u64_t mdc_fhdbk_s;//映像文件中文件头描述的开始偏移 u64_t mdc_fhdbk_e;//映像文件中文件头描述的结束偏移 u64_t mdc_fhdbk_rsz;//映像文件中文件头描述的实际大小 u64_t mdc_fhdbk_sum;//映像文件中文件头描述的校验和 u64_t mdc_filbk_s;//映像文件中文件数据的开始偏移 u64_t mdc_filbk_e;//映像文件中文件数据的结束偏移 u64_t mdc_filbk_rsz;//映像文件中文件数据的实际大小 u64_t mdc_filbk_sum;//映像文件中文件数据的校验和 u64_t mdc_ldrcodenr;//映像文件中二级引导器的文件头描述符的索引号 u64_t mdc_fhdnr;//映像文件中文件头描述符有多少个 u64_t mdc_filnr;//映像文件中文件头有多少个 u64_t mdc_endgic;//映像文件结束标识 u64_t mdc_rv;//映像文件版本 }mlosrddsc_t; #define FHDSC_NMAX 192 //文件名长度 //文件头描述符 typedef struct s_fhdsc { u64_t fhd_type;//文件类型 u64_t fhd_subtype;//文件子类型 u64_t fhd_stuts;//文件状态 u64_t fhd_id;//文件id u64_t fhd_intsfsoff;//文件在映像文件位置开始偏移 u64_t fhd_intsfend;//文件在映像文件的结束偏移 u64_t fhd_frealsz;//文件实际大小 u64_t fhd_fsum;//文件校验和 char fhd_name[FHDSC_NMAX];//文件名 }fhdsc_t; 有了映像文件格式，我们还要有个打包映像的工具，我给你提供了一个Linux命令行下的工具，你只要明白使用方法就可以，如下所示。</description></item><item><title>10_递归：如何用三行代码找到“最终推荐人”？</title><link>https://artisanbox.github.io/2/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/11/</guid><description>推荐注册返佣金的这个功能我想你应该不陌生吧？现在很多App都有这个功能。这个功能中，用户A推荐用户B来注册，用户B又推荐了用户C来注册。我们可以说，用户C的“最终推荐人”为用户A，用户B的“最终推荐人”也为用户A，而用户A没有“最终推荐人”。
一般来说，我们会通过数据库来记录这种推荐关系。在数据库表中，我们可以记录两行数据，其中actor_id表示用户id，referrer_id表示推荐人id。
基于这个背景，我的问题是，给定一个用户ID，如何查找这个用户的“最终推荐人”？ 带着这个问题，我们来学习今天的内容，递归（Recursion）！
如何理解“递归”？从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。
递归是一种应用非常广泛的算法（或者编程技巧）。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如DFS深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。
不过，别看我说了这么多，递归本身可是一点儿都不“高冷”，咱们生活中就有很多用到递归的例子。
周末你带着女朋友去电影院看电影，女朋友问你，咱们现在坐在第几排啊？电影院里面太黑了，看不清，没法数，现在你怎么办？
别忘了你是程序员，这个可难不倒你，递归就开始排上用场了。于是你就问前面一排的人他是第几排，你想只要在他的数字上加一，就知道自己在哪一排了。但是，前面的人也看不清啊，所以他也问他前面的人。就这样一排一排往前问，直到问到第一排的人，说我在第一排，然后再这样一排一排再把数字传回来。直到你前面的人告诉你他在哪一排，于是你就知道答案了。
这就是一个非常标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。基本上，所有的递归问题都可以用递推公式来表示。刚刚这个生活中的例子，我们用递推公式将它表示出来就是这样的：
f(n)=f(n-1)+1 其中，f(1)=1 f(n)表示你想知道自己在哪一排，f(n-1)表示前面一排所在的排数，f(1)=1表示第一排的人知道自己在第一排。有了这个递推公式，我们就可以很轻松地将它改为递归代码，如下：
int f(int n) { if (n == 1) return 1; return f(n-1) + 1; } 递归需要满足的三个条件刚刚这个例子是非常典型的递归，那究竟什么样的问题可以用递归来解决呢？我总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决。
1.一个问题的解可以分解为几个子问题的解
何为子问题？子问题就是数据规模更小的问题。比如，前面讲的电影院的例子，你要知道，“自己在哪一排”的问题，可以分解为“前一排的人在哪一排”这样一个子问题。
2.这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
比如电影院那个例子，你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。
3.存在递归终止条件
把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。
还是电影院的例子，第一排的人不需要再继续询问任何人，就知道自己在哪一排，也就是f(1)=1，这就是递归的终止条件。
如何编写递归代码？刚刚铺垫了这么多，现在我们来看，如何来写递归代码？我个人觉得，写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。
你先记住这个理论。我举一个例子，带你一步一步实现一个递归代码，帮你理解。
假如这里有n个台阶，每次你可以跨1个台阶或者2个台阶，请问走这n个台阶有多少种走法？如果有7个台阶，你可以2，2，2，1这样子上去，也可以1，2，1，1，2这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？
我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了1个台阶，另一类是第一步走了2个台阶。所以n个台阶的走法就等于先走1阶后，n-1个台阶的走法 加上先走2阶后，n-2个台阶的走法。用公式表示就是：
f(n) = f(n-1)+f(n-2) 有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以f(1)=1。这个递归终止条件足够吗？我们可以用n=2，n=3这样比较小的数试验一下。
n=2时，f(2)=f(1)+f(0)。如果递归终止条件只有一个f(1)=1，那f(2)就无法求解了。所以除了f(1)=1这一个递归终止条件外，还要有f(0)=1，表示走0个台阶有一种走法，不过这样子看起来就不符合正常的逻辑思维了。所以，我们可以把f(2)=2作为一种终止条件，表示走2个台阶，有两种走法，一步走完或者分两步来走。
所以，递归终止条件就是f(1)=1，f(2)=2。这个时候，你可以再拿n=3，n=4来验证一下，这个终止条件是否足够并且正确。
我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：
f(1) = 1; f(2) = 2; f(n) = f(n-1)+f(n-2) 有了这个公式，我们转化成递归代码就简单多了。最终的递归代码是这样的：
int f(int n) { if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2); } 我总结一下，写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。</description></item><item><title>10_闭包：理解了原理，它就不反直觉了</title><link>https://artisanbox.github.io/6/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/10/</guid><description>在讲作用域和生存期时，我提到函数里的本地变量只能在函数内部访问，函数退出之后，作用域就没用了，它对应的栈桢被弹出，作用域中的所有变量所占用的内存也会被收回。
但偏偏跑出来闭包（Closure）这个怪物。
在JavaScript中，用外层函数返回一个内层函数之后，这个内层函数能一直访问外层函数中的本地变量。按理说，这个时候外层函数已经退出了，它里面的变量也该作废了。可闭包却非常执着，即使外层函数已经退出，但内层函数仿佛不知道这个事实一样，还继续访问外层函数中声明的变量，并且还真的能够正常访问。
不过，闭包是很有用的，对库的编写者来讲，它能隐藏内部实现细节；对面试者来讲，它几乎是前端面试必问的一个问题，比如如何用闭包特性实现面向对象编程？等等。
本节课，我会带你研究闭包的实现机制，让你深入理解作用域和生存期，更好地使用闭包特性。为此，要解决两个问题：
函数要变成playscript的一等公民。也就是要能把函数像普通数值一样赋值给变量，可以作为参数传递给其他函数，可以作为函数的返回值。 要让内层函数一直访问它环境中的变量，不管外层函数退出与否。 我们先通过一个例子，研究一下闭包的特性，看看它另类在哪里。
闭包的内在矛盾来测试一下JavaScript的闭包特性：
/** * clojure.js * 测试闭包特性 * 作者：宫文学 */ var a = 0; var fun1 = function(){ var b = 0; // 函数内的局部变量
var inner = function(){ // 内部的一个函数 a = a+1; b = b+1; return b; // 返回内部的成员 } return inner; // 返回一个函数 }
console.log(&amp;quot;outside: a=%d&amp;quot;, a);
var fun2 = fun1(); // 生成闭包 for (var i = 0; i&amp;lt; 2; i++){ console.</description></item><item><title>10｜基于C语言的虚拟机（一）：实现一个简单的栈机</title><link>https://artisanbox.github.io/3/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/12/</guid><description>你好，我是宫文学。
到目前为止，我们已经用TypeScript实现了一个小而全的虚拟机，也在这个过程中稍微体会了一下虚拟机设计的一些要点，比如字节码的设计、指令的生成和栈机的运行机理等等，而且我们还通过性能测试，也看到了栈机确实比AST解释器的性能更高。
虽然，上面这些工作我们都是用TypeScript实现的，但既然我们已经生成了字节码，我不由地产生了一个想法：我们能不能用C语言这样的更基础的语言来实现一个虚拟机，同样来运行这些字节码呢？
我这样的想法可不是凭空产生的。你看，字节码最大的好处，就是和平台无关的能力。不管什么平台，只要有个虚拟机，就可以运行字节码，这也是安卓平台一开始选择字节码作为运行机制的原因。你甚至也可以来试一试，假设现在时间回到智能手机刚出现的时代，你是否也能够快速设计一个虚拟机，来运行手机上的应用呢？
那么进一步，在这种移动设备上运行的应用，很重要的功能就是去调用底层操作系统的API。用C和C++实现的虚拟机，显然在这方面有优势，能够尽量降低由于ABI转换所带来的性能损失。
所以，这一节课，我就带你用C语言重新实现一遍虚拟机。在这个过程中，你会对字节码文件的设计有更细致的体会，对于符号表的作用的理解也会加深，也会掌握如何用C语言设计栈桢的知识点。
好了，我们首先实现第一步的目标，把程序保存成字节码文件，再把字节码文件加载到内存。
读写字节码文件在TypeScript版的虚拟机中，我们用了模块（BCModule）来保存程序的相关信息，有了这样一个模块，程序就可以生成一个独立的字节码文件，就像Java语言里，每个.java文件会编译生成一个.class文件那样。
首先，我们要先来定字节码的文件格式。
我们也说过，Java语言的字节码文件，是依据了专门的技术规范。其实我们仍然可以采用Java字节码文件的格式，你查阅相应的技术规格就可以。这样的话，我们编译后的结果，就直接可以用Java虚拟机来运行了！
有时间的同学可以做一下这个尝试。这项工作在某些场景下会很有意义。你可以定义自己的DSL，直接生成字节码，跟Java编写的程序一起混合运行。我认识的一位极客朋友就做了类似的使用，用低代码的编程界面直接生成了.NET的字节码，形成了一个基于Unity的游戏开发平台。
不过，我们目前的语言比较简单，所以不用遵循那么复杂的规范，我们就设计自己的文件格式就好了。
第二，我们需要考虑：保存什么信息到字节码文件里，才足够用于程序的运行？
从我们目前实现的虚拟机来看，其实不需要太多的信息。你可以回忆一下，其实要保证程序的运行，只需要能够从常量表里查找到函数的一些基本信息即可，最重要的信息包括：
这个函数的字节码； 这个函数有几个本地变量？我们需要在栈桢里保留存储位置； 这个函数的操作数栈的最大尺寸是多少？也就是最多的时候，需要在栈里保存几个操作数，以便我们预留存储空间。 除了这些信息外，再就是我们的代码里用到了的部分数字常量，也需要从常量表里加载，就像ldc指令那样。
所以说，只要把函数常量、数字常量存成字节码文件，就足够我们现在的虚拟机使用了。你也可以看到，我们现在甚至连函数名称、函数的签名都不需要，如果需要的话，也是为了在运行期来显示错误信息而已。
不过，如果把函数名称和函数签名的信息加进去，会有利于我们实现多模块的运行机制。也就是说，如果一个模块中的函数要调用另一个模块中的函数，那么我们可以创造一种机制，实现模块之间的代码查找。
这其实就是Java的类加载机制和多个.class文件之间互相调用的机制，我们仍然可以借鉴。而且，即使像C语言那样的编译成本地代码的语言，也是通过暴露出函数签名的信息，来实现多个模块之间的静态链接和动态链接机制的。
那再进一步，既然需要函数签名，那么我们就需要知道一些类型信息，比如往函数里要传递什么类型的参数，返回的是什么类型的数据，这样调用者和被调用者之间才能无缝衔接到一起。
像C语言这样的系统语言，以及在操作系统的ABI里，支持的都是一些基础的数据类型的信息，比如整数、浮点数、整数指针、字符串指针之类的。而像Java等语言，它们建立了具有较高抽象度的类型体系，还可以包含这些高级的类型信息，从而实现像运行时的类型判断、通过反省的方式动态运行程序等高级功能。
上面这几段的分析总结起来，就是我们需要往目标文件里或多或少地保存一些类型信息。
那么现在就清楚了，我们需要把常量信息和类型信息写到字节码里，就足够程序运行了。
现在，我们来到了第三个步骤：序列化。
具体来说，序列化就是把这些信息以一定的格式写到文件里，再从文件里恢复的过程，是一个比较啰嗦的、充满细节的过程。也就是说，我们在内存里是一种比较结构化的数据，而在文件里保存，或者通过网络传输，都是采用一个线性的数据结构。
在我的编程经验里，所有这些序列化的工作都比较繁琐，但大致的实现方式都是一样的。无论是保存成二进制格式、XML格式、json格式，还是基于一种网络协议在网络上传输，都是一个把内存中的数据结构变成线性的数据结构，然后再从线性的数据结构中恢复的过程。
你可以看看BCModuleWriter和BCModuleReader中的代码，实现的技巧也很简单，最重要的就是你要知道每个数据占了多少个字节。比如，当你向文件里写一个字符串的时候，你先要写下字符串的长度，再写字符串的实际数据，用这样的方法，当你读文件的时候，就能把相关信息顺利还原了。
这类程序中稍微有点难的地方，是保证对象之间正确的引用关系。比如，函数引用了变量和类型，而高级的类型之间也是互相有引用关系的，比如子类型的关系等等，这样就构成了一张网状的数据结构，相互之间有引用。
当你写入文件的时候，要注意，这个网的每个节点只能写一次，不能因为两个函数的返回值都引用了某个类型，就把这个类型写了两次。在读的时候呢，则要重新建立起对象之间正确的引用关系。
好了，了解了实现思路以后，再阅读相应的示例代码就很容易了。在TypeScript中，我用BCModuleWriter把斐波那契数列程序的字节码写成了文件，然后用hexdump命令来显示一下看看：
乍一看，这个跟Java的字节码文件还挺像的，不过我们用的是自己的简单格式。我在图中做了标注，标明了字节是什么含义。其中_main函数和fibonacci函数的字节码指令，我也标了出来。
之后，我可以用BCModuleReader把这个字节码文件再读入内存，重建BCModule，包括里面的符号信息。如果基于这个新的BCModule，程序同样可以顺畅地运行，那就说明我们的字节码文件里面确实包含了足够的运行信息。
好了，现在我们的字节码文件以及相应的读写机制已经设计成功，也用TypeScript做完了所有的设计验证。在这个基础上，重新用C语言实现一个虚拟机，就是一个比较简单的事情了。你可以发现，虽然我们的语言换了，但虚拟机的实现机制没有变。
接下来，我们就需要用C语言把字节码文件读到内存，并在内存重建BCModule相关的各种对象结构。
用C语言读入字节码文件关于C语言版本的字节码读取程序，你可以参考一下readBCModule函数的代码。在读取了字节码文件以后，我还写了一个dumpBCModule的函数，可以在控制台显示BCModule的信息，如下图所示：
你可能会注意到，我们最后的模型里的类型信息和函数都比字节码文件里的要多。不要担心，多出来的其实是系统内置的类型（比如number类型）和内置函数（比如println），它们不需要被保存在字节码文件里，但是会被我们的程序引用到，所以我们要在内存的数据结构中体现。
在这里，我重新梳理一下内存里的对象模型，这个对象模型就是我们运行时所需要的所有信息。我们读取了字节码文件以后，会在内存里形成这个结构化的对象模型，来代表一个程序的信息。
这里我再讲一个小技术点，看着上面的类图，你可能会问：C语言不是不支持面向对象吗？你为什么还能用面向对象的方式来保存这些信息？
其实，用C语言也能模拟类似面向对象的机制。以符号为例，我们是这样声明Symbol和FunctionSymbol的，让FunctionSymbol包含基类Symbol中的数据：
typedef struct _Symbol{ char* name; //符号名称 Type* theType; //类型 SymKind kind; //符号种类 } Symbol; typedef struct _FunctionSymbol{ Symbol symbol; //基类数据 int numVars; //本地变量数量 VarSymbol ** vars; //本地变量信息 int opStackSize; //操作数栈大小 int numByteCodes; //字节码数量 unsigned char* byteCode; //字节码指令 } FunctionSymbol; 在内存里，FunctionSymbol最前面的字段，就是Symbol的字段，因此你可以把FunctionSymbol的指针强制转换成Symbol的指针，从而访问Symbol的字段。这种编程方式在一些用C语言编写的系统软件里非常普遍，包括其他作者写的一些编译器的代码，以及Linux操作系统内核中的代码中都有体现。</description></item><item><title>11_Java编译器（三）：属性分析和数据流分析</title><link>https://artisanbox.github.io/7/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/11/</guid><description>你好，我是宫文学。
在上一讲，我们主要讨论了语义分析中的ENTER和PROCESS阶段。今天我们继续往下探索，看看ATTR和FLOW两个阶段。
ATTR的字面意思是做属性计算。在第4讲中，我已经讲过了属性计算的概念，你应该还记得什么是S属性，什么是I属性。那么，Java编译器会计算哪些属性，又会如何计算呢？
FLOW的字面意思是做数据流分析。通过第7讲，你已经初步了解了数据流分析的算法。但那个时候是把数据流分析用于编译期后端的优化算法，包括删除公共子表达式、变量传播、死代码删除等。而这里说的数据流分析，属于编译器前端的工作。那么，前端的数据流分析会做什么工作呢？
这些问题的答案，我今天都会为你一一揭晓。好了，我们进入正题，首先来看看ATTR阶段的工作：属性分析。
ATTR：属性分析现在，你可以打开com.sun.tools.javac.comp.Attr类的代码。在这个类的头注释里，你会发现原来ATTR做了四件事，分别在4个辅助类里实现：
Check：做类型检查。 Resolve：做名称的消解，也就是对于程序中出现的变量和方法，关联到其定义。 ConstFold：常量折叠，比如对于“2+3”这种在编译期就可以计算出结果的表达式，就直接计算出来。 Infer：用于泛型中的类型参数推导。 我们首先来看Check，也就是类型检查。
类型检查类型检查是语义分析阶段的一项重要工作。静态类型系统的语言，比如Java、C、Kotlin、Swift，都可以通过类型检查，避免很多编译错误。
那么，一个基础的问题是：Java都有哪些类型？
你是不是会觉得这个问题挺幼稚？Java的类型，不就是原始数据类型，再加上类、接口这些吗？
说得对，但是并不全面。你已经看到，Java编译器中每个AST节点都有一个type属性。那么，一个模块或者一个包的类型是什么？一个方法的类型又是什么呢？
在java.compile模块中，定义了Java的语言模型，其中有一个包，是对Java的类型体系做了设计，你可以看一下：
图1：Java的类型体系这样你就能理解了：原来模块和包的类型是NoType，而方法的类型是可执行类型（ExecutableType）。你可以看一下源代码，会发现要刻画一个可执行类型是比较复杂的，竟然需要5个要素：
returnType：返回值类型； parameterTypes：参数类型的列表； receiverType：接收者类型，也就是这个方法是定义在哪个类型（类、接口、枚举）上的； thrownTypes：所抛出异常的类型列表； typeVariables：类型参数的列表。 如果你学过C语言，你应该记得描述一个函数的类型只需要这个列表中的前两项，也就是返回值类型和参数类型就可以了。通过这样的对比，想必你会对Java的可执行类型理解得更清楚。
然而，通过一个接口体系来刻画类型还是不够细致，Java又提供了一个TypeKind的枚举类型，把某些类型做进一步的细化，比如原始数据类型进一步细分为BOOLEAN、BYTE、SHORT等。这种设计方式可以减少接口的数量，使类型体系更简洁。你也可以在编程中借鉴这种设计方式，避免产生过多的、没有什么实际意义的子类型。
同样，在jdk.compiler模块中，有一些具体的类实现了上述类型体系的接口：
图2：类型体系的实现好了，现在你已经了解了Java的类型体系。那么，编译器是如何实现类型检查的呢？
我用一个Java程序的例子，来给你做类型检查的说明。在下面这段代码中，变量a的声明语句是错误的，因为等号右边是一个字符串字面量“Hello”，类型是java.lang.String，跟变量声明语句的类型“int”不相符。在做类型检查的时候，编译器应该检查出这个错误来。
而后面那句“float b = 10”，虽然变量b是float型的，而等号右边是一个整型的字面量，但Java能够自动把整型字面量转化为浮点型，所以这个语句是合法的。
public class TypeCheck{ int a = &amp;quot;Hello&amp;quot;; //等号两边的类型不兼容，编译报错 float b = 10; //整型字面量可以赋值给浮点型变量 } 对于“int a = "hello"”这个语句，它的类型检查过程分了四步，如下图所示：
图3：类型检查的过程第1步，计算vartype子节点的类型。这一步是在把a加入符号表的时候（MemberEnter）就顺便一起做了（调用的是“Attr.attribType()方法”）。计算结果是int型。
第2步，在ATTR阶段正式启动以后，深度优先地遍历整棵AST，自底向上计算每个节点的类型。自底向上是S属性的计算方式。你可以看一下Attr类中的各种attribXXX()方法，大多数都是要返回一个类型值，也就是处理完当前子树后的类型。这个时候，能够知道init部分的类型是字符串型（java.lang.String）。
第3步，检查init部分的类型是否正确。这个时候，比对的就是vartype和init这两棵子树的类型。具体实现是在Check类的checkType()方法，这个方法要用到下面这两个参数。
final Type found：“发现”的类型，也就是“Hello”字面量的类型，这里的值是java.lang.String。这个是自底向上计算出来的，属于S属性。 final Type req：“需要”的类型，这里的值是int。也就是说，a这个变量需要初始化部分的类型是int型的。这个变量是自顶向下传递下来的，属于I属性。 所以你能看出，所谓的类型检查，就是所需类型（I属性）和实际类型（S属性）的比对。
这个时候，你就会发现类型不匹配，从而记录下错误信息。
下面是在做类型检查时整个的调用栈：
JavaCompiler.compile() -&amp;gt;JavaCompiler.attribute() -&amp;gt;Attr.attib() -&amp;gt;Attr.attribClass() //计算TypeCheck的属性 -&amp;gt;Attr.</description></item><item><title>11_二进制编码：“手持两把锟斤拷，口中疾呼烫烫烫”？</title><link>https://artisanbox.github.io/4/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/11/</guid><description>上算法和数据结构课的时候，老师们都会和你说，程序 = 算法 + 数据结构。如果对应到组成原理或者说硬件层面，算法就是我们前面讲的各种计算机指令，数据结构就对应我们接下来要讲的二进制数据。
众所周知，现代计算机都是用0和1组成的二进制，来表示所有的信息。前面几讲的程序指令用到的机器码，也是使用二进制表示的；我们存储在内存里面的字符串、整数、浮点数也都是用二进制表示的。万事万物在计算机里都是0和1，所以呢，搞清楚各种数据在二进制层面是怎么表示的，是我们必备的一课。
大部分教科书都会详细地从整数的二进制表示讲起，相信你在各种地方都能看到对应的材料，所以我就不再啰啰嗦嗦地讲这个了，只会快速地浏览一遍整数的二进制表示。
然后呢，我们重点来看一看，大家在实际应用中最常遇到的问题，也就是文本字符串是怎么表示成二进制的，特别是我们会遇到的乱码究竟是怎么回事儿。我们平时在开发的时候，所说的Unicode和UTF-8之间有什么关系。理解了这些，相信以后遇到任何乱码问题，你都能手到擒来了。
理解二进制的“逢二进一”二进制和我们平时用的十进制，其实并没有什么本质区别，只是平时我们是“逢十进一”，这里变成了“逢二进一”而已。每一位，相比于十进制下的0～9这十个数字，我们只能用0和1这两个数字。
任何一个十进制的整数，都能通过二进制表示出来。把一个二进制数，对应到十进制，非常简单，就是把从右到左的第N位，乘上一个2的N次方，然后加起来，就变成了一个十进制数。当然，既然二进制是一个面向程序员的“语言”，这个从右到左的位置，自然是从0开始的。
比如0011这个二进制数，对应的十进制表示，就是$0×2^3+0×2^2+1×2^1+1×2^0$
$=3$，代表十进制的3。
对应地，如果我们想要把一个十进制的数，转化成二进制，使用短除法就可以了。也就是，把十进制数除以2的余数，作为最右边的一位。然后用商继续除以2，把对应的余数紧靠着刚才余数的右侧，这样递归迭代，直到商为0就可以了。
比如，我们想把13这个十进制数，用短除法转化成二进制，需要经历以下几个步骤：
因此，对应的二进制数，就是1101。
刚才我们举的例子都是正数，对于负数来说，情况也是一样的吗？我们可以把一个数最左侧的一位，当成是对应的正负号，比如0为正数，1为负数，这样来进行标记。
这样，一个4位的二进制数， 0011就表示为+3。而1011最左侧的第一位是1，所以它就表示-3。这个其实就是整数的原码表示法。原码表示法有一个很直观的缺点就是，0可以用两个不同的编码来表示，1000代表0， 0000也代表0。习惯万事一一对应的程序员看到这种情况，必然会被“逼死”。
于是，我们就有了另一种表示方法。我们仍然通过最左侧第一位的0和1，来判断这个数的正负。但是，我们不再把这一位当成单独的符号位，在剩下几位计算出的十进制前加上正负号，而是在计算整个二进制值的时候，在左侧最高位前面加个负号。
比如，一个4位的二进制补码数值1011，转换成十进制，就是$-1×2^3+0×2^2+1×2^1+1×2^0$
$=-5$。如果最高位是1，这个数必然是负数；最高位是0，必然是正数。并且，只有0000表示0，1000在这样的情况下表示-8。一个4位的二进制数，可以表示从-8到7这16个整数，不会白白浪费一位。
当然更重要的一点是，用补码来表示负数，使得我们的整数相加变得很容易，不需要做任何特殊处理，只是把它当成普通的二进制相加，就能得到正确的结果。
我们简单一点，拿一个4位的整数来算一下，比如 -5 + 1 = -4，-5 + 6 = 1。我们各自把它们转换成二进制来看一看。如果它们和无符号的二进制整数的加法用的是同样的计算方式，这也就意味着它们是同样的电路。
字符串的表示，从编码到数字不仅数值可以用二进制表示，字符乃至更多的信息都能用二进制表示。最典型的例子就是字符串（Character String）。最早计算机只需要使用英文字符，加上数字和一些特殊符号，然后用8位的二进制，就能表示我们日常需要的所有字符了，这个就是我们常常说的ASCII码（American Standard Code for Information Interchange，美国信息交换标准代码）。
图片来源ASCII码就好比一个字典，用8位二进制中的128个不同的数，映射到128个不同的字符里。比如，小写字母a在ASCII里面，就是第97个，也就是二进制的0110 0001，对应的十六进制表示就是 61。而大写字母 A，就是第65个，也就是二进制的0100 0001，对应的十六进制表示就是41。
在ASCII码里面，数字9不再像整数表示法里一样，用0000 1001来表示，而是用0011 1001 来表示。字符串15也不是用0000 1111 这8位来表示，而是变成两个字符1和5连续放在一起，也就是 0011 0001 和 0011 0101，需要用两个8位来表示。
我们可以看到，最大的32位整数，就是2147483647。如果用整数表示法，只需要32位就能表示了。但是如果用字符串来表示，一共有10个字符，每个字符用8位的话，需要整整80位。比起整数表示法，要多占很多空间。
这也是为什么，很多时候我们在存储数据的时候，要采用二进制序列化这样的方式，而不是简单地把数据通过CSV或者JSON，这样的文本格式存储来进行序列化。不管是整数也好，浮点数也好，采用二进制序列化会比存储文本省下不少空间。
ASCII码只表示了128个字符，一开始倒也堪用，毕竟计算机是在美国发明的。然而随着越来越多的不同国家的人都用上了计算机，想要表示譬如中文这样的文字，128个字符显然是不太够用的。于是，计算机工程师们开始各显神通，给自己国家的语言创建了对应的字符集（Charset）和字符编码（Character Encoding）。
字符集，表示的可以是字符的一个集合。比如“中文”就是一个字符集，不过这样描述一个字符集并不准确。想要更精确一点，我们可以说，“第一版《新华字典》里面出现的所有汉字”，这是一个字符集。这样，我们才能明确知道，一个字符在不在这个集合里面。比如，我们日常说的Unicode，其实就是一个字符集，包含了150种语言的14万个不同的字符。
而字符编码则是对于字符集里的这些字符，怎么一一用二进制表示出来的一个字典。我们上面说的Unicode，就可以用UTF-8、UTF-16，乃至UTF-32来进行编码，存储成二进制。所以，有了Unicode，其实我们可以用不止UTF-8一种编码形式，我们也可以自己发明一套 GT-32 编码，比如就叫作Geek Time 32好了。只要别人知道这套编码规则，就可以正常传输、显示这段代码。
同样的文本，采用不同的编码存储下来。如果另外一个程序，用一种不同的编码方式来进行解码和展示，就会出现乱码。这就好像两个军队用密语通信，如果用错了密码本，那看到的消息就会不知所云。在中文世界里，最典型的就是“手持两把锟斤拷，口中疾呼烫烫烫”的典故。
我曾经听说过这么一个笑话，没有经验的同学，在看到程序输出“烫烫烫”的时候，以为是程序让CPU过热发出报警，于是尝试给CPU降频来解决问题。
既然今天要彻底搞清楚编码知识，我们就来弄清楚“锟斤拷”和“烫烫烫”的来龙去脉。</description></item><item><title>11_怎么给字符串字段加索引？</title><link>https://artisanbox.github.io/1/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/11/</guid><description>现在，几乎所有的系统都支持邮箱登录，如何在邮箱这样的字段上建立合理的索引，是我们今天要讨论的问题。
假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的：
mysql&amp;gt; create table SUser( ID bigint unsigned primary key, email varchar(64), ... )engine=innodb; 由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句：
mysql&amp;gt; select f1, f2 from SUser where email='xxx'; 从第4和第5篇讲解索引的文章中，我们可以知道，如果email这个字段上没有索引，那么这个语句就只能做全表扫描。
同时，MySQL是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。
比如，这两个在email字段上创建索引的语句：
mysql&amp;gt; alter table SUser add index index1(email); 或 mysql&amp;gt; alter table SUser add index index2(email(6)); 第一个语句创建的index1索引里面，包含了每个记录的整个字符串；而第二个语句创建的index2索引里面，对于每个记录都是只取前6个字节。
那么，这两种不同的定义在数据结构和存储上有什么区别呢？如图2和3所示，就是这两个索引的示意图。
图1 email 索引结构图2 email(6) 索引结构从图中你可以看到，由于email(6)这个索引结构中每个邮箱字段都只取前6个字节（即：zhangs），所以占用的空间会更小，这就是使用前缀索引的优势。
但，这同时带来的损失是，可能会增加额外的记录扫描次数。
接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。
select id,name,email from SUser where email='zhangssxyz@xxx.com'; 如果使用的是index1（即email整个字符串的索引结构），执行顺序是这样的：
从index1索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得ID2的值；
到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集；
取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足email='zhangssxyz@xxx.com’的条件了，循环结束。
这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。</description></item><item><title>11_排序（上）：为什么插入排序比冒泡排序更受欢迎？</title><link>https://artisanbox.github.io/2/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/12/</guid><description>排序对于任何一个程序员来说，可能都不会陌生。你学的第一个算法，可能就是排序。大部分编程语言中，也都提供了排序函数。在平常的项目中，我们也经常会用到排序。排序非常重要，所以我会花多一点时间来详细讲一讲经典的排序算法。
排序算法太多了，有很多可能你连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。我只讲众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。我按照时间复杂度把它们分成了三类，分三节课来讲解。
带着问题去学习，是最有效的学习方法。所以按照惯例，我还是先给你出一个思考题：插入排序和冒泡排序的时间复杂度相同，都是O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？
你可以先思考一两分钟，带着这个问题，我们开始今天的内容！
如何分析一个“排序算法”？学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。那分析一个排序算法，要从哪几个方面入手呢？
排序算法的执行效率对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：
1.最好情况、最坏情况、平均情况时间复杂度
我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。
为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。
2.时间复杂度的系数、常数 、低阶
我们知道，时间复杂度反映的是数据规模n很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是10个、100个、1000个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。
3.比较次数和交换（或移动）次数
这一节和下一节讲的都是基于比较的排序算法。基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。
排序算法的内存消耗我们前面讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是O(1)的排序算法。我们今天讲的三种排序算法，都是原地排序算法。
排序算法的稳定性仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。
我通过一个例子来解释一下。比如我们有一组数据2，9，3，4，8，3，按照大小排序之后就是2，3，3，4，8，9。
这组数据里有两个3。经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法；如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。
你可能要问了，两个3哪个在前，哪个在后有什么关系啊，稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？
很多数据结构和算法课程，在讲排序的时候，都是用整数来举例，但在真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。
比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有10万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？
最先想到的方法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。
借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？
稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。
冒泡排序（Bubble Sort）我们从冒泡排序开始，学习今天的三种排序算法。
冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序工作。
我用一个例子，带你看下冒泡排序的整个过程。我们要对一组数据4，5，6，3，2，1，从小到大进行排序。第一次冒泡操作的详细过程就是这样：
可以看出，经过一次冒泡操作之后，6这个元素已经存储在正确的位置上。要想完成所有数据的排序，我们只要进行6次这样的冒泡操作就行了。
实际上，刚讲的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。我这里还有另外一个例子，这里面给6个元素排序，只需要4次冒泡操作就可以了。
冒泡排序算法的原理比较容易理解，具体的代码我贴到下面，你可以结合着代码来看我前面讲的原理。
// 冒泡排序，a表示数组，n表示数组大小 public void bubbleSort(int[] a, int n) { if (n &amp;lt;= 1) return; for (int i = 0; i &amp;lt; n; ++i) { // 提前退出冒泡循环的标志位 boolean flag = false; for (int j = 0; j &amp;lt; n - i - 1; ++j) { if (a[j] &amp;gt; a[j+1]) { // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true; // 表示有数据交换 } } if (!</description></item><item><title>11_索引：怎么提高查询的速度？</title><link>https://artisanbox.github.io/8/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/11/</guid><description>你好，我是朱晓峰。
在我们的超市信息系统刚刚开始运营的时候，因为数据量很少，每一次的查询都能很快拿到结果。但是，系统运转时间长了以后，数据量不断地累积，变得越来越庞大，很多查询的速度就变得特别慢。这个时候，我们就采用了MySQL 提供的高效访问数据的方法—— 索引，有效地解决了这个问题，甚至之前的一个需要8秒钟才能完成的查询，现在只用0.3秒就搞定了，速度提升了20多倍。
那么，索引到底是啥呢？该怎么使用呢？这节课，我们就来聊一聊。
索引是什么？如果你去过图书馆，应该会知道图书馆的检索系统。图书馆为图书准备了检索目录，包括书名、书号、对应的位置信息，包括在哪个区、哪个书架、哪一层。我们可以通过书名或书号，快速获知书的位置，拿到需要的书。
MySQL中的索引，就相当于图书馆的检索目录，它是帮助MySQL系统快速检索数据的一种存储结构。我们可以在索引中按照查询条件，检索索引字段的值，然后快速定位数据记录的位置，这样就不需要遍历整个数据表了。而且，数据表中的字段越多，表中数据记录越多，速度提升越是明显。
我来举个例子，进一步解释下索引的作用。这里要用到销售流水表（demo.trans），表结构如下：
mysql&amp;gt; describe demo.trans; +---------------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------------+----------+------+-----+---------+-------+ | itemnumber | int | YES | MUL | NULL | | | quantity | text | YES | | NULL | | | price | text | YES | | NULL | | | transdate | datetime | YES | MUL | NULL | | | actualvalue | text | YES | | NULL | | | barcode | text | YES | | NULL | | | cashiernumber | int | YES | MUL | NULL | | | branchnumber | int | YES | MUL | NULL | | | transuniqueid | text | YES | | NULL | | +---------------+----------+------+-----+---------+-------+ 9 rows in set (0.</description></item><item><title>11_设置工作模式与环境（中）：建造二级引导器</title><link>https://artisanbox.github.io/9/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/11/</guid><description>你好，我是LMOS。
上节课，我们建造了属于我们的“计算机”，并且在上面安装好了GRUB。这节课我会带你一起实现二级引导器这个关键组件。
看到这儿你可能会问，GRUB不是已经把我们的操作系统加载到内存中了吗？我们有了GRUB，我们为什么还要实现二级引导器呢？
这里我要给你说说我的观点，二级引导器作为操作系统的先驱，它需要收集机器信息，确定这个计算机能不能运行我们的操作系统，对CPU、内存、显卡进行一些初级的配置，放置好内核相关的文件。
因为我们二级引导器不是执行具体的加载任务的，而是解析内核文件、收集机器环境信息，它具体收集哪些信息，我会在下节课详细展开。
设计机器信息结构二级引导器收集的信息，需要地点存放，我们需要设计一个数据结构。信息放在这个数据结构中，这个结构放在内存1MB的地方，方便以后传给我们的操作系统。
为了让你抓住重点，我选取了这个数据结构的关键代码，这里并没有列出该结构的所有字段（Cosmos/initldr/include/ldrtype.h），这个结构如下所示。
typedef struct s_MACHBSTART { u64_t mb_krlinitstack;//内核栈地址 u64_t mb_krlitstacksz;//内核栈大小 u64_t mb_imgpadr;//操作系统映像 u64_t mb_imgsz;//操作系统映像大小 u64_t mb_bfontpadr;//操作系统字体地址 u64_t mb_bfontsz;//操作系统字体大小 u64_t mb_fvrmphyadr;//机器显存地址 u64_t mb_fvrmsz;//机器显存大小 u64_t mb_cpumode;//机器CPU工作模式 u64_t mb_memsz;//机器内存大小 u64_t mb_e820padr;//机器e820数组地址 u64_t mb_e820nr;//机器e820数组元素个数 u64_t mb_e820sz;//机器e820数组大小 //…… u64_t mb_pml4padr;//机器页表数据地址 u64_t mb_subpageslen;//机器页表个数 u64_t mb_kpmapphymemsz;//操作系统映射空间大小 //…… graph_t mb_ghparm;//图形信息 }__attribute__((packed)) machbstart_t; 规划二级引导器在开始写代码之前，我们先来从整体划分一下二级引导器的功能模块，从全局了解下功能应该怎么划分，这里我特意为你梳理了一个表格。
前面表格里的这些文件，我都放在了课程配套源码中了，你可以从这里下载。
上述这些文件都在lesson10～11/Cosmos/initldr/ldrkrl目录中，它们在编译之后会形成三个文件，编译脚本我已经写好了，下面我们用一幅图来展示这个编译过程。
这最后三个文件用我们前面说的映像工具打包成映像文件，其指令如下。
lmoskrlimg -m k -lhf initldrimh.bin -o Cosmos.eki -f initldrkrl.bin initldrsve.bin 实现GRUB头我们的GRUB头有两个文件组成，一个imginithead.asm汇编文件，它有两个功能，既能让GRUB识别，又能设置C语言运行环境，用于调用C函数；第二就是inithead.c文件，它的主要功能是查找二级引导器的核心文件——initldrkrl.bin，然后把它放置到特定的内存地址上。
我们先来实现imginithead.asm，它主要工作是初始化CPU的寄存器，加载GDT，切换到CPU的保护模式，我们一步一步来实现。
首先是GRUB1和GRUB2需要的两个头结构，代码如下。
MBT_HDR_FLAGS EQU 0x00010003 MBT_HDR_MAGIC EQU 0x1BADB002 MBT2_MAGIC EQU 0xe85250d6 global _start extern inithead_entry [section .</description></item><item><title>11_语义分析（上）：如何建立一个完善的类型系统？</title><link>https://artisanbox.github.io/6/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/11/</guid><description>在做语法分析时我们可以得到一棵语法树，而基于这棵树能做什么，是语义的事情。比如，+号的含义是让两个数值相加，并且通常还能进行缺省的类型转换。所以，如果要区分不同语言的差异，不能光看语言的语法。比如Java语言和JavaScript在代码块的语法上是一样的，都是用花括号，但在语义上是不同的，一个有块作用域，一个没有。
这样看来，相比词法和语法的设计与处理，语义设计和分析似乎要复杂很多。虽然我们借作用域、生存期、函数等特性的实现涉猎了很多语义分析的场景，但离系统地掌握语义分析，还差一点儿火候。所以，为了帮你攻破语义分析这个阶段，我会用两节课的时间，再梳理一下语义分析中的重要知识，让你更好地建立起相关的知识脉络。
今天这节课，我们把注意力集中在类型系统这个话题上。
围绕类型系统产生过一些争论，有的程序员会拥护动态类型语言，有的会觉得静态类型语言好。要想探究这个问题，我们需要对类型系统有个清晰的了解，最直接的方式，就是建立一个完善的类型系统。
那么什么是类型系统？我们又该怎样建立一个完善的类型系统呢？
其实，类型系统是一门语言所有的类型的集合，操作这些类型的规则，以及类型之间怎么相互作用的（比如一个类型能否转换成另一个类型）。如果要建立一个完善的类型系统，形成对类型系统比较完整的认知，需要从两个方面出发：
根据领域的需求，设计自己的类型系统的特征。 在编译器中支持类型检查、类型推导和类型转换。 先从第一个方面出发看一下。
设计类型系统的特征在进入这个话题之前，我想先问你一个有意义的问题：类型到底是什么？我们说一个类型的时候，究竟在说什么？
要知道，在机器代码这个层面，其实是分不出什么数据类型的。在机器指令眼里，那就是0101，它并不对类型做任何要求，不需要知道哪儿是一个整数，哪儿代表着一个字符，哪儿又是内存地址。你让它做什么操作都可以，即使这个操作没有意义，比如把一个指针值跟一个字符相加。
那么高级语言为什么要增加类型这种机制呢？
对类型做定义很难，但大家公认的有一个说法：类型是针对一组数值，以及在这组数值之上的一组操作。比如，对于数字类型，你可以对它进行加减乘除算术运算，对于字符串就不行。
所以，类型是高级语言赋予的一种语义，有了类型这种机制，就相当于定了规矩，可以检查施加在数据上的操作是否合法。因此类型系统最大的好处，就是可以通过类型检查降低计算出错的概率。所以，现代计算机语言都会精心设计一个类型系统，而不是像汇编语言那样完全不区分类型。
不过，类型系统的设计有很多需要取舍和权衡的方面，比如：
面向对象的拥护者希望所有的类型都是对象，而重视数据计算性能的人认为应该支持非对象化的基础数据类型； 你想把字符串作为原生数据类型，还是像Java那样只是一个普通的类？ 是静态类型语言好还是动态类型语言好？ …… 虽然类型系统的设计有很多需要取舍和权衡的方面，但它最需要考虑的是，是否符合这门语言想解决的问题，我们用静态类型语言和动态类型语言分析一下。
根据类型检查是在编译期还是在运行期进行的，我们可以把计算机语言分为两类：
静态类型语言（全部或者几乎全部的类型检查是在编译期进行的）。 动态类型语言（类型的检查是在运行期进行的）。 静态类型语言的拥护者说：
因为编译期做了类型检查，所以程序错误较少，运行期不用再检查类型，性能更高。像C、Java和Go语言，在编译时就对类型做很多处理，包括检查类型是否匹配，以及进行缺省的类型转换，大大降低了程序出错的可能性，还能让程序运行效率更高，因为不需要在运行时再去做类型检查和转换。
而动态类型语言的拥护者说：
静态语言太严格，还要一遍遍编译，编程效率低，用动态类型语言方便进行快速开发。JavaScript、Python、PHP等都是动态类型的。
客观地讲，这些说法都有道理。目前的趋势是，某些动态类型语言在想办法增加一些机制，在编译期就能做类型检查，比如用TypeScript代替JavaScript编写程序，做完检查后再输出成JavaScript。而某些静态语言呢，却又发明出一些办法，部分地绕过类型检查，从而提供动态类型语言的灵活性。
再延伸一下，跟静态类型和动态类型概念相关联的，还有强类型和弱类型。强类型语言中，变量的类型一旦声明就不能改变，弱类型语言中，变量类型在运行期时可以改变。二者的本质区别是，强类型语言不允许违法操作，因为能够被检查出来，弱类型语言则从机制上就无法禁止违法操作，所以是不安全的。比如你写了一个表达式a*b。如果a和b这两个变量是数值，这个操作就没有问题，但如果a或b不是数值，那就没有意义了，弱类型语言可能就检查不出这类问题。
也就是，静态类型和动态类型说的是什么时候检查的问题，强类型和弱类型说的是就算检查，也检查不出来，或者没法检查的问题，这两组概念经常会被搞混，所以我在这里带你了解一下。
接着说回来。关于类型特征的取舍，是根据领域问题而定的。举例来说，很多人可能都觉得强类型更好，但对于儿童编程启蒙来说，他们最好尽可能地做各种尝试，如果必须遵守与类型有关的规则，程序总是跑不起来，可能会打击到他们。
对于playscript而言，因为目前是用来做教学演示的，所以我们尽可能地多涉及与类型处理有关的情况，供大家体会算法，或者在自己的工作中借鉴。
首先，playscript是静态类型和强类型的，所以几乎要做各种类型检查，你可以参考看看这些都是怎么做的。
第二，我们既支持对象，也支持原生的基础数据类型。这两种类型的处理特点不一样，你也可以借鉴一下。后面面向对象的一讲，我会再讲与之相关的子类型（Subtyping）和运行时类型信息（Run Time Type Information, RTTI）的概念，这里就不展开了。
第三，我们还支持函数作为一等公民，也就是支持函数的类型。函数的类型是它的原型，包括返回值和参数，原型一样的函数，就看做是同样类型的，可以进行赋值。这样，你也就可以了解实现函数式编程特性时，要处理哪些额外的类型问题。
接下来，我们来说一说如何做类型检查、类型推导和类型转换。
如何做类型检查、类型推导和类型转换先来看一看，如果编写一个编译器，我们在做类型分析时会遇到哪些问题。以下面这个最简单的表达式为例，这个表达式在不同的情况下会有不同的运行结果：
a = b + 10 如果b是一个浮点型，b+10的结果也是浮点型。如果b是字符串型的，有些语言也是允许执行+号运算的，实际的结果是字符串的连接。这个分析过程，就是类型推导（Type Inference）。 当右边的值计算完，赋值给a的时候，要检查左右两边的类型是否匹配。这个过程，就是类型检查（Type Checking）。 如果a的类型是浮点型，而右边传过来的是整型，那么一般就要进行缺省的类型转换（Type Conversion）。 类型的检查、推导和转换是三个工作，可是采用的技术手段差不多，所以我们放在一起讲，先来看看类型的推导。
在早期的playscript的实现中，是假设运算符两边的类型都是整型的，并做了强制转换。
这在实际应用中，当然不够用，因为我们还需要用到其他的数据类型。那怎么办呢？在运行时再去判断和转换吗？当然可以，但我们还有更好的选择，就是在编译期先判断出表达式的类型来。比如下面这段代码，是在RefResolve.java中，推导表达式的类型：
case PlayScriptParser.ADD: if (type1 == PrimitiveType.String || type2 == PrimitiveType.</description></item><item><title>11｜基于C语言的虚拟机（二）：性能增长10倍的秘密</title><link>https://artisanbox.github.io/3/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/13/</guid><description>你好，我是宫文学。
上一节课，我们初步实现了一个C语言版本的虚拟机，让它顺利地跑起来了。你想想看，用TypeScript生成字节码文件，然后在一个C语言实现的虚拟机上去运行，这个设计，其实和Java应用、Andorid应用、Erlang应用、Lua应用等的运行机制是一样的。也就是说，如果退回到智能手机刚诞生的年代，你完全可以像Android的发明人一样，用这种方式提供一个移动应用开发工具。
其实，我国最新的自主操作系统HarmonyOS，也是采用了像我们这门课一样的虚拟机设计机制，而且用的就是TypeScript语言，这也是我这门课采用TypeScript作为教学语言的原因之一。虽然我还没有看到HarmonyOS的虚拟机代码，但并不妨碍我去理解它的实现原理。当然了，你在学完这门课以后，也会更容易理解HarmonyOS的开发方式，而且也有助于你阅读它的虚拟机的代码。
好了，对于我们当前成果的吹捧到此打住。让我们回到现实，现实有点残酷：我们当前实现的基于C语言的虚拟机，在上一节课的性能测试中，竟然排名倒数第一。这显然不正常，这也说明了在虚拟机的设计中，我们还有一些重要的设计考虑被忽视了。
那这一节课呢，我们就来分析一下导致我们虚拟机性能不高的原因，并且针对性地解决掉这个问题。在这个过程中，你会加深对计算机语言的运行时技术的理解，特别是对内存管理的理解。
那首先，让我们把产生性能问题的可能原因分析一下。
性能问题的分析首先，我们应该了解到一点，现代的JavaScript引擎，性能确实挺高的。
在我们TypeScript版本的虚拟机中，TypeScript被编译成了JavaScript，并在Node.js中运行，而Node.js又是基于V8引擎的。
在互联网的早期，JavaScript的运行效率比较低。但是后来，以V8为代表的JavaScript引擎，性能有了大幅度的提升，使得现在的Web前端可以实现很复杂的功能。
V8在运行JavaScript的时候，会做即时编译（JIT）。V8的即时编译器，能够根据运行时收集的信息对类型做推测，这也就避免了由于运行时的类型判断而产生的额外开销，从而生成了跟提前编译（AOT）差不多的代码。如果你想了解更多细节，你可以去看看我在《编译原理实战课》中对V8的剖析。
从原理上来说，运行时的推测机制，甚至会生成比提前编译（AOT）更高效的代码。因为它拥有运行时的统计信息，并通过某些优化算法（参考JVM的局部逃逸分析算法）实现了更好的编译优化。
换一句话说，V8也是编译生成了机器码，甚至有时候会生成更高效的机器码。仅从这一点看，它并不会比C语言的提前编译差。
不过，JavaScript毕竟是动态类型的语言，它的编译和运行过程会有一些额外的开销。
比如，编译后的目标代码总要留出一些口子，用来处理类型预测失效的情况。这个时候，它会从运行本地代码的状态退回到解释器去执行。
所以，平均来说，JavaScript编写的程序，性能不会比C/C++更高。它在某些场景下能接近C/C++的性能，已经相当惊人了。
可是，在上一节中TypeScript版本虚拟机的性能居然是C语言版本的2倍半，这就太不正常了。一定还有别的因素在起作用。
所以，我们来看看第二方面的因素，就是运行时的设计。
在前面的讨论中，我们比较关注的是编译技术与性能的关系。不过，在一个虚拟机中，还会有其他影响性能的因素，这就是语言的运行时。运行时就是支撑我们的应用程序运行所需要的一些软件功能，最常见的运行时功能就是内存管理机制和并发机制。
在这里，我们重点要看一下内存管理机制。通常我们提到内存管理的时候，一下子就想到垃圾收集机制去了。其实，这只是内存管理的一半工作，完整的内存管理功能还要包括内存的申请机制。
在像Java、JavaScript这样的语言中，语言的运行时需要根据程序的指令，随时在内存中创建对象，然后在程序用不到这些对象的时候，再使用垃圾收集机制，把这些对象所占据的内存释放掉。
那么重点就来了：申请和释放内存，有时会导致巨大的性能开销。一个好的运行时，必须想办法降低这些开销。
我们初版的C语言虚拟机可能就存在这方面的问题。不过，计算机语言的运行时，都是从堆里申请和管理内存的。为了让你理解内存管理和性能的关系，更好地排查出影响C语言虚拟机性能的原因，我们首先回顾一下栈和堆这两种基础的内存管理机制。
两种内存管理机制：栈和堆在现代的操作系统中，为了支持应用的运行，通常会提供栈和堆这两种内存管理机制。当我们在一个C语言的函数里使用本地变量时，这些本地变量所需的内存是在栈里申请的，这也就是这个函数所使用的栈桢。而当我们用C语言的malloc函数申请一块内存的时候，这块内存就是从堆里申请的。
不过，只有像C/C++这样直接编译成本地代码的语言，才可以使用操作系统的栈来保存栈桢。在后面的课程中，我们也会生成与栈桢管理有关的汇编代码，管理栈桢通常需要修改特定寄存器的值，以及使用push、pop等辅助的指令。
在我们的解释器所使用的栈桢是自己管理的，本质都是从堆里申请的。从栈里和堆里申请内存的开销是不一样的。
从栈里申请内存很简单，基本上只需要修改栈顶指针，也就是某个特定寄存器的值就行了，栈就会自动地伸缩，整个栈的地址空间始终是连续的一整块内存。
而堆就不是了。从堆里申请的内存，由于每个对象的生存期是不一样的，所以就会形成很多的“空洞”，导致内存碎片化。这样，再次申请内存的时候，操作系统需要找到一块大小合适的自由内存空间。这个过程，就需要消耗一定的计算量。在内存碎片化越来越严重的情况下，找到一块可用内存空间的开销会越来越大。
另外，程序的并发也会为堆的内存申请带来额外的开销。在现代操作系统中，每个线程都有自己独享的栈，相互之间不会干扰，但堆却是各个线程所共享的。所以，在分配内存的时候，操作系统会进行线程间的同步，每次只能为一个线程分配内存，避免同一块内存被分配给多个线程。这显然也会降低系统的性能。
现在你再回头来看看我们的C语言虚拟机的实现。在栈桢和操作数栈这两个数据结构中，有好几个地方都是指针，比如本地变量的数组、操作数栈，以及操作数栈中的数据区。按照常规的编程方法，我们为每个指针都单独申请了内存。
typedef struct _StackFrame{ //本栈桢对应的函数，用来找到代码 FunctionSymbol* functionSym; //返回地址 int returnIndex; //本地变量数组 NUMBER* localVars; //操作数栈 OprandStack* oprandStack; //指向前一个栈桢的链接 struct _StackFrame * prev; }StackFrame; /**
操作数栈 当栈为空的时候，top = -1; */ typedef struct _OprandStack{ NUMBER * data; //数组 int top; //栈顶的索引值 }OprandStack; 这样就导致我们一个栈桢的内存布局被切成了4小块：</description></item><item><title>12_Java编译器（四）：去除语法糖和生成字节码</title><link>https://artisanbox.github.io/7/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/12/</guid><description>你好，我是宫文学。今天是Java编译器的最后一讲，我们来探讨编译过程最后的两个步骤：去除语法糖和生成字节码。
其实今天要讲的这两个编译步骤，总体上都是为生成字节码服务的。在这一阶段，编译器首先会把语法糖对应的AST，转换成更基础的语法对应的AST，然后基于AST和符号表，来生成字节码。
从AST和符号表，到变成字节码，这可是一个很大的转变，就像把源代码转化成AST一样。那么，这个过程的实现思路是什么？有什么难点呢？
今天这一讲，我们就一起来解决以上这些问题，在这个过程中，你对Java编译器的认识会变得更加完整。
好了，我们首先来看看去除语法糖这一处理步骤。
去除语法糖（Syntactic Sugar）Java里面提供了很多的语法糖，比如泛型、Lambda、自动装箱、自动拆箱、foreach循环、变长参数、内部类、枚举类、断言（assert），等等。
你可以这么理解语法糖：它就是提高我们编程便利性的一些语法设计。既然是提高便利性，那就意味着语法糖能做到的事情，用基础语法也能做到，只不过基础语法可能更啰嗦一点儿而已。
不过，我们最终还是要把语法糖还原成基础语法结构。比如，foreach循环会被还原成更加基础的for循环。那么，问题来了，在编译过程中，究竟是如何去除语法糖的？基础语法和语法糖又有什么区别呢？
在第10讲中，我提到过，在JDK14中，去除语法糖涵盖了编译过程的四个小阶段。
TRANSTYPES：泛型处理，具体实现在TransTypes类中。 TRANSPATTERNS：处理模式匹配，具体实现在TransPattern类中。 UNLAMBDA：把LAMBDA转换成普通方法，具体实现在LambdaToMethod类中。 LOWER：其他所有的语法糖处理，如内部类、foreach循环、断言等，具体实现在Lower类中。 以上去除语法糖的处理逻辑都是相似的，它们的本质都是对AST做修改和变换。所以，接下来我挑选了两个比较有代表性的语法糖，泛型和foreach循环，和你分析它们的处理过程。
首先是对泛型的处理。
Java泛型的实现比较简单，LinkedList&amp;lt;String&amp;gt;和LinkedList对应的字节码其实是一样的。泛型信息&amp;lt;String&amp;gt;，只是用来在语义分析阶段做类型的检查。检查完之后，这些类型信息就会被去掉。
所以，Java的泛型处理，就是把AST中与泛型有关的节点简单地删掉（相关的代码在TransTypes类中）。
对于“ List&amp;lt;String&amp;gt; names = new ArrayList&amp;lt;String&amp;gt;() ”这条语句，它对应的AST的变化过程如下，其中，橙色的节点就是被去掉的泛型。
图1：对泛型的处理然后，我们分析下对foreach循环的处理。
foreach循环的意思是“遍历每一个成员”，它能够以更简洁的方式，遍历集合和数组等数据结构。在下面的示例代码中，foreach循环和基础for循环这两种处理方式的结果是等价的，但你可以看到，foreach循环会更加简洁。
public static void main(String args[]) { List&amp;lt;String&amp;gt; names = new ArrayList&amp;lt;String&amp;gt;(); ... //foreach循环 for (String name:names) System.out.println(name); //基础for循环 for ( Iterator i = names.iterator(); i.hasNext(); ) { String name = (String)i.next(); System.out.println(name); } } Java编译器把foreach循环叫做增强for循环，对应的AST节点是JCEnhancedForLoop。
针对上面的示例代码，我们来对比一下增强for循环的AST和去除语法糖之后的AST，如下图所示：
图2：foreach循环被改造成普通的for循环你可以通过反编译，来获得这些没有语法糖的代码，它跟示例代码中用到的基础for循环语句是一样的。
对foreach循环的处理，是在Lower类的visitForeachLoop方法中。
其实，你在阅读编译技术相关的文献时，应该经常会看到Lower这个词。它的意思是，让代码从对人更友好的状态，变换到对机器更友好的状态。比如说，语法糖对编程人员更友好，而基础的语句则相对更加靠近机器实现的一端，所以去除语法糖的过程是Lower。除了去除语法糖，凡是把代码向着机器代码方向所做的变换，都可以叫做Lower。以后你再见到Lower的时候，是不是就非常清楚它的意思了呢。
好了，通过对泛型和foreach循环的处理方式的探讨，现在你应该已经大致了解了去除语法糖的过程。总体来说，去除语法糖就是把AST做一些变换，让它变成更基础的语法要素，从而离生成字节码靠近了一步。</description></item><item><title>12_为什么我的MySQL会“抖”一下？</title><link>https://artisanbox.github.io/1/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/12/</guid><description>平时的工作中，不知道你有没有遇到过这样的场景，一条SQL语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。
看上去，这就像是数据库“抖”了一下。今天，我们就一起来看一看这是什么原因。
你的SQL语句为什么变“慢”了在前面第2篇文章《日志系统：一条SQL更新语句是如何执行的？》中，我为你介绍了WAL机制。现在你知道了，InnoDB在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作redo log（重做日志），也就是《孔乙己》里咸亨酒店掌柜用来记账的粉板，在更新内存写完redo log后，就返回给客户端，本次更新成功。
做下类比的话，掌柜记账的账本是数据文件，记账用的粉板是日志文件（redo log），掌柜的记忆就是内存。
掌柜总要找时间把账本更新一下，这对应的就是把内存里的数据写入磁盘的过程，术语就是flush。在这个flush操作执行之前，孔乙己的赊账总额，其实跟掌柜手中账本里面的记录是不一致的。因为孔乙己今天的赊账金额还只在粉板上，而账本里的记录是老的，还没把今天的赊账算进去。
当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。
不论是脏页还是干净页，都在内存中。在这个例子里，内存对应的就是掌柜的记忆。
接下来，我们用一个示意图来展示一下“孔乙己赊账”的整个操作过程。假设原来孔乙己欠账10文，这次又要赊9文。
图1 “孔乙己赊账”更新和flush过程回到文章开头的问题，你不难想象，平时执行很快的更新操作，其实就是在写内存和日志，而MySQL偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）。
那么，什么情况会引发数据库的flush过程呢？
我们还是继续用咸亨酒店掌柜的这个例子，想一想：掌柜在什么情况下会把粉板上的赊账记录改到账本上？
第一种场景是，粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活儿，将粉板上的记录擦掉一些，留出空位以便继续记账。当然在擦掉之前，他必须先将正确的账目记录到账本中才行。
这个场景，对应的就是InnoDB的redo log写满了。这时候系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写。我在第二讲画了一个redo log的示意图，这里我改成环形，便于大家理解。 图2 redo log状态图checkpoint可不是随便往前修改一下位置就可以的。比如图2中，把checkpoint位置从CP推进到CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都flush到磁盘上。之后，图中从write pos到CP’之间就是可以再写入的redo log的区域。
第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出账本把孔乙己这笔账先加进去。
这种场景，对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。
你一定会说，这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿redo log出来应用不就行了？这里其实是从性能考虑的。如果刷脏页一定会写盘，就保证了每个数据页有两种状态：
一种是内存里存在，内存里就肯定是正确的结果，直接返回； 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。
这样的效率最高。 第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不如更新账本。
这种场景，对应的就是MySQL认为系统“空闲”的时候。当然，MySQL“这家酒店”的生意好起来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，即使是“生意好”的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”。
第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。
这种场景，对应的就是MySQL正常关闭的情况。这时候，MySQL会把内存的脏页都flush到磁盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快。
接下来，你可以分析一下上面四种场景对性能的影响。
其中，第三种情况是属于MySQL空闲时的操作，这时系统没什么压力，而第四种场景是数据库本来就要关闭了。这两种情况下，你不会太关注“性能”问题。所以这里，我们主要来分析一下前两种场景下的性能问题。
第一种是“redo log写满了，要flush脏页”，这种情况是InnoDB要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为0。
第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：
第一种是，还没有使用的； 第二种是，使用了并且是干净页； 第三种是，使用了并且是脏页。 InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。
而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。
所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：
一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。
所以，InnoDB需要有控制脏页比例的机制，来尽量避免上面的这两种情况。
InnoDB刷脏页的控制策略接下来，我就来和你说说InnoDB脏页的控制策略，以及和这些策略相关的参数。
首先，你要正确地告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时候，可以刷多快。</description></item><item><title>12_事务：怎么确保关联操作正确执行？</title><link>https://artisanbox.github.io/8/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/12/</guid><description>你好，我是朱晓峰。
我们经常会遇到这样的场景：几个相互关联的数据操作，必须是全部执行，或者全部不执行，不可以出现部分执行的情况。比如说，你从微信账号里提现100元到银行卡上，这个动作就包括了相互关联的2个步骤，首先是微信账号减100元，然后是银行卡账号加100元（这里假设没有手续费）。假如因为某种异常，这2个操作只执行了一个，另外一个没有执行，就会出现你的钱少了100元，或者你的钱多了100元的情况，这肯定是不能接受的。
如何才能确保多个关联操作全部执行呢？这时就要用到事务了。接下来我就重点讲一讲什么是事务，以及如何正确使用事务。
什么是事务？事务是MySQL的一项功能，它可以使一组数据操作（也叫DML操作，是英文Data Manipulation Language的缩写，包括SELECT、INSERT、UPDATE和DELETE），要么全部执行，要么全部不执行，不会因为某种异常情况（比如硬件故障、停电、网络中断等）出现只执行一部分操作的情况。
事务的语法结构如下所示：
START TRANSACTION 或者 BEGIN （开始事务） 一组DML语句 COMMIT（提交事务） ROLLBACK（事务回滚） 我解释一下这几个关键字。
START TRANSACTION和BEGIN：表示开始事务，意思是通知MySQL，后面的DML操作都是当前事务的一部分。 COMMIT：表示提交事务，意思是执行当前事务的全部操作，让数据更改永久有效。 ROLLBACK：表示回滚当前事务的操作，取消对数据的更改。 事务有4个主要特征，分别是原子性（atomicity）、一致性（consistency）、持久性（durability）和隔离性（isolation）。
原子性：表示事务中的操作要么全部执行，要么全部不执行，像一个整体，不能从中间打断。 一致性：表示数据的完整性不会因为事务的执行而受到破坏。 隔离性：表示多个事务同时执行的时候，不互相干扰。不同的隔离级别，相互独立的程度不同。 持久性：表示事务对数据的修改是永久有效的，不会因为系统故障而失效。 持久性非常好理解，我就不多说了，接下来我重点讲一讲事务的原子性、一致性和隔离性，这是确保关联操作正确执行的关键。
如何确保操作的原子性和数据的一致性？我借助一个超市的收银员帮顾客结账的简单场景来讲解。在系统中，结算的动作主要就是销售流水的产生和库存的消减。这里会涉及销售流水表和库存表，如下所示：
销售流水表（demo.mytrans）：
库存表（demo.inventory）：
现在，假设门店销售了5个商品编号是1的商品，这个动作实际上包括了2个相互关联的数据库操作：
向流水表中插入一条“1号商品卖了5个”的销售流水； 把库存表中的1号商品的库存减5。 这里包含了2个DML操作，为了避免意外事件导致的一个操作执行了而另一个没有执行的情况，我把它们放到一个事务里面，利用事务中数据操作的原子性，来确保数据的一致性。
mysql&amp;gt; START TRANSACTION; -- 开始事务 Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; INSERT INTO demo.mytrans VALUES (1,1,5); -- 插入流水 Query OK, 1 row affected (0.00 sec) mysql&amp;gt; UPDATE demo.inventory SET invquantity = invquantity - 5 WHERE itemnumber = 1; -- 更新库存 Query OK, 1 row affected (0.</description></item><item><title>12_排序（下）：如何用快排思想在O(n)内查找第K大元素？</title><link>https://artisanbox.github.io/2/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/13/</guid><description>上一节我讲了冒泡排序、插入排序、选择排序这三种排序算法，它们的时间复杂度都是O(n2)，比较高，适合小规模数据的排序。今天，我讲两种时间复杂度为O(nlogn)的排序算法，归并排序和快速排序。这两种排序算法适合大规模的数据排序，比上一节讲的那三种排序算法要更常用。
归并排序和快速排序都用到了分治思想，非常巧妙。我们可以借鉴这个思想，来解决非排序的问题，比如：如何在O(n)的时间复杂度内查找一个无序数组中的第K大元素？ 这就要用到我们今天要讲的内容。
归并排序的原理我们先来看归并排序（Merge Sort）。
归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。
归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。
从我刚才的描述，你有没有感觉到，分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。分治算法的思想我后面会有专门的一节来讲，现在不展开讨论，我们今天的重点还是排序算法。
前面我通过举例让你对归并有了一个感性的认识，又告诉你，归并排序用的是分治思想，可以用递归来实现。我们现在就来看看如何用递归代码来实现归并排序。
我在第10节讲的递归代码的编写技巧你还记得吗？写递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，我们先写出归并排序的递推公式。
递推公式： merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r)) 终止条件： p &amp;gt;= r 不用再继续分解 我来解释一下这个递推公式。
merge_sort(p…r)表示，给下标从p到r之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p…q)和merge_sort(q+1…r)，其中下标q等于p和r的中间位置，也就是(p+r)/2。当下标从p到q和从q+1到r这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从p到r之间的数据就也排好序了。
有了递推公式，转化成代码就简单多了。为了阅读方便，我这里只给出伪代码，你可以翻译成你熟悉的编程语言。
// 归并排序算法, A是数组，n表示数组大小 merge_sort(A, n) { merge_sort_c(A, 0, n-1) }
// 递归调用函数 merge_sort_c(A, p, r) { // 递归终止条件 if p &amp;gt;= r then return
// 取p到r之间的中间位置q q = (p+r) / 2 // 分治递归 merge_sort_c(A, p, q) merge_sort_c(A, q+1, r) // 将A[p&amp;hellip;q]和A[q+1&amp;hellip;r]合并为A[p&amp;hellip;r] merge(A[p&amp;hellip;r], A[p&amp;hellip;q], A[q+1&amp;hellip;r]) } 你可能已经发现了，merge(A[p&amp;hellip;r], A[p&amp;hellip;q], A[q+1&amp;hellip;r])这个函数的作用就是，将已经有序的A[p&amp;hellip;q]和A[q+1&amp;hellip;.</description></item><item><title>12_理解电路：从电报机到门电路，我们如何做到“千里传信”？</title><link>https://artisanbox.github.io/4/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/12/</guid><description>我们前面讲过机器指令，你应该知道，所有最终执行的程序其实都是使用“0”和“1”这样的二进制代码来表示的。上一讲里，我也向你展示了，对应的整数和字符串，其实也是用“0”和“1”这样的二进制代码来表示的。
那么你可能要问了，我知道了这个有什么用呢？毕竟我们人用纸和笔来做运算，都是用十进制，直接用十进制和我们最熟悉的符号不是最简单么？为什么计算机里我们最终要选择二进制呢？
这一讲，我和你一起来看看，计算机在硬件层面究竟是怎么表示二进制的，以此你就会明白，为什么计算机会选择二进制。
从信使到电报，我们怎么做到“千里传书”？马拉松的故事相信你听说过。公元前490年，在雅典附近的马拉松海边，发生了波斯和希腊之间的希波战争。雅典和斯巴达领导的希腊联军胜利之后，雅典飞毛腿菲迪皮德斯跑了历史上第一个马拉松，回雅典报喜。这个时候，人们在远距离报信的时候，采用的是派人跑腿，传口信或者送信的方式。
但是，这样靠人传口信或者送信的方式，实在是太慢了。在军事用途中，信息能否更早更准确地传递出去经常是事关成败的大事。所以我们看到中国古代的军队有“击鼓进军”和“鸣金收兵”，通过打鼓和敲钲发出不同的声音，来传递军队的号令。
如果我们把军队当成一台计算机，那“金”和“鼓”就是这台计算机的“1”和“0”。我们可以通过不同的编码方式，来指挥这支军队前进、后退、转向、追击等等。
“金”和“鼓”比起跑腿传口信，固然效率更高了，但是能够传递的范围还是非常有限，超出个几公里恐怕就听不见了。于是，人们发明了更多能够往更远距离传信的方式，比如海上的灯塔、长城上的烽火台。因为光速比声速更快，传的距离也可以更远。
图片来源亚历山大港外的法罗斯灯塔，位列世界七大奇迹之一，可惜现在只剩下遗迹了。可见人类社会很早就学会使用类似二进制信号的方式来传输信息但是，这些传递信息的方式都面临一个问题，就是受限于只有“1”和“0”这两种信号，不能传递太复杂的信息，那电报的发明就解决了这个问题。
从信息编码的角度来说，金、鼓、灯塔、烽火台类似电报的二进制编码。电报传输的信号有两种，一种是短促的点信号（dot信号），一种是长一点的划信号（dash信号）。我们把“点”当成“1”，把“划”当成“0”。这样一来，我们的电报信号就是另一种特殊的二进制编码了。电影里最常见的电报信号是“SOS”，这个信号表示出来就是 “点点点划划划点点点”。
比起灯塔和烽火台这样的设备，电报信号有两个明显的优势。第一，信号的传输距离迅速增加。因为电报本质上是通过电信号来进行传播的，所以从输入信号到输出信号基本上没有延时。第二，输入信号的速度加快了很多。电报机只有一个按钮，按下就是输入信号，按的时间短一点，就是发出了一个“点”信号；按的时间长一些，就是一个“划”信号。只要一个手指，就能快速发送电报。
图片来源一个摩尔斯电码的电报机而且，制造一台电报机也非常容易。电报机本质上就是一个“蜂鸣器+长长的电线+按钮开关”。蜂鸣器装在接收方手里，开关留在发送方手里。双方用长长的电线连在一起。当按钮开关按下的时候，电线的电路接通了，蜂鸣器就会响。短促地按下，就是一个短促的点信号；按的时间稍微长一些，就是一个稍长的划信号。
有了电池开关和铃铛，你就有了最简单的摩尔斯电码发报机理解继电器，给跑不动的信号续一秒有了电报机，只要铺设好电报线路，就可以传输我们需要的讯息了。但是这里面又出现了一个新的挑战，就是随着电线的线路越长，电线的电阻就越大。当电阻很大，而电压不够的时候，即使你按下开关，蜂鸣器也不会响。
你可能要说了，我们可以提高电压或者用更粗的电线，使得电阻更小，这样就可以让整个线路铺得更长一些。但是这个再长，也没办法从北京铺设到上海吧。要想从北京把电报发到上海，我们还得想些别的办法。
对于电报来说，电线太长了，使得线路接通也没有办法让蜂鸣器响起来。那么，我们就不要一次铺太长的线路，而把一小段距离当成一个线路。我们也可以跟驿站建立一个小电报站，在小电报站里面安排一个电报员。他听到上一个小电报站发来的信息，然后原样输入，发到下一个电报站去。这样，我们的信号就可以一段段传输下去，而不会因为距离太长，导致电阻太大，没有办法成功传输信号。为了能够实现这样接力传输信号，在电路里面，工程师们造了一个叫作继电器（Relay）的设备。
中继，其实就是不断地通过新的电源重新放大已经开始衰减的原有信号事实上，这个过程中，我们需要在每一阶段原样传输信号，所以你可以想想，我们是不是可以设计一个设备来代替这个电报员？相比使用人工听蜂鸣器的声音，来重复输入信号，利用电磁效应和磁铁，来实现这个事情会更容易。
我们把原先用来输出声音的蜂鸣器，换成一段环形的螺旋线圈，让电路封闭通上电。因为电磁效应，这段螺旋线圈会产生一个带有磁性的电磁场。我们原本需要输入的按钮开关，就可以用一块磁力稍弱的磁铁把它设在“关”的状态。这样，按下上一个电报站的开关，螺旋线圈通电产生了磁场之后，磁力就会把开关“吸”下来，接通到下一个电报站的电路。
如果我们在中间所有小电报站都用这个“螺旋线圈+磁性开关”的方式，来替代蜂鸣器和普通开关，而只在电报的始发和终点用普通的开关和蜂鸣器，我们就有了一个拆成一段一段的电报线路，接力传输电报信号。这样，我们就不需要中间安排人力来听打电报内容，也不需要解决因为线缆太长导致的电阻太大或者电压不足的问题了。我们只要在终点站安排电报员，听写最终的电报内容就可以了。这样是不是比之前更省事了？
事实上，继电器还有一个名字就叫作电驿，这个“驿”就是驿站的驿，可以说非常形象了。这个接力的策略不仅可以用在电报中，在通信类的科技产品中其实都可以用到。
比如说，你在家里用WiFi，如果你的屋子比较大，可能某些房间的信号就不好。你可以选用支持“中继”的WiFi路由器，在信号衰减的地方，增加一个WiFi设备，接收原来的WiFi信号，再重新从当前节点传输出去。这种中继对应的英文名词和继电器是一样的，也叫Relay。
再比如说，我们现在互联网使用的光缆，是用光信号来传输数据。随着距离的增长、反射次数的增加，信号也会有所衰减，我们同样要每隔一段距离，来增加一个用来重新放大信号的中继。
有了继电器之后，我们不仅有了一个能够接力传输信号的方式，更重要的是，和输入端通过开关的“开”和“关”来表示“1”和“0”一样，我们在输出端也能表示“1”和“0”了。
输出端的作用，不仅仅是通过一个蜂鸣器或者灯泡，提供一个供人观察的输出信号，通过“螺旋线圈 + 磁性开关”，使得我们有“开”和“关”这两种状态，这个“开”和“关”表示的“1”和“0”，还可以作为后续线路的输入信号，让我们开始可以通过最简单的电路，来组合形成我们需要的逻辑。
通过这些线圈和开关，我们也可以很容易地创建出 “与（AND）”“或（OR）”“非（NOT）”这样的逻辑。我们在输入端的电路上，提供串联的两个开关，只有两个开关都打开，电路才接通，输出的开关也才能接通，这其实就是模拟了计算机里面的“与”操作。
我们在输入端的电路，提供两条独立的线路到输出端，两条线路上各有一个开关，那么任何一个开关打开了，到输出端的电路都是接通的，这其实就是模拟了计算机中的“或”操作。
当我们把输出端的“螺旋线圈+磁性开关”的组合，从默认关掉，只有通电有了磁场之后打开，换成默认是打开通电的，只有通电之后才关闭，我们就得到了一个计算机中的“非”操作。输出端开和关正好和输入端相反。这个在数字电路中，也叫作反向器（Inverter）。
反向器的电路，其实就是开关从默认关闭变成默认开启而已与、或、非的电路都非常简单，要想做稍微复杂一点的工作，我们需要很多电路的组合。不过，这也彰显了现代计算机体系中一个重要的思想，就是通过分层和组合，逐步搭建起更加强大的功能。
回到我们前面看的电报机原型，虽然一个按钮开关的电报机很“容易”操作，但是却不“方便”操作。因为电报员要熟记每一个字母对应的摩尔斯电码，并且需要快速按键来进行输入，一旦输错很难纠正。但是，因为电路之间可以通过与、或、非组合完成更复杂的功能，我们完全可以设计一个和打字机一样的电报机，每按下一个字母按钮，就会接通一部分电路，然后把这个字母的摩尔斯电码输出出去。
虽然在电报机时代，我们没有这么做，但是在计算机时代，我们其实就是这样做的。我们不再是给计算机“0”和“1”，而是通过千万个晶体管组合在一起，最终使得我们可以用“高级语言”，指挥计算机去干什么。
总结延伸可以说，电报是现代计算机的一个最简单的原型。它和我们现在使用的现代计算机有很多相似之处。我们通过电路的“开”和“关”，来表示“1”和“0”。就像晶体管在不同的情况下，表现为导电的“1”和绝缘的“0”的状态。
我们通过电报机这个设备，看到了如何通过“螺旋线圈+开关”，来构造基本的逻辑电路，我们也叫门电路。一方面，我们可以通过继电器或者中继，进行长距离的信号传输。另一方面，我们也可以通过设置不同的线路和开关状态，实现更多不同的信号表示和处理方式，这些线路的连接方式其实就是我们在数字电路中所说的门电路。而这些门电路，也是我们创建CPU和内存的基本逻辑单元。我们的各种对于计算机二进制的“0”和“1”的操作，其实就是来自于门电路，叫作组合逻辑电路。
推荐阅读《编码：隐匿在计算机软硬件背后的语言》的第6～11章，是一个很好的入门材料，可以帮助你深入理解数字电路，值得你花时间好好读一读。
课后思考除了与、或、非之外，还有很多基础的门电路，比如“异或（XOR）门”。你可以想一想，试着搜索一些资料，设计一个异或门的电路。
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>12_设置工作模式与环境（下）：探查和收集信息</title><link>https://artisanbox.github.io/9/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/12/</guid><description>你好，我是LMOS。
上节课我们动手实现了自己的二级引导器。今天这节课我们将进入二级引导器，完成具体工作的环节。
在二级引导器中，我们要检查CPU是否支持64位的工作模式、收集内存布局信息，看看是不是合乎我们操作系统的最低运行要求，还要设置操作系统需要的MMU页表、设置显卡模式、释放中文字体文件。
今天课程的配套代码，你可以点击这里，自行下载。
检查与收集机器信息如果ldrkrl_entry()函数是总裁，那么init_bstartparm()函数则是经理，它负责管理检查CPU模式、收集内存信息，设置内核栈，设置内核字体、建立内核MMU页表数据。
为了使代码更加清晰，我们并不直接在ldrkrl_entry()函数中搞事情，而是准备在另一个bstartparm.c文件中实现一个init_bstartparm()。
下面我们就来动手实现它，如下所示。
//初始化machbstart_t结构体，清0,并设置一个标志 void machbstart_t_init(machbstart_t* initp) { memset(initp,0,sizeof(machbstart_t)); initp-&amp;gt;mb_migc=MBS_MIGC; return; } void init_bstartparm() { machbstart_t* mbsp = MBSPADR;//1MB的内存地址 machbstart_t_init(mbsp); return; } 目前我们的经理init_bstartparm()函数只是调用了一个machbstart_t_init()函数，在1MB内存地址处初始化了一个机器信息结构machbstart_t，后面随着干活越来越多，还会调用更多的函数的。
检查CPU首先要检查我们的CPU，因为它是执行程序的关键。我们要搞清楚它能执行什么形式的代码，支持64位长模式吗？
这个工作我们交给init_chkcpu()函数来干，由于我们要CPUID指令来检查CPU是否支持64位长模式，所以这个函数中需要找两个帮工：chk_cpuid、chk_cpu_longmode来干两件事，一个是检查CPU否支持CPUID指令，然后另一个用CPUID指令检查CPU支持64位长模式。
下面我们去写好它们，如下所示。
//通过改写Eflags寄存器的第21位，观察其位的变化判断是否支持CPUID int chk_cpuid() { int rets = 0; __asm__ __volatile__( &amp;quot;pushfl \n\t&amp;quot; &amp;quot;popl %%eax \n\t&amp;quot; &amp;quot;movl %%eax,%%ebx \n\t&amp;quot; &amp;quot;xorl $0x0200000,%%eax \n\t&amp;quot; &amp;quot;pushl %%eax \n\t&amp;quot; &amp;quot;popfl \n\t&amp;quot; &amp;quot;pushfl \n\t&amp;quot; &amp;quot;popl %%eax \n\t&amp;quot; &amp;quot;xorl %%ebx,%%eax \n\t&amp;quot; &amp;quot;jz 1f \n\t&amp;quot; &amp;quot;movl $1,%0 \n\t&amp;quot; &amp;quot;jmp 2f \n\t&amp;quot; &amp;quot;1: movl $0,%0 \n\t&amp;quot; &amp;quot;2: \n\t&amp;quot; : &amp;quot;=c&amp;quot;(rets) : :); return rets; } //检查CPU是否支持长模式 int chk_cpu_longmode() { int rets = 0; __asm__ __volatile__( &amp;quot;movl $0x80000000,%%eax \n\t&amp;quot; &amp;quot;cpuid \n\t&amp;quot; //把eax中放入0x80000000调用CPUID指令 &amp;quot;cmpl $0x80000001,%%eax \n\t&amp;quot;//看eax中返回结果 &amp;quot;setnb %%al \n\t&amp;quot; //不为0x80000001,则不支持0x80000001号功能 &amp;quot;jb 1f \n\t&amp;quot; &amp;quot;movl $0x80000001,%%eax \n\t&amp;quot; &amp;quot;cpuid \n\t&amp;quot;//把eax中放入0x800000001调用CPUID指令，检查edx中的返回数据 &amp;quot;bt $29,%%edx \n\t&amp;quot; //长模式 支持位 是否为1 &amp;quot;setcb %%al \n\t&amp;quot; &amp;quot;1: \n\t&amp;quot; &amp;quot;movzx %%al,%%eax \n\t&amp;quot; : &amp;quot;=a&amp;quot;(rets) : :); return rets; } //检查CPU主函数 void init_chkcpu(machbstart_t *mbsp) { if (!</description></item><item><title>12_语义分析（下）：如何做上下文相关情况的处理？</title><link>https://artisanbox.github.io/6/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/12/</guid><description>我们知道，词法分析和语法分析阶段，进行的处理都是上下文无关的。可仅凭上下文无关的处理，是不能完成一门强大的语言的。比如先声明变量，再用变量，这是典型的上下文相关的情况，我们肯定不能用上下文无关文法表达这种情况，所以语法分析阶段处理不了这个问题，只能在语义分析阶段处理。语义分析的本质，就是针对上下文相关的情况做处理。
我们之前讲到的作用域，是一种上下文相关的情况，因为如果作用域不同，能使用的变量也是不同的。类型系统也是一种上下文相关的情况，类型推导和类型检查都要基于上下文中相关的AST节点。
本节课，我们再讲两个这样的场景：引用的消解、左值和右值，然后再介绍上下文相关情况分析的一种方法：属性计算。这样，你会把语义分析就是上下文处理的本质掌握得更清楚，并掌握属性计算这个强大的方法。
我们先来说说引用的消解这个场景。
语义分析场景：引用的消解在程序里使用变量、函数、类等符号时，我们需要知道它们指的是谁，要能对应到定义它们的地方。下面的例子中，当使用变量a时，我们需要知道它是全局变量a，还是fun()函数中的本地变量a。因为不同作用域里可能有相同名称的变量，所以必须找到正确的那个。这个过程，可以叫引用消解。
/* scope.c 测试作用域 */ #include &amp;lt;stdio.h&amp;gt; int a = 1;
void fun() { a = 2; //这是指全局变量a int a = 3; //声明一个本地变量 int b = a; //这个a指的是本地变量 printf(&amp;quot;in func: a=%d b=%d \n&amp;quot;, a, b); } 在集成开发环境中，当我们点击一个变量、函数或类，可以跳到定义它的地方。另一方面，当我们重构一个变量名称、方法名称或类名称的时候，所有引用它的地方都会同步修改。这是因为IDE分析了符号之间的交叉引用关系。
函数的引用消解比变量的引用消解还要更复杂一些。
它不仅要比对函数名称，还要比较参数和返回值（可以叫函数原型，又或者叫函数的类型）。我们在把函数提升为一等公民的时候，提到函数类型（FunctionType）的概念。两个函数的类型相同，需要返回值、参数个数、每个参数的类型都能匹配得上才行。
在面向对象编程语言中，函数引用的消解也很复杂。
当一个参数需要一个对象的时候，程序中提供其子类的一个实例也是可以的，也就是子类可以用在所有需要父类的地方，例如下面的代码：
class MyClass1{} //父类 class MyClass2 extends MyClass1{} //子类
MyClass1 obj1; MyClass2 obj2;
function fun(MyClass1 obj){} //参数需要父类的实例
fun(obj2); //提供子类的实例 在C++语言中，引用的消解还要更加复杂。
它还要考虑某个实参是否能够被自动转换成形参所要求的类型，比如在一个需要double类型的地方，你给它传一个int也是可以的。
命名空间也是做引用消解的时候需要考虑的因素。
像Java、C++都支持命名空间。如果在代码前头引入了某个命名空间，我们就可以直接引用里面的符号，否则需要冠以命名空间。例如：
play.PlayScriptCompiler.Compile() //Java语言 play::PlayScriptCompiler.</description></item><item><title>12｜物理机上程序运行的硬件环境是怎么样的？</title><link>https://artisanbox.github.io/3/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/14/</guid><description>你好，我是宫文学。
在经过了几节课的努力以后，我们的语言运行引擎，从AST解释器升级成了TypeScript版的虚拟机，又升级成了C语言版的虚拟机。这个过程中，我们的语言的性能在不断地提升。并且，我们的关注点，也越来越从高层的语法语义处理层面，往底层技术方向靠拢了。
虽然我们现在的语言特性还不够丰富，但我还是想先带你继续往下钻。我们的目标是先把技术栈钻透，然后再在各个层次上扩大战果。
所以，在接下来的几节课里，我们会把程序编译成汇编代码，然后再生成二进制的可执行程序。在这个过程中，你会把很多过去比较模糊的底层机制搞清楚，我也会带你去除一些知识点的神秘面纱，让你不再畏惧它们。
在此之前，为了让你编译后的程序能够在计算机上跑起来，你必须把物理计算机上程序的运行机制搞清楚，特别是要搞清楚应用程序、操作系统和底层硬件的互动关系。这里面的一些知识点，通常很多程序员都理解得似是而非，不是太透彻。而理解了这些程序运行机制，除了能够让我们的语言在计算机上顺利地运行，还能够帮助你胜任一些系统级软件的开发任务。
今天这节课，我想先带你透彻了解程序运行的硬件环境，以及硬件架构跟我们实现计算机语言的关系。在下节课，我则会带你透彻了解程序运行的软件环境。
硬件环境和程序的运行机制其实，我们现在用的计算机、手机、物联网等大部分智能设备，它们的硬件架构都是差不多的，基本遵循下面这张图所展示的架构。而这张图上画出来的部分，都是需要我们在实现一门计算机语言的时候需要了解的。
首先我们从整体向部分逐个击破，先来看看计算机的总体架构和程序运行的原理。
对于计算机，我们最关心的是两个硬件，一个是CPU，一个是内存。它们通过计算机的总线连接在一起，这样CPU就可以读取内存中的数据和程序，并把数据写回内存。而CPU内部还会细分成更多的成分，包括高速缓存、寄存器和各种处理单元。
那在这种硬件环境下，程序是怎么运行起来的呢？通常，CPU上会有个寄存器，叫做PC计数器。通过PC计数器的值，CPU能够计算出下一条需要执行的代码的地址，然后读取这个代码并执行（根据不同的CPU架构，PC计数器中的值可能不是直接的内存地址，而需要进行一点转换和计算）。通常情况下，程序都是顺序执行的。但当遇到跳转指令时，PC计数器就会指向新的代码地址，从新的地址开始执行代码。
除了跳转指令会改变PC计数器的值，CPU的异常机制也会改变PC计数器的值，跳转到异常处理程序，处理完毕之后再回来。CPU的异常机制是CPU架构设计的一个重要组成部分。典型的异常是由硬件触发的中断。我们每次敲打键盘，都会触发一个中断，处理完毕以后会再接着运行原来的程序。也有的时候，中断可以由软件触发，比如当你Debug程序的时候，你可以控制着程序一条一条代码的执行，这也是利用了中断机制。
了解了程序总体的运行原理后，我们再通过一段代码的执行过程，深入了解一下其中的机理，并了解各个硬件成分是如何协同工作的。
一段代码的执行过程这段示例代码是三条汇编代码，你先看一下：
movl -4(%rbp), %eax addl $10, %eax movl %eax, -r(%rbp) 这三条代码都采用了统一的格式：操作码的助记符、源操作数和目标操作数。注意，不同CPU的指令集和不同的汇编器，会采用不同的格式，我这里只是举个例子。
这三条代码的意思也很简单，我来解释一下：
第1条是一个movl指令，movl能够把一个整数从一个地方拷贝到另一个地方。这里是从一个内存地址取出一个值，放到%eax寄存器，而这个内存地址是%rpb寄存器的值减去4； 第2条是一个addl指令，它把常数10加到%eax寄存器上； 第3条又是一个movl指令，这次是把%eax寄存器的值又写回第一行的那个内存地址。 理解了这三条代码的意思以后，我们来看看具体执行的时候都发生了些什么。
第一步，CPU读入第一行代码。
我们这三条代码都是存在内存里的。CPU会根据PC计数器的值，从内存里把第一条代码读进CPU里。
这里你要注意，我们刚才使用了汇编代码来表示程序，但内存里保存的，实际上是机器码。汇编代码通过汇编器可以转换成机器码。
在设计CPU的指令集的时候，我们会设计机器码的格式。比如，下图是我在RISC-V手册中找到的一张图，描述了RISC-V指令的几种编码方式。你能看到，每条指令占用32位，也就是一个整数的长度。其中opcode的意思是操作码，占用低7位，rs是源寄存器器，rd的意思是目的寄存器，imm是立即数，也就是常数。
这些指令被读入内存以后，会有一个解码的过程，也就是把操作码、源操作数、目标操作数这些信息从一条指令里拆解出来，用于后续的处理。这个解码的功能，是由CPU内部的一个功能单元完成的。
那么CPU是直接从内存中读入代码的吗？
不是的，其实CPU是从高速缓存中读入代码和数据的。通常代码和数据的高速缓存是分开的，分别叫做Instruction Cache和Data Cache。只有高速缓存中没有这些代码或数据的时候，才会从内存中读取。
高速缓存是内存和CPU之间的缓冲区。高速缓存的读写速度比内存快，能够减少CPU在读写内存过程中的等待时间。当CPU从内存里读一个数据的时候，它其实是从高速缓存中读到的；如果在高速缓存里没有，术语叫做没有命中，CPU会把这个数据旁白的一批数据都读到高速缓存，这样再读下一个数据的时候，又可以直接从高速缓存中读取了。
高速缓存可能分多级，比如叫做L1~L3，速度从高到底，容量则反过来，从低到高。并且，一般较低速的缓存是多个核共享的，而更高速的是每个核独享的。
那高速缓存的相关知识对我们实现计算机语言有什么帮助呢？
有一类优化技术，是提高程序数据的局部性，也就是把代码前后需要用到的数据，尽量都聚集在一起，这样便于一次性地加载到高速缓存。在读取下一个数据的时候，就不需要访问内存了，直接从高速缓存就可以获得了，从而提高了系统的性能。这就是数据局部性的好处。
从这个角度看，你回想一下，上一节课我们就是把栈桢的数据都放在一个连续的内存块里，也是在不经意间提高了数据的局部性。
不过，高速缓存也会带来一些麻烦。比如，当两个内核都去读写同一个内存数据的时候，它们各自使用自己的高速缓存，可能就会出现数据不一致的情况。所以，如果我们在语言层面上支持并发编程的特性，就像Java那样，那么在生成指令时就要保证数据的一致性。如果你想具体了解一下这些技术，可以再去看一下《编译原理实战课》。
理解了高速缓存以后，我们接着继续看第一条指令的执行过程。在这条指令里，目标操作数，也就是数据加载的目的地是一个寄存器。那我们再了解一下寄存器。
寄存器是CPU做运算的操作区。在典型的情况下，CPU都是把数据加载到寄存器，然后再在寄存器里做各种运算。
相比高速缓存来说，寄存器的读写速度更高，大约是内存的100倍。整体来说，寄存器、高速缓存和内存的读写速度是寄存器&amp;gt;高速缓存&amp;gt;内存。
在CPU的设计中，有些寄存器是有特定用途的，比如PC计数器用于计算代码地址，EFlags寄存器用于保存一些运算结果产生的状态等。
还有一些寄存器叫做通用寄存器，它们可以被我们的代码所使用，进行加减乘除等各种计算。在把程序编译成汇编代码的时候，我们要尽量去利用这些通用寄存器来运算。但如果寄存器不够用，就需要临时保存到内存中，把寄存器的空间腾出来。
好，现在我们对寄存器也有了基本的了解了，我们接着往下分析。在第一条指令里，还有一个源操作数，是-4(%rbp)，这代表了一个内存地址。CPU需要从内存地址里获取数据。
那CPU是如何从内存里获取数据的呢？这个过程其实比较复杂，是由多个步骤构成的，并不是一蹴而就的。
首先，CPU需要计算出内存地址。也就是从%rbp寄存器中取出现在的值，再减去4，得到要访问的数据的内存地址。这个地址计算的过程，通常也是由CPU内部一个单独的功能模块负责的。
那是不是从这个地址读取数据就行了呢？还不行，因为这个地址可能是个逻辑地址。现代CPU一般都有一个MMU单元。MMU是Memory Management Unit的缩写，也就是内存管理单元。它提供了虚拟内存管理的功能。也就是说，我们刚才计算出来的地址可能只是个逻辑地址，要经过MMU的翻译，才能获得物理的内存地址。
要实现完整的虚拟内存管理功能，还需要操作系统的支持，这个我们在下一节课还会探讨。
那现在，CPU终于得到了物理内存的地址。那么它会先从高速缓存中读数据，如果高速缓存中没有这个数据，才从内存加载。
你看，一个简单的内存访问功能，竟然涉及到这么多的细节。
解析完毕第一条指令之后，你大致也能理解第二条、第三条指令是如何执行的了。其中第二条指令，是做了一个加法运算，在这个过程中，会用到CPU内部的另一个功能单元：ALU，也就是算术逻辑运算单元。
到这里为止，我们已经提到了计算机内部的多个功能单元了，所以我们再把CPU内部的功能单元和流水线功能给总结一下。
CPU内部的功能单元和流水线对于CPU内部的结构，我们已经了解了高速缓存和寄存器。除此之外，CPU内部还包含了很多的功能单元，每个单元负责不同的功能。比如，有的单元负责获取指令，有的单元负责对指令译码，有的单元负责真正的运算，有的单元负责读取数据，有的单元负责写入数据，等等。
在阅读CPU的手册的时候，你会看到关于这个CPU的内部结构的一些信息，这个内部结构也被叫做微架构。你可以多看看这些图，即使你不能完全理解其中每个单元的含义，这也会有助于你理解CPU到底是如何运作的。下面这张图是我从Intel的手册中看到的Ice Lake型号的CPU的微架构的示意图：
我稍微解释一下这个微架构。你会看到，在图的左上角，指令高速缓存中的指令会被解码，解码后变成微指令。这里就涉及到了X86设计上的一些细节。X86使用的指令属于复杂指令集（CISC），CISC会针对特定的功能来设计一些指令，所以指令的执行效率会比较高，就像我们为了某个应用目的专门写一个程序来处理那样。
但复杂指令集也有坏处，就是指令的条数太多了，导致硬件设计会变得复杂，也不容易利用我们下面将要讲到的流水线的优势。所以，其实现代使用CISC的CPU，在内部设计上也借鉴了RISC的优点，把复杂的指令拆解成了简单的指令，或者叫做微指令，也就是图中的uop。
微指令会排成队列去执行任务，它们会到达一个调度器，由调度器调度不同的处理单元去完成不同的任务。调度器通过不同的端口（Port）来调度任务，不同的功能单元则在端口上接收任务。有的单元负责保存数据，有的单元负责加载数据，这些单元都会接到高速缓存上。还有几个端口是专门做计算的。不同的计算任务又分别由不同的计算单元承担，比如ALU是做算术运算的，LEA是做地址运算的，FMA是做浮点数运算的，等等。
不过，不同的CPU，其内部功能单元的划分是不同的。但总的来说，在执行一条指令的时候，CPU内部实际是多个单元按顺序去处理的，这被叫做指令流水线。不同CPU的流水线设计是不同的，有的分5个步骤，有的分成8个、10个甚至更多个步骤。
采用流水线技术最大的好处，就是我们不用等一条指令完全执行完毕，才去执行第二条指令。假设每条指令需要用到5个功能单元，分成5个步骤。那么在第一条指令的第一个步骤执行完毕以后，第一个功能单元就空出来了，就可以处理第二条指令了。总的来说，相当于有5条指令在并行运行。
当然了，实际上的执行过程并没有这么理想，因为不同的指令会用到不同的功能单元。比如上面示例程序的三条指令中，addl指令用到了ALU单元，而其他两条指令就没用到。而且，每个功能单元所需要的时钟周期也是不同的。所以，各条指令在执行过程中就会出现等待的情况。</description></item><item><title>13_JavaJIT编译器（一）：动手修改Graal编译器</title><link>https://artisanbox.github.io/7/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/13/</guid><description>你好，我是宫文学。
在前面的4讲当中，我们已经解析了OpenJDK中的Java编译器，它是把Java源代码编译成字节码，然后交给JVM运行。
用过Java的人都知道，在JVM中除了可以解释执行字节码以外，还可以通过即时编译（JIT）技术生成机器码来执行程序，这使得Java的性能很高，甚至跟C++差不多。反之，如果不能达到很高的性能，一定会大大影响一门语言的流行。
但是，对很多同学来说，对于编译器中后端的了解，还是比较模糊的。比如说，你已经了解了中间代码、优化算法、指令选择等理论概念，那这些知识在实际的编译器中是如何落地的呢？
所以从今天开始，我会花4讲的时间，来带你了解Java的JIT编译器的组成部分和工作流程、它的IR的设计、一些重要的优化算法，以及生成目标代码的过程等知识点。在这个过程中，你还可以印证关于编译器中后端的一些知识点。
今天这一讲呢，我首先会带你理解JIT编译的基本原理；然后，我会带你进入Graal编译器的代码内部，一起去修改它、运行它、调试它，让你获得第一手的实践经验，消除你对JIT编译器的神秘感。
认识Java的JIT编译器我们先来探究一下JIT编译器的原理。
在第5讲中，我讲过程序运行的原理：把一个指令指针指向一个内存地址，CPU就可以读取其中的内容，并作为指令来执行。
所以，Java后端的编译器只要生成机器码就行了。如果是在运行前一次性生成，就叫做提前编译（AOT）；如果是在运行时按需生成机器码，就叫做即时编译（JIT）。Java以及基于JVM的语言，都受益于JVM的JIT编译器。
在JDK的源代码中，你能找到src/hotspot目录，这是JVM的运行时，它们都是用C++编写的，其中就包括JIT编译器。标准JDK中的虚拟机呢，就叫做HotSpot。
实际上，HotSpot带了两个JIT编译器，一个叫做C1，又叫做客户端编译器，它的编译速度快，但优化程度低。另一个叫做C2，又叫做服务端编译器，它的编译速度比较慢，但优化程度更高。这两个编译器在实际的编译过程中，是被结合起来使用的。而字节码解释器，我们可以叫做是C0，它的运行速度是最慢的。
在运行过程中，HotSpot首先会用C0解释执行；接着，HotSpot会用C1快速编译，生成机器码，从而让运行效率提升。而对于运行频率高的热点（HotSpot）代码，则用C2深化编译，得到运行效率更高的代码，这叫做分层编译（Tiered Compilation）。
图1：分层编译由于C2会做一些激进优化，比如说，它会根据程序运行的统计信息，认为某些程序分支根本不会被执行，从而根本不为这个分支生成代码。不过，有时做出这种激进优化的假设其实并不成立，那这个时候就要做一个逆优化（Deoptimization），退回到使用C1的代码，或退回到用解释器执行。
触发即时编译，需要检测热点代码。一般是以方法为单位，虚拟机会看看该方法的运行频次是否很高，如果运行特别频繁，那么就会被认定为是热点代码，从而就会被触发即时编译。甚至如果一个方法里，有一个循环块是热点代码（比如循环1.5万次以上），这个时候也会触发编译器去做即时编译，在这个方法还没运行完毕的时候，就被替换成了机器码的版本。由于这个时候，该方法的栈帧还在栈上，所以我们把这个技术叫做栈上替换（On-stack Replacement，OSR）。栈上替换的技术难点，在于让本地变量等数据无缝地迁移，让运行过程可以正确地衔接。
Graal：用Java编写的JIT编译器如果想深入地研究Java所采用的JIT编译技术，我们必须去看它的源码。可是，对于大多数Java程序员来说，如果去阅读C++编写的编译器代码，肯定会有些不适应。
一个好消息是，Oracle公司推出了一个完全用Java语言编写的JIT编译器：Graal，并且也有开放源代码的社区版，你可以下载安装并使用。
用Java开发一款编译器的优点是很明显的。
首先，Java是内存安全的，而C++程序的很多Bug都与内存管理有关，比如可能不当地使用了指针之类的。 第二，与Java配套的各种工具（比如IDE）更友好、更丰富。 第三，Java的性能并不低，所以能够满足对编译速度的需求。 最后，用Java编译甚至还能节省内存的占用，因为Java采用的是动态内存管理技术，一些对象没用了，其内存就会被回收。而用C++编写的话，可能会由于程序员的疏忽，导致一些内存没有被及时释放。 从Java9开始，你就可以用Graal来替换JDK中的JIT编译器。这里有一个JVMCI（JVM Compiler Interface）接口标准，符合这个接口标准的JIT编译器，都可以被用于JVM。
Oracle公司还专门推出了一款JVM，叫做GraalVM。它除了用Graal作为即时编译器以外，还提供了一个很创新的功能：在一个虚拟机上支持多种语言，并且支持它们之间的互操作。你知道，传统的JVM上已经能够支持多种语言，比如Scala、Clojure等。而新的GraalVM会更进一步，它通过一个Truffle框架，可以支持JavaScript、Ruby、R、Python等需要解释执行的语言。
再进一步，它还通过一个Sulong框架支持LLVM IR，从而支持那些能够生成LLVM IR的语言，如C、C++、Rust等。想想看，在Java的虚拟机上运行C语言，还是有点开脑洞的！
图2：GraalVM的架构最后，GraalVM还支持AOT编译，这就让Java可以编译成本地代码，让程序能更快地启动并投入高速运行。我听说最近的一些互联网公司，已经在用Graal做AOT编译，来生成本地镜像，提高应用的启动时间，从而能够更好地符合云原生技术的要求。
修改并运行Graal好，那接下来，我就带你一起动手修改一下Graal编译器，在这个过程中，你就能对Graal的程序结构熟悉起来，消除对它的陌生感，有助于后面深入探索其内部的实现机制。
在本课程中，我采用了Graal的20.0.1版本的源代码。你可以参考Graal中的文档来做编译工作。
首先，下载源代码（指定了代码的分支）：
git clone -b vm-20.0.1 https://github.com/oracle/graal.git 接着，下载GraalVM的构建工具mx，它是用Python2.7编写的，你需要有正确的Python环境：
git clone https://github.com/graalvm/mx.git export PATH=$PWD/mx:$PATH 你需要在自己的机器上设置好JDK8或11的环境。我这里是在macOS上，采用JDK8。
export PATH=&amp;quot;/Library/Java/JavaVirtualMachines/openjdk1.8.0_252-jvmci-20.1-b02-fastdebug/Contents/Home/bin:$PATH&amp;quot; export JAVA_HOME=/Library/Java/JavaVirtualMachines/openjdk1.8.0_252-jvmci-20.1-b02-fastdebug/Contents/Home 好了，现在你就可以编译Graal了。你可以在Graal源代码的compiler子目录中，运行mx build：
mx build 编译完毕以后，你可以写一个小小的测试程序，来测试Graal编译器的功能。
javac Foo.java //编译Foo.java mx vm Foo //运行Foo.java，相当于执行java Foo “mx vm”命令在第一次运行的时候，会打包出一个新的GraalVM，它所需要的HotSpot VM，是从JDK中拷贝过来的，然后它会把Graal编译器等其他模块也添加进去。
Foo.java的源代码如下。在这个示例程序中，main方法会无限次地调用add方法，所以add方法就成为了热点代码，这样会逼迫JIT编译器把add方法做即时编译。
public class Foo{ public static void main(String args[]){ int i = 0; while(true){ if(i%1000==0){ System.</description></item><item><title>13_临时表：复杂查询，如何保存中间结果？</title><link>https://artisanbox.github.io/8/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/13/</guid><description>你好，我是朱晓峰。今天，我来和你聊一聊临时表。
当我们遇到一些复杂查询的时候，经常无法一步到位，或者是一步到位会导致查询语句太过复杂，开发和维护的成本过高。这个时候，就可以使用临时表。
下面，我就结合实际的项目来讲解一下，怎么拆解一个复杂的查询，通过临时表来保存中间结果，从而把一个复杂查询变得简单而且容易实现。
临时表是什么？临时表是一种特殊的表，用来存储查询的中间结果，并且会随着当前连接的结束而自动删除。MySQL中有2种临时表，分别是内部临时表和外部临时表：
内部临时表主要用于性能优化，由系统自动产生，我们无法看到； 外部临时表通过SQL语句创建，我们可以使用。 因为我们不能使用内部临时表，所以我就不多讲了。今天，我来重点讲一讲我们可以创建和使用的外部临时表。
首先，你要知道临时表的创建语法结构：
CREATE TEMPORARY TABLE 表名 ( 字段名 字段类型, ... ); 跟普通表相比，临时表有3个不同的特征：
临时表的创建语法需要用到关键字TEMPORARY； 临时表创建完成之后，只有当前连接可见，其他连接是看不到的，具有连接隔离性； 临时表在当前连接结束之后，会被自动删除。 因为临时表有连接隔离性，不同连接创建相同名称的临时表也不会产生冲突，适合并发程序的运行。而且，连接结束之后，临时表会自动删除，也不用担心大量无用的中间数据会残留在数据库中。因此，我们就可以利用这些特点，用临时表来存储SQL查询的中间结果。
如何用临时表简化复杂查询？刚刚提到，临时表可以简化复杂查询，具体是怎么实现的呢？我来介绍一下。
举个例子，超市经营者想要查询2020年12月的一些特定商品销售数量、进货数量、返厂数量，那么，我们就要先把销售、进货、返厂这3个模块分开计算，用临时表来存储中间计算的结果，最后合并在一起，形成超市经营者想要的结果集。
首先，我们统计一下在2020年12月的商品销售数据。
假设我们的销售流水表（mysales）如下所示：
我们可以用下面的SQL语句，查询出每个单品的销售数量和销售金额，并存入临时表：
mysql&amp;gt; CREATE TEMPORARY TABLE demo.mysales -&amp;gt; SELECT -- 用查询的结果直接生成临时表 -&amp;gt; itemnumber, -&amp;gt; SUM(quantity) AS QUANTITY, -&amp;gt; SUM(salesvalue) AS salesvalue -&amp;gt; FROM -&amp;gt; demo.transactiondetails -&amp;gt; GROUP BY itemnumber -&amp;gt; ORDER BY itemnumber; Query OK, 2 rows affected (0.01 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql&amp;gt; SELECT * FROM demo.</description></item><item><title>13_为什么表数据删掉一半，表文件大小不变？</title><link>https://artisanbox.github.io/1/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/13/</guid><description>经常会有同学来问我，我的数据库占用空间太大，我把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？
那么今天，我就和你聊聊数据库表的空间回收，看看如何解决这个问题。
这里，我们还是针对MySQL中应用最广泛的InnoDB引擎展开讨论。一个InnoDB表包含两部分，即：表结构定义和数据。在MySQL 8.0版本以前，表结构是存在以.frm为后缀的文件里。而MySQL 8.0版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。
接下来，我会先和你说明为什么简单地删除表数据达不到表空间回收的效果，然后再和你介绍正确回收空间的方法。
参数innodb_file_per_table表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数innodb_file_per_table控制的：
这个参数设置为OFF表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；
这个参数设置为ON表示的是，每个InnoDB表数据存储在一个以 .ibd为后缀的文件中。
从MySQL 5.6.6版本开始，它的默认值就是ON了。
我建议你不论使用MySQL的哪个版本，都将这个值设置为ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过drop table命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。
所以，将innodb_file_per_table设置为ON，是推荐做法，我们接下来的讨论都是基于这个设置展开的。
我们在删除整个表的时候，可以使用drop table命令回收表空间。但是，我们遇到的更多的删除数据的场景是删除某些行，这时就遇到了我们文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。
我们要彻底搞明白这个问题的话，就要从数据删除流程说起了。
数据删除流程我们先再来看一下InnoDB中一个索引的示意图。在前面第4和第5篇文章中，我和你介绍索引时曾经提到过，InnoDB里的数据都是用B+树的结构组织的。
图1 B+树索引示意图假设，我们要删掉R4这个记录，InnoDB引擎只会把R4这个记录标记为删除。如果之后要再插入一个ID在300和600之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。
现在，你已经知道了InnoDB的数据是按页存储的，那么如果我们删掉了一个数据页上的所有记录，会怎么样？
答案是，整个数据页就可以被复用了。
但是，数据页的复用跟记录的复用是不同的。
记录的复用，只限于符合范围条件的数据。比如上面的这个例子，R4这条记录被删除后，如果插入一个ID是400的行，可以直接复用这个空间。但如果插入的是一个ID是800的行，就不能复用这个位置了。
而当整个页从B+树里面摘掉以后，可以复用到任何位置。以图1为例，如果将数据页page A上的所有记录删除以后，page A会被标记为可复用。这时候如果要插入一条ID=50的记录需要使用新页的时候，page A是可以被复用的。
如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。
进一步地，如果我们用delete命令把整个表的数据删除呢？结果就是，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。
你现在知道了，delete命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过delete命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。
实际上，不止是删除数据会造成空洞，插入数据也会。
如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。
假设图1中page A已经满了，这时我要再插入一行数据，会怎样呢？
图2 插入数据导致页分裂可以看到，由于page A满了，再插入一个ID是550的数据时，就不得不再申请一个新的页面page B来保存数据了。页分裂完成后，page A的末尾就留下了空洞（注意：实际上，可能不止1个记录的位置是空洞）。
另外，更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的。
也就是说，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。
而重建表，就可以达到这样的目的。
重建表试想一下，如果你现在有一个表A，需要做空间收缩，为了把表中存在的空洞去掉，你可以怎么做呢？
你可以新建一个与表A结构相同的表B，然后按照主键ID递增的顺序，把数据一行一行地从表A里读出来再插入到表B中。
由于表B是新建的表，所以表A主键索引上的空洞，在表B中就都不存在了。显然地，表B的主键索引更紧凑，数据页的利用率也更高。如果我们把表B作为临时表，数据从表A导入表B的操作完成后，用表B替换A，从效果上看，就起到了收缩表A空间的作用。
这里，你可以使用alter table A engine=InnoDB命令来重建表。在MySQL 5.5版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表B不需要你自己创建，MySQL会自动完成转存数据、交换表名、删除旧表的操作。
图3 改锁表DDL显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表A的话，就会造成数据丢失。因此，在整个DDL过程中，表A中不能有更新。也就是说，这个DDL不是Online的。
而在MySQL 5.6版本开始引入的Online DDL，对这个操作流程做了优化。
我给你简单描述一下引入了Online DDL之后，重建表的流程：
建立一个临时文件，扫描表A主键的所有数据页；
用数据页中表A的记录生成B+树，存储到临时文件中；</description></item><item><title>13_加法器：如何像搭乐高一样搭电路（上）？</title><link>https://artisanbox.github.io/4/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/13/</guid><description>上一讲，我们看到了如何通过电路，在计算机硬件层面设计最基本的单元，门电路。我给你看的门电路非常简单，只能做简单的 “与（AND）”“或（OR）”“NOT（非）”和“异或（XOR）”，这样最基本的单比特逻辑运算。下面这些门电路的标识，你需要非常熟悉，后续的电路都是由这些门电路组合起来的。
这些基本的门电路，是我们计算机硬件端的最基本的“积木”，就好像乐高积木里面最简单的小方块。看似不起眼，但是把它们组合起来，最终可以搭出一个星球大战里面千年隼这样的大玩意儿。我们今天包含十亿级别晶体管的现代CPU，都是由这样一个一个的门电路组合而成的。
图片来源异或门和半加器我们看到的基础门电路，输入都是两个单独的bit，输出是一个单独的bit。如果我们要对2个8 位（bit）的数，计算与、或、非这样的简单逻辑运算，其实很容易。只要连续摆放8个开关，来代表一个8位数。这样的两组开关，从左到右，上下单个的位开关之间，都统一用“与门”或者“或门”连起来，就是两个8位数的AND或者OR的运算了。
比起AND或者OR这样的电路外，要想实现整数的加法，就需要组建稍微复杂一点儿的电路了。
我们先回归一个最简单的8位的无符号整数的加法。这里的“无符号”，表示我们并不需要使用补码来表示负数。无论高位是“0”还是“1”，这个整数都是一个正数。
我们很直观就可以想到，要表示一个8位数的整数，简单地用8个bit，也就是8个像上一讲的电路开关就好了。那2个8位整数的加法，就是2排8个开关。加法得到的结果也是一个8位的整数，所以又需要1排8位的开关。要想实现加法，我们就要看一下，通过什么样的门电路，能够连接起加数和被加数，得到最后期望的和。
其实加法器就是想一个办法把这三排开关电路连起来要做到这一点，我们先来看看，我们人在计算加法的时候一般会怎么操作。二进制的加法和十进制没什么区别，所以我们一样可以用列竖式来计算。我们仍然是从右到左，一位一位进行计算，只是把从逢10进1变成逢2进1。
你会发现，其实计算一位数的加法很简单。我们先就看最简单的个位数。输入一共是4种组合，00、01、10、11。得到的结果，也不复杂。
一方面，我们需要知道，加法计算之后的个位是什么，在输入的两位是00和11的情况下，对应的输出都应该是0；在输入的两位是10和01的情况下，输出都是1。结果你会发现，这个输入和输出的对应关系，其实就是我在上一讲留给你的思考题里面的“异或门（XOR）”。
讲与、或、非门的时候，我们很容易就能和程序里面的“AND（通常是&amp;amp;符号）”“ OR（通常是 | 符号）”和“ NOT（通常是 !符号）”对应起来。可能你没有想过，为什么我们会需要“异或（XOR）”，这样一个在逻辑运算里面没有出现的形式，作为一个基本电路。其实，异或门就是一个最简单的整数加法，所需要使用的基本门电路。
算完个位的输出还不算完，输入的两位都是11的时候，我们还需要向更左侧的一位进行进位。那这个就对应一个与门，也就是有且只有在加数和被加数都是1的时候，我们的进位才会是1。
所以，通过一个异或门计算出个位，通过一个与门计算出是否进位，我们就通过电路算出了一个一位数的加法。于是，我们把两个门电路打包，给它取一个名字，就叫作半加器（Half Adder）。
半加器的电路演示全加器你肯定很奇怪，为什么我们给这样的电路组合，取名叫半加器（Half Adder）？莫非还有一个全加器（Full Adder）么？你猜得没错。半加器可以解决个位的加法问题，但是如果放到二位上来说，就不够用了。我们这里的竖式是个二进制的加法，所以如果从右往左数，第二列不是十位，我称之为“二位”。对应的再往左，就应该分别是四位、八位。
二位用一个半加器不能计算完成的原因也很简单。因为二位除了一个加数和被加数之外，还需要加上来自个位的进位信号，一共需要三个数进行相加，才能得到结果。但是我们目前用到的，无论是最简单的门电路，还是用两个门电路组合而成的半加器，输入都只能是两个bit，也就是两个开关。那我们该怎么办呢？
实际上，解决方案也并不复杂。我们用两个半加器和一个或门，就能组合成一个全加器。第一个半加器，我们用和个位的加法一样的方式，得到是否进位X和对应的二个数加和后的结果Y，这样两个输出。然后，我们把这个加和后的结果Y，和个位数相加后输出的进位信息U，再连接到一个半加器上，就会再拿到一个是否进位的信号V和对应的加和后的结果W。
全加器就是两个半加器加上一个或门这个W就是我们在二位上留下的结果。我们把两个半加器的进位输出，作为一个或门的输入连接起来，只要两次加法中任何一次需要进位，那么在二位上，我们就会向左侧的四位进一位。因为一共只有三个bit相加，即使3个bit都是1，也最多会进一位。
这样，通过两个半加器和一个或门，我们就得到了一个，能够接受进位信号、加数和被加数，这样三个数组成的加法。这就是我们需要的全加器。
有了全加器，我们要进行对应的两个8 bit数的加法就很容易了。我们只要把8个全加器串联起来就好了。个位的全加器的进位信号作为二位全加器的输入信号，二位全加器的进位信号再作为四位的全加器的进位信号。这样一层层串接八层，我们就得到了一个支持8位数加法的算术单元。如果要扩展到16位、32位，乃至64位，都只需要多串联几个输入位和全加器就好了。
8位加法器可以由8个全加器串联而成唯一需要注意的是，对于这个全加器，在个位，我们只需要用一个半加器，或者让全加器的进位输入始终是0。因为个位没有来自更右侧的进位。而最左侧的一位输出的进位信号，表示的并不是再进一位，而是表示我们的加法是否溢出了。
这也是很有意思的一点。以前我自己在了解二进制加法的时候，一直有这么个疑问，既然int这样的16位的整数加法，结果也是16位数，那我们怎么知道加法最终是否溢出了呢？因为结果也只存得下加法结果的16位数。我们并没有留下一个第17位，来记录这个加法的结果是否溢出。
看到全加器的电路设计，相信你应该明白，在整个加法器的结果中，我们其实有一个电路的信号，会标识出加法的结果是否溢出。我们可以把这个对应的信号，输出给到硬件中其他标志位里，让我们的计算机知道计算的结果是否溢出。而现代计算机也正是这样做的。这就是为什么你在撰写程序的时候，能够知道你的计算结果是否溢出在硬件层面得到的支持。
总结延伸相信到这里，你应该已经体会到了，通过门电路来搭建算术计算的一个小功能，就好像搭乐高积木一样。
我们用两个门电路，搭出一个半加器，就好像我们拿两块乐高，叠在一起，变成一个长方形的乐高，这样我们就有了一个新的积木组件，柱子。我们再用两个柱子和一个长条的积木组合一下，就变成一个积木桥。然后几个积木桥串接在一起，又成了积木楼梯。
当我们想要搭建一个摩天大楼，我们需要很多很多楼梯。但是这个时候，我们已经不再关注最基础的一节楼梯是怎么用一块块积木搭建起来的。这其实就是计算机中，无论软件还是硬件中一个很重要的设计思想，分层。
从简单到复杂，我们一层层搭出了拥有更强能力的功能组件。在上面的一层，我们只需要考虑怎么用下一层的组件搭建出自己的功能，而不需要下沉到更低层的其他组件。就像你之前并没有深入学习过计算机组成原理，一样可以直接通过高级语言撰写代码，实现功能。
在硬件层面，我们通过门电路、半加器、全加器一层层搭出了加法器这样的功能组件。我们把这些用来做算术逻辑计算的组件叫作ALU，也就是算术逻辑单元。当进一步打造强大的CPU时，我们不会再去关注最细颗粒的门电路，只需要把门电路组合而成的ALU，当成一个能够完成基础计算的黑盒子就可以了。
以此类推，后面我们讲解CPU的设计和数据通路的时候，我们以ALU为一个基础单元来解释问题，也就够了。
补充阅读出于性能考虑，实际CPU里面使用的加法器，比起我们今天讲解的电路还有些差别，会更复杂一些。真实的加法器，使用的是一种叫作超前进位加法器的东西。你可以找到北京大学在Coursera上开设的《计算机组成》课程中的Video-306 “加法器优化”一节，了解一下超前进位加法器的实现原理，以及我们为什么要使用它。
课后思考这一讲，我给你详细讲解了无符号数的加法器是怎么通过电路搭建出来的。那么，如果是使用补码表示的有符号数，这个加法器是否可以实现正数加负数这样的运算呢？如果不行，我们应该怎么搭建对应的电路呢？
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>13_第一个C函数：如何实现板级初始化？</title><link>https://artisanbox.github.io/9/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/13/</guid><description>你好，我是LMOS。
前面三节课，我们为调用Cosmos的第一个C函数hal_start做了大量工作。这节课我们要让操作系统Cosmos里的第一个C函数真正跑起来啦，也就是说，我们会真正进入到我们的内核中。
今天我们会继续在这个hal_start函数里，首先执行板级初始化，其实就是hal层（硬件抽象层，下同）初始化，其中执行了平台初始化，hal层的内存初始化，中断初始化，最后进入到内核层的初始化。
这节课的配套代码，你可以从这里下载。
第一个C函数任何软件工程，第一个函数总是简单的，因为它是总调用者，像是一个管理者，坐在那里发号施令，自己却是啥活也不干。
由于这是第一个C函数，也是初始化函数，我们还是要为它单独建立一个文件，以显示对它的尊重，依然在Cosmos/hal/x86/下建立一个hal_start.c文件。写上这样一个函数。
void hal_start() { //第一步：初始化hal层 //第二步：初始化内核层 for(;;); return; } 根据前面的设计，Cosmos是有hal层和内核层之分，所以在上述代码中，要分两步走。第一步是初始化hal层；第二步，初始化内核层。只是这两步的函数我们还没有写。
然而最后的死循环却有点奇怪，其实它的目的很简单，就是避免这个函数返回，因为这个返回了就无处可去，避免走回头路。
hal层初始化为了分离硬件的特性，我们设计了hal层，把硬件相关的操作集中在这个层，并向上提供接口，目的是让内核上层不用关注硬件相关的细节，也能方便以后移植和扩展。(关于hal层的设计，可以回顾第3节课)
也许今天我们是在x86平台上写Cosmos，明天就要在ARM平台上开发Cosmos，那时我们就可以写个ARM平台的hal层，来替换Cosmos中的x86平台的hal层。
下面我们在Cosmos/hal/x86/下建立一个halinit.c文件，写出hal层的初始化函数。
void init_hal() { //初始化平台 //初始化内存 //初始化中断 return; } 这个函数也是一个调用者，没怎么干活。不过根据代码的注释能看出，它调用的函数多一点，但主要是完成初始化平台、初始化内存、初始化中断的功能函数。
初始化平台我们先来写好平台初始化函数，因为它需要最先被调用。
这个函数主要负责完成两个任务，一是把二级引导器建立的机器信息结构复制到hal层中的一个全局变量中，方便内核中的其它代码使用里面的信息，之后二级引导器建立的数据所占用的内存都会被释放。二是要初始化图形显示驱动，内核在运行过程要在屏幕上输出信息。
下面我们在Cosmos/hal/x86/下建立一个halplatform.c文件，写上如下代码。
void machbstart_t_init(machbstart_t *initp) { //清零 memset(initp, 0, sizeof(machbstart_t)); return; } void init_machbstart() { machbstart_t *kmbsp = &amp;amp;kmachbsp; machbstart_t *smbsp = MBSPADR;//物理地址1MB处 machbstart_t_init(kmbsp); //复制，要把地址转换成虚拟地址 memcopy((void *)phyadr_to_viradr((adr_t)smbsp), (void *)kmbsp, sizeof(machbstart_t)); return; } //平台初始化函数 void init_halplaltform() { //复制机器信息结构 init_machbstart(); //初始化图形显示驱动 init_bdvideo(); return; } 这个代码中别的地方很好理解，就是kmachbsp你可能会有点奇怪，它是个结构体变量，结构体类型是machbstart_t，这个结构和二级引导器所使用的一模一样。</description></item><item><title>13_线性排序：如何根据年龄给100万用户数据排序？</title><link>https://artisanbox.github.io/2/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/14/</guid><description>上两节中，我带你着重分析了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。今天，我会讲三种时间复杂度是O(n)的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。
这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以我们今天的学习重点是掌握这些排序算法的适用场景。
按照惯例，我先给你出一道思考题：如何根据年龄给100万用户排序？ 你可能会说，我用上一节课讲的归并、快排就可以搞定啊！是的，它们也可以完成功能，但是时间复杂度最低也是O(nlogn)。有没有更快的排序方法呢？让我们一起进入今天的内容！
桶排序（Bucket sort）首先，我们来看桶排序。桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。
桶排序的时间复杂度为什么是O(n)呢？我们一块儿来分析一下。
如果要排序的数据有n个，我们把它们均匀地划分到m个桶内，每个桶里就有k=n/m个元素。每个桶内部使用快速排序，时间复杂度为O(k * logk)。m个桶排序的时间复杂度就是O(m * k * logk)，因为k=n/m，所以整个桶排序的时间复杂度就是O(n*log(n/m))。当桶的个数m接近数据个数n时，log(n/m)就是一个非常小的常量，这个时候桶排序的时间复杂度接近O(n)。
桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？
答案当然是否定的。为了让你轻松理解桶排序的核心思想，我刚才做了很多假设。实际上，桶排序对要排序数据的要求是非常苛刻的。
首先，要排序的数据需要很容易就能划分成m个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。
其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为O(nlogn)的排序算法了。
桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。
比如说我们有10GB的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百MB，没办法一次性把10GB的数据都加载到内存中。这个时候该怎么办呢？
现在我来讲一下，如何借助桶排序的处理思想来解决这个问题。
我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是1元，最大是10万元。我们将所有订单根据金额划分到100个桶里，第一个桶我们存储金额在1元到1000元之内的订单，第二桶存储金额在1001元到2000元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02...99）。
理想的情况下，如果订单金额在1到10万之间均匀分布，那订单会被均匀划分到100个文件中，每个小文件中存储大约100MB的订单数据，我们就可以将这100个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。
不过，你可能也发现了，订单按照金额在1元到10万元之间并不一定是均匀分布的 ，所以10GB订单数据是无法均匀地被划分到100个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该怎么办呢？
针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在1元到1000元之间的比较多，我们就将这个区间继续划分为10个小区间，1元到100元，101元到200元，201元到300元....901元到1000元。如果划分之后，101元到200元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。
计数排序（Counting sort）我个人觉得，计数排序其实是桶排序的一种特殊情况。当要排序的n个数据，所处的范围并不大的时候，比如最大值是k，我们就可以把数据划分成k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。
我们都经历过高考，高考查分数系统你还记得吗？我们查分数的时候，系统会显示我们的成绩以及所在省的排名。如果你所在的省有50万考生，如何通过成绩快速排序得出名次呢？
考生的满分是900分，最小是0分，这个数据的范围很小，所以我们可以分成901个桶，对应分数从0分到900分。根据考生的成绩，我们将这50万考生划分到这901个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了50万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是O(n)。
计数排序的算法思想就是这么简单，跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？
想弄明白这个问题，我们就要来看计数排序算法的实现方法。我还拿考生那个例子来解释。为了方便说明，我对数据规模做了简化。假设只有8个考生，分数在0到5分之间。这8个考生的成绩我们放在一个数组A[8]中，它们分别是：2，5，3，0，2，3，0，3。
考生的成绩从0到5分，我们使用大小为6的数组C[6]表示桶，其中下标对应分数。不过，C[6]内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到C[6]的值。
从图中可以看出，分数为3分的考生有3个，小于3分的考生有4个，所以，成绩为3分的考生在排序之后的有序数组R[8]中，会保存下标4，5，6的位置。
那我们如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？这个处理方法非常巧妙，很不容易想到。
思路是这样的：我们对C[6]数组顺序求和，C[6]存储的数据就变成了下面这样子。C[k]里存储小于等于分数k的考生个数。
有了前面的数据准备之后，现在我就要讲计数排序中最复杂、最难理解的一部分了，请集中精力跟着我的思路！
我们从后到前依次扫描数组A。比如，当扫描到3时，我们可以从数组C中取出下标为3的值7，也就是说，到目前为止，包括自己在内，分数小于等于3的考生有7个，也就是说3是数组R中的第7个元素（也就是数组R中下标为6的位置）。当3放入到数组R中后，小于等于3的元素就只剩下了6个了，所以相应的C[3]要减1，变成6。
以此类推，当我们扫描到第2个分数为3的考生的时候，就会把它放入数组R中的第6个元素的位置（也就是下标为5的位置）。当我们扫描完整个数组A后，数组R内的数据就是按照分数从小到大有序排列的了。
上面的过程有点复杂，我写成了代码，你可以对照着看下。
// 计数排序，a是数组，n是数组大小。假设数组中存储的都是非负整数。 public void countingSort(int[] a, int n) { if (n &amp;lt;= 1) return; // 查找数组中数据的范围 int max = a[0]; for (int i = 1; i &amp;lt; n; ++i) { if (max &amp;lt; a[i]) { max = a[i]; } }</description></item><item><title>13_继承和多态：面向对象运行期的动态特性</title><link>https://artisanbox.github.io/6/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/13/</guid><description>面向对象是一个比较大的话题。在“09 | 面向对象：实现数据和方法的封装”中，我们了解了面向对象的封装特性，也探讨了对象成员的作用域和生存期特征等内容。本节课，我们再来了解一下面向对象的另外两个重要特征：继承和多态。
你也许会问，为什么没有在封装特性之后，马上讲继承和多态呢？那是因为继承和多态涉及的语义分析阶段的知识点比较多，特别是它对类型系统提出了新的概念和挑战，所以我们先掌握语义分析，再了解这部分内容，才是最好的选择。
继承和多态对类型系统提出的新概念，就是子类型。我们之前接触的类型往往是并列关系，你是整型，我是字符串型，都是平等的。而现在，一个类型可以是另一个类型的子类型，比如我是一只羊，又属于哺乳动物。这会导致我们在编译期无法准确计算出所有的类型，从而无法对方法和属性的调用做完全正确的消解（或者说绑定）。这部分工作要留到运行期去做，也因此，面向对象编程会具备非常好的优势，因为它会导致多态性。这个特性会让面向对象语言在处理某些类型的问题时，更加优雅。
而我们要想深刻理解面向对象的特征，就必须了解子类型的原理和运行期的机制。所以，接下来，我们从类型体系的角度理解继承和多态，然后看看在编译期需要做哪些语义分析，再考察继承和多态的运行期特征。
从类型体系的角度理解继承和多态继承的意思是一个类的子类，自动具备了父类的属性和方法，除非被父类声明为私有的。比如一个类是哺乳动物，它有体重（weight）的属性，还会做叫(speak)的操作。如果基于哺乳动物这个父类创建牛和羊两个子类，那么牛和羊就自动继承了哺乳动物的属性，有体重，还会叫。
所以继承的强大之处，就在于重用。也就是有些逻辑，如果在父类中实现，在子类中就不必重复实现。
多态的意思是同一个类的不同子类，在调用同一个方法时会执行不同的动作。这是因为每个子类都可以重载掉父类的某个方法，提供一个不同的实现。哺乳动物会“叫”，而牛和羊重载了这个方法，发出“哞~”和“咩~”的声音。这似乎很普通，但如果创建一个哺乳动物的数组，并在里面存了各种动物对象，遍历这个数组并调用每个对象“叫”的方法时，就会发出“哞~”“咩~”“喵~”等各种声音，这就有点儿意思了。
下面这段示例代码，演示了继承和多态的特性，a的speak()方法和b的speak()方法会分别打印出牛叫和羊叫，调用的是子类的方法，而不是父类的方法：
/** mammal.play 演示面向对象编程：继承和多态。 */ class Mammal{ int weight = 20; boolean canSpeak(){ return true; } void speak(){ println(&amp;amp;quot;mammal speaking...&amp;amp;quot;); } }
class Cow extends Mammal{ void speak(){ println(&amp;quot;moo~~ moo~~&amp;quot;); } }
class Sheep extends Mammal{ void speak(){ println(&amp;quot;mee~~ mee~~&amp;quot;); println(&amp;quot;My weight is: &amp;quot; + weight); //weight的作用域覆盖子类 } }
//将子类的实例赋给父类的变量 Mammal a = Cow(); Mammal b = Sheep();
//canSpeak()方法是继承的 println(&amp;quot;a.</description></item><item><title>13｜物理机上程序运行的软件环境是怎么样的？</title><link>https://artisanbox.github.io/3/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/15/</guid><description>你好，我是宫文学。
上一节课，我们主要讨论了程序运行的硬件环境。在某些编程场景下（比如嵌入式编程），我们的语言需要直接跑在裸设备上。所以，你要能够理解在裸设备上运行某个语言的程序所需要的技术。
不过，现代语言大部分情况下都运行在某个操作系统里，操作系统为语言提供了基础的运行环境，比如定义了可执行文件的格式、程序在内存中的布局、内存管理机制，还有并发机制等等。计算机语言需要跟操作系统紧密配合，才能更好地运行。
今天这节课，我们就来讨论一下操作系统中与计算机语言有关的那些知识点，包括内存管理、任务管理和ABI，为进一步实现我们的计算机语言打下良好的基础。
首先我们来看看操作系统的内存管理功能和程序的关系。
内存管理操作系统的一个重要功能就是管理内存，它会把内存虚拟化，并进行内存访问的权限管理。
那什么是虚拟化呢？
虚拟化就是让每个进程都有一个自己可以用的寻址空间，不过这些地址是假的地址。但就这些假地址，我们通过指令发给CPU，CPU也是认的。因为CPU中有一个内存管理单元，缩写是MMU，它能够根据这个逻辑地址，计算出在内存里真实的物理地址。MMU还可以跟操作系统配合，设置每个内存页面的权限，包括是否可读、可写和可执行。
这种逻辑地址与物理地址转换的功能，对我们做编译很有用。我们的程序编译成目标代码的时候，里面的每个函数、常量和全局变量的地址，都是确定的。这样，在操作系统加载了可执行程序以后，就可以正确地调用每个函数，访问每个数据了。
如果没有这个逻辑地址的功能，那运行多个程序就困难了。我们要保证每个程序使用的地址都互相不冲突才行，否则大家就乱了套了。在不使用操作系统的编程领域（比如某些嵌入式编程），重点就要解决好这些问题。
在虚拟化机制下，只有你用到某个地址，操作系统才会为这个地址分配真实的物理内存。这些物理内存一般划分成页来管理，MMU会根据这些分页信息把逻辑地址转换成物理地址。
这里我们横向延展一下，我们在第11节课说过，在栈里申请内存的时候很简单，只需要移动一下栈顶指针就行了，其实这个内部机制和我们上面说的虚拟化也是有关的。
在X64架构下，栈顶指针使用的是rsp寄存器。但其实，这时候并没有真正分配内存，你只是改变了寄存器的值而已。但如果你访问栈里的某个地址，而且这个地址又没有被分配物理的内存页，那么CPU在访问内存的时候就会知道这里出错了。它就会触发一个缺页中断，跳转到中断处理程序，去分配页面。分配完毕以后，又跳回原来的程序接着执行，我们的程序并不知道背后发生了这么多的事情。
说到中断，我再额外跟你补充一点。CPU在处理中断的时候，要保护当前的现场，比如各个寄存器的状态。否则，如果各个寄存器的值被弄乱了，原来的程序就没法执行了。然后在返回原来的程序的时候，CPU要恢复现场。整个过程对我们的程序是透明的。
所以说，这个rsp只是起到一个标记作用，是我们的程序跟操作系统之间的一个约定。我们只要修改了rsp里的值，操作系统就要保证给我们提供足够的内存。如果你违背了这个约定去乱访问一些地址，在MMU和操作系统的配合下，也会被识别出来，就会报内存访问的错误。
既然这只是个约定，那么有的操作系统就比较为程序着想了，它规定你可以访问栈顶之外的一定范围内的内存。比如，Linux和大多数类Unix的系统都遵循System V AMD64 ABI，它规定可以访问栈顶之外的128个字节范围内的内存。
这有什么好处呢？好处是，对于程序中的叶子函数，也就是这个函数没有调用其他函数，并且它所使用的数据不会超过128个字节的情况，我们根本不需要去建立栈桢，也就省去了把栈顶指针的值保存到内存，修改栈顶指针，最后再从内存中恢复栈顶指针这一系列操作，这样就节省了大量的内存读写时间，让系统性能得到优化。
在堆中申请内存也是一样，也不是真实的分配，只是提供了一些标记信息，之后可供MMU使用而已。
好了，关于从栈和堆里申请内存的延伸就到这里，我们回归主线，继续来看虚拟化机制带来的结果。简单的说，虚拟化机制，可以让运行中的各种程序，使用相同的逻辑地址，但实际上对应的是不同的物理地址。这样各个程序加载到内存后，就都可以使用标准的内存布局。
那一个可执行程序在内存中的布局情况是怎样的呢？我们用一个C语言的程序的例子来分析一下。
在Linux或macOS系统中，一个C语言的程序加载到内存以后，它的内存布局大概是下面的样子：
你可以看到，其中代码段（.text）和数据段（包括.data和.bss）是从可执行文件直接加载进内存的。可执行文件中提前计算好的函数、常量和全局变量的地址，也就变成了内存中的地址。
另外两个重要的区域是栈和堆。栈是从高地址向低地址延伸的，而堆则是从低地址向高地址延伸的。
但是，在不同的操作系统中，上图中每个部分的具体地址都是不大相同的，比如，macOS和Linux的就不同。不过，你可以写个程序，打印出不同区域中的地址。我写了个示例程序address.c，你可以参考一下，这里你要好好琢磨一下示例代码中每个地址是如何获取的。
另外，我是在macOS上运行的，如果你用的是不同的操作系统，也可以运行一下，看看打印出来的地址跟我的有什么区别。
这个程序是这样的：
#include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;stdio.h&amp;gt; //全局变量 int global_a = 10; char * global_b = &amp;quot;hello&amp;quot;;
int main(int argc, char** argv){ printf(&amp;quot;命令行参数/&amp;amp;quot;address&amp;amp;quot;的地址:\t0x%12lX\n&amp;quot;, (size_t)argv[0]); printf(&amp;quot;命令行参数/argv数组的地址: \t0x%12lX\n&amp;quot;, (size_t)argv);
printf(&amp;amp;quot;栈/参数argc的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;argc); printf(&amp;amp;quot;栈/参数argv的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;argv); int local_a = 20; printf(&amp;amp;quot;栈/local_a的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;local_a); int * local_b = (int*)malloc(sizeof(int)); printf(&amp;amp;quot;栈/local_b的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;local_b); printf(&amp;amp;quot;堆/local_b指向的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)local_b); free(local_b); printf(&amp;amp;quot;data段/global_b的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;global_b); printf(&amp;amp;quot;data段/global_a的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;global_a); printf(&amp;amp;quot;text段/\&amp;amp;quot;hello\&amp;amp;quot;的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)global_b); printf(&amp;amp;quot;text段/main函数的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)main); }</description></item><item><title>14_count(x)这么慢，我该怎么办？</title><link>https://artisanbox.github.io/1/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/14/</guid><description>在开发系统的时候，你可能经常需要计算一个表的行数，比如一个交易系统的所有变更记录总数。这时候你可能会想，一条select count(*) from t 语句不就解决了吗？
但是，你会发现随着系统中记录数越来越多，这条语句执行得也会越来越慢。然后你可能就想了，MySQL怎么这么笨啊，记个总数，每次要查的时候直接读出来，不就好了吗。
那么今天，我们就来聊聊count(*)语句到底是怎样实现的，以及MySQL为什么会这么实现。然后，我会再和你说说，如果应用中有这种频繁变更并需要统计表行数的需求，业务设计上可以怎么做。
count(*)的实现方式你首先要明确的是，在不同的MySQL引擎中，count(*)有不同的实现方式。
MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高； 而InnoDB引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。 这里需要注意的是，我们在这篇文章里讨论的是没有过滤条件的count(*)，如果加了where 条件的话，MyISAM表也是不能返回得这么快的。
在前面的文章中，我们一起分析了为什么要使用InnoDB，因为不论是在事务支持、并发能力还是在数据安全方面，InnoDB都优于MyISAM。我猜你的表也一定是用了InnoDB引擎。这就是当你的记录数越来越多的时候，计算一个表的总行数会越来越慢的原因。
那为什么InnoDB不跟MyISAM一样，也把数字存起来呢？
这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。这里，我用一个算count(*)的例子来为你解释一下。
假设表t中现在有10000条记录，我们设计了三个用户并行的会话。
会话A先启动事务并查询一次表的总行数； 会话B启动事务，插入一行后记录后，查询表的总行数； 会话C先启动一个单独的语句，插入一行记录后，查询表的总行数。 我们假设从上到下是按照时间顺序执行的，同一行语句是在同一时刻执行的。
图1 会话A、B、C的执行流程你会看到，在最后一个时刻，三个会话A、B、C会同时查询表t的总行数，但拿到的结果却不同。
这和InnoDB的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是MVCC来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于count(*)请求来说，InnoDB只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。
备注：如果你对MVCC记忆模糊了，可以再回顾下第3篇文章《事务隔离：为什么你改了我还看不见？》和第8篇文章《事务到底是隔离的还是不隔离的？》中的相关内容。
当然，现在这个看上去笨笨的MySQL，在执行count(*)操作的时候还是做了优化的。
你知道的，InnoDB是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于count(*)这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。
如果你用过show table status 命令的话，就会发现这个命令的输出结果里面也有一个TABLE_ROWS用于显示这个表当前有多少行，这个命令执行挺快的，那这个TABLE_ROWS能代替count(*)吗？
你可能还记得在第10篇文章《 MySQL为什么有时候会选错索引？》中我提到过，索引统计的值是通过采样来估算的。实际上，TABLE_ROWS就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到40%到50%。所以，show table status命令显示的行数也不能直接使用。
到这里我们小结一下：
MyISAM表虽然count(*)很快，但是不支持事务； show table status命令虽然返回很快，但是不准确； InnoDB表直接count(*)会遍历全表，虽然结果准确，但会导致性能问题。 那么，回到文章开头的问题，如果你现在有一个页面经常要显示交易系统的操作记录总数，到底应该怎么办呢？答案是，我们只能自己计数。
接下来，我们讨论一下，看看自己计数有哪些方法，以及每种方法的优缺点有哪些。
这里，我先和你说一下这些方法的基本思路：你需要自己找一个地方，把操作记录表的行数存起来。
用缓存系统保存计数对于更新很频繁的库来说，你可能会第一时间想到，用缓存系统来支持。
你可以用一个Redis服务来保存这个表的总行数。这个表每被插入一行Redis计数就加1，每被删除一行Redis计数就减1。这种方式下，读和更新操作都很快，但你再想一下这种方式存在什么问题吗？
没错，缓存系统可能会丢失更新。
Redis的数据不能永久地留在内存里，所以你会找一个地方把这个值定期地持久化存储起来。但即使这样，仍然可能丢失更新。试想如果刚刚在数据表中插入了一行，Redis中保存的值也加了1，然后Redis异常重启了，重启后你要从存储redis数据的地方把这个值读回来，而刚刚加1的这个计数操作却丢失了。
当然了，这还是有解的。比如，Redis异常重启以后，到数据库里面单独执行一次count(*)获取真实的行数，再把这个值写回到Redis里就可以了。异常重启毕竟不是经常出现的情况，这一次全表扫描的成本，还是可以接受的。
但实际上，将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使Redis正常工作，这个值还是逻辑上不精确的。
你可以设想一下有这么一个页面，要显示操作记录的总数，同时还要显示最近操作的100条记录。那么，这个页面的逻辑就需要先到Redis里面取出计数，再到数据表里面取数据记录。
我们是这么定义不精确的：
一种是，查到的100行结果里面有最新插入记录，而Redis的计数里还没加1；
另一种是，查到的100行结果里没有最新插入的记录，而Redis的计数里已经加了1。
这两种情况，都是逻辑不一致的。
我们一起来看看这个时序图。
图2 会话A、B执行时序图图2中，会话A是一个插入交易记录的逻辑，往数据表里插入一行R，然后Redis计数加1；会话B就是查询页面显示时需要的数据。
在图2的这个时序里，在T3时刻会话B来查询的时候，会显示出新插入的R这个记录，但是Redis的计数还没加1。这时候，就会出现我们说的数据不一致。
你一定会说，这是因为我们执行新增记录逻辑时候，是先写数据表，再改Redis计数。而读的时候是先读Redis，再读数据表，这个顺序是相反的。那么，如果保持顺序一样的话，是不是就没问题了？我们现在把会话A的更新顺序换一下，再看看执行结果。
图3 调整顺序后，会话A、B的执行时序图你会发现，这时候反过来了，会话B在T3时刻查询的时候，Redis计数加了1了，但还查不到新插入的R这一行，也是数据不一致的情况。</description></item><item><title>14_JavaJIT编译器（二）：SeaofNodes为何如此强大？</title><link>https://artisanbox.github.io/7/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/14/</guid><description>你好，我是宫文学。这一讲，我们继续来研究Graal编译器，重点来了解一下它的IR的设计。
在上一讲中，我们发现Graal在执行过程中，创建了一个图的数据结构，这个数据结构就是Graal的IR。之后的很多处理和优化算法，都是基于这个IR的。可以说，这个IR是Graal编译器的核心特性之一。
那么，为什么这个IR采用的是图结构？它有什么特点和优点？编译器的优化算法又是如何基于这个IR来运行的呢？
今天，我就带你一起来攻破以上这些问题。在揭晓问题答案的过程中，你对真实编译器中IR的设计和优化处理过程，也就能获得直观的认识了。
基于图的IRIR对于编译器非常重要，因为它填补了高级语言和机器语言在语义上的巨大差别。比如说，你在高级语言中是使用一个数组，而翻译成最高效的x86机器码，是用间接寻址的方式，去访问一块连续的内存。所以IR的设计必须有利于实现这种转换，并且还要有利于运行优化算法，使得生成的代码更加高效。
在上一讲中，通过跟踪Graal编译器的执行过程，我们会发现它在一开始，就把字节码翻译成了一种新的IR，这个IR是用图的结构来表示的。那这个图长什么样子呢？非常幸运的是，我们可以用工具来直观地看到它的结构。
你可以从Oracle的网站上，下载一个idealgraphvisualizer的工具。下载之后，解压缩，并运行它：
export PATH=&amp;quot;/&amp;lt;上级目录&amp;gt;/idealgraphvisualizer/bin:$PATH&amp;quot; idealgraphvisualizer &amp;amp; 这时，程序会启动一个图形界面，并在4445端口上等待GraalVM发送数据过来。
接着，还是运行Foo示例程序，不过这次你要增加一个参数“-Dgraal.Dump”，这会让GraalVM输出编译过程的一些中间结果。并且在这个示例程序当中，我还增加了一个“-Xcomp”参数，它能让JIT编译器在第一次使用某个方法的时候，就去做编译工作。
mx vm \ -XX:+UnlockExperimentalVMOptions \ -XX:+EnableJVMCI \ -XX:+UseJVMCICompiler \ -XX:-TieredCompilation \ -XX:CompileOnly=Foo \ -Dgraal.Dump \ -Xcomp \ Foo GraalVM会在终端输出“Connected to the IGV on 127.0.0.1:4445”，这表明它连接上了idealgraphvisualizer。接着，在即时编译之后，idealgraphvisualizer就接收到了编译过程中生成的图，你可以点击显示它。
这里我展示了其中两个阶段的图，一个是刚解析完字节码之后（After parsing），一个是在处理完中间层之后（After mid tier）。
图1：After parsing图2：After mid tierGraal IR其实受到了“程序依赖图”的影响。我们在第6讲中提到过程序依赖图（PDG），它是用图来表示程序中的数据依赖和控制依赖。并且你也知道了，这种IR还有一个别名，叫做节点之海（Sea of Nodes）。因为当程序稍微复杂一点以后，图里的节点就会变得非常多，我们用肉眼很难看得清。
基于Sea of Nodes的IR呢，算是后起之秀。在HotSpot的编译器中，就采用了这种IR，而且现在Java的Graal编译器和JavaScript的V8编译器中的IR的设计，都是基于了Sea of Nodes结构，所以我们必须重视它。
这也不禁让我们感到好奇了：Sea of Nodes到底强在哪里？
我们都知道，数据结构的设计对于算法来说至关重要。IR的数据结构，会影响到算法的编写方式。好的IR的设计，会让优化算法的编写和维护都更加容易。
而Sea of Nodes最大的优点，就是能够用一个数据结构同时反映控制流和数据流，并且尽量减少它们之间的互相依赖。
怎么理解这个优点呢？在传统的编译器里，控制流和数据流是分开的。控制流是用控制流图（Control-flow Graph，CFG）来表示的，比如GNU的编译器、LLVM，都是基于控制流图的。而IR本身，则侧重于表达数据流。
以LLVM为例，它采用了SSA格式的IR，这种IR可以很好地体现值的定义和使用关系，从而很好地刻画了数据流。
而问题在于，采用这种比较传统的方式，控制流和数据流会耦合得比较紧，因为IR指令必须归属于某个基本块。
举个例子来说明一下吧。在下面的示例程序中，“int b = a*2;”这个语句，会被放到循环体的基本块中。
int foo(int a){ int sum = 0; for(int i = 0; i&amp;lt; 10; i++){ int b = a*2; //这一句可以提到外面 sum += b; } } 可是，从数据流的角度看，变量b只依赖于a。所以这个语句没必要放在循环体内，而是可以提到外面。在传统的编译器中，这一步是要分析出循环无关的变量，然后再把这条语句提出去。而如果采用Sea of Nodes的数据结构，变量b一开始根本没有归属到特定的基本块，所以也就没有必要专门去做代码的移动了。</description></item><item><title>14_Linux初始化（上）：GRUB与vmlinuz的结构</title><link>https://artisanbox.github.io/9/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/14/</guid><description>你好，我是LMOS。
在前面的课程中，我们建好了二级引导器，启动了我们的Cosmos，并进行了我们Cosmos的Hal层初始化。
我会用两节课带你领会Linux怎样做初始化。虽然我们自己具体实现过了初始化，不过我们也不妨看看Linux的初始化流程，借鉴一下Linux开发者的玩法。
这节课，我会先为你梳理启动的整体流程，重点为你解读Linux上GRUB是怎样启动，以及内核里的“实权人物”——vmlinuz内核文件是如何产生和运转的。下节课，我们从setup.bin文件的_start函数入手，研究Linux初始化流程。
好，接下来我们从全局流程讲起，正式进入今天的学习。
全局流程x86平台的启动流程，是非常复杂的。为了帮助你理解，我们先从全局粗略地看一看整体流程，然后一步步细化。
在机器加电后，BIOS会进行自检，然后由BIOS加载引导设备中引导扇区。在安装有Linux操作系统的情况下，在引导扇区里，通常是安装的GRUB的一小段程序（安装windows的情况则不同）。最后，GRUB会加载Linux的内核映像vmlinuz，如下图所示。
上图中的引导设备通常是机器中的硬盘，但也可以是U盘或者光盘甚至是软盘。BIOS会自动读取保存在CMOS中的引导设备信息。
从BIOS到GRUB从前面的课程我们已经知道，CPU被设计成只能运行内存中的程序，没有办法直接运行储存在硬盘或者U盘中的操作系统程序。
如果想要运行硬盘或者U盘中的程序，就必须要先加载到内存（RAM）中才能运行。这是因为硬盘、U盘（外部储存器）并不和CPU直接相连，它们的访问机制和寻址方式与内存截然不同。
内存在断电后就没法保存数据了，那BIOS又是如何启动的呢？硬件工程师设计CPU时，硬性地规定在加电的瞬间，强制将CS寄存器的值设置为0XF000，IP寄存器的值设置为0XFFF0。
这样一来，CS:IP就指向了0XFFFF0这个物理地址。在这个物理地址上连接了主板上的一块小的ROM芯片。这种芯片的访问机制和寻址方式和内存一样，只是它在断电时不会丢失数据，在常规下也不能往这里写入数据，它是一种只读内存，BIOS程序就被固化在该ROM芯片里。
现在，CS:IP指向了0XFFFF0这个位置，正是BIOS程序的入口地址。这意味着BIOS正式开始启动。
BIOS一开始会初始化CPU，接着检查并初始化内存，然后将自己的一部分复制到内存，最后跳转到内存中运行。BIOS的下一步就是枚举本地设备进行初始化，并进行相关的检查，检查硬件是否损坏，这期间BIOS会调用其它设备上的固件程序，如显卡、网卡等设备上的固件程序。
当设备初始化和检查步骤完成之后，BIOS会在内存中建立中断表和中断服务程序，这是启动Linux至关重要的工作，因为Linux会用到它们。
具体是怎么操作的呢？BIOS会从内存地址（0x00000）开始用1KB的内存空间（0x00000~0x003FF）构建中断表，在紧接着中断表的位置，用256KB的内存空间构建BIOS数据区（0x00400~0x004FF），并在0x0e05b的地址加载了8KB大小的与中断表对应的中断服务程序。
中断表中有256个条目，每个条目占用4个字节，其中两个字节是CS寄存器的值，两个字节是IP寄存器的值。每个条目都指向一个具体的中断服务程序。
为了启动外部储存器中的程序，BIOS会搜索可引导的设备，搜索的顺序是由CMOS中的设置信息决定的（这也是我们平时讲的，所谓的在BIOS中设置的启动设备顺序）。一个是软驱，一个是光驱，一个是硬盘上，还可以是网络上的设备甚至是一个usb 接口的U盘，都可以作为一个启动设备。
当然，Linux通常是从硬盘中启动的。硬盘上的第1个扇区（每个扇区512字节空间），被称为MBR（主启动记录），其中包含有基本的GRUB启动程序和分区表，安装GRUB时会自动写入到这个扇区，当MBR被BIOS装载到0x7c00地址开始的内存空间中后，BIOS就会将控制权转交给了MBR。在当前的情况下，其实是交给了GRUB。
到这里，BIOS到GRUB的过程结束。
GRUB是如何启动的根据前面内容可以发现，BIOS只会加载硬盘上的第1个扇区。不过这个扇区仅有512字节，这512字节中还有64字节的分区表加2字节的启动标志，很显然，剩下446字节的空间，是装不下GRUB这种大型通用引导器的。
于是，GRUB的加载分成了多个步骤，同时GRUB也分成了多个文件，其中有两个重要的文件boot.img和core.img，如下所示：
其中，boot.img被GRUB的安装程序写入到硬盘的MBR中，同时在boot.img文件中的一个位置写入core.img文件占用的第一个扇区的扇区号。
而core.img文件是由GRUB安装程序根据安装时环境信息，用其它GRUB的模块文件动态生成。如下图所示：
如果是从硬盘启动的话，core.img中的第一个扇区的内容就是diskboot.img文件。diskboot.img文件的作用是，读取core.img中剩余的部分到内存中。
由于这时diskboot.img文件还不识别文件系统，所以我们将core.img文件的全部位置，都用文件块列表的方式保存到diskboot.img文件中。这样就能确保diskboot.img文件找到core.img文件的剩余内容，最后将控制权交给kernel.img文件。
因为这时core.img文件中嵌入了足够多的功能模块，所以可以保证GRUB识别出硬盘分区上文件系统，能够访问/boot/grub目录，并且可以加载相关的配置文件和功能模块，来实现相关的功能，例如加载启动菜单、加载目标操作系统等。
正因为GRUB2大量使用了动态加载功能模块，这使得core.img文件的体积变得足够小。而GRUB的core.img文件一旦开始工作，就可以加载Linux系统的vmlinuz内核文件了。
详解vmlinuz文件结构我们在/boot目录下会发现vmlinuz文件，这个文件是怎么来的呢？
其实它是由Linux编译生成的bzImage文件复制而来的，你自己可以下载最新的Linux代码.
我们一致把Linux源码解压到一个linux目录中，也就是说我们后面查找Linux源代码文件总是从linux目录开始的，切换到代码目录执行make ARCH=x86_64，再执行make install，就会产生vmlinuz文件，你可以参考后面的makefile代码。
#linux/arch/x86/boot/Makefile install: sh $(srctree)/$(src)/install.sh $(KERNELRELEASE) $(obj)/bzImage \ System.map &amp;quot;$(INSTALL_PATH)&amp;quot; install.sh脚本文件只是完成复制的功能，所以我们只要搞懂了bzImage文件结构，就等同于理解了vmlinuz文件结构。
那么bzImage文件又是怎么来的呢？我们只要研究bzImage文件在Makefile中的生成规则，就会恍然大悟，代码如下 ：
#linux/arch/x86/boot/Makefile $(obj)/bzImage: $(obj)/setup.bin $(obj)/vmlinux.bin $(obj)/tools/build FORCE $(call if_changed,image) @$(kecho) 'Kernel: $@ is ready' ' (#'`cat .version`')' 从前面的代码可以知道，生成bzImage文件需要三个依赖文件：setup.bin、vmlinux.bin，linux/arch/x86/boot/tools目录下的build。让我们挨个来分析一下。
其实，build只是一个HOSTOS（正在使用的Linux）下的应用程序，它的作用就是将setup.bin、vmlinux.bin两个文件拼接成一个bzImage文件，如下图所示：
剩下的就是搞清楚setup.bin、vmlinux.bin这两个文件的的结构，先来看看setup.bin文件，setup.bin文件是由objcopy命令根据setup.elf生成的。
setup.elf文件又怎么生成的呢？我们结合后面的代码来看看。
#这些目标文件正是由/arch/x86/boot/目录下对应的程序源代码文件编译产生 setup-y += a20.</description></item><item><title>14_乘法器：如何像搭乐高一样搭电路（下）？</title><link>https://artisanbox.github.io/4/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/14/</guid><description>和学习小学数学一样，学完了加法之后，我们自然而然就要来学习乘法。既然是退回到小学，我们就把问题搞得简单一点，先来看两个4位数的乘法。这里的4位数，当然还是一个二进制数。我们是人类而不是电路，自然还是用列竖式的方式来进行计算。
十进制中的13乘以9，计算的结果应该是117。我们通过转换成二进制，然后列竖式的办法，来看看整个计算的过程是怎样的。
顺序乘法的实现过程从列出竖式的过程中，你会发现，二进制的乘法有个很大的优点，就是这个过程你不需要背九九乘法口诀表了。因为单个位置上，乘数只能是0或者1，所以实际的乘法，就退化成了位移和加法。
在13×9这个例子里面，被乘数13表示成二进制是1101，乘数9在二进制里面是1001。最右边的个位是1，所以个位乘以被乘数，就是把被乘数1101复制下来。因为二位和四位都是0，所以乘以被乘数都是0，那么保留下来的都是0000。乘数的八位是1，我们仍然需要把被乘数1101复制下来。不过这里和个位位置的单纯复制有一点小小的差别，那就是要把复制好的结果向左侧移三位，然后把四位单独进行乘法加位移的结果，再加起来，我们就得到了最终的计算结果。
对应到我们之前讲的数字电路和ALU，你可以看到，最后一步的加法，我们可以用上一讲的加法器来实现。乘法因为只有“0”和“1”两种情况，所以可以做成输入输出都是4个开关，中间用1个开关，同时来控制这8个开关的方式，这就实现了二进制下的单位的乘法。
我们可以用一个开关来决定，下面的输出是完全复制输入，还是将输出全部设置为0至于位移也不麻烦，我们只要不是直接连线，把正对着的开关之间进行接通，而是斜着错开位置去接就好了。如果要左移一位，就错开一位接线；如果要左移两位，就错开两位接线。
把对应的线路错位连接，就可以起到位移的作用这样，你会发现，我们并不需要引入任何新的、更复杂的电路，仍然用最基础的电路，只要用不同的接线方式，就能够实现一个“列竖式”的乘法。而且，因为二进制下，只有0和1，也就是开关的开和闭这两种情况，所以我们的计算机也不需要去“背诵”九九乘法口诀表，不需要单独实现一个更复杂的电路，就能够实现乘法。
为了节约一点开关，也就是晶体管的数量。实际上，像13×9这样两个四位数的乘法，我们不需要把四次单位乘法的结果，用四组独立的开关单独都记录下来，然后再把这四个数加起来。因为这样做，需要很多组开关，如果我们计算一个32位的整数乘法，就要32组开关，太浪费晶体管了。如果我们顺序地来计算，只需要一组开关就好了。
我们先拿乘数最右侧的个位乘以被乘数，然后把结果写入用来存放计算结果的开关里面，然后，把被乘数左移一位，把乘数右移一位，仍然用乘数去乘以被乘数，然后把结果加到刚才的结果上。反复重复这一步骤，直到不能再左移和右移位置。这样，乘数和被乘数就像两列相向而驶的列车，仅仅需要简单的加法器、一个可以左移一位的电路和一个右移一位的电路，就能完成整个乘法。
乘法器硬件结构示意图你看这里画的乘法器硬件结构示意图。这里的控制测试，其实就是通过一个时钟信号，来控制左移、右移以及重新计算乘法和加法的时机。我们还是以计算13×9，也就是二进制的1101×1001来具体看。
这个计算方式虽然节约电路了，但是也有一个很大的缺点，那就是慢。
你应该很容易就能发现，在这个乘法器的实现过程里，我们其实就是把乘法展开，变成了“加法+位移”来实现。我们用的是4位数，所以要进行4组“位移+加法”的操作。而且这4组操作还不能同时进行。因为下一组的加法要依赖上一组的加法后的计算结果，下一组的位移也要依赖上一组的位移的结果。这样，整个算法是“顺序”的，每一组加法或者位移的运算都需要一定的时间。
所以，最终这个乘法的计算速度，其实和我们要计算的数的位数有关。比如，这里的4位，就需要4次加法。而我们的现代CPU常常要用32位或者是64位来表示整数，那么对应就需要32次或者64次加法。比起4位数，要多花上8倍乃至16倍的时间。
换个我们在算法和数据结构中的术语来说就是，这样的一个顺序乘法器硬件进行计算的时间复杂度是 O(N)。这里的N，就是乘法的数里面的位数。
并行加速方法那么，我们有没有办法，把时间复杂度上降下来呢？研究数据结构和算法的时候，我们总是希望能够把O(N)的时间复杂度，降低到O(logN)。办法还真的有。和软件开发里面改算法一样，在涉及CPU和电路的时候，我们可以改电路。
32位数虽然是32次加法，但是我们可以让很多加法同时进行。回到这一讲开始，我们把位移和乘法的计算结果加到中间结果里的方法，32位整数的乘法，其实就变成了32个整数相加。
前面顺序乘法器硬件的实现办法，就好像体育比赛里面的单败淘汰赛。只有一个擂台会存下最新的计算结果。每一场新的比赛就来一个新的选手，实现一次加法，实现完了剩下的还是原来那个守擂的，直到其余31个选手都上来比过一场。如果一场比赛需要一天，那么一共要比31场，也就是31天。
目前的乘法实现就像是单败淘汰赛加速的办法，就是把比赛变成像世界杯足球赛那样的淘汰赛，32个球队捉对厮杀，同时开赛。这样一天一下子就淘汰了16支队，也就是说，32个数两两相加后，你可以得到16个结果。后面的比赛也是一样同时开赛捉对厮杀。只需要5天，也就是O(log2N)的时间，就能得到计算的结果。但是这种方式要求我们得有16个球场。因为在淘汰赛的第一轮，我们需要16场比赛同时进行。对应到我们CPU的硬件上，就是需要更多的晶体管开关，来放下中间计算结果。
通过并联更多的ALU，加上更多的寄存器，我们也能加速乘法电路并行上面我们说的并行加速的办法，看起来还是有点儿笨。我们回头来做一个抽象的思考。之所以我们的计算会慢，核心原因其实是“顺序”计算，也就是说，要等前面的计算结果完成之后，我们才能得到后面的计算结果。
最典型的例子就是我们上一讲讲的加法器。每一个全加器，都要等待上一个全加器，把对应的进入输入结果算出来，才能算下一位的输出。位数越多，越往高位走，等待前面的步骤就越多，这个等待的时间有个专门的名词，叫作门延迟（Gate Delay）。
每通过一个门电路，我们就要等待门电路的计算结果，就是一层的门电路延迟，我们一般给它取一个“T”作为符号。一个全加器，其实就已经有了3T的延迟（进位需要经过3个门电路）。而4位整数，最高位的计算需要等待前面三个全加器的进位结果，也就是要等9T的延迟。如果是64位整数，那就要变成63×3=189T的延迟。这可不是个小数字啊！
除了门延迟之外，还有一个问题就是时钟频率。在上面的顺序乘法计算里面，如果我们想要用更少的电路，计算的中间结果需要保存在寄存器里面，然后等待下一个时钟周期的到来，控制测试信号才能进行下一次移位和加法，这个延迟比上面的门延迟更可观。
那么，我们有什么办法可以解决这个问题呢？实际上，在我们进行加法的时候，如果相加的两个数是确定的，那高位是否会进位其实也是确定的。对于我们人来说，我们本身去做计算都是顺序执行的，所以要一步一步计算进位。但是，计算机是连结的各种线路。我们不用让计算机模拟人脑的思考方式，来连结线路。
那怎么才能把线路连结得复杂一点，让高位和低位的计算同时出结果呢？怎样才能让高位不需要等待低位的进位结果，而是把低位的所有输入信号都放进来，直接计算出高位的计算结果和进位结果呢？
我们只要把进位部分的电路完全展开就好了。我们的半加器到全加器，再到加法器，都是用最基础的门电路组合而成的。门电路的计算逻辑，可以像我们做数学里面的多项式乘法一样完全展开。在展开之后呢，我们可以把原来需要较少的，但是有较多层前后计算依赖关系的门电路，展开成需要较多的，但是依赖关系更少的门电路。
我在这里画了一个示意图，展示了一下我们加法器。如果我们完全展开电路，高位的进位和计算结果，可以和低位的计算结果同时获得。这个的核心原因是电路是天然并行的，一个输入信号，可以同时传播到所有接通的线路当中。
C4是前4位的计算结果是否进位的门电路表示如果一个4位整数最高位是否进位，展开门电路图，你会发现，我们只需要3T的延迟就可以拿到是否进位的计算结果。而对于64位的整数，也不会增加门延迟，只是从上往下复制这个电路，接入更多的信号而已。看到没？我们通过把电路变复杂，就解决了延迟的问题。
这个优化，本质上是利用了电路天然的并行性。电路只要接通，输入的信号自动传播到了所有接通的线路里面，这其实也是硬件和软件最大的不同。
无论是这里把对应的门电路逻辑进行完全展开以减少门延迟，还是上面的乘法通过并行计算多个位的乘法，都是把我们完成一个计算的电路变复杂了。而电路变复杂了，也就意味着晶体管变多了。
之前很多同学在我们讨论计算机的性能问题的时候，都提到，为什么晶体管的数量增加可以优化计算机的计算性能。实际上，这里的门电路展开和上面的并行计算乘法都是很好的例子。我们通过更多的晶体管，就可以拿到更低的门延迟，以及用更少的时钟周期完成一个计算指令。
总结延伸讲到这里，相信你已经发现，我们通过之前两讲的ALU和门电路，搭建出来了乘法器。如果愿意的话，我们可以把很多在生活中不得不顺序执行的事情，通过简单地连结一下线路，就变成并行执行了。这是因为，硬件电路有一个很大的特点，那就是信号都是实时传输的。
我们也看到了，通过精巧地设计电路，用较少的门电路和寄存器，就能够计算完成乘法这样相对复杂的运算。是用更少更简单的电路，但是需要更长的门延迟和时钟周期；还是用更复杂的电路，但是更短的门延迟和时钟周期来计算一个复杂的指令，这之间的权衡，其实就是计算机体系结构中RISC和CISC的经典历史路线之争。
推荐阅读如果还有什么细节你觉得还没有彻底弄明白，我推荐你看一看《计算机组成与设计：硬件/软件接口》的3.3节。
课后思考这一讲里，我为你讲解了乘法器是怎么实现的。那么，请你想一想，如果我们想要用电路实现一个除法器，应该怎么做呢？需要注意一下，除法器除了要计算除法的商之外，还要计算出对应的余数。
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>14_前端技术应用（一）：如何透明地支持数据库分库分表？</title><link>https://artisanbox.github.io/6/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/14/</guid><description>从今天开始，我们正式进入了应用篇，我会用两节课的时间，带你应用编译器的前端技术。这样，你会把学到的编译技术和应用领域更好地结合起来，学以致用，让技术发挥应有的价值。还能通过实践加深对原理的理解，形成一个良好的循环。
这节课，我们主要讨论，一个分布式数据库领域的需求。我会带你设计一个中间层，让应用逻辑不必关心数据库的物理分布。这样，无论把数据库拆成多少个分库，编程时都会像面对一个物理库似的没什么区别。
接下来，我们先了解一下分布式数据库的需求和带来的挑战。
分布式数据库解决了什么问题，又带来了哪些挑战随着技术的进步，我们编写的应用所采集、处理的数据越来越多，处理的访问请求也越来越多。而单一数据库服务器的处理能力是有限的，当数据量和访问量超过一定级别以后，就要开始做分库分表的操作。比如，把一个数据库的大表拆成几张表，把一个库拆成几个库，把读和写的操作分离开等等。我们把这类技术统称为分布式数据库技术。
分库分表（Sharding）有时也翻译成“数据库分片”。分片可以依据各种不同的策略，比如我开发过一个与社区有关的应用系统，这个系统的很多业务逻辑都是围绕小区展开的。对于这样的系统，按照地理分布的维度来分片就很合适，因为每次对数据库的操作基本上只会涉及其中一个分库。
假设我们有一个订单表，那么就可以依据一定的规则对订单或客户进行编号，编号中就包含地理编码。比如SDYT代表山东烟台，BJHD代表北京海淀，不同区域的数据放在不同的分库中：
通过数据库分片，我们可以提高数据库服务的性能和可伸缩性。当数据量和访问量增加时，增加数据库节点的数量就行了。不过，虽然数据库的分片带来了性能和伸缩性的好处，但它也带来了一些挑战。
最明显的一个挑战，是数据库分片逻辑侵入到业务逻辑中。过去，应用逻辑只访问一个数据库，现在需要根据分片的规则，判断要去访问哪个数据库，再去跟这个数据库服务器连接。如果增加数据库分片，或者对分片策略进行调整，访问数据库的所有应用模块都要修改。这会让软件的维护变得更复杂，显然也不太符合软件工程中模块低耦合、把变化隔离的理念。
所以如果有一种技术，能让我们访问很多数据库分片时，像访问一个数据库那样就好了。数据库的物理分布，对应用是透明的。
可是，“理想很吸引人，现实很骨感”。要实现这个技术，需要解决很多问题：
首先是跨库查询的难题。如果SQL操作都针对一个库还好，但如果某个业务需求恰好要跨多个库，比如上面的例子中，如果要查询多个小区的住户信息，那么就要在多个库中都执行查询，然后把查询结果合并，一般还要排序。
如果我们前端显示的时候需要分页，每页显示一百行，那就更麻烦了。我们不可能从10个分库中各查出10行，合并成100行，这100行不一定排在前面，最差的情况，可能这100行恰好都在其中一个分库里。所以，你可能要从每个分库查出100行来，合并、排序后，再取出前100行。如果涉及数据库表跨库做连接，你想象一下，那就更麻烦了。
其次就是跨库做写入的难题。如果对数据库写入时遇到了跨库的情况，那么就必须实现分布式事务。所以，虽然分布式数据库的愿景很吸引人，但我们必须解决一系列技术问题。
这一讲，我们先解决最简单的问题，也就是当每次数据操作仅针对一个分库的时候，能否自动确定是哪个分库的问题。解决这个问题我们不需要依据别的信息，只需要提供SQL就行了。这就涉及对SQL语句的解析了，自然要用到编译技术。
解析SQL语句，判断访问哪个数据库我画了一张简化版的示意图：假设有两张表，分别是订单表和客户表，它们的主键是order_id和cust_id：
我们采用的分片策略，是依据这两个主键的前4位的编码来确定数据库分片的逻辑，比如：前四位是SDYT，那就使用山东烟台的分片，如果是BJHD，就使用北京海淀的分片，等等。
在我们的应用中，会对订单表进行一些增删改查的操作，比如会执行下面的SQL语句：
//查询 select * from orders where order_id = 'SDYT20190805XXXX' select * from orders where cust_id = 'SDYT987645' //插入 insert into orders (order_id，&amp;hellip;其他字段) values( &amp;quot;BJHD20190805XXXX&amp;quot;,&amp;hellip;)
//修改 update orders set price=298.00 where order_id=&amp;lsquo;FJXM20190805XXXX&amp;rsquo;
//删除 delete from orders where order_id=&amp;lsquo;SZLG20190805XXXX&amp;rsquo; 我们要能够解析这样的SQL语句，根据主键字段的值，决定去访问哪个分库或者分表。这就需要用到编译器前端技术，包括词法分析、语法分析和语义分析。
听到这儿，你可能会质疑：“解析SQL语句？是在开玩笑吗？”你可能觉得这个任务太棘手，犹豫着是否要忍受业务逻辑和技术逻辑混杂的缺陷，把判断分片的逻辑写到应用代码里，或者想解决这个问题，又或者想自己写一个开源项目，帮到更多的人。
无论你的内心活动如何，应用编译技术，能让你有更强的信心解决这个问题。那么如何去做呢？要想完成解析SQL的任务，在词法分析和语法分析这两个阶段，我建议你采用工具快速落地，比如Antlr。你要找一个现成的SQL语句的语法规则文件。
GitHub中，那个收集了很多示例Antlr规则文件的项目里，有两个可以参考的规则：一个是PLSQL的（它是Oracle数据库的SQL语法）；一个是SQLite的（这是一个嵌入式数据库）。
实际上，我还找到MySQL workbench所使用的一个产品级的规则文件。MySQL workbench是一个图形化工具，用于管理和访问MySQL。这个规则文件还是很靠谱的，不过它里面嵌了很多属性计算规则，而且是C++语言写的，我嫌处理起来麻烦，就先弃之不用，暂且采用SQLite的规则文件来做示范。
先来看一下这个文件里的一些规则，例如select语句相关的语法：
factored_select_stmt : ( K_WITH K_RECURSIVE? common_table_expression ( &amp;lsquo;,&amp;rsquo; common_table_expression )* )?</description></item><item><title>14_排序优化：如何实现一个通用的、高性能的排序函数？</title><link>https://artisanbox.github.io/2/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/15/</guid><description>几乎所有的编程语言都会提供排序函数，比如C语言中qsort()，C++ STL中的sort()、stable_sort()，还有Java语言中的Collections.sort()。在平时的开发中，我们也都是直接使用这些现成的函数来实现业务逻辑中的排序功能。那你知道这些排序函数是如何实现的吗？底层都利用了哪种排序算法呢？
基于这些问题，今天我们就来看排序这部分的最后一块内容：如何实现一个通用的、高性能的排序函数？
如何选择合适的排序算法？如果要实现一个通用的、高效率的排序函数，我们应该选择哪种排序算法？我们先回顾一下前面讲过的几种排序算法。
我们前面讲过，线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。
如果对小规模数据进行排序，可以选择时间复杂度是O(n2)的算法；如果对大规模数据进行排序，时间复杂度是O(nlogn)的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是O(nlogn)的排序算法来实现排序函数。
时间复杂度是O(nlogn)的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。
不知道你有没有发现，使用归并排序的情况其实并不多。我们知道，快排在最坏情况下的时间复杂度是O(n2)，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是O(nlogn)，从这点上看起来很诱人，那为什么它还是没能得到“宠信”呢？
还记得我们上一节讲的归并排序的空间复杂度吗？归并排序并不是原地排序算法，空间复杂度是O(n)。所以，粗略点、夸张点讲，如果要排序100MB的数据，除了数据本身占用的内存之外，排序算法还要额外再占用100MB的内存空间，空间耗费就翻倍了。
前面我们讲到，快速排序比较适合来实现排序函数，但是，我们也知道，快速排序在最坏情况下的时间复杂度是O(n2)，如何来解决这个“复杂度恶化”的问题呢？
如何优化快速排序？我们先来看下，为什么最坏情况下快速排序的时间复杂度是O(n2)呢？我们前面讲过，如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为O(n2)。实际上，这种O(n2)时间复杂度出现的主要原因还是因为我们分区点选得不够合理。
那什么样的分区点是好的分区点呢？或者说如何来选择分区点呢？
最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。
如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现之前讲的那样，在某些情况下，排序的最坏情况时间复杂度是O(n2)。为了提高排序算法的性能，我们也要尽可能地让每次分区都比较平均。
我这里介绍两个比较常用、比较简单的分区算法，你可以直观地感受一下。
1.三数取中法我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这3个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。
2.随机法随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选得很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的O(n2)的情况，出现的可能性不大。
好了，我这里也只是抛砖引玉，如果想了解更多寻找分区点的方法，你可以自己课下深入去学习一下。
我们知道，快速排序是用递归来实现的。我们在递归那一节讲过，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。
举例分析排序函数为了让你对如何实现一个排序函数有一个更直观的感受，我拿Glibc中的qsort()函数举例说明一下。虽说qsort()从名字上看，很像是基于快速排序算法实现的，实际上它并不仅仅用了快排这一种算法。
如果你去看源码，你就会发现，qsort()会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是O(n)，所以对于小数据量的排序，比如1KB、2KB等，归并排序额外需要1KB、2KB的内存空间，这个问题不大。现在计算机的内存都挺大的，我们很多时候追求的是速度。还记得我们前面讲过的用空间换时间的技巧吗？这就是一个典型的应用。
但如果数据量太大，就跟我们前面提到的，排序100MB的数据，这个时候我们再用归并排序就不合适了。所以，要排序的数据量比较大的时候，qsort()会改为用快速排序算法来排序。
那qsort()是如何选择快速排序算法的分区点的呢？如果去看源码，你就会发现，qsort()选择分区点的方法就是“三数取中法”。是不是也并不复杂？
还有我们前面提到的递归太深会导致堆栈溢出的问题，qsort()是通过自己实现一个堆上的栈，手动模拟递归来解决的。我们之前在讲递归那一节也讲过，不知道你还有没有印象？
实际上，qsort()并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于4时，qsort()就退化为插入排序，不再继续用递归来做快速排序，因为我们前面也讲过，在小规模数据面前，O(n2)时间复杂度的算法并不一定比O(nlogn)的算法执行时间长。我们现在就来分析下这个说法。
我们在讲复杂度分析的时候讲过，算法的性能可以通过时间复杂度来分析，但是，这种复杂度分析是比较偏理论的，如果我们深究的话，实际上时间复杂度并不等于代码实际的运行时间。
时间复杂度代表的是一个增长趋势，如果画成增长曲线图，你会发现O(n2)比O(nlogn)要陡峭，也就是说增长趋势要更猛一些。但是，我们前面讲过，在大O复杂度表示法中，我们会省略低阶、系数和常数，也就是说，O(nlogn)在没有省略低阶、系数、常数之前可能是O(knlogn + c)，而且k和c有可能还是一个比较大的数。
假设k=1000，c=200，当我们对小规模数据（比如n=100）排序时，n2的值实际上比knlogn+c还要小。
knlogn+c = 1000 * 100 * log100 + 200 远大于10000 n^2 = 100*100 = 10000 所以，对于小规模数据的排序，O(n2)的排序算法并不一定比O(nlogn)排序算法执行的时间长。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法。
还记得我们之前讲到的哨兵来简化代码，提高执行效率吗？在qsort()插入排序的算法实现中，也利用了这种编程技巧。虽然哨兵可能只是少做一次判断，但是毕竟排序函数是非常常用、非常基础的函数，性能的优化要做到极致。
好了，C语言的qsort()我已经分析完了，你有没有觉得其实也不是很难？基本上都是用了我们前面讲到的知识点，有了前面的知识积累，看一些底层的类库的时候是不是也更容易了呢？
内容小结今天我带你分析了一下如何来实现一个工业级的通用的、高效的排序函数，内容比较偏实战，而且贯穿了一些前面几节的内容，你要多看几遍。我们大部分排序函数都是采用O(nlogn)排序算法来实现，但是为了尽可能地提高性能，会做很多优化。
我还着重讲了快速排序的一些优化策略，比如合理选择分区点、避免递归太深等等。最后，我还带你分析了一个C语言中qsort()的底层实现原理，希望你对此能有一个更加直观的感受。
课后思考在今天的内容中，我分析了C语言的中的qsort()的底层排序算法，你能否分析一下你所熟悉的语言中的排序函数都是用什么排序算法实现的呢？都有哪些优化技巧？
欢迎留言和我分享，我会第一时间给你反馈。
特别说明：
专栏已经更新一月有余，我在留言区看到很多同学说，希望给出课后思考题的标准答案。鉴于留言区里本身就有很多非常好的答案，之后我会将我认为比较好的答案置顶在留言区，供需要的同学参考。
如果文章发布一周后，留言里依旧没有比较好的答案，我会把我的答案写出来置顶在留言区。
最后，希望你把思考的过程看得比标准答案更重要。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>14_视图：如何简化查询？</title><link>https://artisanbox.github.io/8/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/14/</guid><description>你好，我是朱晓峰。今天，我们来聊一聊视图。
视图是一种虚拟表，我们可以把一段查询语句作为视图存储在数据库中，在需要的时候，可以把视图看做一个表，对里面的数据进行查询。
举个小例子，在学校的信息系统里面，为了减少冗余数据，学生档案（包括姓名、年龄等）和考试成绩（包括考试时间、科目、分数等）是分别存放在不同的数据表里面的，但是，我们经常需要查询学生的考试成绩（包括学生姓名、科目、分数）。这个时候，我们就可以把查询学生考试成绩的这个关联查询，用视图的形式保存起来。这样一来，我们不仅可以从视图中直接查询学生考试成绩，让查询变得简单，而且，视图没有实际存储数据，还避免了数据存储过程中可能产生的冗余，提高了存储的效率。
今天，我就结合超市的项目，来具体讲解一下怎么创建和操作视图，来帮助你提高查询效率。
视图的创建及其好处首先，我们来学习下创建视图的方法，以及使用视图的一些好处。
创建视图的语法结构：
CREATE [OR REPLACE] VIEW 视图名称 [(字段列表)] AS 查询语句 现在，假设我们要查询一下商品的每日销售明细，这就要从销售流水表（demo.trans）和商品信息表（demo.goodsmaster）中获取到销售数据和对应的商品信息数据。
销售流水表包含流水单号、商品编号、销售数量、销售金额和交易时间等信息：
商品信息表包含商品编号、条码、名称和售价等信息：
在不使用视图的情况下，我们可以通过对销售流水表和商品信息表进行关联查询，得到每天商品销售统计的结果，包括销售日期、商品名称、每天销售数量的合计和每天销售金额的合计，如下所示：
mysql&amp;gt; SELECT -&amp;gt; a.transdate, -&amp;gt; a.itemnumber, -&amp;gt; b.goodsname, -&amp;gt; SUM(a.quantity) AS quantity, -- 统计销售数量 -&amp;gt; SUM(a.salesvalue) AS salesvalue -- 统计销售金额 -&amp;gt; FROM -&amp;gt; demo.trans AS a -&amp;gt; LEFT JOIN -- 连接查询 -&amp;gt; demo.goodsmaster AS b ON (a.itemnumber = b.itemnumber) -&amp;gt; GROUP BY a.transdate , a.itemnumber; +---------------------+------------+-----------+----------+------------+ | transdate | itemnumber | goodsname | quantity | salesvalue | +---------------------+------------+-----------+----------+------------+ | 2020-12-01 00:00:00 | 1 | 本 | 1.</description></item><item><title>14｜汇编代码学习（一）：熟悉CPU架构和指令集</title><link>https://artisanbox.github.io/3/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/16/</guid><description>你好，我是宫文学。
经过了上一节课的学习，你已经对物理机的运行期机制有了一定的了解。其中最重要的知识点就是，为了让一个程序运行起来，硬件架构、操作系统和计算机语言分别起到了什么作用？对这些知识的深入理解，是让你进入高手行列的关键。
接下来，就让我们把程序编译成汇编代码，从而生成在物理机上运行的可执行程序吧！
慢着，不要太着急。为了让你打下更好的基础，我决定再拿出一节课来，带你了解一下CPU架构和指令集，特别是ARM和X86这两种使用最广泛的CPU架构，为你学习汇编语言打下良好的基础。
首先，我们讨论一下什么是CPU架构，以及它对学习汇编语言的作用。
掌握汇编语言的关键，是了解CPU架构提到汇编语言，很多同学都会觉得很高深、很难学。其实这是个误解，汇编语言并不难掌握。
为什么这么说呢？其实前面在实现虚拟机的时候，我们已经接触了栈机的字节码。你觉得它难吗？JVM的字节码理论上不会超过128条，而我们通过前面几节课已经了解了其中的好几十条指令，并且已经让他们顺利地运转起来了。
而且，汇编代码作为物理机的指令，也不可能有多么复杂。因为CPU的设计，就是要去快速地执行一条条简单的指令，所以这些指令不可能像高级语言那样充满复杂的语义。
我们可以把学习汇编语言跟学习高级语言做一下类比。如果你接触过多门计算机语言，很快就能找到这些计算机语言的相似性，比如都有基础数据类型、都支持加减乘除等各种运算、都支持各种表达式和语句等等。抓住这些基本规律以后，学习一门新语言的速度会很快。
汇编语言也是一样的。不同的CPU有不同的指令集，但它们的指令的格式有一些共性，比如都包含操作码的助记符和操作数，而操作数通常也都包含立即数、寄存器和内存地址这三个类别。它们也都包含数据处理、内存读写、跳转、子过程调用等几种大类的指令。所以，你也可以很快掌握这些规律或模式，做到触类旁通，游刃有余。
说了那么多，那么学习汇编语言的关键是什么呢？
由于汇编语言是跟具体的CPU打交道的，所以不同的CPU架构，它们的汇编语言就会有所差别。如果你能够深刻了解某款CPU的架构，自然也就会为它编写汇编代码了。
这里提到了一个词，架构。所谓架构，是指一个处理器的功能规范，定义了CPU在什么情况下会产生什么行为。你也可以把它理解成软件和硬件之间的一个桥梁，规定了硬件如何提供功能被软件所调用。所以，如果你要搞编译技术，就必须要了解目标CPU的架构。
我把CPU架构里的内容整合在这张表里，你可以保存起来：
看上去，要深入了解一个CPU架构，涉及的知识还蛮多的。不过，作为初学者，我们最重要的是关注两个方面就行了：指令集和寄存器集，因为它们跟汇编代码的关系最密切。
那么如何了解一个CPU的架构呢？正如我在第12节课说的那样，其实最重要的方式，就是阅读CPU的手册。
那么这节课我们就分析两种主流的CPU架构，看看能获得哪些知识点。首先我们就来看一下ARM架构的CPU的特点，它是目前大多数智能手机所采用的CPU。
了解ARM架构ARM处理器是ARM公司推出的一系列处理器的名称。ARMv8是它比较新的架构，当前大多数高端智能手机都是采用这个架构。了解这个架构的方法呢，当然是下载ARMv8的手册。
不过这本手册比较厚，有8000多页。为了加速你的理解，我挑其中最有用、跟编写汇编代码最相关的几个知识点跟你聊聊。
首先，ARMv8支持32位和64位运行模式，分别叫做AArch32和AArch64。在64位模式下，它的指令集叫做A64。
接着，我们看看ARMv8的寄存器。在AArch64架构下，它的寄存器有下面这几个（参见手册的B1.2.1部分）。
R0-R30：是31个通用寄存器。当它们被用于64位计算或32位计算的时候，分别被叫做X0-X30（X表示64位），以及W0-W30（W是Word的意思，表示32位）。这31个处理器是我们用做数据处理的主力。 SP寄存器：64位的栈指针寄存器，用于指向栈顶的地址。 PC寄存器：64位的程序代码寄存器，PC是Program Code的意思。这个寄存器记录了内存中当前指令的地址，CPU会从这个地址读取指令并执行。当程序执行跳转指令、进入异常或退出异常的时候，这个寄存器的值会被自动修改。 另外，还有一组寄存器是用于处理浮点数运算和矢量计算的，你可以去官方手册看看。 这些就是我们的汇编代码中会涉及到的寄存器，还有一些寄存器是系统级的寄存器，我们平常的应用代码用不上，就先不管了。
谈到寄存器，我插个话题。我注意到，你如果在网上搜索某个CPU架构的文章，往往得到的是模棱两可的、甚至是错误的信息，你千万要注意不要被它们误导了。
比如，我看到有的文章说在某个架构的CPU中，哪些寄存器是用来放返回值的，哪些寄存器是用来传参数的，而哪些寄存器又分别是由调用者或被调用者保护的，等等，还配了图做说明。
但如果你对调用约定或ABI的概念有所了解的话马上就会知道，这些其实都是软件层面上的一些约定，不是CPU架构层面上的规定。如果你还想了解得更具体一些，可以参考涉及到ARM架构的一些ABI规范文档，特别是其中的“Procedure Call Standard for the Arm® 64-bit”，这篇文档就规定了如何使用这些通用寄存器等信息。但作为语言的作者，你其实可以设计自己的ABI，你拥有更大的自由度。
与寄存器相关的一个概念，叫做Process State，或者PSTATE，它是CPU在执行指令时形成的一些状态信息，这些状态信息在物理层面是保存在一些特殊目的寄存器里。
PSTATE有什么用呢？比如，它的用途之一是辅助跳转指令的运行。当我们执行一个条件跳转指令之前，会先执行一个比较指令，这个比较指令就会设置某个状态信息，而后续的跳转指令就可以基于这个状态信息进行正确的跳转了。
此外，PSTATE还可以用于判断算术运算是否溢出、是否需要进位等等。
最后，我们终于谈到CPU架构中的主角，指令集了。你先看一下A64指令集中的一些常见的指令：
你看这些指令，是不是跟前面学过的Java字节码的指令集有很多相似之处？我们来比较一下看，上面的指令大概可以分为四组。
第一组指令，是加减乘除等算术运算的指令。
回忆一下，在我们之前学栈机的时候，是不是也有这些运算指令？但栈机和寄存器机的指令有一些差别。栈机的运算指令，是不需要带操作数的，因为操作数已经在操作数栈里。这几个指令会从栈顶取出两个操作数，做完加减乘除运算后，再放回栈顶。这样，下一条指令就可以把这个值作为操作数，继续进行计算。
但寄存器机上没有操作数栈，典型的寄存器机，所有运算都发生在寄存器里。我们来看看下面这个示例程序：
int foo(int a){ return a + 10; } 你可以用下面这个命令生成ARM64指令集的汇编代码：
clang -arch arm64 -S&amp;nbsp; foo.c -o foo_arm64.s -O2 汇编代码的主要部分是下面这几行：
_foo: ; @foo add w0, w0, #10 ; =10 ret 其中“add w0, w0, #10”的意思，是把w0寄存器的值加上10，结果仍然放到w0。</description></item><item><title>15_JavaJIT编译器（三）：探究内联和逃逸分析的算法原理</title><link>https://artisanbox.github.io/7/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/15/</guid><description>你好，我是宫文学。
基于Graal IR进行的优化处理有很多。但有些优化，针对Java语言的特点，会显得更为重要。
今天这一讲，我就带你来认识两个对Java来说很重要的优化算法。如果没有这两个优化算法，你的程序执行效率会大大下降。而如果你了解了这两个算法的机理，则有可能写出更方便编译器做优化的程序，从而让你在实际工作中受益。这两个算法，分别是内联和逃逸分析。
另外，我还会给你介绍一种JIT编译所特有的优化模式：基于推理的优化。这种优化模式会让某些程序比AOT编译的性能更高。这个知识点，可能会改变你对JIT和AOT的认知，因为通常来说，你可能会认为AOT生成的机器码速度更快，所以通过这一讲的学习，你也会对“全生命周期优化”的概念有所体会。
好，首先，我们来看看内联优化。
内联（Inlining）内联优化是Java JIT编译器非常重要的一种优化策略。简单地说，内联就是把被调用的方法的方法体，在调用的地方展开。这样做最大的好处，就是省去了函数调用的开销。对于频繁调用的函数，内联优化能大大提高程序的性能。
执行内联优化是有一定条件的。第一，被内联的方法要是热点方法；第二，被内联的方法不能太大，否则编译后的目标代码量就会膨胀得比较厉害。
在Java程序里，你经常会发现很多短方法，特别是访问类成员变量的getter和setter方法。你可以看看自己写的程序，是否也充斥着很多对这种短方法的调用？这些调用如果不做优化的话，性能损失是很厉害的。你可以做一个性能对比测试，通过“-XX:-Inlining”参数来阻止JVM做内联优化，看看性能降低得会有多大。
但是这些方法有一个好处：它们往往都特别短，内联之后，实际上并不会显著增加目标代码长度。
比如，针对add2示例方法，我们采用内联选项优化后，方法调用被替换成了LoadField（加载成员变量）。
public int add2(){ return getX() + getY(); } 图1：将getter方法内联在做了Lower处理以后，LoadField会被展开成更底层的操作：根据x和y的地址相对于对象地址的偏移量，获取x和y的值。
图2：计算字段x和y的地址而要想正确地计算字段的偏移量，我们还需要了解Java对象的内存布局。
在64位平台下，每个Java对象头部都有8字节的标记字，里面有对象ID的哈希值、与内存收集有关的标记位、与锁有关的标记位；标记字后面是一个指向类定义的指针，在64位平台下也是8位，不过如果堆不是很大，我们可以采用压缩指针，只占4个字节；在这后面才是x和y字段。因此，x和y的偏移量分别是12和16。
图3：内存中，Java对象头占据的空间在Low Tier编译完毕以后，图2还会进一步被Lower，形成AMD64架构下的地址。这样的话，编译器再进一步翻译成汇编代码就很容易了。
图4：生成AMD64架构的地址计算节点内联优化除了会优化getter、setter这样的短方法，它实际上还起到了另一个重要的作用，即跨过程的优化。一般的优化算法，只会局限在一个方法内部。而启动内联优化后，多个方法会合并成一个方法，所以就带来了更多的优化的可能性。
我们来看看下面这个inlining示例方法。它调用了一个atLeastTen方法，这个方法对于&amp;lt;10的参数，会返回10；对于≥10的参数，会返回该参数本身。所以你用肉眼就可以看出来，inlining方法的返回值应该是10。
public int inliningTest(int a){ return atLeastTen(3); //应该返回10 } //至少返回10 public int atLeastTen(int a){ if (a &amp;lt; 10) return 10; else return a; } 如果不启用编译器的内联选项，那么inliningTest方法对应的IR图，就是常规的方法调用而已：
图5：不启用内联时调用atLeastTen()方法而一旦启用了内联选项，就可以触发一系列的优化。在把字节码解析生成IR的时候，编译器就启动了内联分析过程，从而会发现this参数和常量3对于inliningTest方法根本是无用的，在图里表现成了一些孤岛。在Mid Tier处理完毕之后，inliningTest方法就直接返回常量10了。
图6：启用内联后，调用atLeastTen()方法另外，方法的类型也会影响inlining。如果方法是final的，或者是private的，那么它就不会被子类重载，所以可以大胆地内联。
但如果存在着多重继承的类体系，方法就有可能被重载，这就会导致多态。在运行时，JVM会根据对象的类型来确定到底采用哪个子类的具体实现。这种运行时确定具体方法的过程，叫做虚分派（Virtual Dispatch）。
在存在多态的情况下，JIT编译器做内联就会遇到困难了。因为它不知道把哪个版本的实现内联进来。不过编译器仍然没有放弃。这时候所采用的技术，就叫做“多态内联（Polymorphic inlining）”。
它的具体做法是，在运行时，编译器会统计在调用多态方法的时候，到底用了哪几个实现。然后针对这几个实现，同时实现几个分支的内联，并在一开头根据对象类型判断应该走哪个分支。这个方法的缺陷是生成的代码量会比较大，但它毕竟可以获得内联的好处。最后，如果实际运行中遇到的对象，与提前生成的几个分支都不匹配，那么编译器还可以继续用缺省的虚分派模式来做函数调用，保证程序不出错。
这个案例也表明了，JIT编译器是如何充分利用运行时收集的信息来做优化的。对于AOT模式的编译来说，由于无法收集到这些信息，因此反倒无法做这种优化。
如果你想对多态内联做更深入的研究，还可以参考这一篇经典论文《Inlining of Virtual Methods》。
总结起来，内联优化不仅能降低由于函数调用带来的开销，还能制造出新的优化机会，因此带来的优化效果非常显著。接下来，我们看看另一个能带来显著优化效果的算法：逃逸分析。
逃逸分析（Escape Analysis, EA）逃逸分析是JVM的另一个重要的优化算法，它同样可以起到巨大的性能提升作用。
逃逸分析能够让编译器判断出，一个对象是否能够在创建它的方法或线程之外访问。如果只能在创建它的方法内部访问，我们就说这个对象不是方法逃逸的；如果仅仅逃逸出了方法，但对这个对象的访问肯定都是在同一个线程中，那么我们就说这个对象不是线程逃逸的。</description></item><item><title>15_Linux初始化（下）：从_start到第一个进程</title><link>https://artisanbox.github.io/9/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/15/</guid><description>你好，我是LMOS。
今天我们继续来研究Linux的初始化流程，为你讲解如何解压内核，然后讲解Linux内核第一个C函数。最后，我们会用Linux的第一个用户进程的建立来收尾。
如果用你上手去玩一款新游戏做类比的话，那么上节课只是新手教程，而这节课就是更深入的实战了。后面你会看到很多熟悉的“面孔”，像是我们前面讲过的CPU工作模式、MMU页表等等基础知识，这节课都会得到运用。
解压后内核初始化下面，我们先从setup.bin文件的入口_start开始，了解启动信息结构，接着由16位main函数切换CPU到保护模式，然后跳入vmlinux.bin文件中的startup_32函数重新加载段描述符。
如果是64位的系统，就要进入startup_64函数，切换到CPU到长模式，最后调用extract_kernel函数解压Linux内核，并进入内核的startup_64函数，由此Linux内核开始运行。
为何要从_start开始通过上节课对vmlinuz文件结构的研究，我们已经搞清楚了其中的vmlinux.bin是如何产生的，它是由linux/arch/x86/boot/compressed目录下的一些目标文件，以及piggy.S包含的一个vmlinux.bin.gz的压缩文件一起生成的。
vmlinux.bin.gz文件则是由编译的Linux内核所生成的elf格式的vmlinux文件，去掉了文件的符号信息和重定位信息后，压缩得到的。
CPU是无法识别压缩文件中的指令直接运行的，必须先进行解压后，然后解析elf格式的文件，把其中的指令段和数据段加载到指定的内存空间中，才能由CPU执行。
这就需要用到前面的setup.bin文件了，_start正是setup.bin文件的入口，在head.S文件中定义，代码如下。
#linux/arch/x86/boot/head.S .code16 .section &amp;quot;.bstext&amp;quot;, &amp;quot;ax&amp;quot; .global bootsect_start bootsect_start: ljmp $BOOTSEG, $start2 start2: #…… #这里的512字段bootsector对于硬盘启动是用不到的 #…… .globl _start _start: .byte 0xeb # short (2-byte) jump .byte start_of_setup-1f #这指令是用.byte定义出来的，跳转start_of_setup-1f #…… #这里是一个庞大的数据结构，没展示出来，与linux/arch/x86/include/uapi/asm/bootparam.h文件中的struct setup_header一一对应。这个数据结构定义了启动时所需的默认参数 #…… start_of_setup: movw %ds, %ax movw %ax, %es #ds = es cld #主要指定si、di寄存器的自增方向，即si++ di++ movw %ss, %dx cmpw %ax, %dx # ds 是否等于 ss movw %sp, %dx je 2f # 如果ss为空则建立新栈 movw $_end, %dx testb $CAN_USE_HEAP, loadflags jz 1f movw heap_end_ptr, %dx 1: addw $STACK_SIZE, %dx jnc 2f xorw %dx, %dx 2: andw $~3, %dx jnz 3f movw $0xfffc, %dx 3: movw %ax, %ss movzwl %dx, %esp sti # 栈已经初始化好，开中断 pushw %ds pushw $6f lretw # cs=ds ip=6：跳转到标号6处 6: cmpl $0x5a5aaa55, setup_sig #检查setup标记 jne setup_bad movw $__bss_start, %di movw $_end+3, %cx xorl %eax, %eax subw %di, %cx shrw $2, %cx rep; stosl #清空setup程序的bss段 calll main #调用C语言main函数 setup_header结构下面我们重点研究一下setup_header结构，这对我们后面的流程很关键。它定义在linux/arch/x86/include/uapi/asm/bootparam.</description></item><item><title>15_二分查找（上）：如何用最省内存的方式实现快速查找功能？</title><link>https://artisanbox.github.io/2/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/16/</guid><description>今天我们讲一种针对有序数据集合的查找算法：二分查找（Binary Search）算法，也叫折半查找算法。二分查找的思想非常简单，很多非计算机专业的同学很容易就能理解，但是看似越简单的东西往往越难掌握好，想要灵活应用就更加困难。
老规矩，我们还是来看一道思考题。
假设我们有1000万个整数数据，每个数据占8个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这1000万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过100MB，你会怎么做呢？带着这个问题，让我们进入今天的内容吧！
无处不在的二分思想二分查找是一种非常简单易懂的快速查找算法，生活中到处可见。比如说，我们现在来做一个猜字游戏。我随机写一个0到99之间的数字，然后你来猜我写的是什么。猜的过程中，你每猜一次，我就会告诉你猜的大了还是小了，直到猜中为止。你来想想，如何快速猜中我写的数字呢？
假设我写的数字是23，你可以按照下面的步骤来试一试。（如果猜测范围的数字有偶数个，中间数有两个，就选择较小的那个。）
7次就猜出来了，是不是很快？这个例子用的就是二分思想，按照这个思想，即便我让你猜的是0到999的数字，最多也只要10次就能猜中。不信的话，你可以试一试。
这是一个生活中的例子，我们现在回到实际的开发场景中。假设有1000条订单数据，已经按照订单金额从小到大排序，每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于19元的订单。如果存在，则返回订单数据，如果不存在则返回null。
最简单的办法当然是从第一个订单开始，一个一个遍历这1000个订单，直到找到金额等于19元的订单为止。但这样查找会比较慢，最坏情况下，可能要遍历完这1000条记录才能找到。那用二分查找能不能更快速地解决呢？
为了方便讲解，我们假设只有10个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98。
还是利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。为了更加直观，我画了一张查找过程的图。其中，low和high表示待查找区间的下标，mid表示待查找区间的中间元素下标。
看懂这两个例子，你现在对二分的思想应该掌握得妥妥的了。我这里稍微总结升华一下，二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0。
O(logn)惊人的查找速度二分查找是一种非常高效的查找算法，高效到什么程度呢？我们来分析一下它的时间复杂度。
我们假设数据大小是n，每次查找后数据都会缩小为原来的一半，也就是会除以2。最坏情况下，直到查找区间被缩小为空，才停止。
可以看出来，这是一个等比数列。其中n/2k=1时，k的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了k次区间缩小操作，时间复杂度就是O(k)。通过n/2k=1，我们可以求得k=log2n，所以时间复杂度就是O(logn)。
二分查找是我们目前为止遇到的第一个时间复杂度为O(logn)的算法。后面章节我们还会讲堆、二叉树的操作等等，它们的时间复杂度也是O(logn)。我这里就再深入地讲讲O(logn)这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级O(1)的算法还要高效。为什么这么说呢？
因为logn是一个非常“恐怖”的数量级，即便n非常非常大，对应的logn也很小。比如n等于2的32次方，这个数很大了吧？大约是42亿。也就是说，如果我们在42亿个数据中用二分查找一个数据，最多需要比较32次。
我们前面讲过，用大O标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1)有可能表示的是一个非常大的常量值，比如O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有O(logn)的算法执行效率高。
反过来，对数对应的就是指数。有一个非常著名的“阿基米德与国王下棋的故事”，你可以自行搜索一下，感受一下指数的“恐怖”。这也是为什么我们说，指数时间复杂度的算法在大规模数据面前是无效的。
二分查找的递归与非递归实现实际上，简单的二分查找并不难写，注意我这里的“简单”二字。下一节，我们会讲到二分查找的变体问题，那才是真正烧脑的。今天，我们来看如何来写最简单的二分查找。
最简单的情况就是有序数组中不存在重复元素，我们在其中用二分查找值等于给定值的数据。我用Java代码实现了一个最简单的二分查找算法。
public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &amp;lt;= high) { int mid = (low + high) / 2; if (a[mid] == value) { return mid; } else if (a[mid] &amp;lt; value) { low = mid + 1; } else { high = mid - 1; } }</description></item><item><title>15_前端技术应用（二）：如何设计一个报表工具？</title><link>https://artisanbox.github.io/6/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/15/</guid><description>众所周知，很多软件都需要面向开发者甚至最终用户提供自定义功能，在开篇词里，我提到自己曾经做过工作流软件和电子表单软件，它们都需要提供自定义功能，报表软件也是其中的典型代表。
在每个应用系统中，我们对数据的处理大致会分成两类：一类是在线交易，叫做OLTP，比如在网上下订单；一类是在线分析，叫做OLAP，它是对应用中积累的数据进行进一步分析利用。而报表工具就是最简单，但也是最常用的数据分析和利用的工具。
本节课，我们就来分析一下，如果我们要做一个通用的报表工具，需要用到哪些编译技术，又该怎样去实现。
报表工具所需要的编译技术如果要做一个报表软件，我们要想清楚软件面对的用户是谁。有一类报表工具面向的用户是程序员，那么这种软件可以暴露更多技术细节。比如，如果报表要从数据库获取数据，你可以写一个SQL语句作为数据源。
还有一类软件是给业务级的用户使用的，很多BI软件包都是这种类型。带有IT背景的顾问给用户做一些基础配置，然后用户就可以用这个软件包了。Excel可以看做是这种报表工具，IT人员建立Excel与数据库之间的连接，剩下的就是业务人员自己去操作了。
这些业务人员可以采用一个图形化的界面设计报表，对数据进行加工处理。我们来看看几个场景。
第一个场景是计算字段。计算字段的意思是，原始数据里没有这个数据，我们需要基于原始数据，通过一个自定义的公式来把它计算出来。比如在某个CRM系统中保存着销售数据，我们有每个部门的总销售额，也有每个部门的人数，要想在报表中展示每个部门的人均销售额，这个时候就可以用到计算公式功能，计算公式如下：
人均销售额=部门销售额/部门人数 得到的结果如下图所示：
进一步，我们可以在计算字段中支持函数。比如我们可以把各个部门按照人均销售额排名次。这可以用一个函数来计算：
=rank(人均销售额) rank就是排名次的意思，其他统计函数还包括：
min()，求最小值。 max()，求最大值。 avg()，求平均值。 sum()，求和。 还有一些更有意思的函数，比如：
runningsum()，累计汇总值。 runningavg()，累计平均值。 这些有意思的函数是什么意思呢？因为很多明细性的报表，都是逐行显示的，累计汇总值和累计平均值，就是累计到当前行的计算结果。当然了，我们还可以支持更多的函数，比如当前日期、当前页数等等。更有意思的是，上述字段也好、函数也好，都可以用来组合成计算字段的公式，比如：
=部门销售额/sum(部门销售额) //本部门的销售额在全部销售额的占比 =max(部门销售额)-部门销售额 //本部门的销售额与最高部门的差距 =max(部门销售额/部门人数)-部门销售额/部门人数 //本部门人均销售额与最高的那个部门的差 =sum(部门销售额)/sum(人数)-部门销售额/部门人数 //本部门的人均销售额与全公司人均销售额的差 怎么样，是不是越来越有意思了呢？现在你已经知道了在报表中会用到普通字段和各种各样的计算公式，那么，我们如何用这样的字段和公式来定义一张报表呢？
如何设计报表假设我们的报表是一行一行地展现数据，也就是最简单的那种。那我们将报表的定义做成一个XML文件，可能是下面这样的，它定义了表格中每一列的标题和所采用字段或公式：
&amp;lt;playreport title=&amp;quot;Report 1&amp;quot;&amp;gt; &amp;lt;section&amp;gt; &amp;lt;column&amp;gt; &amp;lt;title&amp;gt;部门&amp;lt;/title&amp;gt; &amp;lt;field&amp;gt;dept&amp;lt;/field&amp;gt; &amp;lt;/column&amp;gt; &amp;lt;column&amp;gt; &amp;lt;title&amp;gt;人数&amp;lt;/title&amp;gt; &amp;lt;field&amp;gt;num_person&amp;lt;/field&amp;gt; &amp;lt;/column&amp;gt; &amp;lt;column&amp;gt; &amp;lt;title&amp;gt;销售额&amp;lt;/title&amp;gt; &amp;lt;field&amp;gt;sales_amount&amp;lt;/field&amp;gt; &amp;lt;/column&amp;gt; &amp;lt;column&amp;gt; &amp;lt;title&amp;gt;人均销售额&amp;lt;/title&amp;gt; &amp;lt;field&amp;gt;sales_amount/num_person&amp;lt;/field&amp;gt; &amp;lt;/column&amp;gt; &amp;lt;/section&amp;gt; &amp;lt;datasource&amp;gt; &amp;lt;connection&amp;gt;数据库连接信息...&amp;lt;/connection&amp;gt; &amp;lt;sql&amp;gt;select dept, num_person, sales_amount from sales&amp;lt;/sql&amp;gt; &amp;lt;/datasource&amp;gt; &amp;lt;/playreport&amp;gt; 这个报表定义文件还是蛮简单的，它主要表达的是数据逻辑，忽略了表现层的信息。如果我们想要优先表达表现层的信息，例如字体大小、界面布局等，可以采用HTML模板的方式来定义报表，其实就是在一个HTML中嵌入了公式，比如：
&amp;lt;html&amp;gt; &amp;lt;body&amp;gt; &amp;lt;div class=&amp;quot;report&amp;quot; datasource=&amp;quot;这里放入数据源信息&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;table_header&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;column_header&amp;quot;&amp;gt;部门&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;column_header&amp;quot;&amp;gt;人数&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;column_header&amp;quot;&amp;gt;销售额&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;column_header&amp;quot;&amp;gt;人均销售额&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;table_body&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;field&amp;quot;&amp;gt;{=dept}&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;field&amp;quot;&amp;gt;{=num_person}&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;field&amp;quot;&amp;gt;{=sales_amount}&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;field&amp;quot;&amp;gt;{=sales_amount/num_person}&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 这样的HTML模板看上去是不是很熟悉？其实在很多语言里，比如PHP，都提供模板引擎功能，实现界面设计和应用代码的分离。这样一个模板，可以直接解释执行，或者先翻译成PHP或Java代码，然后再执行。只要运用我们学到的编译技术，这些都可以实现。</description></item><item><title>15_存储过程：如何提高程序的性能和安全性？</title><link>https://artisanbox.github.io/8/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/15/</guid><description>你好，我是朱晓峰。今天呢，我们来聊一聊MySQL的存储过程。
在我们的超市项目中，每天营业结束后，超市经营者都要计算当日的销量，核算成本和毛利等营业数据，这也就意味着每天都要做重复的数据统计工作。其实，这种数据量大，而且计算过程复杂的场景，就非常适合使用存储过程。
简单来说呢，存储过程就是把一系列SQL语句预先存储在MySQL服务器上，需要执行的时候，客户端只需要向服务器端发出调用存储过程的命令，服务器端就可以把预先存储好的这一系列SQL语句全部执行。
这样一来，不仅执行效率非常高，而且客户端不需要把所有的SQL语句通过网络发给服务器，减少了SQL语句暴露在网上的风险，也提高了数据查询的安全性。
今天，我就借助真实的超市项目，给你介绍一下如何创建和使用存储过程，帮助你提升查询的效率，并且让你开发的应用更加简洁安全。
如何创建存储过程？在创建存储过程的时候，我们需要用到关键字CREATE PROCEDURE。具体的语法结构如下：
CREATE PROCEDURE 存储过程名 （[ IN | OUT | INOUT] 参数名称 类型）程序体 接下来，我以超市的日结计算为例，给你讲一讲怎么创建存储过程。当然，为了方便你理解，我对计算的过程进行了简化。
假设在日结计算中，我们需要统计每天的单品销售，包括销售数量、销售金额、成本、毛利、毛利率等。同时，我们还要把计算出来的结果存入单品统计表中。
这个计算需要用到几个数据表，我分别来展示下这些表的基本信息。
销售单明细表（demo.transactiondetails）中包括了每笔销售中的商品编号、销售数量、销售价格和销售金额。
mysql&amp;gt; SELECT * -&amp;gt; FROM demo.transactiondetails; +---------------+------------+----------+------------+------------+ | transactionid | itemnumber | quantity | salesprice | salesvalue | +---------------+------------+----------+------------+------------+ | 1 | 1 | 1.000 | 89.00 | 89.00 | | 1 | 2 | 2.000 | 5.00 | 10.00 | | 2 | 1 | 2.000 | 89.</description></item><item><title>15_浮点数和定点数（上）：怎么用有限的Bit表示尽可能多的信息？</title><link>https://artisanbox.github.io/4/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/15/</guid><description>在我们日常的程序开发中，不只会用到整数。更多情况下，我们用到的都是实数。比如，我们开发一个电商App，商品的价格常常会是9块9；再比如，现在流行的深度学习算法，对应的机器学习里的模型里的各个权重也都是1.23这样的数。可以说，在实际的应用过程中，这些有零有整的实数，是和整数同样常用的数据类型，我们也需要考虑到。
浮点数的不精确性那么，我们能不能用二进制表示所有的实数，然后在二进制下计算它的加减乘除呢？先不着急，我们从一个有意思的小案例来看。
你可以在Linux下打开Python的命令行Console，也可以在Chrome浏览器里面通过开发者工具，打开浏览器里的Console，在里面输入“0.3 + 0.6”，然后看看你会得到一个什么样的结果。
&amp;gt;&amp;gt;&amp;gt; 0.3 + 0.6 0.8999999999999999 不知道你有没有大吃一惊，这么简单的一个加法，无论是在Python还是在JavaScript里面，算出来的结果居然不是准确的0.9，而是0.8999999999999999这么个结果。这是为什么呢？
在回答为什么之前，我们先来想一个更抽象的问题。通过前面的这么多讲，你应该知道我们现在用的计算机通常用16/32个比特（bit）来表示一个数。那我问你，我们用32个比特，能够表示所有实数吗？
答案很显然是不能。32个比特，只能表示2的32次方个不同的数，差不多是40亿个。如果表示的数要超过这个数，就会有两个不同的数的二进制表示是一样的。那计算机可就会一筹莫展，不知道这个数到底是多少。
40亿个数看似已经很多了，但是比起无限多的实数集合却只是沧海一粟。所以，这个时候，计算机的设计者们，就要面临一个问题了：我到底应该让这40亿个数映射到实数集合上的哪些数，在实际应用中才能最划得来呢？
定点数的表示有一个很直观的想法，就是我们用4个比特来表示0～9的整数，那么32个比特就可以表示8个这样的整数。然后我们把最右边的2个0～9的整数，当成小数部分；把左边6个0～9的整数，当成整数部分。这样，我们就可以用32个比特，来表示从0到999999.99这样1亿个实数了。
这种用二进制来表示十进制的编码方式，叫作BCD编码（Binary-Coded Decimal）。其实它的运用非常广泛，最常用的是在超市、银行这样需要用小数记录金额的情况里。在超市里面，我们的小数最多也就到分。这样的表示方式，比较直观清楚，也满足了小数部分的计算。
不过，这样的表示方式也有几个缺点。
第一，这样的表示方式有点“浪费”。本来32个比特我们可以表示40亿个不同的数，但是在BCD编码下，只能表示1亿个数，如果我们要精确到分的话，那么能够表示的最大金额也就是到100万。如果我们的货币单位是人民币或者美元还好，如果我们的货币单位变成了津巴布韦币，这个数量就不太够用了。
第二，这样的表示方式没办法同时表示很大的数字和很小的数字。我们在写程序的时候，实数的用途可能是多种多样的。有时候我们想要表示商品的金额，关心的是9.99这样小的数字；有时候，我们又要进行物理学的运算，需要表示光速，也就是$3×10^8$这样很大的数字。那么，我们有没有一个办法，既能够表示很小的数，又能表示很大的数呢？
浮点数的表示答案当然是有的，就是你可能经常听说过的浮点数（Floating Point），也就是float类型。
我们先来想一想。如果我们想在一张便签纸上，用一行来写一个十进制数，能够写下多大范围的数？因为我们要让人能够看清楚，所以字最小也有一个限制。你会发现一个和上面我们用BCD编码表示数一样的问题，就是纸张的宽度限制了我们能够表示的数的大小。如果宽度只放得下8个数字，那么我们还是只能写下最大到99999999这样的数字。
有限宽度的便签，只能写下有限大小的数字其实，这里的纸张宽度，就和我们32个比特一样，是在空间层面的限制。那么，在现实生活中，我们是怎么表示一个很大的数的呢？比如说，我们想要在一本科普书里，写一下宇宙内原子的数量，莫非是用一页纸，用好多行写下很多个0么？
当然不是了，我们会用科学计数法来表示这个数字。宇宙内的原子的数量，大概在 10的82次方左右，我们就用$1.0×10^82$这样的形式来表示这个数值，不需要写下82个0。
在计算机里，我们也可以用一样的办法，用科学计数法来表示实数。浮点数的科学计数法的表示，有一个IEEE的标准，它定义了两个基本的格式。一个是用32比特表示单精度的浮点数，也就是我们常常说的float或者float32类型。另外一个是用64比特表示双精度的浮点数，也就是我们平时说的double或者float64类型。
双精度类型和单精度类型差不多，这里，我们来看单精度类型，双精度你自然也就明白了。
单精度的32个比特可以分成三部分。
第一部分是一个符号位，用来表示是正数还是负数。我们一般用s来表示。在浮点数里，我们不像正数分符号数还是无符号数，所有的浮点数都是有符号的。
接下来是一个8个比特组成的指数位。我们一般用e来表示。8个比特能够表示的整数空间，就是0～255。我们在这里用1～254映射到-126～127这254个有正有负的数上。因为我们的浮点数，不仅仅想要表示很大的数，还希望能够表示很小的数，所以指数位也会有负数。
你发现没，我们没有用到0和255。没错，这里的 0（也就是8个比特全部为0） 和 255 （也就是8个比特全部为1）另有它用，我们等一下再讲。
最后，是一个23个比特组成的有效数位。我们用f来表示。综合科学计数法，我们的浮点数就可以表示成下面这样：
$(-1)^s×1.f×2^e$
你会发现，这里的浮点数，没有办法表示0。的确，要表示0和一些特殊的数，我们就要用上在e里面留下的0和255这两个表示，这两个表示其实是两个标记位。在e为0且f为0的时候，我们就把这个浮点数认为是0。至于其它的e是0或者255的特殊情况，你可以看下面这个表格，分别可以表示出无穷大、无穷小、NAN以及一个特殊的不规范数。
我们可以以0.5为例子。0.5的符号为s应该是0，f应该是0，而e应该是-1，也就是
$0.5= (-1)^0×1.0×2^{-1}=0.5$，对应的浮点数表示，就是32个比特。
$s=0，e = 2^{-1}$，需要注意，e表示从-126到127个，-1是其中的第126个数，这里的e如果用整数表示，就是$2^6+2^5+2^4+2^3+2^2+2^1=126$，$1.f=1.0$。
在这样的浮点数表示下，不考虑符号的话，浮点数能够表示的最小的数和最大的数，差不多是$1.17×10^{-38}$和$3.40×10^{38}$。比前面的BCD编码能够表示的范围大多了。
总结延伸你会看到，在这样的表示方式下，浮点数能够表示的数据范围一下子大了很多。正是因为这个数对应的小数点的位置是“浮动”的，它才被称为浮点数。随着指数位e的值的不同，小数点的位置也在变动。对应的，前面的BCD编码的实数，就是小数点固定在某一位的方式，我们也就把它称为定点数。
回到我们最开头，为什么我们用0.3 + 0.6不能得到0.9呢？这是因为，浮点数没有办法精确表示0.3、0.6和0.9。事实上，我们拿出0.1～0.9这9个数，其中只有0.5能够被精确地表示成二进制的浮点数，也就是s = 0、e = -1、f = 0这样的情况。
而0.3、0.6乃至我们希望的0.9，都只是一个近似的表达。这个也为我们带来了一个挑战，就是浮点数无论是表示还是计算其实都是近似计算。那么，在使用过程中，我们该怎么来使用浮点数，以及使用浮点数会遇到些什么问题呢？下一讲，我会用更多的实际代码案例，来带你看看浮点数计算中的各种“坑”。
推荐阅读如果对浮点数的表示还不是很清楚，你可以仔细阅读一下《计算机组成与设计：硬件/软件接口》的3.5.1节。
课后思考对于BCD编码的定点数，如果我们用7个比特来表示连续两位十进制数，也就是00～99，是不是可以让32比特表示更大一点的数据范围？如果我们还需要表示负数，那么一个32比特的BCD编码，可以表示的数据范围是多大？
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>15_答疑文章（一）：日志和索引相关问题</title><link>https://artisanbox.github.io/1/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/15/</guid><description>在今天这篇答疑文章更新前，MySQL实战这个专栏已经更新了14篇。在这些文章中，大家在评论区留下了很多高质量的留言。现在，每篇文章的评论区都有热心的同学帮忙总结文章知识点，也有不少同学提出了很多高质量的问题，更有一些同学帮忙解答其他同学提出的问题。
在浏览这些留言并回复的过程中，我倍受鼓舞，也尽我所知地帮助你解决问题、和你讨论。可以说，你们的留言活跃了整个专栏的氛围、提升了整个专栏的质量，谢谢你们。
评论区的大多数留言我都直接回复了，对于需要展开说明的问题，我都拿出小本子记了下来。这些被记下来的问题，就是我们今天这篇答疑文章的素材了。
到目前为止，我已经收集了47个问题，很难通过今天这一篇文章全部展开。所以，我就先从中找了几个联系非常紧密的问题，串了起来，希望可以帮你解决关于日志和索引的一些疑惑。而其他问题，我们就留着后面慢慢展开吧。
日志相关问题我在第2篇文章《日志系统：一条SQL更新语句是如何执行的？》中，和你讲到binlog（归档日志）和redo log（重做日志）配合崩溃恢复的时候，用的是反证法，说明了如果没有两阶段提交，会导致MySQL出现主备数据不一致等问题。
在这篇文章下面，很多同学在问，在两阶段提交的不同瞬间，MySQL如果发生异常重启，是怎么保证数据完整性的？
现在，我们就从这个问题开始吧。
我再放一次两阶段提交的图，方便你学习下面的内容。
图1 两阶段提交示意图这里，我要先和你解释一个误会式的问题。有同学在评论区问到，这个图不是一个update语句的执行流程吗，怎么还会调用commit语句？
他产生这个疑问的原因，是把两个“commit”的概念混淆了：
他说的“commit语句”，是指MySQL语法中，用于提交一个事务的命令。一般跟begin/start transaction 配对使用。 而我们图中用到的这个“commit步骤”，指的是事务提交过程中的一个小步骤，也是最后一步。当这个步骤执行完成后，这个事务就提交完成了。 “commit语句”执行的时候，会包含“commit 步骤”。 而我们这个例子里面，没有显式地开启事务，因此这个update语句自己就是一个事务，在执行完成后提交事务时，就会用到这个“commit步骤“。
接下来，我们就一起分析一下在两阶段提交的不同时刻，MySQL异常重启会出现什么现象。
如果在图中时刻A的地方，也就是写入redo log 处于prepare阶段之后、写binlog之前，发生了崩溃（crash），由于此时binlog还没写，redo log也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog还没写，所以也不会传到备库。到这里，大家都可以理解。
大家出现问题的地方，主要集中在时刻B，也就是binlog写完，redo log还没commit前发生crash，那崩溃恢复的时候MySQL会怎么处理？
我们先来看一下崩溃恢复时的判断规则。
如果redo log里面的事务是完整的，也就是已经有了commit标识，则直接提交；
如果redo log里面的事务只有完整的prepare，则判断对应的事务binlog是否存在并完整：
a. 如果是，则提交事务；
b. 否则，回滚事务。
这里，时刻B发生crash对应的就是2(a)的情况，崩溃恢复过程中事务会被提交。
现在，我们继续延展一下这个问题。
追问1：MySQL怎么知道binlog是完整的?回答：一个事务的binlog是有完整格式的：
statement格式的binlog，最后会有COMMIT； row格式的binlog，最后会有一个XID event。 另外，在MySQL 5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQL可以通过校验checksum的结果来发现。所以，MySQL还是有办法验证事务binlog的完整性的。
追问2：redo log 和 binlog是怎么关联起来的?回答：它们有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log：
如果碰到既有prepare、又有commit的redo log，就直接提交； 如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。 追问3：处于prepare阶段的redo log加上完整binlog，重启就能恢复，MySQL为什么要这么设计?回答：其实，这个问题还是跟我们在反证法中说到的数据与备份的一致性有关。在时刻B，也就是binlog写完以后MySQL发生崩溃，这时候binlog已经写入了，之后就会被从库（或者用这个binlog恢复出来的库）使用。
所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。
追问4：如果这样的话，为什么还要两阶段提交呢？干脆先redo log写完，再写binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？回答：其实，两阶段提交是经典的分布式系统问题，并不是MySQL独有的。
如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。
对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回滚不了，数据和binlog日志又不一致了。</description></item><item><title>15｜汇编语言学习（二）：熟悉X86汇编代码</title><link>https://artisanbox.github.io/3/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/17/</guid><description>你好，我是宫文学。
上一节课，在开始写汇编代码之前，我先带着你在CPU架构方面做了一些基础的铺垫工作。我希望能让你有个正确的认知：其实汇编语言的语法等层面的知识是很容易掌握的。但要真正学懂汇编语言，关键还是要深入了解CPU架构。
今天这一节课，我们会再进一步，特别针对X86汇编代码来近距离分析一下。我会带你吃透一个汇编程序的例子，在这个过程中，你会获得关于汇编程序构成、指令构成、内存访问方式、栈桢维护，以及汇编代码优化等方面的知识点。掌握这些知识点之后，我们后面生成汇编代码的工作就会顺畅很多了！
好了，我们开始第一步，通过实际的示例程序，看看X86的汇编代码是什么样子的。
学习编译器生成的汇编代码按我个人的经验来说，学习汇编最快的方法，就是让别的编译器生成汇编代码给我们看。
比如，你可以用C语言写出表达式计算、函数调用、条件分支等不同的逻辑，然后让C语言的编译器编译一下，就知道这些逻辑对应的汇编代码是什么样子了，而且你还可以分析每条代码的作用。这样看多了、分析多了以后，你自然就会对汇编语言越来越熟悉，也敢自己上手写了。
我们还是采用上一节课那个用C语言写的示例函数foo，我们让这个函数接受一个整型的参数，把它加上10以后返回：
int foo(int a){ return a+10; } 接着，再输入下面的clang或gcc命令：
clang -S foo.c -o foo.s 或 gcc -S foo.c -o foo.s 然后我们用一个文本编辑器打开foo.s，你就会看到下面这些汇编代码：
.section __TEXT,__text,regular,pure_instructions .build_version macos, 11, 0 sdk_version 11, 3 .globl _foo ## -- Begin function foo .p2align 4, 0x90 _foo: ## @foo .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp movl %edi, -4(%rbp) movl -4(%rbp), %eax addl $10, %eax popq %rbp retq .</description></item><item><title>16_JavaJIT编译器（四）：Graal的后端是如何工作的？</title><link>https://artisanbox.github.io/7/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/16/</guid><description>你好，我是宫文学。
前面两讲中，我介绍了Sea of Nodes类型的HIR，以及基于HIR的各种分析处理，这可以看做是编译器的中端。
可编译器最终还是要生成机器码的。那么，这个过程是怎么实现的呢？与硬件架构相关的LIR是什么样子的呢？指令选择是怎么做的呢？
这一讲，我就带你了解Graal编译器的后端功能，回答以上这些问题，破除你对后端处理过程的神秘感。
首先，我们来直观地了解一下后端处理的流程。
后端的处理流程在第14讲中，我们在运行Java示例程序的时候（比如atLeastTen()方法），使用了“-Dgraal.Dump=:5”的选项，这个选项会dump出整个编译过程最详细的信息。
对于HIR的处理过程，程序会通过网络端口，dump到IdealGraphVisualizer里面。而后端的处理过程，缺省则会dump到工作目录下的一个“graal_dumps”子目录下。你可以用文本编辑器打开查看里面的信息。
//至少返回10 public int atLeastTen(int a){ if (a &amp;lt; 10) return 10; else return a; } 不过，你还可以再偷懒一下，使用一个图形工具c1visualizer来查看。
补充：c1visualizer原本是用于查看Hopspot的C1编译器（也就是客户端编译器）的LIR的工具，这也就是说，Graal的LIR和C1的是一样的。另外，该工具不能用太高版本的JDK运行，我用的是JDK1.8。
图1：atLeatTen()方法对应的LIR在窗口的左侧，你能看到后端的处理流程。
首先是把HIR做最后一次排序（HIR Final Schedule），这个处理会把HIR节点分配到基本块，并且排序； 第二是生成LIR，在这个过程中要做指令选择； 第三，寄存器分配工作，Graal采用的算法是线性扫描（Linear Scan）； 第四，是基于LIR的一些优化工作，比如ControlFlowOptimizer等； 最后一个步骤，是生成目标代码。 接下来，我们来认识一下这个LIR：它是怎样生成的，用什么数据结构保存的，以及都有什么特点。
认识LIR在对HIR的处理过程中，前期（High Tier、Mid Tier）基本上都是与硬件无关。到了后期（Low Tier），你会看到IR中的一些节点逐步开始带有硬件的特征，比如上一讲中，计算AMD64地址的节点。而LIR就更加反映目标硬件的特征了，基本上可以跟机器码一对一地翻译。所以，从HIR生成LIR的过程，就要做指令选择。
我把与LIR相关的包和类整理成了类图，里面划分成了三个包，分别包含了与HIR、LIR和CFG有关的类。你可以重点看看它们之间的相互关系。
图2：HIR、LIR和CFG的关联关系在HIR的最后的处理阶段，程序会通过一个Schedule过程，把HIR节点排序，并放到控制流图中，为生成LIR和目标代码做准备。我之前说过，HIR的一大好处，就是那些浮动节点，可以最大程度地免受控制流的约束。但在最后生成的目标代码中，我们还是要把每行指令归属到某个具体的基本块的。而且，基本块中的HIR节点是按照顺序排列的，在ScheduleResult中保存着这个顺序（blockToNodesMap中顺序保存了每个Block中的节点）。
你要注意，这里所说的Schedule，跟编译器后端的指令排序不是一回事儿。这里是把图变成线性的程序；而编译器后端的指令排序（也叫做Schedule），则是为了实现指令级并行的优化。
当然，把HIR节点划分到不同的基本块，优化程度是不同的。比如，与循环无关的代码，放在循环内部和外部都是可以的，但显然放在循环外部更好一些。把HIR节点排序的Schedule算法，复杂度比较高，所以使用了很多启发式的规则。刚才提到的把循环无关代码放在循环外面，就是一种启发式的规则。
图2中的ControlFlowGraph类和Block类构成了控制流图，控制流图和最后阶段的HIR是互相引用的。这样，你就可以知道HIR中的每个节点属于哪个基本块，也可以知道每个基本块中包含的HIR节点。
做完Schedule以后，接着就会生成LIR。与声明式的HIR不同，LIR是命令式的，由一行行指令构成。
图1显示的是Foo.atLeatTen方法对应的LIR。你会看到一个控制流图（CFG），里面有三个基本块。B0是B1和B2的前序基本块，B0中的最后一个语句是分支语句（基本块中，只有最后一个语句才可以是导致指令跳转的语句）。
LIR中的指令是放到基本块中的，LIR对象的LIRInstructions属性中，保存了每个基本块中的指令列表。
OK，那接下来，我们来看看LIR的指令都有哪些，它们都有什么特点。
LIRInstruction的子类，主要放在三个包中，你可以看看下面的类图。
图3：LIR中的指令类型首先，在org.graalvm.compiler.lir包中，声明了一些与架构无关的指令，比如跳转指令、标签指令等。因为无论什么架构的CPU，一定都会有跳转指令，也一定有作为跳转目标的标签。
然后，在org.graalvm.compiler.lir.amd64包中，声明了几十个AMD64架构的指令，为了降低你的阅读负担，这里我只列出了有代表性的几个。这些指令是LIR代码中的主体。
最后，在org.graalvm.compiler.hotspot.amd64包中，也声明了几个指令。这几个指令是利用HotSpot虚拟机的功能实现的。比如，要获取某个类的定义的地址，只能由虚拟机提供。
好了，通过这样的一个分析，你应该对LIR有更加具体的认识了：LIR中的指令，大多数是与架构相关的。这样才适合运行后端的一些算法，比如指令选择、寄存器分配等。你也可以据此推测，其他编译器的LIR，差不多也是这个特点。
接下来，我们就来了解一下Graal编译器是如何生成LIR，并且在这个过程中，它是如何实现指令选择的。
生成LIR及指令选择我们已经知道了，Graal在生成LIR的过程中，要进行指令选择。
我们先看一下Graal对一个简单的示例程序Foo.add1，是如何生成LIR的。
public static int add1(int x, int y){ return x + y + 10; } 这个示例程序，在转LIR之前，它的HIR是下面这样。其中有两个加法节点，操作数包括了参数（ParameterNode）和常数（ConstantNode）两种类型。最后是一个Return节点。这个例子足够简单。实际上，它简单到只是一棵树，而不是图。</description></item><item><title>16_NFA和DFA：如何自己实现一个正则表达式工具？</title><link>https://artisanbox.github.io/6/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/16/</guid><description>回顾之前讲的内容，原理篇重在建立直观理解，帮你建立信心，这是第一轮的认知迭代。应用篇帮你涉足应用领域，在解决领域问题时发挥编译技术的威力，积累运用编译技术的一手经验，也启发你用编译技术去解决更多的领域问题，这是第二轮的认知迭代。而为时三节课的算法篇将你是第三轮的认知迭代。
在第三轮的认知迭代中，我会带你掌握前端技术中的核心算法。而本节课，我就借“怎样实现正则表达式工具？”这个问题，探讨第一组算法：与正则表达式处理有关的算法。
在词法分析阶段，我们可以手工构造有限自动机（FSA，或FSM）实现词法解析，过程比较简单。现在我们不再手工构造词法分析器，而是直接用正则表达式解析词法。
你会发现，我们只要写一些规则，就能基于这些规则分析和处理文本。这种能够理解正则表达式的功能，除了能生成词法分析器，还有很多用途。比如Linux的三个超级命令，又称三剑客（grep、awk和sed），都是因为能够直接支持正则表达式，功能才变得强大的。
接下来，我就带你完成编写正则表达式工具的任务，与此同时，你就能用正则文法生成词法分析器了：
首先，把正则表达式翻译成非确定的有限自动机（Nondeterministic Finite Automaton，NFA）。
其次，基于NFA处理字符串，看看它有什么特点。
然后，把非确定的有限自动机转换成确定的有限自动机（Deterministic Finite Automaton，DFA）
最后，运行DFA，看看它有什么特点。
强调一下，不要被非确定的有限自动机、确定的有限自动机这些概念吓倒，我肯定让你学明白。
认识DFA和NFA在讲词法分析时，我提到有限自动机（FSA）有有限个状态。识别Token的过程，就是FSA状态迁移的过程。其中，FSA分为确定的有限自动机（DFA）和非确定的有限自动机（NFA）。
DFA的特点是，在任何一个状态，我们基于输入的字符串，都能做一个确定的转换，比如：
NFA的特点是，它存在某些状态，针对某些输入，不能做一个确定的转换，这又细分成两种情况：
对于一个输入，它有两个状态可以转换。 存在ε转换。也就是没有任何输入的情况下，也可以从一个状态迁移到另一个状态。 比如，“a[a-zA-Z0-9]*bc”这个正则表达式对字符串的要求是以a开头，以bc结尾，a和bc之间可以有任意多个字母或数字。在图中状态1的节点输入b时，这个状态是有两条路径可以选择的，所以这个有限自动机是一个NFA。
这个NFA还有引入ε转换的画法，它们是等价的。实际上，第二个NFA可以用我们今天讲的算法，通过正则表达式自动生成出来。
需要注意的是，无论是NFA还是DFA，都等价于正则表达式。也就是，所有的正则表达式都能转换成NFA或DFA，所有的NFA或DFA，也都能转换成正则表达式。
理解了NFA和DFA之后，来看看我们如何从正则表达式生成NFA。
从正则表达式生成NFA我们需要把它分为两个子任务：
第一个子任务，是把正则表达式解析成一个内部的数据结构，便于后续的程序使用。因为正则表达式也是个字符串，所以要先做一个小的编译器，去理解代表正则表达式的字符串。我们可以偷个懒，直接针对示例的正则表达式生成相应的数据结构，不需要做出这个编译器。
用来测试的正则表达式可以是int关键字、标识符，或者数字字面量：
int | [a-zA-Z][a-zA-Z0-9]* | [0-9]+ 我用下面这段代码创建了一个树状的数据结构，来代表用来测试的正则表达式：
private static GrammarNode sampleGrammar1() { GrammarNode node = new GrammarNode(&amp;quot;regex1&amp;quot;,GrammarNodeType.Or); //int关键字 GrammarNode intNode = node.createChild(GrammarNodeType.And); intNode.createChild(new CharSet('i')); intNode.createChild(new CharSet('n')); intNode.createChild(new CharSet('t')); //标识符 GrammarNode idNode = node.createChild(GrammarNodeType.And); GrammarNode firstLetter = idNode.createChild(CharSet.letter); GrammarNode letterOrDigit = idNode.createChild(CharSet.letterOrDigit); letterOrDigit.setRepeatTimes(0, -1); //数字字面量 GrammarNode literalNode = node.</description></item><item><title>16_“orderby”是怎么工作的？</title><link>https://artisanbox.github.io/1/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/16/</guid><description>在你开发应用的时候，一定会经常碰到需要根据指定的字段排序来显示结果的需求。还是以我们前面举例用过的市民表为例，假设你要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前1000个人的姓名、年龄。
假设这个表的部分定义是这样的：
CREATE TABLE `t` ( `id` int(11) NOT NULL, `city` varchar(16) NOT NULL, `name` varchar(16) NOT NULL, `age` int(11) NOT NULL, `addr` varchar(128) DEFAULT NULL, PRIMARY KEY (`id`), KEY `city` (`city`) ) ENGINE=InnoDB; 这时，你的SQL语句可以这么写：
select city,name,age from t where city='杭州' order by name limit 1000 ; 这个语句看上去逻辑很清晰，但是你了解它的执行流程吗？今天，我就和你聊聊这个语句是怎么执行的，以及有什么参数会影响执行的行为。
全字段排序前面我们介绍过索引，所以你现在就很清楚了，为避免全表扫描，我们需要在city字段加上索引。
在city字段上创建索引之后，我们用explain命令来看看这个语句的执行情况。
图1 使用explain命令查看语句的执行情况Extra这个字段中的“Using filesort”表示的就是需要排序，MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。
为了说明这个SQL查询语句的执行过程，我们先来看一下city这个索引的示意图。
图2 city字段的索引示意图从图中可以看到，满足city='杭州’条件的行，是从ID_X到ID_(X+N)的这些记录。
通常情况下，这个语句执行流程如下所示 ：
初始化sort_buffer，确定放入name、city、age这三个字段；
从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X；
到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中；
从索引city取下一个记录的主键id；</description></item><item><title>16_二分查找（下）：如何快速定位IP对应的省份地址？</title><link>https://artisanbox.github.io/2/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/17/</guid><description>通过IP地址来查找IP归属地的功能，不知道你有没有用过？没用过也没关系，你现在可以打开百度，在搜索框里随便输一个IP地址，就会看到它的归属地。
这个功能并不复杂，它是通过维护一个很大的IP地址库来实现的。地址库中包括IP地址范围和归属地的对应关系。
当我们想要查询202.102.133.13这个IP地址的归属地时，我们就在地址库中搜索，发现这个IP地址落在[202.102.133.0, 202.102.133.255]这个地址范围内，那我们就可以将这个IP地址范围对应的归属地“山东东营市”显示给用户了。
[202.102.133.0, 202.102.133.255] 山东东营市 [202.102.135.0, 202.102.136.255] 山东烟台 [202.102.156.34, 202.102.157.255] 山东青岛 [202.102.48.0, 202.102.48.255] 江苏宿迁 [202.102.49.15, 202.102.51.251] 江苏泰州 [202.102.56.0, 202.102.56.255] 江苏连云港 现在我的问题是，在庞大的地址库中逐一比对IP地址所在的区间，是非常耗时的。假设我们有12万条这样的IP区间与归属地的对应关系，如何快速定位出一个IP地址的归属地呢？
是不是觉得比较难？不要紧，等学完今天的内容，你就会发现这个问题其实很简单。
上一节我讲了二分查找的原理，并且介绍了最简单的一种二分查找的代码实现。今天我们来讲几种二分查找的变形问题。
不知道你有没有听过这样一个说法：“十个二分九个错”。二分查找虽然原理极其简单，但是想要写出没有Bug的二分查找并不容易。
唐纳德·克努特（Donald E.Knuth）在《计算机程序设计艺术》的第3卷《排序和查找》中说到：“尽管第一个二分查找算法于1946年出现，然而第一个完全正确的二分查找算法实现直到1962年才出现。”
你可能会说，我们上一节学的二分查找的代码实现并不难写啊。那是因为上一节讲的只是二分查找中最简单的一种情况，在不存在重复元素的有序数组中，查找值等于给定值的元素。最简单的二分查找写起来确实不难，但是，二分查找的变形问题就没那么好写了。
二分查找的变形问题很多，我只选择几个典型的来讲解，其他的你可以借助我今天讲的思路自己来分析。
需要特别说明一点，为了简化讲解，今天的内容，我都以数据是从小到大排列为前提，如果你要处理的数据是从大到小排列的，解决思路也是一样的。同时，我希望你最好先自己动手试着写一下这4个变形问题，然后再看我的讲述，这样你就会对我说的“二分查找比较难写”有更加深的体会了。
变体一：查找第一个值等于给定值的元素上一节中的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。如果我们将这个问题稍微修改下，有序数据集合中存在重复的数据，我们希望找到第一个值等于给定值的数据，这样之前的二分查找代码还能继续工作吗？
比如下面这样一个有序数组，其中，a[5]，a[6]，a[7]的值都等于8，是重复的数据。我们希望查找第一个等于8的数据，也就是下标是5的元素。
如果我们用上一节课讲的二分查找的代码实现，首先拿8与区间的中间值a[4]比较，8比6大，于是在下标5到9之间继续查找。下标5和9的中间位置是下标7，a[7]正好等于8，所以代码就返回了。
尽管a[7]也等于8，但它并不是我们想要找的第一个等于8的元素，因为第一个值等于8的元素是数组下标为5的元素。我们上一节讲的二分查找代码就无法处理这种情况了。所以，针对这个变形问题，我们可以稍微改造一下上一节的代码。
100个人写二分查找就会有100种写法。网上有很多关于变形二分查找的实现方法，有很多写得非常简洁，比如下面这个写法。但是，尽管简洁，理解起来却非常烧脑，也很容易写错。
public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &amp;lt;= high) { int mid = low + ((high - low) &amp;gt;&amp;gt; 1); if (a[mid] &amp;gt;= value) { high = mid - 1; } else { low = mid + 1; } } if (low &amp;lt; n &amp;amp;&amp;amp; a[low]==value) return low; else return -1; } 看完这个实现之后，你是不是觉得很不好理解？如果你只是死记硬背这个写法，我敢保证，过不了几天，你就会全都忘光，再让你写，90%的可能会写错。所以，我换了一种实现方法，你看看是不是更容易理解呢？</description></item><item><title>16_划分土地（上）：如何划分与组织内存？</title><link>https://artisanbox.github.io/9/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/16/</guid><description>你好，我是LMOS。
内存跟操作系统的关系，就像土地和政府的关系一样。政府必须合理规划这个国家的土地，才能让人民安居乐业。为了发展，政府还要进而建立工厂、学校，发展工业和教育，规划城镇，国家才能繁荣富强。
而作为计算机的实际掌权者，操作系统必须科学合理地管理好内存，应用程序才能高效稳定地运行。
内存管理是一项复杂的工作，我会用三节课带你搞定它。
具体我是这么安排的：这节课，我们先解决内存的划分方式和内存页的表示、组织问题，设计好数据结构。下一节课，我会带你在内存中建立数据结构对应的实例变量，搞定内存页的初始化问题。最后一节课，我们会依赖前面建好的数据结构，实现内存页面管理算法。
好，今天我们先从内存的划分单位讲起，一步步为内存管理工作做好准备。
今天课程的配套代码，你可以点击这里，自行下载。
分段还是分页要划分内存，我们就要先确定划分的单位是按段还是按页，就像你划分土地要选择按亩还是按平方分割一样。
其实分段与分页的优缺点，前面MMU相关的课程已经介绍过了。这里我们从内存管理角度，理一理分段与分页的问题。
第一点，从表示方式和状态确定角度考虑。段的长度大小不一，用什么数据结构表示一个段，如何确定一个段已经分配还是空闲呢？而页的大小固定，我们只需用位图就能表示页的分配与释放。
比方说，位图中第1位为1，表示第一个页已经分配；位图中第2位为0，表示第二个页是空闲，每个页的开始地址和大小都是固定的。
第二点，从内存碎片的利用看，由于段的长度大小不一，更容易产生内存碎片，例如内存中有A段（内存地址：0～5000）、 B段（内存地址：5001～8000）、C段（内存地址：8001～9000），这时释放了B段，然后需要给D段分配内存空间，且D段长度为5000。
你立马就会发现A段和C段之间的空间（B段）不能满足，只能从C段之后的内存空间开始分配，随着程序运行，这些情况会越来越多。段与段之间存在着不大不小的空闲空间，内存总的空闲空间很多，但是放不下一个新段。
而页的大小固定，分配最小单位是页，页也会产生碎片，比如我需要请求分配4个页，但在内存中从第1～3个页是空闲的，第4个页是分配出去了，第5个页是空闲的。这种情况下，我们通过修改页表的方式，就能让连续的虚拟页面映射到非连续的物理页面。
第三点，从内存和硬盘的数据交换效率考虑，当内存不足时，操作系统希望把内存中的一部分数据写回硬盘，来释放内存。这就涉及到内存和硬盘交换数据，交换单位是段还是页？
如果是段的话，其大小不一，A段有50MB，B段有1KB，A、B段写回硬盘的时间也不同，有的段需要时间长，有的段需要时间短，硬盘的空间分配也会有上面第二点同样的问题，这样会导致系统性能抖动。如果每次交换一个页，则没有这些问题。
还有最后一点，段最大的问题是使得虚拟内存地址空间，难于实施。（后面的课还会展开讲）
综上，我们自然选择分页模式来管理内存，其实现在所有的商用操作系统都使用了分页模式管理内存。我们用4KB作为页大小，这也正好对应x86 CPU长模式下MMU 4KB的分页方式。
如何表示一个页我们使用分页模型来管理内存。首先是把物理内存空间分成4KB大小页，这页表示从地址x开始到x+0xFFF这一段的物理内存空间，x必须是0x1000对齐的。这一段x+0xFFF的内存空间，称为内存页。
在逻辑上的结构图如下所示：
上图这是一个接近真实机器的情况，不过一定不要忘记前面的内存布局示图，真实的物理内存地址空间不是连续的，这中间可能有空洞，可能是显存，也可能是外设的寄存器。
真正的物理内存空间布局信息来源于e820map_t结构数组，之前的初始化中，我们已经将其转换成phymmarge_t结构数组了，由 kmachbsp-&amp;gt;mb_e820expadr指向。
那问题来了，现在我们已经搞清楚了什么是页，但如何表示一个页呢？
你可能会想到位图或者整型变量数组，用其中一个位代表一个页，位值为0时表示页空闲，位值为1时表示页已分配；或者用整型数组中一个元素表示一个页，用具体数组元素的数值代表页的状态。
如果这样的话，分配、释放内存页的算法就确定了，就是扫描位图或者扫描数组。这样确实可以做出最简单的内存页管理器，但这也是最低效的。
上面的方案之所以低效，是因为我们仅仅只是保存了内存页的空闲和已分配的信息，这是不够的。我们的Cosmos当然不能这么做，我们需要页的状态、页的地址、页的分配记数、页的类型、页的链表，你自然就会想到，这些信息可以用一个C语言结构体封装起来。
让我们马上就来实现这个结构体，在cosmos/include/halinc/下建立一个msadsc_t.h文件，在其中实现这个结构体，代码如下所示。
//内存空间地址描述符标志 typedef struct s_MSADFLGS { u32_t mf_olkty:2; //挂入链表的类型 u32_t mf_lstty:1; //是否挂入链表 u32_t mf_mocty:2; //分配类型，被谁占用了，内核还是应用或者空闲 u32_t mf_marty:3; //属于哪个区 u32_t mf_uindx:24; //分配计数 }__attribute__((packed)) msadflgs_t; //物理地址和标志 typedef struct s_PHYADRFLGS { u64_t paf_alloc:1; //分配位 u64_t paf_shared:1; //共享位 u64_t paf_swap:1; //交换位 u64_t paf_cache:1; //缓存位 u64_t paf_kmap:1; //映射位 u64_t paf_lock:1; //锁定位 u64_t paf_dirty:1; //脏位 u64_t paf_busy:1; //忙位 u64_t paf_rv2:4; //保留位 u64_t paf_padrs:52; //页物理地址位 }__attribute__((packed)) phyadrflgs_t; //内存空间地址描述符 typedef struct s_MSADSC { list_h_t md_list; //链表 spinlock_t md_lock; //保护自身的自旋锁 msadflgs_t md_indxflgs; //内存空间地址描述符标志 phyadrflgs_t md_phyadrs; //物理地址和标志 void* md_odlink; //相邻且相同大小msadsc的指针 }__attribute__((packed)) msadsc_t; msadsc_t结构看似很大，实则很小，也必须要小，因为它表示一个页面，物理内存页有多少就需要有多少个msadsc_t结构。正是因为页面地址总是按4KB对齐，所以phyadrflgs_t结构的低12位才可以另作它用。</description></item><item><title>16_浮点数和定点数（下）：深入理解浮点数到底有什么用？</title><link>https://artisanbox.github.io/4/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/16/</guid><description>上一讲，我们讲了用“浮点数”这样的数据形式，来表示一个不能确定大小的数据范围。浮点数可以大到$3.40×10^{38}$，也可以小到$1.17×10^{-38}$这样的数值。同时，我们也发现，其实我们平时写的0.1、0.2并不是精确的数值，只是一个近似值。只有0.5这样，可以表示成$2^{-1}$这种形式的，才是一个精确的浮点数。
你是不是感到很疑惑，浮点数的近似值究竟是怎么算出来的？浮点数的加法计算又是怎么回事儿？在实践应用中，我们怎么才用好浮点数呢？这一节，我们就一起来看这几个问题。
浮点数的二进制转化我们首先来看，十进制的浮点数怎么表示成二进制。
我们输入一个任意的十进制浮点数，背后都会对应一个二进制表示。比方说，我们输入了一个十进制浮点数9.1。那么按照之前的讲解，在二进制里面，我们应该把它变成一个“符号位s+指数位e+有效位数f”的组合。第一步，我们要做的，就是把这个数变成二进制。
首先，我们把这个数的整数部分，变成一个二进制。这个我们前面讲二进制的时候已经讲过了。这里的9，换算之后就是1001。
接着，我们把对应的小数部分也换算成二进制。小数怎么换成二进制呢？我们先来定义一下，小数的二进制表示是怎么回事。我们拿0.1001这样一个二进制小数来举例说明。和上面的整数相反，我们把小数点后的每一位，都表示对应的2的-N次方。那么0.1001，转化成十进制就是：
$1×2^{-1}+0×2^{-2}+0×2^{-3}+$
$1×2^{-4}=0.5625$
和整数的二进制表示采用“除以2，然后看余数”的方式相比，小数部分转换成二进制是用一个相似的反方向操作，就是乘以2，然后看看是否超过1。如果超过1，我们就记下1，并把结果减去1，进一步循环操作。在这里，我们就会看到，0.1其实变成了一个无限循环的二进制小数，0.000110011。这里的“0011”会无限循环下去。
然后，我们把整数部分和小数部分拼接在一起，9.1这个十进制数就变成了1001.000110011…这样一个二进制表示。
上一讲我们讲过，浮点数其实是用二进制的科学计数法来表示的，所以我们可以把小数点左移三位，这个数就变成了：
$1.0010$$0011$$0011… × 2^3$
那这个二进制的科学计数法表示，我们就可以对应到了浮点数的格式里了。这里的符号位s = 0，对应的有效位f=001000110011…。因为f最长只有23位，那这里“0011”无限循环，最多到23位就截止了。于是，f=00100011001100110011 001。最后的一个“0011”循环中的最后一个“1”会被截断掉。对应的指数为e，代表的应该是3。因为指数位有正又有负，所以指数位在127之前代表负数，之后代表正数，那3其实对应的是加上127的偏移量130，转化成二进制，就是130，对应的就是指数位的二进制，表示出来就是10000010。
然后，我们把“s+e+f”拼在一起，就可以得到浮点数9.1的二进制表示了。最终得到的二进制表示就变成了：
010000010 0010 0011001100110011 001
如果我们再把这个浮点数表示换算成十进制， 实际准确的值是9.09999942779541015625。相信你现在应该不会感觉奇怪了。
我在这里放一个链接，这里提供了直接交互式地设置符号位、指数位和有效位数的操作。你可以直观地看到，32位浮点数每一个bit的变化，对应的有效位数、指数会变成什么样子以及最后的十进制的计算结果是怎样的。
这个也解释了为什么，在上一讲一开始，0.3+0.6=0.899999。因为0.3转化成浮点数之后，和这里的9.1一样，并不是精确的0.3了，0.6和0.9也是一样的，最后的计算会出现精度问题。
浮点数的加法和精度损失搞清楚了怎么把一个十进制的数值，转化成IEEE-754标准下的浮点数表示，我们现在来看一看浮点数的加法是怎么进行的。其实原理也很简单，你记住六个字就行了，那就是先对齐、再计算。
两个浮点数的指数位可能是不一样的，所以我们要把两个的指数位，变成一样的，然后只去计算有效位的加法就好了。
比如0.5，表示成浮点数，对应的指数位是-1，有效位是00…（后面全是0，记住f前默认有一个1）。0.125表示成浮点数，对应的指数位是-3，有效位也还是00…（后面全是0，记住f前默认有一个1）。
那我们在计算0.5+0.125的浮点数运算的时候，首先要把两个的指数位对齐，也就是把指数位都统一成两个其中较大的-1。对应的有效位1.00…也要对应右移两位，因为f前面有一个默认的1，所以就会变成0.01。然后我们计算两者相加的有效位1.f，就变成了有效位1.01，而指数位是-1，这样就得到了我们想要的加法后的结果。
实现这样一个加法，也只需要位移。和整数加法类似的半加器和全加器的方法就能够实现，在电路层面，也并没有引入太多新的复杂性。
同样的，你可以用刚才那个链接来试试看，我们这个加法计算的浮点数的结果是不是正确。
回到浮点数的加法过程，你会发现，其中指数位较小的数，需要在有效位进行右移，在右移的过程中，最右侧的有效位就被丢弃掉了。这会导致对应的指数位较小的数，在加法发生之前，就丢失精度。两个相加数的指数位差的越大，位移的位数越大，可能丢失的精度也就越大。当然，也有可能你的运气非常好，右移丢失的有效位都是0。这种情况下，对应的加法虽然丢失了需要加的数字的精度，但是因为对应的值都是0，实际的加法的数值结果不会有精度损失。
32位浮点数的有效位长度一共只有23位，如果两个数的指数位差出23位，较小的数右移24位之后，所有的有效位就都丢失了。这也就意味着，虽然浮点数可以表示上到$3.40×10^{38}$，下到$1.17×10^{-38}$这样的数值范围。但是在实际计算的时候，只要两个数，差出$2^{24}$，也就是差不多1600万倍，那这两个数相加之后，结果完全不会变化。
你可以试一下，我下面用一个简单的Java程序，让一个值为2000万的32位浮点数和1相加，你会发现，+1这个过程因为精度损失，被“完全抛弃”了。
public class FloatPrecision { public static void main(String[] args) { float a = 20000000.0f; float b = 1.0f; float c = a + b; System.out.println(&amp;quot;c is &amp;quot; + c); float d = c - a; System.</description></item><item><title>16_游标：对于数据集中的记录，该怎么逐条处理？</title><link>https://artisanbox.github.io/8/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/16/</guid><description>你好，我是朱晓峰。今天，我来和你聊一聊游标。
咱们前面学习的MySQL数据操作语句，都是针对结果集合的。也就是说，每次处理的对象都是一个数据集合。如果需要逐一处理结果集中的记录，就会非常困难。
虽然我们也可以通过筛选条件WHERE和HAVING，或者是限定返回记录的关键字LIMIT返回一条记录，但是，却无法在结果集中像指针一样，向前定位一条记录、向后定位一条记录，或者是随意定位到某一条记录，并对记录的数据进行处理。
这个时候，就可以用到游标。所谓的游标，也就是能够对结果集中的每一条记录进行定位，并对指向的记录中的数据进行操作的数据结构。
这么说可能有点抽象，我举一个生活中的例子，你一看就明白了。比如，你想去某个城市旅游，现在需要订酒店。你打开预订酒店的App，设置好价格区间后进行搜索，得到了一个酒店列表。接下来，你可能要逐条查看列表中每个酒店的客户评价，最后选择一个口碑不错的酒店。这个逐条搜索并对选中的数据进行操作的过程，就相当于游标对数据记录进行操作的过程。
今天我就来给你讲一讲游标的使用方法，同时还会通过一个案例串讲，帮助你更好地使用游标，让你能够轻松地处理数据集中的记录。
游标的使用步骤游标只能在存储程序内使用，存储程序包括存储过程和存储函数。关于存储过程，我们上节课刚刚学过，这里我简单介绍一下存储函数。创建存储函数的语法是：
CREATE FUNCTION 函数名称 （参数）RETURNS 数据类型 程序体 存储函数与存储过程很像，但有几个不同点：
存储函数必须返回一个值或者数据表，存储过程可以不返回。 存储过程可以通过CALL语句调用，存储函数不可以。 存储函数可以放在查询语句中使用，存储过程不行。 存储过程的功能更加强大，包括能够执行对表的操作（比如创建表，删除表等）和事务操作，这些功能是存储函数不具备的。 这节课，我们主要学习下游标在存储过程中的使用方法，因为游标在存储过程中更常用。游标在存储函数中的使用方法和在存储过程中的使用方法是一样的。
在使用游标的时候，主要有4个步骤。
第一步，定义游标。语法结构如下：
DECLARE 游标名 CURSOR FOR 查询语句 这里就是声明一个游标，它可以操作的数据集是“查询语句”返回的结果集。
第二步，打开游标。语法结构如下：
OPEN 游标名称； 打开游标之后，系统会为游标准备好查询的结果集，为后面游标的逐条读取结果集中的记录做准备。
第三步，从游标的数据结果集中读取数据。语法结构是这样的：
FETCH 游标名 INTO 变量列表； 这里的意思是通过游标，把当前游标指向的结果集中那一条记录的数据，赋值给列表中的变量。
需要注意的是，游标的查询结果集中的字段数，必须跟INTO后面的变量数一致，否则，在存储过程执行的时候，MySQL会提示错误。
第四步，关闭游标。语法结构如下：
CLOSE 游标名； 用完游标之后，你一定要记住及时关闭游标。因为游标会占用系统资源，如果不及时关闭，游标会一直保持到存储过程结束，影响系统运行的效率。而关闭游标的操作，会释放游标占用的系统资源。
知道了基本步骤，下面我就结合超市项目的实际案例，带你实战一下。
案例串讲在超市项目的进货模块中，有一项功能是对进货单数据进行验收。其实就是在对进货单的数据确认无误后，对进货单的数据进行处理，包括增加进货商品的库存，并修改商品的平均进价。下面我用实际数据来演示一下这个操作流程。
这里我们要用到进货单头表（demo.importheadl）、进货单明细表（demo.importdetails）、库存表（demo.inventory）和商品信息表（demo.goodsmaster）。
进货单头表：
进货单明细表：
库存表：
商品信息表：
要验收进货单，我们就需要对每一个进货商品进行两个操作：
在现有库存数量的基础上，加上本次进货的数量； 根据本次进货的价格、数量，现有商品的平均进价和库存，计算新的平均进价：（本次进货价格 * 本次进货数量+现有商品平均进价 * 现有商品库存）/（本次进货数量+现有库存数量）。 针对这个操作，如果只用我们在第4讲里学习的SQL语句，完成起来就比较困难。
因为我们需要通过应用程序来控制操作流程，做成一个循环操作，每次只查询一种商品的数据记录并进行处理，一直到把进货单中的数据全部处理完。这样一来，应用必须发送很多的SQL指令到服务器，跟服务器的交互多，不仅代码复杂，而且也不够安全。
这个时候，如果使用游标，就很容易了。因为所有的操作都可以在服务器端完成，应用程序只需要发送一个命令调用存储过程就可以了。现在，我们就来看看如何用游标来解决这个问题。
我用代码创建了一个存储过程demo.mytest（）。当然，你也完全可以在Workbench中创建存储过程，非常简单，我就不多说了。创建存储过程的代码如下：
mysql&amp;gt; DELIMITER // mysql&amp;gt; CREATE PROCEDURE demo.mytest(mylistnumber INT) -&amp;gt; BEGIN -&amp;gt; DECLARE mystockid INT; -&amp;gt; DECLARE myitemnumber INT; -&amp;gt; DECLARE myquantity DECIMAL(10,3); -&amp;gt; DECLARE myprice DECIMAL(10,2); -&amp;gt; DECLARE done INT DEFAULT FALSE; -- 用来控制循环结束 -&amp;gt; DECLARE cursor_importdata CURSOR FOR -- 定义游标 -&amp;gt; SELECT b.</description></item><item><title>16｜生成本地代码第1关：先把基础搭好</title><link>https://artisanbox.github.io/3/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/18/</guid><description>你好，我是宫文学。
到目前为止，我们已经初步了解了CPU架构和X86汇编代码的相关知识点，为我们接下来的让编译器生成汇编代码的工作打下了不错的基础。
不过，我总是相信最好的学习方法就是实践。因为，只有你自己动手尝试过用编译器生成汇编代码，你才会对CPU架构和汇编的知识有更深刻的了解，总之，遇到问题，解决问题就好了。
所以呢，今天这节课，我们就开始着手生成X86汇编代码。我会带你分析生成汇编代码的算法思路，理解寄存器机与栈机在生成代码上的差别，以及了解如何在内存里表示汇编代码。
看上去工作有点多，不着急，我们一步步来。那首先，我们就通过一个实例，让你对生成汇编代码的算法思路有一个直觉上的认知。
生成汇编代码的算法思路在前面的课里，我们已经学会了如何生成字节码。你也知道了，基本上，我们只需要通过遍历AST就能生成栈机的字节码。
但当时我们也说过，为栈机生成代码是比较简单的，比寄存器机要简单，这具体是为什么呢？
你可以保留这个疑问，先来跟我分析一个例子，看看我们要怎么把它转化成汇编代码，以及在这个过程中会遇到什么问题：
function foo(a:number, b:number):number{ let c = a+b+10; return a+c; } 你可以看到，这个函数有两个参数。为了提高程序的运行效率，在参数数量不多的情况下，参数通常都是通过寄存器传递的，我们暂且把传递这两个参数的两个寄存器叫做r1和r2。
接下来，我们要执行运算，也就是a+b+10。这该怎么做呢？这里你要注意的是，在生成指令的时候，我们通常不能直接把b的值加到a上，也就是从r2加到r1上。因为这样就破坏了r1原来的值，而这个值后面的代码有可能用到。
所以呢，我们这里就要生成两条指令。第一条指令，是把a的值从r1拷贝到一个新的寄存器r3；第2条指令，是把b的值从r2加到r3上，最终结果也就保存到了r3。
mov r1, r3 add r2, r3 之后，我们要再加10。这个时候，我们可以放心地把10加到r3上。因为r3是我们自己生成的一个临时变量，我们可以确保其他代码不会用到它，所以我们可以放心地改变它的值。
add 10, r3 接着，我们要把表达式a+b+10的值赋给本地变量c。对于本地变量，我们也是尽可能地把它放到寄存器里。不过呢，由于r3作为临时变量的任务已经圆满完成了，所以这个时候，我们可以用r3来表示c的值，这下我们就节省了一个寄存器。因此，这里我们不需要添加任何新的指令。
再接着，我们要计算a+c的值。为了不影响已有寄存器的值，我们又使用了一个新的寄存器r4，用来保存计算结果：
mov r1, r4 add r3, r4 最后，我们要把计算结果返回。通常，根据调用约定，返回值也是放在某个寄存器里的。我们这里假设这个寄存器是r0。所以，我们可以用这两条指令返回：
mov r4, r0 ret 到此为止，我们就已经成功地为foo函数生成了汇编代码。当然，这个汇编代码只是示意性的、逻辑性的，如果要让它成为真正可用的汇编代码，还要做一些调整，比如把寄存器的名称换成正式的物理寄存器的名称，如rax等。我这样叙述，是为了尽量保持简洁，避免你过早陷入到具体CPU架构的细节中去，增加认知负担。
好了，回顾我们的工作成果，你可能很快会发现一个问题：这么小的一个程序就占据了4个寄存器，如果我再多加点参数或者本地变量，那寄存器岂不会很快就会被用光？
这个担心是很有道理的，你可以先看看下面的例子：
function foo(a:number, b:number, d:number, f:number):number{ let c = a+b+10; let g = ... let h = ... return g+h; } 在这个例子中，参数a、b、d、f和本地变量c、g、h，都会额外占用一个寄存器，并且每个变量的计算过程都有可能消耗额外的寄存器来保存临时变量，所以寄存器很快就会被用光。
到这里，你可能已经体会到了为什么给寄存器机生成代码会更难了。在栈机里，根本没有这样的问题，因为我们可以用操作数栈来保存中间结果，而操作数栈的大小是没有限制的。</description></item><item><title>17_First和Follow集合：用LL算法推演一个实例</title><link>https://artisanbox.github.io/6/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/17/</guid><description>在前面的课程中，我讲了递归下降算法。这个算法很常用，但会有回溯的现象，在性能上会有损失。所以我们要把算法升级一下，实现带有预测能力的自顶向下分析算法，避免回溯。而要做到这一点，就需要对自顶向下算法有更全面的了解。
另外，在留言区，有几个同学问到了一些问题，涉及到对一些基本知识点的理解，比如：
基于某个语法规则做解析的时候，什么情况下算是成功，什么情况下算是失败？ 使用深度优先的递归下降算法时，会跟广度优先的思路搞混。 要搞清这些问题，也需要全面了解自顶向下算法。比如，了解Follow集合和$符号的用法，能帮你解决第一个问题；了解广度优先算法能帮你解决第二个问题。
所以，本节课，我先把自顶向下分析的算法体系梳理一下，让你先建立更加清晰的全景图，然后我再深入剖析LL算法的原理，讲清楚First集合与Follow集合这对核心概念，最终让你把自顶向下的算法体系吃透。
自顶向下分析算法概述自顶向下分析的算法是一大类算法。总体来说，它是从一个非终结符出发，逐步推导出跟被解析的程序相同的Token串。
这个过程可以看做是一张图的搜索过程，这张图非常大，因为针对每一次推导，都可能产生一个新节点。下面这张图只是它的一个小角落。
算法的任务，就是在大图中，找到一条路径，能产生某个句子（Token串）。比如，我们找到了三条橘色的路径，都能产生“2+3*5”这个表达式。
根据搜索的策略，有深度优先（Depth First）和广度优先（Breadth First）两种，这两种策略的推导过程是不同的。
深度优先是沿着一条分支把所有可能性探索完。以“add-&amp;gt;mul+add”产生式为例，它会先把mul这个非终结符展开，比如替换成pri，然后再把它的第一个非终结符pri展开。只有把这条分支都向下展开之后，才会回到上一级节点，去展开它的兄弟节点。
递归下降算法就是深度优先的，这也是它不能处理左递归的原因，因为左边的分支永远也不能展开完毕。
而针对“add-&amp;gt;add+mul”这个产生式，广度优先会把add和mul这两个都先展开，这样就形成了四条搜索路径，分别是mul+mul、add+mul+mul、add+pri和add+mul*pri。接着，把它们的每个非终结符再一次展开，会形成18条新的搜索路径。
所以，广度优先遍历，需要探索的路径数量会迅速爆炸，成指数级上升。哪怕用下面这个最简单的语法，去匹配“2+3”表达式，都需要尝试20多次，更别提针对更复杂的表达式或者采用更加复杂的语法规则了。
//一个很简单的语法 add -&amp;gt; pri //1 add -&amp;gt; add + pri //2 pri -&amp;gt; Int //3 pri -&amp;gt; (add) //4 这样看来，指数级上升的内存消耗和计算量，使得广度优先根本没有实用价值。虽然上面的算法有优化空间，但无法从根本上降低算法复杂度。当然了，它也有可以使用左递归文法的优点，不过我们不会为了这个优点去忍受算法的性能。
而深度优先算法在内存占用上是线性增长的。考虑到回溯的情况，在最坏的情况下，它的计算量也会指数式增长，但我们可以通过优化，让复杂度降为线性增长。
了解广度优先算法，你的思路会得到拓展，对自顶向下算法的本质有更全面的理解。另外，在写算法时，你也不会一会儿用深度优先，一会儿用广度优先了。
针对深度优先算法的优化方向是减少甚至避免回溯，思路就是给算法加上预测能力。比如，我在解析statement的时候，看到一个if，就知道肯定这是一个条件语句，不用再去尝试其他产生式了。
LL算法就属于这类预测性的算法。第一个L，是Left-to-right，代表从左向右处理程序代码。第二个L，是Leftmost，意思是最左推导。
按照语法规则，一个非终结符展开后，会形成多个子节点，其中包含终结符和非终结符。最左推导是指，从左到右依次推导展开这些非终结符。采用Leftmost的方法，在推导过程中，句子的左边逐步都会被替换成终结符，只有右边的才可能包含非终结符。
以“2+3*5”为例，它的推导顺序从左到右，非终结符逐步替换成了终结符：
下图是上述推导过程建立起来的AST，“1、2、3……”等编号是AST节点创建的顺序：
好了，我们把自顶向下分析算法做了总体概述，并讲清楚了最左推导的含义，现在来看看LL算法到底是怎么回事。
计算和使用First集合LL算法是带有预测能力的自顶向下算法。在推导的时候，我们希望当存在多个候选的产生式时，瞄一眼下一个（或多个）Token，就知道采用哪个产生式。如果只需要预看一个Token，就是LL(1)算法。
拿statement的语法举例子，它有好几个产生式，分别产生if语句、while语句、switch语句……
statement : block | IF parExpression statement (ELSE statement)? | FOR '(' forControl ')' statement | WHILE parExpression statement | DO statement WHILE parExpression ';' | SWITCH parExpression '{' switchBlockStatementGroup* switchLabel* | RETURN expression?</description></item><item><title>17_Python编译器（一）：如何用工具生成编译器？</title><link>https://artisanbox.github.io/7/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/17/</guid><description>你好，我是宫文学。
最近几年，Python在中国变得越来越流行，我想可能有几个推动力：第一个是因为人工智能热的兴起，用Python可以很方便地使用流行的AI框架，比如TensorFlow；第二个重要的因素是编程教育，特别是很多面对青少年的编程课程，都是采用的Python语言。
不过，Python之所以变得如此受欢迎，虽然有外在的机遇，但也得益于它内在的一些优点。比如说：
Python的语法比较简单，容易掌握，它强调一件事情只能用一种方法去做。对于老一代的程序员来说，Python就像久远的BASIC语言，很适合作为初学者的第一门计算机语言去学习，去打开计算机编程这个充满魅力的世界。 Python具备丰富的现代语言特性，实现方式又比较简洁。比如，它既支持面向对象特性，也支持函数式编程特性，等等。这对于学习编程很有好处，能够带给初学者比较准确的编程概念。 我个人比较欣赏Python的一个原因，是它能够充分利用开源世界的一些二进制的库，比如说，如果你想研究计算机视觉和多媒体，可以用它调用OpenCV和FFmpeg。Python跟AI框架的整合也是同样的道理，这也是Python经常用于系统运维领域的原因，因为它很容易调用操作系统的一些库。 最后，Python还有便于扩展的优势。如果你觉得Python有哪方面能力的不足，你也可以用C语言来写一些扩展。而且，你不仅仅可以扩展出几个函数，你还能扩展出新的类型，并在Python里使用这些新类型。比如，Python的数学计算库是NumPy，它的核心代码是用C语言编写的，性能很高。 看到这里，你自然会好奇，这么一门简洁有力的语言，是如何实现的呢？吉多·范罗苏姆（Python初始设计者）在编写Python的编译器的时候，脑子里是怎么想的呢？
从这一讲开始，我们就进入到Python语言的编译器内部，去看看它作为一门动态、解释执行语言的代表，是如何做词法分析、语法分析和语义分析的，又是如何解释执行的，以及它的运行时有什么设计特点，让它可以具备这些优势。你在这个过程中，也会对编译技术的应用场景了解得更加全面。这也正是我要花3讲的时间，带领你来解析Python编译器的主要原因。
今天这一讲，我们重点来研究Python的词法分析和语法分析功能，一起来看看它在这两个处理阶段都有什么特点。你会学到一种新的语法分析实现思路，还能够学到CST跟AST的区别。
好了，让我们开始吧。
编译源代码，并跟踪调试首先，你可以从python.org网站下载3.8.1版本的源代码。解压后你可以先自己浏览一下，看看能不能找到它的词法分析器、语法分析器、符号表处理程序、解释器等功能的代码。
Python源代码划分了多个子目录，每个子目录的内容整理如下：
首先，你会发现Python编译器是用C语言编写的。这跟Java、Go的编译器不同，Java和Go语言的编译器是支持自举的编译器，也就是这两门语言的编译器是用这两门语言自身实现的。
实际上，用C语言实现的Python编译器叫做CPython，是Python的几个编译器之一。它的标准库也是由C语言和Python混合编写的。我们课程中所讨论的就是CPython，它是Python语言的参考实现，也是macOS和Linux缺省安装的版本。
不过，Python也有一个编译器是用Python本身编写的，这个编译器是PyPy。它的图标是一条咬着自己尾巴的衔尾蛇，表明这个编译器是自举的。除此之外，还有基于JVM的Jython，这个版本的优势是能够借助成熟的JVM生态，比如可以不用自己写垃圾收集器，还能够调用丰富的Java类库。如果你觉得理解C语言的代码比较困难，你也可以去看看这两个版本的实现。
在Python的“开发者指南”网站上，有不少关于Python内部实现机制的技术资料。请注意，这里的开发者，指的是有兴趣参与Python语言开发的程序员，而不是Python语言的使用者。这就是像Python这种开源项目的优点，它欢迎热爱Python的程序员来修改和增强Python语言，甚至你还可以增加一些自己喜欢的语言特性。
根据开发者指南的指引，你可以编译一下Python的源代码。注意，你要用调试模式来编译，因为接下来我们要跟踪Python编译器的运行过程。这就要使用调试工具GDB。
GDB是GNU的调试工具，做C语言开发的人一般都会使用这个工具。它支持通过命令行调试程序，包括设置断点、单步跟踪、观察变量的值等，这跟你在IDE里调试程序的操作很相似。
开发者指南中有如何用调试模式编译Python，并如何跟GDB配合使用的信息。实际上，GDB现在可以用Python来编写扩展，从而给我们带来更多的便利。比如，我们在调试Python编译器的时候，遇到Python对象的指针（PyObject*），就可以用更友好的方式来显示Python对象的信息。
好了，接下来我们就通过跟踪Python编译器执行过程，看看它在编译过程中都涉及了哪些主要的程序模块。
在tokenizer.c的tok_get()函数中打一个断点，通过GDB观察Python的运行，你会发现下面的调用顺序（用bt命令打印输出后整理的结果）：
这个过程是运行Python并执行到词法分析环节，你可以看到完整的程序执行路径：
首先是python.c，这个文件很短，只是提供了一个main()函数。你运行python命令的时候，就会先进入这里。 接着进入Modules/main.c文件，这个文件里提供了运行环境的初始化等功能，它能执行一个python文件，也能启动REPL提供一个交互式界面。 之后是Python/pythonrun.c文件，这是Python的解释器，它调用词法分析器、语法分析器和字节码生成功能，最后解释执行。 再之后来到Parser目录的parsetok.c文件，这个文件会调度词法分析器和语法分析器，完成语法分析过程，最后生成AST。 最后是toknizer.c，它是词法分析器的具体实现。 拓展：REPL是Read-Evaluate-Print-Loop的缩写，也就是通过一个交互界面接受输入并回显结果。
通过上述的跟踪过程，我们就进入了Python的词法分析功能。下面我们就来看一下它是怎么实现的，再一次对词法分析的原理做一下印证。
Python的词法分析功能首先，你可以看一下tokenizer.c的tok_get()函数。你一阅读源代码，就会发现，这是我们很熟悉的一个结构，它也是通过有限自动机把字符串变成Token。
你还可以用另一种更直接的方法来查看Python词法分析的结果。
./python.exe -m tokenize -e foo.py 补充：其中的python.exe指的是Python的可执行文件，如果是在Linux系统，可执行文件是python。
运行上面的命令会输出所解析出的Token：
其中的第二列是Token的类型，第三列是Token对应的字符串。各种Token类型的定义，你可以在Grammar/Tokens文件中找到。
我们曾在研究Java编译器的时候，探讨过如何解决关键字和标识符的词法规则冲突的问题。那么Python是怎么实现的呢？
原来，Python在词法分析阶段根本没有区分这两者，只是都是作为“NAME”类型的Token来对待。
补充：Python里面有两个词法分析器，一个是用C语言实现的（tokenizer.c），一个是用Python实现的（tokenizer.py）。C语言版本的词法分析器由编译器使用，性能更高。
所以，Python的词法分析功能也比较常规。其实你会发现，每个编译器的词法分析功能都大同小异，你完全可以借鉴一个比较成熟的实现。Python跟Java的编译器稍微不同的一点，就是没有区分关键字和标识符。
接下来，我们来关注下这节课的重点内容：语法分析功能。
Python的语法分析功能在GDB中继续跟踪执行过程，你会在parser.c中找到语法分析的相关逻辑：
那么，Python的语法分析有什么特点呢？它采用的是什么算法呢？是自顶向下的算法，还是自底向上的算法？
首先，我们到Grammar目录，去看一下Grammar文件。这是一个用EBNF语法编写的Python语法规则文件，下面是从中节选的几句，你看是不是很容易读懂呢？
//声明函数 funcdef: 'def' NAME parameters ['-&amp;gt;' test] ':' [TYPE_COMMENT] func_body_suite //语句 simple_stmt: small_stmt (';' small_stmt)* [';'] NEWLINE small_stmt: (expr_stmt | del_stmt | pass_stmt | flow_stmt | import_stmt | global_stmt | nonlocal_stmt | assert_stmt) 通过阅读规则文件，你可以精确地了解Python的语法规则。</description></item><item><title>17_划分土地（中）：如何实现内存页面初始化？</title><link>https://artisanbox.github.io/9/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/17/</guid><description>你好，我是LMOS。
上节课，我们确定了用分页方式管理内存，并且一起动手设计了表示内存页、内存区相关的内存管理数据结构。不过，虽然内存管理相关的数据结构已经定义好了，但是我们还没有在内存中建立对应的实例变量。
我们都知道，在代码中实际操作的数据结构必须在内存中有相应的变量，这节课我们就去建立对应的实例变量，并初始化它们。
初始化前面的课里，我们在hal层初始化中，初始化了从二级引导器中获取的内存布局信息，也就是那个e820map_t数组，并把这个数组转换成了phymmarge_t结构数组，还对它做了排序。
但是，我们Cosmos物理内存管理器剩下的部分还没有完成初始化，下面我们就去实现它。
Cosmos的物理内存管理器，我们依然要放在Cosmos的hal层。
因为物理内存还和硬件平台相关，所以我们要在cosmos/hal/x86/目录下建立一个memmgrinit.c文件，在这个文件中写入一个Cosmos物理内存管理器初始化的大总管——init_memmgr函数，并在init_halmm函数中调用它，代码如下所示。
//cosmos/hal/x86/halmm.c中 //hal层的内存初始化函数 void init_halmm() { init_phymmarge(); init_memmgr(); return; } //Cosmos物理内存管理器初始化 void init_memmgr() { //初始化内存页结构msadsc_t //初始化内存区结构memarea_t return; } 根据前面我们对内存管理相关数据结构的设计，你应该不难想到，在init_memmgr函数中应该要完成内存页结构msadsc_t和内存区结构memarea_t的初始化，下面就分别搞定这两件事。
内存页结构初始化内存页结构的初始化，其实就是初始化msadsc_t结构对应的变量。因为一个msadsc_t结构体变量代表一个物理内存页，而物理内存由多个页组成，所以最终会形成一个msadsc_t结构体数组。
这会让我们的工作变得简单，我们只需要找一个内存地址，作为msadsc_t结构体数组的开始地址，当然这个内存地址必须是可用的，而且之后内存空间足以存放msadsc_t结构体数组。
然后，我们要扫描phymmarge_t结构体数组中的信息，只要它的类型是可用内存，就建立一个msadsc_t结构体，并把其中的开始地址作为第一个页面地址。
接着，要给这个开始地址加上0x1000，如此循环，直到其结束地址。
当这个phymmarge_t结构体的地址区间，它对应的所有msadsc_t结构体都建立完成之后，就开始下一个phymmarge_t结构体。依次类推，最后，我们就能建好所有可用物理内存页面对应的msadsc_t结构体。
下面，我们去cosmos/hal/x86/目录下建立一个msadsc.c文件。在这里写下完成这些功能的代码，如下所示。
void write_one_msadsc(msadsc_t *msap, u64_t phyadr) { //对msadsc_t结构做基本的初始化，比如链表、锁、标志位 msadsc_t_init(msap); //这是把一个64位的变量地址转换成phyadrflgs_t*类型方便取得其中的地址位段 phyadrflgs_t *tmp = (phyadrflgs_t *)(&amp;amp;phyadr); //把页的物理地址写入到msadsc_t结构中 msap-&amp;gt;md_phyadrs.paf_padrs = tmp-&amp;gt;paf_padrs; return; } u64_t init_msadsc_core(machbstart_t *mbsp, msadsc_t *msavstart, u64_t msanr) { //获取phymmarge_t结构数组开始地址 phymmarge_t *pmagep = (phymmarge_t *)phyadr_to_viradr((adr_t)mbsp-&amp;gt;mb_e820expadr); u64_t mdindx = 0; //扫描phymmarge_t结构数组 for (u64_t i = 0; i &amp;lt; mbsp-&amp;gt;mb_e820exnr; i++) { //判断phymmarge_t结构的类型是不是可用内存 if (PMR_T_OSAPUSERRAM == pmagep[i].</description></item><item><title>17_如何正确地显示随机消息？</title><link>https://artisanbox.github.io/1/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/17/</guid><description>我在上一篇文章，为你讲解完order by语句的几种执行模式后，就想到了之前一个做英语学习App的朋友碰到过的一个性能问题。今天这篇文章，我就从这个性能问题说起，和你说说MySQL中的另外一种排序需求，希望能够加深你对MySQL排序逻辑的理解。
这个英语学习App首页有一个随机显示单词的功能，也就是根据每个用户的级别有一个单词表，然后这个用户每次访问首页的时候，都会随机滚动显示三个单词。他们发现随着单词表变大，选单词这个逻辑变得越来越慢，甚至影响到了首页的打开速度。
现在，如果让你来设计这个SQL语句，你会怎么写呢？
为了便于理解，我对这个例子进行了简化：去掉每个级别的用户都有一个对应的单词表这个逻辑，直接就是从一个单词表中随机选出三个单词。这个表的建表语句和初始数据的命令如下：
mysql&amp;gt; CREATE TABLE `words` ( `id` int(11) NOT NULL AUTO_INCREMENT, `word` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; delimiter ;; create procedure idata() begin declare i int; set i=0; while i&amp;lt;10000 do insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10)))); set i=i+1; end while; end;; delimiter ;
call idata(); 为了便于量化说明，我在这个表里面插入了10000行记录。接下来，我们就一起看看要随机选择3个单词，有什么方法实现，存在什么问题以及如何改进。
内存临时表首先，你会想到用order by rand()来实现这个逻辑。</description></item><item><title>17_建立数据通路（上）：指令+运算=CPU</title><link>https://artisanbox.github.io/4/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/17/</guid><description>前面几讲里，我从两个不同的部分为你讲解了CPU的功能。
在“指令”部分，我为你讲解了计算机的“指令”是怎么运行的，也就是我们撰写的代码，是怎么变成一条条的机器能够理解的指令的，以及是按照什么样的顺序运行的。
在“计算”部分，我为你讲解了计算机的“计算”部分是怎么执行的，数据的二进制表示是怎么样的，我们执行的加法和乘法又是通过什么样的电路来实现的。
然而，光知道这两部分还不能算是真正揭开了CPU的秘密，只有把“指令”和“计算”这两部分功能连通起来，我们才能构成一个真正完整的CPU。这一讲，我们就在前面知识的基础上，来看一个完整的CPU是怎么运转起来的。
指令周期（Instruction Cycle）前面讲计算机机器码的时候，我向你介绍过PC寄存器、指令寄存器，还介绍过MIPS体系结构的计算机所用到的R、I、J类指令。如果我们仔细看一看，可以发现，计算机每执行一条指令的过程，可以分解成这样几个步骤。
1.Fetch（取得指令），也就是从PC寄存器里找到对应的指令地址，根据指令地址从内存里把具体的指令，加载到指令寄存器中，然后把PC寄存器自增，好在未来执行下一条指令。
2.Decode（指令译码），也就是根据指令寄存器里面的指令，解析成要进行什么样的操作，是R、I、J中的哪一种指令，具体要操作哪些寄存器、数据或者内存地址。
3.Execute（执行指令），也就是实际运行对应的R、I、J这些特定的指令，进行算术逻辑操作、数据传输或者直接的地址跳转。
4.重复进行1～3的步骤。
这样的步骤，其实就是一个永不停歇的“Fetch - Decode - Execute”的循环，我们把这个循环称之为指令周期（Instruction Cycle）。
指令周期（Instruction Cycle）在这个循环过程中，不同部分其实是由计算机中的不同组件完成的。不知道你还记不记得，我们在专栏一开始讲的计算机组成的五大组件？
在取指令的阶段，我们的指令是放在存储器里的，实际上，通过PC寄存器和指令寄存器取出指令的过程，是由控制器（Control Unit）操作的。指令的解码过程，也是由控制器进行的。一旦到了执行指令阶段，无论是进行算术操作、逻辑操作的R型指令，还是进行数据传输、条件分支的I型指令，都是由算术逻辑单元（ALU）操作的，也就是由运算器处理的。不过，如果是一个简单的无条件地址跳转，那么我们可以直接在控制器里面完成，不需要用到运算器。
不同步骤在不同组件之内完成除了Instruction Cycle这个指令周期，在CPU里面我们还会提到另外两个常见的Cycle。一个叫Machine Cycle，机器周期或者CPU周期。CPU内部的操作速度很快，但是访问内存的速度却要慢很多。每一条指令都需要从内存里面加载而来，所以我们一般把从内存里面读取一条指令的最短时间，称为CPU周期。
还有一个是我们之前提过的Clock Cycle，也就是时钟周期以及我们机器的主频。一个CPU周期，通常会由几个时钟周期累积起来。一个CPU周期的时间，就是这几个Clock Cycle的总和。
对于一个指令周期来说，我们取出一条指令，然后执行它，至少需要两个CPU周期。取出指令至少需要一个CPU周期，执行至少也需要一个CPU周期，复杂的指令则需要更多的CPU周期。
三个周期（Cycle）之间的关系所以，我们说一个指令周期，包含多个CPU周期，而一个CPU周期包含多个时钟周期。
建立数据通路在专栏一开始，不少同学留言问到，ALU就是运算器吗？在讨论计算机五大组件的运算器的时候，我们提到过好几个不同的相关名词，比如ALU、运算器、处理器单元、数据通路，它们之间到底是什么关系呢？
名字是什么其实并不重要，一般来说，我们可以认为，数据通路就是我们的处理器单元。它通常由两类原件组成。
第一类叫操作元件，也叫组合逻辑元件（Combinational Element），其实就是我们的ALU。在前面讲ALU的过程中可以看到，它们的功能就是在特定的输入下，根据下面的组合电路的逻辑，生成特定的输出。
第二类叫存储元件，也有叫状态元件（State Element）的。比如我们在计算过程中需要用到的寄存器，无论是通用寄存器还是状态寄存器，其实都是存储元件。
我们通过数据总线的方式，把它们连接起来，就可以完成数据的存储、处理和传输了，这就是所谓的建立数据通路了。
下面我们来说控制器。它的逻辑就没那么复杂了。我们可以把它看成只是机械地重复“Fetch - Decode - Execute“循环中的前两个步骤，然后把最后一个步骤，通过控制器产生的控制信号，交给ALU去处理。
听起来是不是很简单？实际上，控制器的电路特别复杂。下面我给你详细解析一下。
一方面，所有CPU支持的指令，都会在控制器里面，被解析成不同的输出信号。我们之前说过，现在的Intel CPU支持2000个以上的指令。这意味着，控制器输出的控制信号，至少有2000种不同的组合。
运算器里的ALU和各种组合逻辑电路，可以认为是一个固定功能的电路。控制器“翻译”出来的，就是不同的控制信号。这些控制信号，告诉ALU去做不同的计算。可以说正是控制器的存在，让我们可以“编程”来实现功能，能让我们的“存储程序型计算机”名副其实。
指令译码器将输入的机器码，解析成不同的操作码和操作数，然后传输给ALU进行计算CPU所需要的硬件电路那么，要想搭建出来整个CPU，我们需要在数字电路层面，实现这样一些功能。
首先，自然是我们之前已经讲解过的ALU了，它实际就是一个没有状态的，根据输入计算输出结果的第一个电路。
第二，我们需要有一个能够进行状态读写的电路元件，也就是我们的寄存器。我们需要有一个电路，能够存储到上一次的计算结果。这个计算结果并不一定要立刻拿到电路的下游去使用，但是可以在需要的时候拿出来用。常见的能够进行状态读写的电路，就有锁存器（Latch），以及我们后面要讲的D触发器（Data/Delay Flip-flop）的电路。
第三，我们需要有一个“自动”的电路，按照固定的周期，不停地实现PC寄存器自增，自动地去执行“Fetch - Decode - Execute“的步骤。我们的程序执行，并不是靠人去拨动开关来执行指令的。我们希望有一个“自动”的电路，不停地去一条条执行指令。
我们看似写了各种复杂的高级程序进行各种函数调用、条件跳转。其实只是修改PC寄存器里面的地址。PC寄存器里面的地址一修改，计算机就可以加载一条指令新指令，往下运行。实际上，PC寄存器还有一个名字，就叫作程序计数器。顾名思义，就是随着时间变化，不断去数数。数的数字变大了，就去执行一条新指令。所以，我们需要的就是一个自动数数的电路。
第四，我们需要有一个“译码”的电路。无论是对于指令进行decode，还是对于拿到的内存地址去获取对应的数据或者指令，我们都需要通过一个电路找到对应的数据。这个对应的自然就是“译码器”的电路了。
好了，现在我们把这四类电路，通过各种方式组合在一起，就能最终组成功能强大的CPU了。但是，要实现这四种电路中的中间两种，我们还需要时钟电路的配合。下一节，我们一起来看一看，这些基础的电路功能是怎么实现的，以及怎么把这些电路组合起来变成一个CPU。
总结延伸好了，到这里，我们已经把CPU运转需要的数据通路和控制器介绍完了，也找出了需要完成这些功能，需要的4种基本电路。它们分别是，ALU这样的组合逻辑电路、用来存储数据的锁存器和D触发器电路、用来实现PC寄存器的计数器电路，以及用来解码和寻址的译码器电路。
虽然CPU已经是由几十亿个晶体管组成的及其复杂的电路，但是它仍然是由这样一个个基本功能的电路组成的。只要搞清楚这些电路的运作原理，你自然也就弄明白了CPU的工作原理。
推荐阅读如果想要了解数据通路，可以参看《计算机组成与设计 硬件软件接口》的第5版的4.1到4.4节。专栏里的内容是从更高一层的抽象逻辑来解释这些问题，而教科书里包含了更多电路的技术细节。这两者结合起来学习，能够帮助你更深入地去理解数据通路。
课后思考这一讲，我们说CPU好像一个永不停歇的机器，一直在不停地读取下一条指令去运行。那为什么CPU还会有满载运行和Idle闲置的状态呢？请你自己搜索研究一下这是为什么，并在留言区写下你的思考和答案。
欢迎你留言和我分享，你也可以把今天的文章分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>17_触发器：如何让数据修改自动触发关联操作，确保数据一致性？</title><link>https://artisanbox.github.io/8/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/17/</guid><description>你好，我是朱晓峰。今天，我来和你聊一聊触发器。
在实际开发中，我们经常会遇到这样的情况：有2个或者多个相互关联的表，如商品信息和库存信息分别存放在2个不同的数据表中，我们在添加一条新商品记录的时候，为了保证数据的完整性，必须同时在库存表中添加一条库存记录。
这样一来，我们就必须把这两个关联的操作步骤写到程序里面，而且要用事务包裹起来，确保这两个操作成为一个原子操作，要么全部执行，要么全部不执行。要是遇到特殊情况，可能还需要对数据进行手动维护，这样就很容易忘记其中的一步，导致数据缺失。
这个时候，其实咱们可以使用触发器。你可以创建一个触发器，让商品信息数据的插入操作自动触发库存数据的插入操作。这样一来，就不用担心因为忘记添加库存数据而导致的数据缺失了。
听上去好像很不错，那触发器到底怎么使用呢？接下来，我就重点给你聊聊。我会先给你讲解创建、查看和删除触发器的具体操作，然后借助一个案例带你实战一下。
如何操作触发器？首先，咱们来学习下触发器的基本操作。
创建触发器创建触发器的语法结构是：
CREATE TRIGGER 触发器名称 {BEFORE|AFTER} {INSERT|UPDATE|DELETE} ON 表名 FOR EACH ROW 表达式； 在创建时，你一定要注意触发器的三个要素。
表名：表示触发器监控的对象。 INSERT|UPDATE|DELETE：表示触发的事件。INSERT表示插入记录时触发；UPDATE表示更新记录时触发；DELETE表示删除记录时触发。 BEFORE|AFTER：表示触发的时间。BEFORE表示在事件之前触发；AFTER表示在事件之后触发。 只有把这三个要素定义好，才能正确使用触发器。
创建好触发器之后，咱们还要知道触发器是不是创建成功了。怎么查看呢？我来介绍下。
查看触发器查看触发器的语句是：
SHOW TRIGGERS\G; 删除触发器删除触发器很简单，你只要知道语法结构就可以了：
DROP TRIGGER 触发器名称; 知道了触发器的操作方法，接下来咱们就借助超市项目的实际案例，在真实的场景中实战一下，毕竟，实战是掌握操作的最好方法。
案例讲解超市项目实际实施过程中，客户经常要查询储值余额变动的明细，但是，查询会员消费流水时，存在数据汇总不及时、查询速度比较慢的问题。这时，我们就想到用触发器，及时把会员储值金额的变化信息记录到一个专门的表中。
我先用咱们熟悉的SQL语句来实现记录储值金额变动的操作，后面再带你使用触发器来操作。通过两种操作的对比，你就能更好地理解，在什么情况下，触发器能够比普通的SQL语句更加简洁高效，从而帮助你用好触发器这个工具，提高开发的能力。
下面我就借助具体数据，来详细说明一下。这里我们需要用到会员信息表（demo.membermaster）和会员储值历史表（demo.deposithist）。
会员信息表：
会员储值历史表：
假如在2020年12月20日这一天，会员编号是2的会员李四，到超市的某家连锁店购买了一条烟，消费了150元。现在，我们用之前学过的SQL语句，把这个会员储值余额的变动情况记录到会员储值历史表中。
第一步，查询出编号是2的会员卡的储值金额是多少。我们可以用下面的代码来实现：
mysql&amp;gt; SELECT memberdeposit -&amp;gt; FROM demo.membermaster -&amp;gt; WHERE memberid = 2; +---------------+ | memberdeposit | +---------------+ | 200.00 | +---------------+ 1 row in set (0.00 sec) 第二步，我们把会员编号是2的会员的储值金额减去150。
mysql&amp;gt; UPDATE demo.</description></item><item><title>17_跳表：为什么Redis一定要用跳表来实现有序集合？</title><link>https://artisanbox.github.io/2/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/18/</guid><description>上两节我们讲了二分查找算法。当时我讲到，因为二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，就真的没法用二分查找算法了吗？
实际上，我们只需要对链表稍加改造，就可以支持类似“二分”的查找算法。我们把改造之后的数据结构叫做跳表（Skip list），也就是今天要讲的内容。
跳表这种数据结构对你来说，可能会比较陌生，因为一般的数据结构和算法书籍里都不怎么会讲。但是它确实是一种各方面性能都比较优秀的动态数据结构，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。
Redis中的有序集合（Sorted Set）就是用跳表来实现的。如果你有一定基础，应该知道红黑树也可以实现快速地插入、删除和查找操作。那Redis为什么会选择用跳表来实现有序集合呢？ 为什么不用红黑树呢？学完今天的内容，你就知道答案了。
如何理解“跳表”？对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是O(n)。
那怎么来提高查找效率呢？如果像图中那样，对链表建立一级“索引”，查找起来是不是就会更快一些呢？每两个结点提取一个结点到上一级，我们把抽出来的那一级叫做索引或索引层。你可以看我画的图。图中的down表示down指针，指向下一级结点。
如果我们现在要查找某个结点，比如16。我们可以先在索引层遍历，当遍历到索引层中值为13的结点时，我们发现下一个结点是17，那要查找的结点16肯定就在这两个结点之间。然后我们通过索引层结点的down指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历2个结点，就可以找到值等于16的这个结点了。这样，原来如果要查找16，需要遍历10个结点，现在只需要遍历7个结点。
从这个例子里，我们看出，加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。那如果我们再加一级索引呢？效率会不会提升更多呢？
跟前面建立第一级索引的方式相似，我们在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在我们再来查找16，只需要遍历6个结点了，需要遍历的结点数量又减少了。
我举的例子数据量不大，所以即便加了两级索引，查找效率的提升也并不明显。为了让你能真切地感受索引提升查询效率。我画了一个包含64个结点的链表，按照前面讲的这种思路，建立了五级索引。
从图中我们可以看出，原来没有索引的时候，查找62需要遍历62个结点，现在只需要遍历11个结点，速度是不是提高了很多？所以，当链表的长度n比较大时，比如1000、10000的时候，在构建索引之后，查找效率的提升就会非常明显。
前面讲的这种链表加多级索引的结构，就是跳表。我通过例子给你展示了跳表是如何减少查询次数的，现在你应该比较清晰地知道，跳表确实是可以提高查询效率的。接下来，我会定量地分析一下，用跳表查询到底有多快。
用跳表查询到底有多快？前面我讲过，算法的执行效率可以通过时间复杂度来度量，这里依旧可以用。我们知道，在一个单链表中查询某个数据的时间复杂度是O(n)。那在一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢？
这个时间复杂度的分析方法比较难想到。我把问题分解一下，先来看这样一个问题，如果链表里有n个结点，会有多少级索引呢？
按照我们刚才讲的，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2，第二级索引的结点个数大约就是n/4，第三级索引的结点个数大约就是n/8，依次类推，也就是说，第k级索引的结点个数是第k-1级索引的结点个数的1/2，那第k级索引结点的个数就是n/(2k)。
假设索引有h级，最高级的索引有2个结点。通过上面的公式，我们可以得到n/(2h)=2，从而求得h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历m个结点，那在跳表中查询一个数据的时间复杂度就是O(m*logn)。
那这个m的值是多少呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历3个结点，也就是说m=3，为什么是3呢？我来解释一下。
假设我们要查找的数据是x，在第k级索引中，我们遍历到y结点之后，发现x大于y，小于后面的结点z，所以我们通过y的down指针，从第k级索引下降到第k-1级索引。在第k-1级索引中，y和z之间只有3个结点（包含y和z），所以，我们在K-1级索引中最多只需要遍历3个结点，依次类推，每一级索引都最多只需要遍历3个结点。
通过上面的分析，我们得到m=3，所以在跳表中查询任意数据的时间复杂度就是O(logn)。这个查找的时间复杂度跟二分查找是一样的。换句话说，我们其实是基于单链表实现了二分查找，是不是很神奇？不过，天下没有免费的午餐，这种查询效率的提升，前提是建立了很多级索引，也就是我们在第6节讲过的空间换时间的设计思路。
跳表是不是很浪费内存？比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。那到底需要消耗多少额外的存储空间呢？我们来分析一下跳表的空间复杂度。
跳表的空间复杂度分析并不难，我在前面说了，假设原始链表大小为n，那第一级索引大约有n/2个结点，第二级索引大约有n/4个结点，以此类推，每上升一级就减少一半，直到剩下2个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。
这几级索引的结点总和就是n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是O(n)。也就是说，如果将包含n个结点的单链表构造成跳表，我们需要额外再用接近n个结点的存储空间。那我们有没有办法降低索引占用的内存空间呢？
我们前面都是每两个结点抽一个结点到上级索引，如果我们每三个结点或五个结点，抽一个结点到上级索引，是不是就不用那么多索引结点了呢？我画了一个每三个结点抽一个的示意图，你可以看下。
从图中可以看出，第一级索引需要大约n/3个结点，第二级索引需要大约n/9个结点。每往上一级，索引结点个数都除以3。为了方便计算，我们假设最高一级的索引结点个数是1。我们把每级索引的结点个数都写下来，也是一个等比数列。
通过等比数列求和公式，总的索引结点大约就是n/3+n/9+n/27+...+9+3+1=n/2。尽管空间复杂度还是O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。
实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。
高效的动态插入和删除跳表长什么样子我想你应该已经很清楚了，它的查找操作我们刚才也讲过了。实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是O(logn)。
我们现在来看下， 如何在跳表中插入一个数据，以及它是如何做到O(logn)的时间复杂度的？
我们知道，在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。
对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的时间复杂度是O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是O(logn)。我画了一张图，你可以很清晰地看到插入的过程。
好了，我们再来看删除操作。
如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。
跳表索引动态更新当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某2个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。
作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。
如果你了解红黑树、AVL树这样平衡二叉树，你就知道它们是通过左右旋的方式保持左右子树的大小平衡（如果不了解也没关系，我们后面会讲），而跳表是通过随机函数来维护前面提到的“平衡性”。
当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。如何选择加入哪些索引层呢？
我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值K，那我们就将这个结点添加到第一级到第K级这K级索引中。
随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。至于随机函数的选择，我就不展开讲解了。如果你感兴趣的话，可以看看我在GitHub上的代码或者Redis中关于有序集合的跳表实现。
跳表的实现还是稍微有点复杂的，我将Java实现的代码放到了GitHub中，你可以根据我刚刚的讲解，对照着代码仔细思考一下。你不用死记硬背代码，跳表的实现并不是我们这节的重点。
解答开篇今天的内容到此就讲完了。现在，我来讲解一下开篇的思考题：为什么Redis要用跳表来实现有序集合，而不是红黑树？
Redis中的有序集合是通过跳表来实现的，严格点讲，其实还用到了散列表。不过散列表我们后面才会讲到，所以我们现在暂且忽略这部分。如果你去查看Redis的开发手册，就会发现，Redis中的有序集合支持的核心操作主要有下面这几个：
插入一个数据；
删除一个数据；
查找一个数据；
按照区间查找数据（比如查找值在[100, 356]之间的数据）；
迭代输出有序序列。
其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。
对于按照区间查找数据这个操作，跳表可以做到O(logn)的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。
当然，Redis之所以用跳表来实现有序集合，还有其他原因，比如，跳表更容易代码实现。虽然跳表的实现也不简单，但比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。
不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的Map类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果你想使用跳表，必须要自己实现。
内容小结今天我们讲了跳表这种数据结构。跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是O(logn)。
跳表的空间复杂度是O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。</description></item><item><title>17｜生成本地代码第2关：变量存储、函数调用和栈帧维护</title><link>https://artisanbox.github.io/3/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/19/</guid><description>你好，我是宫文学。
在上一节课里，我们已经初步生成了汇编代码和可执行文件。不过，很多技术细节我还没有来得及给你介绍，而且我们支持的语言特性也比较简单。
那么，这一节课，我就来给你补上这些技术细节。比如，我们要如何把逻辑寄存器映射到物理寄存器或内存地址、如何管理栈桢，以及如何让程序符合调用约定等等。
好了，我们开始吧。先让我们解决逻辑寄存器的映射问题，这其中涉及一个简单的寄存器分配算法。
给变量分配物理寄存器或内存在上一节课，我们在生成汇编代码的时候，给参数、本地变量和临时变量使用的都是逻辑寄存器，也就是只保存了变量的下标。那么我们要怎么把这些逻辑寄存器对应到物理的存储方式上来呢？
我们还是先来梳理一下实现思路吧。
其实，我们接下来要实现的寄存器分配算法，是一个比较初级的算法。你如果用clang或gcc把一个C语言的文件编译成汇编代码，并且不带-O1、-O2这样的优化选项，生成出来的汇编代码就是采用了类似的寄存器分配算法。现在我们就来看看这种汇编代码在实际存储变量上的特点。
首先，程序的参数都被保存到了内存里。具体是怎么来保存的呢？你可以先看看示例程序param.c：
void println(int a); int foo(int p1, int p2, int p3, int p4, int p5, int p6, int p7, int p8){ int x1 = p1p2; int x2 = p3p4; return x1 + x2 + p5p6 + p7p8; }
int main(){ int a = 10; int b = 12; int c = ab + foo(a,b,1,2,3,4,5,6) + foo(b,a,7,8,9,10,11,12); println(c); return 0; } &amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;这个示例程序所对应的汇编代码是param.</description></item><item><title>18_Python编译器（二）：从AST到字节码</title><link>https://artisanbox.github.io/7/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/18/</guid><description>你好，我是宫文学。
今天这一讲，我们继续来研究Python的编译器，一起来看看它是如何做语义分析的，以及是如何生成字节码的。学完这一讲以后，你就能回答出下面几个问题了：
像Python这样的动态语言，在语义分析阶段都要做什么事情呢，跟Java这样的静态类型语言有什么不同？ Python的字节码有什么特点？生成字节码的过程跟Java有什么不同？ 好了，让我们开始吧。首先，我们来了解一下从AST到生成字节码的整个过程。
编译过程Python编译器把词法分析和语法分析叫做“解析（Parse）”，并且放在Parser目录下。而从AST到生成字节码的过程，才叫做“编译（Compile）”。当然，这里编译的含义是比较狭义的。你要注意，不仅是Python编译器，其他编译器也是这样来使用这两个词汇，包括我们已经研究过的Java编译器，你要熟悉这两个词汇的用法，以便阅读英文文献。
Python的编译工作的主干代码是在Python/compile.c中，它主要完成5项工作。
第一步，检查future语句。future语句是Python的一个特性，让你可以提前使用未来版本的特性，提前适应语法和语义上的改变。这显然会影响到编译器如何工作。比如，对于“8/7”，用不同版本的语义去处理，得到的结果是不一样的。有的会得到整数“1”，有的会得到浮点数“1.14285…”，编译器内部实际上是调用了不同的除法函数。
第二步，建立符号表。
第三步，为基本块产生指令。
第四步，汇编过程：把所有基本块的代码组装在一起。
第五步，对字节码做窥孔优化。
其中的第一步，它是Python语言的一个特性，但不是我们编译技术关注的重点，所以这里略过。我们从建立符号表开始。
语义分析：建立符号表和引用消解通常来说，在语义分析阶段首先是建立符号表，然后在此基础上做引用消解和类型检查。
而Python是动态类型的语言，类型检查应该是不需要了，但引用消解还是要做的。并且你会发现，Python的引用消解有其独特之处。
首先，我们来看看Python的符号表是一个什么样的数据结构。在Include/symtable.h中定义了两个结构，分别是符号表和符号表的条目：
图1：符号表和符号表条目在编译的过程中，针对每个模块（也就是一个Python文件）会生成一个符号表（symtable）。
Python程序被划分成“块（block）”，块分为三种：模块、类和函数。每种块其实就是一个作用域，而在Python里面还叫做命名空间。每个块对应一个符号表条目（PySTEntryObject），每个符号表条目里存有该块里的所有符号（ste_symbols）。每个块还可以有多个子块（ste_children），构成树状结构。
在符号表里，有一个st_blocks字段，这是个字典，它能通过模块、类和函数的AST节点，查找到Python程序的符号表条目，通过这种方式，就把AST和符号表关联在了一起。
我们来看看，对于下面的示例程序，它对应的符号表是什么样子的。
a = 2 #模块级变量 class myclass: def __init__(self, x): self.x = x def foo(self, b): c = a + self.x + b #引用了外部变量a return c 这个示例程序有模块、类和函数三个级别的块。它们分别对应一条符号表条目。
图2：示例程序对应的符号表你可以看到，每个块里都有ste_symbols字段，它是一个字典，里面保存了本命名空间涉及的符号，以及每个符号的各种标志位（flags）。关于标志位，我下面会给你解释。
然后，我们再看看针对这个示例程序，符号表里的主要字段的取值：
好了，通过这样一个例子，你大概就知道了Python的符号表是怎样设计的了。下面我们来看看符号表的建立过程。
建立符号表的主程序是Python/symtable.c中的PySymtable_BuildObject()函数。
Python建立符号表的过程，需要做两遍处理，如下图所示。
图3：Python建立符号表的过程第一遍，主要做了两件事情。第一件事情是建立一个个的块（也就是符号表条目），并形成树状结构，就像示例程序那样；第二件事情，就是给块中的符号打上一定的标记（flag）。
我们用GDB跟踪一下第一遍处理后生成的结果。你可以参考下图，看一下我在Python的REPL中的输入信息：
我在symtable_add_def_helper()函数中设置了断点，便于调试。当编译器处理到foo函数的时候，我在GDB中打印输出了一些信息：
在这些输出信息中，你能看到前面我给你整理的表格中的信息，比如，符号表中各个字段的取值。
我重点想让你看的，是foo块中各个符号的标志信息：self和b是20，c是2，a是16。这是什么意思呢？
ste_symbols = {'self': 20, 'b': 20, 'c': 2, 'a': 16} 这就需要看一下symtable.h中，对这些标志位的定义：
我给你整理成了一张更容易理解的图，你参考一下：</description></item><item><title>18_为什么这些SQL语句逻辑相同，性能却差异巨大？</title><link>https://artisanbox.github.io/1/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/18/</guid><description>在MySQL中，有很多看上去逻辑相同，但性能却差异巨大的SQL语句。对这些语句使用不当的话，就会不经意间导致整个数据库的压力变大。
我今天挑选了三个这样的案例和你分享。希望再遇到相似的问题时，你可以做到举一反三、快速解决问题。
案例一：条件字段函数操作假设你现在维护了一个交易系统，其中交易记录表tradelog包含交易流水号（tradeid）、交易员id（operator）、交易时间（t_modified）等字段。为了便于描述，我们先忽略其他字段。这个表的建表语句如下：
mysql&amp;gt; CREATE TABLE `tradelog` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `operator` int(11) DEFAULT NULL, `t_modified` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`), KEY `t_modified` (`t_modified`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 假设，现在已经记录了从2016年初到2018年底的所有数据，运营部门有一个需求是，要统计发生在所有年份中7月份的交易记录总数。这个逻辑看上去并不复杂，你的SQL语句可能会这么写：
mysql&amp;gt; select count(*) from tradelog where month(t_modified)=7; 由于t_modified字段上有索引，于是你就很放心地在生产库中执行了这条语句，但却发现执行了特别久，才返回了结果。
如果你问DBA同事为什么会出现这样的情况，他大概会告诉你：如果对字段做了函数计算，就用不上索引了，这是MySQL的规定。
现在你已经学过了InnoDB的索引结构了，可以再追问一句为什么？为什么条件是where t_modified='2018-7-1’的时候可以用上索引，而改成where month(t_modified)=7的时候就不行了？
下面是这个t_modified索引的示意图。方框上面的数字就是month()函数对应的值。
图1 t_modified索引示意图如果你的SQL语句条件用的是where t_modified='2018-7-1’的话，引擎就会按照上面绿色箭头的路线，快速定位到 t_modified='2018-7-1’需要的结果。
实际上，B+树提供的这个快速定位能力，来源于同一层兄弟节点的有序性。
但是，如果计算month()函数的话，你会看到传入7的时候，在树的第一层就不知道该怎么办了。
也就是说，对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。
需要注意的是，优化器并不是要放弃使用这个索引。
在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引t_modified，优化器对比索引大小后发现，索引t_modified更小，遍历这个索引比遍历主键索引来得更快。因此最终还是会选择索引t_modified。
接下来，我们使用explain命令，查看一下这条SQL语句的执行结果。
图2 explain 结果key="t_modified"表示的是，使用了t_modified这个索引；我在测试表数据中插入了10万行数据，rows=100335，说明这条语句扫描了整个索引的所有值；Extra字段的Using index，表示的是使用了覆盖索引。
也就是说，由于在t_modified字段加了month()函数操作，导致了全索引扫描。为了能够用上索引的快速定位能力，我们就要把SQL语句改成基于字段本身的范围查询。按照下面这个写法，优化器就能按照我们预期的，用上t_modified索引的快速定位能力了。
mysql&amp;gt; select count(*) from tradelog where -&amp;gt; (t_modified &amp;gt;= '2016-7-1' and t_modified&amp;lt;'2016-8-1') or -&amp;gt; (t_modified &amp;gt;= '2017-7-1' and t_modified&amp;lt;'2017-8-1') or -&amp;gt; (t_modified &amp;gt;= '2018-7-1' and t_modified&amp;lt;'2018-8-1'); 当然，如果你的系统上线时间更早，或者后面又插入了之后年份的数据的话，你就需要再把其他年份补齐。</description></item><item><title>18_划分土地（下）：如何实现内存页的分配与释放？</title><link>https://artisanbox.github.io/9/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/18/</guid><description>你好，我是LMOS。
通过前面两节课的学习，我们已经组织好了内存页，也初始化了内存页和内存区。我们前面做了这么多准备工作，就是为了实现分配和释放内存页面，达到内存管理的目的。
那有了前面的基础，我想你自己也能大概实现这个分配和释放的代码。但是，根据前面我们设计的数据结构和对其初始化的工作，估计你也可以隐约感觉到，我们的内存管理的算法还是有一点点难度的。
今天这节课，就让我们一起来实现这项富有挑战性的任务吧！这节课的配套代码，你可以通过这里下载。
内存页的分配如果让你实现一次只分配一个页面，我相信这个问题很好解决，因为你只需要写一段循环代码，在其中遍历出一个空闲的msadsc_t结构，就可以返回了，这个算法就可以结束了。
但现实却不容许我们这么简单地处理问题，我们内存管理器要为内核、驱动，还有应用提供服务，它们对请求内存页面的多少、内存页面是不是连续，内存页面所处的物理地址都有要求。
这样一来，问题就复杂了。不过你也不必担心，我们可以从内存分配的接口函数下手。
下面我们根据上述要求来设计实现内存分配接口函数。我们还是先来建立一个新的C语言代码文件，在cosmos/hal/x86目录中建立一个memdivmer.c文件，在其中写一个内存分配接口函数，代码如下所示。
//内存分配页面框架函数 msadsc_t *mm_divpages_fmwk(memmgrob_t *mmobjp, uint_t pages, uint_t *retrelpnr, uint_t mrtype, uint_t flgs) { //返回mrtype对应的内存区结构的指针 memarea_t *marea = onmrtype_retn_marea(mmobjp, mrtype); if (NULL == marea) { *retrelpnr = 0; return NULL; } uint_t retpnr = 0; //内存分配的核心函数 msadsc_t *retmsa = mm_divpages_core(marea, pages, &amp;amp;retpnr, flgs); if (NULL == retmsa) { *retrelpnr = 0; return NULL; } *retrelpnr = retpnr; return retmsa; } //内存分配页面接口
//mmobjp-&amp;gt;内存管理数据结构指针 //pages-&amp;gt;请求分配的内存页面数 //retrealpnr-&amp;gt;存放实际分配内存页面数的指针 //mrtype-&amp;gt;请求的分配内存页面的内存区类型 //flgs-&amp;gt;请求分配的内存页面的标志位 msadsc_t *mm_division_pages(memmgrob_t *mmobjp, uint_t pages, uint_t *retrealpnr, uint_t mrtype, uint_t flgs) { if (NULL == mmobjp || NULL == retrealpnr || 0 == mrtype) { return NULL; }</description></item><item><title>18_建立数据通路（中）：指令+运算=CPU</title><link>https://artisanbox.github.io/4/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/18/</guid><description>上一讲，我们看到，要能够实现一个完整的CPU功能，除了加法器这样的电路之外，我们还需要实现其他功能的电路。其中有一些电路，和我们实现过的加法器一样，只需要给定输入，就能得到固定的输出。这样的电路，我们称之为组合逻辑电路（Combinational Logic Circuit）。
但是，光有组合逻辑电路是不够的。你可以想一下，如果只有组合逻辑电路，我们的CPU会是什么样的？电路输入是确定的，对应的输出自然也就确定了。那么，我们要进行不同的计算，就要去手动拨动各种开关，来改变电路的开闭状态。这样的计算机，不像我们现在每天用的功能强大的电子计算机，反倒更像古老的计算尺或者机械计算机，干不了太复杂的工作，只能协助我们完成一些计算工作。
这样，我们就需要引入第二类的电路，也就是时序逻辑电路（Sequential Logic Circuit）。时序逻辑电路可以帮我们解决这样几个问题。
第一个就是自动运行的问题。时序电路接通之后可以不停地开启和关闭开关，进入一个自动运行的状态。这个使得我们上一讲说的，控制器不停地让PC寄存器自增读取下一条指令成为可能。
第二个是存储的问题。通过时序电路实现的触发器，能把计算结果存储在特定的电路里面，而不是像组合逻辑电路那样，一旦输入有任何改变，对应的输出也会改变。
第三个本质上解决了各个功能按照时序协调的问题。无论是程序实现的软件指令，还是到硬件层面，各种指令的操作都有先后的顺序要求。时序电路使得不同的事件按照时间顺序发生。
时钟信号的硬件实现想要实现时序逻辑电路，第一步我们需要的就是一个时钟。我在第3讲说过，CPU的主频是由一个晶体振荡器来实现的，而这个晶体振荡器生成的电路信号，就是我们的时钟信号。
实现这样一个电路，和我们之前讲的，通过电的磁效应产生开关信号的方法是一样的。只不过，这里的磁性开关，打开的不再是后续的线路，而是当前的线路。
在下面这张图里你可以看到，我们在原先一般只放一个开关的信号输入端，放上了两个开关。一个开关A，一开始是断开的，由我们手工控制；另外一个开关B，一开始是合上的，磁性线圈对准一开始就合上的开关B。
于是，一旦我们合上开关A，磁性线圈就会通电，产生磁性，开关B就会从合上变成断开。一旦这个开关断开了，电路就中断了，磁性线圈就失去了磁性。于是，开关B又会弹回到合上的状态。这样一来，电路接通，线圈又有了磁性。我们的电路就会来回不断地在开启、关闭这两个状态中切换。
开关A闭合（也就是相当于接通电路之后），开关B就会不停地在开和关之间切换，生成对应的时钟信号这个不断切换的过程，对于下游电路来说，就是不断地产生新的0和1这样的信号。如果你在下游的电路上接上一个灯泡，就会发现这个灯泡在亮和暗之间不停切换。这个按照固定的周期不断在0和1之间切换的信号，就是我们的时钟信号（Clock Signal）。
一般这样产生的时钟信号，就像你在各种教科书图例中看到的一样，是一个振荡产生的0、1信号。
时钟信号示意图这种电路，其实就相当于把电路的输出信号作为输入信号，再回到当前电路。这样的电路构造方式呢，我们叫作反馈电路（Feedback Circuit）。
接下来，我们还会看到更多的反馈电路。上面这个反馈电路一般可以用下面这个示意图来表示，其实就是一个输出结果接回输入的反相器（Inverter），也就是我们之前讲过的非门。
通过一个反相器实现时钟信号通过D触发器实现存储功能有了时钟信号，我们的系统里就有了一个像“自动门”一样的开关。利用这个开关和相同的反馈电路，我们就可以构造出一个有“记忆”功能的电路。这个有记忆功能的电路，可以实现在CPU中用来存储计算结果的寄存器，也可以用来实现计算机五大组成部分之一的存储器。
我们先来看下面这个RS触发器电路。这个电路由两个或非门电路组成。我在图里面，把它标成了A和B。
或非门的真值表 在这个电路一开始，输入开关都是关闭的，所以或非门（NOR）A的输入是0和0。对应到我列的这个真值表，输出就是1。而或非门B的输入是0和A的输出1，对应输出就是0。B的输出0反馈到A，和之前的输入没有变化，A的输出仍然是1。而整个电路的输出Q，也就是0。
当我们把A前面的开关R合上的时候，A的输入变成了1和0，输出就变成了0，对应B的输入变成0和0，输出就变成了1。B的输出1反馈给到了A，A的输入变成了1和1，输出仍然是0。所以把A的开关合上之后，电路仍然是稳定的，不会像晶振那样振荡，但是整个电路的输出Q变成了1。
这个时候，如果我们再把A前面的开关R打开，A的输入变成和1和0，输出还是0，对应的B的输入没有变化，输出也还是1。B的输出1反馈给到了A，A的输入变成了1和0，输出仍然是0。这个时候，电路仍然稳定。开关R和S的状态和上面的第一步是一样的，但是最终的输出Q仍然是1，和第1步里Q状态是相反的。我们的输入和刚才第二步的开关状态不一样，但是输出结果仍然保留在了第2步时的输出没有发生变化。
这个时候，只有我们再去关闭下面的开关S，才可以看到，这个时候，B有一个输入必然是1，所以B的输出必然是0，也就是电路的最终输出Q必然是0。
这样一个电路，我们称之为触发器（Flip-Flop）。接通开关R，输出变为1，即使断开开关，输出还是1不变。接通开关S，输出变为0，即使断开开关，输出也还是0。也就是，当两个开关都断开的时候，最终的输出结果，取决于之前动作的输出结果，这个也就是我们说的记忆功能。
这里的这个电路是最简单的RS触发器，也就是所谓的复位置位触发器（Reset-Set Flip Flop) 。对应的输出结果的真值表，你可以看下面这个表格。可以看到，当两个开关都是0的时候，对应的输出不是1或者0，而是和Q的上一个状态一致。
再往这个电路里加两个与门和一个小小的时钟信号，我们就可以实现一个利用时钟信号来操作一个电路了。这个电路可以帮我们实现什么时候可以往Q里写入数据。
我们看看下面这个电路，这个在我们的上面的R-S触发器基础之上，在R和S开关之后，加入了两个与门，同时给这两个与门加入了一个时钟信号CLK作为电路输入。
这样，当时钟信号CLK在低电平的时候，与门的输入里有一个0，两个实际的R和S后的与门的输出必然是0。也就是说，无论我们怎么按R和S的开关，根据R-S触发器的真值表，对应的Q的输出都不会发生变化。
只有当时钟信号CLK在高电平的时候，与门的一个输入是1，输出结果完全取决于R和S的开关。我们可以在这个时候，通过开关R和S，来决定对应Q的输出。
通过一个时钟信号，我们可以在特定的时间对输出的Q进行写入操作如果这个时候，我们让R和S的开关，也用一个反相器连起来，也就是通过同一个开关控制R和S。只要CLK信号是1，R和S就可以设置输出Q。而当CLK信号是0的时候，无论R和S怎么设置，输出信号Q是不变的。这样，这个电路就成了我们最常用的D型触发器。用来控制R和S这两个开关的信号呢，我们视作一个输入的数据信号D，也就是Data，这就是D型触发器的由来。
把R和S两个信号通过一个反相器合并，我们可以通过一个数据信号D进行Q的写入操作一个D型触发器，只能控制1个比特的读写，但是如果我们同时拿出多个D型触发器并列在一起，并且把用同一个CLK信号控制作为所有D型触发器的开关，这就变成了一个N位的D型触发器，也就可以同时控制N位的读写。
CPU里面的寄存器可以直接通过D型触发器来构造。我们可以在D型触发器的基础上，加上更多的开关，来实现清0或者全部置为1这样的快捷操作。
总结延伸好了，到了这里，我们可以顺一顺思路了。通过引入了时序电路，我们终于可以把数据“存储”下来了。我们通过反馈电路，创建了时钟信号，然后再利用这个时钟信号和门电路组合，实现了“状态记忆”的功能。
电路的输出信号不单单取决于当前的输入信号，还要取决于输出信号之前的状态。最常见的这个电路就是我们的D触发器，它也是我们实际在CPU内实现存储功能的寄存器的实现方式。
这也是现代计算机体系结构中的“冯·诺伊曼”机的一个关键，就是程序需要可以“存储”，而不是靠固定的线路连接或者手工拨动开关，来实现计算机的可存储和可编程的功能。
有了时钟信号和触发器之后，我们还差一个“自动”需求没有实现。我们的计算机还不能做到自动地不停地从内存里面读取指令去执行。这一部分，我们留在下一讲。下一讲里，我们看看怎么让程序自动运转起来。
推荐阅读想要深入了解计算机里面的各种功能组件，是怎么通过电路来实现的，推荐你去阅读《编码：隐匿在计算机软硬件背后的语言》这本书的第14章和16章。
如果对于数字电路和数字逻辑特别感兴趣，想要彻底弄清楚数字电路、时序逻辑电路，也可以看一看计算机学科的一本专业的教科书《数字逻辑应用与设计》。
课后思考现在我们的CPU主频非常高了，通常在几GHz了，但是实际上我们的晶振并不能提供这么高的频率，而是通过“外频+倍频“的方式来实现高频率的时钟信号。请你研究一下，倍频和分频的信号是通过什么样的电路实现的？
欢迎留言和我分享你的疑惑和见解，也欢迎你把今天的内容分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>18_散列表（上）：Word文档中的单词拼写检查功能是如何实现的？</title><link>https://artisanbox.github.io/2/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/19/</guid><description>Word这种文本编辑器你平时应该经常用吧，那你有没有留意过它的拼写检查功能呢？一旦我们在Word里输入一个错误的英文单词，它就会用标红的方式提示“拼写错误”。Word的这个单词拼写检查功能，虽然很小但却非常实用。你有没有想过，这个功能是如何实现的呢？
其实啊，一点儿都不难。只要你学完今天的内容，散列表（Hash Table）。你就能像微软Office的工程师一样，轻松实现这个功能。
散列思想散列表的英文叫“Hash Table”，我们平时也叫它“哈希表”或者“Hash表”，你一定也经常听过它，我在前面的文章里，也不止一次提到过，但是你是不是真的理解这种数据结构呢？
散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。
我用一个例子来解释一下。假如我们有89名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。这89名选手的编号依次是1到89。现在我们希望编程实现这样一个功能，通过编号快速找到对应的选手信息。你会怎么做呢？
我们可以把这89名选手的信息放在数组里。编号为1的选手，我们放到数组中下标为1的位置；编号为2的选手，我们放到数组中下标为2的位置。以此类推，编号为k的选手放到数组中下标为k的位置。
因为参赛编号跟数组下标一一对应，当我们需要查询参赛编号为x的选手的时候，我们只需要将下标为x的数组元素取出来就可以了，时间复杂度就是O(1)。这样按照编号查找选手信息，效率是不是很高？
实际上，这个例子已经用到了散列的思想。在这个例子里，参赛编号是自然数，并且与数组的下标形成一一映射，所以利用数组支持根据下标随机访问的时候，时间复杂度是O(1)这一特性，就可以实现快速查找编号对应的选手信息。
你可能要说了，这个例子中蕴含的散列思想还不够明显，那我来改造一下这个例子。
假设校长说，参赛编号不能设置得这么简单，要加上年级、班级这些更详细的信息，所以我们把编号的规则稍微修改了一下，用6位数字来表示。比如051167，其中，前两位05表示年级，中间两位11表示班级，最后两位还是原来的编号1到89。这个时候我们该如何存储选手信息，才能够支持通过编号来快速查找选手信息呢？
思路还是跟前面类似。尽管我们不能直接把编号作为数组下标，但我们可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。当通过参赛编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。
这就是典型的散列思想。其中，参赛选手的编号我们叫做键（key）或者关键字。我们用它来标识一个选手。我们把参赛编号转化为数组下标的映射方法就叫作散列函数（或“Hash函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash值”“哈希值”）。
通过这个例子，我们可以总结出这样的规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是O(1)的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。
散列函数从上面的例子我们可以看到，散列函数在散列表中起着非常关键的作用。现在我们就来学习下散列函数。
散列函数，顾名思义，它是一个函数。我们可以把它定义成hash(key)，其中key表示元素的键值，hash(key)的值表示经过散列函数计算得到的散列值。
那第一个例子中，编号就是数组下标，所以hash(key)就等于key。改造后的例子，写成散列函数稍微有点复杂。我用伪代码将它写成函数就是下面这样：
int hash(String key) { // 获取后两位字符 string lastTwoChars = key.substr(length-2, length); // 将后两位字符转换为整数 int hashValue = convert lastTwoChas to int-type; return hashValue; } 刚刚举的学校运动会的例子，散列函数比较简单，也比较容易想到。但是，如果参赛选手的编号是随机生成的6位数字，又或者用的是a到z之间的字符串，该如何构造散列函数呢？我总结了三点散列函数设计的基本要求：
散列函数计算得到的散列值是一个非负整数；
如果key1 = key2，那hash(key1) == hash(key2)；
如果key1 ≠ key2，那hash(key1) ≠ hash(key2)。
我来解释一下这三点。其中，第一点理解起来应该没有任何问题。因为数组下标是从0开始的，所以散列函数生成的散列值也要是非负整数。第二点也很好理解。相同的key，经过散列函数得到的散列值也应该是相同的。
第三点理解起来可能会有问题，我着重说一下。这个要求看起来合情合理，但是在真实的情况下，要想找到一个不同的key对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。
所以我们几乎无法找到一个完美的无冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，我们需要通过其他途径来解决。
散列冲突再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。
1.开放寻址法开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法，线性探测（Linear Probing）。
当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。
我说的可能比较抽象，我举一个例子具体给你说明一下。这里面黄色的色块表示空闲位置，橙色的色块表示已经存储了数据。</description></item><item><title>18_权限管理：如何控制数据库访问，消除安全隐患？</title><link>https://artisanbox.github.io/8/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/18/</guid><description>你好，我是朱晓峰，今天，我来和你聊一聊权限管理。
我们在开发应用的时候，经常会遇到一种需求，就是要根据用户的不同，对数据进行横向和纵向的分组。
所谓横向的分组，就是指用户可以接触到的数据的范围，比如可以看到哪些表的数据；所谓纵向的分组，就是指用户对接触到的数据能访问到什么程度，比如能看、能改，甚至是删除。
我们把具有相同数据访问范围和程度的用户归成不同的类别，这种类别就叫角色。通过角色，管理用户对数据库访问的范围和程度就更加方便了。这也就是对用户的数据访问权限的管理。
恰当的权限设定，可以确保数据的安全性，这是至关重要的。
那么，怎么进行权限管理呢？这节课，我就结合超市项目的实际案例，给你讲一下权限管理的具体操作，包括怎么操作角色和用户，怎么通过角色给用户授权，怎么直接给用户授权，从而帮助你管理好用户的权限，提升数据库的安全性。
下面我就来先讲讲角色。我们可以通过角色对相同权限的用户进行分组管理，这样可以使权限管理更加简单高效。
角色的作用角色是在MySQL 8.0中引入的新功能，相当于一个权限的集合。引入角色的目的是方便管理拥有相同权限的用户。
下面我举个超市项目中的例子，来具体说明一下如何通过角色管理用户权限。
超市项目中有库管、营运和财务等不同的模块，它们各自对应不同的数据表。比如库存模块中的盘点表（demo.invcount）、营运模块中的商品信息表（demo.goodsmaster），还有财务模块中的应付账款表（demo.settlement）。下面是这些表的具体信息。
盘点表：
商品信息表：
应付账款表：
在超市项目中，员工的职责不同，包括库管、营运和财务等，不同的职责有不同的数据访问权限。比如：
张三是库管，他就可以查询商品信息表，对盘点表有增删改查的权限，但无权访问应付账款表； 李四是营运，他就拥有对商品信息表有增删改查的权限，而对库存表和应付账款表，只有查看的权限； 王五是财务，他就有对应付账款表有增删改查的权限，对商品信息表和库存表，只有查看的权限。 所以，我们需要为每一个职责创建一个对应的角色，为每个员工创建一个对应的数据库用户。然后通过给角色赋予相关的权限，再把角色赋予用户，实现对超市员工访问数据权限的管理，从而保证数据的安全性。
这样说有点抽象，下面我们具体操作一下角色和用户。
如何操作角色？首先，我们要创建一个角色，为后面的授权做好准备。
如何创建角色？MySQL中的角色名称由角色名称加主机名称组成。创建角色的语法结构如下：
CREATE ROLE 角色名; 假设我们现在需要创建一个经理的角色，就可以用下面的代码：
mysql&amp;gt; CREATE ROLE 'manager'@'localhost'; Query OK, 0 rows affected (0.06 sec) 这里的意思是，创建一个角色，角色名称是“manager”，角色可以登录的主机是“localhost”，意思是只能从数据库服务器运行的这台计算机登录这个账号。你也可以不写主机名，直接创建角色“manager”：
mysql&amp;gt; CREATE ROLE 'manager'; Query OK, 0 rows affected (0.01 sec) 如果不写主机名，MySQL默认是通配符“%”，意思是这个账号可以从任何一台主机上登录数据库。
同样道理，如果我们要创建库管的角色，就可以用下面的代码：
mysql&amp;gt; CREATE ROLE 'stocker'; Query OK, 0 rows affected (0.02 sec) 创建角色之后，默认这个角色是没有任何权限的，我们需要给角色授权。
怎么给角色赋予权限？给角色授权的语法结构是：
GRANT 权限 ON 表名 TO 角色名; 假设我们现在想给经理角色授予商品信息表、盘点表和应付账款表的只读权限，就可以用下面的代码来实现：</description></item><item><title>18_移进和规约：用LR算法推演一个实例</title><link>https://artisanbox.github.io/6/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/18/</guid><description>到目前为止，我们所讨论的语法分析算法，都是自顶向下的。与之相对应的，是自底向上的算法，比如本节课要探讨的LR算法家族。
LR算法是一种自底向上的算法，它能够支持更多的语法，而且没有左递归的问题。第一个字母L，与LL算法的第一个L一样，代表从左向右读入程序。第二个字母R，指的是RightMost（最右推导），也就是在使用产生式的时候，是从右往左依次展开非终结符。例如，对于“add-&amp;gt;add+mul”这样一个产生式，是优先把mul展开，然后再是add。在接下来的讲解过程中，你会看到这个过程。
自顶向下的算法，是递归地做模式匹配，从而逐步地构造出AST。那么自底向上的算法是如何构造出AST的呢？答案是用移进-规约的算法。
本节课，我就带你通过移进-规约方法，自底向上地构造AST，完成语法的解析。接下来，我们先通过一个例子看看自底向上语法分析的过程。
通过实例了解自底向上语法分析的过程我们选择熟悉的语法规则：
add -&amp;gt; mul add -&amp;gt; add + mul mul -&amp;gt; pri mul -&amp;gt; mul * pri pri -&amp;gt; Int | (add) 然后来解析“2+3*5”这个表达式，AST如下：
我们分步骤看一下解析的具体过程。
第1步，看到第一个Token，是Int，2。我们把它作为AST的第一个节点，同时把它放到一个栈里（就是图中红线左边的部分）。这个栈代表着正在处理的一些AST节点，把Token移到栈里的动作叫做移进（Shift）。
第2步，根据语法规则，Int是从pri推导出来的（pri-&amp;gt;Int），那么它的上级AST肯定是pri，所以，我们给它加了一个父节点pri，同时，也把栈里的Int替换成了pri。这个过程是语法推导的逆过程，叫做规约（Reduce）。
Reduce这个词你在学Map-Reduce时可能接触过，它相当于我们口语化的“倒推”。具体来讲，它是从工作区里倒着取出1到n个元素，根据某个产生式，组合出上一级的非终结符，也就是AST的上级节点，然后再放进工作区（也就是竖线的左边）。
这个时候，栈里可能有非终结符，也可能有终结符，它仿佛是我们组装AST的一个工作区。竖线的右边全都是Token（也就是终结符），它们在等待处理。
第3步，与第2步一样，因为pri只能是mul推导出来的，产生式是“mul-&amp;gt;pri”，所以我们又做了一次规约。
第4步，我们根据“add-&amp;gt;mul”产生式，将mul规约成add。至此，我们对第一个Token做了3次规约，已经到头了。这里为什么做规约，而不是停在mul上，移进+号，是有原因的。因为没有一个产生式，是mul后面跟+号，而add后面却可以跟+号。
第5步，移进+号。现在栈里有两个元素了，分别是add和+。
第6步，移进Int，也就是数字3。栈里现在有3个元素。
第7到第8步，Int规约到pri，再规约到mul。
到目前为止，我们做规约的方式都比较简单，就是对着栈顶的元素，把它反向推导回去。
第9步，我们面临3个选择，比较难。
第一个选择是继续把mul规约成add，第二个选择是把“add+mul”规约成add。这两个选择都是错误的，因为它们最终无法形成正确的AST。
第三个选择，也就是按照“mul-&amp;gt;mul*pri”，继续移进 *号 ，而不是做规约。只有这样，才能形成正确的AST，就像图中的虚线。
第10步，移进Int，也就是数字5。
第11步，Int规约成pri。
第12步，mul*pri规约成mul。
注意，这里也有两个选择，比如把pri继续规约成mul。但它显然也是错误的选择。
第13步，add+mul规约成add。
至此，我们就构建完成了一棵正确的AST，并且，栈里也只剩下了一个元素，就是根节点。
整个语法解析过程，实质是反向最右推导（Reverse RightMost Derivation）。什么意思呢？如果把AST节点根据创建顺序编号，就是下面这张图呈现的样子，根节点编号最大是13：
但这是规约的过程，如果是从根节点开始的推导过程，顺序恰好是反过来的，先是13号，再是右子节点12号，再是12号的右子节点11号，以此类推。我们把这个最右推导过程写在下面：
在语法解析的时候，我们是从底下反推回去，所以叫做反向的最右推导过程。从这个意义上讲，LR算法中的R，带有反向（Reverse）和最右（Reightmost）这两层含义。
在最右推导过程中，我加了下划线的部分，叫做一个句柄（Handle）。句柄是一个产生式的右边部分，以及它在一个右句型（最右推导可以得到的句型）中的位置。以最底下一行为例，这个句柄“Int”是产生式“pri-&amp;gt;Int”的右边部分，它的位置是句型“Int + Int * Int”的第一个位置。
简单来说，句柄，就是产生式是在这个位置上做推导的，如果需要做反向推导的话，也是从这个位置去做规约。
针对这个简单的例子，我们可以用肉眼进行判断，找到正确的句柄，做出正确的选择。不过，要把这种判断过程变成严密的算法，做到在每一步都采取正确的行动，知道该做移进还是规约，做规约的话，按照哪个产生式，这就是LR算法要解决的核心问题了。
那么，如何找到正确的句柄呢？
找到正确的句柄我们知道，最右推导是从最开始的产生式出发，经过多步推导（多步推导记做-&amp;gt;*），一步步形成当前的局面 （也就是左边栈里有一些非终结符和终结符，右边还可以预看1到k个Token）。
add -&amp;gt;* 栈 | Token 我们要像侦探一样，根据手头掌握的信息，反向推导出这个多步推导的路径，从而获得正确的句柄。我们依据的是左边栈里的信息，以及右边的Token串。对于LR(0)算法来说，我们只依据左边的栈，就能找到正确的句柄，对于LR(1)算法来说，我们可以从右边预看一个Token。</description></item><item><title>18｜生成本地代码第3关：实现完整的功能</title><link>https://artisanbox.github.io/3/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/20/</guid><description>你好，我是宫文学。
到目前为止，我们已经把挑战生成本地代码的过程中会遇到的各种难点都解决了，也就是说，我们已经实现基本的寄存器分配算法，并维护好了栈桢。在这个基础上，我们只需要再实现其他的语法特性就行了。
所以，在今天这节课，我们要让编译器支持条件语句和循环语句。这样的话，我们就可以为前面一直在使用的一些例子，比如生成斐波那契数列的程序，生成本地代码了。然后，我们可以再比较一次不同运行时机制下的性能表现。
还记得吗？我们在前面已经分别使用了AST解释器、基于JavaScript的虚拟机和基于C语言的虚拟机来生成斐波那契数列。现在我们就看看用我们自己生成的本地代码，性能上是否会有巨大的变化。
在这个过程中，我们还会再次认识CFG这种数据结构，也会考察一下如何支持一元运算，让我们的语言特性更加丰富。
那首先我们来看一下，如何实现if语句和for循环语句。
支持if语句和for循环语句在前面的课程中，我们曾经练习过为if语句和for循环语句生成字节码。不知道你还记不记得，其中的难点就是生成跳转指令。在今天这节课，我们会完成类似的任务，但采用的是一个稍微不同的方法。
我们还是先来研究一下if语句，看看针对if语句，编译器需要生成什么代码。我们采用下面一个C语言的示例程序，更详细的代码你可以参见代码库中的if.c：
int foo(int a){ if (a &amp;gt; 10) return a + 8; else return a - 8; } C语言的编译器针对这段示例程序，会生成下面的汇编代码（参见代码库中的if.s），我对汇编代码进行了整理，并添加了注释。
这段汇编代码是未经优化的。不过，我相信你经过前面课程的训练，应该可以看出来很多可以用手工优化的地方。不过现在我们关注的重点是跳转指令，所以你可以重点看一下代码中的cmpl指令、jle指令和jmp指令。
我们现在来分析一下。第一个cmpl指令的作用是比较两个整数的大小。在这个例子中，是比较a和10的大小，计算结果会设置到eflags寄存器中相应的标志位。
第二个jle指令，它的作用是根据eflags寄存器中标志位决定是否进行跳转，如果发现是小于等于的结果，那么就进行跳转，这里是跳转到else块。如果是大于呢，就会顺着执行下面的指令，也就是if块的内容。
最后我们来看jmp指令，这是无条件跳转指令，相当于我们前面学过的字节码中的goto指令。
认识了这三个指令以后，我们就知道程序的跳转逻辑了。在这个C语言的示例程序中，一共有四个基本块，我把它们之间的跳转关系画成了图，可以更加直观一些：
在分析清楚了整个思路以后，为if语句生成本地代码的逻辑也就很清楚了，我们现在就动手吧。完整的代码你可以查看代码库里的visitIfStmt方法，我这里挑重点和你分析一下。
首先，我们要生成4个基本块：
//条件 let bbCondition = this.getCurrentBB(); let compOprand = this.visit(ifStmt.condition) as Oprand; //if块 let bbIfBlcok = this.newBlock(); this.visit(ifStmt.stmt);
//else块 let bbElseBlock:BasicBlock|null = null if (ifStmt.elseStmt != null){ bbElseBlock = this.newBlock(); this.visit(ifStmt.elseStmt); }
//最后，要新建一个基本块,用于If后面的语句。 let bbFollowing = this.newBlock(); 接着，我们要添加跳转指令，在4个基本块之间建立正确的跳转关系。这其中，最关键的就是我们怎么来为基本块0，也就是if条件所在的基本块生成跳转指令。</description></item><item><title>19_Python编译器（三）：运行时机制</title><link>https://artisanbox.github.io/7/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/19/</guid><description>你好，我是宫文学。
在前面两讲中，我们已经分析了Python从开始编译到生成字节码的机制。但是，我们对Python只是了解了一半，还有很多问题需要解答。比如：Python字节码是如何运行的呢？它是如何管理程序所用到的数据的？它的类型体系是如何设计的，有什么特点？等等。
所以今天这一讲，我们就来讨论一下Python的运行时机制。其中的核心，是Python对象机制的设计。
我们先来研究一下字节码的运行机制。你会发现，它跟Python的对象机制密切相关。
理解字节码的执行过程我们用GDB跟踪执行一个简单的示例程序，它只有一行：“a=1”。
这行代码对应的字节码如下。其中，前两行指令实现了“a=1”的功能（后两行是根据Python的规定，在执行完一个模块之后，缺省返回一个None值）。
你需要在_PyEval_EvalFrameDefault()函数这里设置一个断点，在这里实际解释指令并执行。
首先是执行第一行指令，LOAD_CONST。
你会看到，解释器做了三件事情：
从常数表里取出0号常数。你知道，编译完毕以后会形成PyCodeObject，而在这个对象里会记录所有的常量、符号名称、本地变量等信息。常量1就是从它的常量表中取出来的。 把对象引用值加1。对象引用跟垃圾收集机制相关。 把这个常数对象入栈。 从这第一行指令的执行过程，你能得到什么信息呢？
第一个信息，常量1在Python内部，它是一个对象。你可以在GDB里显示这个对象的信息：该对象的类型是PyLong_Type型，这是Python的整型在内部的实现。
另外，该对象的引用数是126个，说明这个常量对象其实是被共享的，LOAD_CONST指令会让它的引用数加1。我们用的常数是1，这个值在Python内部也是会经常被用到，所以引用数会这么高。你可以试着选个不那么常见的常数，看看它的引用数是多少，都是在哪里被引用的。
进一步，我们会发现，往栈里放的数据，其实是个对象指针，而不像Java的栈机那样，是放了个整数。
总结上述信息，我其实可以告诉你一个结论：在Python里，程序中的任何符号都是对象，包括整数、浮点数这些基础数据，或者是自定义的类，或者是函数，它们都是对象。在栈机里处理的，是这些对象的引用。
我们再继续往下分析一条指令，也就是STORE_NAME指令，来加深一下对Python运行机制的理解。
执行STORE_NAME指令时，解释器做了5件事情：
根据指令的参数，从名称表里取出变量名称。这个名称表也是来自于PyCodeObject。前面我刚说过了，Python程序中的一切都是对象，那么name也是对象。你可以查看它的类型，是PyUnicode_Type，也就是Unicode的字符串。 从栈顶弹出上一步存进去的常量对象。 获取保存了所有本地变量的字典，这也是来自PyCodeObject。 在字典里，设置a的值为该常量。如果你深入跟踪其执行过程，你会发现在存入字典的时候，name对象和v对象的引用都会加1。这也是可以理解的，因为它们一个作为key，一个作为value，都要被字典所引用。 减少常量对象的引用计数。意思是栈机本身不再引用该常量。 好了，通过详细解读这两条指令的执行过程，我相信你对Python的运行机制摸到一点头绪了，但可能还是会提出很多问题来，比如说：
既然栈里的操作数都是对象指针，那么如何做加减乘除等算术运算？ 如果函数也是对象，那么执行函数的过程又是怎样的？ …… 别着急，我在后面会带你探究清楚这些问题。不过在此之前，我们有必要先加深一下对Python对象的了解。
Python对象的设计Python的对象定义在object.h中。阅读文件头部的注释和对各类数据结构的定义，你就可以理解Python对象的设计思路。
首先是PyObject和PyVarObject两个基础的数据结构，它们分别表示定长的数据和变长的数据。
typedef struct _object { //定长对象 Py_ssize_t ob_refcnt; //对象引用计数 struct _typeobject *ob_type; //对象类型 } PyObject; typedef struct { //变长对象 PyObject ob_base; Py_ssize_t ob_size; //变长部分的项目数量，在申请内存时有确定的值，不再变 } PyVarObject; PyObject是最基础的结构，所有的对象在Python内部都表示为一个PyObject指针。它里面只包含两个成员：对象引用计数（ob_refcnt）和对象类型（ob_type），你在用GDB跟踪执行时也见过它们。可能你会问，为什么只有这两个成员呢？对象的数据（比如一个整数）保存在哪里？
实际上，任何对象都会在一开头包含PyObject，其他数据都跟在PyObject的后面。比如说，Python3的整数的设计是一个变长对象，会用一到多个32位的段，来表示任意位数的整数：
#define PyObject_VAR_HEAD PyVarObject ob_base; struct _longobject { PyObject_VAR_HEAD //PyVarObject digit ob_digit[1]; //数字段的第一个元素 }; typedef struct _longobject PyLongObject; //整型 它在内存中的布局是这样的：</description></item><item><title>19_为什么我只查一行的语句，也执行这么慢？</title><link>https://artisanbox.github.io/1/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/19/</guid><description>一般情况下，如果我跟你说查询性能优化，你首先会想到一些复杂的语句，想到查询需要返回大量的数据。但有些情况下，“查一行”，也会执行得特别慢。今天，我就跟你聊聊这个有趣的话题，看看什么情况下，会出现这个现象。
需要说明的是，如果MySQL数据库本身就有很大的压力，导致数据库服务器CPU占用率很高或ioutil（IO利用率）很高，这种情况下所有语句的执行都有可能变慢，不属于我们今天的讨论范围。
为了便于描述，我还是构造一个表，基于这个表来说明今天的问题。这个表有两个字段id和c，并且我在里面插入了10万行记录。
mysql&amp;gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=100000) do insert into t values(i,i); set i=i+1; end while; end;; delimiter ;
call idata(); 接下来，我会用几个不同的场景来举例，有些是前面的文章中我们已经介绍过的知识点，你看看能不能一眼看穿，来检验一下吧。
第一类：查询长时间不返回如图1所示，在表t执行下面的SQL语句：
mysql&amp;gt; select * from t where id=1; 查询结果长时间不返回。
图1 查询长时间不返回一般碰到这种情况的话，大概率是表t被锁住了。接下来分析原因的时候，一般都是首先执行一下show processlist命令，看看当前语句处于什么状态。
然后我们再针对每种状态，去分析它们产生的原因、如何复现，以及如何处理。
等MDL锁&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;如图2所示，就是使用show processlist命令查看Waiting for table metadata lock的示意图。</description></item><item><title>19_土地不能浪费：如何管理内存对象？</title><link>https://artisanbox.github.io/9/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/19/</guid><description>你好，我是LMOS。
在前面的课程中，我们建立了物理内存页面管理器，它既可以分配单个页面，也可以分配多个连续的页面，还能指定在特殊内存地址区域中分配页面。
但你发现没有，物理内存页面管理器一次分配至少是一个页面，而我们对内存分页是一个页面4KB，即4096字节。对于小于一个页面的内存分配请求，它无能为力。如果要实现小于一个页面的内存分配请求，又该怎么做呢？
这节课我们就一起来解决这个问题。课程配套代码，你可以从这里获得。
malloc给我们的启发首先，我想和你说说，为什么小于一个页面的内存我们也要格外珍惜？
如果你在大学学过C程序设计语言的话，相信你对C库中的malloc函数也不会陌生，它负责完成分配一块内存空间的功能。
下面的代码。我相信你也写过，或者写过类似的，不用多介绍你也可以明白。
#include &amp;lt;stdio.h&amp;gt; #include &amp;lt;string.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; int main() { char *str; //内存分配 存放15个char字符类型 str = (char *) malloc(15); if (str == NULL) { printf(&amp;quot;mem alloc err\n&amp;quot;); return -1; } //把hello world字符串复制到str开始的内存地址空间中 strcpy(str, &amp;quot;hello world&amp;quot;); //打印hello world字符串和它的地址 printf(&amp;quot;String = %s, Address = %u\n&amp;quot;, str, str); //释放分配的内存 free(str); return(0); } 这个代码流程很简单，就是分配一块15字节大小的内存空间，然后把字符串复制到分配的内存空间中，最后用字符串的形式打印了那个块内存，最后释放该内存空间。
但我们并不是要了解malloc、free函数的工作原理，而是要清楚，像这样分配几个字节内存空间的操作，这在内核中比比皆是。
页还能细分吗是的，单从内存角度来看，页最小是以字节为单位的。但是从MMU角度看，内存是以页为单位的，所以我们的Cosmos的物理内存分配器也以页为单位。现在的问题是，内核中有大量远小于一个页面的内存分配请求，如果对此还是分配一个页面，就会浪费内存。
要想解决这个问题，就要细分“页”这个单位。虽然从MMU角度来看，页不能细分，但是从软件逻辑层面页可以细分，但是如何分，则十分讲究。
结合历史经验和硬件特性（Cache行大小）来看，我们可以把一个页面或者连续的多个页面，分成32字节、64字节、128字节、256字节、512字节、1024字节、2048字节、4096字节（一个页）。这些都是Cache行大小的倍数。我们给这些小块内存取个名字，叫内存对象。
我们可以这样设计：把一个或者多个内存页面分配出来，作为一个内存对象的容器，在这个容器中容纳相同的内存对象，即同等大小的内存块。你可以把这个容器，想像成一个内存对象数组。为了让你更好理解，我还给你画了张图解释。
如何表示一个内存对象前面只是进行了理论上的设计和构想，下面我们就通过代码来实现这些构想，真正把想法变成现实。
我们从内存对象开始入手。如何表示一个内存对象呢？当然是要设计一个表示内存对象的数据结构，代码如下所示：
typedef struct s_FREOBJH { list_h_t oh_list; //链表 uint_t oh_stus; //对象状态 void* oh_stat; //对象的开始地址 }freobjh_t; 我们在后面的代码中就用freobjh_t结构表示一个对象，其中的链表是为了找到这个对象。是不是很简单？没错，表示一个内存对象就是如此简单。</description></item><item><title>19_建立数据通路（下）：指令+运算=CPU</title><link>https://artisanbox.github.io/4/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/19/</guid><description>上一讲，我们讲解了时钟信号是怎么实现的，以及怎么利用这个时钟信号，来控制数据的读写，可以使得我们能把需要的数据“存储”下来。那么，这一讲，我们要让计算机“自动”跑起来。
通过一个时钟信号，我们可以实现计数器，这个会成为我们的PC寄存器。然后，我们还需要一个能够帮我们在内存里面寻找指定数据地址的译码器，以及解析读取到的机器指令的译码器。这样，我们就能把所有学习到的硬件组件串联起来，变成一个CPU，实现我们在计算机指令的执行部分的运行步骤。
PC寄存器所需要的计数器我们常说的PC寄存器，还有个名字叫程序计数器。下面我们就来看看，它为什么叫作程序计数器。
有了时钟信号，我们可以提供定时的输入；有了D型触发器，我们可以在时钟信号控制的时间点写入数据。我们把这两个功能组合起来，就可以实现一个自动的计数器了。
加法器的两个输入，一个始终设置成1，另外一个来自于一个D型触发器A。我们把加法器的输出结果，写到这个D型触发器A里面。于是，D型触发器里面的数据就会在固定的时钟信号为1的时候更新一次。
这样，我们就有了一个每过一个时钟周期，就能固定自增1的自动计数器了。这个自动计数器，可以拿来当我们的PC寄存器。事实上，PC寄存器的这个PC，英文就是Program Counter，也就是程序计数器的意思。
每次自增之后，我们可以去对应的D型触发器里面取值，这也是我们下一条需要运行指令的地址。前面第5讲我们讲过，同一个程序的指令应该要顺序地存放在内存里面。这里就和前面对应上了，顺序地存放指令，就是为了让我们通过程序计数器就能定时地不断执行新指令。
加法计数、内存取值，乃至后面的命令执行，最终其实都是由我们一开始讲的时钟信号，来控制执行时间点和先后顺序的，这也是我们需要时序电路最核心的原因。
在最简单的情况下，我们需要让每一条指令，从程序计数，到获取指令、执行指令，都在一个时钟周期内完成。如果PC寄存器自增地太快，程序就会出错。因为前一次的运算结果还没有写回到对应的寄存器里面的时候，后面一条指令已经开始读取里面的数据来做下一次计算了。这个时候，如果我们的指令使用同样的寄存器，前一条指令的计算就会没有效果，计算结果就错了。
在这种设计下，我们需要在一个时钟周期里，确保执行完一条最复杂的CPU指令，也就是耗时最长的一条CPU指令。这样的CPU设计，我们称之为单指令周期处理器（Single Cycle Processor）。
很显然，这样的设计有点儿浪费。因为即便只调用一条非常简单的指令，我们也需要等待整个时钟周期的时间走完，才能执行下一条指令。在后面章节里我们会讲到，通过流水线技术进行性能优化，可以减少需要等待的时间，这里我们暂且说到这里。
读写数据所需要的译码器现在，我们的数据能够存储在D型触发器里了。如果我们把很多个D型触发器放在一起，就可以形成一块很大的存储空间，甚至可以当成一块内存来用。像我现在手头这台电脑，有16G内存。那我们怎么才能知道，写入和读取的数据，是在这么大的内存的哪几个比特呢？
于是，我们就需要有一个电路，来完成“寻址”的工作。这个“寻址”电路，就是我们接下来要讲的译码器。
在现在实际使用的计算机里面，内存所使用的DRAM，并不是通过上面的D型触发器来实现的，而是使用了一种CMOS芯片来实现的。不过，这并不影响我们从基础原理方面来理解译码器。在这里，我们还是可以把内存芯片，当成是很多个连在一起的D型触发器来实现的。
如果把“寻址”这件事情退化到最简单的情况，就是在两个地址中，去选择一个地址。这样的电路，我们叫作2-1选择器。我把它的电路实现画在了这里。
我们通过一个反相器、两个与门和一个或门，就可以实现一个2-1选择器。通过控制反相器的输入是0还是1，能够决定对应的输出信号，是和地址A，还是地址B的输入信号一致。
2-1选择器电路示意图一个反向器只能有0和1这样两个状态，所以我们只能从两个地址中选择一个。如果输入的信号有三个不同的开关，我们就能从$2^3$，也就是8个地址中选择一个了。这样的电路，我们就叫3-8译码器。现代的计算机，如果CPU是64位的，就意味着我们的寻址空间也是$2^{64}$，那么我们就需要一个有64个开关的译码器。
当我们把译码器和内存连到一起时，通常会组成这样一个电路所以说，其实译码器的本质，就是从输入的多个位的信号中，根据一定的开关和电路组合，选择出自己想要的信号。除了能够进行“寻址”之外，我们还可以把对应的需要运行的指令码，同样通过译码器，找出我们期望执行的指令，也就是在之前我们讲到过的opcode，以及后面对应的操作数或者寄存器地址。只是，这样的“译码器”，比起2-1选择器和3-8译码器，要复杂的多。
建立数据通路，构造一个最简单的CPUD触发器、自动计数以及译码器，再加上一个我们之前说过的ALU，我们就凑齐了一个拼装一个CPU必须要的零件了。下面，我们就来看一看，怎么把这些零件组合起来，才能实现指令执行和算术逻辑计算的CPU。
CPU实现的抽象逻辑图 首先，我们有一个自动计数器。这个自动计数器会随着时钟主频不断地自增，来作为我们的PC寄存器。 在这个自动计数器的后面，我们连上一个译码器。译码器还要同时连着我们通过大量的D触发器组成的内存。 自动计数器会随着时钟主频不断自增，从译码器当中，找到对应的计数器所表示的内存地址，然后读取出里面的CPU指令。 读取出来的CPU指令会通过我们的CPU时钟的控制，写入到一个由D触发器组成的寄存器，也就是指令寄存器当中。 在指令寄存器后面，我们可以再跟一个译码器。这个译码器不再是用来寻址的了，而是把我们拿到的指令，解析成opcode和对应的操作数。 当我们拿到对应的opcode和操作数，对应的输出线路就要连接ALU，开始进行各种算术和逻辑运算。对应的计算结果，则会再写回到D触发器组成的寄存器或者内存当中。 这样的一个完整的通路，也就完成了我们的CPU的一条指令的执行过程。在这个过程中，你会发现这样几个有意思的问题。
第一个，是我们之前在第6讲讲过的程序跳转所使用的条件码寄存器。那时，讲计算机的指令执行的时候，我们说高级语言中的if…else，其实是变成了一条cmp指令和一条jmp指令。cmp指令是在进行对应的比较，比较的结果会更新到条件码寄存器当中。jmp指令则是根据条件码寄存器当中的标志位，来决定是否进行跳转以及跳转到什么地址。
不知道你当时看到这个知识点的时候，有没有一些疑惑，为什么我们的if…else会变成这样两条指令，而不是设计成一个复杂的电路，变成一条指令？到这里，我们就可以解释了。这样分成两个指令实现，完全匹配好了我们在电路层面，“译码-执行-更新寄存器“这样的步骤。
cmp指令的执行结果放到了条件码寄存器里面，我们的条件跳转指令也是在ALU层面执行的，而不是在控制器里面执行的。这样的实现方式在电路层面非常直观，我们不需要一个非常复杂的电路，就能实现if…else的功能。
第二个，是关于我们在第17讲里讲到的指令周期、CPU周期和时钟周期的差异。在上面的抽象的逻辑模型中，你很容易发现，我们执行一条指令，其实可以不放在一个时钟周期里面，可以直接拆分到多个时钟周期。
我们可以在一个时钟周期里面，去自增PC寄存器的值，也就是指令对应的内存地址。然后，我们要根据这个地址从D触发器里面读取指令，这个还是可以在刚才那个时钟周期内。但是对应的指令写入到指令寄存器，我们可以放在一个新的时钟周期里面。指令译码给到ALU之后的计算结果，要写回到寄存器，又可以放到另一个新的时钟周期。所以，执行一条计算机指令，其实可以拆分到很多个时钟周期，而不是必须使用单指令周期处理器的设计。
因为从内存里面读取指令时间很长，所以如果使用单指令周期处理器，就意味着我们的指令都要去等待一些慢速的操作。这些不同指令执行速度的差异，也正是计算机指令有指令周期、CPU周期和时钟周期之分的原因。因此，现代我们优化CPU的性能时，用的CPU都不是单指令周期处理器，而是通过流水线、分支预测等技术，来实现在一个周期里同时执行多个指令。
总结延伸好了，今天我们讲完了，怎么通过连接不同功能的电路，实现出一个完整的CPU。
我们可以通过自动计数器的电路，来实现一个PC寄存器，不断生成下一条要执行的计算机指令的内存地址。然后通过译码器，从内存里面读出对应的指令，写入到D触发器实现的指令寄存器中。再通过另外一个译码器，把它解析成我们需要执行的指令和操作数的地址。这些电路，组成了我们计算机五大组成部分里面的控制器。
我们把opcode和对应的操作数，发送给ALU进行计算，得到计算结果，再写回到寄存器以及内存里面来，这个就是我们计算机五大组成部分里面的运算器。
我们的时钟信号，则提供了协调这样一条条指令的执行时间和先后顺序的机制。同样的，这也带来了一个挑战，那就是单指令周期处理器去执行一条指令的时间太长了。而这个挑战，也是我们接下来的几讲里要解答的问题。
推荐阅读《编码：隐匿在计算机软硬件背后的语言》的第17章，用更多细节的流程来讲解了CPU的数据通路。《计算机组成与设计 硬件/软件接口》的4.1到4.4小节，从另外一个层面和角度讲解了CPU的数据通路的建立，推荐你阅读一下。
课后思考CPU在执行无条件跳转的时候，不需要通过运算器以及ALU，可以直接在控制器里面完成，你能说说这是为什么吗？
欢迎在留言区写下你的思考和疑惑，你也可以把今天的内容分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>19_散列表（中）：如何打造一个工业级水平的散列表？</title><link>https://artisanbox.github.io/2/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/20/</guid><description>通过上一节的学习，我们知道，散列表的查询效率并不能笼统地说成是O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。
在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果我们使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从O(1)急剧退化为O(n)。
如果散列表中有10万个数据，退化后的散列表查询的效率就下降了10万倍。更直接点说，如果之前运行100次查询只需要0.1秒，那现在就需要1万秒。这样就有可能因为查询操作消耗大量CPU或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就是散列表碰撞攻击的基本原理。
今天，我们就来学习一下，如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？
如何设计散列函数？散列函数设计的好坏，决定了散列表冲突的概率大小，也直接决定了散列表的性能。那什么才是好的散列函数呢？
首先，散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接地影响到散列表的性能。其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。
实际工作中，我们还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。散列函数各式各样，我举几个常用的、简单的散列函数的设计方法，让你有个直观的感受。
第一个例子就是我们上一节的学生运动会的例子，我们通过分析参赛编号的特征，把编号中的后两位作为散列值。我们还可以用类似的散列函数处理手机号码，因为手机号码前几位重复的可能性很大，但是后面几位就比较随机，我们可以取手机号的后四位作为散列值。这种散列函数的设计方法，我们一般叫做“数据分析法”。
第二个例子就是上一节的开篇思考题，如何实现Word拼写检查功能。这里面的散列函数，我们就可以这样设计：将单词中每个字母的ASCll码值“进位”相加，然后再跟散列表的大小求余、取模，作为散列值。比如，英文单词nice，我们转化出来的散列值就是下面这样：
hash(&amp;quot;nice&amp;quot;)=((&amp;quot;n&amp;quot; - &amp;quot;a&amp;quot;) * 26*26*26 + (&amp;quot;i&amp;quot; - &amp;quot;a&amp;quot;)*26*26 + (&amp;quot;c&amp;quot; - &amp;quot;a&amp;quot;)*26+ (&amp;quot;e&amp;quot;-&amp;quot;a&amp;quot;)) / 78978 实际上，散列函数的设计方法还有很多，比如直接寻址法、平方取中法、折叠法、随机数法等，这些你只要了解就行了，不需要全都掌握。
装载因子过大了怎么办？我们上一节讲到散列表的装载因子的时候说过，装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。
对于没有频繁插入和删除的静态数据集合来说，我们很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数，因为毕竟之前数据都是已知的。
对于动态散列表来说，数据集合是频繁变动的，我们事先无法预估将要加入的数据个数，所以我们也无法事先申请一个足够大的散列表。随着数据慢慢加入，装载因子就会慢慢变大。当装载因子大到一定程度之后，散列冲突就会变得不可接受。这个时候，我们该如何处理呢？
还记得我们前面多次讲的“动态扩容”吗？你可以回想一下，我们是如何做数组、栈、队列的动态扩容的。
针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了0.4。
针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。
你可以看我图里这个例子。在原来的散列表中，21这个元素原来存储在下标为0的位置，搬移到新的散列表中，存储在下标为7的位置。
对于支持动态扩容的散列表，插入操作的时间复杂度是多少呢？前面章节我已经多次分析过支持动态扩容的数组、栈等数据结构的时间复杂度了。所以，这里我就不啰嗦了，你要是还不清楚的话，可以回去复习一下。
插入一个数据，最好情况下，不需要扩容，最好时间复杂度是O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是O(1)。
实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。
我们前面讲到，当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。
装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于1。
如何避免低效的扩容？我们刚刚分析得到，大部分情况下，动态扩容的散列表插入一个数据都很快，但是在特殊情况下，当装载因子已经到达阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢，甚至会无法接受。
我举一个极端的例子，如果散列表当前大小为1GB，要想扩容为原来的两倍大小，那就需要对1GB的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表，听起来就很耗时，是不是？
如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，“一次性”扩容的机制就不合适了。
为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。
当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。
这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。
通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是O(1)。
如何选择冲突解决方法？上一节我们讲了两种主要的散列冲突的解决办法，开放寻址法和链表法。这两种冲突解决办法在实际的软件开发中都非常常用。比如，Java中LinkedHashMap就采用了链表法解决冲突，ThreadLocalMap是通过线性探测的开放寻址法来解决冲突。那你知道，这两种冲突解决方法各有什么优势和劣势，又各自适用哪些场景吗？
1.开放寻址法我们先来看看，开放寻址法的优点有哪些。
开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用CPU缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。你可不要小看序列化，很多场合都会用到的。我们后面就有一节会讲什么是数据结构序列化、如何序列化，以及为什么要序列化。
我们再来看下，开放寻址法有哪些缺点。
上一节我们讲到，用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。
所以，我总结一下，当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。
2.链表法首先，链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。
链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于1的情况。接近1时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。
还记得我们之前在链表那一节讲的吗？链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对CPU缓存是不友好的，这方面对于执行效率也有一定的影响。
当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4个字节或者8个字节），那链表中指针的内存消耗在大对象面前就可以忽略了。
实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。
所以，我总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。
工业级散列表举例分析刚刚我讲了实现一个工业级散列表需要涉及的一些关键技术，现在，我就拿一个具体的例子，Java中的HashMap这样一个工业级的散列表，来具体看下，这些技术是怎么应用的。
1.初始大小HashMap默认的初始大小是16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高HashMap的性能。
2.装载因子和动态扩容最大装载因子默认是0.75，当HashMap中元素个数超过0.75*capacity（capacity表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。
3.散列冲突解决方法HashMap底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。
于是，在JDK1.8版本中，为了对HashMap做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高HashMap的性能。当红黑树结点个数少于8个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。
4.散列函数散列函数的设计并不复杂，追求的是简单高效、分布均匀。我把它摘抄出来，你可以看看。
int hash(Object key) { int h = key.</description></item><item><title>19_日志（上）：系统出现问题，如何及时发现？</title><link>https://artisanbox.github.io/8/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/19/</guid><description>你好，我是朱晓峰。
我们曾经开发过一个数据库应用系统，但是却突然遭遇了数据库宕机。在这种情况下，定位宕机的原因就非常关键，毕竟，知道了问题，才能确定解决方案。
这时，我们就想到了查看数据库的错误日志，因为日志中记录了数据库运行中的诊断信息，包括了错误、警告和注释等信息。从日志中，我们发现，原来某个连接中的SQL操作发生了死循环，导致内存不足，被系统强行终止了。知道了原因，处理起来也就比较轻松了，系统很快就恢复了运行。
除了发现错误，日志在数据复制、数据恢复、操作审计，以及确保数据的永久性和一致性等方面，都有着不可替代的作用，对提升你的数据库应用的开发能力至关重要。
今天，我就结合超市项目的实际案例，给你讲解一下怎么通过查看系统日志，来了解数据库中实际发生了什么，从而快速定位原因。
MySQL的日志种类非常多，包括通用查询日志、慢查询日志、错误日志、二进制日志、中继日志、重做日志和回滚日志，内容比较多，而且都很重要，所以我们来花两节课的时间学习一下。
这节课，我会先具体讲一讲通用查询日志、慢查询日志和错误日志。
通用查询日志通用查询日志记录了所有用户的连接开始时间和截止时间，以及发给MySQL数据库服务器的所有SQL指令。当我们的数据发生异常时，开启通用查询日志，还原操作时的具体场景，可以帮助我们准确定位问题。
举个小例子，在超市项目实施的过程中，我们曾遇到过这样一件事：超市经营者月底查账的时候发现，超市的1号门店在12月1日销售了5件化妆品，但是当天对应的历史库存并没有减少。化妆品的金额都比较大，库存不对的话，会在报表查询中产生巨额差异，触发到报警机制，对超市经营者的决策产生影响。超市经营者找到我们，对系统的可靠性提出质疑。
我们对系统进行了仔细检查，没有发现数据问题。可是商品确实卖出去了，当天的历史库存也确实没有消减。这个时候，我们想到了检查通用查询日志，看看当天到底发生了什么。
查看之后，我们就复原了当天的情况：12月1日下午，门店的收银台销售了5件化妆品，但是由于网络故障，流水没有及时上传到总部。12月1日晚上11:59，总部的历史库存被保存下来，但是因为没有收到门店的流水，所以没有消减库存。12月2日上午，门店的网络恢复了，流水得以上传总部，这个时候，对应化妆品的库存才被消减掉。
这样，我们就确定了故障的原因，也就是超市的网络问题，而系统本身是没有问题的。
你看，通用查询日志可以帮助我们了解操作发生的具体时间和操作的细节，对找出异常发生的原因极其关键。
下面我来具体介绍一下控制通用查询日志的系统变量。通过这些变量，你会清楚怎么控制通用查询日志的开启和关闭，以及保存日志的文件是哪个。
mysql&amp;gt; SHOW VARIABLES LIKE '%general%'; +------------------+---------------+ | Variable_name | Value | +------------------+---------------+ | general_log | OFF | -- 通用查询日志处于关闭状态 | general_log_file | GJTECH-PC.log | -- 通用查询日志文件的名称是GJTECH-PC.log +------------------+---------------+ 2 rows in set, 1 warning (0.00 sec) 在这个查询的结果中，有2点需要我们注意一下。
系统变量general_log的值是OFF，表示通用查询日志处于关闭状态。在MySQL中，这个参数的默认值是关闭的。因为一旦开启记录通用查询日志，MySQL会记录所有的连接起止和相关的SQL操作，这样会消耗系统资源并且占用磁盘空间。我们可以通过手动修改变量的值，在需要的时候开启日志。 通用查询日志文件的名称是GJTECH-PC.log。这样我们就知道在哪里可以查看通用查询日志的内容了。 下面我们来看看如何开启通用查询日志，把所有连接的起止和连接的SQL操作都记录下来。这个操作可以帮助我们追踪SQL操作故障的原因。
开启通用查询日志我们可以通过设置系统变量的值，来开启通用查询日志，并且指定通用查询日志的文件夹和文件名为“H:\mytest.log”。这个操作如下：
mysql&amp;gt; SET GLOBAL general_log = 'ON'; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SET @@global.</description></item><item><title>19_案例总结与热点问题答疑：对于左递归的语法，为什么我的推导不是左递归的？</title><link>https://artisanbox.github.io/6/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/19/</guid><description>目前为止，“编译原理”的前端部分已经讲完了，你学到现在，感受如何呢？
不得不说，订阅这门课程的同学，都是很有追求的。因为编译原理这门课，肯定给你的学习生涯多多少少地带来过“伤害”，你现在有勇气重拾“编译原理”，下决心将它攻克，本身就是一种有追求的表现。
在课程开始之初，很多同学当场立下（入）了Flag（坑），比如：
@andylo25：立下Flag，想写一个解释性语言。
@陈越 ：许诺会跟着学完。
@许。：强调自己因为面试华为来学习编译原理。
……
还有同学认为自己半路出家，为了长远的发展，一定要补好基本功。要我说，乔布斯还是辍学加半路出家的呢，终生学习是互联网时代的常态：
@一只豪猪 ：半路出家的野路子码农来补课了。
……
在准备课程的过程中，我努力把晦涩的知识点变得通俗易懂，希望得到你的认可。当我在留言区看到一些留言时，我的内心是欣慰的，也是欣喜的：
@许童童：之前看到词法分析什么的就是一脸蒙，看了老师的文章，醍醐灌顶。
@VVK：老师讲的太好了，十几年没搞懂的概念终于整理明白了。
……
与此同时，我也在不断优化课程，力求将内容做到深入浅出，比如，在策划算法篇的内容时，我吸取一些同学的建议，尽可能画成可视化的图形，并且让整个算法的推导过程很直观地呈现。
但是我不能回避一个事实，就是即便这些内容你认为很好，但你要想学好编译原理，还是要花费不少精力将这些内容反复地看上几遍。你需要认真跟上课程的思路和进程，用心思考和实践，才会有所得，单看内容不动手尝试是没办法学为所用的。所以，在这里，我想表扬一些有耐心，愿意尝试的同学，比如@曾经瘦过@Unrestrained@周小明@Sam 当然，还有很多同学在一直坚持，我为你们点赞！
而且，我发现，很多同学还有探知和质疑精神，比如，@沉淀的梦想 发现我在示例代码里用的都是左值，也跟我讨论在实现闭包的时候，如何仍然正常访问全局变量。@mcuking 指出JavaScript的ES6版本已经支持块作用域 @李梁|东大 也与我讨论了关于C++ auto变量的类型推导等等。
我知道大部分同学的时间很紧，但我感谢你们的坚持，感谢你们在努力抽时间动手实践，比如@Smallfly 自己动手写规则；@曾经瘦过 再次动手跟着敲代码。
还有很多同学花了很多时间，用自己熟悉的语言，参照课程的示例代码重写了词法分析器、语法分析器，并分享了代码：
@（——_ ——)：写了一晚上，终于用C语言模仿实现了第二节课的内容。
@windpiaoxue：也做了一个C语言实现。
……
其他有Go语言的（@catplanet）、Swift语言的（@Smallfly@Rockbean@贾献华）、C++语言的（@阿尔伯特@中年男子@蛋黄儿）、TypeScript的（@缺个豆饼吗@好吃的呆梨）、PHP的（@吴军旗）等等，我通常都会编译并运行一下。
@catplanet 甚至提供了一个界面，可以通过浏览器调用自己写的编译程序，运行并显示结果。
@京京beaver 还分享了在Windows环境下如何做Antlr的配置，让其他同学可以更顺畅地运行Antlr。
@knull 建议我在写BNF的时候，用到+号Token要带上引号，避免跟原来BNF表示重复1到多次的+号冲突。
@kaixiao7 提醒我在Windows下，EOF是用Ctl+z输入。
我对你们取得的成果以及建议感到由衷的高兴和感谢，我相信，你们的分享也激励了其他同学克服困难，继续前进！
当然了，你在学习的过程中，还会遇到一些问题，我很感谢提问题的同学。其中一些问题，我认为是比较典型，有通用意义的，所以选了4个典型的问题，再带你详细地探究一下。
问题一：对于左递归的语法，为什么我的推导不是左递归的？
这个问题本身反映了，进行递归下降分析的时候，如何保持清晰的思路，值得讲一讲。
在03讲，我们刚开始接触到语法分析，也刚开始接触递归下降算法。这时，我介绍了左递归的概念，但你可能在实际推导的过程中，觉得不是左递归，比如用下面这个语法，来推导“2+3”这个简单的表达式：
//简化的左递归文法 add-&amp;gt;Int add-&amp;gt;add + Int 你可能会拿第一个产生式做推导：
add-&amp;gt;2
成功返回
因为没有采用第二条产生式，所以不会触发递归调用。但这里的问题是，“2+3”是一个加法表达式，2也是一个合法的加法表达式，但仅仅解析出2是不行的，我们必须完整地解析出“2+3”来。
在17讲，我提到，任何自顶向下的算法，都是在一个大的图里找到一条搜索路径的过程。最后的结果，是经过多次推导，生成跟输入的Token串相同的结果，解析完毕以后，所有Token也耗光。
如果只匹配上2，那就证明这条搜索路径是错误的，我们必须尝试另一种可能性，也就是第二个产生式。
要找到正确的搜索路径，在递归下降算法或者LL算法时，我们都是采用“贪婪”策略，这个策略在16讲关于正则表达式时讲过。也就是要匹配尽量多的Token才可以。就算是换成右递归的文法，也不能采用第一个产生式。因为解析完Int以后，接下来的Token是+号，还可以尝试用第二个产生式，那我们就要启动贪婪策略，用第二个，而不是第一个。
//简化的右递归文法 add-&amp;gt;Int add-&amp;gt;Int + add 以上是第一种情况。</description></item><item><title>19｜怎么实现一个更好的寄存器分配算法：原理篇</title><link>https://artisanbox.github.io/3/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/21/</guid><description>你好，我是宫文学。
到目前为止，我们的语言已经能够生成机器码了，并且性能确实还挺高的。不过我们也知道，现在我们采用的寄存器分配算法呀，还是很初级的。
那这个初级的寄存器分配算法会遇到什么问题呢？我们还有更优化的分配寄存器的思路吗？
当然是有的。接下来的这两节课，我们就会来回答这两个问题，我会带你从原理到实操，理解和实现一个更好的算法，叫做线性扫描算法，让寄存器的分配获得更好的优化效果。
首先，我们来分析一下当前寄存器分配算法的局限性。
初级算法的不足在前两节课中，我们实现了一个初级的寄存器分配算法。这个算法的特点呢，是主要的数据都保存在内存的栈桢中，包括参数和本地变量。而临时变量，则是映射到寄存器，从而保证各类运算指令的合法性，因为像加减乘数这种运算，不能两个操作数都是内存地址。
这个算法有什么不足呢？你可以暂停一会儿，先自己想一下，大概有两点。
我们现在来揭晓答案。
第一点不足在生成的代码性能上。
你知道，我们做编译的目标，是要让生成的代码的性能最高，但这个算法在这方面显然是不合格的。因为参数和本地变量都是从内存中访问的，这会导致代码的性能大大降低。
第二点不足就在对需要Caller保护的寄存器的处理上。
在上一节课后面的性能比拼中，我们发现，其实我们自己的语言编译生成的可执行程序，它的性能还略低于C语言生成的、同样未经优化的版本，按理说它们的性能应该是一样的才对。
深究原因，还是在调用函数的时候，程序需要保存那些需要Caller保护的寄存器。而我们的算法，多保护了一些其实已经不需要被保护的寄存器，从而拖累了性能。
不过，这两个方面的局限性，我们通过今天的算法，都可以很好地解决。我们现在就通过一个示例程序来找一下更好的寄存器分配算法的思路。
寄存器分配算法的改进思路你先看看我们下面这个示例程序：
function foo(p1:number,p2:number,p3:number,p4:number,p5:number,p6:number){ let x7 = p1; let x8 = p2; let x9 = p3; let x10 = p4; let x11 = p5; let x12 = p6 + x7 + x8 + x9 + x10 + x11; let sum = x12; for (let i:number = 0; i&amp;amp;lt; 10000; i++){ sum += i; } return sum; } 你看这里有p1~p6共6个参数，还有x7~x12这6个本地变量。但在变量x12的计算过程中，我们还需要用到1个临时变量t1。接下来是一个循环语句，这个语句又涉及到sum和i两个本地变量。</description></item><item><title>20_JavaScript编译器（一）：V8的解析和编译过程</title><link>https://artisanbox.github.io/7/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/20/</guid><description>你好，我是宫文学。从这一讲开始，我们就进入另一个非常重要的编译器：V8编译器。
V8是谷歌公司在2008年推出的一款JavaScript编译器，它也可能是世界上使用最广泛的编译器。即使你不是编程人员，你每天也会运行很多次V8，因为JavaScript是Web的语言，我们在电脑和手机上浏览的每个页面，几乎都会运行一点JavaScript脚本。
扩展：V8这个词，原意是8缸的发动机，换算成排量，大约是4.0排量，属于相当强劲的发动机了。它的编译器，叫做Ignition，是点火装置的意思。而它最新的JIT编译器，叫做TurboFan，是涡轮风扇发动机的意思。
在浏览器诞生的早期，就开始支持JavaScript了。但在V8推出以后，它重新定义了Web应用可以胜任的工作。到今天，在浏览器里，我们可以运行很多高度复杂的应用，比如办公套件等，这些都得益于以V8为代表的JavaScript引擎的进步。2008年V8发布时，就已经比当时的竞争对手快10倍了；到目前，它的速度又已经提升了10倍以上。从中你可以看到，编译技术有多大的潜力可挖掘！
对JavaScript编译器来说，它最大的挑战就在于，当我们打开一个页面的时候，源代码的下载、解析（Parse）、编译（Compile）和执行，都要在很短的时间内完成，否则就会影响到用户的体验。
那么，V8是如何做到既编译得快，又要运行得快的呢？所以接下来，我将会花两讲的时间，来带你一起剖析一下V8里面的编译技术。在这个过程中，你能了解到V8是如何完成前端解析、后端优化等功能的，它都有哪些突出的特点；另外，了解了V8的编译原理，对你以后编写更容易优化的程序，也会非常有好处。
今天这一讲，我们先来透彻了解一下V8的编译过程，以及每个编译阶段的工作原理，看看它跟我们已经了解的其他编译器相比，有什么不同。
初步了解V8首先，按照惯例，我们肯定要下载V8的源代码。按照官方文档中的步骤，你可以下载源代码，并在本地编译。注意，你最好把它编译成Debug模式，这样便于用调试工具去跟踪它的执行，所以你要使用下面的命令来进行编译。
tools/dev/gm.py x64.debug 编译完毕以后，进入v8/out/x64.debug目录，你可以运行./d8，这就是编译好的V8的命令行工具。如果你用过Node.js，那么d8的使用方法，其实跟它几乎是完全一样的，因为Node.js就封装了一个V8引擎。你还可以用GDB或LLDB工具来调试d8，这样你就可以知道，它是怎么编译和运行JavaScript程序了。
而v8/src目录下的，就是V8的源代码了。V8是用C++编写的。你可以重点关注这几个目录中的代码，它们是与编译有关的功能，而别的代码主要是运行时功能：
V8的编译器的构成跟Java的编译器很像，它们都有从源代码编译到字节码的编译器，也都有解释器（叫Ignition），也都有JIT编译器（叫TurboFan）。你可以看下V8的编译过程的图例。在这个图中，你能注意到两个陌生的节点：流处理节点（Stream）和预解析器（PreParser），这是V8编译过程中比较有特色的两个处理阶段。
图1：V8的编译过程注意：这是比较新的V8版本的架构。在更早的版本里，有时会用到两个JIT编译器，类似于HotSpot的C1和C2，分别强调编译速度和优化效果。在更早的版本里，还没有字节码解释器。现在的架构，引入了字节码解释器，其速度够快，所以就取消了其中一级的JIT编译器。
下面我们就进入到V8编译过程中的各个阶段，去了解一些编译器的细节。
超级快的解析过程（词法分析和语法分析）首先，我们来了解一下V8解析源代码的过程。我在开头就已经说过，V8解析源代码的速度必须要非常快才行。源代码边下载边解析完毕，在这个过程中，用户几乎感觉不到停顿。那它是如何实现的呢？
有两篇文章就非常好地解释了V8解析速度快的原因。
一个是“optimizing the scanner”这篇文章，它解释了V8在词法分析上做的优化。V8的作者们真是锱铢必较地在每一个可能优化的步骤上去做优化，他们所采用的技术很具备参考价值。
那我就按照我对这篇文章的理解，来给你解释一下V8解析速度快的原因吧：
第一个原因，是V8的整个解析过程是流（Stream）化的，也就是一边从网络下载源代码，一边解析。在下载后，各种不同的编码还被统一转化为UTF-16编码单位，这样词法解析器就不需要处理多种编码了。
第二个原因，是识别标识符时所做的优化，这也让V8的解析速度更快了一点。你应该知道，标识符的第一个字符（ID_START）只允许用字母、下划线和$来表示，而之后的字符（ID_CONTINUE）还可以包括数字。所以，当词法解析器遇到一个字符的时候，我们首先要判断它是否是合法的ID_START。
那么，这样一个逻辑，通常你会怎么写？我一般想也不想，肯定是这样的写法：
if(ch &amp;gt;= 'A' &amp;amp;&amp;amp; ch &amp;lt;= 'Z' || ch &amp;gt;='a' &amp;amp;&amp;amp; ch&amp;lt;='z' || ch == '$' || ch == '_'){ return true; } 但你要注意这里的一个问题，if语句中的判断条件需要做多少个运算？
最坏的情况下，要做6次比较运算和3次逻辑“或”运算。不过，V8的作者们认为这太奢侈了。所以他们通过查表的方法，来识别每个ASCII字符是否是合法的标识符开头字符。
这相当于准备了一张大表，每个字符在里面对应一个位置，标明了该字符是否是合法的标识符开头字符。这是典型的牺牲空间来换效率的方法。虽然你在阅读代码的时候，会发现它调用了几层函数来实现这个功能，但这些函数其实是内联的，并且在编译优化以后，产生的指令要少很多，所以这个方法的性能更高。
第三个原因，是如何从标识符中挑出关键字。
与Java的编译器一样，JavaScript的Scanner，也是把标识符和关键字一起识别出来，然后再从中挑出关键字。所以，你可以认为这是一个最佳实践。那你应该也会想到，识别一个字符串是否是关键字的过程，使用的方法仍然是查表。查表用的技术是“完美哈希（perfect hashing）”，也就是每个关键字对应的哈希值都是不同的，不会发生碰撞。并且，计算哈希值只用了三个元素：前两个字符（ID_START、ID_CONTINUE），以及字符串的长度，不需要把每个字符都考虑进来，进一步降低了计算量。
文章里还有其他细节，比如通过缩窄对Unicode字符的处理范围来进行优化，等等。从中你能体会到V8的作者们在提升性能方面，无所不用其极的设计思路。
除了词法分析，在语法分析方面，V8也做了很多的优化来保证高性能。其中，最重要的是“懒解析”技术（lazy parsing）。
一个页面中包含的代码，并不会马上被程序用到。如果在一开头就把它们全部解析成AST并编译成字节码，就会产生很多开销：占用了太多CPU时间；过早地占用内存；编译后的代码缓存到硬盘上，导致磁盘IO的时间很长，等等。
所以，所有浏览器中的JavaScript编译器，都采用了懒解析技术。在V8里，首先由预解析器，也就是Preparser粗略地解析一遍程序，在正式运行某个函数的时候，编译器才会按需解析这个函数。你要注意，Preparser只检查语法的正确性，而基于上下文的检查则不是这个阶段的任务。你如果感兴趣的话，可以深入阅读一下这篇介绍Preparser的文章，我在这里就不重复了。
你可以在终端测试一下懒解析和完整解析的区别。针对foo.js示例程序，你输入“./d8 – ast-print foo.js”命令。
function add(a,b){ return a + b; } //add(1,2) //一开始，先不调用add函数 得到的输出结果是：</description></item><item><title>20_土地需求扩大与保障：如何表示虚拟内存？</title><link>https://artisanbox.github.io/9/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/20/</guid><description>你好，我是LMOS。
在现实中，有的人需要向政府申请一大块区域，在这块区域中建楼办厂，但是土地有限且已经被占用。所以可能的方案是，只给你分配一个总的面积区域，今年湖北有空地就在湖北建立一部分厂房，明年广东有空地就在广东再建另一部分厂房，但是总面积不变。
其实在计算机系统中也有类似的情况，一个应用往往拥有很大的连续地址空间，并且每个应用都是一样的，只有在运行时才能分配到真正的物理内存，在操作系统中这称为虚拟内存。
那问题来了，操作系统要怎样实现虚拟内存呢？由于内容比较多，我会用两节课的时间带你解决这个问题。今天这节课，我们先进行虚拟地址空间的划分，搞定虚拟内存数据结构的设计。下节课再动手实现虚拟内存的核心功能。
好，让我们进入正题，先从虚拟地址空间的划分入手，配套代码你可以从这里获得。
虚拟地址空间的划分虚拟地址就是逻辑上的一个数值，而虚拟地址空间就是一堆数值的集合。通常情况下，32位的处理器有0～0xFFFFFFFF的虚拟地址空间，而64位的虚拟地址空间则更大，有0～0xFFFFFFFFFFFFFFFF的虚拟地址空间。
对于如此巨大的地址空间，我们自然需要一定的安排和设计，比如什么虚拟地址段放应用，什么虚拟地址段放内核等。下面我们首先看看处理器硬件层面的划分，再来看看在此基础上我们系统软件层面是如何划分的。
x86 CPU如何划分虚拟地址空间我们Cosmos工作在x86 CPU上，所以我们先来看看x86 CPU是如何划分虚拟地址空间的。
由于x86 CPU支持虚拟地址空间时，要么开启保护模式，要么开启长模式，保护模式下是32位的，有0～0xFFFFFFFF个地址，可以使用完整的4GB虚拟地址空间。
在保护模式下，对这4GB的虚拟地址空间没有进行任何划分，而长模式下是64位的虚拟地址空间有0～0xFFFFFFFFFFFFFFFF个地址，这个地址空间非常巨大，硬件工程师根据需求设计，把它分成了3段，如下图所示。
长模式下，CPU目前只实现了48位地址空间，但寄存器却是64位的，CPU自己用地址数据的第47位的值扩展到最高16位，所以64位地址数据的最高16位，要么是全0，要么全1，这就是我们在上图看到的情形。
Cosmos如何划分虚拟地址空间现在我们来规划一下，Cosmos对x86 CPU长模式下虚拟地址空间的使用。由前面的图形可以看出，在长模式下，整个虚拟地址空间只有两段是可以用的，很自然一段给内核，另一段就给应用。
我们把0xFFFF800000000000～0xFFFFFFFFFFFFFFFF虚拟地址空间分给内核，把0～0x00007FFFFFFFFFFF虚拟地址空间分给应用，内核占用的称为内核空间，应用占用的就叫应用空间。
在内核空间和应用空间中，我们又继续做了细分。后面的图并不是严格按比例画的，应用程序在链接时，会将各个模块的指令和数据分别放在一起，应用程序的栈是在最顶端，向下增长，应用程序的堆是在应用程序数据区的后面，向上增长。
内核空间中有个线性映射区0xFFFF800000000000～0xFFFF800400000000，这是我们在二级引导器中建立的MMU页表映射。
如何设计数据结构根据前面经验，我们要实现一个功能模块，首先要设计出相应的数据结构，虚拟内存模块也一样。
这里涉及到虚拟地址区间，管理虚拟地址区间以及它所对应的物理页面，最后让进程和虚拟地址空间相结合。这些数据结构小而多，下面我们一个个来设计。
虚拟地址区间我们先来设计虚拟地址区间数据结构，由于虚拟地址空间非常巨大，我们绝不能像管理物理内存页面那样，一个页面对应一个结构体。那样的话，我们整个物理内存空间或许都放不下所有的虚拟地址区间数据结构的实例变量。
由于虚拟地址空间往往是以区为单位的，比如栈区、堆区，指令区、数据区，这些区内部往往是连续的，区与区之间却间隔了很大空间，而且每个区的空间扩大时我们不会建立新的虚拟地址区间数据结构，而是改变其中的指针，这就节约了内存空间。
下面我们来设计这个数据结构，代码如下所示。
typedef struct KMVARSDSC { spinlock_t kva_lock; //保护自身自旋锁 u32_t kva_maptype; //映射类型 list_h_t kva_list; //链表 u64_t kva_flgs; //相关标志 u64_t kva_limits; void* kva_mcstruct; //指向它的上层结构 adr_t kva_start; //虚拟地址的开始 adr_t kva_end; //虚拟地址的结束 kvmemcbox_t* kva_kvmbox; //管理这个结构映射的物理页面 void* kva_kvmcobj; }kmvarsdsc_t; 如你所见，除了自旋锁、链表、类型等字段外，最重要的就是虚拟地址的开始与结束字段，它精确描述了一段虚拟地址空间。
整个虚拟地址空间如何描述有了虚拟地址区间的数据结构，怎么描述整个虚拟地址空间呢？我们整个的虚拟地址空间，正是由多个虚拟地址区间连接起来组成，也就是说，只要把许多个虚拟地址区间数据结构按顺序连接起来，就可以表示整个虚拟地址空间了。
这个数据结构我们这样来设计。
typedef struct s_VIRMEMADRS { spinlock_t vs_lock; //保护自身的自旋锁 u32_t vs_resalin; list_h_t vs_list; //链表，链接虚拟地址区间 uint_t vs_flgs; //标志 uint_t vs_kmvdscnr; //多少个虚拟地址区间 mmadrsdsc_t* vs_mm; //指向它的上层的数据结构 kmvarsdsc_t* vs_startkmvdsc; //开始的虚拟地址区间 kmvarsdsc_t* vs_endkmvdsc; //结束的虚拟地址区间 kmvarsdsc_t* vs_currkmvdsc; //当前的虚拟地址区间 adr_t vs_isalcstart; //能分配的开始虚拟地址 adr_t vs_isalcend; //能分配的结束虚拟地址 void* vs_privte; //私有数据指针 void* vs_ext; //扩展数据指针 }virmemadrs_t; 从上述代码可以看出，virmemadrs_t结构管理了整个虚拟地址空间的kmvarsdsc_t结构，kmvarsdsc_t结构表示一个虚拟地址区间。这样我们就能知道，虚拟地址空间中哪些地址区间没有分配，哪些地址区间已经分配了。</description></item><item><title>20_幻读是什么，幻读有什么问题？</title><link>https://artisanbox.github.io/1/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/20/</guid><description>在上一篇文章最后，我给你留了一个关于加锁规则的问题。今天，我们就从这个问题说起吧。
为了便于说明问题，这一篇文章，我们就先使用一个小一点儿的表。建表和初始化语句如下（为了便于本期的例子说明，我把上篇文章中用到的表结构做了点儿修改）：
CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 这个表除了主键id外，还有一个索引c，初始化语句在表中插入了6行数据。
上期我留给你的问题是，下面的语句序列，是怎么加锁的，加的锁又是什么时候释放的呢？
begin; select * from t where d=5 for update; commit; 比较好理解的是，这个语句会命中d=5的这一行，对应的主键id=5，因此在select 语句执行完成后，id=5这一行会加一个写锁，而且由于两阶段锁协议，这个写锁会在执行commit语句的时候释放。
由于字段d上没有索引，因此这条查询语句会做全表扫描。那么，其他被扫描到的，但是不满足条件的5行记录上，会不会被加锁呢？
我们知道，InnoDB的默认事务隔离级别是可重复读，所以本文接下来没有特殊说明的部分，都是设定在可重复读隔离级别下。
幻读是什么？现在，我们就来分析一下，如果只在id=5这一行加锁，而其他行的不加锁的话，会怎么样。
下面先来看一下这个场景（注意：这是我假设的一个场景）：
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;图 1 假设只在id=5这一行加行锁可以看到，session A里执行了三次查询，分别是Q1、Q2和Q3。它们的SQL语句相同，都是select * from t where d=5 for update。这个语句的意思你应该很清楚了，查所有d=5的行，而且使用的是当前读，并且加上写锁。现在，我们来看一下这三条SQL语句，分别会返回什么结果。
Q1只返回id=5这一行；
在T2时刻，session B把id=0这一行的d值改成了5，因此T3时刻Q2查出来的是id=0和id=5这两行；</description></item><item><title>20_散列表（下）：为什么散列表和链表经常会一起使用？</title><link>https://artisanbox.github.io/2/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/21/</guid><description>我们已经学习了20节内容，你有没有发现，有两种数据结构，散列表和链表，经常会被放在一起使用。你还记得，前面的章节中都有哪些地方讲到散列表和链表的组合使用吗？我带你一起回忆一下。
在链表那一节，我讲到如何用链表来实现LRU缓存淘汰算法，但是链表实现的LRU缓存淘汰算法的时间复杂度是O(n)，当时我也提到了，通过散列表可以将这个时间复杂度降低到O(1)。
在跳表那一节，我提到Redis的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表。当时我们也提到，Redis有序集合不仅使用了跳表，还用到了散列表。
除此之外，如果你熟悉Java编程语言，你会发现LinkedHashMap这样一个常用的容器，也用到了散列表和链表两种数据结构。
今天，我们就来看看，在这几个问题中，散列表和链表都是如何组合起来使用的，以及为什么散列表和链表会经常放到一块使用。
LRU缓存淘汰算法在链表那一节中，我提到，借助散列表，我们可以把LRU缓存淘汰算法的时间复杂度降低为O(1)。现在，我们就来看看它是如何做到的。
首先，我们来回顾一下当时我们是如何通过链表实现LRU缓存淘汰算法的。
我们需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。
当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的LRU缓存淘汰算法的时间复杂很高，是O(n)。
实际上，我总结一下，一个缓存（cache）系统主要包含下面这几个操作：
往缓存中添加一个数据；
从缓存中删除一个数据；
在缓存中查找一个数据。
这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是O(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到O(1)。具体的结构就是下面这个样子：
我们使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段hnext。这个hnext有什么作用呢？
因为我们的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext指针是为了将结点串在散列表的拉链中。
了解了这个散列表和双向链表的组合存储结构之后，我们再来看，前面讲到的缓存的三个操作，是如何做到时间复杂度是O(1)的？
首先，我们来看如何查找一个数据。我们前面讲过，散列表中查找数据的时间复杂度接近O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。
其次，我们来看如何删除一个数据。我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在O(1)时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针O(1)时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要O(1)的时间复杂度。
最后，我们来看如何添加一个数据。添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。
这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在O(1)的时间复杂度内完成。所以，这三个操作的时间复杂度都是O(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持LRU缓存淘汰算法的缓存系统原型。
Redis有序集合在跳表那一节，讲到有序集合的操作时，我稍微做了些简化。实际上，在有序集合中，每个成员对象有两个重要的属性，key（键值）和score（分值）。我们不仅会通过score来查找数据，还会通过key来查找数据。
举个例子，比如用户积分排行榜有这样一个功能：我们可以通过用户的ID来查找积分信息，也可以通过积分区间来查找用户ID或者姓名信息。这里包含ID、姓名和积分的用户信息，就是成员对象，用户ID就是key，积分就是score。
所以，如果我们细化一下Redis有序集合的操作，那就是下面这样：
添加一个成员对象；
按照键值来删除一个成员对象；
按照键值来查找一个成员对象；
按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象；
按照分值从小到大排序成员变量；
如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢，解决方法与LRU缓存淘汰算法的解决方法类似。我们可以再按照键值构建一个散列表，这样按照key来删除、查找一个成员对象的时间复杂度就变成了O(1)。同时，借助跳表结构，其他操作也非常高效。
实际上，Redis有序集合的操作还有另外一类，也就是查找成员对象的排名（Rank）或者根据排名区间查找成员对象。这个功能单纯用刚刚讲的这种组合结构就无法高效实现了。这块内容我后面的章节再讲。
Java LinkedHashMap前面我们讲了两个散列表和链表结合的例子，现在我们再来看另外一个，Java中的LinkedHashMap这种容器。
如果你熟悉Java，那你几乎天天会用到这个容器。我们之前讲过，HashMap底层是通过散列表这种数据结构实现的。而LinkedHashMap前面比HashMap多了一个“Linked”，这里的“Linked”是不是说，LinkedHashMap是一个通过链表法解决散列冲突的散列表呢？
实际上，LinkedHashMap并没有这么简单，其中的“Linked”也并不仅仅代表它是通过链表法解决散列冲突的。关于这一点，在我是初学者的时候，也误解了很久。
我们先来看一段代码。你觉得这段代码会以什么样的顺序打印3，1，5，2这几个key呢？原因又是什么呢？
HashMap&amp;lt;Integer, Integer&amp;gt; m = new LinkedHashMap&amp;lt;&amp;gt;(); m.put(3, 11); m.put(1, 12); m.put(5, 23); m.put(2, 22); for (Map.</description></item><item><title>20_日志（下）：系统故障，如何恢复数据？</title><link>https://artisanbox.github.io/8/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/20/</guid><description>你好，我是朱晓峰。
上节课，咱们学习了通用查询日志、慢查询日志和错误日志，它们可以帮助我们快速定位系统问题。但实际上，日志也可以帮助我们找回由于误操作而丢失的数据，比如二进制日志（binary log）、中继日志（relay log）、回滚日志（undo log）和重做日志（redo log）。
这节课，我们就来学习下这4种日志。
二进制日志二进制日志主要记录数据库的更新事件，比如创建数据表、更新表中的数据、数据更新所花费的时长等信息。通过这些信息，我们可以再现数据更新操作的全过程。而且，由于日志的延续性和时效性，我们还可以利用日志，完成无损失的数据恢复和主从服务器之间的数据同步。
可以说，二进制日志是进行数据恢复和数据复制的利器。所以，接下来我就结合一个实际案例，重点给你讲一讲怎么操作它。
如何操作二进制日志？操作二进制日志，主要包括查看、刷新二进制日志，用二进制日志恢复数据，以及删除二进制日志。
1.查看二进制日志
查看二进制日志主要有3种情况，分别是查看当前正在写入的二进制日志、查看所有的二进制日志和查看二进制日志中的所有数据更新事件。
查看当前正在写入的二进制日志的SQL语句是：
SHOW MASTER STATUS; 我们可以通过这条语句，查看当前正在写入的二进制日志的名称和当前写入的位置：
mysql&amp;gt; SHOW MASTER STATUS; +----------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +----------------------+----------+--------------+------------------+-------------------+ | GJTECH-PC-bin.000011 | 2207 | | | | +----------------------+----------+--------------+------------------+-------------------+ -- 当前正在写入的二进制日志是“GJTECH-PC-bin.000011”，当前的位置是2207。 1 row in set (0.00 sec) 查看所有的二进制日志的SQL语句是：
SHOW BINARY LOGS; 查看二进制日志中所有数据更新事件的SQL语句是：
SHOW BINLOG EVENTS IN 二进制文件名; 2.刷新二进制日志
刷新二进制日志的SQL语句是：
FLUSH BINARY LOGS; 这条语句的意思是，关闭服务器正在写入的二进制日志文件，并重新打开一个新文件，文件名的后缀在现有的基础上加1。
3.用二进制日志恢复数据
我们可以用mysqlbinlog工具进行数据恢复：</description></item><item><title>20_面向流水线的指令设计（上）：一心多用的现代CPU</title><link>https://artisanbox.github.io/4/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/20/</guid><description>前面我们用了三讲，用一个个的电路组合，制作出了一个完整功能的CPU。这里面一下子给你引入了三个“周期”的概念，分别是指令周期、机器周期（或者CPU周期）以及时钟周期。
你可能会有点摸不着头脑了，为什么小小一个CPU，有那么多的周期（Cycle）呢？我们在专栏一开始，不是把CPU的性能定义得非常清楚了吗？我们说程序的性能，是由三个因素相乘来衡量的，我们还专门说过“指令数×CPI×时钟周期”这个公式。这里面和周期相关的只有一个时钟周期，也就是我们CPU的主频倒数。当时讲的时候我们说，一个CPU的时钟周期可以认为是可以完成一条最简单的计算机指令的时间。
那么，为什么我们在构造CPU的时候，一下子出来了那么多个周期呢？这一讲，我就来为你说道说道，带你更深入地看看现代CPU是怎么一回事儿。
愿得一心人，白首不相离：单指令周期处理器学过前面三讲，你现在应该知道，一条CPU指令的执行，是由“取得指令（Fetch）-指令译码（Decode）-执行指令（Execute） ”这样三个步骤组成的。这个执行过程，至少需要花费一个时钟周期。因为在取指令的时候，我们需要通过时钟周期的信号，来决定计数器的自增。
那么，很自然地，我们希望能确保让这样一整条指令的执行，在一个时钟周期内完成。这样，我们一个时钟周期可以执行一条指令，CPI也就是1，看起来就比执行一条指令需要多个时钟周期性能要好。采用这种设计思路的处理器，就叫作单指令周期处理器（Single Cycle Processor），也就是在一个时钟周期内，处理器正好能处理一条指令。
不过，我们的时钟周期是固定的，但是指令的电路复杂程度是不同的，所以实际一条指令执行的时间是不同的。在第13讲和第14讲讲加法器和乘法器电路的时候，我给你看过，随着门电路层数的增加，由于门延迟的存在，位数多、计算复杂的指令需要的执行时间会更长。
不同指令的执行时间不同，但是我们需要让所有指令都在一个时钟周期内完成，那就只好把时钟周期和执行时间最长的那个指令设成一样。这就好比学校体育课1000米考试，我们要给这场考试预留的时间，肯定得和跑得最慢的那个同学一样。因为就算其他同学先跑完，也要等最慢的同学跑完间，我们才能进行下一项活动。
快速执行完成的指令，需要等待满一个时钟周期，才能执行下一条指令所以，在单指令周期处理器里面，无论是执行一条用不到ALU的无条件跳转指令，还是一条计算起来电路特别复杂的浮点数乘法运算，我们都等要等满一个时钟周期。在这个情况下，虽然CPI能够保持在1，但是我们的时钟频率却没法太高。因为太高的话，有些复杂指令没有办法在一个时钟周期内运行完成。那么在下一个时钟周期到来，开始执行下一条指令的时候，前一条指令的执行结果可能还没有写入到寄存器里面。那下一条指令读取的数据就是不准确的，就会出现错误。
前一条指令的写入，在后一条指令的读取之前到这里你会发现，这和我们之前第3讲和第4讲讲时钟频率时候的说法不太一样。当时我们说，一个CPU时钟周期，可以认为是完成一条简单指令的时间。为什么到了这里，单指令周期处理器，反而变成了执行一条最复杂的指令的时间呢？
这是因为，无论是PC上使用的Intel CPU，还是手机上使用的ARM CPU，都不是单指令周期处理器，而是采用了一种叫作指令流水线（Instruction Pipeline）的技术。
无可奈何花落去，似曾相识燕归来：现代处理器的流水线设计其实，CPU执行一条指令的过程和我们开发软件功能的过程很像。
如果我们想开发一个手机App上的功能，并不是找来一个工程师，告诉他“你把这个功能开发出来”，然后他就吭哧吭哧把功能开发出来。真实的情况是，无论只有一个工程师，还是有一个开发团队，我们都需要先对开发功能的过程进行切分，把这个过程变成“撰写需求文档、开发后台API、开发客户端App、测试、发布上线”这样多个独立的过程。每一个后面的步骤，都要依赖前面的步骤。
我们的指令执行过程也是一样的，它会拆分成“取指令、译码、执行”这样三大步骤。更细分一点的话，执行的过程，其实还包含从寄存器或者内存中读取数据，通过ALU进行运算，把结果写回到寄存器或者内存中。
如果我们有一个开发团队，我们不会让后端工程师开发完API之后，就歇着等待前台App的开发、测试乃至发布，而是会在客户端App开发的同时，着手下一个需求的后端API开发。那么，同样的思路我们可以一样应用在CPU执行指令的过程中。
通过过去三讲，你应该已经知道了，CPU的指令执行过程，其实也是由各个电路模块组成的。我们在取指令的时候，需要一个译码器把数据从内存里面取出来，写入到寄存器中；在指令译码的时候，我们需要另外一个译码器，把指令解析成对应的控制信号、内存地址和数据；到了指令执行的时候，我们需要的则是一个完成计算工作的ALU。这些都是一个一个独立的组合逻辑电路，我们可以把它们看作一个团队里面的产品经理、后端工程师和客户端工程师，共同协作来完成任务。
流水线执行示意图这样一来，我们就不用把时钟周期设置成整条指令执行的时间，而是拆分成完成这样的一个一个小步骤需要的时间。同时，每一个阶段的电路在完成对应的任务之后，也不需要等待整个指令执行完成，而是可以直接执行下一条指令的对应阶段。
这就好像我们的后端程序员不需要等待功能上线，就会从产品经理手中拿到下一个需求，开始开发API。这样的协作模式，就是我们所说的指令流水线。这里面每一个独立的步骤，我们就称之为流水线阶段或者流水线级（Pipeline Stage）。
如果我们把一个指令拆分成“取指令-指令译码-执行指令”这样三个部分，那这就是一个三级的流水线。如果我们进一步把“执行指令”拆分成“ALU计算（指令执行）-内存访问-数据写回”，那么它就会变成一个五级的流水线。
五级的流水线，就表示我们在同一个时钟周期里面，同时运行五条指令的不同阶段。这个时候，虽然执行一条指令的时钟周期变成了5，但是我们可以把CPU的主频提得更高了。我们不需要确保最复杂的那条指令在时钟周期里面执行完成，而只要保障一个最复杂的流水线级的操作，在一个时钟周期内完成就好了。
如果某一个操作步骤的时间太长，我们就可以考虑把这个步骤，拆分成更多的步骤，让所有步骤需要执行的时间尽量都差不多长。这样，也就可以解决我们在单指令周期处理器中遇到的，性能瓶颈来自于最复杂的指令的问题。像我们现代的ARM或者Intel的CPU，流水线级数都已经到了14级。
虽然我们不能通过流水线，来减少单条指令执行的“延时”这个性能指标，但是，通过同时在执行多条指令的不同阶段，我们提升了CPU的“吞吐率”。在外部看来，我们的CPU好像是“一心多用”，在同一时间，同时执行5条不同指令的不同阶段。在CPU内部，其实它就像生产线一样，不同分工的组件不断处理上游传递下来的内容，而不需要等待单件商品生产完成之后，再启动下一件商品的生产过程。
超长流水线的性能瓶颈既然流水线可以增加我们的吞吐率，你可能要问了，为什么我们不把流水线级数做得更深呢？为什么不做成20级，乃至40级呢？这个其实有很多原因，我在之后几讲里面会详细讲解。这里，我先讲一个最基本的原因，就是增加流水线深度，其实是有性能成本的。
我们用来同步时钟周期的，不再是指令级别的，而是流水线阶段级别的。每一级流水线对应的输出，都要放到流水线寄存器（Pipeline Register）里面，然后在下一个时钟周期，交给下一个流水线级去处理。所以，每增加一级的流水线，就要多一级写入到流水线寄存器的操作。虽然流水线寄存器非常快，比如只有20皮秒（ps，$10^{-12}$秒）。
但是，如果我们不断加深流水线，这些操作占整个指令的执行时间的比例就会不断增加。最后，我们的性能瓶颈就会出现在这些overhead上。如果我们指令的执行有3纳秒，也就是3000皮秒。我们需要20级的流水线，那流水线寄存器的写入就需要花费400皮秒，占了超过10%。如果我们需要50级流水线，就要多花费1纳秒在流水线寄存器上，占到25%。这也就意味着，单纯地增加流水线级数，不仅不能提升性能，反而会有更多的overhead的开销。所以，设计合理的流水线级数也是现代CPU中非常重要的一点。
总结延伸讲到这里，相信你已经能够理解，为什么我们的CPU需要流水线设计了，也能把每一个流水线阶段在干什么，和上一讲的整个CPU的数据通路的连接过程对上了。
可以看到，为了能够不浪费CPU的性能，我们通过把指令的执行过程，切分成一个一个流水线级，来提升CPU的吞吐率。而我们本身的CPU的设计，又是由一个个独立的组合逻辑电路串接起来形成的，天然能够适合这样采用流水线“专业分工”的工作方式。
因为每一级的overhead，一味地增加流水线深度，并不能无限地提高性能。同样地，因为指令的执行不再是顺序地一条条执行，而是在上一条执行到一半的时候，下一条就已经启动了，所以也给我们的程序带来了很多挑战。这些挑战和对应的解决方案，就要请你坚持关注后面的几讲，我们一起来揭开答案了。
推荐阅读想要了解CPU的流水线设计，可以参看《深入理解计算机系统》的4.4章节，以及《计算机组成与设计 硬件/软件接口》的4.5章节。
课后思考我们在前面讲过，一个CPU的时钟周期，可以认为是完成一条简单指令的时间。在这一讲之后，你觉得这句话正确吗？为什么？在了解了CPU的流水线设计之后，你是怎么理解这句话的呢？
欢迎留言和我分享你的疑惑和见解。你也可以把今天的内容，分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>20_高效运行：编译器的后端技术</title><link>https://artisanbox.github.io/6/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/20/</guid><description>前18节课，我们主要探讨了编译器的前端技术，它的重点，是让编译器能够读懂程序。无结构的代码文本，经过前端的处理以后，就变成了Token、AST和语义属性、符号表等结构化的信息。基于这些信息，我们可以实现简单的脚本解释器，这也从另一个角度证明了我们的前端处理工作确实理解了程序代码，否则程序不可能正确执行嘛。
实际上，学完前端技术以后，我们已经能做很多事情了，比如让软件有自定义功能，就像我们在15讲中提到的报表系统，这时，不需要涉及编译器后端技术。
但很多情况下，我们需要继续把程序编译成机器能读懂的代码，并高效运行。这时，我们就面临了三个问题：
1.我们必须了解计算机运行一个程序的原理（也就是运行期机制），只有这样，才知道如何生成这样的程序。
2.要能利用前端生成的AST和属性信息，将其正确翻译成目标代码。
3.需要对程序做尽可能多的优化，比如让程序执行效率更高，占空间更少等等。
弄清这三个问题，是顺利完成编译器后端工作的关键，本节课，我会让你对程序运行机制、生成代码和优化代码有个直观的了解，然后再在接下来的课程中，将这些问题逐一击破。
弄清程序的运行机制总的来说，编译器后端要解决的问题是：现在给你一台计算机，你怎么生成一个可以运行的程序，然后还能让这个程序在计算机上正确和高效地运行？
我画了一个模型：
基本上，我们需要面对的是两个硬件：
一个是CPU，它能接受机器指令和数据，并进行计算。它里面有寄存器、高速缓存和运算单元，充分利用寄存器和高速缓存会让系统的性能大大提升。
另一个是内存。我们要在内存里保存编译好的代码和数据，还要设计一套机制，让程序最高效地利用这些内存。
通常情况下，我们的程序要受某个操作系统的管理，所以也要符合操作系统的一些约定。但有时候我们的程序也可能直接跑在硬件上，单片机和很多物联网设备采用这样的结构，甚至一些服务端系统，也可以不跑在操作系统上。
你可以看出，编译器后端技术跟计算机体系结构的关系很密切。我们必须清楚地理解计算机程序是怎么运行的，有了这个基础，才能探讨如何编译生成这样的程序。
所以，我会在下一节课，也就是21讲，将运行期的机制讲清楚，比如内存空间如何划分和组织；程序是如何启动、跳转和退出的；执行过程中指令和数据如何传递到CPU；整个过程中需要如何跟操作系统配合，等等。
也有的时候，我们的面对的机器是虚拟机，Java的运行环境就是一个虚拟机（JVM），那我们需要就了解这个虚拟机的特点，以便生成可以在这个虚拟机上运行的代码，比如Java的字节码。同时，字节码有时仍然需要编译成机器码。
在对运行期机制有了一定的了解之后，我们就有底气来进行下一步了，生成符合运行期机制的代码。
生成代码编译器后端的最终结果，就是生成目标代码。如果目标是在计算机上直接运行，就像C语言程序那样，那这个目标代码指的是汇编代码。而如果运行目标是Java虚拟机，那这个目标代码就是指JVM的字节码。
基于我们在编译器前端所生成的成果，我们其实可以直接生成汇编代码，在后面的课程中，我会带你做一个这样的尝试。
你可能惧怕汇编代码，觉得它肯定很难，能写汇编的人一定很牛。在我看来，这是一个偏见，因为汇编代码并不难写，为什么呢？
其实汇编没有类型，也没有那么多的语法结构，它要做的通常就是把数据拷贝到寄存器，处理一下，再保存回内存。所以，从汇编语言的特性看，就决定了它不可能复杂到哪儿去。
你如果问问硬件工程师就知道了，因为他们经常拿汇编语言操作寄存器、调用中断，也没多难。但另一方面，正是因为汇编的基础机制太简单，而且不太安全，用它编写程序的效率太低，所以现在直接用汇编写的程序，都是处理很小、很单一的问题，我们不会再像阿波罗登月计划那样，用汇编写整个系统，这个项目的代码最近已经开源了，如果现在用高级语言去做这项工作，会容易得多，还可以像现在的汽车自动驾驶系统一样实现更多的功能。
所以，在22和23讲，我会带你从AST直接翻译成汇编代码，并编译成可执行文件，这样你就会看到这个过程没有你想象的那么困难，你对汇编代码的恐惧感，也会就此消失了。
当然，写汇编跟使用高级语言有很多不同，其中一点就是要关心CPU和内存这样具体的硬件。比如，你需要了解不同的CPU指令集的差别，你还需要知道CPU是64位的还是32位的，有几个寄存器，每个寄存器可以用于什么指令，等等。但这样导致的问题是，每种语言，针对每种不同的硬件，都要生成不同的汇编代码。你想想看，一般我们设计一门语言要支持尽可能多的硬件平台，这样的工作量是不是很庞大？
所以，为了降低后端工作量，提高软件复用度，就需要引入中间代码（Intermediate Representation，IR）的机制，它是独立于具体硬件的一种代码格式。各个语言的前端可以先翻译成IR，然后再从IR翻译成不同硬件架构的汇编代码。如果有n个前端语言，m个后端架构，本来需要做m*n个翻译程序，现在只需要m+n个了。这就大大降低了总体的工作量。
甚至，很多语言主要做好前端就行了，后端可以尽量重用已有的库和工具，这也是现在推出新语言越来越快的原因之一。像Rust就充分利用了LLVM，GCC的各种语言，如C、C++、Object C等，也是充分共享了后端技术。
IR可以有多种格式，在第24讲，我们会介绍三地址代码、静态单赋值码等不同的IR。比如，“x + y * z”翻译成三地址代码是下面的样子，每行代码最多涉及三个地址，其中t1和t2是临时变量：
t1 := y * z t2 := x + t1 Java语言生成的字节码也是一种IR，我们还会介绍LLVM的IR，并且基于LLVM这个工具来加速我们后端的开发。
其实，IR这个词直译成中文，是“中间表示方式”的意思，不一定非是像汇编代码那样的一条条的指令。所以，AST其实也可以看做一种IR。我们在前端部分实现的脚本语言，就是基于AST这个IR来运行的。
每种IR的目的和用途是不一样的：
AST主要用于前端的工作。 Java的字节码，是设计用来在虚拟机上运行的。 LLVM的中间代码，主要是用于做代码翻译和编译优化的。 …… 总的来说，我们可以把各种语言翻译成中间代码，再针对每一种目标架构，通过一个程序将中间代码翻译成相应的汇编代码就可以了。然而事情真的这么简单吗？答案是否定的，因为我们还必须对代码进行优化。
代码分析和优化生成正确的、能够执行的代码比较简单，可这样的代码执行效率很低，因为直接翻译生成的代码往往不够简洁，比如会生成大量的临时变量，指令数量也较多。因为翻译程序首先照顾的是正确性，很难同时兼顾是否足够优化，这是一方面。另一方面，由于高级语言本身的限制和程序员的编程习惯，也会导致代码不够优化，不能充分发挥计算机的性能。所以我们一定要对代码做优化。程序员在比较各种语言的时候，一定会比较它们的性能差异。一个语言的性能太差，就会影响它的使用和普及。
实际上，就算是现在常见的脚本语言，如Python和JavaScript，也做了很多后端优化的工作，包括编译成字节码、支持即时编译等，这些都是为了进一步提高性能。从谷歌支持的开源项目V8开始，JavaScript的性能获得了巨大的提高，这才导致了JavaScript再一次的繁荣，包括支持体验更好的前端应用和基于Node.js的后端应用。
优化工作又分为“独立于机器的优化”和“依赖于机器的优化”两种。
独立于机器的优化，是基于IR进行的。它可以通过对代码的分析，用更加高效的代码代替原来的代码。比如下面这段代码中的foo()函数，里面有多个地方可以优化。甚至，我们连整个对foo()函数的调用，也可以省略，因为foo()的值一定是101。这些优化工作在编译期都可以去做。
int foo(){ int a = 10*10; //这里在编译时可以直接计算出100这个值 int b = 20; //这个变量没有用到，可以在代码中删除 if (a&amp;amp;gt;0){ //因为a一定大于0，所以判断条件和else语句都可以去掉 return a+1; //这里可以在编译器就计算出是101 } else{ return a-1; } } int a = foo(); //这里可以直接地换成 a=101; 上面的代码，通过优化，可以消除很多冗余的逻辑。这就好比你正在旅行，先从北京飞到了上海，然后又飞到厦门，最后飞回北京。然后你朋友问你现在在哪时，你告诉他在北京。那么他虽然知道你在北京，但并没有意识到你已经在几个城市折腾了一圈，因为他只关心你现在在哪儿，并不关心你的中间过程。 我们在给a赋值的时候，只需要知道这个值是101就行了。完全不需要在运行时去兜一大圈来计算。</description></item><item><title>20｜怎么实现一个更好的寄存器分配算法：实现篇</title><link>https://artisanbox.github.io/3/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/22/</guid><description>你好，我是宫文学。
在上一节课，我们已经介绍了寄存器分配算法的原理。不过呢，我们这门课，不是停留在对原理的理解上就够了，还要把它具体实现出来才行。在实现的过程中，你会发现有不少实际的具体问题要去解决。而你一旦解决好了它们，你对寄存器分配相关原理的理解也会变得更加通透和深入。
所以，今天这一节课，我就会带你具体实现寄存器分配算法。在这个过程中，你会解决这些具体的技术问题：
首先，我们会了解如何基于我们现在的LIR来具体实现变量活跃性分析。特别是，当程序中存在多个基本块的时候，分析算法该如何设计。 第二，我们也会学习到在实现线性扫描算法中的一些技术点，包括如何分配寄存器、在调用函数时如何保存Caller需要保护的寄存器，以及如何正确的维护栈桢。 解决了这些问题之后，我们会对我们的语言再做一次性能测试，看看这次性能的提升有多大。那么接下来，就让我们先看看实现变量活跃性分析，需要考虑哪些技术细节吧。
实现变量活跃性分析我们先来总结一下，在实现变量活跃性分析的时候，我们会遇到哪几个技术点。我们一般要考虑如何保存变量活跃性分析的结果、如何表达变量的定义，以及如何基于CFG来做变量活跃性分析这三个方面。
现在我们就一一来分析一下。
首先，我们要设计一个数据结构，把活跃性分析的结果保存下来，方便我们后面在寄存器分配算法中使用。
这个数据结构很简单，我们使用一个Map即可。这个Map的key是指令，而value是一个数组，也就是执行当前指令时，活跃变量的集合。
liveVars:Map&amp;lt;Inst, number[]&amp;gt; = new Map(); 确定了数据结构以后，我们再讨论一下算法的实现。在算法的执行过程中呢，我们倒着扫描一条条指令。对于每条指令，我们要分析它的操作数。如果操作数是一个变量下标，那我们就把这个变量加到活跃变量的集合中。所以，往集合里加变量实现起来很简单。
可是，从集合里减变量就不那么简单了。为什么呢？根据我们上一节课讲过的算法，我们需要在变量声明的时候，把这个变量从集合里去掉。可是，我们当前的LIR中并没有记录哪个变量是在什么时候声明的，也就没办法知道变量的生存期是从什么时候开始的了。
那怎么来解决这个问题呢？我的办法是，向LIR里再加一条指令，这条指令专门用来指示变量的声明。我把这条指令的OpCode叫做declVar。
由于这条指令并不能转化成具体的可执行的指令，所以你可以把它叫做伪指令。它仅用于我们的寄存器分配算法。
好了，在加入了这条指令以后，我们就能对一个基本块进行变量活跃性分析了。具体实现你可以参考代码LivenessAnalyzer，其中的核心逻辑我放在下面了：
//为每一条指令计算活跃变量集合 for (let i = bb.insts.length - 1; i &amp;gt;=0; i--){ let inst = bb.insts[i]; if (inst.numOprands == 1){ let inst_1 = inst as Inst_1; //变量声明伪指令，从liveVars集合中去掉该变量 if (inst_1.op == OpCode.declVar){ let varIndex = inst_1.oprand.value as number; let indexInArray = vars.indexOf(varIndex); if (indexInArray != -1){ vars.splice(indexInArray,1); } } //查看指令中引用了哪个变量，就加到liveVars集合中去 else{ this.</description></item><item><title>21_JavaScript编译器（二）：V8的解释器和优化编译器</title><link>https://artisanbox.github.io/7/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/21/</guid><description>你好，我是宫文学。通过前一讲的学习，我们已经了解了V8的整个编译过程，并重点探讨了一个问题，就是V8的编译速度为什么那么快。
V8把解析过程做了重点的优化，解析完毕以后就可以马上通过Ignition解释执行了。这就让JavaScript可以快速运行起来。
今天这一讲呢，我们重点来讨论一下，V8的运行速度为什么也这么快，一起来看看V8所采用的优化技术。
上一讲我也提及过，V8在2008年刚推出的时候，它提供了一个快速编译成机器码的编译器，虽然没做太多优化，但性能已经是当时其他JavaScript引擎的10倍了。而现在，V8的速度又是2008年刚发布时候的10倍。那么，是什么技术造成了这么大的性能优化的呢？
这其中，一方面原因，是TurboFan这个优化编译器，采用了很多的优化技术。那么，它采用了什么优化算法？采用了什么IR？其优化思路跟Java的JIT编译器有什么相同点和不同点？
另一方面，最新的Ignition解释器，虽然只是做解释执行的功能，但竟然也比一个基础的编译器生成的代码慢不了多少。这又是什么原因呢？
所以今天，我们就一起把这些问题都搞清楚，这样你就能全面了解V8所采用的编译技术的特点了，你对动态类型语言的编译，也就能有更深入的了解，并且这也有助于你编写更高效的JavaScript程序。
好，首先，我们来了解一下TurboFan的优化编译技术。
TurboFan的优化编译技术TurboFan是一个优化编译器。不过它跟Java的优化编译器要完成的任务是不太相同的。因为JavaScript是动态类型的语言，所以如果它能够推断出准确的类型再来做优化，就会带来巨大的性能提升。
同时，TurboFan也会像Java的JIT编译器那样，基于IR来运行各种优化算法，以及在后端做指令选择、寄存器分配等优化。所有的这些因素加起来，才使得TurboFan能达到很高的性能。
我们先来看看V8最特别的优化，也就是通过对类型的推理所做的优化。
基于推理的优化（Speculative Optimazition）对于基于推理的优化，我们其实并不陌生。在研究Java的JIT编译器时，你就发现了Graal会针对解释器收集的一些信息，对于代码做一些推断，从而做一些激进的优化，比如说会跳过一些不必要的程序分支。
而JavaScript是动态类型的语言，所以对于V8来说，最重要的优化，就是能够在运行时正确地做出类型推断。举个例子来说，假设示例函数中的add函数，在解释器里多次执行的时候，接受的参数都是整型，那么TurboFan就处理整型加法运算的代码就行了。这也就是上一讲中我们生成的汇编代码。
function add(a,b){ return a+b; } for (i = 0; i&amp;lt;100000; i++){ if (i%1000==0) console.log(i);
add(i, i+1); } 但是，如果不在解释器里执行，直接要求TurboFan做编译，会生成什么样的汇编代码呢？
你可以在运行d8的时候，加上“–always-opt”参数，这样V8在第一次遇到add函数的时候，就会编译成机器码。
./d8 &amp;ndash;trace-opt-verbose &amp;ndash;trace-turbo &amp;ndash;turbo-filter=add &amp;ndash;print-code &amp;ndash;print-opt-code &amp;ndash;code-comments &amp;ndash;always-opt add.js 这一次生成的汇编代码，跟上一讲生成的就不一样了。由于编译器不知道add函数的参数是什么类型的，所以实际上，编译器是去调用实现Add指令的内置函数，来生成了汇编代码。
这个内置函数当然支持所有加法操作的语义，但是它也就无法启动基于推理的优化机制了。这样的代码，跟解释器直接解释执行，性能上没太大的差别，因为它们本质上都是调用一个全功能的内置函数。
而推理式优化的版本则不同，它直接生成了针对整型数字进行处理的汇编代码：
我来给你解释一下这几行指令的意思：
第1行和第3行，是把参数1和参数2分别拷贝到r8和r9寄存器。注意，这里是从物理寄存器里取值，而不是像前一个版本一样，在栈里取值。前一个版本遵循的是更加保守和安全的调用约定。 第2行和第4行，是把r8和r9寄存器的值向右移1位。 第5行，是把r8和r9相加。 看到这里，你可能就发现了一个问题：只是做个简单的加法而已，为什么要做移位操作呢？实际上，如果你熟悉汇编语言的话，要想实现上面的功能，其实只需要下面这两行代码就可以了：
movq rax, rdi #把参数1拷贝到rax寄存器 addq rax, rcx #把参数2加到rax寄存器上，作为返回值 那么，多出来的移位操作是干什么的呢？
这就涉及到了V8的内存管理机制。原来，V8对象都保存在堆中。在栈帧中保存的数值，都是指向堆的指针。垃圾收集器可以通过这些指针，知道哪些内存对象是不再被使用的，从而把它们释放掉。我们前面学过，Java的虚拟机和Python对于对象引用，本质上也是这么处理的。
但是，这种机制对于基础数据类型，比如整型，就不太合适了。因为你没有必要为一个简单的整型数据在堆中申请内存，这样既浪费内存，又降低了访问效率，V8需要访问两次内存才能读到一个整型变量的值（第一次读到地址，第二次根据该地址到堆里读到值）。你记得，Python就是这么访问基础数据的。
V8显然不能忍受这种低效的方式。它采用的优化机制，是一种被广泛采用的技术，叫做标记指针（Tagged Pointer）或者标记值（Tagged Value）。《Pointer Compression in V8》这篇文章，就介绍了V8中采用Tagged Pointer技术的细节。</description></item><item><title>21_为什么我只改一行的语句，锁这么多？</title><link>https://artisanbox.github.io/1/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/21/</guid><description>在上一篇文章中，我和你介绍了间隙锁和next-key lock的概念，但是并没有说明加锁规则。间隙锁的概念理解起来确实有点儿难，尤其在配合上行锁以后，很容易在判断是否会出现锁等待的问题上犯错。
所以今天，我们就先从这个加锁规则开始吧。
首先说明一下，这些加锁规则我没在别的地方看到过有类似的总结，以前我自己判断的时候都是想着代码里面的实现来脑补的。这次为了总结成不看代码的同学也能理解的规则，是我又重新刷了代码临时总结出来的。所以，这个规则有以下两条前提说明：
MySQL后面的版本可能会改变加锁策略，所以这个规则只限于截止到现在的最新版本，即5.x系列&amp;lt;=5.7.24，8.0系列 &amp;lt;=8.0.13。
如果大家在验证中有发现bad case的话，请提出来，我会再补充进这篇文章，使得一起学习本专栏的所有同学都能受益。
因为间隙锁在可重复读隔离级别下才有效，所以本篇文章接下来的描述，若没有特殊说明，默认是可重复读隔离级别。
我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。
原则1：加锁的基本单位是next-key lock。希望你还记得，next-key lock是前开后闭区间。
原则2：查找过程中访问到的对象才会加锁。
优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。
我还是以上篇文章的表t为例，和你解释一下这些规则。表t的建表语句和初始化语句如下。
CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 接下来的例子基本都是配合着图片说明的，所以我建议你可以对照着文稿看，有些例子可能会“毁三观”，也建议你读完文章后亲手实践一下。
案例一：等值查询间隙锁第一个例子是关于等值条件操作间隙：
图1 等值查询的间隙锁由于表t中没有id=7的记录，所以用我们上面提到的加锁规则判断一下的话：</description></item><item><title>21_哈希算法（上）：如何防止数据库中的用户信息被脱库？</title><link>https://artisanbox.github.io/2/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/22/</guid><description>还记得2011年CSDN的“脱库”事件吗？当时，CSDN网站被黑客攻击，超过600万用户的注册邮箱和密码明文被泄露，很多网友对CSDN明文保存用户密码行为产生了不满。如果你是CSDN的一名工程师，你会如何存储用户密码这么重要的数据吗？仅仅MD5加密一下存储就够了吗？ 要想搞清楚这个问题，就要先弄明白哈希算法。
哈希算法历史悠久，业界著名的哈希算法也有很多，比如MD5、SHA等。在我们平时的开发中，基本上都是拿现成的直接用。所以，我今天不会重点剖析哈希算法的原理，也不会教你如何设计一个哈希算法，而是从实战的角度告诉你，在实际的开发中，我们该如何用哈希算法解决问题。
什么是哈希算法？我们前面几节讲到“散列表”“散列函数”，这里又讲到“哈希算法”，你是不是有点一头雾水？实际上，不管是“散列”还是“哈希”，这都是中文翻译的差别，英文其实就是“Hash”。所以，我们常听到有人把“散列表”叫作“哈希表”“Hash表”，把“哈希算法”叫作“Hash算法”或者“散列算法”。那到底什么是哈希算法呢？
哈希算法的定义和原理非常简单，基本上一句话就可以概括了。将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。但是，要想设计一个优秀的哈希算法并不容易，根据我的经验，我总结了需要满足的几点要求：
从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
对输入数据非常敏感，哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同；
散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。
这些定义和要求都比较理论，可能还是不好理解，我拿MD5这种哈希算法来具体说明一下。
我们分别对“今天我来讲哈希算法”和“jiajia”这两个文本，计算MD5哈希值，得到两串看起来毫无规律的字符串（MD5的哈希值是128位的Bit长度，为了方便表示，我把它们转化成了16进制编码）。可以看出来，无论要哈希的文本有多长、多短，通过MD5哈希之后，得到的哈希值的长度都是相同的，而且得到的哈希值看起来像一堆随机数，完全没有规律。
MD5(&amp;quot;今天我来讲哈希算法&amp;quot;) = bb4767201ad42c74e650c1b6c03d78fa MD5(&amp;quot;jiajia&amp;quot;) = cd611a31ea969b908932d44d126d195b 我们再来看两个非常相似的文本，“我今天讲哈希算法！”和“我今天讲哈希算法”。这两个文本只有一个感叹号的区别。如果用MD5哈希算法分别计算它们的哈希值，你会发现，尽管只有一字之差，得到的哈希值也是完全不同的。
MD5(&amp;quot;我今天讲哈希算法！&amp;quot;) = 425f0d5a917188d2c3c3dc85b5e4f2cb MD5(&amp;quot;我今天讲哈希算法&amp;quot;) = a1fb91ac128e6aa37fe42c663971ac3d 我在前面也说了，通过哈希算法得到的哈希值，很难反向推导出原始数据。比如上面的例子中，我们就很难通过哈希值“a1fb91ac128e6aa37fe42c663971ac3d”反推出对应的文本“我今天讲哈希算法”。
哈希算法要处理的文本可能是各种各样的。比如，对于非常长的文本，如果哈希算法的计算时间很长，那就只能停留在理论研究的层面，很难应用到实际的软件开发中。比如，我们把今天这篇包含4000多个汉字的文章，用MD5计算哈希值，用不了1ms的时间。
哈希算法的应用非常非常多，我选了最常见的七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。这节我们先来看前四个应用。
应用一：安全加密说到哈希算法的应用，最先想到的应该就是安全加密。最常用于加密的哈希算法是MD5（MD5 Message-Digest Algorithm，MD5消息摘要算法）和SHA（Secure Hash Algorithm，安全散列算法）。
除了这两个之外，当然还有很多其他加密算法，比如DES（Data Encryption Standard，数据加密标准）、AES（Advanced Encryption Standard，高级加密标准）。
前面我讲到的哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。
第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。所以我着重讲一下第二点。实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说呢？
这里就基于组合数学中一个非常基础的理论，鸽巢原理（也叫抽屉原理）。这个原理本身很简单，它是说，如果有10个鸽巢，有11只鸽子，那肯定有1个鸽巢中的鸽子数量多于1个，换句话说就是，肯定有2只鸽子在1个鸽巢内。
有了鸽巢原理的铺垫之后，我们再来看，为什么哈希算法无法做到零冲突？
我们知道，哈希算法产生的哈希值的长度是固定且有限的。比如前面举的MD5的例子，哈希值是固定的128位二进制串，能表示的数据是有限的，最多能表示2^128个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对2^128+1个数据求哈希值，就必然会存在哈希值相同的情况。这里你应该能想到，一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。
2^128=340282366920938463463374607431768211456 为了让你能有个更加直观的感受，我找了两段字符串放在这里。这两段字符串经过MD5哈希算法加密之后，产生的哈希值是相同的。
不过，即便哈希算法存在散列冲突的情况，但是因为哈希值的范围很大，冲突的概率极低，所以相对来说还是很难破解的。像MD5，有2^128个不同的哈希值，这个数据已经是一个天文数字了，所以散列冲突的概率要小于1/2^128。
如果我们拿到一个MD5哈希值，希望通过毫无规律的穷举的方法，找到跟这个MD5值相同的另一个数据，那耗费的时间应该是个天文数字。所以，即便哈希算法存在冲突，但是在有限的时间和资源下，哈希算法还是很难被破解的。
除此之外，没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长。比如SHA-256比SHA-1要更复杂、更安全，相应的计算时间就会比较长。密码学界也一直致力于找到一种快速并且很难被破解的哈希算法。我们在实际的开发过程中，也需要权衡破解难度和计算时间，来决定究竟使用哪种加密算法。
应用二：唯一标识我先来举一个例子。如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那我们该如何搜索呢？
我们知道，任何文件在计算中都可以表示成二进制码串，所以，比较笨的办法就是，拿要查找的图片的二进制码串与图库中所有图片的二进制码串一一比对。如果相同，则说明图片在图库中存在。但是，每个图片小则几十KB、大则几MB，转化成二进制是一个非常长的串，比对起来非常耗时。有没有比较快的方法呢？
我们可以给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取100个字节，从中间取100个字节，从最后再取100个字节，然后将这300个字节放到一块，通过哈希算法（比如MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。
如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。
如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。
应用三：数据校验电驴这样的BT下载软件你肯定用过吧？我们知道，BT下载的原理是基于P2P协议的。我们从多个机器上并行下载一个2GB的电影，这个电影文件可能会被分割成很多文件块（比如可以分成100块，每块大约20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。
我们知道，网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？
具体的BT协议很复杂，校验方法也有很多，我来说其中的一种思路。
我们通过哈希算法，对100个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。
应用四：散列函数前面讲了很多哈希算法的应用，实际上，散列函数也是哈希算法的一种应用。
我们前两节讲到，散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。
不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。
解答开篇好了，有了前面的基础，现在你有没有发现开篇的问题其实很好解决？</description></item><item><title>21_土地需求扩大与保障：如何分配和释放虚拟内存？</title><link>https://artisanbox.github.io/9/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/21/</guid><description>你好，我是LMOS。
今天，我们继续研究操作系统如何实现虚拟内存。在上节课，我们已经建立了虚拟内存的初始流程，这节课我们来实现虚拟内存的核心功能：写出分配、释放虚拟地址空间的代码，最后实现虚拟地址空间到物理地址空间的映射。
这节课的配套代码，你可以点击这里下载。
虚拟地址的空间的分配与释放通过上节课的学习，我们知道整个虚拟地址空间就是由一个个虚拟地址区间组成的。那么不难猜到，分配一个虚拟地址空间就是在整个虚拟地址空间分割出一个区域，而释放一块虚拟地址空间，就是把这个区域合并到整个虚拟地址空间中去。
虚拟地址空间分配接口我们先来研究地址的分配，依然从虚拟地址空间的分配接口开始实现，一步步带着你完成虚拟 空间的分配。
在我们的想像中，分配虚拟地址空间应该有大小、有类型、有相关标志，还有从哪里开始分配等信息。根据这些信息，我们在krlvadrsmem.c文件中设计好分配虚拟地址空间的接口，如下所示。
adr_t vma_new_vadrs_core(mmadrsdsc_t *mm, adr_t start, size_t vassize, u64_t vaslimits, u32_t vastype) { adr_t retadrs = NULL; kmvarsdsc_t *newkmvd = NULL, *currkmvd = NULL; virmemadrs_t *vma = &amp;amp;mm-&amp;gt;msd_virmemadrs; knl_spinlock(&amp;amp;vma-&amp;gt;vs_lock); //查找虚拟地址区间 currkmvd = vma_find_kmvarsdsc(vma, start, vassize); if (NULL == currkmvd) { retadrs = NULL; goto out; } //进行虚拟地址区间进行检查看能否复用这个数据结构 if (((NULL == start) || (start == currkmvd-&amp;gt;kva_end)) &amp;amp;&amp;amp; (vaslimits == currkmvd-&amp;gt;kva_limits) &amp;amp;&amp;amp; (vastype == currkmvd-&amp;gt;kva_maptype)) {//能复用的话，当前虚拟地址区间的结束地址返回 retadrs = currkmvd-&amp;gt;kva_end; //扩展当前虚拟地址区间的结束地址为分配虚拟地址区间的大小 currkmvd-&amp;gt;kva_end += vassize; vma-&amp;gt;vs_currkmvdsc = currkmvd; goto out; } //建立一个新的kmvarsdsc_t虚拟地址区间结构 newkmvd = new_kmvarsdsc(); if (NULL == newkmvd) { retadrs = NULL; goto out; } //如果分配的开始地址为NULL就由系统动态决定 if (NULL == start) {//当然是接着当前虚拟地址区间之后开始 newkmvd-&amp;gt;kva_start = currkmvd-&amp;gt;kva_end; } else {//否则这个新的虚拟地址区间的开始就是请求分配的开始地址 newkmvd-&amp;gt;kva_start = start; } //设置新的虚拟地址区间的结束地址 newkmvd-&amp;gt;kva_end = newkmvd-&amp;gt;kva_start + vassize; newkmvd-&amp;gt;kva_limits = vaslimits; newkmvd-&amp;gt;kva_maptype = vastype; newkmvd-&amp;gt;kva_mcstruct = vma; vma-&amp;gt;vs_currkmvdsc = newkmvd; //将新的虚拟地址区间加入到virmemadrs_t结构中 list_add(&amp;amp;newkmvd-&amp;gt;kva_list, &amp;amp;currkmvd-&amp;gt;kva_list); //看看新的虚拟地址区间是否是最后一个 if (list_is_last(&amp;amp;newkmvd-&amp;gt;kva_list, &amp;amp;vma-&amp;gt;vs_list) == TRUE) { vma-&amp;gt;vs_endkmvdsc = newkmvd; } //返回新的虚拟地址区间的开始地址 retadrs = newkmvd-&amp;gt;kva_start; out: knl_spinunlock(&amp;amp;vma-&amp;gt;vs_lock); return retadrs; } //分配虚拟地址空间的接口 adr_t vma_new_vadrs(mmadrsdsc_t *mm, adr_t start, size_t vassize, u64_t vaslimits, u32_t vastype) { if (NULL == mm || 1 &amp;gt; vassize) { return NULL; } if (NULL !</description></item><item><title>21_数据备份：异常情况下，如何确保数据安全？</title><link>https://artisanbox.github.io/8/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/21/</guid><description>你好，我是朱晓峰。今天，我来和你聊一聊数据备份。
数据备份，对咱们技术人员来说十分重要。当成千上万的用户，每天使用我们开发的应用做着他们的日常工作的时候，数据的安全性就不光是你一个人的事了。要是有一天，突然发生了某种意想不到的情况，导致数据库服务器上的数据全部丢失，所有使用这个应用的人都会受到严重影响。
所以，我们必须“未雨绸缪”，及时把数据备份到安全的地方。这样，当突发的异常来临时，我们就能把数据及时恢复回来，就不会造成太大损失。
MySQL的数据备份有2种，一种是物理备份，通过把数据文件复制出来，达到备份的目的；另外一种是逻辑备份，通过把描述数据库结构和内容的信息保存起来，达到备份的目的。逻辑备份这种方式是免费的，广泛得到使用；而物理备份的方式需要收费，用得比较少。所以，这节课我重点和你聊聊逻辑备份。
我还会给你介绍一下MySQL中的数据备份工具mysqldump、数据恢复的命令行客户端工具mysql，以及数据表中数据导出到文件和从文件导入的SQL语句，帮助你提高你所开发的应用中的数据安全性。
如何进行数据备份？首先，我们来学习下用于数据备份的工具mysqldump。它总共有三种模式：
备份数据库中的表； 备份整个数据库； 备份整个数据库服务器。 接下来，我就来介绍下这3种备份的具体方法。
如何备份数据库中的表？mysqldump备份数据库中的表的语法结构是：
mysqldump -h 服务器 -u 用户 -p 密码 数据库名称 [表名称 … ] &amp;gt; 备份文件名称 我简单解释一下这里的核心内容。
“-h”后面跟的服务器名称，如果省略，默认是本机“localhost”。 “-u”后面跟的是用户名。 “-p”后面跟的是密码，如果省略，执行的时候系统会提示录入密码。 我举个小例子，带你看看怎么使用这个工具。
假设数据库demo中有2个表，分别是商品信息表（demo.goodsmaster）和会员表（demo.membermaster）。
商品信息表：
会员表：
现在，我需要把数据库demo备份到文件中，就可以用下面的代码实现：
H:\&amp;gt;mysqldump -u root -p demo goodsmaster membermaster &amp;gt; test.sql Enter password: ***** 这个指令的意思，就是备份本机数据库服务器上demo数据库中的商品信息表和会员信息表的所有信息。
备份文件是以文本格式保存的，我们可以用记事本打开，看一下备份的内容：
-- MySQL dump 10.13 Distrib 8.0.23, for Win64 (x86_64) -- -- Host: localhost Database: demo -- 表示从本地进行备份，数据库是demo -- ------------------------------------------------------ -- Server version 8.</description></item><item><title>21_运行时机制：突破现象看本质，透过语法看运行时</title><link>https://artisanbox.github.io/6/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/21/</guid><description>编译器的任务，是要生成能够在计算机上运行的代码，但要生成代码，我们必须对程序的运行环境和运行机制有比较透彻的了解。
你要知道，大型的、复杂一点儿的系统，比如像淘宝一样的电商系统、搜索引擎系统等等，都存在一些技术任务，是需要你深入了解底层机制才能解决的。比如淘宝的基础技术团队就曾经贡献过，Java虚拟机即时编译功能中的一个补丁。
这反映出掌握底层技术能力的重要性，所以，如果你想进阶成为这个层次的工程师，不能只学学上层的语法，而是要把计算机语言从上层的语法到底层的运行机制都了解透彻。
本节课，我会对计算机程序如何运行，做一个解密，话题分成两个部分：
1.了解程序运行的环境，包括CPU、内存和操作系统，探知它们跟程序到底有什么关系。
2.了解程序运行的过程。比如，一个程序是怎么跑起来的，代码是怎样执行和跳转的，又是如何管理内存的。
首先，我们先来了解一下程序运行的环境。
程序运行的环境程序运行的过程中，主要是跟两个硬件（CPU和内存）以及一个软件（操作系统）打交道。
本质上，我们的程序只关心CPU和内存这两个硬件。你可能说：“不对啊，计算机还有其他硬件，比如显示器和硬盘啊。”但对我们的程序来说，操作这些硬件，也只是执行某些特定的驱动代码，跟执行其他代码并没有什么差异。
1.关注CPU和内存CPU的内部有很多组成部分，对于本课程来说，我们重点关注的是寄存器以及高速缓存，它们跟程序的执行机制和优化密切相关。
寄存器是CPU指令在进行计算的时候，临时数据存储的地方。CPU指令一般都会用到寄存器，比如，典型的一个加法计算（c=a+b）的过程是这样的：
指令1（mov）：从内存取a的值放到寄存器中；
指令2（add）：再把内存中b的值取出来与这个寄存器中的值相加，仍然保存在寄存器中；
指令3（mov）：最后再把寄存器中的数据写回内存中c的地址。
寄存器的速度也很快，所以能用寄存器就别用内存。尽量充分利用寄存器，是编译器做优化的内容之一。
而高速缓存可以弥补CPU的处理速度和内存访问速度之间的差距。所以，我们的指令在内存读一个数据的时候，它不是老老实实地只读进当前指令所需要的数据，而是把跟这个数据相邻的一组数据都读进高速缓存了。这就相当于外卖小哥送餐的时候，不会为每一单来回跑一趟，而是一次取一批，如果这一批外卖恰好都是同一个写字楼里的，那小哥的送餐效率就会很高。
内存和高速缓存的速度差异差不多是两个数量级，也就是一百倍。比如，高速缓存的读取时间可能是0.5ns，而内存的访问时间可能是50ns。不同硬件的参数可能有差异，但总体来说是几十倍到上百倍的差异。
你写程序时，尽量把某个操作所需的数据都放在内存中的连续区域中，不要零零散散地到处放，这样有利于充分利用高速缓存。这种优化思路，叫做数据的局部性。
这里提一句，在写系统级的程序时，你要对各种IO的时间有基本的概念，比如高速缓存、内存、磁盘、网络的IO大致都是什么数量级的。因为这都影响到系统的整体性能，也影响到你如何做程序优化。如果你需要对程序做更多的优化，还需要了解更多的CPU运行机制，包括流水线机制、并行机制等等，这里就不展开了。
讲完CPU之后，还有内存这个硬件。
程序在运行时，操作系统会给它分配一块虚拟的内存空间，让它在运行期可以使用。我们目前使用的都是64位的机器，你可以用一个64位的长整型来表示内存地址，它能够表示的所有地址，我们叫做寻址空间。
64位机器的寻址空间就有2的64次方那么大，也就是有很多很多个TB（Terabyte），大到你的程序根本用不完。不过，操作系统一般会给予一定的限制，不会给你这么大的寻址空间，比如给到100来个G，这对一般的程序，也足够用了。
在存在操作系统的情况下，程序逻辑上可使用的内存一般大于实际的物理内存。程序在使用内存的时候，操作系统会把程序使用的逻辑地址映射到真实的物理内存地址。有的物理内存区域会映射进多个进程的地址空间。
对于不太常用的内存数据，操作系统会写到磁盘上，以便腾出更多可用的物理内存。
当然，也存在没有操作系统的情况，这个时候你的程序所使用的内存就是物理内存，我们必须自己做好内存的管理。
对于这个内存，该怎么用呢？
本质上来说，你想怎么用就怎么用，并没有什么特别的限制。一个编译器的作者，可以决定在哪儿放代码，在哪儿放数据，当然了，别的作者也可能采用其他的策略。实际上，C语言和Java虚拟机对内存的管理和使用策略就是不同的。
尽管如此，大多数语言还是会采用一些通用的内存管理模式。以C语言为例，会把内存划分为代码区、静态数据区、栈和堆。
一般来讲，代码区是在最低的地址区域，然后是静态数据区，然后是堆。而栈传统上是从高地址向低地址延伸，栈的最顶部有一块区域，用来保存环境变量。
代码区（也叫文本段）存放编译完成以后的机器码。这个内存区域是只读的，不会再修改，但也不绝对。现代语言的运行时已经越来越动态化，除了保存机器码，还可以存放中间代码，并且还可以在运行时把中间代码编译成机器码，写入代码区。
静态数据区保存程序中全局的变量和常量。它的地址在编译期就是确定的，在生成的代码里直接使用这个地址就可以访问它们，它们的生存期是从程序启动一直到程序结束。它又可以细分为Data和BSS两个段。Data段中的变量是在编译期就初始化好的，直接从程序装在进内存。BSS段中是那些没有声明初始化值的变量，都会被初始化成0。
堆适合管理生存期较长的一些数据，这些数据在退出作用域以后也不会消失。比如，我们在某个方法里创建了一个对象并返回，并希望代表这个对象的数据在退出函数后仍然可以访问。
而栈适合保存生存期比较短的数据，比如函数和方法里的本地变量。它们在进入某个作用域的时候申请内存，退出这个作用域的时候就可以释放掉。
讲完了CPU和内存之后，我们再来看看跟程序打交道的操作系统。
2.程序和操作系统的关系程序跟操作系统的关系比较微妙：
一方面我们的程序可以编译成不需要操作系统也能运行，就像一些物联网应用那样，完全跑在裸设备上。
另一方面，有了操作系统的帮助，可以为程序提供便利，比如可以使用超过物理内存的存储空间，操作系统负责进行虚拟内存的管理。
在存在操作系统的情况下，因为很多进程共享计算机资源，所以就要遵循一些约定。这就仿佛办公室是所有同事共享的，那么大家就都要遵守一些约定，如果一个人大声喧哗，就会影响到其他人。
程序需要遵守的约定包括：程序文件的二进制格式约定，这样操作系统才能程序正确地加载进来，并为同一个程序的多个进程共享代码区。在使用寄存器和栈的时候也要遵守一些约定，便于操作系统在不同的进程之间切换的时候、在做系统调用的时候，做好上下文的保护。
所以，我们编译程序的时候，要知道需要遵守哪些约定。因为就算是使用同样的CPU，针对不同的操作系统，编译的结果也是非常不同的。
好了，我们了解了程序运行时的硬件和操作系统环境。接下来，我们看看程序运行时，是怎么跟它们互动的。
程序运行的过程你天天运行程序，可对于程序运行的细节，真的清楚吗？
1.程序运行的细节首先，可运行的程序一般是由操作系统加载到内存的，并且定位到代码区里程序的入口开始执行。比如，C语言的main函数的第一行代码。
每次加载一条代码，程序都会顺序执行，碰到跳转语句，才会跳到另一个地址执行。CPU里有一个指令寄存器，里面保存了下一条指令的地址。
假设我们运行这样一段代码编译后形成的程序：
int main(){ int a = 1; foo(3); bar(); } int foo(int c){ int b = 2; return b+c; }</description></item><item><title>21_面向流水线的指令设计（下）：奔腾4是怎么失败的？</title><link>https://artisanbox.github.io/4/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/21/</guid><description>上一讲，我给你初步介绍了CPU的流水线技术。乍看起来，流水线技术是一个提升性能的灵丹妙药。它通过把一条指令的操作切分成更细的多个步骤，可以避免CPU“浪费”。每一个细分的流水线步骤都很简单，所以我们的单个时钟周期的时间就可以设得更短。这也变相地让CPU的主频提升得很快。
这一系列的优点，也引出了现代桌面CPU的最后一场大战，也就是Intel的Pentium 4和AMD的Athlon之间的竞争。在技术上，这场大战Intel可以说输得非常彻底，Pentium 4系列以及后续Pentium D系列所使用的NetBurst架构被完全抛弃，退出了历史舞台。但是在商业层面，Intel却通过远超过AMD的财力、原本就更大的市场份额、无所不用的竞争手段，以及最终壮士断腕般放弃整个NetBurst架构，最终依靠新的酷睿品牌战胜了AMD。
在此之后，整个CPU领域竞争的焦点，不再是Intel和AMD之间的桌面CPU之战。在ARM架构通过智能手机的快速普及，后来居上，超越Intel之后，移动时代的CPU之战，变成了高通、华为麒麟和三星之间的“三国演义”。
“主频战争”带来的超长流水线我们在第3讲里讲过，我们其实并不能简单地通过CPU的主频，就来衡量CPU乃至计算机整机的性能。因为不同的CPU实际的体系架构和实现都不一样。同样的CPU主频，实际的性能可能差别很大。所以，在工业界，更好的衡量方式通常是，用SPEC这样的跑分程序，从多个不同的实际应用场景，来衡量计算机的性能。
但是，跑分对于消费者来说还是太复杂了。在Pentium 4的CPU面世之前，绝大部分消费者并不是根据跑分结果来判断CPU的性能的。大家判断一个CPU的性能，通常只看CPU的主频。而CPU的厂商们也通过不停地提升主频，把主频当成技术竞赛的核心指标。
Intel一向在“主频战争”中保持领先，但是到了世纪之交的1999年到2000年，情况发生了变化。
1999年，AMD发布了基于K7架构的Athlon处理器，其综合性能超越了当年的Pentium III。2000年，在大部分CPU还在500～850MHz的频率下运行的时候，AMD推出了第一代Athlon 1000处理器，成为第一款1GHz主频的消费级CPU。在2000年前后，AMD的CPU不但性能和主频比Intel的要强，价格还往往只有Intel的2/3。
在巨大的外部压力之下，Intel在2001年推出了新一代的NetBurst架构CPU，也就是Pentium 4和Pentium D。Pentium 4的CPU有个最大的特点，就是高主频。2000年的Athlon 1000的主频在当时是最高的，1GHz，然而Pentium 4设计的目标最高主频是10GHz。
为了达到这个10GHz，Intel的工程师做出了一个重大的错误决策，就是在NetBurst架构上，使用超长的流水线。这个超长流水线有多长呢？我们拿在Pentium 4之前和之后的CPU的数字做个比较，你就知道了。
Pentium 4之前的Pentium III CPU，流水线的深度是11级，也就是一条指令最多会拆分成11个更小的步骤来操作，而CPU同时也最多会执行11条指令的不同Stage。随着技术发展到今天，你日常用的手机ARM的CPU或者Intel i7服务器的CPU，流水线的深度是14级。
可以看到，差不多20年过去了，通过技术进步，现代CPU还是增加了一些流水线深度的。那2000年发布的Pentium 4的流水线深度是多少呢？答案是20级，比Pentium III差不多多了一倍，而到了代号为Prescott的90纳米工艺处理器Pentium 4，Intel更是把流水线深度增加到了31级。
要知道，增加流水线深度，在同主频下，其实是降低了CPU的性能。因为一个Pipeline Stage，就需要一个时钟周期。那么我们把任务拆分成31个阶段，就需要31个时钟周期才能完成一个任务；而把任务拆分成11个阶段，就只需要11个时钟周期就能完成任务。在这种情况下，31个Stage的3GHz主频的CPU，其实和11个Stage的1GHz主频的CPU，性能是差不多的。事实上，因为每个Stage都需要有对应的Pipeline寄存器的开销，这个时候，更深的流水线性能可能还会更差一些。
我在上一讲也说过，流水线技术并不能缩短单条指令的响应时间这个性能指标，但是可以增加在运行很多条指令时候的吞吐率。因为不同的指令，实际执行需要的时间是不同的。我们可以看这样一个例子。我们顺序执行这样三条指令。
一条整数的加法，需要200ps。 一条整数的乘法，需要300ps。 一条浮点数的乘法，需要600ps。 如果我们是在单指令周期的CPU上运行，最复杂的指令是一条浮点数乘法，那就需要600ps。那这三条指令，都需要600ps。三条指令的执行时间，就需要1800ps。
如果我们采用的是6级流水线CPU，每一个Pipeline的Stage都只需要100ps。那么，在这三个指令的执行过程中，在指令1的第一个100ps的Stage结束之后，第二条指令就开始执行了。在第二条指令的第一个100ps的Stage结束之后，第三条指令就开始执行了。这种情况下，这三条指令顺序执行所需要的总时间，就是800ps。那么在1800ps内，使用流水线的CPU比单指令周期的CPU就可以多执行一倍以上的指令数。
虽然每一条指令从开始到结束拿到结果的时间并没有变化，也就是响应时间没有变化。但是同样时间内，完成的指令数增多了，也就是吞吐率上升了。
新的挑战：冒险和分支预测那到这里可能你就要问了，这样看起来不是很好么？Intel的CPU支持的指令集很大，我们之前说过有2000多条指令。有些指令很简单，执行也很快，比如无条件跳转指令，不需要通过ALU进行任何计算，只要更新一下PC寄存器里面的内容就好了。而有些指令很复杂，比如浮点数的运算，需要进行指数位比较、对齐，然后对有效位进行移位，然后再进行计算。两者的执行时间相差二三十倍也很正常。
既然这样，Pentium 4的超长流水线看起来很合理呀，为什么Pentium 4最终成为Intel在技术架构层面的大失败呢？
第一个，自然是我们在第3讲里讲过的功耗问题。提升流水线深度，必须要和提升CPU主频同时进行。因为在单个Pipeline Stage能够执行的功能变简单了，也就意味着单个时钟周期内能够完成的事情变少了。所以，只有提升时钟周期，CPU在指令的响应时间这个指标上才能保持和原来相同的性能。
同时，由于流水线深度的增加，我们需要的电路数量变多了，也就是我们所使用的晶体管也就变多了。
主频的提升和晶体管数量的增加都使得我们CPU的功耗变大了。这个问题导致了Pentium 4在整个生命周期里，都成为了耗电和散热的大户。而Pentium 4是在2000～2004年作为Intel的主打CPU出现在市场上的。这个时间段，正是笔记本电脑市场快速发展的时间。在笔记本电脑上，功耗和散热比起台式机是一个更严重的问题了。即使性能更好，别人的笔记本可以用上2小时，你的只能用30分钟，那谁也不爱买啊！
更何况，Pentium 4的性能还更差一些。这个就要我们说到第二点了，就是上面说的流水线技术带来的性能提升，是一个理想情况。在实际的程序执行中，并不一定能够做得到。
还回到我们刚才举的三条指令的例子。如果这三条指令，是下面这样的三条代码，会发生什么情况呢？
int a = 10 + 5; // 指令1 int b = a * 2; // 指令2 float c = b * 1.</description></item><item><title>21｜加深对栈的理解：实现尾递归和尾调用优化</title><link>https://artisanbox.github.io/3/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/23/</guid><description>你好，我是宫文学。
前面几节课，我们在实现生成本地代码的过程中，对汇编语言、栈和栈桢有关的知识点都进行了比较深入的了解。通过这些学习，你应该对程序的运行机制有了更加透彻的理解。
那么今天这节课，作为第一部分起步篇的结尾，我们就来检验一下自己的学习成果吧！具体一点，我们就是要运用我们前面已经学过的知识点，特别是关于栈和栈桢的知识点，来实现两个有用的优化功能，也就是尾递归和尾调用的优化。
这两个优化有助于我们更好地利用栈里的内存空间，也能够提高程序的性能，对于我们后面实现函数式编程特性也具有很重要的意义。另外，这样的练习，也会加深我们对栈和栈桢、对程序的控制流，还有对程序的运行机制的理解。
好了，我们先从尾递归入手吧，说说尾递归是怎么回事，还有它有怎样的运行特点，看看我们为什么需要去优化它。
递归函数和尾递归学习编程的同学都应该知道，递归是一种重要的思维方式。我们现实世界的很多事物，用递归来表达是非常自然的。在递归的思维里，解决整体的问题和解决局部问题的思路是相同的。
在我们这个课程里，我们学习的语法分析的方法，也采用了递归的思维：我们给一个大程序做语法分析，会分解成给每一个函数、每一条语句做语法分析。不管在哪个颗粒度上，算法的思路都是相同的。
递归思想的一个更具体的使用方式，就是递归函数。当前的各种高级语言，都会支持递归函数，也就是允许在一个函数内部调用自身。
你可以看看下面这个例子，这个例子是用来实现阶乘的计算的。
function factorial (n:number):number{ if (n &amp;lt; 1) return 1; else return n * factorial(n-1); } 在这里，n的阶乘f(n)，就等于n * f(n-1)。这是典型的递归思维，解决一个整体问题f(n)，能够被转化为解决其局部问题f(n-1)。n的值变化了，但解决问题的思路是一致的。
最近几年，函数式编程的思想又重新流行起来。在一些纯函数式的编程语言中，递归是其核心编程机制，被大量使用。
不过，递归函数的大量使用，对程序的运行时机制是一个挑战。因为我们已经知道，在标准的程序运行模式下，每一次函数调用，都要为这个函数创建一个栈桢。如果递归的层次很深，那么栈桢的数量就会非常多，最终引起“stack overflow”，也就是栈溢出的错误，这是我们在使用栈的时候最怕遇到的问题。
另外，我们还知道，当我们在进行函数调用的时候，还会产生比较大的性能开销。这些开销包括：设置参数、设置返回地址、移动栈顶指针、保护相关的寄存器，等等。特别是，在这个过程中，一般都会产生内存读写的动作，这会对性能产生比较大的影响。
所以说，虽然递归函数很有用，但你在学习编程的时候，可能你的老师会告诉你，如果对性能和内存占用有较高的要求，那么我们尽量不用递归算法实现，而是把递归算法改成等价的非递归算法。
不过，现代编译器也在努力帮助解决这个问题。比如在上一节课中，我们就已经见到了C语言编译器的一个功能，它在编译斐波那契数列的过程中，能够把其中一半的递归调用转变成一个循环语句，从而减少了递归调用导致的开销。
但在这一节课呢，我们不会试图一下子就实现这么复杂的编译优化功能，而是先针对递归调用中的一个特殊情况而进行优化，这个特殊情况就是尾递归。
那什么是尾递归呢？尾递归就是在return语句中，return后面只跟了一个递归调用的情况。在上面的例子中，你会看到return后面跟着的是n * factorial(n-1)，这种情况不是尾递归。不过，我们可以把示例程序改写成尾递归的情形，我写在了下面：
function factorial(n:number, total:number):number{ if (n &amp;lt;= 1) return total; else return factorial(n-1, n*total); } 这个新的阶乘函数使用了两个参数，其中第二个参数保存的是阶乘的累积值。如果要计算10的阶乘，那么我们需要函数factorial(10, 1)。你可以仔细看一下factorial函数的两个不同的版本，它们确实是等价的。但第二个版本中的第二个return语句呢，就是一个标准的尾递归调用。
我们为什么要谈论尾递归呢？这是因为尾递归在栈桢的使用上有其独特的特点，使得我们可以用很简单的方法就能实现优化。
那么接下来，我们就分析一下递归函数在栈的使用上的特点，这有利于我们制定优化策略。
递归函数对栈的使用你可以用我们上一节课的PlayScript版本，使用make example_fact命令来生成上面示例程序的汇编代码和可执行文件。
这个汇编文件是没有做尾递归优化的，你可以看一下它的内容，看看它的栈桢是什么结构。
_factorial: .cfi_startproc ## bb.0 pushq %rbp movq %rsp, %rbp cmpl $1, %edi # cmpl $1, var0 jg LBB0_2 ## bb.</description></item><item><title>22_Julia编译器（一）：如何让动态语言性能很高？</title><link>https://artisanbox.github.io/7/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/22/</guid><description>你好，我是宫文学。
Julia这门语言，其实是最近几年才逐渐获得人们越来越多的关注的。有些人会拿它跟Python进行比较，认为Julia已经成为了Python的劲敌，甚至还有人觉得它在未来可能会取代Python的地位。虽然这样的说法可能是有点夸张了，不过Julia确实是有它的可取之处的。
为什么这么说呢？前面我们已经研究了Java、Python和JavaScript这几门主流语言的编译器，这几门语言都是很有代表性的：Java语言是静态类型的、编译型的语言；Python语言是动态类型的、解释型的语言；JavaScript是动态类型的语言，但可以即时编译成本地代码来执行。
而Julia语言却声称同时兼具了静态编译型和动态解释型语言的优点：一方面它的性能很高，可以跟Java和C语言媲美；而另一方面，它又是动态类型的，编写程序时不需要指定类型。一般来说，我们很难能期望一门语言同时具有动态类型和静态类型语言的优点的，那么Julia又是如何实现这一切的呢？
原来它是充分利用了LLVM来实现即时编译的功能。因为LLVM是Clang、Rust、Swift等很多语言所采用的后端工具，所以我们可以借助Julia语言的编译器，来研究如何恰当地利用LLVM。不过，Julia使用LLVM的方法很有创造性，使得它可以同时具备这两类语言的优点。我将在这一讲中给你揭秘。
此外，Julia编译器的类型系统的设计也很独特，它体现了函数式编程的一些设计哲学，能够帮助你启迪思维。
还有一点，Julia来自MIT，这里也曾经是Lisp的摇篮，所以Julia有一种学术风和极客风相结合的品味，也值得你去仔细体会一下。
所以，接下来的两讲，我会带你来好好探究一下Julia的编译器。你从中能够学习到Julia编译器的处理过程，如何创造性地使用LLVM的即时编译功能、如何使用LLVM的优化功能，以及它的独特的类型系统和方法分派。
那今天这一讲，我会先带你来了解Julia的编译过程，以及它高性能背后的原因。
初步认识JuliaJulia的性能有多高呢？你可以去它的网站上看看与其他编程语言的性能对比：
图1：Julia和各种语言的性能对比可以看出，它的性能是在C、Rust这一个级别的，很多指标甚至比Java还要好，比起那些动态语言（如Python、R和Octave），那更是高了一到两个数量级。
所以，Julia的编译器声称它具备了静态类型语言的性能，确实是不虚此言的。
你可以从Julia的官网下载Julia的二进制版本和源代码。如果你下载的是源代码，那你可以用make debug编译成debug版本，这样比较方便用GDB或LLDB调试。
Julia的设计目的主要是用于科学计算。过去，这一领域的用户主要是使用R语言和Python，但麻省理工（MIT）的研究者们对它们的性能不够满意，同时又想要保留R和Python的友好性，于是就设计出了这门新的语言。目前这门语言受到了很多用户的欢迎，使用者也在持续地上升中。
我个人对它感兴趣的点，正是因为它打破了静态编译和动态编译语言的边界，我认为这体现了未来语言的趋势：编译和优化的过程是全生命周期的，而不局限在某个特定阶段。
好了，让我们先通过一个例子来认识Juia，直观了解一下这门语言的特点：
julia&amp;gt; function countdown(n) if n &amp;lt;= 0 println(&amp;quot;end&amp;quot;) else print(n, &amp;quot; &amp;quot;) countdown(n-1) end end countdown (generic function with 1 method) julia&amp;gt; countdown(10) 10 9 8 7 6 5 4 3 2 1 end 所以从这段示例代码中，可以看出，Julia主要有这样几个特点：
用function关键字来声明一个函数； 用end关键字作为块（函数声明、if语句、for语句等）的结尾； 函数的参数可以不用指定类型（变量声明也不需要），因为它是动态类型的； Julia支持递归函数。 那么Julia的编译器是用什么语言实现的呢？又是如何支持它的这些独特的特性的呢？带着这些好奇，让我们来看一看Julia编译器的源代码。
图2：Julia的源代码目录其实Julia的实现会让人有点困扰，因为它使用了4种语言：C、C++、Lisp和Julia自身。相比而言，CPython的实现只用了两种语言：C语言和Python。这种情况，就对社区的其他技术人员理解这个编译器和参与开发，带来了不小的困难。
Julia的作者用C语言实现了一些运行时的核心功能，包括垃圾收集器。他们是比较偏爱C语言的。C++主要是用来实现跟LLVM衔接的功能，因为LLVM是用C++实现的。
但是，为什么又冒出了一个Lisp语言呢？而且前端部分的主要功能都是用Lisp实现的。
原来，Julia中用到Lisp叫做femtolisp（简称flisp），这是杰夫·贝赞松（Jeff Bezanson）做的一个开源Lisp实现，当时的目标是做一个最小的、编译速度又最快的Lisp版本。后来Jeff Bezanson作为Julia的核心开发人员，又把flisp带进了Julia。
实际上，Julia语言本身也宣称是继承了Lisp语言的精髓。在其核心的设计思想里，在函数式编程风格和元编程功能方面，也确实是如此。Lisp在研究界一直有很多的追随者，Julia这个项目诞生于MIT，同时又主要服务于各种科研工作者，所以它也就带上了这种科学家的味道。它还有其他特性，也能看出这种科研工作者的倾向，比如说：
对于类型系统，Julia的开发者们进行了很好的形式化，是我在所有语言中看到的最像数学家做的类型系统。 在它的语法和语义设计上，带有Metalab和Mathematics这些数学软件的痕迹，科研工作者们应该很熟悉这种感觉。 在很多特性的实现上，都带有很强的前沿探索的特征，锋芒突出，不像我们平常使用的那些商业公司设计的计算机语言一样，追求四平八稳。 以上就是我对Julia的感觉，一种结合了数据家风格的自由不羁的极客风。实际上，Lisp最早的设计者约翰·麦卡锡（John McCarthy）就是一位数学博士，所以数学上的美感是Lisp给人的感受之一。而且，Lisp语言本身也是在MIT发源的，所以Julia可以说是继承了这个传统、这种风格。</description></item><item><title>22_MySQL有哪些“饮鸩止渴”提高性能的方法？</title><link>https://artisanbox.github.io/1/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/22/</guid><description>不知道你在实际运维过程中有没有碰到这样的情景：业务高峰期，生产环境的MySQL压力太大，没法正常响应，需要短期内、临时性地提升一些性能。
我以前做业务护航的时候，就偶尔会碰上这种场景。用户的开发负责人说，不管你用什么方案，让业务先跑起来再说。
但，如果是无损方案的话，肯定不需要等到这个时候才上场。今天我们就来聊聊这些临时方案，并着重说一说它们可能存在的风险。
短连接风暴正常的短连接模式就是连接到数据库后，执行很少的SQL语句就断开，下次需要的时候再重连。如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。
我在第1篇文章《基础架构：一条SQL查询语句是如何执行的？》中说过，MySQL建立连接的过程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的数据读写权限。
在数据库压力比较小的时候，这些额外的成本并不明显。
但是，短连接模型存在一个风险，就是一旦数据库处理得慢一些，连接数就会暴涨。max_connections参数，用来控制一个MySQL实例同时存在的连接数的上限，超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。对于被拒绝连接的请求来说，从业务角度看就是数据库不可用。
在机器负载比较高的时候，处理现有请求的时间变长，每个连接保持的时间也更长。这时，再有新建连接的话，就可能会超过max_connections的限制。
碰到这种情况时，一个比较自然的想法，就是调高max_connections的值。但这样做是有风险的。因为设计max_connections这个参数的目的是想保护MySQL，如果我们把它改得太大，让更多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑上，结果可能是适得其反，已经连接的线程拿不到CPU资源去执行业务的SQL请求。
那么这种情况下，你还有没有别的建议呢？我这里还有两种方法，但要注意，这些方法都是有损的。
第一种方法：先处理掉那些占着连接但是不工作的线程。
max_connections的计算，不是看谁在running，是只要连着就占用一个计数位置。对于那些不需要保持的连接，我们可以通过kill connection主动踢掉。这个行为跟事先设置wait_timeout的效果是一样的。设置wait_timeout参数表示的是，一个线程空闲wait_timeout这么多秒之后，就会被MySQL直接断开连接。
但是需要注意，在show processlist的结果里，踢掉显示为sleep的线程，可能是有损的。我们来看下面这个例子。
图1 sleep线程的两种状态在上面这个例子里，如果断开session A的连接，因为这时候session A还没有提交，所以MySQL只能按照回滚事务来处理；而断开session B的连接，就没什么大影响。所以，如果按照优先级来说，你应该优先断开像session B这样的事务外空闲的连接。
但是，怎么判断哪些是事务外空闲的呢？session C在T时刻之后的30秒执行show processlist，看到的结果是这样的。
图2 sleep线程的两种状态，show processlist结果图中id=4和id=5的两个会话都是Sleep 状态。而要看事务具体状态的话，你可以查information_schema库的innodb_trx表。
图3 从information_schema.innodb_trx查询事务状态这个结果里，trx_mysql_thread_id=4，表示id=4的线程还处在事务中。
因此，如果是连接数过多，你可以优先断开事务外空闲太久的连接；如果这样还不够，再考虑断开事务内空闲太久的连接。
从服务端断开连接使用的是kill connection + id的命令， 一个客户端处于sleep状态时，它的连接被服务端主动断开后，这个客户端并不会马上知道。直到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。
从数据库端主动断开连接可能是有损的，尤其是有的应用端收到这个错误后，不重新连接，而是直接用这个已经不能用的句柄重试查询。这会导致从应用端看上去，“MySQL一直没恢复”。
你可能觉得这是一个冷笑话，但实际上我碰到过不下10次。
所以，如果你是一个支持业务的DBA，不要假设所有的应用代码都会被正确地处理。即使只是一个断开连接的操作，也要确保通知到业务开发团队。
第二种方法：减少连接过程的消耗。
有的业务代码会在短时间内先大量申请数据库连接做备用，如果现在数据库确认是被连接行为打挂了，那么一种可能的做法，是让数据库跳过权限验证阶段。
跳过权限验证的方法是：重启数据库，并使用–skip-grant-tables参数启动。这样，整个MySQL会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。
但是，这种方法特别符合我们标题里说的“饮鸩止渴”，风险极高，是我特别不建议使用的方案。尤其你的库外网可访问的话，就更不能这么做了。
在MySQL 8.0版本里，如果你启用–skip-grant-tables参数，MySQL会默认把 --skip-networking参数打开，表示这时候数据库只能被本地的客户端连接。可见，MySQL官方对skip-grant-tables这个参数的安全问题也很重视。
除了短连接数暴增可能会带来性能问题外，实际上，我们在线上碰到更多的是查询或者更新语句导致的性能问题。其中，查询问题比较典型的有两类，一类是由新出现的慢查询导致的，一类是由QPS（每秒查询数）突增导致的。而关于更新语句导致的性能问题，我会在下一篇文章和你展开说明。
慢查询性能问题在MySQL中，会引发性能问题的慢查询，大体有以下三种可能：
索引没有设计好；
SQL语句没写好；
MySQL选错了索引。
接下来，我们就具体分析一下这三种可能，以及对应的解决方案。</description></item><item><title>22_冒险和预测（一）：hazard是“危”也是“机”</title><link>https://artisanbox.github.io/4/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/22/</guid><description>过去两讲，我为你讲解了流水线设计CPU所需要的基本概念。接下来，我们一起来看看，要想通过流水线设计来提升CPU的吞吐率，我们需要冒哪些风险。
任何一本讲解CPU的流水线设计的教科书，都会提到流水线设计需要解决的三大冒险，分别是结构冒险（Structural Hazard）、数据冒险（Data Hazard）以及控制冒险（Control Hazard）。
这三大冒险的名字很有意思，它们都叫作hazard（冒险）。喜欢玩游戏的话，你应该知道一个著名的游戏，生化危机，英文名就叫Biohazard。的确，hazard还有一个意思就是“危机”。那为什么在流水线设计里，hazard没有翻译成“危机”，而是要叫“冒险”呢？
在CPU的流水线设计里，固然我们会遇到各种“危险”情况，使得流水线里的下一条指令不能正常运行。但是，我们其实还是通过“抢跑”的方式，“冒险”拿到了一个提升指令吞吐率的机会。流水线架构的CPU，是我们主动进行的冒险选择。我们期望能够通过冒险带来更高的回报，所以，这不是无奈之下的应对之举，自然也算不上什么危机了。
事实上，对于各种冒险可能造成的问题，我们其实都准备好了应对的方案。这一讲里，我们先从结构冒险和数据冒险说起，一起来看看这些冒险及其对应的应对方案。
结构冒险：为什么工程师都喜欢用机械键盘？我们先来看一看结构冒险。结构冒险，本质上是一个硬件层面的资源竞争问题，也就是一个硬件电路层面的问题。
CPU在同一个时钟周期，同时在运行两条计算机指令的不同阶段。但是这两个不同的阶段，可能会用到同样的硬件电路。
最典型的例子就是内存的数据访问。请你看看下面这张示意图，其实就是第20讲里对应的5级流水线的示意图。
可以看到，在第1条指令执行到访存（MEM）阶段的时候，流水线里的第4条指令，在执行取指令（Fetch）的操作。访存和取指令，都要进行内存数据的读取。我们的内存，只有一个地址译码器的作为地址输入，那就只能在一个时钟周期里面读取一条数据，没办法同时执行第1条指令的读取内存数据和第4条指令的读取指令代码。
同一个时钟周期，两个不同指令访问同一个资源类似的资源冲突，其实你在日常使用计算机的时候也会遇到。最常见的就是薄膜键盘的“锁键”问题。常用的最廉价的薄膜键盘，并不是每一个按键的背后都有一根独立的线路，而是多个键共用一个线路。如果我们在同一时间，按下两个共用一个线路的按键，这两个按键的信号就没办法都传输出去。
这也是为什么，重度键盘用户，都要买贵一点儿的机械键盘或者电容键盘。因为这些键盘的每个按键都有独立的传输线路，可以做到“全键无冲”，这样，无论你是要大量写文章、写程序，还是打游戏，都不会遇到按下了键却没生效的情况。
“全键无冲”这样的资源冲突解决方案，其实本质就是增加资源。同样的方案，我们一样可以用在CPU的结构冒险里面。对于访问内存数据和取指令的冲突，一个直观的解决方案就是把我们的内存分成两部分，让它们各有各的地址译码器。这两部分分别是存放指令的程序内存和存放数据的数据内存。
这样把内存拆成两部分的解决方案，在计算机体系结构里叫作哈佛架构（Harvard Architecture），来自哈佛大学设计Mark I型计算机时候的设计。对应的，我们之前说的冯·诺依曼体系结构，又叫作普林斯顿架构（Princeton Architecture）。从这些名字里，我们可以看到，早年的计算机体系结构的设计，其实产生于美国各个高校之间的竞争中。
不过，我们今天使用的CPU，仍然是冯·诺依曼体系结构的，并没有把内存拆成程序内存和数据内存这两部分。因为如果那样拆的话，对程序指令和数据需要的内存空间，我们就没有办法根据实际的应用去动态分配了。虽然解决了资源冲突的问题，但是也失去了灵活性。
现代CPU架构，借鉴了哈佛架构，在高速缓存层面拆分成指令缓存和数据缓存不过，借鉴了哈佛结构的思路，现代的CPU虽然没有在内存层面进行对应的拆分，却在CPU内部的高速缓存部分进行了区分，把高速缓存分成了指令缓存（Instruction Cache）和数据缓存（Data Cache）两部分。
内存的访问速度远比CPU的速度要慢，所以现代的CPU并不会直接读取主内存。它会从主内存把指令和数据加载到高速缓存中，这样后续的访问都是访问高速缓存。而指令缓存和数据缓存的拆分，使得我们的CPU在进行数据访问和取指令的时候，不会再发生资源冲突的问题了。
数据冒险：三种不同的依赖关系结构冒险是一个硬件层面的问题，我们可以靠增加硬件资源的方式来解决。然而还有很多冒险问题，是程序逻辑层面的事儿。其中，最常见的就是数据冒险。
数据冒险，其实就是同时在执行的多个指令之间，有数据依赖的情况。这些数据依赖，我们可以分成三大类，分别是先写后读（Read After Write，RAW）、先读后写（Write After Read，WAR）和写后再写（Write After Write，WAW）。下面，我们分别看一下这几种情况。
先写后读（Read After Write）我们先来一起看看先写后读这种情况。这里有一段简单的C语言代码编译出来的汇编指令。这段代码简单地定义两个变量 a 和 b，然后计算 a = a + 2。再根据计算出来的结果，计算 b = a + 3。
int main() { int a = 1; int b = 2; a = a + 2; b = a + 3; } int main() { 0: 55 push rbp 1: 48 89 e5 mov rbp,rsp int a = 1; 4: c7 45 fc 01 00 00 00 mov DWORD PTR [rbp-0x4],0x1 int b = 2; b: c7 45 f8 02 00 00 00 mov DWORD PTR [rbp-0x8],0x2 a = a + 2; 12: 83 45 fc 02 add DWORD PTR [rbp-0x4],0x2 b = a + 3; 16: 8b 45 fc mov eax,DWORD PTR [rbp-0x4] 19: 83 c0 03 add eax,0x3 1c: 89 45 f8 mov DWORD PTR [rbp-0x8],eax } 1f: 5d pop rbp 20: c3 ret 你可以看到，在内存地址为12的机器码，我们把0x2添加到 rbp-0x4 对应的内存地址里面。然后，在紧接着的内存地址为16的机器码，我们又要从rbp-0x4这个内存地址里面，把数据写入到eax这个寄存器里面。</description></item><item><title>22_哈希算法（下）：哈希算法在分布式系统中有哪些应用？</title><link>https://artisanbox.github.io/2/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/23/</guid><description>上一节，我讲了哈希算法的四个应用，它们分别是：安全加密、数据校验、唯一标识、散列函数。今天，我们再来看剩余三种应用：负载均衡、数据分片、分布式存储。
你可能已经发现，这三个应用都跟分布式系统有关。没错，今天我就带你看下，哈希算法是如何解决这些分布式问题的。
应用五：负载均衡我们知道，负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。
最直接的方法就是，维护一张映射关系表，这张表的内容是客户端IP地址或者会话ID与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：
如果客户端很多，映射表可能会很大，比较浪费内存空间；
客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；
如果借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端IP地址或者会话ID计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个IP过来的所有请求，都路由到同一个后端服务器上。
应用六：数据分片哈希算法还可以用于数据的分片。我这里有两个例子。
1.如何统计“搜索关键词”出现的次数？假如我们有1T的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？
我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。
针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用n台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号。
这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。
实际上，这里的处理过程也是MapReduce的基本设计思想。
2.如何快速判断图片是否在图库中？如何快速判断图片是否在图库中？上一节我们讲过这个例子，不知道你还记得吗？当时我介绍了一种方法，即给每个图片取唯一标识（或者信息摘要），然后构建散列表。
假设现在我们的图库中有1亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而1亿张图片构建散列表显然远远超过了单台机器的内存上限。
我们同样可以对数据进行分片，然后采用多机处理。我们准备n台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数n求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。
当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数n求余取模。假设得到的值是k，那就去编号k的机器构建的散列表中查找。
现在，我们来估算一下，给这1亿张图片构建散列表大约需要多少台机器。
散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设我们通过MD5来计算哈希值，那长度就是128比特，也就是16字节。文件路径长度的上限是256字节，我们可以假设平均长度是128字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用8字节。所以，散列表中每个数据单元就占用152字节（这里只是估算，并不准确）。
假设一台机器的内存大小为2GB，散列表的装载因子为0.75，那一台机器可以给大约1000万（2GB*0.75/152）张图片构建散列表。所以，如果要对1亿张图片构建索引，需要大约十几台机器。在工程中，这种估算还是很重要的，能让我们事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性。
实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU等资源的限制。
应用七：分布式存储现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。
该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。
但是，如果数据增多，原来的10个机器已经无法承受了，我们就需要扩容了，比如扩到11个机器，这时候麻烦就来了。因为，这里并不是简单地加个机器就可以了。
原来的数据是通过与10来取模的。比如13这个数据，存储在编号为3这台机器上。但是新加了一台机器中，我们对数据按照11取模，原来13这个数据就被分配到2号这台机器上了。
因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。
所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了。
假设我们有k个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成m个小区间（m远大于k），每个机器负责m/k个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。
一致性哈希算法的基本思想就是这么简单。除此之外，它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来。这里我就不展开讲了，如果感兴趣，你可以看下这个介绍。
除了我们上面讲到的分布式缓存，实际上，一致性哈希算法的应用非常广泛，在很多分布式存储系统中，都可以见到一致性哈希算法的影子。
解答开篇&amp;amp;内容小结这两节的内容理论不多，比较贴近具体的开发。今天我讲了三种哈希算法在分布式系统中的应用，它们分别是：负载均衡、数据分片、分布式存储。
在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。
课后思考这两节我总共讲了七个哈希算法的应用。实际上，我讲的也只是冰山一角，哈希算法还有很多其他的应用，比如网络协议中的CRC校验、Git commit id等等。除了这些，你还能想到其他用到哈希算法的地方吗？
欢迎留言和我分享，我会第一时间给你反馈。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>22_生成汇编代码（一）：汇编语言其实不难学</title><link>https://artisanbox.github.io/6/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/22/</guid><description>敲黑板：课程用的是GNU汇编器，macOS和Linux已内置，本文的汇编语言的写法是GNU汇编器规定的写法。Windows系统可安装MinGW或Linux虚拟机。
对于静态编译型语言，比如C语言和Go语言，编译器后端的任务就是生成汇编代码，然后再由汇编器生成机器码，生成的文件叫目标文件，最后再使用链接器就能生成可执行文件或库文件了。
就算像JavaScript这样的解释执行的语言，也要在运行时利用类似的机制生成机器码，以便调高执行的速度。Java的字节码，在运行时通常也会通过JIT机制编译成机器码。而汇编语言是完成这些工作的基础。
对你来说，掌握汇编语言是十分有益的，因为哪怕掌握一小点儿汇编技能，就能应用到某项工作中，比如，在C语言里嵌入汇编，实现某个特殊功能；或者读懂某些底层类库或驱动程序的代码，因为它可能是用汇编写的。
本节课，我先带你了解一下汇编语言，来个破冰之旅。然后在接下来的课程中再带你基于AST手工生成汇编代码，破除你对汇编代码的恐惧，了解编译期后端生成汇编代码的原理。
以后，当你看到高级语言的代码，以及IR时，就可以想象出来它对应的汇编代码是什么样子，实现从上层到底层认知的贯通。
了解汇编语言机器语言都是0101的二进制的数据，不适合我们阅读。而汇编语言，简单来说，是可读性更好的机器语言，基本上每条指令都可以直接翻译成一条机器码。
跟你日常使用的高级语言相比，汇编语言的语法特别简单，但它要跟硬件（CPU和内存）打交道，我们来体会一下。
计算机的处理器有很多不同的架构，比如x86-64、ARM、Power等，每种处理器的指令集都不相同，那也就意味着汇编语言不同。我们目前用的电脑，CPU一般是x86-64架构，是64位机。（如不做特别说明，本课程都是以x86-64架构作为例子的）。
说了半天，汇编代码长什么样子呢？我用C语言写的例子来生成一下汇编代码。
#include &amp;lt;stdio.h&amp;gt; int main(int argc, char* argv[]){ printf(&amp;quot;Hello %s!\n&amp;quot;, &amp;quot;Richard&amp;quot;); return 0; } 在macOS中输入下面的命令，其中的-S参数就是告诉编译器把源代码编译成汇编代码，而-O2参数告诉编译器进行2级优化，这样生成的汇编代码会短一些：
clang -S -O2 hello.c -o hello.s 或者： gcc -S -O2 hello.c -o hello.s 生成的汇编代码是下面的样子：
.section __TEXT,__text,regular,pure_instructions .build_version macos, 10, 14 sdk_version 10, 14 .globl _main ## -- Begin function main .p2align 4, 0x90 _main: ## @main .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .</description></item><item><title>22_瞧一瞧Linux：伙伴系统如何分配内存？</title><link>https://artisanbox.github.io/9/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/22/</guid><description>你好，我是LMOS。
前面我们实现了Cosmos的内存管理组件，相信你对计算机内存管理已经有了相当深刻的认识和见解。那么，像Linux这样的成熟操作系统，又是怎样实现内存管理的呢？
这就要说到Linux系统中，用来管理物理内存页面的伙伴系统，以及负责分配比页更小的内存对象的SLAB分配器了。
我会通过两节课给你理清这两种内存管理技术，这节课我们先来说说伙伴系统，下节课再讲SLAB。只要你紧跟我的思路，再加上前面的学习，真正理解这两种技术也并不难。
伙伴系统伙伴系统源于Sun公司的Solaris操作系统，是Solaris操作系统上极为优秀的物理内存页面管理算法。
但是，好东西总是容易被别人窃取或者效仿，伙伴系统也成了Linux的物理内存管理算法。由于Linux的开放和非赢利，这自然无可厚非，这不得不让我们想起了鲁迅《孔乙己》中的：“窃书不算偷”。
那Linux上伙伴系统算法是怎样实现的呢？我们不妨从一些重要的数据结构开始入手。
怎样表示一个页Linux也是使用分页机制管理物理内存的，即Linux把物理内存分成4KB大小的页面进行管理。那Linux用了一个什么样的数据结构，表示一个页呢？
早期Linux使用了位图，后来使用了字节数组，但是现在Linux定义了一个page结构体来表示一个页，代码如下所示。
struct page { //page结构体的标志，它决定页面是什么状态 unsigned long flags; union { struct { //挂载上级结构的链表 struct list_head lru; //用于文件系统，address_space结构描述上文件占用了哪些内存页面 struct address_space *mapping; pgoff_t index; unsigned long private; }; //DMA设备的地址 struct { dma_addr_t dma_addr; }; //当页面用于内存对象时指向相关的数据结构 struct { union { struct list_head slab_list; struct { struct page *next; #ifdef CONFIG_64BIT int pages; int pobjects; #else short int pages; short int pobjects; #endif }; }; //指向管理SLAB的结构kmem_cache struct kmem_cache *slab_cache; //指向SLAB的第一个对象 void *freelist; union { void *s_mem; unsigned long counters; struct { unsigned inuse:16; unsigned objects:15; unsigned frozen:1; }; }; }; //用于页表映射相关的字段 struct { unsigned long _pt_pad_1; pgtable_t pmd_huge_pte; unsigned long _pt_pad_2; union { struct mm_struct *pt_mm; atomic_t pt_frag_refcount; }; //自旋锁 #if ALLOC_SPLIT_PTLOCKS spinlock_t *ptl; #else spinlock_t ptl; #endif }; //用于设备映射 struct { struct dev_pagemap *pgmap; void *zone_device_data; }; struct rcu_head rcu_head; }; //页面引用计数 atomic_t _refcount; #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS int _last_cpupid; #endif } _struct_page_alignment; 这个page结构看上去非常巨大，信息量很多，但其实它占用的内存很少，根据Linux内核配置选项不同，占用20～40个字节空间。page结构大量使用了C语言union联合体定义结构字段，这个联合体的大小，要根据它里面占用内存最大的变量来决定。</description></item><item><title>22_范式：如何消除冗余，实现高效存取？</title><link>https://artisanbox.github.io/8/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/22/</guid><description>你好，我是朱晓峰。今天，我们来聊一聊数据表设计的范式。
在超市项目的设计阶段，超市经营者把他们正在用的Excel表格给到我们，要求我们把这些数据存储到超市管理系统的数据库中。为了方便你理解，我挑选了1个有代表性的表来举例说明。
进货单表（import）：
这个表中的字段很多，包含了各种信息，表里的数据量也很惊人。我们刚拿到这个表的时候，光是打开表这个操作，就需要等十几秒。
仔细一看，发现表里重复的数据非常多：比如第一行和第二行的数据，同样是3478号单据，供货商编号、供货商名称和仓库，这3个字段的信息完全相同。可是这2条数据的后半部分又不相同，因此，并不能认为它们是冗余数据而删除。
其实，造成这种问题的原因是这张表的设计非常不合理，大量重复导致表变得庞大，效率极低。
在我们的工作场景中，这种由于数据表结构设计不合理，而导致的数据重复的现象并不少见，往往是系统虽然能够运行，承载能力却很差，稍微有点流量，就会出现内存不足、CUP使用率飙升的情况，甚至会导致整个项目失败。
所以，高效可靠的设计是提升数据库工作效率的关键。那该怎么设计呢？有没有什么可以参考的设计规范呢？自然是有了。
接下来，我就带你重新设计一下刚刚的进货单表，在这个过程中给你具体介绍一下数据表设计的三大范式，分别是第一范式（1NF）、第二范式（2NF）和第三范式（3NF），这些范式可以帮助我们设计出简洁高效的数据表，进而提高系统的效率。
我先来介绍一下最基本的第一范式。
第一范式我们对这张进货单表重新设计的第一步，就是要把所有的列，也就是字段，都确认一遍，确保每个字段只包含一种数据。如果各种数据都混合在一起，就无法通过后面的拆解，把重复的数据去掉。
其实，这就是第一范式所要求的：所有的字段都是基本数据字段，不可进一步拆分。
在我们的这张表里，“property”这一字段可以继续拆分。其他字段已经都是基本数据字段，不能再拆了。
经过优化，我们把“property”这一字段，拆分成“specification（规格）”和“unit（单位）”，这2个字段如下：
这样处理之后，字段多了一个，但是每一个字段都成了不可拆分的最小信息单元，我们就可以在这个表的基础之上，着手进行进一步的优化了。这就要用到数据表设计的第二范式了。
第二范式通过观察，我们可以发现，这个表的前2条记录的前4个字段完全一样。那可不可以通过拆分，把它们变成一条记录呢？当然是可以的，而且为了优化，必须要进行拆分。
具体怎么拆分呢？第二范式就告诉了我们拆分的原则。
第二范式要求，在满足第一范式的基础上，还要满足数据表里的每一条数据记录，都是可唯一标识的。而且所有字段，都必须完全依赖主键，不能只依赖主键的一部分。
根据这个要求，我们可以对表进行重新设计。
重新设计的第一步，就是要确定这个表的主键。通过观察发现，字段“listnumber”+“barcode”可以唯一标识每一条记录，可以作为主键。确定好了主键以后，我们判断一下，哪些字段完全依赖主键，哪些字段只依赖于主键的一部分。同时，把只依赖于主键一部分的字段拆分出去，形成新的数据表。
首先，进货单明细表里面的“goodsname”“specification”“unit”这些信息是商品的属性，只依赖于“barcode”，不完全依赖主键，可以拆分出去。我们把这3个字段加上它们所依赖的字段“barcode”，拆分形成一个新的数据表“商品信息表”。
这样一来，原来的数据表就被拆分成了两个表。
商品信息表：
进货单表：
同样道理，字段“supplierid”“suppliername”“stock”只依赖于“listnumber”，不完全依赖于主键，所以，我们可以把“supplierid”“suppliername”“stock”这3个字段拆出去，再加上它们依赖的字段“listnumber”，就形成了一个新的表“进货单头表”。剩下的字段，会组成新的表，我们叫它“进货单明细表”。
这样一来，原来的数据表就拆分成了3个表。
进货单头表：
进货单明细表：
商品信息表：
到这里，我们就按照第二范式的要求，把原先的一个数据表拆分成了3个数据表。
现在，我们再来分析一下拆分后的3个表，保证这3个表都满足第二范式的要求。
在“商品信息表”中，字段“barcode”是有可能存在重复的，比如，用户门店可能有散装称重商品和自产商品，会存在条码共用的情况。所以，所有的字段都不能唯一标识表里的记录。这个时候，我们必须给这个表加上一个主键，比如说是自增字段“itemnumber”。
现在，我们就可以把进货单明细表里面的字段“barcode”都替换成字段“itemnumber”，这就得到了新的进货单明细表和商品信息表。
进货单明细表：
商品信息表：
这样一来，我们拆分后的3个数据表中的数据都不存在重复，可以唯一标识。而且，表中的其他字段，都完全依赖于表的主键，不存在部分依赖的情况。所以，拆分后的3个数据表就全部满足了第二范式的要求。
第三范式如果你仔细看的话，会发现，我们的进货单头表，还有数据冗余的可能。因为“suppliername”依赖“supplierid”。那么，这个时候，就可以按照第三范式的原则进行拆分了。
第三范式要求数据表在满足第二范式的基础上，不能包含那些可以由非主键字段派生出来的字段，或者说，不能存在依赖于非主键字段的字段。
在刚刚的进货单头表中，字段“suppliername”依赖于非主键字段“supplierid”。因此，这个表不满足第三范式的要求。
那接下来，我们就进一步拆分下进货单头表，把它拆解成供货商表和进货单头表。
供货商表：
进货单头表：
这样一来，供货商表和进货单头表中的所有字段，都完全依赖于主键，不存在任何一个字段依赖于非主键字段的情况了。所以，这2个表就都满足第三范式的要求了。
但是，在进货单明细表中，quantity * importprice = importvalue，“importprice”“quantity”和“importvalue”这3个字段，可以通过任意两个计算出第三个来，这就存在冗余字段。如果严格按照第三范式的要求，现在我们应该进行进一步优化。优化的办法是删除其中一个字段，只保留另外2个，这样就没有冗余数据了。
可是，真的可以这样做吗？要回答这个问题，我们就要先了解下实际工作中的业务优先原则。
业务优先的原则所谓的业务优先原则，就是指一切以业务需求为主，技术服务于业务。完全按照理论的设计不一定就是最优，还要根据实际情况来决定。这里我们就来分析一下不同选择的利与弊。
对于quantity * importprice = importvalue，看起来“importvalue”似乎是冗余字段，但并不会导致数据不一致。可是，如果我们把这个字段取消，是会影响业务的。
因为有的时候，供货商会经常进行一些促销活动，按金额促销，那他们拿来的进货单只有金额，没有价格。而“importprice”反而是通过“importvalue”÷“quantity”计算出来的。因此，如果不保留“importvalue”字段，只有“importprice”和“quantity”的话，经过四舍五入，会产生较大的误差。这样日积月累，最终会导致查询结果出现较大偏差，影响系统的可靠性。
我借助一个例子来说明下为什么会有偏差。
假设进货金额是25.5元，数量是34，那么进货价格就等于25.5÷34=0.74元，但是如果用这个计算出来的进货价格来计算进货金额，那么，进货金额就等于0.74×34=25.16元，其中相差了25.5-25.16=0.34元。代码如下所示：
“importvalue”=25.5元，“quantity”=34，“importprice”=25.5÷34=0.74 “importprice”=0.74元，“quantity”=34，“importvalue”=0.74*34=25.16 误差 = 25.5 - 25.16 = 0.34 现在你知道了，在我们这个场景下，“importvalue”是必须要保留的。
那么，换一种思路，如果我们保留“quantity”和“importvalue”，取消“importprice”，这样不是既能节省存储空间，又不会影响精确度吗？</description></item><item><title>22｜增强编译器前端功能第1步：再识数据流分析技术</title><link>https://artisanbox.github.io/3/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/24/</guid><description>你好，我是宫文学。
到目前为止，实现一门计算机语言的流程，我已经带你完整地走了一遍：从编译器前端的技术，到AST解释器，再到字节码虚拟机，最后生成了汇编代码并编译成了可执行文件。在这个过程中，我们领略了沿途的风光，初步了解了实现一门计算机语言的各种关键技术。
可是，我们在第一部分起步篇里，都是顾着奋力攀爬，去开出一条路来。可是这条路实在有点窄，是条羊肠小道，现在我们就需要把它拓宽一下。也就是把我们PlayScript语言的特性增强一下，拓宽我们的知识面。
这个拓宽的方式呢，我选择的是围绕数据类型这条主线进行。这是因为，现代计算机语言的很多特性，都是借助类型体系来呈现的。
不知道你注意到没有，到目前为止，除了最早的AST解释器以外，我们的后几个运行机制都只支持整型。所以，我们要在第二部分进阶篇中让PlayScript支持浮点型、字符串和自定义对象等数据类型。做这些工作的目的，不仅仅是增加我们语言支持的数据类型，而且，随着你对字符串类型、和自定义对象等类型的学习，你也会对对象的处理能力，包括对象的属性、方法、对象内存的自动管理等知识有更深刻的理解。
为了降低工作量，我后面的课程主要实现的是实现静态编译的版本。因为这种运行机制涉及的知识点比较广，并且我的目标之一就是要实现一个高效的、静态编译的TypeScript版本，到这里我的目标也算达到了。如果你有兴趣，你也可以把字节码虚拟机版本扩展一下，用于支持对象特性等高级特性。
不过，在我们开启第二段征程之时，我们需要回到编译器的前端部分，把词法分析、语法分析和语义分析等功能都增强一下，以便支持后面的语法特性。在这个过程中，你会学习到前端的数据流分析技术、前端的优化技术和类型计算方面的知识点，让你的编译器前端的知识得到迭代提升。
那么，首先我们就来看看，我们语言在编译器前端功能方面的现状，找找增强这些功能的办法。
编译器前端功能上的现状编译器的前端，也就是词法分析、语法分析和语义分析功能。我们逐个回顾一下，看看我们现在做到怎么样了。
首先来看我们的词法分析功能，跟语法分析和语义分析功能相比，它应该算是最完善的了。为什么这么说呢？因为我们目前的语言特性已经涉及到了大部分种类的Token，这些词法分析器都能提供。但相对来说，我们支持的语法规则还比较有限，所需要的语义分析功能也有很多缺失。不过，词法分析仍然有一些功能不够充分，比如数字字面量和字符串字面量等。
在数字字面量这边，我们目前虽然已经支持比较简单的整数和浮点数的字面量。但还有二进制、八进制和十六进制的数字、科学计数法表达的数字，我们都还没去支持。而且，字符串字面量上，我们也不支持Unicode和转义字符，并且字符串只能用双引号，还没使用单引号的版本。
接着，我们再来看语法分析功能。我们目前使用的都是一些比较高频的语法规则，忽略了一些比较低频的语法。比如，目前函数的参数只支持固定的参数数量，不支持变动数量的参数，也不支持参数的缺省值。再比如，我们目前的循环语句只支持for循环，并且不支持对集合的枚举，等等。
所以说，我们的词法分析和语法分析都还有不少功课需要去补呢。不过，我目前并不着急补这两方面。我的计划是，随着课程的推进，每当我们需要增加新特性的时候，就扩展一下这方面所需的词法和语法分析功能就好了。这样能够让你见证到像计算机语言这样的高难度软件一步步迭代成熟的过程，增强你自己驾驭类似的软件的信心。
既然词法和语法分析功能都不是我们这节课的重点，那语义分析功能自然就是重点了。
这是因为，实际上，在我们实现编译器的前端功能的时候，语义分析的工作量是最大的，但我们目前实现的功能确实有限。如果你有兴趣，可以参考我在《编译原理实战课》中对Java前端编译器的分析。在把编译工作分成的多个阶段中，大部分阶段都是去做语义分析相关的工作。
那我们现在的语义分析功能做到哪一步了呢？
在前面的课程中，我们已经实现了一些必要的语义分析功能，比如建立符号表、进行引用消解、分析哪个表达式是左值，以及进行简单的类型检查等等。不过这些功能其实还远远不够，因为还有很多潜在的语义错误没有被检查出来，因此需要我们逐步把这些工作补上。
在这个过程中，你会学习如何把数据流分析技术、类型计算技术用于语义分析工作。今天这节课，我们就先主要聚焦在数据流分析技术上。接下来，我们就举几个典型的场景，来学习如何在语义分析中使用数据流分析技术。
场景一：代码活跃性分析之程序是否return了？我们在写函数的时候，如果这个函数需要返回值，那么在编译时，编译器会检查一下，是不是你所有的程序分支都以return语句结尾了。如果没有，编译器就会报错。我们举个例子：
function foo(a:number):number{ if (a &amp;gt; 10){ let b:number = a+5; return b; b = a + 10; //这段代码不可到达。 } } 你可以看一下，这段代码有什么问题呢？
首先，你会发现，这段代码里只有在if语句块有return语句。所以，当不满足if条件的时候，程序的执行流程就不会遇到这个return语句。那根据TypeScript的语义，此时的返回值是undefined。而函数的返回值类型里呢，又不包含undefined。所以这时，如果你用“tsc --strict example_return.ts”命令去编译它，tsc会报下面的错误：
当然，如果函数的前面是下面的样子，在返回值里包含undefined，那就是正确的。
function foo(a:number):number|undefined 好，这是我们从示例代码中发现的第一个问题。那么第二个问题是什么呢？
你会看到，在return语句的下面还有一行代码“b = a + 10”，这一行代码其实是永远也不会被执行的。当然，这并不是一个错误，用tsc来编译也不会报错。但是，编译器或IDE工具最好要能够检查出这些问题，给程序员以提示。在编译生成代码的时候，编译器也可以直接把这些代码优化掉。
那如何检查出上面这些语义问题呢？那又需要用到数据流分析技术了。
到目前为止，我们已经多次接触到数据流分析技术了。在进行变量引用分析的时候，我们就曾实现过一个功能，检查出“变量是否在声明前就被引用”的错误。
它的处理逻辑是：语义分析程序遍历整个AST，相当于自上而下地分析每一条代码。当程序遇到变量声明节点的时候，就会标记该变量已经被声明了。而当程序遇到变量引用节点时，如果它发现该变量虽然属于某个作用域，但它当前还没有被声明，那么它就会报语义分析错误。具体的实现，你可以参考RefResolver类中的代码。
另外，在实现寄存器分配算法时，我们也曾经使用过数据流分析技术，来计算每个变量的生存期，从而确定多个变量如何共享寄存器。在那个时候，我们是在CFG上进行数据流分析的，并且分析方向是自下而上的顺序。
针对我们前面实操过的这两个例子，你可以总结出来数据流分析的几个特点：
首先，数据流分析技术可以用在像AST和CFG等多种数据结构上，未来你还会见到我们把它用到其他的数据结构上；
第二，针对不同的分析任务，数据流分析方向是不同的，有的是自上而下，有的是自下而上，你需要确定清楚；
第三，数据流分析的过程，都会针对一个分析变量，并会不断改变这个变量的值。分析变量可能是一个单个的值，或者叫做标量，也可能是一组数值，比如向量和集合。在我们的前面的两个例子中，这个变量都是集合。第一个例子的分析变量是“已声明的变量的集合”，第二个例子的分析变量是“活跃变量集合”。
第四，我们需要有一个规则或函数，基于这个规则来处理每行代码，从而计算新的变量值。比如，在变量活跃性分析中，这个规则是只要遇到变量使用的语句，就往集合里添加该变量，遇到变量声明的语句，就从集合中去掉该变量。
第五，要确定变量的初始值。在第一个例子中，初始值是一个空集。在第二个例子中，每个基本块可能会有一个活跃变量的初始值，这些初始值是由CFG中的其他基本块决定的。
还有最后一个共性，它们都有交汇函数。交汇函数是用来在多个控制流交汇的时候，计算出交汇的值。在第二个例子中，当两个基本块交汇的时候，活跃变量集合是取两个集合的并集。
好了，上面这些就是数据流分析技术的核心特点。抓住这些核心特点，我们可以把这个技术用于更多的场景。比如说，我们就可以用这些特性解决上面这个程序是否正确return的问题。
在开始解决这个问题之前，我们先来梳理一个分析框架，看看我们具体要从哪些方面着手。
我们可以把一个程序在执行过程中是否遇到了return语句，用一个变量来描述，就是当前执行流程是不是alive的。我们从程序的开头，一行行代码的往下分析。在一开始，alive的初始值是true。当遇到return语句以后，alive就变成了false。
对于分支语句，比如if分支语句，则需要每个分支都要遇到一个return语句。如果一个分支的alive值是alive1，另一个分支的alive值是alive2，那么合起来的alive值是什么呢？是alive1 || alive2。也就是说，必须每个分支都遇到return语句后，总的alive才是false。这就是我们的交汇函数。</description></item><item><title>23_ER模型：如何理清数据库设计思路？</title><link>https://artisanbox.github.io/8/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/23/</guid><description>你好，我是朱晓峰。
在超市项目的数据库设计阶段，超市经营者交给我们一大堆Excel表格。我们需要把这些表格的数据都整理清楚，并且按照一定的规则存储起来，从而进行高效的管理。
比如，当时我们有这样一张进货表：
为了提高数据存储的效率，我们按照第三范式的原则进行拆分，这样就得到了4个表，分别是供货商表、进货单头表、进货单明细表和商品信息表。
供货商表：
进货单头表：
进货单明细表：
商品信息表：
其中，商品信息表、供货商表和进货单头表都满足第三范式的原则，进货单明细表虽然不满足第三范式的原则，但是满足第二范式的要求，而且保留的冗余字段也是基于业务优先的原则保留的。因此，超市经营者给我们提供的进货单表，经过我们的拆解，已经是存取效率最佳的方案了。在进货管理这个局部模块中，是最优的数据库设计方案。
但是，当我们按照这样的方式拆分一连串数据表时，却发现越拆越多，而且支离破碎。事实上，局部最优的表，不仅有可能存在进一步拆分的情况，还有可能会出现数据缺失。
毕竟，数据库设计是牵一发而动全身的。那有没有什么办法提前看到数据库的全貌呢？比如需要哪些数据表、数据表中应该有哪些字段，数据表与数据表之间有什么关系、通过什么字段进行连接，等等。这样我们才能进行整体的梳理和设计。
其实，ER模型就是一个这样的工具。ER模型也叫作实体关系模型，是用来描述现实生活中客观存在的事物、事物的属性，以及事物之间关系的一种数据模型。在开发基于数据库的信息系统的设计阶段，通常使用ER模型来描述信息需求和信息特性，帮助我们理清业务逻辑，从而设计出优秀的数据库。
今天，我还是借助实际案例，带你使用ER模型分析一下超市的业务流程，具体给你讲一讲怎么通过ER模型来理清数据库设计的思路，从而设计出优秀的数据库。
在使用之前，咱们得先知道ER模型里都包括啥。
ER模型包括哪些要素？在ER模型里面，有三个要素，分别是实体、属性和关系。
实体，可以看做是数据对象，往往对应于现实生活中的真实存在的个体。比如，这个连锁超市就可以看做一个实体。在ER模型中，用矩形来表示。实体分为两类，分别是强实体和弱实体。强实体是指不依赖于其他实体的实体；弱实体是指对另一个实体有很强的依赖关系的实体。 属性，则是指实体的特性。比如超市的地址、联系电话、员工数等。在ER模型中用椭圆形来表示。 关系，则是指实体之间的联系。比如超市把商品卖给顾客，就是一种超市与顾客之间的联系。在ER模型中用菱形来表示。 需要注意的是，有的时候，实体和属性不容易区分。比如刚刚商品信息表中的商品的单位，到底是实体还是属性呢？如果从进货的角度出发，单位是商品的属性，但是从超市信息系统的整体出发，单位可以看做一个实体。
那么，该如何区分实体和属性呢？
我给你提供一个原则：我们要从系统整体的角度出发去看，可以独立存在的是实体，不可再分的是属性。也就是说，属性不需要进一步描述，不能包含其他属性。
在ER模型的3个要素中，关系又可以分为3种类型，分别是1对1、1对多和多对多。
1对1：指实体之间的关系是一一对应的，比如个人与身份证信息之间的关系就是1对1的关系。一个人只能有一个身份证信息，一个身份证信息也只属于一个人。 1对多：指一边的实体通过关系，可以对应多个另外一边的实体。相反，另外一边的实体通过这个关系，则只能对应唯一的一边的实体。比如超市与超市里的收款机之间的从属关系，超市可以拥有多台收款机，但是每一条收款机只能从属于一个超市。 多对多：指关系两边的实体都可以通过关系对应多个对方的实体。比如在进货模块中，供货商与超市之间的关系就是多对多的关系，一个供货商可以给多个超市供货，一个超市也可以从多个供货商那里采购商品。 知道了这些要素，我们就可以给超市业务创建ER模型了，如下图所示：
我来简单解释一下这个图。
在这个图中，供货商和超市之间的供货关系，两边的数字都不是1，表示多对多的关系。同样，超市和顾客之间的零售关系，也是多对多的关系。
这个ER模型，包括了3个实体之间的2种关系：
超市从供货商那里采购商品； 超市把商品卖给顾客。 有了这个ER模型，我们就可以从整体上理解超市的业务了。但是，这里没有包含属性，这样就无法体现实体和关系的具体特征。现在，我们需要把属性加上，用椭圆来表示，这样我们得到的ER模型就更加完整了。
ER模型的细化刚刚的ER模型展示了超市业务的框架，但是只包括了供货商、超市和顾客这三个实体，以及它们之间的关系，还不能对应到具体的表，以及表与表之间的关联。
因此，我们需要进一步去设计一下这个ER模型的各个局部，也就是细化下超市的具体业务流程，然后把它们综合到一起，形成一个完整的ER模型。这样可以帮助我们理清数据库的设计思路。
我们刚才的超市业务模型，包括了两个模块，分别是进货模块和销售模块。下面我们分别对这2个模块进行细化。
首先，我们来看一下超市业务中的进货模块的ER模型，整理一下其中包含哪些实体、哪些关系和哪些属性。
在我们的进货模块里，有5个实体：
供货商 商品 门店 仓库 员工 其中，供货商、商品和门店是强实体，因为它们不需要依赖其他任何实体。而仓库和员工是弱实体，因为它们虽然都可以独立存在，但是它们都依赖门店这个实体，因此都是弱实体。
接下来，我们再分析一下各个实体都有哪些属性。
供货商：名称、地址、电话、联系人。 商品：条码、名称、规格、单位、价格。 门店：编号、地址、电话、联系人。 仓库：编号、名称。 员工：工号、姓名、住址、电话、身份证号、职位。 这样细分之后，我们就可以重新设计进货模块了，ER模型如下：
需要注意的是，这里我是用粗框矩形表示弱实体，用粗框菱形，表示弱实体与它依赖的强实体之间的关系。
第二步，我们再分析一下零售模块。
经过分析，我们发现，在超市的业务流程中，零售业务包括普通零售和会员零售两种模式。普通零售包含的实体，包括门店、商品和收银款台；会员零售包含的实体，包括门店、商品、会员和收银款台。
这样我们就可以提炼出零售业务模块中的实体：
商品 门店 会员 收银款台 其中，商品和门店不依赖于任何其他实体，所以是强实体；会员和收银款台都依赖于门店，所以是弱实体。
有了实体之后，我们就可以确定实体的属性了。
商品：条码、名称、规格、单位、价格。 会员：卡号、发卡门店、名称、电话、身份证、地址、积分、储值。 门店：编号、地址、电话、联系人。 收银款台：编号、名称。 现在，我们就可以重新设计零售模块的ER模型了：</description></item><item><title>23_Julia编译器（二）：如何利用LLVM的优化和后端功能？</title><link>https://artisanbox.github.io/7/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/23/</guid><description>你好，我是宫文学。
上一讲，我给你概要地介绍了一下Julia这门语言，带你一起解析了它的编译器的编译过程。另外我也讲到，Julia创造性地使用了LLVM，再加上它高效的分派机制，这就让一门脚本语言的运行速度，可以跟C、Java这种语言媲美。更重要的是，你用Julia本身，就可以编写需要高性能的数学函数包，而不用像Python那样，需要用另外的语言来编写（如C语言）高性能的代码。
那么今天这一讲，我就带你来了解一下Julia运用LLVM的一些细节。包括以下几个核心要点：
如何生成LLVM IR？ 如何基于LLVM IR做优化？ 如何利用内建（Intrinsics）函数实现性能优化和语义个性化？ 这样，在深入解读了这些问题和知识点以后，你对如何正确地利用LLVM，就能建立一个直观的认识了，从而为自己使用LLVM打下很好的基础。
好，首先，我们来了解一下Julia做即时编译的过程。
即时编译的过程我们用LLDB来跟踪一下生成IR的过程。
$ lldb #启动lldb (lldb)attach --name julia #附加到julia进程 c #让julia进程继续运行 首先，在Julia的REPL中，输入一个简单的add函数的定义：
julia&amp;gt; function add(a, b) x = a+b x end 接着，在LLDB或GDB中设置一个断点“br emit_funciton”，这个断点是在codegen.cpp中。
(lldb) br emit_function #添加断点 然后在Julia里执行函数add：
julia&amp;gt; add(2,3) 这会触发Julia的编译过程，并且程序会停在断点上。我整理了一下调用栈的信息，你可以看看，即时编译是如何被触发的。
通过跟踪执行和阅读源代码，你会发现Julia中最重要的几个源代码：
gf.c：Julia以方法分派快速而著称。对于类似加法的这种运算，它会有上百个方法的实现，所以在运行时，就必须能迅速定位到准确的方法。分派就是在gf.c里。 interpreter.c：它是Julia的解释器。虽然Julia中的函数都是即时编译的，但在REPL中的简单的交互，靠解释执行就可以了。 codegen.cpp：生成LLVM IR的主要逻辑都在这里。 我希望你能自己动手跟踪执行一下，这样你就会彻底明白Julia的运行机制。
Julia的IR：采用SSA形式在上一讲中，你已经通过@code_lowered和@code_typed宏，查看过了Julia的IR。
Julia的IR也经历了一个发展演化过程，它的IR最早不是SSA的，而是后来才改成了SSA形式。这一方面是因为，SSA真的是有优势，它能简化优化算法的编写；另一方面也能看出，SSA确实是趋势呀，我们目前接触到的Graal、V8和LLVM的IR，都是SSA格式的。
Julia的IR主要承担了两方面的任务。
第一是类型推断，推断出来的类型被保存到IR中，以便于生成正确版本的代码。
第二是基于这个IR做一些优化，其实主要就是实现了内联优化。内联优化是可以发生在比较早的阶段，你在Go的编译器中就会看到类似的现象。
你可以在Julia中写两个短的函数，让其中一个来调用另一个，看看它所生成的LLVM代码和汇编代码是否会被自动内联。
另外，你还可以查看一下传给emit_function函数的Julia IR是什么样子的。在LLDB里，你可以用下面的命令来显示src参数的值（其中，jl_(obj)是Julia为了调试方便提供的一个函数，它能够更好地显示Julia对象的信息，注意显示是在julia窗口中）。src参数里面包含了要编译的Julia代码的信息。
(lldb) expr jl_(src) 为了让你能更容易看懂，我稍微整理了一下输出的信息的格式：
你会发现，这跟用@code_typed(add(2,3))命令打印出来的信息是一致的，只不过宏里显示的信息会更加简洁：
接下来，查看emit_function函数，你就能够看到生成LLVM IR的整个过程。
生成LLVM IRLLVM的IR有几个特点：
第一，它是SSA格式的。 第二，LLVM IR有一个类型系统。类型系统能帮助生成正确的机器码，因为不同的字长对应的机器码指令是不同的。 第三，LLVM的IR不像其他IR，一般只有内存格式，它还有文本格式和二进制格式。你完全可以用文本格式写一个程序，然后让LLVM读取，进行编译和执行。所以，LLVM的IR也可以叫做LLVM汇编。 第四，LLVM的指令有丰富的元数据，这些元数据能够被用于分析和优化工作中。 基本上，生成IR的程序没那么复杂，就是用简单的语法制导的翻译即可，从AST或别的IR生成LLVM的IR，属于那种比较幼稚的翻译方法。</description></item><item><title>23_MySQL是怎么保证数据不丢的？</title><link>https://artisanbox.github.io/1/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/23/</guid><description>今天这篇文章，我会继续和你介绍在业务高峰期临时提升性能的方法。从文章标题“MySQL是怎么保证数据不丢的？”，你就可以看出来，今天我和你介绍的方法，跟数据的可靠性有关。
在专栏前面文章和答疑篇中，我都着重介绍了WAL机制（你可以再回顾下第2篇、第9篇、第12篇和第15篇文章中的相关内容），得到的结论是：只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启后，数据可以恢复。
评论区有同学又继续追问，redo log的写入流程是怎么样的，如何保证redo log真实地写入了磁盘。那么今天，我们就再一起看看MySQL写入binlog和redo log的流程。
binlog的写入机制其实，binlog的写入逻辑比较简单：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。
一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了binlog cache的保存问题。
系统给binlog cache分配了一片内存，每个线程一个，参数 binlog_cache_size用于控制单个线程内binlog cache所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。
事务提交的时候，执行器把binlog cache里的完整事务写入到binlog中，并清空binlog cache。状态如图1所示。
图1 binlog写盘状态可以看到，每个线程有自己binlog cache，但是共用同一份binlog文件。
图中的write，指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快。 图中的fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为fsync才占磁盘的IOPS。 write 和fsync的时机，是由参数sync_binlog控制的：
sync_binlog=0的时候，表示每次提交事务都只write，不fsync；
sync_binlog=1的时候，表示每次提交事务都会执行fsync；
sync_binlog=N(N&amp;gt;1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。
因此，在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成0，比较常见的是将其设置为100~1000中的某个数值。
但是，将sync_binlog设置为N，对应的风险是：如果主机发生异常重启，会丢失最近N个事务的binlog日志。
redo log的写入机制接下来，我们再说说redo log的写入机制。
在专栏的第15篇答疑文章中，我给你介绍了redo log buffer。事务在执行过程中，生成的redo log是要先写到redo log buffer的。
然后就有同学问了，redo log buffer里面的内容，是不是每次生成后都要直接持久化到磁盘呢？
答案是，不需要。
如果事务执行期间MySQL发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。
那么，另外一个问题是，事务还没提交的时候，redo log buffer中的部分日志有没有可能被持久化到磁盘呢？
答案是，确实会有。
这个问题，要从redo log可能存在的三种状态说起。这三种状态，对应的就是图2 中的三个颜色块。
图2 MySQL redo log存储状态这三种状态分别是：
存在redo log buffer中，物理上是在MySQL进程内存中，就是图中的红色部分；</description></item><item><title>23_二叉树基础（上）：什么样的二叉树适合用数组来存储？</title><link>https://artisanbox.github.io/2/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/24/</guid><description>前面我们讲的都是线性表结构，栈、队列等等。今天我们讲一种非线性表结构，树。树这种数据结构比线性表的数据结构要复杂得多，内容也比较多，所以我会分四节来讲解。
我反复强调过，带着问题学习，是最有效的学习方式之一，所以在正式的内容开始之前，我还是给你出一道思考题：二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？
带着这些问题，我们就来学习今天的内容，树！
树（Tree）我们首先来看，什么是“树”？再完备的定义，都没有图直观。所以我在图中画了几棵“树”。你来看看，这些“树”都有什么特征？
你有没有发现，“树”这种数据结构真的很像我们现实生活中的“树”，这里面每个元素我们叫做“节点”；用来连接相邻节点之间的关系，我们叫做“父子关系”。
比如下面这幅图，A节点就是B节点的父节点，B节点是A节点的子节点。B、C、D这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫做根节点，也就是图中的节点E。我们把没有子节点的节点叫做叶子节点或者叶节点，比如图中的G、H、I、J、K、L都是叶子节点。
除此之外，关于“树”，还有三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）。它们的定义是这样的：
这三个概念的定义比较容易混淆，描述起来也比较空洞。我举个例子说明一下，你一看应该就能明白。
记这几个概念，我还有一个小窍门，就是类比“高度”“深度”“层”这几个名词在生活中的含义。
在我们的生活中，“高度”这个概念，其实就是从下往上度量，比如我们要度量第10层楼的高度、第13层楼的高度，起点都是地面。所以，树这种数据结构的高度也是一样，从最底层开始计数，并且计数的起点是0。
“深度”这个概念在生活中是从上往下度量的，比如水中鱼的深度，是从水平面开始度量的。所以，树这种数据结构的深度也是类似的，从根结点开始度量，并且计数起点也是0。
“层数”跟深度的计算类似，不过，计数起点是1，也就是说根节点位于第1层。
二叉树（Binary Tree）树结构多种多样，不过我们最常用还是二叉树。
二叉树，顾名思义，每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。我画的这几个都是二叉树。以此类推，你可以想象一下四叉树、八叉树长什么样子。
这个图里面，有两个比较特殊的二叉树，分别是编号2和编号3这两个。
其中，编号2的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做满二叉树。
编号3的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做完全二叉树。
满二叉树很好理解，也很好识别，但是完全二叉树，有的人可能就分不清了。我画了几个完全二叉树和非完全二叉树的例子，你可以对比着看看。
你可能会说，满二叉树的特征非常明显，我们把它单独拎出来讲，这个可以理解。但是完全二叉树的特征不怎么明显啊，单从长相上来看，完全二叉树并没有特别特殊的地方啊，更像是“芸芸众树”中的一种。
那我们为什么还要特意把它拎出来讲呢？为什么偏偏把最后一层的叶子节点靠左排列的叫完全二叉树？如果靠右排列就不能叫完全二叉树了吗？这个定义的由来或者说目的在哪里？
要理解完全二叉树定义的由来，我们需要先了解，如何表示（或者存储）一棵二叉树？
想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。
我们先来看比较简单、直观的链式存储法。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。
我们再来看，基于数组的顺序存储法。我们把根节点存储在下标i = 1的位置，那左子节点存储在下标2 * i = 2的位置，右子节点存储在2 * i + 1 = 3的位置。以此类推，B节点的左子节点存储在2 * i = 2 * 2 = 4的位置，右子节点存储在2 * i + 1 = 2 * 2 + 1 = 5的位置。
我来总结一下，如果节点X存储在数组中下标为i的位置，下标为2 * i 的位置存储的就是左子节点，下标为2 * i + 1的位置存储的就是右子节点。反过来，下标为i/2的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为1的位置），这样就可以通过下标计算，把整棵树都串起来。
不过，我刚刚举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为0的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。你可以看我举的下面这个例子。
所以，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。
当我们讲到堆和堆排序的时候，你会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。
二叉树的遍历前面我讲了二叉树的基本定义和存储方法，现在我们来看二叉树中非常重要的操作，二叉树的遍历。这也是非常常见的面试题。</description></item><item><title>23_冒险和预测（二）：流水线里的接力赛</title><link>https://artisanbox.github.io/4/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/23/</guid><description>上一讲，我为你讲解了结构冒险和数据冒险，以及应对这两种冒险的两个解决方案。一种方案是增加资源，通过添加指令缓存和数据缓存，让我们对于指令和数据的访问可以同时进行。这个办法帮助CPU解决了取指令和访问数据之间的资源冲突。另一种方案是直接进行等待。通过插入NOP这样的无效指令，等待之前的指令完成。这样我们就能解决不同指令之间的数据依赖问题。
着急的人，看完上一讲的这两种方案，可能已经要跳起来问了：“这也能算解决方案么？”的确，这两种方案都有点儿笨。
第一种解决方案，好比是在软件开发的过程中，发现效率不够，于是研发负责人说：“我们需要双倍的人手和研发资源。”而第二种解决方案，好比你在提需求的时候，研发负责人告诉你说：“来不及做，你只能等我们需求排期。” 你应该很清楚地知道，“堆资源”和“等排期”这样的解决方案，并不会真的提高我们的效率，只是避免冲突的无奈之举。
那针对流水线冒险的问题，我们有没有更高级或者更高效的解决方案呢？既不用简单花钱加硬件电路这样“堆资源”，也不是纯粹等待之前的任务完成这样“等排期”。
答案当然是有的。这一讲，我们就来看看计算机组成原理中，一个更加精巧的解决方案，操作数前推。
NOP操作和指令对齐要想理解操作数前推技术，我们先来回顾一下，第5讲讲过的，MIPS体系结构下的R、I、J三类指令，以及第20讲里的五级流水线“取指令（IF）-指令译码（ID）-指令执行（EX）-内存访问（MEM）-数据写回（WB） ”。
我把对应的图片放进来了，你可以看一下。如果印象不深，建议你先回到这两节去复习一下，再来看今天的内容。
在MIPS的体系结构下，不同类型的指令，会在流水线的不同阶段进行不同的操作。
我们以MIPS的LOAD，这样从内存里读取数据到寄存器的指令为例，来仔细看看，它需要经历的5个完整的流水线。STORE这样从寄存器往内存里写数据的指令，不需要有写回寄存器的操作，也就是没有数据写回的流水线阶段。至于像ADD和SUB这样的加减法指令，所有操作都在寄存器完成，所以没有实际的内存访问（MEM）操作。
有些指令没有对应的流水线阶段，但是我们并不能跳过对应的阶段直接执行下一阶段。不然，如果我们先后执行一条LOAD指令和一条ADD指令，就会发生LOAD指令的WB阶段和ADD指令的WB阶段，在同一个时钟周期发生。这样，相当于触发了一个结构冒险事件，产生了资源竞争。
所以，在实践当中，各个指令不需要的阶段，并不会直接跳过，而是会运行一次NOP操作。通过插入一个NOP操作，我们可以使后一条指令的每一个Stage，一定不和前一条指令的同Stage在一个时钟周期执行。这样，就不会发生先后两个指令，在同一时钟周期竞争相同的资源，产生结构冒险了。
流水线里的接力赛：操作数前推通过NOP操作进行对齐，我们在流水线里，就不会遇到资源竞争产生的结构冒险问题了。除了可以解决结构冒险之外，这个NOP操作，也是我们之前讲的流水线停顿插入的对应操作。
但是，插入过多的NOP操作，意味着我们的CPU总是在空转，干吃饭不干活。那么，我们有没有什么办法，尽量少插入一些NOP操作呢？不要着急，下面我们就以两条先后发生的ADD指令作为例子，看看能不能找到一些好的解决方案。
add $t0, $s2,$s1 add $s2, $s1,$t0 这两条指令很简单。
第一条指令，把 s1 和 s2 寄存器里面的数据相加，存入到 t0 这个寄存器里面。 第二条指令，把 s1 和 t0 寄存器里面的数据相加，存入到 s2 这个寄存器里面。 因为后一条的 add 指令，依赖寄存器 t0 里的值。而 t0 里面的值，又来自于前一条指令的计算结果。所以后一条指令，需要等待前一条指令的数据写回阶段完成之后，才能执行。就像上一讲里讲的那样，我们遇到了一个数据依赖类型的冒险。于是，我们就不得不通过流水线停顿来解决这个冒险问题。我们要在第二条指令的译码阶段之后，插入对应的NOP指令，直到前一天指令的数据写回完成之后，才能继续执行。
这样的方案，虽然解决了数据冒险的问题，但是也浪费了两个时钟周期。我们的第2条指令，其实就是多花了2个时钟周期，运行了两次空转的NOP操作。
不过，其实我们第二条指令的执行，未必要等待第一条指令写回完成，才能进行。如果我们第一条指令的执行结果，能够直接传输给第二条指令的执行阶段，作为输入，那我们的第二条指令，就不用再从寄存器里面，把数据再单独读出来一次，才来执行代码。
我们完全可以在第一条指令的执行阶段完成之后，直接将结果数据传输给到下一条指令的ALU。然后，下一条指令不需要再插入两个NOP阶段，就可以继续正常走到执行阶段。
这样的解决方案，我们就叫作操作数前推（Operand Forwarding），或者操作数旁路（Operand Bypassing）。其实我觉得，更合适的名字应该叫操作数转发。这里的Forward，其实就是我们写Email时的“转发”（Forward）的意思。不过现有的经典教材的中文翻译一般都叫“前推”，我们也就不去纠正这个说法了，你明白这个意思就好。
转发，其实是这个技术的逻辑含义，也就是在第1条指令的执行结果，直接“转发”给了第2条指令的ALU作为输入。另外一个名字，旁路（Bypassing），则是这个技术的硬件含义。为了能够实现这里的“转发”，我们在CPU的硬件里面，需要再单独拉一根信号传输的线路出来，使得ALU的计算结果，能够重新回到ALU的输入里来。这样的一条线路，就是我们的“旁路”。它越过（Bypass）了写入寄存器，再从寄存器读出的过程，也为我们节省了2个时钟周期。
操作数前推的解决方案不但可以单独使用，还可以和流水线冒泡一起使用。有的时候，虽然我们可以把操作数转发到下一条指令，但是下一条指令仍然需要停顿一个时钟周期。
比如说，我们先去执行一条LOAD指令，再去执行ADD指令。LOAD指令在访存阶段才能把数据读取出来，所以下一条指令的执行阶段，需要在访存阶段完成之后，才能进行。
总的来说，操作数前推的解决方案，比流水线停顿更进了一步。流水线停顿的方案，有点儿像游泳比赛的接力方式。下一名运动员，需要在前一个运动员游玩了全程之后，触碰到了游泳池壁才能出发。而操作数前推，就好像短跑接力赛。后一个运动员可以提前抢跑，而前一个运动员会多跑一段主动把交接棒传递给他。
总结延伸这一讲，我给你介绍了一个更加高级，也更加复杂的解决数据冒险问题方案，就是操作数前推，或者叫操作数旁路。
操作数前推，就是通过在硬件层面制造一条旁路，让一条指令的计算结果，可以直接传输给下一条指令，而不再需要“指令1写回寄存器，指令2再读取寄存器“这样多此一举的操作。这样直接传输带来的好处就是，后面的指令可以减少，甚至消除原本需要通过流水线停顿，才能解决的数据冒险问题。
这个前推的解决方案，不仅可以单独使用，还可以和前面讲解过的流水线冒泡结合在一起使用。因为有些时候，我们的操作数前推并不能减少所有“冒泡”，只能去掉其中的一部分。我们仍然需要通过插入一些“气泡”来解决冒险问题。
通过操作数前推，我们进一步提升了CPU的运行效率。那么，我们是不是还能找到别的办法，进一步地减少浪费呢？毕竟，看到现在，我们仍然少不了要插入很多NOP的“气泡”。那就请你继续坚持学习下去。下一讲，我们来看看，CPU是怎么通过乱序执行，进一步减少“气泡”的。
推荐阅读想要深入了解操作数前推相关的内容，推荐你读一下《计算机组成与设计：硬件/软件接口》的4.5～4.7章节。
课后思考前面讲5级流水线指令的时候，我们说，STORE指令是没有数据写回阶段的，而ADD指令是没有访存阶段的。那像CMP或者JMP这样的比较和跳转指令，5个阶段都是全的么？还是说不需要哪些阶段呢？
欢迎留言和我分享你的疑惑和见解。你也可以把今天的内容，分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>23_生成汇编代码（二）：把脚本编译成可执行文件</title><link>https://artisanbox.github.io/6/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/23/</guid><description>学完两节课之后，对于后端编译过程，你可能还会产生一些疑问，比如：
1.大致知道汇编程序怎么写，却不知道如何从AST生成汇编代码，中间有什么挑战。
2.编译成汇编代码之后需要做什么，才能生成可执行文件。
本节课，我会带你真正动手，基于AST把playscript翻译成正确的汇编代码，并将汇编代码编译成可执行程序。
通过这样一个过程，可以实现从编译器前端到后端的完整贯通，帮你对编译器后端工作建立比较清晰的认识。这样一来，你在日常工作中进行大型项目的编译管理的时候，或者需要重用别人的类库的时候，思路会更加清晰。
从playscript生成汇编代码先来看看如何从playscript生成汇编代码。
我会带你把playscript的几个有代表性的功能，而不是全部的功能翻译成汇编代码，一来工作量少一些，二来方便做代码优化。这几个有代表性的功能如下：
1.支持函数调用和传参（这个功能可以回顾加餐）。
2.支持整数的加法运算（在这个过程中要充分利用寄存器提高性能）。
3.支持变量声明和初始化。
具体来说，要能够把下面的示例程序正确生成汇编代码：
//asm.play int fun1(int x1, int x2, int x3, int x4, int x5, int x6, int x7, int x8){ int c = 10; return x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + c; } println(&amp;quot;fun1:&amp;quot; + fun1(1,2,3,4,5,6,7,8)); 在加餐中，我提供了一段手写的汇编代码，功能等价于这段playscript代码，并讲述了如何在多于6个参数的情况下传参，观察栈帧的变化过程，你可以看看下面的图片和代码，回忆一下：
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;# function-call2-craft.s 函数调用和参数传递 # 文本段,纯代码 .section __TEXT,__text,regular,pure_instructions
_fun1: # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来</description></item><item><title>23_瞧一瞧Linux：SLAB如何分配内存？</title><link>https://artisanbox.github.io/9/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/23/</guid><description>你好，我是LMOS。
上节课我们学习了伙伴系统，了解了它是怎样管理物理内存页面的。那么你自然会想到这个问题：Linux系统中，比页更小的内存对象要怎样分配呢？
带着这个问题，我们来一起看看SLAB分配器的原理和实现。在学习过程中，你也可以对照一下我们Cosmos的内存管理组件，看看两者的内存管理有哪些异同。
SLAB与Cosmos物理内存页面管理器一样，Linux中的伙伴系统是以页面为最小单位分配的，现实更多要以内核对象为单位分配内存，其实更具体一点说，就是根据内核对象的实例变量大小来申请和释放内存空间，这些数据结构实例变量的大小通常从几十字节到几百字节不等，远远小于一个页面的大小。
如果一个几十字节大小的数据结构实例变量，就要为此分配一个页面，这无疑是对宝贵物理内存的一种巨大浪费，因此一个更好的技术方案应运而生，就是Slab分配器（由Sun公司的雇员Jeff Bonwick在Solaris 2.4中设计并实现）。
由于作者公开了实现方法，后来被Linux所借鉴，用于实现内核中更小粒度的内存分配。看看吧，你以为Linux很强大，真的强大吗？不过是站在巨人的肩膀上飞翔的。
走进SLAB对象何为SLAB对象？在SLAB分配器中，它把一个内存页面或者一组连续的内存页面，划分成大小相同的块，其中这一个小的内存块就是SLAB对象，但是这一组连续的内存页面中不只是SLAB对象，还有SLAB管理头和着色区。
我画个图你就明白了，如下所示。
上图中有一个内存页面和两个内存页面的SLAB，你可能对着色区有点陌生，我来给你讲解一下。
这个着色区也是一块动态的内存块，建立SLAB时才会设置它的大小，目的是为了错开不同SLAB中的对象地址，降低硬件Cache行中的地址争用，以免导致Cache抖动效应，整个系统性能下降。
SLAB头其实是一个数据结构，但是它不一定放在保存对象内存页面的开始。通常会有一个保存SLAB管理头的SLAB，在Linux中，SLAB管理头用kmem_cache结构来表示，代码如下。
struct array_cache { unsigned int avail; unsigned int limit; void *entry[]; }; struct kmem_cache { //是每个CPU一个array_cache类型的变量，cpu_cache是用于管理空闲对象的 struct array_cache __percpu *cpu_cache; unsigned int size; //cache大小 slab_flags_t flags;//slab标志 unsigned int num;//对象个数 unsigned int gfporder;//分配内存页面的order gfp_t allocflags; size_t colour;//着色区大小 unsigned int colour_off;//着色区的开始偏移 const char *name;//本SLAB的名字 struct list_head list;//所有的SLAB都要链接起来 int refcount;//引用计数 int object_size;//对象大小 int align;//对齐大小 struct kmem_cache_node *node[MAX_NUMNODES];//指向管理kmemcache的上层结构 }; 上述代码中，有多少个CPU，就会有多少个array_cache类型的变量。这种为每个CPU构造一个变量副本的同步机制，就是每CPU变量（per-cpu-variable）。array_cache结构中"entry[]"表示了一个遵循LIFO顺序的数组，"avail"和"limit"分别指定了当前可用对象的数目和允许容纳对象的最大数目。</description></item><item><title>23｜增强编译器前端功能第2步：增强类型体系</title><link>https://artisanbox.github.io/3/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/25/</guid><description>你好，我是宫文学。
你可能也注意到了，我们在第二部分的主要任务，是要让PlayScript扩展到支持更多的类型。在这个任务中，对类型的处理能力就是一个很重要的功能。
其实在第一部分，我们已经实现了一定的类型处理功能，包括类型检查、类型自动推断等，但其实还有更多的类型处理能力需要支持。
对于一门语言来说，类型系统是它的核心。语言之间的差别很多时候都体现在类型系统的设计上，程序员们通常也会对类型处理的内部机制很感兴趣。而TypeScript比JavaScript语言增强的部分，恰恰就是一个强大而又灵活的类型系统，所以我们就更有必要讨论一下与类型有关的话题了。
那么通过今天这节课，我们就来增强一下PlayScript的类型处理能力，在这过程中，我们也能学习到更多与类型系统有关的知识点，特别是能对类型计算的数学实质有所认知。
首先，我们来看看TypeScript的类型系统有什么特点。
TypeScript的类型系统从TypeScript的名字上，你就可以看出来，这门语言在类型系统的设计上，一定是下了功夫的。也确实是这样，TypeScript在设计之初，就想弥补JavaScript弱类型、动态类型所带来的缺点。特别是，当程序规模变大的时候，弱类型、动态类型很容易不经意地引入一些错误，而且还比较难以发现。
所以TypeScript的设计者，希望通过提供一个强类型体系，让编译器能够检查出程序中潜在的错误，这也有助于IDE工具提供更友好的特性，比如准确提示类的属性和方法，从而帮助程序员编写更高质量的程序。
而TypeScript也确实实现了这个设计目标。它的类型系统功能很强大，表达能力很强，既有利于提高程序的正确性，同时又没有削弱程序员自由表达各种设计思想的能力。
那么我们现在就来看一看TypeScript的类型系统到底有什么特点。
首先，TypeScript继承了JavaScript的几个预定义的类型，比如number、string和boolean等。
在JavaScript中，我们不需要声明类型，比如下面两句代码就是。在程序运行的时候，系统会自动给age和name1分别关联一个number和string类型的值。
var age = 18; var name1 = "richard"; 而在TypeScript中呢，你需要用let关键字来声明变量。在下面的示例程序中，age和number被我们用let关键字分别赋予了number和string类型。
let age = 18; let name1 = "richard"; 这两行代码里的类型是被推导出来的，它们跟显式声明类型的方式是等价的。
let age:number = 18; let name1:string = "richard"; 第二，TypeScript禁止了变量类型的动态修改。
在JavaScript中，我们可以动态地修改变量的类型。比如在下面两行代码中，age一开头是number型的，后来被改成了string型，也是允许的：
var age = 18; age = "eighteen"; 但在TypeScript中，如果你一开头给age赋一个number的值，后面再赋一个string类型的值，编译器就会报错：
let age = 18; age = "eighteen"; //错误！ 这是因为，上面的第一行代码等价于显式声明age为number类型，因为TypeScript会根据变量初始化的部分，来推断出age的类型。而这个类型一旦确定，后面就不允许再修改了。
let age:number = 18; age = "eighteen"; 不过，如果完全不允许类型动态变化，可能会失去JavaScript灵活性这个优点，会让某些程序员觉得用起来不舒服。所以，TypeScript还留了一个口子，就是any类型。
第三，只有any类型允许动态修改变量的类型。
在TypeScript中，如果你声明变量的时候不指定任何类型，或者显式地指定变量类型为any，那变量的类型都是any，程序也就可以动态地修改变量的类型，我们可以看看下面这个例子：
let age; //等价于 let age:any; age = 18; console.</description></item><item><title>24_Go语言编译器：把它当作教科书吧</title><link>https://artisanbox.github.io/7/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/24/</guid><description>你好，我是宫文学。今天这一讲，我来带你研究一下Go语言自带的编译器，它可以被简称为gc。
我之所以要来带你研究Go语言的编译器，一方面是因为Go现在确实非常流行，很多云端服务都用Go开发，Docker项目更是巩固了Go语言的地位；另一方面，我希望你能把它当成编译原理的教学参考书来使用。这是因为：
Go语言的编译器完全用Go语言本身来实现，它完全实现了从前端到后端的所有工作，而不像Java要分成多个编译器来实现不同的功能模块，不像Python缺少了后端，也不像Julia用了太多的语言。所以你研究它所采用的编译技术会更方便。 Go编译器里基本上使用的都是经典的算法：经典的递归下降算法、经典的SSA格式的IR和CFG、经典的优化算法、经典的Lower和代码生成，因此你可以通过一个编译器就把这些算法都贯穿起来。 除了编译器，你还可以学习到一门语言的其他构成部分的实现思路，包括运行时（垃圾收集器、并发调度机制等）、标准库和工具链，甚至连链接器都是用Go语言自己实现的，从而对实现一门语言所需要做的工作有更完整的认识。 最后，Go语言的实现继承了从Unix系统以来形成的一些良好的设计哲学，因为Go语言的核心设计者都是为Unix的发展，做出过重要贡献的极客。因此了解了Go语言编译器的实现机制，会提高你的软件设计品味。 扩展：每种语言都有它的个性，而这个个性跟语言设计者的背景密切相关。Go语言的核心设计者，是Unix领域的极客，包括Unix的创始人和C语言的共同发明人之一，Ken Tompson。Rob Pike也是Unix的核心作者。
Go语言的作者们显然希望新的语言体现出他们的设计哲学和口味。比如，致力于像Unix那样的简洁和优雅，并且致力于让Go再次成为一款经典作品。
所以，在已经研究了多个高级语言的编译器之后，我们可以拿Go语言的编译器，把整个编译过程再重新梳理和印证一遍。
好了，现在就开始我们今天探索的旅途吧。
首先，我们来看看Go语言编译器的前端。
重要提示：照例，你要下载Go语言的源代码，本讲采用的是1.14.2版本。并且，你最好使用一个IDE，便于跟踪调试编译器的执行过程。
Go的源代码中附带的介绍编译器的文档，写得很好、很清晰，你可以参考一下。
词法分析和语法分析Go的编译器的词法分析和语法分析功能的实现，是在cmd/compile/internal/syntax目录下。
词法分析器是scanner.go。其实大部分编程语言的词法分析器的算法，都已经很标准了，我们在Java编译器里就曾经分析过。甚至它们处理标识符和关键字的方式也都一致，都是先作为标识符识别出来，然后再查表挑出关键字来。Go的词法分析器并没有像V8那样在不遗余力地压榨性能，它跟你平常编码的方式是很一致的，非常容易阅读。
语法分析器是parser.go。它是一个标准的手写的递归下降算法。在解析二元表达式的时候，Go的语法分析器也是采用了运算符优先级算法，这个已经是我们第N次见到这个算法了，所以你一定要掌握！不过，每个编译器的实现都不大一样，而Go的实现方式相当的简洁，你可以去自己看一下，或者用调试器来跟踪一下它的执行过程。
图1：用IDE工具Goland跟踪调试编译过程Go的AST的节点，是在nodes.go中定义的，它异常简洁，可以说简洁得让你惊讶。你可以欣赏一下。
Go的语法分析器还有一个很有特色的地方，就是对错误的处理。它在处理编译错误时，有一个原则，就是不要遇到一个错误就停止编译，而是要尽可能跳过当前这个出错的地方，继续往下编译，这样可以一次多报几个语法错误。
parser.go的处理方式是，当语法分析器在处理某个产生式的时候，如果发现了错误，那就记录下这个错误，并且往下跳过一些Token，直到找到一个Token是属于这个产生式的Follow集合的。这个时候编译器就认为找到了这个产生式的结尾。这样分析器就可以跳过这个语法单元，继续处理下面的语法单元。
比如，在解析函数声明语句时，如果Go的语法分析器没有找到函数名称，就报错“expecting name or (”，然后往后找到“{”或者“;”，这样就跳过了函数名称的声明部分，继续去编译后面的函数体部分。
在cmd/compile/internal/syntax目录下，还有词法分析器和语法分析器的测试程序，你可以去运行测试一下。
最后，如果你还想对Go语言的语法分析有更加深入地了解，我建议你去阅读一下Go语言的规范，它里面对于每个语法单元，都有EBNF格式的语法规则定义，比如对语句的定义。你通过看代码、看语言规范，积累语法规则的第一手经验，以后再看到一段程序，你的脑子里就能反映出它的语法规则，并且能随手画出AST了，这是你学习编译原理需要建立的硬功夫。比如说，这里我节选了一段Go语言的规范中针对语句的部分语法规则。
Statement = Declaration | LabeledStmt | SimpleStmt | GoStmt | ReturnStmt | BreakStmt | ContinueStmt | GotoStmt | FallthroughStmt | Block | IfStmt | SwitchStmt | SelectStmt | ForStmt | DeferStmt . SimpleStmt = EmptyStmt | ExpressionStmt | SendStmt | IncDecStmt | Assignment | ShortVarDecl .</description></item><item><title>24_MySQL是怎么保证主备一致的？</title><link>https://artisanbox.github.io/1/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/24/</guid><description>在前面的文章中，我不止一次地和你提到了binlog，大家知道binlog可以用来归档，也可以用来做主备同步，但它的内容是什么样的呢？为什么备库执行了binlog就可以跟主库保持一致了呢？今天我就正式地和你介绍一下它。
毫不夸张地说，MySQL能够成为现下最流行的开源数据库，binlog功不可没。
在最开始，MySQL是以容易学习和方便的高可用架构，被开发人员青睐的。而它的几乎所有的高可用架构，都直接依赖于binlog。虽然这些高可用架构已经呈现出越来越复杂的趋势，但都是从最基本的一主一备演化过来的。
今天这篇文章我主要为你介绍主备的基本原理。理解了背后的设计原理，你也可以从业务开发的角度，来借鉴这些设计思想。
MySQL主备的基本原理如图1所示就是基本的主备切换流程。
图 1 MySQL主备切换流程在状态1中，客户端的读写都直接访问节点A，而节点B是A的备库，只是将A的更新都同步过来，到本地执行。这样可以保持节点B和A的数据是相同的。
当需要切换的时候，就切成状态2。这时候客户端读写访问的都是节点B，而节点A是B的备库。
在状态1中，虽然节点B没有被直接访问，但是我依然建议你把节点B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑：
有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作；
防止切换逻辑有bug，比如切换过程中出现双写，造成主备不一致；
可以用readonly状态，来判断节点的角色。
你可能会问，我把备库设置成只读了，还怎么跟主库保持同步更新呢？
这个问题，你不用担心。因为readonly设置对超级(super)权限用户是无效的，而用于同步更新的线程，就拥有超级权限。
接下来，我们再看看节点A到B这条线的内部流程是什么样的。图2中画出的就是一个update语句在节点A执行，然后同步到节点B的完整流程图。
图2 主备流程图图2中，包含了我在上一篇文章中讲到的binlog和redo log的写入机制相关的内容，可以看到：主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写binlog。
备库B跟主库A之间维持了一个长连接。主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程是这样的：
在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量。
在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和sql_thread。其中io_thread负责与主库建立连接。
主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。
备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。
sql_thread读取中转日志，解析出日志里的命令，并执行。
这里需要说明，后来由于多线程复制方案的引入，sql_thread演化成为了多个线程，跟我们今天要介绍的原理没有直接关系，暂且不展开。
分析完了这个长连接的逻辑，我们再来看一个问题：binlog里面到底是什么内容，为什么备库拿过去可以直接执行。
binlog的三种格式对比我在第15篇答疑文章中，和你提到过binlog有两种格式，一种是statement，一种是row。可能你在其他资料上还会看到有第三种格式，叫作mixed，其实它就是前两种格式的混合。
为了便于描述binlog的这三种格式间的区别，我创建了一个表，并初始化几行数据。
mysql&amp;gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `t_modified`(`t_modified`) ) ENGINE=InnoDB; insert into t values(1,1,&amp;lsquo;2018-11-13&amp;rsquo;); insert into t values(2,2,&amp;lsquo;2018-11-12&amp;rsquo;); insert into t values(3,3,&amp;lsquo;2018-11-11&amp;rsquo;); insert into t values(4,4,&amp;lsquo;2018-11-10&amp;rsquo;); insert into t values(5,5,&amp;lsquo;2018-11-09&amp;rsquo;); 如果要在表中删除一行数据的话，我们来看看这个delete语句的binlog是怎么记录的。</description></item><item><title>24_中间代码：兼容不同的语言和硬件</title><link>https://artisanbox.github.io/6/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/24/</guid><description>前几节课，我带你尝试不通过IR，直接生成汇编代码，这是为了帮你快速破冰，建立直觉。在这个过程中，你也遇到了一些挑战，比如：
你要对生成的代码进行优化，才有可能更好地使用寄存器和内存，同时也能减少代码量；
另外，针对不同的CPU和操作系统，你需要调整生成汇编代码的逻辑。
这些实际体验，都进一步验证了20讲中，IR的作用：我们能基于IR对接不同语言的前端，也能对接不同的硬件架构，还能做很多的优化。
既然IR有这些作用，那你可能会问，IR都是什么样子的呢？有什么特点？如何生成IR呢？
本节课，我就带你了解IR的特点，认识常见的三地址代码，学会如何把高级语言的代码翻译成IR。然后，我还会特别介绍LLVM的IR，以便后面使用LLVM这个工具。
首先，来看看IR的特征。
介于中间的语言IR的意思是中间表达方式，它在高级语言和汇编语言的中间，这意味着，它的特征也是处于二者之间的。
与高级语言相比，IR丢弃了大部分高级语言的语法特征和语义特征，比如循环语句、if语句、作用域、面向对象等等，它更像高层次的汇编语言；而相比真正的汇编语言，它又不会有那么多琐碎的、与具体硬件相关的细节。
相信你在学习汇编语言的时候，会发现汇编语言的细节特别多。比如，你要知道很多指令的名字和用法，还要记住很多不同的寄存器。在22讲，我提到，如果你想完整地掌握x86-64架构，还需要接触很多指令集，以及调用约定的细节、内存使用的细节等等（参见Intel的手册）。
仅仅拿指令的数量来说，据有人统计，Intel指令的助记符有981个之多！都记住怎么可能啊。所以说，汇编语言并不难，而是麻烦。
IR不会像x86-64汇编语言那么繁琐，但它却包含了足够的细节信息，能方便我们实现优化算法，以及生成针对目标机器的汇编代码。
另外，我在20讲提到，IR有很多种类（AST也是一种IR），每种IR都有不同的特点和用途，有的编译器，甚至要用到几种不同的IR。
我们在后端部分所讲的IR，目的是方便执行各种优化算法，并有利于生成汇编。这种IR，可以看做是一种高层次的汇编语言，主要体现在：
它可以使用寄存器，但寄存器的数量没有限制； 控制结构也跟汇编语言比较像，比如有跳转语句，分成多个程序块，用标签来标识程序块等； 使用相当于汇编指令的操作码。这些操作码可以一对一地翻译成汇编代码，但有时一个操作码会对应多个汇编指令。 下面来看看一个典型IR：三地址代码，简称TAC。
认识典型的IR：三地址代码（TAC）下面是一种常见的IR的格式，它叫做三地址代码（Three Address Code, TAC），它的优点是很简洁，所以适合用来讨论算法：
x := y op z //二元操作 x := op y //一元操作 每条三地址代码最多有三个地址，其中两个是源地址（比如第一行代码的y和z），一个是目的地址（也就是x），每条代码最多有一个操作（op）。
我来举几个例子，带你熟悉一下三地址代码，这样，你能掌握三地址代码的特点，从高级语言的代码转换生成三地址代码。
1.基本的算术运算：
int a, b, c, d; a = b + c * d; TAC：
t1 := c * d a := b + t1 t1是新产生的临时变量。当源代码的表达式中包含一个以上的操作符时，就需要引入临时变量，并把原来的一条代码拆成多条代码。
2.布尔值的计算：
int a, b; bool x, y; x = a * 2 &amp;lt; b; y = a + 3 == b; TAC：</description></item><item><title>24_二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？</title><link>https://artisanbox.github.io/2/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/25/</guid><description>上一节我们学习了树、二叉树以及二叉树的遍历，今天我们再来学习一种特殊的二叉树，二叉查找树。二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。
我们之前说过，散列表也是支持这些操作的，并且散列表的这些操作比二叉查找树更高效，时间复杂度是O(1)。既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？
带着这些问题，我们就来学习今天的内容，二叉查找树！
二叉查找树（Binary Search Tree）二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。它是怎么做到这些的呢？
这些都依赖于二叉查找树的特殊结构。二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。 我画了几个二叉查找树的例子，你一看应该就清楚了。
前面我们讲到，二叉查找树支持快速查找、插入、删除操作，现在我们就依次来看下，这三个操作是如何实现的。
1.二叉查找树的查找操作首先，我们看如何在二叉查找树中查找一个节点。我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。
这里我把查找的代码实现了一下，贴在下面了，结合代码，理解起来会更加容易。
public class BinarySearchTree { private Node tree; public Node find(int data) { Node p = tree; while (p != null) { if (data &amp;lt; p.data) p = p.left; else if (data &amp;gt; p.data) p = p.right; else return p; } return null; }
public static class Node { private int data; private Node left; private Node right;
public Node(int data) { this.</description></item><item><title>24_冒险和预测（三）：CPU里的“线程池”</title><link>https://artisanbox.github.io/4/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/24/</guid><description>过去两讲，我为你讲解了通过增加资源、停顿等待以及主动转发数据的方式，来解决结构冒险和数据冒险问题。对于结构冒险，由于限制来自于同一时钟周期不同的指令，要访问相同的硬件资源，解决方案是增加资源。对于数据冒险，由于限制来自于数据之间的各种依赖，我们可以提前把数据转发到下一个指令。
但是即便综合运用这三种技术，我们仍然会遇到不得不停下整个流水线，等待前面的指令完成的情况，也就是采用流水线停顿的解决方案。比如说，上一讲里最后给你的例子，即使我们进行了操作数前推，因为第二条加法指令依赖于第一条指令从内存中获取的数据，我们还是要插入一次NOP的操作。
那这个时候你就会想了，那我们能不能让后面没有数据依赖的指令，在前面指令停顿的时候先执行呢？
答案当然是可以的。毕竟，流水线停顿的时候，对应的电路闲着也是闲着。那我们完全可以先完成后面指令的执行阶段。
填上空闲的NOP：上菜的顺序不必是点菜的顺序之前我为你讲解的，无论是流水线停顿，还是操作数前推，归根到底，只要前面指令的特定阶段还没有执行完成，后面的指令就会被“阻塞”住。
但是这个“阻塞”很多时候是没有必要的。因为尽管你的代码生成的指令是顺序的，但是如果后面的指令不需要依赖前面指令的执行结果，完全可以不必等待前面的指令运算完成。
比如说，下面这三行代码。
a = b + c d = a * e x = y * z 计算里面的 x ，却要等待 a 和 d 都计算完成，实在没啥必要。所以我们完全可以在 d 的计算等待 a 的计算的过程中，先把 x 的结果给算出来。
在流水线里，后面的指令不依赖前面的指令，那就不用等待前面的指令执行，它完全可以先执行。
可以看到，因为第三条指令并不依赖于前两条指令的计算结果，所以在第二条指令等待第一条指令的访存和写回阶段的时候，第三条指令就已经执行完成了。
这就好比你开了一家餐馆，顾客会排队来点菜。餐馆的厨房里会有洗菜、切菜、炒菜、上菜这样的各个步骤。后厨也是按照点菜的顺序开始做菜的。但是不同的菜需要花费的时间和工序可能都有差别。有些菜做起来特别麻烦，特别慢。比如做一道佛跳墙有好几道工序。我们没有必要非要等先点的佛跳墙上菜了，再开始做后面的炒鸡蛋。只要有厨子空出来了，就可以先动手做前面的简单菜，先给客户端上去。
这样的解决方案，在计算机组成里面，被称为乱序执行（Out-of-Order Execution，OoOE）。乱序执行，最早来自于著名的IBM 360。相信你一定听说过《人月神话》这本软件工程届的经典著作，它讲的就是IBM 360开发过程中的“人生体会”。而IBM 360困难的开发过程，也少不了第一次引入乱序执行这个新的CPU技术。
CPU里的“线程池”：理解乱序执行那么，我们的CPU怎样才能实现乱序执行呢？是不是像玩俄罗斯方块一样，把后面的指令，找一个前面的坑填进去就行了？事情并没有这么简单。其实，从今天软件开发的维度来思考，乱序执行好像是在指令的执行阶段，引入了一个“线程池”。我们下面就来看一看，在CPU里，乱序执行的过程究竟是怎样的。
使用乱序执行技术后，CPU里的流水线就和我之前给你看的5级流水线不太一样了。我们一起来看一看下面这张图。
1.在取指令和指令译码的时候，乱序执行的CPU和其他使用流水线架构的CPU是一样的。它会一级一级顺序地进行取指令和指令译码的工作。
2.在指令译码完成之后，就不一样了。CPU不会直接进行指令执行，而是进行一次指令分发，把指令发到一个叫作保留站（Reservation Stations）的地方。顾名思义，这个保留站，就像一个火车站一样。发送到车站的指令，就像是一列列的火车。
3.这些指令不会立刻执行，而要等待它们所依赖的数据，传递给它们之后才会执行。这就好像一列列的火车都要等到乘客来齐了才能出发。
4.一旦指令依赖的数据来齐了，指令就可以交到后面的功能单元（Function Unit，FU），其实就是ALU，去执行了。我们有很多功能单元可以并行运行，但是不同的功能单元能够支持执行的指令并不相同。就和我们的铁轨一样，有些从上海北上，可以到北京和哈尔滨；有些是南下的，可以到广州和深圳。
5.指令执行的阶段完成之后，我们并不能立刻把结果写回到寄存器里面去，而是把结果再存放到一个叫作重排序缓冲区（Re-Order Buffer，ROB）的地方。
6.在重排序缓冲区里，我们的CPU会按照取指令的顺序，对指令的计算结果重新排序。只有排在前面的指令都已经完成了，才会提交指令，完成整个指令的运算结果。
7.实际的指令的计算结果数据，并不是直接写到内存或者高速缓存里，而是先写入存储缓冲区（Store Buffer面，最终才会写入到高速缓存和内存里。
可以看到，在乱序执行的情况下，只有CPU内部指令的执行层面，可能是“乱序”的。只要我们能在指令的译码阶段正确地分析出指令之间的数据依赖关系，这个“乱序”就只会在互相没有影响的指令之间发生。
即便指令的执行过程中是乱序的，我们在最终指令的计算结果写入到寄存器和内存之前，依然会进行一次排序，以确保所有指令在外部看来仍然是有序完成的。
有了乱序执行，我们重新去执行上面的3行代码。
a = b + c d = a * e x = y * z 里面的 d 依赖于 a 的计算结果，不会在 a 的计算完成之前执行。但是我们的CPU并不会闲着，因为 x = y * z 的指令同样会被分发到保留站里。因为 x 所依赖的 y 和 z 的数据是准备好的， 这里的乘法运算不会等待计算 d，而会先去计算 x 的值。</description></item><item><title>24_查询有点慢，语句该如何写？</title><link>https://artisanbox.github.io/8/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/24/</guid><description>你好，我是朱晓峰。这节课，我想和你聊一聊怎么对查询语句进行调优。
你肯定遇到过这样的情况：你写的SQL语句执行起来特别慢，要等好久才出结果，或者是干脆就“死”在那里，一点反应也没有。一旦遇到这种问题，你就要考虑进行优化了。
如果你开发过数据库应用，肯定会有这样的体会：让应用运行起来不难，但是要运行得又快又好，就没那么不容易了。这很考验我们的内功。
而要想提高应用的运行效率，你就必须掌握优化查询的方法。今天，我就给你讲一下MySQL的查询分析语句和2种优化查询的方法。
查询分析语句虽然MySQL的查询分析语句并不能直接优化查询，但是却可以帮助你了解SQL语句的执行计划，有助于你分析查询效率低下的原因，进而有针对性地进行优化。查询分析语句的语法结构是：
{ EXPLAIN | DESCRIBE | DESC }查询语句; 下面我借助一个小例子，给你详细地讲解一下，怎么使用查询分析语句，来分析一个查询的执行计划。
假设有一个销售流水表（demo.trans），里面有400万条数据，如下所示：
现在，我要查询一下商品编号是1的商品，在2020年6月18日上午9点到12点之间的销售明细。代码如下所示：
mysql&amp;gt; SELECT itemnumber,quantity,price,transdate -&amp;gt; FROM demo.trans -&amp;gt; WHERE itemnumber=1 -&amp;gt; AND transdate&amp;gt;'2020-06-18 09:00:00' -&amp;gt; AND transdate&amp;lt;'2020-06-18 12:00:00'; +------------+----------+-------+---------------------+ | itemnumber | quantity | price | transdate | +------------+----------+-------+---------------------+ | 1 | 0.276 | 70.00 | 2020-06-18 11:04:00 | | 1 | 1.404 | 70.00 | 2020-06-18 11:10:57 | | 1 | 0.554 | 70.00 | 2020-06-18 11:18:12 | | 1 | 0.</description></item><item><title>24_活动的描述：到底什么是进程？</title><link>https://artisanbox.github.io/9/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/24/</guid><description>你好，我是LMOS。
在前面的课程里，我们已经实现了数据同步、hal层的初始化，中断框架、物理内存、内存对象、虚拟内存管理，这些都是操作系统中最核心的东西。
今天，我再给你讲讲操作系统里一个层次非常高的组件——进程，而它又非常依赖于内存管理、中断、硬件体系结构。好在前面课程中，这些基础知识我们已经搞得清清楚楚，安排得明明白白了，所以我们今天理解进程就变得顺理成章。
感受一下在你看来，什么是进程呢？日常我们跟计算机打交道的时候，最常接触的就是一些应用程序，比如Word、浏览器，你可以直观感受到它们的存在。而我们却很难直观感受到什么是进程，自然也就不容易描述它的模样与形态了。
其实，在我们启用Word这些应用时，操作系统在背后就会建立至少一个进程。虽然我们难以观察它的形态，但我们绝对可以通过一些状态数据来发现进程的存在。
在Linux的终端下输入ps命令， 我们就可以看到系统中有多少个进程了。如下图所示。
这是进程吗？是的，不过这只是一些具体进程的数据，如创建进程和用户、进程ID、使用CPU的百分比，进程运行状态，进程的建立时间、进程的运行时间、进程名等，这些数据综合起来就代表了一个进程。
也许看到这，你会呵呵一笑，觉得原来抽象的进程背后，不过是一堆数据而已，关于进程这就是我们能直观感受到的东西，这就完了吗？当然没有，我们接着往下看。
什么是进程如果你要组织一个活动怎么办？你首先会想到，这个活动的流程是什么，需要配备哪些人员和物资，中途要不要休息，活动当前进行到哪里了……如果你是个精明的人，你大概会用表格把这些信息记录下来。
同理，你运行一个应用程序时，操作系统也要记录这个应用程序使用多少内存，打开了什么文件，当有些资源不可用的时候要不要睡眠，当前进程运行到哪里了。操作系统把这些信息综合统计，存放在内存中，抽象为进程。
现在你就可以回答什么是进程了：进程是一个应用程序运行时刻的实例（从进程的结构看）；进程是应用程序运行时所需资源的容器（从进程的功能看）；甚至进程是一堆数据结构（从操作系统对进程实现的角度来说）。
这也太简单了吧？对，进程的抽象概念就是这么简单。我知道这一定不能让你真正明白什么是进程，抽象的概念就是如此，你不在实践中设计并实现它，是很难真正明白的。下面我们先来细化设计。
进程的结构首先，进程是一个应用程序运行时刻的实例，它的目的就是操作系统用于管理和运行多个应用程序的；其次，从前面我们实现的内存管理组件角度看，操作系统是给应用程序提供服务的。
所以，从这两个角度看，进程必须要有一个地址空间，这个地址空间至少包括两部分内容：一部分是内核，一部分是用户的应用程序。
最后，结合x86硬件平台对虚拟地址空间的制约，我给你画了一幅图，如下所示。
上图中有8个进程，每个进程拥有x86 CPU的整个虚拟地址空间，这个虚拟地址空间被分成了两个部分，上半部分是所有进程都共享的内核部分 ，里面放着一份内核代码和数据，下半部分是应用程序，分别独立，互不干扰。
还记得我们讲过的x86 CPU的特权级吗？
当CPU在R0特权级运行时，就运行在上半部分内核的地址空间中，当CPU在R3特权级时，就运行在下半部分的应用程序地址空间中。各进程的虚拟地址空间是相同的，它们之间物理地址不同，是由MMU页表进行隔离的，所以每个进程的应用程序的代码绝对不能随意访问内核的代码和数据。
以上是整体结构，下面我们来细化一下进程需要实现哪些功能？
我们先从应用程序和内核的关系看。应用程序需要内核提供资源，而内核需要控制应用程序的运行。那么内核必须能够命令应用程序，让它随时中断（进入内核地址空间）或恢复执行，这就需要保存应用程序的机器上下文和它运行时刻的栈。
接着，我们深入内核提供服务的机制。众所周知，内核是这样提供服务的：通过停止应用程序代码运行，进入内核地址空间运行内核代码，然后返回结果。就像活动组织者会用表格备案一样，内核还需要记录一个应用程序都访问了哪些资源，比如打开了某个文件，或是访问了某个设备。而这样的“记录表”，我们就用“资源描述符”来表示。
而我们前面已经说了，进程是一个应用程序运行时刻的实例。那这样一来，一个细化的进程结构，就可以像下图这样设计。
上图中表示了一个进程详细且必要的结构，其中带*号是每个进程都有独立一份，有了这样的设计结构，多个进程就能并发运行了。前面这些内容还是纸上谈兵，你重点搞明白进程的概念和结构就行了。
实现进程前面我们简单介绍了进程的概念和结构，之所以简单，是为了不在理论层面就把问题复杂化，这对我们实现Cosmos的进程组件没有任何好处。
但只懂理论还是空中阁楼，我们可以一步步在设计实现中，由浅到深地理解什么是进程。我们这就把前面的概念和设计，一步步落实到代码，设计出对应的数据结构。
如何表示一个进程根据前面课程的经验，如果要在软件代码中表示一个什么东西时，就要设计出对应的数据结构。
那么对于一个进程，它有状态，id，运行时间，优先级，应用程序栈，内核栈，机器上下文，资源描述符，地址空间，我们将这些信息组织在一起，就形成了一个进程的数据结构。
下面我带你把它变成代码，在cosmos/include/knlinc/目录下建立一个krlthread_t.h文件，在其中写上代码，如下所示。
typedef struct s_THREAD { spinlock_t td_lock; //进程的自旋锁 list_h_t td_list; //进程链表 uint_t td_flgs; //进程的标志 uint_t td_stus; //进程的状态 uint_t td_cpuid; //进程所在的CPU的id uint_t td_id; //进程的id uint_t td_tick; //进程运行了多少tick uint_t td_privilege; //进程的权限 uint_t td_priority; //进程的优先级 uint_t td_runmode; //进程的运行模式 adr_t td_krlstktop; //应用程序内核栈顶地址 adr_t td_krlstkstart; //应用程序内核栈开始地址 adr_t td_usrstktop; //应用程序栈顶地址 adr_t td_usrstkstart; //应用程序栈开始地址 mmadrsdsc_t* td_mmdsc; //地址空间结构 context_t td_context; //机器上下文件结构 objnode_t* td_handtbl[TD_HAND_MAX];//打开的对象数组 }thread_t; 在Cosmos中，我们就使用thread_t结构的一个实例变量代表一个进程。进程的内核栈和进程的应用程序栈是两块内存空间，进程的权限表示一个进程是用户进程还是系统进程。进程的权限不同，它们能完成功能也不同。</description></item><item><title>24｜增强编译器前端功能第3步：全面的集合运算</title><link>https://artisanbox.github.io/3/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/26/</guid><description>你好，我是宫文学。
在上一节课，我们扩展了我们语言的类型体系，还测试了几个简单的例子。从中，我们已经能体会出一些TypeScript类型体系的特点了。
不过，TypeScript的类型体系其实比我们前面测试的还要强大得多，能够在多种场景下进行复杂的类型处理。
今天这节课，我们会通过多个实际的例子，来探索TypeScript的类型处理能力。并且，在这个过程中，你还会进一步印证我们上一节课的一个知识点，就是类型计算实际上就是集合运算。在我们今天的这些例子中，你会见到多种集合运算，包括子集判断、重叠判断，以及交集、并集和补集的计算。
首先，让我们看几个例子，来理解一下类型计算的使用场景。
类型计算的场景我们先看第一个例子：
function foo1(age : number|null){ let age1 : string|number; age1 = age; //编译器在这里会检查出错误。 console.log(age1); } 在这个例子中，我们用到了age和age1两个变量，它们都采用了联合类型。一个是number|null，一个是string|number。
如果你用–strict选项来编译这个程序，那么tsc会报错：
这个错误信息的意思是：类型number|null不能赋给类型string|number。具体来说，null是不能赋给string|number的。
这说明什么呢？这说明对于赋值语句，比如x = y来说，它会有一个默认要求，要求y的类型要么跟x一样，要么是x的子集才可以。我们把这个关系记做y.type &amp;lt;= x.type。
那么，其他的二元运算，是不是也像赋值运算那样，需要一个类型是另一个类型的子集呢？
不是的。不同的运算，做类型检查的规则是不同的。比如，对于“==”和“!=”这两个运算符，只需要两个类型有交集就可以。你可以用tsc编译一下这个例子：
function foo2(age1 : number|null, age2:string|number){ if (age1 == age2){ //OK。只要两个类型有交集就可以。 console.log("same age!"); } } 你会看到，编译器并不会报错。这说明，两个不同的类型，只要它们有交集，就可以进行等值和不等值比较。并且，即使age1的值是null，age2的值是一个字符串，等值比较仍然是有意义的，比较的结果是不相等。
那如果两个类型没有交集，会发生什么情况呢？我们看看下面的例子，参数x和y属于不同的类型，它们之间没有交集。
function foo3(x : number|null, y:string|boolean){ if (x == y){ //编译器报错：两个类型没有交集 console.log("x and y is the same"); } } 这次，如果你用tsc去编译，即使不加–strict选项，编译器也会报错：
编译器会说，这个条件表达式会永远返回false，因为这两个类型没有交集。
到此为止，我们就了解清楚等值比较的规则了，也就是要求两个类型有交集才可以，或者说两个类型要存在重叠。
那其他的比较运算符，比如&amp;gt;，&amp;gt;=，&amp;lt;，&amp;lt;=，也遵循相同的规则吗？
我们把foo2中的==运算符改为&amp;gt;=运算符，得到一个新的示例程序：</description></item><item><title>25_MySQL是怎么保证高可用的？</title><link>https://artisanbox.github.io/1/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/25/</guid><description>在上一篇文章中，我和你介绍了binlog的基本内容，在一个主备关系中，每个备库接收主库的binlog并执行。
正常情况下，只要主库执行更新生成的所有binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。
但是，MySQL要提供高可用能力，只有最终一致性是不够的。为什么这么说呢？今天我就着重和你分析一下。
这里，我再放一次上一篇文章中讲到的双M结构的主备切换流程图。
图 1 MySQL主备切换流程--双M结构主备延迟主备切换可能是一个主动运维动作，比如软件升级、主库所在机器按计划下线等，也可能是被动操作，比如主库所在机器掉电。
接下来，我们先一起看看主动切换的场景。
在介绍主动切换流程的详细步骤之前，我要先跟你说明一个概念，即“同步延迟”。与数据同步有关的时间点主要包括以下三个：
主库A执行完成一个事务，写入binlog，我们把这个时刻记为T1;
之后传给备库B，我们把备库B接收完这个binlog的时刻记为T2;
备库B执行完成这个事务，我们把这个时刻记为T3。
所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是T3-T1。
你可以在备库上执行show slave status命令，它的返回结果里面会显示seconds_behind_master，用于表示当前备库延迟了多少秒。
seconds_behind_master的计算方法是这样的：
每个事务的binlog 里面都有一个时间字段，用于记录主库上写入的时间；
备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时间的差值，得到seconds_behind_master。
可以看到，其实seconds_behind_master这个参数计算的就是T3-T1。所以，我们可以用seconds_behind_master来作为主备延迟的值，这个值的时间精度是秒。
你可能会问，如果主备库机器的系统时间设置不一致，会不会导致主备延迟的值不准？
其实不会的。因为，备库连接到主库的时候，会通过执行SELECT UNIX_TIMESTAMP()函数来获得当前主库的系统时间。如果这时候发现主库的系统时间与自己不一致，备库在执行seconds_behind_master计算的时候会自动扣掉这个差值。
需要说明的是，在网络正常的时候，日志从主库传给备库所需的时间是很短的，即T2-T1的值是非常小的。也就是说，网络正常情况下，主备延迟的主要来源是备库接收完binlog和执行完这个事务之间的时间差。
所以说，主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产binlog的速度要慢。接下来，我就和你一起分析下，这可能是由哪些原因导致的。
主备延迟的来源首先，有些部署条件下，备库所在机器的性能要比主库所在的机器性能差。
一般情况下，有人这么部署时的想法是，反正备库没有请求，所以可以用差一点儿的机器。或者，他们会把20个主库放在4台机器上，而把备库集中在一台机器上。
其实我们都知道，更新请求对IOPS的压力，在主库和备库上是无差别的。所以，做这种部署时，一般都会将备库设置为“非双1”的模式。
但实际上，更新过程中也会触发大量的读操作。所以，当备库主机上的多个备库都在争抢资源的时候，就可能会导致主备延迟了。
当然，这种部署现在比较少了。因为主备可能发生切换，备库随时可能变成主库，所以主备库选用相同规格的机器，并且做对称部署，是现在比较常见的情况。
追问1：但是，做了对称部署以后，还可能会有延迟。这是为什么呢？
这就是第二种常见的可能了，即备库的压力大。一般的想法是，主库既然提供了写能力，那么备库可以提供一些读能力。或者一些运营后台需要的分析语句，不能影响正常业务，所以只能在备库上跑。
我真就见过不少这样的情况。由于主库直接影响业务，大家使用起来会比较克制，反而忽视了备库的压力控制。结果就是，备库上的查询耗费了大量的CPU资源，影响了同步速度，造成主备延迟。
这种情况，我们一般可以这么处理：
一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。
通过binlog输出到外部系统，比如Hadoop这类系统，让外部系统提供统计类查询的能力。
其中，一主多从的方式大都会被采用。因为作为数据库系统，还必须保证有定期全量备份的能力。而从库，就很适合用来做备份。
备注：这里需要说明一下，从库和备库在概念上其实差不多。在我们这个专栏里，为了方便描述，我把会在HA过程中被选成新主库的，称为备库，其他的称为从库。
追问2：采用了一主多从，保证备库的压力不会超过主库，还有什么情况可能导致主备延迟吗？
这就是第三种可能了，即大事务。
大事务这种情况很好理解。因为主库上必须等事务执行完成才会写入binlog，再传给备库。所以，如果一个主库上的语句执行10分钟，那这个事务很可能就会导致从库延迟10分钟。
不知道你所在公司的DBA有没有跟你这么说过：不要一次性地用delete语句删除太多数据。其实，这就是一个典型的大事务场景。
比如，一些归档类的数据，平时没有注意删除历史数据，等到空间快满了，业务开发人员要一次性地删掉大量历史数据。同时，又因为要避免在高峰期操作会影响业务（至少有这个意识还是很不错的），所以会在晚上执行这些大量数据的删除操作。
结果，负责的DBA同学半夜就会收到延迟报警。然后，DBA团队就要求你后续再删除数据的时候，要控制每个事务删除的数据量，分成多次删除。
另一种典型的大事务场景，就是大表DDL。这个场景，我在前面的文章中介绍过。处理方案就是，计划内的DDL，建议使用gh-ost方案（这里，你可以再回顾下第13篇文章《为什么表数据删掉一半，表文件大小不变？》中的相关内容）。
追问3：如果主库上也不做大事务了，还有什么原因会导致主备延迟吗？
造成主备延迟还有一个大方向的原因，就是备库的并行复制能力。这个话题，我会留在下一篇文章再和你详细介绍。
其实还是有不少其他情况会导致主备延迟，如果你还碰到过其他场景，欢迎你在评论区给我留言，我来和你一起分析、讨论。
由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略。</description></item><item><title>25_MySQL编译器（一）：解析一条SQL语句的执行过程</title><link>https://artisanbox.github.io/7/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/25/</guid><description>你好，我是宫文学。现在，就到了我们编译之旅的最后一站了，我们一起来探索一下MySQL编译器。
数据库系统能够接受SQL语句，并返回数据查询的结果，或者对数据库中的数据进行修改，可以说几乎每个程序员都使用过它。
而MySQL又是目前使用最广泛的数据库。所以，解析一下MySQL编译并执行SQL语句的过程，一方面能帮助你加深对数据库领域的编译技术的理解；另一方面，由于SQL是一种最成功的DSL（特定领域语言），所以理解了MySQL编译器的内部运作机制，也能加深你对所有使用数据操作类DSL的理解，比如文档数据库的查询语言。另外，解读SQL与它的运行时的关系，也有助于你在自己的领域成功地使用DSL技术。
那么，数据库系统是如何使用编译技术的呢？接下来，我就会花两讲的时间，带你进入到MySQL的内部，做一次全面的探秘。
今天这一讲，我先带你了解一下如何跟踪MySQL的运行，了解它处理一个SQL语句的过程，以及MySQL在词法分析和语法分析方面的实现机制。
好，让我们开始吧！
编译并调试MySQL按照惯例，你要下载MySQL的源代码。我下载的是8.0版本的分支。
源代码里的主要目录及其作用如下，我们需要分析的代码基本都在sql目录下，它包含了编译器和服务端的核心组件。
图1：MySQL的源代码包含的主要目录MySQL的源代码主要是.cc结尾的，也就是说，MySQL主要是用C++编写的。另外，也有少量几个代码文件是用C语言编写的。
为了跟踪MySQL的执行过程，你要用Debug模式编译MySQL，具体步骤可以参考这篇开发者文档。
如果你用单线程编译，大约需要1个小时。编译好以后，先初始化出一个数据库来：
./mysqld --initialize --user=mysql 这个过程会为root@localhost用户，生成一个缺省的密码。
接着，运行MySQL服务器：
./mysqld &amp;amp; 之后，通过客户端连接数据库服务器，这时我们就可以执行SQL了：
./mysql -uroot -p #连接mysql server 最后，我们把GDB调试工具附加到mysqld进程上，就可以对它进行调试了。
gdb -p `pidof mysqld` #pidof是一个工具，用于获取进程的id，你可以安装一下 提示：这一讲中，我是采用了一个CentOS 8的虚拟机来编译和调试MySQL。我也试过在macOS下编译，并用LLDB进行调试，也一样方便。
注意，你在调试程序的时候，有两个设置断点的好地方：
dispatch_command：在sql/sql_parse.cc文件里。在接受客户端请求的时候（比如一个SQL语句），会在这里集中处理。 my_message_sql：在sql/mysqld.cc文件里。当系统需要输出错误信息的时候，会在这里集中处理。 这个时候，我们在MySQL的客户端输入一个查询命令，就可以从雇员表里查询姓和名了。在这个例子中，我采用的数据库是MySQL的一个示例数据库employees，你可以根据它的文档来生成示例数据库。
mysql&amp;gt; select first_name, last_name from employees; #从mysql库的user表中查询信息 这个命令被mysqld接收到以后，就会触发断点，并停止执行。这个时候，客户端也会老老实实地停在那里，等候从服务端传回数据。即使你在后端跟踪代码的过程会花很长的时间，客户端也不会超时，一直在安静地等待。给我的感觉就是，MySQL对于调试程序还是很友好的。
在GDB中输入bt命令，会打印出调用栈，这样你就能了解一个SQL语句，在MySQL中执行的完整过程。为了方便你理解和复习，这里我整理成了一个表格：
我也把MySQL执行SQL语句时的一些重要程序入口记录了下来，这也需要你重点关注。它反映了执行SQL过程中的一些重要的处理阶段，包括语法分析、处理上下文、引用消解、优化和执行。你在这些地方都可以设置断点。
图2：MySQL执行SQL语句时的部分重要程序入口好了，现在你就已经做好准备，能够分析MySQL的内部实现机制了。不过，由于MySQL执行的是SQL语言，它跟我们前面分析的高级语言有所不同。所以，我们先稍微回顾一下SQL语言的特点。
SQL语言：数据库领域的DSLSQL是结构化查询语言（Structural Query Language）的英文缩写。举个例子，这是一个很简单的SQL语句：
select emp_no, first_name, last_name from employees; 其实在大部分情况下，SQL都是这样一个一个来做语句执行的。这些语句又分为DML（数据操纵语言）和DDL（数据定义语言）两类。前者是对数据的查询、修改和删除等操作，而后者是用来定义数据库和表的结构（又叫模式）。
我们平常最多使用的是DML。而DML中，执行起来最复杂的是select语句。所以，在本讲，我都是用select语句来给你举例子。
那么，SQL跟我们前面分析的高级语言相比有什么不同呢？
第一个特点：SQL是声明式（Declarative）的。这是什么意思呢？其实就是说，SQL语句能够表达它的计算逻辑，但它不需要描述控制流。
高级语言一般都有控制流，也就是详细规定了实现一个功能的流程：先调用什么功能，再调用什么功能，比如if语句、循环语句等等。这种方式叫做命令式（imperative）编程。
更深入一点，声明式编程说的是“要什么”，它不关心实现的过程；而命令式编程强调的是“如何做”。前者更接近人类社会的领域问题，而后者更接近计算机实现。
第二个特点：SQL是一种特定领域语言（DSL，Domain Specific Language），专门针对关系数据库这个领域的。SQL中的各个元素能够映射成关系代数中的操作术语，比如选择、投影、连接、笛卡尔积、交集、并集等操作。它采用的是表、字段、连接等要素，而不需要使用常见的高级语言的变量、类、函数等要素。
所以，SQL就给其他DSL的设计提供了一个很好的参考：
采用声明式，更加贴近领域需求。比如，你可以设计一个报表的DSL，这个DSL只需要描述报表的特征，而不需要描述其实现过程。 采用特定领域的模型、术语，甚至是数学理论。比如，针对人工智能领域，你完全就可以用张量计算（力学概念）的术语来定义DSL。 好了，现在我们分析了SQL的特点，从而也让你了解了DSL的一些共性特点。那么接下来，顺着MySQL运行的脉络，我们先来了解一下MySQL是如何做词法分析和语法分析的。</description></item><item><title>25_冒险和预测（四）：今天下雨了，明天还会下雨么？</title><link>https://artisanbox.github.io/4/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/25/</guid><description>过去三讲，我主要为你介绍了结构冒险和数据冒险，以及增加资源、流水线停顿、操作数前推、乱序执行，这些解决各种“冒险”的技术方案。
在结构冒险和数据冒险中，你会发现，所有的流水线停顿操作都要从指令执行阶段开始。流水线的前两个阶段，也就是取指令（IF）和指令译码（ID）的阶段，是不需要停顿的。CPU会在流水线里面直接去取下一条指令，然后进行译码。
取指令和指令译码不会需要遇到任何停顿，这是基于一个假设。这个假设就是，所有的指令代码都是顺序加载执行的。不过这个假设，在执行的代码中，一旦遇到 if…else 这样的条件分支，或者 for/while 循环，就会不成立。
回顾一下第6讲的条件跳转流程我们先来回顾一下，第6讲里讲的cmp比较指令、jmp和jle这样的条件跳转指令。可以看到，在jmp指令发生的时候，CPU可能会跳转去执行其他指令。jmp后的那一条指令是否应该顺序加载执行，在流水线里面进行取指令的时候，我们没法知道。要等jmp指令执行完成，去更新了PC寄存器之后，我们才能知道，是否执行下一条指令，还是跳转到另外一个内存地址，去取别的指令。
这种为了确保能取到正确的指令，而不得不进行等待延迟的情况，就是今天我们要讲的控制冒险（Control Harzard）。这也是流水线设计里最后一种冒险。
分支预测：今天下雨了，明天还会继续下雨么？在遇到了控制冒险之后，我们的CPU具体会怎么应对呢？除了流水线停顿，等待前面的jmp指令执行完成之后，再去取最新的指令，还有什么好办法吗？当然是有的。我们一起来看一看。
缩短分支延迟第一个办法，叫作缩短分支延迟。回想一下我们的条件跳转指令，条件跳转指令其实进行了两种电路操作。
第一种，是进行条件比较。这个条件比较，需要的输入是，根据指令的opcode，就能确认的条件码寄存器。
第二种，是进行实际的跳转，也就是把要跳转的地址信息写入到PC寄存器。无论是opcode，还是对应的条件码寄存器，还是我们跳转的地址，都是在指令译码（ID）的阶段就能获得的。而对应的条件码比较的电路，只要是简单的逻辑门电路就可以了，并不需要一个完整而复杂的ALU。
所以，我们可以将条件判断、地址跳转，都提前到指令译码阶段进行，而不需要放在指令执行阶段。对应的，我们也要在CPU里面设计对应的旁路，在指令译码阶段，就提供对应的判断比较的电路。
这种方式，本质上和前面数据冒险的操作数前推的解决方案类似，就是在硬件电路层面，把一些计算结果更早地反馈到流水线中。这样反馈变得更快了，后面的指令需要等待的时间就变短了。
不过只是改造硬件，并不能彻底解决问题。跳转指令的比较结果，仍然要在指令执行的时候才能知道。在流水线里，第一条指令进行指令译码的时钟周期里，我们其实就要去取下一条指令了。这个时候，我们其实还没有开始指令执行阶段，自然也就不知道比较的结果。
分支预测所以，这个时候，我们就引入了一个新的解决方案，叫作分支预测（Branch Prediction）技术，也就是说，让我们的CPU来猜一猜，条件跳转后执行的指令，应该是哪一条。
最简单的分支预测技术，叫作“假装分支不发生”。顾名思义，自然就是仍然按照顺序，把指令往下执行。其实就是CPU预测，条件跳转一定不发生。这样的预测方法，其实也是一种静态预测技术。就好像猜硬币的时候，你一直猜正面，会有50%的正确率。
如果分支预测是正确的，我们自然赚到了。这个意味着，我们节省下来本来需要停顿下来等待的时间。如果分支预测失败了呢？那我们就把后面已经取出指令已经执行的部分，给丢弃掉。这个丢弃的操作，在流水线里面，叫作Zap或者Flush。CPU不仅要执行后面的指令，对于这些已经在流水线里面执行到一半的指令，我们还需要做对应的清除操作。比如，清空已经使用的寄存器里面的数据等等，这些清除操作，也有一定的开销。
所以，CPU需要提供对应的丢弃指令的功能，通过控制信号清除掉已经在流水线中执行的指令。只要对应的清除开销不要太大，我们就是划得来的。
动态分支预测第三个办法，叫作动态分支预测。
上面的静态预测策略，看起来比较简单，预测的准确率也许有50%。但是如果运气不好，可能就会特别差。于是，工程师们就开始思考，我们有没有更好的办法呢？比如，根据之前条件跳转的比较结果来预测，是不是会更准一点？
我们日常生活里，最经常会遇到的预测就是天气预报。如果没有气象台给你天气预报，你想要猜一猜明天是不是下雨，你会怎么办？
有一个简单的策略，就是完全根据今天的天气来猜。如果今天下雨，我们就预测明天下雨。如果今天天晴，就预测明天也不会下雨。这是一个很符合我们日常生活经验的预测。因为一般下雨天，都是连着下几天，不断地间隔地发生“天晴-下雨-天晴-下雨”的情况并不多见。
那么，把这样的实践拿到生活中来是不是有效呢？我在这里给了一张2019年1月上海的天气情况的表格。
我们用前一天的是不是下雨，直接来预测后一天会不会下雨。这个表格里一共有31天，那我们就可以预测30次。你可以数一数，按照这种预测方式，我们可以预测正确23次，正确率是76.7%，比随机预测的50%要好上不少。
而同样的策略，我们一样可以放在分支预测上。这种策略，我们叫一级分支预测（One Level Branch Prediction），或者叫1比特饱和计数（1-bit saturating counter）。这个方法，其实就是用一个比特，去记录当前分支的比较情况，直接用当前分支的比较情况，来预测下一次分支时候的比较情况。
只用一天下雨，就预测第二天下雨，这个方法还是有些“草率”，我们可以用更多的信息，而不只是一次的分支信息来进行预测。于是，我们可以引入一个状态机（State Machine）来做这个事情。
如果连续发生下雨的情况，我们就认为更有可能下雨。之后如果只有一天放晴了，我们仍然认为会下雨。在连续下雨之后，要连续两天放晴，我们才会认为之后会放晴。整个状态机的流转，可以参考我在文稿里放的图。
这个状态机里，我们一共有4个状态，所以我们需要2个比特来记录对应的状态。这样这整个策略，就可以叫作2比特饱和计数，或者叫双模态预测器（Bimodal Predictor）。
好了，现在你可以用这个策略，再去对照一下上面的天气情况。如果天气的初始状态我们放在“多半放晴”的状态下，我们预测的结果的正确率会是22次，也就是73.3%的正确率。可以看到，并不是更复杂的算法，效果一定就更好。实际的预测效果，和实际执行的指令高度相关。
如果想对各种分支预测技术有所了解，Wikipedia里面有更详细的内容和更多的分支预测算法，你可以看看。
为什么循环嵌套的改变会影响性能？说完了分支预测，现在我们先来看一个Java程序。
public class BranchPrediction { public static void main(String args[]) { long start = System.currentTimeMillis(); for (int i = 0; i &amp;lt; 100; i++) { for (int j = 0; j &amp;lt;1000; j ++) { for (int k = 0; k &amp;lt; 10000; k++) { } } } long end = System.</description></item><item><title>25_后端技术的重用：LLVM不仅仅让你高效</title><link>https://artisanbox.github.io/6/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/25/</guid><description>在编译器后端，做代码优化和为每个目标平台生成汇编代码，工作量是很大的。那么，有什么办法能降低这方面的工作量，提高我们的工作效率呢？答案就是利用现成的工具。
在前端部分，我就带你使用Antlr生成了词法分析器和语法分析器。那么在后端部分，我们也可以获得类似的帮助，比如利用LLVM和GCC这两个后端框架。
相比前端的编译器工具，如Lex（Flex）、Yacc（Bison）和Antlr等，对于后端工具，了解的人比较少，资料也更稀缺，如果你是初学者，那么上手的确有一些难度。不过我们已经用20～24讲，铺垫了必要的基础知识，也尝试了手写汇编代码，这些知识足够你学习和掌握后端工具了。
本节课，我想先让你了解一些背景信息，所以会先概要地介绍一下LLVM和GCC这两个有代表性的框架的情况，这样，当我再更加详细地讲解LLVM，带你实际使用一下它的时候，你接受起来就会更加容易了。
两个编译器后端框架：LLVM和GCCLLVM是一个开源的编译器基础设施项目，主要聚焦于编译器的后端功能（代码生成、代码优化、JIT……）。它最早是美国伊利诺伊大学的一个研究性项目，核心主持人员是Chris Lattner（克里斯·拉特纳）。
LLVM的出名是由于苹果公司全面采用了这个框架。苹果系统上的C语言、C++、Objective-C的编译器Clang就是基于LLVM的，最新的Swift编程语言也是基于LLVM，支撑了无数的移动应用和桌面应用。无独有偶，在Android平台上最新的开发语言Kotlin，也支持基于LLVM编译成本地代码。
另外，由Mozilla公司（Firefox就是这个公司的产品）开发的系统级编程语言RUST，也是基于LLVM开发的。还有一门相对小众的科学计算领域的语言，叫做Julia，它既能像脚本语言一样灵活易用，又可以具有C语言一样的速度，在数据计算方面又有特别的优化，它的背后也有LLVM的支撑。
OpenGL和一些图像处理领域也在用LLVM，我还看到一个资料，说阿里云的工程师实现了一个Cava脚本语言，用于配合其搜索引擎系统HA3。
LLVM的logo，一只漂亮的龙：
还有，在人工智能领域炙手可热的TensorFlow框架，在后端也是用LLVM来编译。它把机器学习的IR翻译成LLVM的IR，然后再翻译成支持CPU、GPU和TPU的程序。
所以这样看起来，你所使用的很多语言和工具，背后都有LLVM的影子，只不过你可能没有留意罢了。所以在我看来，要了解编译器的后端技术，就不能不了解LLVM。
与LLVM起到类似作用的后端编译框架是GCC（GNU Compiler Collection，GNU编译器套件）。它支持了GNU Linux上的很多语言，例如C、C++、Objective-C、Fortran、Go语言和Java语言等。其实，它最初只是一个C语言的编译器，后来把公共的后端功能也提炼了出来，形成了框架，支持多种前端语言和后端平台。最近华为发布的方舟编译器，据说也是建立在GCC基础上的。
LLVM和GCC很难比较优劣，因为这两个项目都取得了很大的成功。
在本课程中，我们主要采用LLVM，但其中学到的一些知识，比如IR的设计、代码优化算法、适配不同硬件的策略，在学习GCC或其他编译器后端的时候，也是有用的，从而大大提升学习效率。
接下来，我们先来看看LLVM的构成和特点，让你对它有个宏观的认识。
了解LLVM的特点LLVM能够支持多种语言的前端、多种后端CPU架构。在LLVM内部，使用类型化的和SSA特点的IR进行各种分析、优化和转换：
LLVM项目包含了很多组成部分：
LLVM核心（core）。就是上图中的优化和分析工具，还包括了为各种CPU生成目标代码的功能；这些库采用的是LLVM IR，一个良好定义的中间语言，在上一讲，我们已经初步了解它了。
Clang前端（是基于LLVM的C、C++、Objective-C编译器）。
LLDB（一个调试工具）。
LLVM版本的C++标准类库。
其他一些子项目。
我个人很喜欢LLVM，想了想，主要有几点原因：
首先，LLVM有良好的模块化设计和接口。以前的编译器后端技术很难复用，而LLVM具备定义了良好接口的库，方便使用者选择在什么时候，复用哪些后端功能。比如，针对代码优化，LLVM提供了很多算法，语言的设计者可以自己选择合适的算法，或者实现自己特殊的算法，具有很好的灵活性。
第二，LLVM同时支持JIT（即时编译）和AOT（提前编译）两种模式。过去的语言要么是解释型的，要么编译后运行。习惯了使用解释型语言的程序员，很难习惯必须等待一段编译时间才能看到运行效果。很多科学工作者，习惯在一个REPL界面中一边写脚本，一边实时看到反馈。LLVM既可以通过JIT技术支持解释执行，又可以完全编译后才执行，这对于语言的设计者很有吸引力。
第三，有很多可以学习借鉴的项目。Swift、Rust、Julia这些新生代的语言，实现了很多吸引人的特性，还有很多其他的开源项目，而我们可以研究、借鉴它们是如何充分利用LLVM的。
第四，全过程优化的设计思想。LLVM在设计上支持全过程的优化。Lattner和Adve最早关于LLVM设计思想的文章《LLVM: 一个全生命周期分析和转换的编译框架》，就提出计算机语言可以在各个阶段进行优化，包括编译时、链接时、安装时，甚至是运行时。
以运行时优化为例，基于LLVM我们能够在运行时，收集一些性能相关的数据对代码编译优化，可以是实时优化的、动态修改内存中的机器码；也可以收集这些性能数据，然后做离线的优化，重新生成可执行文件，然后再加载执行，这一点非常吸引我，因为在现代计算环境下，每种功能的计算特点都不相同，确实需要针对不同的场景做不同的优化。下图展现了这个过程（图片来源《 LLVM: A Compilation Framework for Lifelong Program Analysis &amp;amp; Transformation》）：
我建议你读一读Lattner和Adve的这篇论文（另外强调一下，当你深入学习编译技术的时候，阅读领域内的论文就是必不可少的一项功课了）。
第五，LLVM的授权更友好。GNU的很多软件都是采用GPL协议的，所以如果用GCC的后端工具来编写你的语言，你可能必须要按照GPL协议开源。而LLVM则更友好一些，你基于LLVM所做的工作，完全可以是闭源的软件产品。
而我之所以说：“LLVM不仅仅让你更高效”，就是因为上面它的这些特点。
现在，你已经对LLVM的构成和特点有一定的了解了，接下来，我带你亲自动手操作和体验一下LLVM的功能，这样你就可以迅速消除对它的陌生感，快速上手了。
体验一下LLVM的功能首先你需要安装一下LLVM（参照官方网站上的相关介绍下载安装）。因为我使用的是macOS，所以用brew就可以安装。
brew install llvm 因为LLVM里面带了一个版本的Clang和C++的标准库，与本机原来的工具链可能会有冲突，所以brew安装的时候并没有在/usr/local下建立符号链接。你在用LLVM工具的时候，要配置好相关的环境变量。
# 可执行文件的路径 export PATH=&amp;quot;/usr/local/opt/llvm/bin:$PATH&amp;quot; # 让编译器能够找到LLVM export LDFLAGS=&amp;quot;-L/usr/local/opt/llvm/lib&amp;quot; export CPPFLAGS=&amp;quot;-I/usr/local/opt/llvm/include” 安装完毕之后，我们使用一下LLVM自带的命令行工具，分几步体验一下LLVM的功能：</description></item><item><title>25_多个活动要安排（上）：多进程如何调度？</title><link>https://artisanbox.github.io/9/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/25/</guid><description>你好，我是LMOS。
上节课，我们了解了什么是进程，还一起写好了建立进程的代码。不知道你想过没有，如果在系统中只有一个进程，那我们提出进程相关的概念和实现与进程有关的功能，是不是就失去了意义呢？
显然，提出进程的目的之一，就是为了实现多个进程，使系统能运行多个应用程序。今天我们就在单进程的基础上扩展多进程，并在进程与进程之间进行调度。
“你存在，我深深的脑海里，我的梦里，我的心里，我的代码里”，我经常一边哼着歌，一边写着代码，这就是我们大脑中最典型“多进程”场景。
再来举一个例子：你在Windows上，边听音乐，边浏览网页，还能回复微信消息。Windows之所以能同时运行多个应用程序，就是因为Windows内核支持多进程机制，这就是最典型的多进程场景了。
这节课配套代码，你可以点击这里下载。
为什么需要多进程调度我们先来搞清楚多进程调度的原因是什么，我来归纳一下。
第一，CPU同一时刻只能运行一个进程，而CPU个数总是比进程个数少，这就需要让多进程共用一个CPU，每个进程在这个CPU上运行一段时间。
第二点原因，当一个进程不能获取某种资源，导致它不能继续运行时，就应该让出CPU。当然你也可以把第一点中的CPU时间，也归纳为一种资源，这样就合并为一点：进程拿不到资源就要让出CPU。我来为你画幅图就明白了，如下所示。
上图中，有五个进程，其中浏览器进程和微信进程依赖于网络和键盘的数据资源，如果不能满足它们，就应该通过进程调度让出CPU。
而两个科学计算进程，则更多的依赖于CPU，但是如果它们中的一个用完了自己的CPU时间，也得借助进程调度让出CPU，不然它就会长期霸占CPU，导致其它进程无法运行。需要注意的是，每个进程都会依赖一种资源，那就是CPU时间，你可以把CPU时间理解为它就是CPU，一个进程必须要有CPU才能运行。
这里我们只需要明白，多个进程为什么要进行调度，就可以了。
管理进程下面我们一起来看看怎么管理进程，我们的Cosmos操作系统也支持多个进程，有了多个进程就要把它们管理起来。说白了，就是弄清楚这些进程有哪些状态，是如何组织起来的，又要从哪找到它们。
进程的生命周期人有生老病死，对于一个进程来说也是一样。一个进程从建立开始，接着运行，然后因为资源问题不得不暂停运行，最后退出系统。这一过程，我们称为进程的生命周期。在系统实现中，通常用进程的状态表示进程的生命周期。进程的状态我们用几个宏来定义，如下所示。
#define TDSTUS_RUN 0 //进程运行状态 #define TDSTUS_SLEEP 3 //进程睡眠状态 #define TDSTUS_WAIT 4 //进程等待状态 #define TDSTUS_NEW 5 //进程新建状态 #define TDSTUS_ZOMB 6 //进程僵死状态 可以发现，我们的进程有5个状态。其中进程僵死状态，表示进程将要退出系统不再进行调度。那么进程状态之间是如何转换的，别急，我来给画一幅图解释，如下所示。
上图中已经为你展示了，从建立进程到进程退出系统各状态之间的转换关系和需要满足的条件。
如何组织进程首先我们来研究如何组织进程。由于系统中会有许多个进程，在上节课中我们用thread_t结构表示一个进程，因此会有多个thread_t结构。而根据刚才我们对进程生命周期的解读，我们又知道了进程是随时可能建立或者退出的，所以系统中会随时分配或者删除thread_t结构。
要应对这样的情况，最简单的办法就是使用链表数据结构，而且我们的进程有优先级，所以我们可以设计成每个优先级对应一个链表头。
下面我们来把设计落地成数据结构，由于这是调度器模块，所以我们要建立几个文件krlsched.h、krlsched.c，在其中写上代码，如下所示。
typedef struct s_THRDLST { list_h_t tdl_lsth; //挂载进程的链表头 thread_t* tdl_curruntd; //该链表上正在运行的进程 uint_t tdl_nr; //该链表上进程个数 }thrdlst_t; typedef struct s_SCHDATA { spinlock_t sda_lock; //自旋锁 uint_t sda_cpuid; //当前CPU id uint_t sda_schdflgs; //标志 uint_t sda_premptidx; //进程抢占计数 uint_t sda_threadnr; //进程数 uint_t sda_prityidx; //当前优先级 thread_t* sda_cpuidle; //当前CPU的空转进程 thread_t* sda_currtd; //当前正在运行的进程 thrdlst_t sda_thdlst[PRITY_MAX]; //进程链表数组 }schdata_t; typedef struct s_SCHEDCALSS { spinlock_t scls_lock; //自旋锁 uint_t scls_cpunr; //CPU个数 uint_t scls_threadnr; //系统中所有的进程数 uint_t scls_threadid_inc; //分配进程id所用 schdata_t scls_schda[CPUCORE_MAX]; //每个CPU调度数据结构 }schedclass_t; 从上述代码中，我们发现schedclass_t是个全局数据结构，这个结构里包含一个schdata_t结构数组，数组大小根据CPU的数量决定。在每个schdata_t结构中，又包含一个进程优先级大小的thrdlst_t结构数组。我画幅图，你就明白了。这幅图能让你彻底理清以上数据结构之间的关系。</description></item><item><title>25_红黑树（上）：为什么工程中都用红黑树这种二叉树？</title><link>https://artisanbox.github.io/2/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/26/</guid><description>上两节，我们依次讲了树、二叉树、二叉查找树。二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是O(logn)。
不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于log2n的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到O(n)。我上一节说了，要解决这个复杂度退化的问题，我们需要设计一种平衡二叉查找树，也就是今天要讲的这种数据结构。
很多书籍里，但凡讲到平衡二叉查找树，就会拿红黑树作为例子。不仅如此，如果你有一定的开发经验，你会发现，在工程中，很多用到平衡二叉查找树的地方都会用红黑树。你有没有想过，为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？
带着这个问题，让我们一起来学习今天的内容吧！
什么是“平衡二叉查找树”？平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于1。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。
平衡二叉查找树不仅满足上面平衡二叉树的定义，还满足二叉查找树的特点。最先被发明的平衡二叉查找树是AVL树，它严格符合我刚讲到的平衡二叉查找树的定义，即任何节点的左右子树高度相差不超过1，是一种高度平衡的二叉查找树。
但是很多平衡二叉查找树其实并没有严格符合上面的定义（树中任意一个节点的左右子树的高度相差不能大于1），比如我们下面要讲的红黑树，它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。
我们学习数据结构和算法是为了应用到实际的开发中的，所以，我觉得没必去死抠定义。对于平衡二叉查找树这个概念，我觉得我们要从这个数据结构的由来，去理解“平衡”的意思。
发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。
所以，平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。
所以，如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比log2n大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。
如何定义一棵“红黑树”？平衡二叉查找树其实有很多，比如，Splay Tree（伸展树）、Treap（树堆）等，但是我们提到平衡二叉查找树，听到的基本都是红黑树。它的出镜率甚至要高于“平衡二叉查找树”这几个字，有时候，我们甚至默认平衡二叉查找树就是红黑树，那我们现在就来看看这个“明星树”。
红黑树的英文是“Red-Black Tree”，简称R-B Tree。它是一种不严格的平衡二叉查找树，我前面说了，它的定义是不严格符合平衡二叉查找树的定义的。那红黑树究竟是怎么定义的呢？
顾名思义，红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：
根节点是黑色的；
每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；
这里的第二点要求“叶子节点都是黑色的空节点”，稍微有些奇怪，它主要是为了简化红黑树的代码实现而设置的，下一节我们讲红黑树的实现的时候会讲到。这节我们暂时不考虑这一点，所以，在画图和讲解的时候，我将黑色的、空的叶子节点都省略掉了。
为了让你更好地理解上面的定义，我画了两个红黑树的图例，你可以对照着看下。
为什么说红黑树是“近似平衡”的？我们前面也讲到，平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化得太严重。
我们在上一节讲过，二叉查找树很多操作的性能都跟树的高度成正比。一棵极其平衡的二叉树（满二叉树或完全二叉树）的高度大约是log2n，所以如果要证明红黑树是近似平衡的，我们只需要分析，红黑树的高度是否比较稳定地趋近log2n就好了。
红黑树的高度不是很好分析，我带你一步一步来推导。
首先，我们来看，如果我们将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？
红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。
前面红黑树的定义里有这么一条：从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。
上一节我们说，完全二叉树的高度近似log2n，这里的四叉“黑树”的高度要低于完全二叉树，所以去掉红色节点的“黑树”的高度也不会超过log2n。
我们现在知道只包含黑色节点的“黑树”的高度，那我们现在把红色节点加回去，高度会变成多少呢？
从上面我画的红黑树的例子和定义看，在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过log2n，所以加入红色节点之后，最长路径不会超过2log2n，也就是说，红黑树的高度近似2log2n。
所以，红黑树的高度只比高度平衡的AVL树的高度（log2n）仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。
解答开篇我们刚刚提到了很多平衡二叉查找树，现在我们就来看下，为什么在工程中大家都喜欢用红黑树这种平衡二叉查找树？
我们前面提到Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。
AVL树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用AVL树的代价就有点高了。
红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比AVL树要低。
所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。
内容小结很多同学都觉得红黑树很难，的确，它算是最难掌握的一种数据结构。其实红黑树最难的地方是它的实现，我们今天还没有涉及，下一节我会专门来讲。
不过呢，我认为，我们其实不应该把学习的侧重点，放到它的实现上。那你可能要问了，关于红黑树，我们究竟需要掌握哪些东西呢？
还记得我多次说过的观点吗？我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。对于红黑树，也不例外。你如果能搞懂这几个问题，其实就已经足够了。
红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是O(logn)。
因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。
课后思考动态数据结构支持动态的数据插入、删除、查找操作，除了红黑树，我们前面还学习过哪些呢？能对比一下各自的优势、劣势，以及应用场景吗？
欢迎留言和我分享，我会第一时间给你反馈。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>25_表太大了，如何设计才能提高性能？</title><link>https://artisanbox.github.io/8/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/25/</guid><description>你好，我是朱晓峰。
随着数据量的不断增加，表会变得越来越大，查询的速度也会越来越慢。针对这种情况，该怎么处理呢？
咱们上节课学习的优化查询语句是一种方法，不过它并不足以解决所有问题。如果表的设计不合理，会导致数据记录占用不必要的存储空间。
MySQL在存取数据时，并不是一条条去处理的，而是会按照固定大小的页进行处理，如果数据记录占用了不必要的存储空间，就会导致一次读入的有效数据很少。那么，无论怎么改写语句，都无法提升这步操作的效率。这个时候，对表的设计进行优化，就是必不可少的了。
所以，今天，我就给你介绍一下怎么通过优化数据类型、合理增加冗余字段、拆分表和使用非空约束等方法，来改进表的设计，从而提高查询性能。
数据类型优化在改进表的设计时，首先可以考虑优化字段的数据类型。下面我就来讲解2种方法，一种是针对整数类型数据，尽量使用小的整数类型来定义；另外一种是，如果字段既可以用文本类型，也可以用整数类型，尽量使用整数类型。
先说第一种方法，对整数类型数据进行优化。
在第2讲中，我建议你，遇到整数类型的字段可以用INT型。这样做的理由是，INT型数据有足够大的取值范围，不用担心数据超出取值范围的问题。刚开始做项目的时候，首先要保证系统的稳定性，这样设计字段类型是可以的。
但是，随着你的经验越来越丰富，参与的项目越来越大，数据量也越来越多的时候，你就不能只从系统稳定性的角度来思考问题了，还要考虑到系统整体的效率。
这是因为，在数据量很大的时候，数据类型的定义，在很大程度上会影响到系统整体的执行效率。这个时候，你就必须同时考虑稳定性和效率。
第2种优化方法，就是既可以使用文本类型也可以使用整数类型的字段，要使用整数类型，而不要用文本类型。
跟文本类型数据相比，大整数往往占用更少的存储空间，因此，在存取和比对的时候，可以占用更少的内存。所以，遇到既可以使用文本类型，又可以使用整数类型来定义的字段，尽量使用整数类型，这样可以提高查询的效率。
接下来，我就结合超市项目的案例来讲解下具体的优化方法。
在这个项目中，我们有一个400万条记录的流水数据。为了方便你理解，这里我只保留2个字段，分别是商品编号字段itemnumber和流水唯一编号字段transuniqueid。流水唯一编号用于在系统中唯一标识一条流水。
为了对比方便，我创建了2个表demo.test和demo.test1：
在demo.test的表中，我给商品编号设定的数据类型是INT，给流水唯一编号设定的数据类型是TEXT； 在demo.test1中，我给商品编号设定的数据类型是MEDIUMINT，给流水唯一编号设定的数据类型是BIGINT。 这样设定的原因是，MEDIUMINT类型的取值范围是“无符号数0 – 16777215”。对于商品编号来说，其实够用了。我的400万条数据中没有超过这个范围的值。而流水唯一编号是一个长度为18位的数字，用字符串数据类型TEXT肯定是可以的，大整数类型BIGINT的取值范围是“无符号数0 – 18446744083709551616”，有20位，所以，用大整数类型数据来定义流水唯一编号，也是可以的。
创建表demo.test和demo.test1的语句如下所示：
mysql&amp;gt; CREATE TABLE demo.test (itemnumber INT,transuniqueid TEXT); Query OK, 0 rows affected (0.23 sec) mysql&amp;gt; CREATE TABLE demo.test1 (itemnumber MEDIUMINT,transuniqueid BIGINT); Query OK, 0 rows affected (0.25 sec) 然后，我们来对这两个表进行数据导入和查询操作，看看哪个效率更高：
mysql&amp;gt; LOAD DATA INFILE &amp;lsquo;C:\ProgramData\MySQL\MySQL Server 8.0\Uploads\trans.txt&amp;rsquo; INTO TABLE demo.test FIELDS TERMINATED BY &amp;lsquo;,&amp;rsquo; LINES TERMINATED BY &amp;lsquo;\n&amp;rsquo;; Query OK, 4328021 rows affected (3 min 23.</description></item><item><title>25｜增强编译器前端功能第4步：综合运用多种语义分析技术</title><link>https://artisanbox.github.io/3/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/27/</guid><description>你好，我是宫文学。
在上一节课，我们比较全面地分析了怎么用集合运算的算法思路实现类型计算。不过，在实际的语义分析过程中，我们往往需要综合运用多种技术。
不知道你还记不记得，我们上一节课举了一个例子，里面涉及了数据流分析和类型计算技术。不过这还不够，今天这节课，我们还要多举几个例子，来看看如何综合运用各种技术来达到语义分析的目的。在这个过程中，你还会加深对类型计算的理解、了解常量折叠和常量传播技术，以及实现更精准的类型推导。
好，我们首先接着上一节课的思路，看一看怎么把数据流分析与类型计算结合起来。
在类型计算中使用数据流分析技术我们再用一下上节课的示例程序foo7。在这个程序中，age的类型是number|null，age1的类型是string|number。我们先让age=18，这时候把age赋给age1是合法的。之后又给age赋值为null，然后再把age赋给age1，这时编译器就会报错。
function foo7(age : number|null){ let age1 : string|number; age = 18; //age的值域现在变成了一个值类型：18 age1 = age; //OK age = null; //age的值域现在变成了null age1 = age; //错误！ console.log(age1); } 在这个过程中，age的值域是动态变化的。在这里，我用了“值域”这个词。它其实跟类型是同一个意思。我这里用值域这个词，是强调动态变化的特征。毕竟，如果说到类型，你通常会觉得变量的类型是不变的。如果你愿意，也可以直接把它叫做类型。
你马上就会想到，数据流分析技术很擅长处理这种情况。具体来说，就是在扫描程序代码的过程中，某个值会不断地变化。
提到数据流分析，那自然我们就要先来识别它的5大关键要素了。我们来分析一下。
首先是分析方向。这个场景中，分析方向显然是自上而下的。
第二，是数据流分析针对的变量。在这个场景中，我们需要分析的是变量的值域。所以，我用了一个varRanges变量，来保存每个变量的值域。varRanges是一个map，每个变量在里面有一个key。
varRanges:Map&amp;lt;VarSymbol, Type&amp;gt; = new Map(); 第三，我们要确定varRanges的初始值。在这个例子中，每个变量的值域的初始值就是它原来的类型。比如age一开始的值域就是number|null。
第四，我们要确定转换函数，也就是在什么情况下，变量的值域会发生变化。在当前的例子中，我们只需要搞清楚变量赋值的情况就可以了。如果我们要在变量声明中进行初始化，那也可以看做是变量赋值。
在变量赋值时，如果=号右边的值是一个常量，那么变量的值域都会变成一个值对象，这种情况我们已经在前一节课分析过了。
那如果=号右边的值不是常量，而是另一个变量呢？比如下面一个例子foo10，x的类型是number|string，y的类型是string。然后把y赋给x。我相信你也看出来，现在x的值域就应该跟y的一样了，都是string。
function foo10(x : number|string, y : string){ x = y; //x的值域变成了string if (typeof x == 'string'){ //其实这个条件一定为true println("x is string"); } } 研究一下这个例子，你会发现通过赋值操作，我们把x的值域收窄了。在TypeScript的文档中，这被叫做"Narrowing"。翻译成汉语的话，我们姑且称之为“窄化”吧。
不过，除了赋值语句，还有其他情况可以让变量的值域窄化，包括使用typeof运算符、真值判断、等值判断、instanceof运算符，以及使用类型断言等等。其中最后两种方法，涉及到对象，我们目前还没有支持对象特性，所以先不讨论了。我们就讨论一下typeof运算符、真值判断和等值判断这三种情况。</description></item><item><title>26_MySQL编译器（二）：编译技术如何帮你提升数据库性能？</title><link>https://artisanbox.github.io/7/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/26/</guid><description>你好，我是宫文学。今天这一讲，我们继续来探究MySQL编译器。
通过上一讲的学习，你已经了解了MySQL编译器是怎么做词法和语法分析的了。那么在做完语法分析以后，MySQL编译器又继续做了哪些处理，才能成功地执行这个SQL语句呢？
所以今天，我就带你来探索一下MySQL的实现机制，我会把重点放在SQL的语义分析和优化机制上。当你学完以后，你就能真正理解以下这些问题了：
高级语言的编译器具有语义分析功能，那么MySQL编译器也会做语义分析吗？它有没有引用消解问题？有没有作用域？有没有类型检查？ MySQL有没有类似高级语言的那种优化功能呢？ 好，让我们开始今天的探究吧。不过，在讨论MySQL的编译过程之前，我想先带你了解一下MySQL会用到的一些重要的数据结构，因为你在解读代码的过程中经常会见到它们。
认识MySQL编译器的一些重要的数据结构第一组数据结构，是下图中的几个重要的类或结构体，包括线程、保存编译上下文信息的LEX，以及保存编译结果SELECT_LEX_UNIT和SELECT_LEX。
图1：MySQL编译器的中的几个重要的类和结构体首先是THD，也就是线程对象。对于每一个客户端的连接，MySQL编译器都会启动一个线程来处理它的查询请求。
THD中的一个重要数据成员是LEX对象。你可以把LEX对象想象成是编译SQL语句的工作区，保存了SQL语句编译过程中的上下文信息，编译器会把编译的成果放在这里，而编译过程中所需要的信息也是从这里查找。
在把SQL语句解析完毕以后，编译器会形成一些结构化的对象来表示一个查询。其中SELECT_LEX_UNIT结构体，就代表了一个查询表达式（Query Expression）。一个查询表达式可能包含了多个查询块，比如使用UNION的情况。
而SELECT_LEX则代表一个基本的查询块（Query Block），它里面的信息包括了所有的列和表达式、查询用到的表、where条件等。在SELECT_LEX中会保存查询块中涉及的表、字段和表达式等，它们也都有对应的数据结构。
第二组需要了解的数据结构，是表示表、字段等信息的对象。Table_ident对象保存了表的信息，包括数据库名、表名和所在的查询语句（SELECT_LEX_UNIT对象）。
图2：Table_indent对象，代表一个表而字段和表达式等表示一个值的对象，用Item及其子类来表示。SQL语句中的每个字段、每个计算字段，最后都对应一个Item。where条件，其实也是用一个Item就能表示。具体包括：
字段（Item_field）。 各种常数，包括数字、字符和null等（Item_basic_constant）。 能够产生出值的运算（Item_result_field），包括算术表达式（Item_num_op）、存储过程（Item_func_sp）、子查询（Item_subselect）等。 在语法分析过程中产生的Item（Parse_tree_item）。它们是一些占位符，因为在语法分析阶段，不容易一下子创建出真正的Item，这些Parse_tree_item需要在上下文分析阶段，被替换成真正的Item。 图3：Item及其子类好了，上面这些就是MySQL会用到的最核心的一些数据结构了。接下来的编译工作，就会生成和处理上述的数据结构。
上下文分析我们先来看一下MySQL编译器的上下文分析工作。
你已经知道，语法分析仅仅完成的是上下文无关的分析，还有很多的工作，需要基于上下文来做处理。这些工作，就属于语义分析。
MySQL编译器中，每个AST节点，都会有一个contextualize()方法。从这个方法的名称来看，你就能知道它是做上下文处理的（contextualize，置于上下文中）。
对一个Select语句来说，编译器会调用其根节点PT_select_stmt的contextualize()方法，从而深度遍历整个AST，并调用每个节点的contextualize()方法。
那么，MySQL编译器的上下文处理，都完成了什么工作呢？
首先，是检查数据库名、表名和字段名是否符合格式要求（在table.cc中实现）。
比如，MySQL会规定表名、字段名等名称不能超过64个字符，字段名不能包含ASCII值为255的字符，等等。这些规则在词法分析阶段是不检查的，要留在语义分析阶段检查。
然后，创建并填充SELECT_LEX_UNIT和SELECT_LEX对象。
前面我提到了，SELECT_LEX_UNIT和SELECT_LEX中，保存了查询表达式和查询块所需的所有信息，依据这些信息，MySQL就可以执行实际的数据库查询操作。
那么，在contextualize的过程中，编译器就会生成上述对象，并填充它们的成员信息。
比如，对于查询中用到的表，在语法分析阶段就会生成Table_ident对象。但其中的数据库名称可能是缺失的，那么在上下文的分析处理当中，就会被编译器设置成当前连接所采用的默认数据库。这个信息可以从线程对象（THD）中获得，因为每个线程对应了一个数据库连接，而每个数据库连接是针对一个具体的数据库的。
好了，经过上下文分析的编译阶段以后，我们就拥有了可以执行查询的SELECT_LEX_UNIT和SELECT_LEX对象。可是，你可能会注意到一个问题：为什么在语义分析阶段，MySQL没有做引用的消解呢？不要着急，接下来我就给你揭晓这个答案。
MySQL是如何做引用消解的？我们在SQL语句中，会用到数据库名、表名、列名、表的别名、列的别名等信息，编译器肯定也需要检查它们是不是正确的。这就是引用消解（或名称消解）的过程。一般编译器是在语义分析阶段来做这项工作的，而MySQL是在执行SQL命令的时候才做引用消解。
引用消解的入口是在SQL命令的的prepare()方法中，它会去检查表名、列名都对不对。
通过GDB调试工具，我们可以跟踪编译器做引用消解的过程。你可以在my_message_sql()函数处设个断点，然后写个SQL语句，故意使用错误的表名或者列名，来看看MySQL是在什么地方检查出这些错误的。
比如说，你可以执行“select * from fake_table”，其中的fake_table这个表，在数据库中其实并不存在。
下面是打印出的调用栈。你会注意到，MySQL在准备执行SQL语句的过程中，会试图去打开fake_table表，这个时候编译器就会发现这个表不存在。
你还可以再试一下“select fake_column from departments”这个语句，也一样会查出，fake_column并不是departments表中的一列。
那么，MySQL是如何知道哪些表和字段合法，哪些不合法的呢？
原来，它是通过查表的定义，也就是数据库模式信息，或者可以称为数据字典、元数据。MySQL在一个专门的库中，保存了所有的模式信息，包括库、表、字段、存储过程等定义。
你可以跟高级语言做一下类比。高级语言，比如说Java也会定义一些类型，类型中包含了成员变量。那么，MySQL中的表，就相当于高级语言的类型；而表的字段（或列）就相当于高级语言的类型中的成员变量。所以，在这个方面，MySQL和高级语言做引用消解的思路其实是一样的。
但是，高级语言在做引用消解的时候有作用域的概念，那么MySQL有没有类似的概念呢？
有的。举个例子，假设一个SQL语句带了子查询，那么子查询中既可以引用本查询块中的表和字段，也可以引用父查询中的表和字段。这个时候就存在了两个作用域，比如下面这个查询语句：
select dept_name from departments where dept_no in (select dept_no from dept_emp where dept_name != 'Sales' #引用了上一级作用域中的字段 group by dept_no having count(*)&amp;gt; 20000) 其中的dept_name字段是dept_emp表中所没有的，它其实是上一级作用域中departments表中的字段。</description></item><item><title>26_Superscalar和VLIW：如何让CPU的吞吐率超过1？</title><link>https://artisanbox.github.io/4/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/26/</guid><description>到今天为止，专栏已经过半了。过去的20多讲里，我给你讲的内容，很多都是围绕着怎么提升CPU的性能这个问题展开的。
我们先回顾一下第4讲，不知道你是否还记得这个公式：
程序的CPU执行时间 = 指令数 × CPI × Clock Cycle Time这个公式里，有一个叫CPI的指标。我们知道，CPI的倒数，又叫作IPC（Instruction Per Clock），也就是一个时钟周期里面能够执行的指令数，代表了CPU的吞吐率。那么，这个指标，放在我们前面几节反复优化流水线架构的CPU里，能达到多少呢？
答案是，最佳情况下，IPC也只能到1。因为无论做了哪些流水线层面的优化，即使做到了指令执行层面的乱序执行，CPU仍然只能在一个时钟周期里面，取一条指令。
这说明，无论指令后续能优化得多好，一个时钟周期也只能执行完这样一条指令，CPI只能是1。但是，我们现在用的Intel CPU或者ARM的CPU，一般的CPI都能做到2以上，这是怎么做到的呢？
今天，我们就一起来看看，现代CPU都使用了什么“黑科技”。
多发射与超标量：同一时间执行的两条指令之前讲CPU的硬件组成的时候，我们把所有算术和逻辑运算都抽象出来，变成了一个ALU这样的“黑盒子”。你应该还记得第13讲到第16讲，关于加法器、乘法器、乃至浮点数计算的部分，其实整数的计算和浮点数的计算过程差异还是不小的。实际上，整数和浮点数计算的电路，在CPU层面也是分开的。
一直到80386，我们的CPU都是没有专门的浮点数计算的电路的。当时的浮点数计算，都是用软件进行模拟的。所以，在80386时代，Intel给386配了单独的387芯片，专门用来做浮点数运算。那个时候，你买386芯片的话，会有386sx和386dx这两种芯片可以选择。386dx就是带了387浮点数计算芯片的，而sx就是不带浮点数计算芯片的。
其实，我们现在用的Intel CPU芯片也是一样的。虽然浮点数计算已经变成CPU里的一部分，但并不是所有计算功能都在一个ALU里面，真实的情况是，我们会有多个ALU。这也是为什么，在第24讲讲乱序执行的时候，你会看到，其实指令的执行阶段，是由很多个功能单元（FU）并行（Parallel）进行的。
不过，在指令乱序执行的过程中，我们的取指令（IF）和指令译码（ID）部分并不是并行进行的。
既然指令的执行层面可以并行进行，为什么取指令和指令译码不行呢？如果想要实现并行，该怎么办呢？
其实只要我们把取指令和指令译码，也一样通过增加硬件的方式，并行进行就好了。我们可以一次性从内存里面取出多条指令，然后分发给多个并行的指令译码器，进行译码，然后对应交给不同的功能单元去处理。这样，我们在一个时钟周期里，能够完成的指令就不只一条了。IPC也就能做到大于1了。
这种CPU设计，我们叫作多发射（Mulitple Issue）和超标量（Superscalar）。
什么叫多发射呢？这个词听起来很抽象，其实它意思就是说，我们同一个时间，可能会同时把多条指令发射（Issue）到不同的译码器或者后续处理的流水线中去。
在超标量的CPU里面，有很多条并行的流水线，而不是只有一条流水线。“超标量“这个词是说，本来我们在一个时钟周期里面，只能执行一个标量（Scalar）的运算。在多发射的情况下，我们就能够超越这个限制，同时进行多次计算。
你可以看我画的这个超标量设计的流水线示意图。仔细看，你应该能看到一个有意思的现象，每一个功能单元的流水线的长度是不同的。事实上，不同的功能单元的流水线长度本来就不一样。我们平时所说的14级流水线，指的通常是进行整数计算指令的流水线长度。如果是浮点数运算，实际的流水线长度则会更长一些。
Intel的失败之作：安腾的超长指令字设计无论是之前几讲里讲的乱序执行，还是现在更进一步的超标量技术，在实际的硬件层面，其实实施起来都挺麻烦的。这是因为，在乱序执行和超标量的体系里面，我们的CPU要解决依赖冲突的问题。这也就是前面几讲我们讲的冒险问题。
CPU需要在指令执行之前，去判断指令之间是否有依赖关系。如果有对应的依赖关系，指令就不能分发到执行阶段。因为这样，上面我们所说的超标量CPU的多发射功能，又被称为动态多发射处理器。这些对于依赖关系的检测，都会使得我们的CPU电路变得更加复杂。
于是，计算机科学家和工程师们就又有了一个大胆的想法。我们能不能不把分析和解决依赖关系的事情，放在硬件里面，而是放到软件里面来干呢？
如果你还记得的话，我在第4讲也讲过，要想优化CPU的执行时间，关键就是拆解这个公式：
程序的CPU执行时间 = 指令数 × CPI × Clock Cycle Time当时我们说过，这个公式里面，我们可以通过改进编译器来优化指令数这个指标。那接下来，我们就来看看一个非常大胆的CPU设计想法，叫作超长指令字设计（Very Long Instruction Word，VLIW）。这个设计呢，不仅想让编译器来优化指令数，还想直接通过编译器，来优化CPI。
围绕着这个设计的，是Intel一个著名的“史诗级”失败，也就是著名的IA-64架构的安腾（Itanium）处理器。只不过，这一次，责任不全在Intel，还要拉上可以称之为硅谷起源的另一家公司，也就是惠普。
之所以称为“史诗”级失败，这个说法来源于惠普最早给这个架构取的名字，显式并发指令运算（Explicitly Parallel Instruction Computer），这个名字的缩写EPIC，正好是“史诗”的意思。
好巧不巧，安腾处理器和和我之前给你介绍过的Pentium 4一样，在市场上是一个失败的产品。在经历了12年之久的设计研发之后，安腾一代只卖出了几千套。而安腾二代，在从2002年开始反复挣扎了16年之后，最终在2018年被Intel宣告放弃，退出了市场。自此，世上再也没有这个“史诗”服务器了。
那么，我们就来看看，这个超长指令字的安腾处理器是怎么回事儿。
在乱序执行和超标量的CPU架构里，指令的前后依赖关系，是由CPU内部的硬件电路来检测的。而到了超长指令字的架构里面，这个工作交给了编译器这个软件。
我从专栏第5讲开始，就给你看了不少C代码到汇编代码和机器代码的对照。编译器在这个过程中，其实也能够知道前后数据的依赖。于是，我们可以让编译器把没有依赖关系的代码位置进行交换。然后，再把多条连续的指令打包成一个指令包。安腾的CPU就是把3条指令变成一个指令包。
CPU在运行的时候，不再是取一条指令，而是取出一个指令包。然后，译码解析整个指令包，解析出3条指令直接并行运行。可以看到，使用超长指令字架构的CPU，同样是采用流水线架构的。也就是说，一组（Group）指令，仍然要经历多个时钟周期。同样的，下一组指令并不是等上一组指令执行完成之后再执行，而是在上一组指令的指令译码阶段，就开始取指令了。
值得注意的一点是，流水线停顿这件事情在超长指令字里面，很多时候也是由编译器来做的。除了停下整个处理器流水线，超长指令字的CPU不能在某个时钟周期停顿一下，等待前面依赖的操作执行完成。编译器需要在适当的位置插入NOP操作，直接在编译出来的机器码里面，就把流水线停顿这个事情在软件层面就安排妥当。
虽然安腾的设想很美好，Intel也曾经希望能够让安腾架构成为替代x86的新一代架构，但是最终安腾还是在前前后后折腾将近30年后失败了。2018年，Intel宣告安腾9500会在2021年停止供货。
安腾失败的原因有很多，其中有一个重要的原因就是“向前兼容”。
一方面，安腾处理器的指令集和x86是不同的。这就意味着，原来x86上的所有程序是没有办法在安腾上运行的，而需要通过编译器重新编译才行。
另一方面，安腾处理器的VLIW架构决定了，如果安腾需要提升并行度，就需要增加一个指令包里包含的指令数量，比方说从3个变成6个。一旦这么做了，虽然同样是VLIW架构，同样指令集的安腾CPU，程序也需要重新编译。因为原来编译器判断的依赖关系是在3个指令以及由3个指令组成的指令包之间，现在要变成6个指令和6个指令组成的指令包。编译器需要重新编译，交换指令顺序以及NOP操作，才能满足条件。甚至，我们需要重新来写编译器，才能让程序在新的CPU上跑起来。
于是，安腾就变成了一个既不容易向前兼容，又不容易向后兼容的CPU。那么，它的失败也就不足为奇了。
可以看到，技术思路上的先进想法，在实际的业界应用上会遇到更多具体的实践考验。无论是指令集向前兼容性，还是对应CPU未来的扩展，在设计的时候，都需要更多地去考虑实践因素。
总结延伸这一讲里，我和你一起向CPU的性能发起了一个新的挑战：让CPU的吞吐率，也就是IPC能够超过1。
我先是为你介绍了超标量，也就是Superscalar这个方法。超标量可以让CPU不仅在指令执行阶段是并行的，在取指令和指令译码的时候，也是并行的。通过超标量技术，可以使得你所使用的CPU的IPC超过1。
在Intel的x86的CPU里，从Pentium时代，第一次开始引入超标量技术，整个CPU的性能上了一个台阶。对应的技术，一直沿用到了现在。超标量技术和你之前看到的其他流水线技术一样，依赖于在硬件层面，能够检测到对应的指令的先后依赖关系，解决“冒险”问题。所以，它也使得CPU的电路变得更复杂了。
因为这些复杂性，惠普和Intel又共同推出了著名的安腾处理器。通过在编译器层面，直接分析出指令的前后依赖关系。于是，硬件在代码编译之后，就可以直接拿到调换好先后顺序的指令。并且这些指令中，可以并行执行的部分，会打包在一起组成一个指令包。安腾处理器在取指令和指令译码的时候，拿到的不再是单个指令，而是这样一个指令包。并且在指令执行阶段，可以并行执行指令包里所有的指令。
虽然看起来，VLIW在技术层面更具有颠覆性，不仅仅只是一个硬件层面的改造，而且利用了软件层面的编译器，来组合解决提升CPU指令吞吐率的问题。然而，最终VLIW却没有得到市场和业界的认可。
惠普和Intel强强联合开发的安腾处理器命运多舛。从1989开始研发，直到2001年才发布了第一代安腾处理器。然而12年的开发过程后，第一代安腾处理器最终只卖出了几千套。而2002年发布的安腾2处理器，也没能拯救自己的命运。最终在2018年，Intel宣布安腾退出市场。自此之后，市面上再没有能够大规模商用的VLIW架构的处理器了。</description></item><item><title>26_备库为什么会延迟好几个小时？</title><link>https://artisanbox.github.io/1/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/26/</guid><description>在上一篇文章中，我和你介绍了几种可能导致备库延迟的原因。你会发现，这些场景里，不论是偶发性的查询压力，还是备份，对备库延迟的影响一般是分钟级的，而且在备库恢复正常以后都能够追上来。
但是，如果备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成了小时级别。而且对于一个压力持续比较高的主库来说，备库很可能永远都追不上主库的节奏。
这就涉及到今天我要给你介绍的话题：备库并行复制能力。
为了便于你理解，我们再一起看一下第24篇文章《MySQL是怎么保证主备一致的？》的主备流程图。
图1 主备流程图谈到主备的并行复制能力，我们要关注的是图中黑色的两个箭头。一个箭头代表了客户端写入主库，另一箭头代表的是备库上sql_thread执行中转日志（relay log）。如果用箭头的粗细来代表并行度的话，那么真实情况就如图1所示，第一个箭头要明显粗于第二个箭头。
在主库上，影响并发度的原因就是各种锁了。由于InnoDB引擎支持行锁，除了所有并发事务都在更新同一行（热点行）这种极端场景外，它对业务并发度的支持还是很友好的。所以，你在性能测试的时候会发现，并发压测线程32就比单线程时，总体吞吐量高。
而日志在备库上的执行，就是图中备库上sql_thread更新数据(DATA)的逻辑。如果是用单线程的话，就会导致备库应用日志不够快，造成主备延迟。
在官方的5.6版本之前，MySQL只支持单线程复制，由此在主库并发高、TPS高时就会出现严重的主备延迟问题。
从单线程复制到最新版本的多线程复制，中间的演化经历了好几个版本。接下来，我就跟你说说MySQL多线程复制的演进过程。
其实说到底，所有的多线程复制机制，都是要把图1中只有一个线程的sql_thread，拆成多个线程，也就是都符合下面的这个模型：
图2 多线程模型图2中，coordinator就是原来的sql_thread, 不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了worker线程。而work线程的个数，就是由参数slave_parallel_workers决定的。根据我的经验，把这个值设置为8~16之间最好（32核物理机的情况），毕竟备库还有可能要提供读查询，不能把CPU都吃光了。
接下来，你需要先思考一个问题：事务能不能按照轮询的方式分发给各个worker，也就是第一个事务分给worker_1，第二个事务发给worker_2呢？
其实是不行的。因为，事务被分发给worker以后，不同的worker就独立执行了。但是，由于CPU的调度策略，很可能第二个事务最终比第一个事务先执行。而如果这时候刚好这两个事务更新的是同一行，也就意味着，同一行上的两个事务，在主库和备库上的执行顺序相反，会导致主备不一致的问题。
接下来，请你再设想一下另外一个问题：同一个事务的多个更新语句，能不能分给不同的worker来执行呢？
答案是，也不行。举个例子，一个事务更新了表t1和表t2中的各一行，如果这两条更新语句被分到不同worker的话，虽然最终的结果是主备一致的，但如果表t1执行完成的瞬间，备库上有一个查询，就会看到这个事务“更新了一半的结果”，破坏了事务逻辑的隔离性。
所以，coordinator在分发的时候，需要满足以下这两个基本要求：
不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个worker中。
同一个事务不能被拆开，必须放到同一个worker中。
各个版本的多线程复制，都遵循了这两条基本原则。接下来，我们就看看各个版本的并行复制策略。
MySQL 5.5版本的并行复制策略官方MySQL 5.5版本是不支持并行复制的。但是，在2012年的时候，我自己服务的业务出现了严重的主备延迟，原因就是备库只有单线程复制。然后，我就先后写了两个版本的并行策略。
这里，我给你介绍一下这两个版本的并行策略，即按表分发策略和按行分发策略，以帮助你理解MySQL官方版本并行复制策略的迭代。
按表分发策略按表分发事务的基本思路是，如果两个事务更新不同的表，它们就可以并行。因为数据是存储在表里的，所以按表分发，可以保证两个worker不会更新同一行。
当然，如果有跨表的事务，还是要把两张表放在一起考虑的。如图3所示，就是按表分发的规则。
图3 按表并行复制程模型可以看到，每个worker线程对应一个hash表，用于保存当前正在这个worker的“执行队列”里的事务所涉及的表。hash表的key是“库名.表名”，value是一个数字，表示队列中有多少个事务修改这个表。
在有事务分配给worker时，事务里面涉及的表会被加到对应的hash表中。worker执行完成后，这个表会被从hash表中去掉。
图3中，hash_table_1表示，现在worker_1的“待执行事务队列”里，有4个事务涉及到db1.t1表，有1个事务涉及到db2.t2表；hash_table_2表示，现在worker_2中有一个事务会更新到表t3的数据。
假设在图中的情况下，coordinator从中转日志中读入一个新事务T，这个事务修改的行涉及到表t1和t3。
现在我们用事务T的分配流程，来看一下分配规则。
由于事务T中涉及修改表t1，而worker_1队列中有事务在修改表t1，事务T和队列中的某个事务要修改同一个表的数据，这种情况我们说事务T和worker_1是冲突的。
按照这个逻辑，顺序判断事务T和每个worker队列的冲突关系，会发现事务T跟worker_2也冲突。
事务T跟多于一个worker冲突，coordinator线程就进入等待。
每个worker继续执行，同时修改hash_table。假设hash_table_2里面涉及到修改表t3的事务先执行完成，就会从hash_table_2中把db1.t3这一项去掉。
这样coordinator会发现跟事务T冲突的worker只有worker_1了，因此就把它分配给worker_1。
coordinator继续读下一个中转日志，继续分配事务。
也就是说，每个事务在分发的时候，跟所有worker的冲突关系包括以下三种情况：
如果跟所有worker都不冲突，coordinator线程就会把这个事务分配给最空闲的woker;
如果跟多于一个worker冲突，coordinator线程就进入等待状态，直到和这个事务存在冲突关系的worker只剩下1个；
如果只跟一个worker冲突，coordinator线程就会把这个事务分配给这个存在冲突关系的worker。</description></item><item><title>26_多个活动要安排（下）：如何实现进程的等待与唤醒机制？</title><link>https://artisanbox.github.io/9/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/26/</guid><description>你好，我是LMOS。
上节课，我带你一起设计了我们Cosmos的进程调度器，但有了进程调度器还不够，因为调度器它始终只是让一个进程让出CPU，切换到它选择的下一个进程上去运行。
结合前面我们对进程生命周期的讲解，估计你已经反应过来了。没错，多进程调度方面，我们还要实现进程的等待与唤醒机制，今天我们就来搞定它。
这节课的配套代码，你可以从这里下载。
进程的等待与唤醒我们已经知道，进程得不到所需的某个资源时就会进入等待状态，直到这种资源可用时，才会被唤醒。那么进程的等待与唤醒机制到底应该这样设计呢，请听我慢慢为你梳理。
进程等待结构很显然，在实现进程的等待与唤醒的机制之前，我们需要设计一种数据结构，用于挂载等待的进程，在唤醒的时候才可以找到那些等待的进程 ，这段代码如下所示。
typedef struct s_KWLST { spinlock_t wl_lock; //自旋锁 uint_t wl_tdnr; //等待进程的个数 list_h_t wl_list; //挂载等待进程的链表头 }kwlst_t; 其实，这个结构在前面讲信号量的时候，我们已经见过了。这是因为它经常被包含在信号量等上层数据结构中，而信号量结构，通常用于保护访问受限的共享资源。这个结构非常简单，我们不用多说。
进程等待现在我们来实现让进程进入等待状态的机制，它也是一个函数。这个函数会设置进程状态为等待状态，让进程从调度系统数据结构中脱离，最后让进程加入到kwlst_t等待结构中，代码如下所示。
void krlsched_wait(kwlst_t *wlst) { cpuflg_t cufg, tcufg; uint_t cpuid = hal_retn_cpuid(); schdata_t *schdap = &amp;amp;osschedcls.scls_schda[cpuid]; //获取当前正在运行的进程 thread_t *tdp = krlsched_retn_currthread(); uint_t pity = tdp-&amp;gt;td_priority; krlspinlock_cli(&amp;amp;schdap-&amp;gt;sda_lock, &amp;amp;cufg); krlspinlock_cli(&amp;amp;tdp-&amp;gt;td_lock, &amp;amp;tcufg); tdp-&amp;gt;td_stus = TDSTUS_WAIT;//设置进程状态为等待状态 list_del(&amp;amp;tdp-&amp;gt;td_list);//脱链 krlspinunlock_sti(&amp;amp;tdp-&amp;gt;td_lock, &amp;amp;tcufg); if (schdap-&amp;gt;sda_thdlst[pity].tdl_curruntd == tdp) { schdap-&amp;gt;sda_thdlst[pity].tdl_curruntd = NULL; } schdap-&amp;gt;sda_thdlst[pity].tdl_nr--; krlspinunlock_sti(&amp;amp;schdap-&amp;gt;sda_lock, &amp;amp;cufg); krlwlst_add_thread(wlst, tdp);//将进程加入等待结构中 return; } 上述代码也不难，你结合注释就能理解。有一点需要注意，这个函数使进程进入等待状态，而这个进程是当前正在运行的进程，而当前正在运行的进程正是调用这个函数的进程，所以一个进程想要进入等待状态，只要调用这个函数就好了。</description></item><item><title>26_如何充分利用系统资源？</title><link>https://artisanbox.github.io/8/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/26/</guid><description>你好，我是朱晓峰。
内存和CPU都是有限的资源，因此，把它们的作用发挥到极致，对提高应用的承载能力来说至关重要。
磁盘读写需要计算位置、发出读写指令等，这些都要消耗CPU资源，很容易成为提升系统效能的瓶颈。
如果采取“先把数据放在内存，然后集中写入磁盘”的办法，可以节省CPU资源和磁盘读取的时间，但是也会面临系统故障时会丢失数据的风险；相反，如果每次都写入磁盘，数据最安全，但是频繁的磁盘读写，会导致系统效率低下。这就需要我们提升优化资源配置的能力。
今天，我就给你介绍一下优化系统配置的方法，同时还会讲解系统自带的监控工具，从而帮助你合理配置系统资源，精准发现系统资源的瓶颈，进一步提升你处理大并发、大数据的能力。
优化系统资源配置对CPU资源的掌控，关系到系统整体的成败。因为CPU资源是系统最核心的资源，无可替代，而且获取成本高。如果应用无法控制CPU的使用率，就有可能是失败的，不管你的界面多么人性化，功能多么强大。
因此，我们需要管理好系统配置，把资源效率提升到极致。系统参数控制着资源的配置，调整系统参数的值，可以帮助我们提升资源的利用效率。
我来借助一个小例子，给你介绍下怎么通过对系统变量进行调优，来提升系统的整体效率。
我曾参与过一个点餐系统应用的开发，其实就是一个为客户提供点餐服务的应用，类似于美团。商家购买服务，入住平台，开通之后，商家可以在系统中录入自己能够提供的各类餐食品种，客户通过手机App、微信小程序等点餐，商家接到订单以后进行制作，并根据客户需求提供堂食或者送餐服务。
系统中包括订单表（orderlist）、客户信息表（clientlist）和客户储值表（clientdeposit）。
订单表：
客户信息表：
客户储值表：
刚刚上线的时候，系统运行状态良好。但是，随着入住的商家不断增多，使用系统的用户量越来越多，每天的订单数据达到了2万条以上。这个时候，系统开始出现问题，CPU使用率不断飙升。终于，有一天午餐高峰的时候，CPU使用率达到99%，这实际上就意味着，系统的计算资源已经耗尽，再也无法处理任何新的订单了。换句话说，系统已经崩溃了。
这个时候，我们想到了对系统参数进行调整，因为参数的值决定了资源配置的方式和投放的程度。为了解决这个问题，我们一共调整了3个系统参数，分别是InnoDB_flush_log_at_trx_commit、InnoDB_buffer_pool_size、InnoDB_buffer_pool_instances。
接下来，我就给你讲一讲怎么对这三个参数进行调整。
1. 调整系统参数InnoDB_flush_log_at_trx_commit这个参数适用于InnoDB存储引擎。因为刚刚的表用的存储引擎都是InnoDB，因此，这个参数对我们系统的效能就有影响。需要注意的是，如果你用的存储引擎不是InnoDB，调整这个参数对系统性能的提升就没有什么用了。
这个参数存储在MySQL的配置文件my.ini里面，默认的值是1，意思是每次提交事务的时候，都把数据写入日志，并把日志写入磁盘。这样做的好处是数据安全性最佳，不足之处在于每次提交事务，都要进行磁盘写入的操作。在大并发的场景下，过于频繁的磁盘读写会导致CPU资源浪费，系统效率变低。
这个参数的值还有2个可能的选项，分别是0和2。其中，0表示每隔1秒将数据写入日志，并将日志写入磁盘；2表示，每次提交事务的时候都将数据写入日志，但是日志每间隔1秒写入磁盘。
最后，我们把这个参数的值改成了2。这样一来，就不用每次提交事务的时候都启动磁盘读写了，在大并发的场景下，可以改善系统效率，降低CPU使用率。即便出现故障，损失的数据也比较小。0虽然效率更高一些，但是数据安全性方面不如2。
2. 调整系统参数InnoDB_buffer_pool_size这个参数的意思是，InnoDB存储引擎使用缓存来存储索引和数据。这个值越大，可以加载到缓存区的索引和数据量就越多，需要的磁盘读写就越少。
因为我们的MySQL服务器是数据库专属服务器，只用来运行MySQL数据库服务，没有其他应用了，而我们的计算机是64位机，内存也有128G。于是我们把这个参数的值调整为64G。这样一来，磁盘读写次数可以大幅降低，我们就可以充分利用内存，释放出一些CPU的资源。
3. 调整系统参数InnoDB_buffer_pool_instances这个参数的意思是，将InnoDB的缓存区分成几个部分，这样一来，就可以提高系统的并行处理能力，因为可以允许多个进程同时处理不同部分的缓存区。这就好比买电影票，如果大家都挤在一个窗口、一个接一个地进行交易，效率肯定是很慢的。如果一次开很多售票窗口，多笔交易同时进行，那速度就快得多了。
我们把InnoDB_buffer_pool_instances的值修改为64，意思就是把InnoDB的缓存区分成64个分区，这样就可以同时有多个进程进行数据操作，CPU的效率就高多了。
修改好了系统参数的值，我们需要重新保存MySQL的配置文件my.ini，并且重启MySQL数据库服务器。
这里有个坑你要注意：由于my.ini文件是文本格式文件，你完全可以用记事本对文件进行修改操作。但是，如果你只是简单地进行保存，就会发现，MySQL服务器停止之后，再次启动时没有响应，服务器起不来了。其实，这就是文件的码制出了问题。
记事本保存文件默认的码制是UTF-8，但配置文件的码制必须是ANSI才行。所以，当你修改完MySQL的配置文件my.ini之后，保存的时候，记得用ANSI的格式。如下图所示：
经过我们对系统参数的调整，重启MySQL服务器之后，系统效率提高了，CPU资源的使用率下来了，系统得以正常运行。
咱们来小结下。CPU资源是系统的核心资源，获取成本非常高。CPU的特点就是阻塞，只要CPU一开始计算，就意味着等待。遇到CPU资源不足的问题，可以从2个思路去解决：
疏通拥堵路段，消除瓶颈，让等待的时间更短； 开拓新的通道，增加并行处理能力。 刚刚的调优思路，其实就是围绕着这2个点展开的。如果遇到CPU资源不足的问题，我建议你也从这2个角度出发去思考解决办法。
如何利用系统资源来诊断问题？在刚刚的例子中，我提到了解决CPU资源不足需要消除瓶颈。而消除瓶颈的第一步就是要发现它。如何发现呢？幸运的是，MySQL提供了很好的工具：Performance Schema。
这是一种专门用来监控服务器执行情况的存储引擎，它会把监控服务器执行情况的数据记录在系统自带的数据库performance_schema中。我们可以利用监控的数据，对服务器中执行查询的问题进行诊断。
我还是以刚刚的那个点餐系统为例，来解释一下。
当我们调整完系统参数之后，系统恢复了运行。可是随着数据量的不断增大，单日订单量超过20万，我们再次遇到了问题：CPU飙升到99%，系统无法工作了。这个时候，我们就可以利用performance_schema记录的监控数据来发现问题。
我先讲一讲怎么让Performance Schema监控查询执行事件，并且把我们需要的监控数据记录下来。
如何启用系统监控？系统数据库performance_schema中的表setup_instruments和setup_consumers中的数据，是启用监控的关键。
setup_instruments保存的数据，表示哪些对象发生的事件可以被系统捕获（在MySQL中，把这些事件称作信息生产者）。
我们可以通过下面的代码，来查看一下当前MySQL会监控哪些事件的信息：
mysql&amp;gt; SELECT NAME,ENABLED,TIMED -&amp;gt; FROM performance_schema.setup_instruments -&amp;gt; LIMIT 1,10; +---------------------------------------------------------+---------+-------+ | NAME | ENABLED | TIMED | +---------------------------------------------------------+---------+-------+ | wait/synch/mutex/sql/TC_LOG_MMAP::LOCK_tc | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_commit | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_commit_queue | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_done | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_flush_queue | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_index | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_log | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_binlog_end_pos | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_sync | YES | YES | | wait/synch/mutex/sql/MYSQL_BIN_LOG::LOCK_sync_queue | YES | YES | +---------------------------------------------------------+---------+-------+ 10 rows in set (0.</description></item><item><title>26_生成IR：实现静态编译的语言</title><link>https://artisanbox.github.io/6/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/26/</guid><description>目前来讲，你已经初步了解了LLVM和它的IR，也能够使用它的命令行工具。不过，我们还是要通过程序生成LLVM的IR，这样才能复用LLVM的功能，从而实现一门完整的语言。
不过，如果我们要像前面生成汇编语言那样，通过字符串拼接来生成LLVM的IR，除了要了解LLVM IR的很多细节之外，代码一定比较啰嗦和复杂，因为字符串拼接不是结构化的方法，所以，最好用一个定义良好的数据结构来表示IR。
好在LLVM项目已经帮我们考虑到了这一点，它提供了代表LLVM IR的一组对象模型，我们只要生成这些对象，就相当于生成了IR，这个难度就低多了。而且，LLVM还提供了一个工具类，IRBuilder，我们可以利用它，进一步提升创建LLVM IR的对象模型的效率，让生成IR的过程变得更加简单！
接下来，就让我们先来了解LLVM IR的对象模型。
LLVM IR的对象模型LLVM在内部有用C++实现的对象模型，能够完整表示LLVM IR，当我们把字节码读入内存时，LLVM就会在内存中构建出这个模型。只有基于这个对象模型，我们才可以做进一步的工作，包括代码优化，实现即时编译和运行，以及静态编译生成目标文件。所以说，这个对象模型是LLVM运行时的核心。
IR对象模型的头文件在include/llvm/IR目录下，其中最重要的类包括：
Module（模块） Module类聚合了一个模块中的所有数据，它可以包含多个函数。你可以通过Model::iterator来遍历模块中所有的函数。它也包含了一个模块的全局变量。
Function（函数） Function包含了与函数定义（definition）或声明（declaration）有关的所有对象。函数定义包含了函数体，而函数声明，则仅仅包含了函数的原型，它是在其他模块中定义的，在本模块中使用。
你可以通过getArgumentList()方法来获得函数参数的列表，也可以遍历函数体中的所有基本块，这些基本块会形成一个CFG（控制流图）。
//函数声明，没有函数体。这个函数是在其他模块中定义的，在本模块中使用 declare void @foo(i32) //函数定义，包含函数体 define i32 @fun3(i32 %a) { %calltmp1 = call void @foo(i32 %a) //调用外部函数 ret i32 10 }
BasicBlock（基本块） BasicBlock封装了一系列的LLVM指令，你可以借助bigin()/end()模式遍历这些指令，还可以通过getTerminator()方法获得最后一条指令（也就是终结指令）。你还可以用到几个辅助方法在CFG中导航，比如获得某个基本块的前序基本块。
Instruction（指令） Instruction类代表了LLVM IR的原子操作（也就是一条指令），你可以通过getOpcode()来获得它代表的操作码，它是一个llvm::Instruction枚举值，你可以通过op_begin()和op_end()方法对获得这个指令的操作数。
Value（值） Value类代表一个值。在LLVM的内存IR中，如果一个类是从Value继承的，意味着它定义了一个值，其他方可以去使用。函数、基本块和指令都继承了Value。
LLVMContext（上下文） 这个类代表了LLVM做编译工作时的一个上下文，包含了编译工作中的一些全局数据，比如各个模块用到的常量和类型。
这些内容是LLVM IR对象模型的主要部分，我们生成IR的过程，就是跟这些类打交道，其他一些次要的类，你可以在阅读和编写代码的过程中逐渐熟悉起来。
接下来，就让我们用程序来生成LLVM的IR。
尝试生成LLVM IR我刚刚提到的每个LLVM IR类，都可以通过程序来构建。那么，为下面这个fun1()函数生成IR，应该怎么办呢？
int fun1(int a, int b){ return a+b; } 第一步，我们可以来生成一个LLVM模块，也就是顶层的IR对象。
Module *mod = new Module(&amp;quot;fun1.</description></item><item><title>26_红黑树（下）：掌握这些技巧，你也可以实现一个红黑树</title><link>https://artisanbox.github.io/2/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/27/</guid><description>红黑树是一个让我又爱又恨的数据结构，“爱”是因为它稳定、高效的性能，“恨”是因为实现起来实在太难了。我今天讲的红黑树的实现，对于基础不太好的同学，理解起来可能会有些困难。但是，我觉得没必要去死磕它。
我为什么这么说呢？因为，即便你将左右旋背得滚瓜烂熟，我保证你过不几天就忘光了。因为，学习红黑树的代码实现，对于你平时做项目开发没有太大帮助。对于绝大部分开发工程师来说，这辈子你可能都用不着亲手写一个红黑树。除此之外，它对于算法面试也几乎没什么用，一般情况下，靠谱的面试官也不会让你手写红黑树的。
如果你对数据结构和算法很感兴趣，想要开拓眼界、训练思维，我还是很推荐你看一看这节的内容。但是如果学完今天的内容你还觉得懵懵懂懂的话，也不要纠结。我们要有的放矢去学习。你先把平时要用的、基础的东西都搞会了，如果有余力了，再来深入地研究这节内容。
好，我们现在就进入正式的内容。上一节，我们讲到红黑树定义的时候，提到红黑树的叶子节点都是黑色的空节点。当时我只是粗略地解释了，这是为了代码实现方便，那更加确切的原因是什么呢？ 我们这节就来说一说。
实现红黑树的基本思想不知道你有没有玩过魔方？其实魔方的复原解法是有固定算法的：遇到哪几面是什么样子，对应就怎么转几下。你只要跟着这个复原步骤，就肯定能将魔方复原。
实际上，红黑树的平衡过程跟魔方复原非常神似，大致过程就是：遇到什么样的节点排布，我们就对应怎么去调整。只要按照这些固定的调整规则来操作，就能将一个非平衡的红黑树调整成平衡的。
还记得我们前面讲过的红黑树的定义吗？今天的内容里，我们会频繁用到它，所以，我们现在再来回顾一下。一棵合格的红黑树需要满足这样几个要求：
根节点是黑色的；
每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点。
在插入、删除节点的过程中，第三、第四点要求可能会被破坏，而我们今天要讲的“平衡调整”，实际上就是要把被破坏的第三、第四点恢复过来。
在正式开始之前，我先介绍两个非常重要的操作，左旋（rotate left）、右旋（rotate right）。左旋全称其实是叫围绕某个节点的左旋，那右旋的全称估计你已经猜到了，就叫围绕某个节点的右旋。
我们下面的平衡调整中，会一直用到这两个操作，所以我这里画了个示意图，帮助你彻底理解这两个操作。图中的a，b，r表示子树，可以为空。
前面我说了，红黑树的插入、删除操作会破坏红黑树的定义，具体来说就是会破坏红黑树的平衡，所以，我们现在就来看下，红黑树在插入、删除数据之后，如何调整平衡，继续当一棵合格的红黑树的。
插入操作的平衡调整首先，我们来看插入操作。
红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。所以，关于插入操作的平衡调整，有这样两种特殊情况，但是也都非常好处理。
如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。
如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。
除此之外，其他情况都会违背红黑树的定义，于是我们就需要进行调整，调整的过程包含两种基础的操作：左右旋转和改变颜色。
红黑树的平衡调整过程是一个迭代的过程。我们把正在处理的节点叫做关注节点。关注节点会随着不停地迭代处理，而不断发生变化。最开始的关注节点就是新插入的节点。
新节点插入之后，如果红黑树的平衡被打破，那一般会有下面三种情况。我们只需要根据每种情况的特点，不停地调整，就可以让红黑树继续符合定义，也就是继续保持平衡。
我们下面依次来看每种情况的调整过程。提醒你注意下，为了简化描述，我把父节点的兄弟节点叫做叔叔节点，父节点的父节点叫做祖父节点。
CASE 1：如果关注节点是a，它的叔叔节点d是红色，我们就依次执行下面的操作：
将关注节点a的父节点b、叔叔节点d的颜色都设置成黑色；
将关注节点a的祖父节点c的颜色设置成红色；
关注节点变成a的祖父节点c；
跳到CASE 2或者CASE 3。
CASE 2：如果关注节点是a，它的叔叔节点d是黑色，关注节点a是其父节点b的右子节点，我们就依次执行下面的操作：
关注节点变成节点a的父节点b；
围绕新的关注节点b左旋；
跳到CASE 3。</description></item><item><title>26｜增强更丰富的类型第1步：如何支持浮点数？</title><link>https://artisanbox.github.io/3/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/28/</guid><description>你好，我是宫文学。
我们前面几节课，讲的都是编译器前端的功能。虽然，要实现完善的前端功能，我们要做的工作还有很多。不过，我们现在已经不“虚”了！因为我们已经把编译器前端部分的主要知识点都讲得差不多了，其他的我们可以慢慢完善。
所以，现在我们重新把精力放回到编译器后端功能和运行时上来，这部分的功能我们还有待加强。在第一部分起步篇中，为了尽量简化实现过程，我们的语言只支持了整数的运算，甚至都没区分整型的长度，统一使用了32位的整型。
但这在实用级的语言中可行不通，我们还需要在里面添加各种丰富的数据类型。所以，接下来，我们会花几节课的时间，丰富一下我们语言支持的数据类型。首先我们会添加一些内置的基础类型，比如浮点型、字符串和数组。之后，我们还要通过对面向对象编程特性，支持用户自定义自己的类型。
在这一节课，我们先来看一下如何让我们的语言支持浮点型数据。为实现这个目的，我们需要先了解CPU为了支持浮点数有哪些特别的设计，ABI方面又有一些什么规定，以及如何修改汇编代码生成逻辑。而且，为了正确地在汇编代码中表示浮点型字面量，你还会学到浮点数编码方面的国际标准。
首先，让我们了解一下CPU硬件和ABI对浮点数运算提供的支持。
CPU和ABI对浮点数运算的支持我们先来回顾一下起步篇中关于整数运算的知识。已经有些日子没见到它们了，不知道你还记不记得？你可以和下面我们重点讲解的浮点数的处理模式对比着来看，看看它们有怎样的不同，这也能加强你对这些重点知识的记忆。
X86架构的CPU在64位模式下对整数运算的支持，最重要的就是这两个知识点：
寄存器。整数运算可以使用16个通用寄存器； 指令。对整数进行加减乘除的指令分别是addl、subl、imull和idivl。 另外，ABI也针对整数运算做了一些规定，比如：
参数传递。根据ABI，参数传递过程中会使用6个寄存器。超过6个参数，则放在调用者的栈桢里； 寄存器保护。有一些寄存器要能够跨函数调用保存数据，也就是说函数调用者需要保护这些寄存器，而另一些寄存器则不需要保护； 返回值。根据ABI，从函数中返回整数值时，使用的是%eax寄存器； 栈桢结构。ABI对于栈桢里存放参数、返回地址做了规定，并且规定栈桢需要16字节内存对齐，还规定了如何使用栈桢外的红区等等。 那么，我们再来看看CPU对浮点数运算的支持是怎么样的。
其实，最早期的X86CPU只支持整数运算，并不支持浮点数运算。如果我们要进行浮点数运算，就要用整数运算来模拟。但是这样的话，浮点数运算的速度就会比较慢。
而现代CPU解决了这个问题，普遍从硬件层面进行浮点运算，所以编译器也要直接生成浮点运算的机器码，最大程度地发挥硬件的性能。
我们之前说过，一款CPU可能支持多个指令集。而某些指令集，就是用于支持浮点数计算的。在X86的历史上，CPU最早是通过一个协处理器来处理浮点数运算，这个协处理器叫做FPU（浮点处理单元），它采用的指令集叫做X87。后来这个协处理器就被整合到CPU中了。
再后来，为了提高对多媒体数据的处理能力，厂商往CPU里增加了新的指令集，叫做MMX指令集。MMX的具体含义，有人说是多媒体扩展（MultiMedia eXtension），有人说是矩阵数学扩展（Matrix Math eXtension）。不管缩写的含义是什么，MMX主要就是增强了对浮点数的处理能力，因为多媒体的处理主要就是浮点数运算。
并且，MMX还属于SIMD类型的指令集。SIMD（Single Instruction Multiple Data）是一条指令对多个数据完成加减乘数运算的意思，因此MMX指令能让CPU的处理效率更高。
MMX指令集后来又升级成为了SSE指令集，还形成了多个版本，每个版本都会增加一些新的指令和功能。最新的版本是SSE4.2。SSE是流式SIMD扩展（Streaming SIMD Extensions）的意思。到今天，X86计算机进行浮点数运算的时候，基本上都是采用SSE指令集，不再使用x87指令集，除非是使用那些特别早的型号的CPU。
不知道你还记不记得，我们之前提过，你可以查询自己电脑的CPU所支持的指令集。在macOS上，我用下面的命令就可以查到：
sysctl machdep.cpu.features machdep.cpu.leaf7_features 然后你会在命令行终端，得到关于CPU特性的信息。这些特性就对应着指令集。比如，出现在第一个的FPU，就对应着X87指令集。你也会从其中看到多个版本的SSE指令集。
如果你嫌上面的命令太长，那也可以使用一个短一点的命令。这个命令会打印出更多关于CPU的信息，比如CPU所支持的线程数，等等。其中也包括该CPU的指令集。
sysctl machdep.cpu 这里我插一个小知识点，不知道你会不会有这个疑惑，我们操作系统是怎么知道某CPU支持哪些指令集的呢？原来，X86架构的CPU提供了一个cpuid指令。你用这个指令就可以得到CPU类型、型号、制造商信息、商标信息、序列号、缓存，还有支持特性（也就是指令集）等一系列信息了。所以你看，要理解软件的功能，经常都需要底层硬件架构的知识。
好了，既然我们需要用到SSE指令集，那就需要了解一下SSE指令集的特点。并且，SSE其实不仅能处理浮点数，还能处理整数。不过现在我们主要关心与浮点数有关的特性。这些信息从哪里获得呢？当然是从Intel的手册。下面这些信息就来自于《Intel® 64 and IA-32 Architectures Software Developer’s Manual，Volume 1: Basic Architecture》，我给你稍微总结一下。
首先，我们看看SSE指令所使用的寄存器。
在64位模式下，SSE可以使用16个128位的寄存器，分别叫做xmm0~xmm15。
此外，SSE还会使用一个32位的MXCSR寄存器，用于保存浮点数运算时的控制信息和状态信息。比如，如果你做除法的时候，除数是0，那么就会触发一个异常。而MXCSR寄存器上的某个标志位会决定如何处理该异常：是采用内置的标准方法来处理呢，还是触发一个软件异常来处理。关于MXCSR的详细信息，你可以按需要查看一下手册。
第二，我们看一下SSE对数据类型的支持。
SSE指令支持32位的单精度数，也支持64位的双精度数。不过，单精度数和双精度数的格式，都遵循IEEE 754标准。
在SSE指令中，寄存器里可以只放一个浮点数，这个时候我们把它叫做标量（Scalar）。还可以把多个浮点数打包放在一个寄存器里，这种数据格式叫做打包格式（&amp;nbsp;Packed Data Types），或者叫做向量格式。下图就显示了在一个128位寄存器里存放4个单精度浮点数的情况。
打包格式是用于SIMD类型的指令的，这样一条指令就能处理寄存器里的4个单精度浮点数的计算。不过，我们关注的还是对标量数据的处理，所以就先忽略向量数据处理的情况，有需要我们再补充。
第三，我们看看SSE指令的情况。
SSE对处理浮点数的指令，包括向量指令和标量指令。另外，在JavaScript中，number是以双精度数来表示的，所以我们的语言也就可以忽略与单精度浮点数有关的指令，直接关注双精度浮点数指令就好了。
我在下面这张表中，列出了SSE中与标量的、双精度浮点数处理有关的一些主要的指令：
你能看到，其实这些指令数量也并不太多，很容易掌握。当然，SSE完整的指令还是不少的。SSE针对向量数据处理、整型数据处理都有单独的指令，还有一些指令是用于管理MXCSR寄存器的状态，以及对高速缓存进行管理的。如果你想了解这些，可以阅读Intel手册的第二卷：《Intel® 64 and IA-32 architectures software developer’s manual combined volumes 2A, 2B, 2C, and&amp;nbsp; 2D: Instruction set reference, A- Z》</description></item><item><title>27_SIMD：如何加速矩阵乘法？</title><link>https://artisanbox.github.io/4/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/27/</guid><description>上一讲里呢，我进一步为你讲解了CPU里的“黑科技”，分别是超标量（Superscalar）技术和超长指令字（VLIW）技术。
超标量（Superscalar）技术能够让取指令以及指令译码也并行进行；在编译的过程，超长指令字（VLIW）技术可以搞定指令先后的依赖关系，使得一次可以取一个指令包。
不过，CPU里的各种神奇的优化我们还远远没有说完。这一讲里，我就带你一起来看看，专栏里最后两个提升CPU性能的架构设计。它们分别是，你应该常常听说过的超线程（Hyper-Threading）技术，以及可能没有那么熟悉的单指令多数据流（SIMD）技术。
超线程：Intel多卖给你的那一倍CPU不知道你是不是还记得，在第21讲，我给你介绍了Intel是怎么在Pentium 4处理器上遭遇重大失败的。如果不太记得的话，你可以回过头去回顾一下。
那时我和你说过，Pentium 4失败的一个重要原因，就是它的CPU的流水线级数太深了。早期的Pentium 4的流水线深度高达20级，而后期的代号为Prescott的Pentium 4的流水线级数，更是到了31级。超长的流水线，使得之前我们讲的很多解决“冒险”、提升并发的方案都用不上。
因为这些解决“冒险”、提升并发的方案，本质上都是一种指令级并行（Instruction-level parallelism，简称IPL）的技术方案。换句话说就是，CPU想要在同一个时间，去并行地执行两条指令。而这两条指令呢，原本在我们的代码里，是有先后顺序的。无论是我们在流水线里面讲到的流水线架构、分支预测以及乱序执行，还是我们在上一讲说的超标量和超长指令字，都是想要通过同一时间执行两条指令，来提升CPU的吞吐率。
然而在Pentium 4这个CPU上，这些方法都可能因为流水线太深，而起不到效果。我之前讲过，更深的流水线意味着同时在流水线里面的指令就多，相互的依赖关系就多。于是，很多时候我们不得不把流水线停顿下来，插入很多NOP操作，来解决这些依赖带来的“冒险”问题。
不知道是不是因为当时面临的竞争太激烈了，为了让Pentium 4的CPU在性能上更有竞争力一点，2002年底，Intel在的3.06GHz主频的Pentium 4 CPU上，第一次引入了超线程（Hyper-Threading）技术。
什么是超线程技术呢？Intel想，既然CPU同时运行那些在代码层面有前后依赖关系的指令，会遇到各种冒险问题，我们不如去找一些和这些指令完全独立，没有依赖关系的指令来运行好了。那么，这样的指令哪里来呢？自然同时运行在另外一个程序里了。
你所用的计算机，其实同一个时间可以运行很多个程序。比如，我现在一边在浏览器里写这篇文章，后台同样运行着一个Python脚本程序。而这两个程序，是完全相互独立的。它们两个的指令完全并行运行，而不会产生依赖问题带来的“冒险”。
然而这个时候，你可能就会觉得奇怪了，这么做似乎不需要什么新技术呀。现在我们用的CPU都是多核的，本来就可以用多个不同的CPU核心，去运行不同的任务。即使当时的Pentium 4是单核的，我们的计算机本来也能同时运行多个进程，或者多个线程。这个超线程技术有什么特别的用处呢？
无论是上面说的多个CPU核心运行不同的程序，还是在单个CPU核心里面切换运行不同线程的任务，在同一时间点上，一个物理的CPU核心只会运行一个线程的指令，所以其实我们并没有真正地做到指令的并行运行。
超线程可不是这样。超线程的CPU，其实是把一个物理层面CPU核心，“伪装”成两个逻辑层面的CPU核心。这个CPU，会在硬件层面增加很多电路，使得我们可以在一个CPU核心内部，维护两个不同线程的指令的状态信息。
比如，在一个物理CPU核心内部，会有双份的PC寄存器、指令寄存器乃至条件码寄存器。这样，这个CPU核心就可以维护两条并行的指令的状态。在外面看起来，似乎有两个逻辑层面的CPU在同时运行。所以，超线程技术一般也被叫作同时多线程（Simultaneous Multi-Threading，简称SMT）技术。
不过，在CPU的其他功能组件上，Intel可不会提供双份。无论是指令译码器还是ALU，一个CPU核心仍然只有一份。因为超线程并不是真的去同时运行两个指令，那就真的变成物理多核了。超线程的目的，是在一个线程A的指令，在流水线里停顿的时候，让另外一个线程去执行指令。因为这个时候，CPU的译码器和ALU就空出来了，那么另外一个线程B，就可以拿来干自己需要的事情。这个线程B可没有对于线程A里面指令的关联和依赖。
这样，CPU通过很小的代价，就能实现“同时”运行多个线程的效果。通常我们只要在CPU核心的添加10%左右的逻辑功能，增加可以忽略不计的晶体管数量，就能做到这一点。
不过，你也看到了，我们并没有增加真的功能单元。所以超线程只在特定的应用场景下效果比较好。一般是在那些各个线程“等待”时间比较长的应用场景下。比如，我们需要应对很多请求的数据库应用，就很适合使用超线程。各个指令都要等待访问内存数据，但是并不需要做太多计算。
于是，我们就可以利用好超线程。我们的CPU计算并没有跑满，但是往往当前的指令要停顿在流水线上，等待内存里面的数据返回。这个时候，让CPU里的各个功能单元，去处理另外一个数据库连接的查询请求就是一个很好的应用案例。
我的移动工作站的CPU信息我这里放了一张我的电脑里运行CPU-Z的截图。你可以看到，在右下角里，我的CPU的Cores，被标明了是4，而Threads，则是8。这说明我手头的这个CPU，只有4个物理的CPU核心，也就是所谓的4核CPU。但是在逻辑层面，它“装作”有8个CPU核心，可以利用超线程技术，来同时运行8条指令。如果你用的是Windows，可以去下载安装一个CPU-Z来看看你手头的CPU里面对应的参数。
SIMD：如何加速矩阵乘法？在上面的CPU信息的图里面，你会看到，中间有一组信息叫作Instructions，里面写了有MMX、SSE等等。这些信息就是这个CPU所支持的指令集。这里的MMX和SSE的指令集，也就引出了我要给你讲的最后一个提升CPU性能的技术方案，SIMD，中文叫作单指令多数据流（Single Instruction Multiple Data）。
我们先来体会一下SIMD的性能到底怎么样。下面是两段示例程序，一段呢，是通过循环的方式，给一个list里面的每一个数加1。另一段呢，是实现相同的功能，但是直接调用NumPy这个库的add方法。在统计两段程序的性能的时候，我直接调用了Python里面的timeit的库。
$ python &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; import timeit &amp;gt;&amp;gt;&amp;gt; a = list(range(1000)) &amp;gt;&amp;gt;&amp;gt; b = np.array(range(1000)) &amp;gt;&amp;gt;&amp;gt; timeit.timeit(&amp;quot;[i + 1 for i in a]&amp;quot;, setup=&amp;quot;from __main__ import a&amp;quot;, number=1000000) 32.82800309999993 &amp;gt;&amp;gt;&amp;gt; timeit.timeit(&amp;quot;np.add(1, b)&amp;quot;, setup=&amp;quot;from __main__ import np, b&amp;quot;, number=1000000) 0.</description></item><item><title>27_主库出问题了，从库怎么办？</title><link>https://artisanbox.github.io/1/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/27/</guid><description>在前面的第24、25和26篇文章中，我和你介绍了MySQL主备复制的基础结构，但这些都是一主一备的结构。
大多数的互联网应用场景都是读多写少，因此你负责的业务，在发展过程中很可能先会遇到读性能的问题。而在数据库层解决读性能问题，就要涉及到接下来两篇文章要讨论的架构：一主多从。
今天这篇文章，我们就先聊聊一主多从的切换正确性。然后，我们在下一篇文章中再聊聊解决一主多从的查询逻辑正确性的方法。
如图1所示，就是一个基本的一主多从结构。
图1 一主多从基本结构图中，虚线箭头表示的是主备关系，也就是A和A’互为主备， 从库B、C、D指向的是主库A。一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担。
今天我们要讨论的就是，在一主多从架构下，主库故障后的主备切换问题。
如图2所示，就是主库发生故障，主备切换后的结果。
图2 一主多从基本结构--主备切换相比于一主一备的切换流程，一主多从结构在切换完成后，A’会成为新的主库，从库B、C、D也要改接到A’。正是由于多了从库B、C、D重新指向的这个过程，所以主备切换的复杂性也相应增加了。
接下来，我们再一起看看一个切换系统会怎么完成一主多从的主备切换过程。
基于位点的主备切换这里，我们需要先来回顾一个知识点。
当我们把节点B设置成节点A’的从库的时候，需要执行一条change master命令：
CHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password MASTER_LOG_FILE=$master_log_name MASTER_LOG_POS=$master_log_pos 这条命令有这么6个参数：
MASTER_HOST、MASTER_PORT、MASTER_USER和MASTER_PASSWORD四个参数，分别代表了主库A’的IP、端口、用户名和密码。 最后两个参数MASTER_LOG_FILE和MASTER_LOG_POS表示，要从主库的master_log_name文件的master_log_pos这个位置的日志继续同步。而这个位置就是我们所说的同步位点，也就是主库对应的文件名和日志偏移量。 那么，这里就有一个问题了，节点B要设置成A’的从库，就要执行change master命令，就不可避免地要设置位点的这两个参数，但是这两个参数到底应该怎么设置呢？
原来节点B是A的从库，本地记录的也是A的位点。但是相同的日志，A的位点和A’的位点是不同的。因此，从库B要切换的时候，就需要先经过“找同步位点”这个逻辑。
这个位点很难精确取到，只能取一个大概位置。为什么这么说呢？
我来和你分析一下看看这个位点一般是怎么获取到的，你就清楚其中不精确的原因了。
考虑到切换过程中不能丢数据，所以我们找位点的时候，总是要找一个“稍微往前”的，然后再通过判断跳过那些在从库B上已经执行过的事务。
一种取同步位点的方法是这样的：
等待新主库A’把中转日志（relay log）全部同步完成；
在A’上执行show master status命令，得到当前A’上最新的File 和 Position；
取原主库A故障的时刻T；
用mysqlbinlog工具解析A’的File，得到T时刻的位点。
mysqlbinlog File --stop-datetime=T --start-datetime=T 图3 mysqlbinlog 部分输出结果图中，end_log_pos后面的值“123”，表示的就是A’这个实例，在T时刻写入新的binlog的位置。然后，我们就可以把123这个值作为$master_log_pos ，用在节点B的change master命令里。
当然这个值并不精确。为什么呢？
你可以设想有这么一种情况，假设在T这个时刻，主库A已经执行完成了一个insert 语句插入了一行数据R，并且已经将binlog传给了A’和B，然后在传完的瞬间主库A的主机就掉电了。
那么，这时候系统的状态是这样的：
在从库B上，由于同步了binlog， R这一行已经存在；</description></item><item><title>27_代码优化：为什么你的代码比他的更高效？</title><link>https://artisanbox.github.io/6/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/27/</guid><description>在使用LLVM的过程中，你应该觉察到了，优化之后和优化之前的代码相差很大。代码优化之后，数量变少了，性能也更高了。而针对这个看起来很神秘的代码优化，我想问你一些问题：
代码优化的目标是什么？除了性能上的优化，还有什么优化？ 代码优化可以在多大的范围内执行？是在一个函数内，还是可以针对整个应用程序？ 常见的代码优化场景有哪些？ 这些问题是代码优化的基本问题，很重要，我会用两节课的时间带你了解和掌握。
当然了，代码优化是编译器后端的两大工作之一（另一个是代码生成），弄懂它，你就掌握了一大块后端技术。而学习代码优化的原理，然后通过LLVM实践一下，这样原理与实践相结合，会帮你早日弄懂代码优化。
接下来，我带你概要地了解一下代码优化的目标、对象、范围和策略等内容。
了解代码优化的目标、对象、范围和策略 代码优化的目标 代码优化的目标，是优化程序对计算机资源的使用。我们平常最关心的就是CPU资源，最大效率地利用CPU资源可以提高程序的性能。代码优化有时候还会有其他目标，比如代码大小、内存占用大小、磁盘访问次数、网络通讯次数等等。
代码优化的对象 从代码优化的对象看，大多数的代码优化都是在IR上做的，而不是在前一阶段的AST和后一阶段汇编代码上进行的，为什么呢？
其实，在AST上也能做一些优化，比如在讲前端内容的时候，我们曾经会把一些不必要的AST层次削减掉（例如add-&amp;gt;mul-&amp;gt;pri-&amp;gt;Int，每个父节点只有一个子节点，可以直接简化为一个Int节点），但它抽象层次太高，含有的硬件架构信息太少，难以执行很多优化算法。 在汇编代码上进行优化会让算法跟机器相关，当换一个目标机器的时候，还要重新编写优化代码。所以，在IR上是最合适的，它能尽量做到机器独立，同时又暴露出很多的优化机会。
代码优化的范围 从优化的范围看，分为本地优化、全局优化和过程间优化。
优化通常针对一组指令，最常用也是最重要的指令组，就是基本块。基本块的特点是：每个基本块只能从入口进入，从最后一条指令退出，每条指令都会被顺序执行。因着这个特点，我们在做某些优化时会比较方便。比如，针对下面的基本块，我们可以很安全地把第3行的“y:=t+x”改成“y:= 3 * x”，因为t的赋值一定是在y的前面：
BB1: t:=2 * x y:=t + x Goto BB2 这种针对基本块的优化，我们叫做本地优化（Local Optimization）。
那么另一个问题来了：我们能否把第二行的“t:=2 * x”也优化删掉呢？这取决于是否有别的代码会引用t。所以，我们需要进行更大范围的分析，才能决定是否把第二行优化掉。
超越基本块的范围进行分析，我们需要用到控制流图（Control Flow Graph，CFG）。CFG是一种有向图，它体现了基本块之前的指令流转关系。如果从BB1的最后一条指令是跳转到BB2，那么从BB1到BB2就有一条边。一个函数（或过程）里如果包含多个基本块，可以表达为一个CFG。
如果通过分析CFG，我们发现t在其他地方没有被使用，就可以把第二行删掉。这种针对一个函数、基于CFG的优化，叫做全局优化（Global Optimization）。
比全局优化更大范围的优化，叫做过程间优化（Inter-procedural Optimization），它能跨越函数的边界，对多个函数之间的关系进行优化，而不是仅针对一个函数做优化。
代码优化的策略 最后，你不需要每次都把代码优化做彻底，因为做代码优化本身也需要消耗计算机的资源。所以，你需要权衡代码优化带来的好处和优化本身的开支这两个方面，然后确定做多少优化。比如，在浏览器里加载JavaScript的时候，JavaScript引擎一定会对JavaScript做优化，但如果优化消耗的时间太长，界面的响应会变慢，反倒影响用户使用页面的体验，所以JavaScript引擎做优化时要掌握合适的度或调整优化时机。
接下来，我带你认识一些常见的代码优化的场景，这样可以让你对代码优化的认识更加直观，然后我们也可以将这部分知识作为后面讨论算法的基础。
一些优化的场景 代数优化（Algebraic Optimazation） 代数优化是最简单的一种优化，当操作符是代数运算的时候，你可以根据学过的数学知识进行优化。
比如“x:=x+0 ”这行代码，操作前后x没有任何变化，所以这样的代码可以删掉；又比如“x:=x*0” 可以简化成“x:=0”；对某些机器来说，移位运算的速度比乘法的快，那么“x:=x*8”可以优化成“x:=x&amp;lt;&amp;lt;3”。
常数折叠（Constant Folding） 它是指，对常数的运算可以在编译时计算，比如 “x:= 20 * 3 ”可以优化成“x:=60”。另外，在if条件中，如果条件是一个常量，那就可以确定地取某个分支。比如：“If 2&amp;gt;0 Goto BB2” 可以简化成“Goto BB2”就好了。</description></item><item><title>27_手把手带你设计一个完整的连锁超市信息系统数据库（上）</title><link>https://artisanbox.github.io/8/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/27/</guid><description>你好，我是朱晓峰。
从创建第一个MySQL数据库开始到现在，我们已经学完了MySQL的核心操作。最后这两节课，我想带你实际设计一个超市信息系统的数据库。毕竟，设计数据库很考验我们综合运用单个技术点的能力。所以，通过这个项目，我不仅会带你把前面的内容串联起来，而且还会教你设计数据库的通用思路。
为什么选择超市项目呢？一方面呢，超市的场景与我们的日常生活密切相关，你比较容易理解其中的业务逻辑，另一方面，超市的业务又相当复杂，几乎能用到我们学到的所有知识点，利于我们对前面学过的内容进行整合。
今天，我就带你一起，从需求分析开始入手，一直到容灾和备份，完成一个全流程的连锁超市数据库设计。这节课，我会主要给你讲解需求分析、ER模型、分库分表和数据库设计方案。我们做项目，首先要从大处着眼，把握方向，这样才不容易出现大的偏差。下节课，我们再设计具体的细节，比如创建数据表、外键约束，设计灾备方案和备份数据等。
在开始设计之前，咱们得先了解一下项目背景。
随着互联网使用的日益广泛，传统的桌面管理系统的数据不能互通、资源利用率低等弊端越来越明显，显然不能满足用户的需求了。因此，我们需要开发一款基于云服务的连锁超市管理信息系统。具体的要求是：
基于浏览器，无需安装，账号开通即可使用，方便快捷； 数据部署在云端，由运营商负责维护，安全可靠； 用户无需自备服务器，只需租用信息服务，资源利用率高。 知道了具体要求，那该怎么进行设计呢？下面我带你来分析一下。
如何设计数据结构？用户账号开通即可使用，所以必然要设计分层的数据结构；数据要部署在云端，所以必然要使用云。根据这些要求，我们可以设计一个基于云服务的2层数据结构，这个结构的示意图如下所示：
我来解释一下图中展示的内容。
首先，你可以看到，所有的系统资源和服务都部署在云端。
其次，我们来看下数据结构层面，主要有2层。
第一层是商户。每个入驻的商户都有一个组号，所有与这个商户有关的数据，通过这个组号进行识别。 第二层是分支机构。分支机构从属于商户，相同商户的分支机构有相同的组号。分支机构分为几种，包括总部、门店、配送中心等。门店又分为直营店和加盟店。每个分支机构有一个机构编号，同一分支机构的数据，有相同的组号和机构编号。 这样一来，新商户只需要开通账号，分配一个新的组号，就可以使用了。组号用于隔离商户之间的数据，使商户之间互相不影响。
最后，数据由我们统一进行运维，安全性有保障。商户自己不需要采购服务器，只需租用服务，资源的利用率更高。
系统的整体结构设计思路有了，那具体在应用层面如何实现呢？
我先用一张图来展示具体的应用构成：
这个图展示了应用的3个层级。
展现层：包括门店收款机App、移动端的手机App、小程序，以及通过浏览器访问的后台管理部分。 服务层：包括云端的销售模块、库存模块、营运模块、会员模块等。 数据层：MySQL数据库。 门店收款App、移动端的手机App，小程序等与数据库设计无关，我就不多说了。下面我重点介绍一下后台管理部分下面的服务层和数据层的相关内容。
服务层包括了销售、库存、营运、会员等管理模块。下面我就以库存管理中的盘点模块为例，详细介绍一下。因为这个模块比较简单，容易理解。
盘点，简单来说，就是把超市仓库里的商品都数一遍，然后跟电脑里的库存数比对，看看有没有不对的地方。实际数出来的库存数量叫做盘存数量，电脑中的库存数量叫做结存数量，比对的结果叫做盈亏数量。要是盘存数量比结存数量多，叫盘盈，否则叫做盘亏。
盘点操作是超市库存管理模块中的一个重要环节，是掌握实际库存的唯一办法。盘点盈亏数据也是衡量超市管理水平的重要指标。
盘点作业一般都在晚上门店营业结束以后进行。这也很好理解，毕竟，在白天营业的过程中，商品不断被顾客取走，又不断得到补充，库存处于一种变化状态，无法获取准确数据。
下面我来介绍下盘点的步骤。
先生成一张盘点表，把当前系统中的库存数量提取出来，获得结存数量； 把员工实际数出来的库存数据录入盘点表中，获得盘存数量； 计算盈亏数量，公式是“盈亏数量=盘存数量-结存数量”； 数据确认无误后，验收盘点表，并调整当前库存：库存数量=库存数量+盈亏数量。 经过这些操作，系统中的库存数量与实际的库存数量就一致了，盘点盈亏也被记录下来了，体现在日报等报表中，超市经营者可以进行查看，为经营决策提供依据。
介绍完了盘点业务，现在回到数据库设计的主题上来，看看如何把盘点业务用数据表的形式表现出来。
盘点业务都是在门店进行，由员工实际操作，对仓库中的商品进行清点。因此，盘点业务模块中肯定要包含员工、门店、仓库、商品等实体。这个时候，我们就可以使用ER模型这个工具，来理清盘点模块的业务逻辑。
盘点模块的ER模型我先把模型直接展示给你，一会儿我再带你具体分析一下。
首先，我们来分析下模型中的实体和关系。
这个ER模型中包括了5个实体，分别是：
商户 门店 员工 商品 仓库 其中，商户和商品是强实体，门店、仓库和员工是弱实体。
这个ER模型中还包含了5个关系，我们按照1对多和多对多来分下类。
1对多：
商户与门店的从属关系 门店与员工的雇佣关系 门店与仓库的拥有关系 多对多：
仓库与商品的库存关系 仓库、商品和员工参与的盘点关系 接下来，我们再分析一下这5个实体各自的属性。
商户：组号、名称、地址、电话、联系人。 门店：组号、门店编号、名称、地址、电话、类别。 员工：组号、门店编号、工号、名称、身份证、电话、职责。 仓库：组号、门店编号、仓库编号、类别。 商品：组号、条码、名称、规格、单位、价格。 除此之外，还有2个多对多关系的属性。</description></item><item><title>27_瞧一瞧Linux：Linux如何实现进程与进程调度</title><link>https://artisanbox.github.io/9/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/27/</guid><description>你好，我是LMOS。
在前面的课程中，我们已经写好了Cosmos的进程管理组件，实现了多进程调度运行，今天我们一起探索Linux如何表示进程以及如何进行多进程调度。
好了，话不多说，我们开始吧。
Linux如何表示进程在Cosmos中，我们设计了一个thread_t数据结构来代表一个进程，Linux也同样是用一个数据结构表示一个进程。
下面我们先来研究Linux的进程数据结构，然后看看Linux进程的地址空间数据结构，最后再来理解Linux的文件表结构。
Linux进程的数据结构Linux系统下，把运行中的应用程序抽象成一个数据结构task_struct，一个应用程序所需要的各种资源，如内存、文件等都包含在task_struct结构中。
因此，task_struct结构是非常巨大的一个数据结构，代码如下。
struct task_struct { struct thread_info thread_info;//处理器特有数据 volatile long state; //进程状态 void *stack; //进程内核栈地址 refcount_t usage; //进程使用计数 int on_rq; //进程是否在运行队列上 int prio; //动态优先级 int static_prio; //静态优先级 int normal_prio; //取决于静态优先级和调度策略 unsigned int rt_priority; //实时优先级 const struct sched_class *sched_class;//指向其所在的调度类 struct sched_entity se;//普通进程的调度实体 struct sched_rt_entity rt;//实时进程的调度实体 struct sched_dl_entity dl;//采用EDF算法调度实时进程的调度实体 struct sched_info sched_info;//用于调度器统计进程的运行信息 struct list_head tasks;//所有进程的链表 struct mm_struct *mm; //指向进程内存结构 struct mm_struct *active_mm; pid_t pid; //进程id struct task_struct __rcu *parent;//指向其父进程 struct list_head children; //链表中的所有元素都是它的子进程 struct list_head sibling; //用于把当前进程插入到兄弟链表中 struct task_struct *group_leader;//指向其所在进程组的领头进程 u64 utime; //用于记录进程在用户态下所经过的节拍数 u64 stime; //用于记录进程在内核态下所经过的节拍数 u64 gtime; //用于记录作为虚拟机进程所经过的节拍数 unsigned long min_flt;//缺页统计 unsigned long maj_flt; struct fs_struct *fs; //进程相关的文件系统信息 struct files_struct *files;//进程打开的所有文件 struct vm_struct *stack_vm_area;//内核栈的内存区 }; 为了帮你掌握核心思路，关于task_struct结构体，我省略了进程的权能、性能跟踪、信号、numa、cgroup等相关的近500行内容，你若有兴趣可以自行阅读，这里你只需要明白，在内存中，一个task_struct结构体的实例变量代表一个Linux进程就行了。</description></item><item><title>27_课前导读：学习现代语言设计的正确姿势</title><link>https://artisanbox.github.io/7/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/27/</guid><description>你好，我是宫文学。
到目前为止，你就已经学完了这门课程中前两个模块的所有内容了。在第一个模块“预备知识篇”，我带你梳理了编译原理的关键概念、算法等核心知识点，帮你建立了一个直观的编译原理基础知识体系；在第二个模块“真实编译器解析篇”，我带你探究了7个真实世界的编译器，让你对编译器所实际采用的各种编译技术都有所涉猎。那么在接下来的第三个模块，我会继续带你朝着提高编译原理实战能力的目标前进。这一次，我们从计算机语言设计的高度，来印证一下编译原理的核心知识点。
对于一门完整的语言来说，编译器只是其中的一部分。它通常还有两个重要的组成部分：一个是运行时，包括内存管理、并发机制、解释器等模块；还有一个是标准库，包含了一些标准的功能，如算术计算、字符串处理、文件读写，等等。
再进一步来看，我们在实现一门语言的时候，首先要做的，就是确定这门语言所要解决的问题是什么，也就是需求问题；其次，针对需要解决的问题，我们要选择合适的技术方案，而这些技术方案正是分别由编译器、运行时和标准库去实现的。
所以，从计算机语言设计的高度来印证编译原理知识，我们也能更容易理解编译器的任务，更容易理解它是如何跟运行时环境去做配合的，这也会让你进一步掌握编译技术。
好了，那接下来就一起来看看，到底用什么样的方式，我们才能真正理解计算机语言的设计思路。
首先，我们来聊一聊实现一门计算机语言的关键因素：需求和设计。
如何实现一门计算机语言？我们学习编译原理的一个重要的目标，就是能够实现一门计算机语言。这种语言可能是你熟悉的某些高级语言，也可能是某个领域、为了解决某个具体问题而设计的DSL。就像我们在第二个模块中见到的SQL，以及编译器在内部实现时用到的一些DSL，如Graal生成LIR时的模式匹配规则、Python编译器中的ASDL解析器，还有Go语言编译器中IR的重写规则等。
那么要如何实现一门优秀的语言呢？我们都知道，要实现一个软件，有两个因素是最重要的，一个是需求，一个是设计。计算机语言作为一种软件，具有清晰的需求和良好的设计，当然也是至关重要的。
我先来说说需求问题，也就是计算机语言要解决的问题。
这里你要先明确一件事，如果需求不清晰、目标不明确，那么想要实现这门语言其实是很难成功的。通常来说，我们不能指望任何一种语言是全能的，让它擅长解决所有类型的问题。所以，每一门语言都有其所要解决的针对性问题。
举个例子，JavaScript如果单从设计的角度来看，有很多细节值得推敲，有不少的“坑”，比如null、undefined和NaN几个值就很令人困惑，你知道“null==undefined”的值是true还是false吗？但是它所能解决的问题也非常清晰，就是作为浏览器的脚本语言，提供Web的交互功能。在这个方面，它比同时期诞生的其他竞争技术，如ActiveX和Java Applet，都更具优势，所以它才能胜出。
历史上的计算机语言，都是像JavaScript那样，在满足了那个时代的某个需求以后而流行起来的。其中，根据“硅谷创业之父”保罗·格雷厄姆（Paul Graham）在《黑客与画家》中的说法，这些语言往往是一个流行的系统的脚本。比如说，C语言是Unix系统的脚本，COBOL是大型机的脚本，SQL是数据库系统的脚本，JavaScript、Java和C#都是浏览器的脚本，Swift和Objective-C是苹果系统的脚本，Kotlin是Android的脚本。让一门语言成为某个流行的技术系统的脚本，为这个生态提供编程支持，就是一种定位很清晰的需求。
好，明确了语言的需求以后，我们再来说说设计问题。
设计是实现计算机语言这种软件所要做的技术选择。你已经看到，我们研究的不同语言，其实现技术都各有特点，分别服务于该语言的需求问题，或者说设计目标。
我还是拿JavaScript来举例子。JavaScript被设计成了一门解释执行的语言，这样的话，它就能很方便地嵌入到HTML文本中，随着HTML下载和执行，并且支持不同的客户端操作系统和浏览器平台。而如果是需要静态编译的语言，就没有这么方便。
再进一步，由于HTML下载完毕后，JavaScript就要马上执行，从而也对JavaScript的编译速度有了更高的要求，所以我们才会看到V8里面的那些有利于快速解析的技术，比如通过查表做词法分析、懒解析等。
另外，因为JavaScript早期只是在浏览器里做一些比较简单的工作，所以它一开始没有设计并发计算的能力。还有，由于每个页面运行的JavaScript都是单独的，并且在页面退出时就可以收回内存，因此JavaScript的垃圾收集功能也不需要太复杂。
作为对比，Go语言的设计主要是用来编写服务端程序的，那么它的关键特性也是与这个定位相适应。
并发：服务端的软件最重要的一项能力就是支持大量并发任务。Go在语言设计上把并发能力作为第一等级的语言要素。 垃圾收集：由于垃圾收集会造成整个应用程序停下来，所以Go语言特别重视降低由于垃圾收集而产生的停顿。 那么总结起来，我们要想成功地实现一门语言，要把握两个要点：第一，要弄清楚该语言的需求，也就是它要去解决的问题；第二，要确定合适的技术方案，来更好地解决它要面对的问题。
计算机语言的设计会涉及到比较多的内容，为了防止你在学习时抓不到重点，我在第三个模块里，挑了一些重点的内容来做讲解，比如前面提到的垃圾收集的特性等。我会以第二个模块所研究的多门语言和编译器作为素材，一起探讨一下，各门语言都是采用了什么样的技术方案来满足各自的设计目标的，从而让你对计算机语言设计所考虑的因素、编译技术如何跟其他相关技术打配合，形成一个宏观的认识。
“现代语言设计篇”都会讲哪些内容？这个模块的内容，我根据计算机语言的组成和设计中的关键点，将其分成了三个部分。
第一部分，是对各门语言的编译器的前端、中端和后端技术做一下对比和总结。
这样，通过梳理和总结，我们就可以找出各种编译器之间的异同点。对于其共同的部分，我们可以看作是一些最佳实践，你在自己的项目中可以大胆地采用；而有差异的部分，则往往是某种编译器为了实现该语言的设计目标而采用的技术策略，你可以去体会各门语言是如何做取舍的，这样也能变成你自己的经验储备。
第二部分，主要是对语言的运行时和标准库的实现技术做一个解析。
我们说过，一门语言要包括编译器、运行时和标准库。在学习第二个模块的时候，你应该已经有了一些体会，你能发现编译器的很多特性是跟语言的运行时密切相关的。比如，Python有自己独特的对象体系的设计，那么Python的字节码就体现了对这些对象的操作，字节码中的操作数都是对象的引用。
那么在这一部分，我就分为了几个话题来进行讲解：
第一，是对语言的运行时和标准库的宏观探讨。我们一起来看看不同的语言的运行时和它的编译器之间是如何互相影响的。另外，我还会和你探讨语言的基础功能和标准库的实现策略，这是非常值得探讨的知识点，它让一门语言具备了真正的实用价值。 第二，是垃圾收集机制。本课程分析、涉及的几种语言，它们所采用的垃圾收集机制都各不相同。那么，为什么一门语言会选择这个机制，而另一种语言会选择另一种机制呢？带着这样的问题所做的分析，会让你把垃圾收集方面的原理落到实践中去。 第三，是并发模型。对并发的支持，对现代语言来说也越来越重要。在后面的课程中，我会带你了解线程、协程、Actor三种并发模式，理解它们的优缺点，同时你也会了解到，如何在编译器和运行时中支持这些并发特性。 第三部分，是计算机语言设计上的4个高级话题。
第一，是元编程技术。元编程技术是一种对语言做扩展的技术，相当于能够定制一门语言，从而更好地解决特定领域的问题。Java语言的注解功能、Python的对象体系的设计，都体现了元编程功能。而Julia语言，更是集成了Lisp语言在元编程方面的强大能力。因此我会带你了解一下这些元编程技术的具体实现机制和特点，便于你去采纳和做好取舍。
第二，是泛型编程技术。泛型，或者说参数化类型，大大增强了现代语言的类型体系，使得很多计算逻辑的表达变得更简洁。最典型的应用就是容器类型，比如列表、树什么的，采用泛型技术实现的容器类型，能够方便地保存各种数据类型。像Java、C++和Julia等语言都支持泛型机制，但它们各自实现的技术又有所不同。我会带你了解这些不同实现技术背后的原因，以及各自的特点。
第三，是面向对象语言的实现机制。面向对象特性是当前很多主流语言都支持的特性。那么要在编译器和运行时上做哪些工作，来支持面向对象的特性呢？对象在内存里的表示都有哪些不同的方式？如何实现继承和多态的特性？为什么Java支持基础数据类型和对象类型，而有些语言中所有的数据都是对象？要在编译技术上做哪些工作来支持纯面向对象特性？这些问题，我会花一讲的时间来带你分析清楚，让你理解面向对象语言的底层机制。
第四，是函数式编程语言的实现机制。函数式编程这个范式出现得很早，不少人可能不太了解或者不太关注它，但最近几年出现了复兴的趋势。像Java等面向对象语言，也开始加入对函数式编程机制的支持。在第三个模块中，我会带你分析函数式编程的关键特征，比如函数作为一等公民、不变性等，并会一起探讨函数式编程语言实现上的一些关键技术，比如函数类型的内部表示、针对函数式编程特有的优化算法等，让你真正理解函数式编程语言的底层机制。
该模块的最后一讲，也是本课程的最后一讲，是对我们所学知识的一个综合检验。这个检验的题目，就是解析方舟编译器。
方舟编译器，应该是第一个引起国内IT界广泛关注的编译器。俗话说，外行看热闹，内行看门道。做一个编译器，到底有哪些关键的技术点？它们在方舟编译器里是如何体现的？我们在学习了编译原理的核心基础知识，在考察了多个编译器之后，应该能够有一定的能力去考察方舟编译器了。这也是学以致用、紧密结合实际的表现。通过这样的分析，你能了解到中国编译技术崛起的趋势，甚至还可能会思考如何参与到这个趋势中来。这一讲，我希望同学们都能发表自己的看法，而我的看法呢，只是一家之言，你作为参考就好了。
小结总结一下。咱们课程的名称是《编译原理实战课》，而最体现实战精神的，莫过于去实现一门计算机语言了。而在第三个模块，我就会带你解析实现一门计算机语言所要考虑的那些关键技术，并且通过学习，你也能够根据语言的设计目标来选择合适的技术方案。
从计算机语言设计的高度出发，这个模块会带你对编译原理形成更全面的认知，从而提高你把编译原理用于实战的能力。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>27_递归树：如何借助树来求解递归算法的时间复杂度？</title><link>https://artisanbox.github.io/2/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/28/</guid><description>今天，我们来讲这种数据结构的一种特殊应用，递归树。
我们都知道，递归代码的时间复杂度分析起来很麻烦。我们在第12节《排序（下）》那里讲过，如何利用递推公式，求解归并排序、快速排序的时间复杂度，但是，有些情况，比如快排的平均时间复杂度的分析，用递推公式的话，会涉及非常复杂的数学推导。
除了用递推公式这种比较复杂的分析方法，有没有更简单的方法呢？今天，我们就来学习另外一种方法，借助递归树来分析递归算法的时间复杂度。
递归树与时间复杂度分析我们前面讲过，递归的思想就是，将大问题分解为小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止。
如果我们把这个一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫作递归树。我这里画了一棵斐波那契数列的递归树，你可以看看。节点里的数字表示数据的规模，一个节点的求解可以分解为左右子节点两个问题的求解。
通过这个例子，你对递归树的样子应该有个感性的认识了，看起来并不复杂。现在，我们就来看，如何用递归树来求解时间复杂度。
归并排序算法你还记得吧？它的递归实现代码非常简洁。现在我们就借助归并排序来看看，如何用递归树，来分析递归代码的时间复杂度。
归并排序的原理我就不详细介绍了，如果你忘记了，可以回看一下第12节的内容。归并排序每次会将数据规模一分为二。我们把归并排序画成递归树，就是下面这个样子：
因为每次分解都是一分为二，所以代价很低，我们把时间上的消耗记作常量$1$。归并算法中比较耗时的是归并操作，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记作$n$。
现在，我们只需要知道这棵树的高度$h$，用高度$h$乘以每一层的时间消耗$n$，就可以得到总的时间复杂度$O(n*h)$。
从归并排序的原理和递归树，可以看出来，归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是$\log_{2}n$，所以，归并排序递归实现的时间复杂度就是$O(n\log n)$。我这里的时间复杂度都是估算的，对树的高度的计算也没有那么精确，但是这并不影响复杂度的计算结果。
利用递归树的时间复杂度分析方法并不难理解，关键还是在实战，所以，接下来我会通过三个实际的递归算法，带你实战一下递归的复杂度分析。学完这节课之后，你应该能真正掌握递归代码的复杂度分析。
实战一：分析快速排序的时间复杂度在用递归树推导之前，我们先来回忆一下用递推公式的分析方法。你可以回想一下，当时，我们为什么说用递推公式来求解平均时间复杂度非常复杂？
快速排序在最好情况下，每次分区都能一分为二，这个时候用递推公式$T(n)=2T(\frac{n}{2})+n$，很容易就能推导出时间复杂度是$O(n\log n)$。但是，我们并不可能每次分区都这么幸运，正好一分为二。
我们假设平均情况下，每次分区之后，两个分区的大小比例为$1:k$。当$k=9$时，如果用递推公式的方法来求解时间复杂度的话，递推公式就写成$T(n)=T(\frac{n}{10})+T(\frac{9n}{10})+n$。
这个公式可以推导出时间复杂度，但是推导过程非常复杂。那我们来看看，用递归树来分析快速排序的平均情况时间复杂度，是不是比较简单呢？
我们还是取$k$等于$9$，也就是说，每次分区都很不平均，一个分区是另一个分区的$9$倍。如果我们把递归分解的过程画成递归树，就是下面这个样子：
快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是$n$。我们现在只要求出递归树的高度$h$，这个快排过程遍历的数据个数就是 $h * n$ ，也就是说，时间复杂度就是$O(h * n)$。
因为每次分区并不是均匀地一分为二，所以递归树并不是满二叉树。这样一个递归树的高度是多少呢？
我们知道，快速排序结束的条件就是待排序的小区间，大小为$1$，也就是说叶子节点里的数据规模是$1$。从根节点$n$到叶子节点$1$，递归树中最短的一个路径每次都乘以$\frac{1}{10}$，最长的一个路径每次都乘以$\frac{9}{10}$。通过计算，我们可以得到，从根节点到叶子节点的最短路径是$\log_{10}n$，最长的路径是$\log_{\frac{10}{9}}n$。
所以，遍历数据的个数总和就介于$n\log_{10}n$和$n\log_{\frac{10}{9}}n$之间。根据复杂度的大O表示法，对数复杂度的底数不管是多少，我们统一写成$\log n$，所以，当分区大小比例是$1:9$时，快速排序的时间复杂度仍然是$O(n\log n)$。
刚刚我们假设$k=9$，那如果$k=99$，也就是说，每次分区极其不平均，两个区间大小是$1:99$，这个时候的时间复杂度是多少呢？
我们可以类比上面$k=9$的分析过程。当$k=99$的时候，树的最短路径就是$\log_{100}n$，最长路径是$\log_{\frac{100}{99}}n$，所以总遍历数据个数介于$n\log_{100}n$和$n\log_{\frac{100}{99}}n$之间。尽管底数变了，但是时间复杂度也仍然是$O(n\log n)$。
也就是说，对于$k$等于$9$，$99$，甚至是$999$，$9999$……，只要$k$的值不随$n$变化，是一个事先确定的常量，那快排的时间复杂度就是$O(n\log n)$。所以，从概率论的角度来说，快排的平均时间复杂度就是$O(n\log n)$。
实战二：分析斐波那契数列的时间复杂度在递归那一节中，我们举了一个跨台阶的例子，你还记得吗？那个例子实际上就是一个斐波那契数列。为了方便你回忆，我把它的代码实现贴在这里。
int f(int n) { if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2); } 这样一段代码的时间复杂度是多少呢？你可以先试着分析一下，然后再来看，我是怎么利用递归树来分析的。
我们先把上面的递归代码画成递归树，就是下面这个样子：
这棵递归树的高度是多少呢？
$f(n)$分解为$f(n-1)$和$f(n-2)$，每次数据规模都是$-1$或者$-2$，叶子节点的数据规模是$1$或者$2$。所以，从根节点走到叶子节点，每条路径是长短不一的。如果每次都是$-1$，那最长路径大约就是$n$；如果每次都是$-2$，那最短路径大约就是$\frac{n}{2}$。
每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作$1$。所以，从上往下，第一层的总时间消耗是$1$，第二层的总时间消耗是$2$，第三层的总时间消耗就是$2^{2}$。依次类推，第$k$层的时间消耗就是$2^{k-1}$，那整个算法的总的时间消耗就是每一层时间消耗之和。
如果路径长度都为$n$，那这个总和就是$2^{n}-1$。
如果路径长度都是$\frac{n}{2}$ ，那整个算法的总的时间消耗就是$2^{\frac{n}{2}}-1$。
所以，这个算法的时间复杂度就介于$O(2^{n})$和$O(2^{\frac{n}{2}})$之间。虽然这样得到的结果还不够精确，只是一个范围，但是我们也基本上知道了上面算法的时间复杂度是指数级的，非常高。
实战三：分析全排列的时间复杂度前面两个复杂度分析都比较简单，我们再来看个稍微复杂的。</description></item><item><title>27｜增加更丰富的类型第2步：如何支持字符串？</title><link>https://artisanbox.github.io/3/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/29/</guid><description>你好，我是宫文学。
今天我们继续来丰富我们语言的类型体系，让它能够支持字符串。字符串是我们在程序里最常用的数据类型之一。每一门高级语言，都需要对字符串类型的数据提供充分的支持。
但是，跟我们前面讨论过的整型和浮点型数据不同，在CPU层面并没有直接支持字符串运算的指令。所以，相比我们前面讲过的这两类数据类型，要让语言支持字符串，我们需要做更多的工作才可以。
那么，在这一节课里，我们就看看要支持字符串类型的话，我们语言需要做哪些工作。在这个过程中，我们会接触到对象内存布局、内置函数（Intrinsics），以及字符串、字面量的表示等知识点。
首先，我们来分析一下，在这种情况下，我们的编译器和运行时需要完成哪些任务，然后我们再依次完成它们就可以了。
任务分析你可以看到，在一些强调易用性的脚本语言里，字符串常常作为内置的数据类型，并拥有更高优先级的支持。比如，在JavaScript里，你可以用+号连接字符串，并且，其他数据类型和字符串连接时，也会自动转换成字符串。这比在Java、C等语言使用字符串更方便。
为了支持字符串类型，实现最基础的字符串操作功能，我们就需要解决下面这几个技术问题：
第一，如何在语言内部表示一个字符串？
在像JavaScript、Java、Go和C#这样的高级语言中，所有的数据类型可以分为两大类。一类是CPU在底层就支持的，就像整数和浮点数，我们一般叫做基础类型（Primitive Type）或者叫做值类型（Value Type）。
这些类型可以直接表示为指令的操作数，在赋值、传参的时候，也是直接传递值。比如，当我们声明一个number类型的变量时，我们在语言内部用CPU支持的双精度浮点数来存储变量的值就可以了。当给变量赋值的时候，我们也是把这个double值用mov指令拷贝过去就行。
但对于基础类型之外的复杂数据类型来说，它们并不能受到CPU指令级别的直接支持。所以，我们就需要设计，当我们声明一个字符串，以及给字符串赋值的时候，它对应的确切操作是什么。
那么计算机语言的设计者，通常会怎么做呢？我们要把这些复杂数据类型在内部实现成一个内存对象，而变量赋值、传参这样的操作，实际上传递的是对象的引用，对象引用能够转换为对象的内存地址。
所以，从今天这节课开始，我们也将正式支持对象机制。其实，string也好，数组也好，还是后面的自定义类型也好，它们在内存里都是一个对象。当进行赋值操作的时候，传递的都是对象的引用。那这个时候，我们就需要设计对象的内存结构，以及确定什么是对象的引用。
第二，在运行时里提供一些内置函数，用于支持字符串的基本功能。
为了支持字符串类型的数据，我们要能够支持字符串对象的创建、字符串拼接、其他类型的数据转为字符串，还有字符串的比较，等等功能。这些功能是以内置函数（intrincics）的形式来实现的。编译成汇编代码的时候，我们要调用这些内置函数来完成相应的功能。
第三，我们还要处理一些编译器后端的工作。
在编译器的后端方面，我们要能生成对字符串进行访问和处理的汇编代码。这里面的重点就是，我们要知道如何在汇编代码里表示字符串字面量，以及如何获取字符串字面量的地址。
好了，任务安排妥当了，我们开始行动吧。首先我们来看第一个任务，如何在语言内部表示一个字符串类型的数据。
如何表示一个字符串这个问题其实又包含三个子问题：字符编码的问题、string对象的内存布局，以及如何来表示一个对象的引用。
首先，我们看看字符的编码问题。
我们知道，CPU只知道0101这些值，并不知道abcd这些概念。实际上，是我们人类给每个字符编了码，让CPU来理解的。比如规定65代表大写字母a，97代表小写字母a，而48代表字符0，这就是广为使用的ASCII编码标准。但要支持像中文这么多的字符，ASCII标准还不够用，就需要Unicode这样的编码标准。
不过，在我们当前的实现中，我们还是先做一些简化吧，先不支持Unicode，只支持ASCII码就好了。这样，在内存里，我们只需要用一个字节来表示字符就行了，这跟C语言是一样的。至于Unicode，我们后面再支持。毕竟我们的语言PlayScript，是一个开源项目，会继续扩展功能。你也可以走在我前面，自己先去思考并实现一下怎么支持Unicode编码。
第二，我们看看string的内存布局。
如何在内存里表示一个字符串呢？
我们站在巨人的肩膀上，看看C语言是怎么做的。在C语言中，字符串在内存里就相当于一个char的数组，这个数组以0结尾。所以，“Hello”在内存里大概是这样保存的，加起来一共是6个字节：
我们也可以借鉴C语言的做法，用一个数组来表示字符串。不过，C语言需要程序员自己去处理字符串使用的内存：要么通过声明一个数组，在栈里申请内存；要么在堆里申请一块内存，使用完毕以后再手工释放掉。
而JavaScript是不需要程序员来手工管理内存的，而是采用了自动内存管理机制。自动内存管理机制管理的是一个个内存对象。当对象不再被使用以后，就可以被回收。
那么我们的设计，也必须实现自动的内存管理，因为TypeScript并没有底层的内存管理能力。
说到内存对象，我们还有一个设计目标，就是在语言内部，对各种类型的对象都有统一的管理机制，包括统一的内存管理机制、统一的运行时类型查询机制等等。这样，才能铺垫好TypeScript对象化的基础，并在后面实现更丰富的语言特性。所以，我们就需要对如何在内存里表示一个对象进行一下设计。
这方面，我们又可以参考一下其他语言是怎么做的。比如，在Java等语言里，对象都有一些统一的内存布局设计。其典型特征，就是每个对象都有一个固定的对象头，对象头之后才是对象的实际数据。
对象头里面保存了一些信息，用来对这个对象进行管理。进行哪些管理呢？首先是自动内存管理。对象头里有一些标志位，是用于垃圾收集程序的。比如，通过算法来标记某个对象是否是垃圾。我们在后面会具体实现一个垃圾收集算法，那个时候就会用到这些标志位。
标志位还有一个用途就是并发管理。你可以用一些特殊的指令，锁住一个对象，使得该对象在同一时间只可以被一个线程访问。在锁住对象的时候，也要在对象头做标识。此外，对象头里还有引用了类的定义，这样我们就可以在运行时知道这个对象属于哪个类，甚至通过反射等元编程机制去动态地调用对象的方法。
我们可以参考一下Java对象头的设计。它包含类指针和标志位两个部分。类指针指向类定义的地址。标志位就是内部分割成多个部分，用来存放与锁、垃圾收集等标记，还会存放对象的哈希值。
当然，其他语言的对象，也都有类似的内存布局设计。我在《编译原理实战课》中，对Java、Python和Julia等语言的对象内存布局都做了讨论，如果你感兴趣可以去看看。
参考这些设计，我们也可以做出自己的设计。在PlayScript中，我们首先设计一个Object对象，里面有一个标志位的字段和一个指向对象的类定义的指针。我们后面再探讨它们的用途。
//所有对象的对象头。目前的设计占用16个字节。 typedef struct _Object{ //指向类的指针 struct _Object * ptrKlass; //与并发、垃圾收集有关的标志位。 unsigned long flags; }Object; 所有对象都要继承自Object对象，字符串对象也不例外。我们把字符串对象叫做PlayString，其数据结构中包含了字符串的长度。真实的字符串数据是接在PlayString之后的。而且，我们基于PlayString的地址，就能计算出字符串的存储位置，所以并不需要一个单独的指针，这样也就节省了内存空间。
typedef struct _PlayString{ Object object; //字符串的长度 size_t length;
//后面跟以0结尾的字符串，以便复用C语言的一些功能。实际占用内存是length+1。 //我们不需要保存这个指针，只需要在PlayString对象地址的基础上增加一个偏移量就行。 //char* data; }PlayString; 采用这个结构后，实际上PlayString的内存布局如下。对象头占16个字节，字符串长度占4个字节，其余的才是字符串数据，占用空间的大小是字符串的长度再加1个字节：
不过，在这里，我们还有一个技术细节需要做一下决策。</description></item><item><title>28_前端总结：语言设计也有人机工程学</title><link>https://artisanbox.github.io/7/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/28/</guid><description>你好，我是宫文学。
正如我在上一讲的“课程导读”中所提到的，在“现代语言设计篇”，我们会开始探讨现代语言设计中的一些典型特性，包括前端、中后端、运行时的特性等，并会研究它们与编译技术的关系。
今天这一讲，我先以前面的“真实编译器解析篇”所分析的7种编译器作为基础，来总结一下它们的前端技术的特征，为你以后的前端工作做好清晰的指引。
在此基础上，我们还会进一步讨论语言设计方面的问题。近些年，各种新语言都涌现出了一个显著特征，那就是越来越考虑对程序员的友好性，运用了人机工程的思维。比如说，自动类型推导、Null安全性等。那么在这里，我们就一起来分析一下，要支持这些友好的语法特征，在编译技术上都要做一些什么工作。
好，首先，我们就来总结一下各个编译器的前端技术特征。
前端编译技术总结通过前面课程中对7个编译器的解读分析，我们现在已经知道了，编译器的前端有一些共性的特征，包括：手写的词法分析器、自顶向下分析为主的语法分析器和差异化的语义分析功能。
手写的词法分析器我们分析的这几个编译器，全部都采用了手写的词法分析器。主要原因有几个：
第一，手写的词法分析实现起来比较简单，再加上每种语言的词法规则实际上是大同小异的，所以实现起来也都差不多。 第二，手写词法分析器便于做一些优化。典型的优化是把关键字作为标识符的子集来识别，而不用为识别每个关键字创建自动机。V8的词法分析器还在性能上做了调优，比如判断一个字符是否是合法的标识符字符，是采用了查表的方法，以空间换性能，提高了解析速度。 第三，手写词法分析器便于处理一些特殊的情况。在 MySQL的词法分析器中，我们会发现，它需要根据当前字符集来确定某个字符串是否是合法的Token。如果采用工具自动生成词法分析器，则不容易处理这种情况。 结论：如果你要实现词法分析器，可以参考这些编译器，来实现你自己手写的版本。
自顶向下分析为主的语法分析器在“解析篇”中，我们还见到了多个语法分析器。
手写 vs 工具生成
在前面解析的编译器当中，大部分都是手写的语法分析器，只有Python和MySQL这两个是用工具生成的。
一方面，手写实现能够在某些地方做一些优化的实现，比如在Java语言里，我们可以根据需要，预读一到多个Token。另外，手写实现也有利于编译错误的处理，这样可以尽量给用户提供更友好的编译错误信息，并且当一个地方发生错误以后，也能尽量不影响对后面的语句的解析。手写的语法分析器在这些方面都能提供更好的灵活性。
另一方面，Python和MySQL的编译器也证明了，用工具生成的语法分析器，也是完全可以用于高要求的产品之中的。所以，如果你的项目时间和资源有限，你要优先考虑用工具生成语法分析器。
自顶向下 vs 自底向上
我们知道，语法分析有两大算法体系。一是自顶向下，二是自底向上。
从我们分析过的7种编译器里可以发现，自顶向下的算法体系占了绝对的主流，只有MySQL的语法分析器，采用的是自底向上的LALR算法。
而在自顶向下的算法中，又几乎全是采用了递归下降算法，Java、JavaScript和Go三大语言的编译器都是如此。并且对于左递归这个技术点，我们用标准的改写方法就可以解决。
不过，我们还看到了自顶向下算法和自底向上算法的融合。Java语言和Go语言在处理二元表达式时，引入了运算符优先级解析器，从而避免了左递归问题，并且在处理优先级和结合性的问题上，也会更加容易。而运算符优先级解析器，实际上采用的是一种LR算法。
差异化的语义分析功能不同编译器的语义分析功能有其共性，那就是都要建立符号表、做引用消解。对于静态类型的语言来说，还一定要做类型检查。
语义分析最大的特点是上下文相关，AST加上这些上下文相关的关系，就从树变成了图。由于处理图的算法一般比较复杂，这就给引用消解带来了困难，因此我们在算法上必须采用一定的启发式规则，让算法简化。
比如，我们可以先把类型加入符号表，再去消解用到这些类型的地方：变量声明、方法声明、类继承的声明，等等。你还需要注意的是，在消解本地变量的时候，还必须一边消解，一边把本地变量加入符号表，这样才能避免形成错误的引用关系。
不过，在建立符号表，并做完引用消解以后，上下文相关导致的复杂性就被消除了。所以，后续的语义分析算法，我们仍然可以通过简单地遍历AST来实现。所以，你会看到这些编译器当中，大量的算法都是实现了Visitor模式。
另外，除了建立符号表、做引用消解和类型检查等语义分析功能，不同的编译器还要去处理自己特有的语义。比如说，Java编译器花了很多的工作量在处理语法糖上，还有对注解的处理上；Julia的编译器会去做类型推断；Python的编译器会去识别变量的作用域范围，等等。
这其中，很多的语义处理功能，都是为了支持更加友好的语言特性，比如Java的语法糖。在现代语言中，还增加了很多的特性，能够让程序员的编程工作更加容易。接下来，我就挑几个共性的特性，跟你一起探讨一下它们的实现。
支持友好的语言特性自动类型推导、Null安全性、通过语法糖提高语法的友好性，以及提供一些友好的词法规则，等等。这些都是现代语言努力提高其友好性的表现。
自动类型推导自动类型推导可以减少编程时与类型声明有关的工作量。我们来看看下面这几门语言，都是如何声明变量的。
C++语言是一门不断与时俱进的语言。在C++ 11中，采用了auto关键字做类型推导。比如：
int a = 10; auto b = a; //能够自动推导b的类型是int cout &amp;lt;&amp;lt; typeid(b).name() &amp;lt;&amp;lt; endl; //输出int 你可能会觉得，这看上去似乎也没啥呀，把int换成了auto好像并没有省多少事儿。但在下面这个例子中，你会发现用于枚举的变量的类型很长（std::vector&amp;lt;std::string&amp;gt;::iterator），那么你就大可以直接用一个auto来代替，省了很多事，代码也更加整洁。所以实际上，auto关键字也成为了在C++中使用枚举器的标准用法：
std::vector&amp;lt;std::string&amp;gt; vs; for(std::vector&amp;lt;std::string&amp;gt;::iterator i=vs.begin(); i!=vs.end();i++){ //... } //使用auto以后，简化为： fora(auto i=vs.begin(); i!=vs.end();i++){ //... } 我们接着来看看其他的语言，都是如何做类型推导的。
Kotlin中用var声明变量，也支持显式类型声明和类型推导两种方式。
var a : Int = 10; //显式声明 var b = 10; //类型推导 Go语言，会用“:=” 让编译器去做类型推导：</description></item><item><title>28_堆和堆排序：为什么说堆排序没有快速排序快？</title><link>https://artisanbox.github.io/2/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/29/</guid><description>我们今天讲另外一种特殊的树，“堆”（$Heap$）。堆这种数据结构的应用场景非常多，最经典的莫过于堆排序了。堆排序是一种原地的、时间复杂度为$O(n\log n)$的排序算法。
前面我们学过快速排序，平均情况下，它的时间复杂度为$O(n\log n)$。尽管这两种排序算法的时间复杂度都是$O(n\log n)$，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？
现在，你可能还无法回答，甚至对问题本身还有点疑惑。没关系，带着这个问题，我们来学习今天的内容。等你学完之后，或许就能回答出来了。
如何理解“堆”？前面我们提到，堆是一种特殊的树。我们现在就来看看，什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。
堆是一个完全二叉树；
堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。
我分别解释一下这两点。
第一点，堆必须是一个完全二叉树。还记得我们之前讲的完全二叉树的定义吗？完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。
第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。
对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫做“小顶堆”。
定义解释清楚了，你来看看，下面这几个二叉树是不是堆？
其中第$1$个和第$2$个是大顶堆，第$3$个是小顶堆，第$4$个不是堆。除此之外，从图中还可以看出来，对于同一组数据，我们可以构建多种不同形态的堆。
如何实现一个堆？要实现一个堆，我们先要知道，堆都支持哪些操作以及如何存储一个堆。
我之前讲过，完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。
我画了一个用数组存储堆的例子，你可以先看下。
从图中我们可以看到，数组中下标为$i$的节点的左子节点，就是下标为$i*2$的节点，右子节点就是下标为$i*2+1$的节点，父节点就是下标为$\frac{i}{2}$的节点。
知道了如何存储一个堆，那我们再来看看，堆上的操作有哪些呢？我罗列了几个非常核心的操作，分别是往堆中插入一个元素和删除堆顶元素。（如果没有特殊说明，我下面都是拿大顶堆来讲解）。
1.往堆中插入一个元素往堆中插入一个元素后，我们需要继续满足堆的两个特性。
如果我们把新插入的元素放到堆的最后，你可以看我画的这个图，是不是不符合堆的特性了？于是，我们就需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做堆化（heapify）。
堆化实际上有两种，从下往上和从上往下。这里我先讲从下往上的堆化方法。
堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。
我这里画了一张堆化的过程分解图。我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。
我将上面讲的往堆中插入数据的过程，翻译成了代码，你可以结合着一块看。
public class Heap { private int[] a; // 数组，从下标1开始存储数据 private int n; // 堆可以存储的最大数据个数 private int count; // 堆中已经存储的数据个数 public Heap(int capacity) { a = new int[capacity + 1]; n = capacity; count = 0; }
public void insert(int data) { if (count &amp;gt;= n) return; // 堆满了 ++count; a[count] = data; int i = count; while (i/2 &amp;gt; 0 &amp;amp;&amp;amp; a[i] &amp;gt; a[i/2]) { // 自下往上堆化 swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素 i = i/2; } } } 2.</description></item><item><title>28_异常和中断：程序出错了怎么办？</title><link>https://artisanbox.github.io/4/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/28/</guid><description>过去这么多讲，我们的程序都是自动运行且正常运行的。自动运行的意思是说，我们的程序和指令都是一条条顺序执行，你不需要通过键盘或者网络给这个程序任何输入。正常运行是说，我们的程序都是能够正常执行下去的，没有遇到计算溢出之类的程序错误。
不过，现实的软件世界可没有这么简单。一方面，程序不仅是简单的执行指令，更多的还需要和外部的输入输出打交道。另一方面，程序在执行过程中，还会遇到各种异常情况，比如除以0、溢出，甚至我们自己也可以让程序抛出异常。
那这一讲，我就带你来看看，如果遇到这些情况，计算机是怎么运转的，也就是说，计算机究竟是如何处理异常的。
异常：硬件、系统和应用的组合拳一提到计算机当中的异常（Exception），可能你的第一反应就是C++或者Java中的Exception。不过我们今天讲的，并不是这些软件开发过程中遇到的“软件异常”，而是和硬件、系统相关的“硬件异常”。
当然，“软件异常”和“硬件异常”并不是实际业界使用的专有名词，只是我为了方便给你说明，和C++、Java中软件抛出的Exception进行的人为区分，你明白这个意思就好。
尽管，这里我把这些硬件和系统相关的异常，叫作“硬件异常”。但是，实际上，这些异常，既有来自硬件的，也有来自软件层面的。
比如，我们在硬件层面，当加法器进行两个数相加的时候，会遇到算术溢出；或者，你在玩游戏的时候，按下键盘发送了一个信号给到CPU，CPU要去执行一个现有流程之外的指令，这也是一个“异常”。
同样，来自软件层面的，比如我们的程序进行系统调用，发起一个读文件的请求。这样应用程序向系统调用发起请求的情况，一样是通过“异常”来实现的。
关于异常，最有意思的一点就是，它其实是一个硬件和软件组合到一起的处理过程。异常的前半生，也就是异常的发生和捕捉，是在硬件层面完成的。但是异常的后半生，也就是说，异常的处理，其实是由软件来完成的。
计算机会为每一种可能会发生的异常，分配一个异常代码（Exception Number）。有些教科书会把异常代码叫作中断向量（Interrupt Vector）。异常发生的时候，通常是CPU检测到了一个特殊的信号。比如，你按下键盘上的按键，输入设备就会给CPU发一个信号。或者，正在执行的指令发生了加法溢出，同样，我们可以有一个进位溢出的信号。这些信号呢，在组成原理里面，我们一般叫作发生了一个事件（Event）。CPU在检测到事件的时候，其实也就拿到了对应的异常代码。
这些异常代码里，I/O发出的信号的异常代码，是由操作系统来分配的，也就是由软件来设定的。而像加法溢出这样的异常代码，则是由CPU预先分配好的，也就是由硬件来分配的。这又是另一个软件和硬件共同组合来处理异常的过程。
拿到异常代码之后，CPU就会触发异常处理的流程。计算机在内存里，会保留一个异常表（Exception Table）。也有地方，把这个表叫作中断向量表（Interrupt Vector Table），好和上面的中断向量对应起来。这个异常表有点儿像我们在第10讲里讲的GOT表，存放的是不同的异常代码对应的异常处理程序（Exception Handler）所在的地址。
我们的CPU在拿到了异常码之后，会先把当前的程序执行的现场，保存到程序栈里面，然后根据异常码查询，找到对应的异常处理程序，最后把后续指令执行的指挥权，交给这个异常处理程序。
这样“检测异常，拿到异常码，再根据异常码进行查表处理”的模式，在日常开发的过程中是很常见的。
比如说，现在我们日常进行的Web或者App开发，通常都是前后端分离的。前端的应用，会向后端发起HTTP的请求。当后端遇到了异常，通常会给到前端一个对应的错误代码。前端的应用根据这个错误代码，在应用层面去进行错误处理。在不能处理的时候，它会根据错误代码向用户显示错误信息。
public class LastChanceHandler implements Thread.UncaughtExceptionHandler { @Override public void uncaughtException(Thread t, Throwable e) { // do something here - log to file and upload to server/close resources/delete files... } } Thread.setDefaultUncaughtExceptionHandler(new LastChanceHandler()); Java里面，可以设定ExceptionHandler，来处理线程执行中的异常情况再比如说，Java里面，我们使用一个线程池去运行调度任务的时候，可以指定一个异常处理程序。对于各个线程在执行任务出现的异常情况，我们是通过异常处理程序进行处理，而不是在实际的任务代码里处理。这样，我们就把业务处理代码就和异常处理代码的流程分开了。
异常的分类：中断、陷阱、故障和中止我在前面说了，异常可以由硬件触发，也可以由软件触发。那我们平时会碰到哪些异常呢？下面我们就一起来看看。
第一种异常叫中断（Interrupt）。顾名思义，自然就是程序在执行到一半的时候，被打断了。这个打断执行的信号，来自于CPU外部的I/O设备。你在键盘上按下一个按键，就会对应触发一个相应的信号到达CPU里面。CPU里面某个开关的值发生了变化，也就触发了一个中断类型的异常。
第二种异常叫陷阱（Trap）。陷阱，其实是我们程序员“故意“主动触发的异常。就好像你在程序里面打了一个断点，这个断点就是设下的一个&amp;quot;陷阱&amp;quot;。当程序的指令执行到这个位置的时候，就掉到了这个陷阱当中。然后，对应的异常处理程序就会来处理这个&amp;quot;陷阱&amp;quot;当中的猎物。
最常见的一类陷阱，发生在我们的应用程序调用系统调用的时候，也就是从程序的用户态切换到内核态的时候。我们在第3讲讲CPU性能的时候说过，可以用Linux下的time指令，去查看一个程序运行实际花费的时间，里面有在用户态花费的时间（user time），也有在内核态发生的时间（system time）。
我们的应用程序通过系统调用去读取文件、创建进程，其实也是通过触发一次陷阱来进行的。这是因为，我们用户态的应用程序没有权限来做这些事情，需要把对应的流程转交给有权限的异常处理程序来进行。
第三种异常叫故障（Fault）。它和陷阱的区别在于，陷阱是我们开发程序的时候刻意触发的异常，而故障通常不是。比如，我们在程序执行的过程中，进行加法计算发生了溢出，其实就是故障类型的异常。这个异常不是我们在开发的时候计划内的，也一样需要有对应的异常处理程序去处理。
故障和陷阱、中断的一个重要区别是，故障在异常程序处理完成之后，仍然回来处理当前的指令，而不是去执行程序中的下一条指令。因为当前的指令因为故障的原因并没有成功执行完成。
最后一种异常叫中止（Abort）。与其说这是一种异常类型，不如说这是故障的一种特殊情况。当CPU遇到了故障，但是恢复不过来的时候，程序就不得不中止了。
在这四种异常里，中断异常的信号来自系统外部，而不是在程序自己执行的过程中，所以我们称之为“异步”类型的异常。而陷阱、故障以及中止类型的异常，是在程序执行的过程中发生的，所以我们称之为“同步“类型的异常。
在处理异常的过程当中，无论是异步的中断，还是同步的陷阱和故障，我们都是采用同一套处理流程，也就是上面所说的，“保存现场、异常代码查询、异常处理程序调用“。而中止类型的异常，其实是在故障类型异常的一种特殊情况。当故障发生，但是我们发现没有异常处理程序能够处理这种异常的情况下，程序就不得不进入中止状态，也就是最终会退出当前的程序执行。
异常的处理：上下文切换在实际的异常处理程序执行之前，CPU需要去做一次“保存现场”的操作。这个保存现场的操作，和我在第7讲里讲解函数调用的过程非常相似。
因为切换到异常处理程序的时候，其实就好像是去调用一个异常处理函数。指令的控制权被切换到了另外一个&amp;quot;函数&amp;quot;里面，所以我们自然要把当前正在执行的指令去压栈。这样，我们才能在异常处理程序执行完成之后，重新回到当前的指令继续往下执行。
不过，切换到异常处理程序，比起函数调用，还是要更复杂一些。原因有下面几点。</description></item><item><title>28_手把手带你设计一个完整的连锁超市信息系统数据库（下）</title><link>https://artisanbox.github.io/8/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/28/</guid><description>你好，我是朱晓峰。
上节课，我们完成了项目的需求分析和业务流程的梳理，为设计数据库做好了准备工作，接下来我们就可以开始具体的设计了。所以，今天，我就带你来建库建表、创建外键约束、视图、存储过程和触发器，最后制定容灾和备份的策略，从而完成一个完整的连锁超市项目数据库的设计，帮助你提高设计高效可靠的数据库的能力。
首先，我们一起来创建数据库和数据表。
如何创建数据库和数据表？经过上节课的分库分表操作，我们把数据库按照业务模块，拆分成了多个数据库。其中，盘点模块中的数据表分别被拆分到了营运数据库（operation）和库存数据库（inventory）中。
下面我们就按照上节课的分库策略，分别创建营运数据库和库存数据库：
mysql&amp;gt; CREATE DATABASE operation; Query OK, 1 row affected (0.03 sec) mysql&amp;gt; CREATE DATABASE inventory; Query OK, 1 row affected (0.02 sec) 接下来，我们来分别创建下这两个数据库中的表。
商户表、门店表、员工表、商品常用信息表和商品不常用信息表从属于营运数据库，我们先把这5个表创建出来。
商户表（operation.enterprice）：
mysql&amp;gt; CREATE TABLE operation.enterprice -&amp;gt; ( -&amp;gt; groupnumber SMALLINT PRIMARY KEY, &amp;ndash; 组号 -&amp;gt; groupname VARCHAR(100) NOT NULL, &amp;ndash; 名称 -&amp;gt; address TEXT NOT NULL, &amp;ndash; 地址 -&amp;gt; phone VARCHAR(20) NOT NULL, &amp;ndash; 电话 -&amp;gt; contactor VARCHAR(50) NOT NULL &amp;ndash; 联系人 -&amp;gt; ); Query OK, 0 rows affected (0.</description></item><item><title>28_数据流分析：你写的程序，它更懂</title><link>https://artisanbox.github.io/6/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/28/</guid><description>上一讲，我提到了删除公共子表达式、拷贝传播等本地优化能做的工作，其实，这几个工作也可以在全局优化中进行。
只不过，全局优化中的算法，不会像在本地优化中一样，只针对一个基本块。而是更复杂一些，因为要覆盖多个基本块。这些基本块构成了一个CFG，代码在运行时有多种可能的执行路径，这会造成多路径下，值的计算问题，比如活跃变量集合的计算。
当然了，还有些优化只能在全局优化中做，在本地优化中做不了，比如：
代码移动（code motion）能够将代码从一个基本块挪到另一个基本块，比如从循环内部挪到循环外部，来减少不必要的计算。 部分冗余删除（Partial Redundancy Elimination），它能把一个基本块都删掉。 总之，全局优化比本地优化能做的工作更多，分析算法也更复杂，因为CFG中可能存在多条执行路径。不过，我们可以在上一节课提到的本地优化的算法思路上，解决掉多路径情况下，V值的计算问题。而这种基于CFG做优化分析的方法框架，就叫做数据流分析。
本节课，我会把全局优化的算法思路讲解清楚，借此引入数据流分析的完整框架。而且在解决多路径情况下，V值的计算问题时，我还会带你学习一个数学工具：半格理论。这样，你会对基于数据流分析的代码优化思路建立清晰的认识，从而有能力根据需要编写自己的优化算法。
数据流分析的场景：活跃性分析上一讲，我已经讲了本地优化时的活跃性分析，那时，情况比较简单，你不需要考虑多路径问题。而在做全局优化时，情况就要复杂一些：代码不是在一个基本块里简单地顺序执行，而可能经过控制流图（CFG）中的多条路径。我们来看一个例子（例子由if语句形成了两条分支语句）：
基于这个CFG，我们可以做全局的活跃性分析，从最底下的基本块开始，倒着向前计算活跃变量的集合（也就是从基本块5倒着向基本块1计算）。
这里需要注意，对基本块1进行计算的时候，它的输入是基本块2的输出，也就是{a, b, c}，和基本块3的输出，也就是{a, c}，计算结果是这两个集合的并集{a, b, c}。也就是说，基本块1的后序基本块，有可能用到这三个变量。这里就是与本地优化不同的地方，我们要基于多条路径来计算。
基于这个分析图，我们马上发现y变量可以被删掉（因为它前面的活变量集合{x}不包括y，也就是不被后面的代码所使用），并且影响到了活跃变量的集合。
删掉y变量以后，再继续优化一轮，会发现d也可以删掉。
d删掉以后，2号基本块里面已经没有代码了，也可以被删掉，最后的CFG是下面这样：
到目前为止，我们发现：全局优化总体来说跟本地优化很相似，唯一的不同，就是要基于多个分支计算集合的内容（也就是V值）。在进入基本块1时，2和3两个分支相遇（meet），我们取了2和3V值的并集。这就是数据流分析的基本特征，你可以记住这个例子，建立直观印象。
但是，上面这个CFG还是比较简单的，因为它没有循环，属于有向无环图。这种图的特点是：针对图中的每一个节点，我们总能找到它的前序节点和后序节点，所以我们只需要按照顺序计算就好了。但是如果加上了环路，就不那么简单了，来看一看下面这张图：
基本块4有两个后序节点，分别是5和1，所以要计算4的活跃变量，就需要知道5和1的输出是什么。5的输出好说，但1的呢？还没计算出来呢。因为要计算1，就要依赖2和3，从而间接地又依赖了4。这样一来，1和4是循环依赖的。再进一步探究的话，你发现其实1、2、3、4四个节点之间，都是循环依赖的。
所以说，一旦在CFG中引入循环回路，严格的前后计算顺序就不存在了。那你要怎么办呢？
其实，我们不是第一次面对这个处境了。在前端部分，我们计算First和Follow集合的时候，就会遇到循环依赖的情况，只不过那时候没有像这样展开，细细地分析。不过，你可以回顾一下17讲和18讲，那个时候你是用什么算法来破解僵局的呢？是不动点法。在这里，我们还是要运用不动点法，具体操作是：给每个基本块的V值都分配初始值，也就是空集合。
然后对所有节点进行多次计算，直到所有集合都稳定为止。第一遍的时候，我们按照5-4-3-2-1的顺序计算（实际上，采取任何顺序都可以），计算结果如下：
如果现在计算就结束，我们实际上可以把基本块2中的d变量删掉。但如果我们再按照5-4-3-2-1的顺序计算一遍，就会往集合里增加一些新的元素（在图中标的是橙色）。这是因为，在计算基本块4的时候，基本块1的输出{b, c, d}也会变成4的输入。这时，我们发现，进入基本块2时，活变量集合里是含有d的，所以d是不能删除的。
你再仔细看看，这个d是哪里需要的呢？是基本块3需要的：它会跟1去要，1会跟4要，4跟2要。所以，再次证明，1、2、3、4四个节点是互相依赖的。
我们再来看一下，对于活变量集合的计算，当两个分支相遇的情况下，最终的结果我们取了两个分支的并集。
在上一讲，我们说一个本地优化分析包含四个元素：方向（D）、值（V）、转换函数（F）和初始值（I）。在做全局优化的时候，我们需要再多加一个元素，就是两个分支相遇的时候，要做一个运算，计算他们相交的值，这个运算我们可以用大写的希腊字母Λ（lambda）表示。包含了D、V、F、I和Λ的分析框架，就叫做数据流分析。
那么Λ怎么计算呢？研究者们用了一个数学工具，叫做“半格”（Semilattice），帮助做Λ运算。
直观地理解半格理论如果要从数学理论角度完全把“半格”这个概念说清楚，需要依次介绍清楚“格”（Lattice）、“半格”（Semilattice）和“偏序集”（Partially Ordered Set）等概念。我想这个可以作为爱好数学的同学的一个研究题目，或者去向离散数学的老师求教。在我们的课程里，我只是通过举例子，让你对它有直观的认识。
首先，半格是一种偏序集。偏序集就是集合中只有部分成员能够互相比较大小。举例来说会比较直观。在做全局活跃性分析的时候，{a, b, c}和{a, c}相遇，产生的新值是{a, b, c}。我们形式化地写成{a, b, c} Λ {a, c} = {a, b, c}。
这时候我们说{a, b, c}是可以跟{a, c}比较大小的。那么哪个大哪个小呢？
如果XΛY=X，我们说X&amp;lt;=Y。
所以，{a, b, c}是比较小的，{a, c}是比较大的。
当然，{a, b, c}也可以跟{a, b}比较大小，但它没有办法跟{c, d}比较大小。所以把包含了{{a, b, c}、{a, c}、{a, b}、{c, d}…}这样的一个集合，叫做偏序集，它们中只有部分成员之间可以比较大小。哪些成员可以比较呢？就是下面的半格图中，可以通过有方向的线连起来的。</description></item><item><title>28_读写分离有哪些坑？</title><link>https://artisanbox.github.io/1/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/28/</guid><description>在上一篇文章中，我和你介绍了一主多从的结构以及切换流程。今天我们就继续聊聊一主多从架构的应用场景：读写分离，以及怎么处理主备延迟导致的读写分离问题。
我们在上一篇文章中提到的一主多从的结构，其实就是读写分离的基本结构了。这里，我再把这张图贴过来，方便你理解。
图1 读写分离基本结构读写分离的主要目标就是分摊主库的压力。图1中的结构是客户端（client）主动做负载均衡，这种模式下一般会把数据库的连接信息放在客户端的连接层。也就是说，由客户端来选择后端数据库进行查询。
还有一种架构是，在MySQL和客户端之间有一个中间代理层proxy，客户端只连接proxy， 由proxy根据请求类型和上下文决定请求的分发路由。
图2 带proxy的读写分离架构接下来，我们就看一下客户端直连和带proxy的读写分离架构，各有哪些特点。
客户端直连方案，因为少了一层proxy转发，所以查询性能稍微好一点儿，并且整体架构简单，排查问题更方便。但是这种方案，由于要了解后端部署细节，所以在出现主备切换、库迁移等操作的时候，客户端都会感知到，并且需要调整数据库连接信息。
你可能会觉得这样客户端也太麻烦了，信息大量冗余，架构很丑。其实也未必，一般采用这样的架构，一定会伴随一个负责管理后端的组件，比如Zookeeper，尽量让业务端只专注于业务逻辑开发。
带proxy的架构，对客户端比较友好。客户端不需要关注后端细节，连接维护、后端信息维护等工作，都是由proxy完成的。但这样的话，对后端维护团队的要求会更高。而且，proxy也需要有高可用架构。因此，带proxy架构的整体就相对比较复杂。
理解了这两种方案的优劣，具体选择哪个方案就取决于数据库团队提供的能力了。但目前看，趋势是往带proxy的架构方向发展的。
但是，不论使用哪种架构，你都会碰到我们今天要讨论的问题：由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态。
这种“在从库上会读到系统的一个过期状态”的现象，在这篇文章里，我们暂且称之为“过期读”。
前面我们说过了几种可能导致主备延迟的原因，以及对应的优化策略，但是主从延迟还是不能100%避免的。
不论哪种结构，客户端都希望查询从库的数据结果，跟查主库的数据结果是一样的。
接下来，我们就来讨论怎么处理过期读问题。
这里，我先把文章中涉及到的处理过期读的方案汇总在这里，以帮助你更好地理解和掌握全文的知识脉络。这些方案包括：
强制走主库方案； sleep方案； 判断主备无延迟方案； 配合semi-sync方案； 等主库位点方案； 等GTID方案。 强制走主库方案强制走主库方案其实就是，将查询请求做分类。通常情况下，我们可以将查询请求分为这么两类：
对于必须要拿到最新结果的请求，强制将其发到主库上。比如，在一个交易平台上，卖家发布商品以后，马上要返回主页面，看商品是否发布成功。那么，这个请求需要拿到最新的结果，就必须走主库。
对于可以读到旧数据的请求，才将其发到从库上。在这个交易平台上，买家来逛商铺页面，就算晚几秒看到最新发布的商品，也是可以接受的。那么，这类请求就可以走从库。
你可能会说，这个方案是不是有点畏难和取巧的意思，但其实这个方案是用得最多的。
当然，这个方案最大的问题在于，有时候你会碰到“所有查询都不能是过期读”的需求，比如一些金融类的业务。这样的话，你就要放弃读写分离，所有读写压力都在主库，等同于放弃了扩展性。
因此接下来，我们来讨论的话题是：可以支持读写分离的场景下，有哪些解决过期读的方案，并分析各个方案的优缺点。
Sleep 方案主库更新后，读从库之前先sleep一下。具体的方案就是，类似于执行一条select sleep(1)命令。
这个方案的假设是，大多数情况下主备延迟在1秒之内，做一个sleep可以有很大概率拿到最新的数据。
这个方案给你的第一感觉，很可能是不靠谱儿，应该不会有人用吧？并且，你还可能会说，直接在发起查询时先执行一条sleep语句，用户体验很不友好啊。
但，这个思路确实可以在一定程度上解决问题。为了看起来更靠谱儿，我们可以换一种方式。
以卖家发布商品为例，商品发布后，用Ajax（Asynchronous JavaScript + XML，异步JavaScript和XML）直接把客户端输入的内容作为“新的商品”显示在页面上，而不是真正地去数据库做查询。
这样，卖家就可以通过这个显示，来确认产品已经发布成功了。等到卖家再刷新页面，去查看商品的时候，其实已经过了一段时间，也就达到了sleep的目的，进而也就解决了过期读的问题。
也就是说，这个sleep方案确实解决了类似场景下的过期读问题。但，从严格意义上来说，这个方案存在的问题就是不精确。这个不精确包含了两层意思：
如果这个查询请求本来0.5秒就可以在从库上拿到正确结果，也会等1秒；
如果延迟超过1秒，还是会出现过期读。
看到这里，你是不是有一种“你是不是在逗我”的感觉，这个改进方案虽然可以解决类似Ajax场景下的过期读问题，但还是怎么看都不靠谱儿。别着急，接下来我就和你介绍一些更准确的方案。
判断主备无延迟方案要确保备库无延迟，通常有三种做法。
通过前面的第25篇文章，我们知道show slave status结果里的seconds_behind_master参数的值，可以用来衡量主备延迟时间的长短。
所以第一种确保主备无延迟的方法是，每次从库执行查询请求前，先判断seconds_behind_master是否已经等于0。如果还不等于0 ，那就必须等到这个参数变为0才能执行查询请求。
seconds_behind_master的单位是秒，如果你觉得精度不够的话，还可以采用对比位点和GTID的方法来确保主备无延迟，也就是我们接下来要说的第二和第三种方法。
如图3所示，是一个show slave status结果的部分截图。</description></item><item><title>28_部门分类：如何表示设备类型与设备驱动？</title><link>https://artisanbox.github.io/9/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/28/</guid><description>你好，我是LMOS。
小到公司，大到国家，都有各种下属部门，比如我们国家现在有教育部、科学技术部、外交部，财政部等，这些部门各自负责完成不同的职能工作，如教育部负责教育事业和语言文字工作，科学技术部负责推动解决经济社会发展的重大科技问题。
既然大道相通，那我们的Cosmos中是否也是类似这样的结构呢？
答案是肯定的，在前面的课中，我们搞定了内存管理和进程管理，它们是内核不可分隔的，但是计算机中还有各种类型的设备需要管理。
我们的Cosmos也会“成立各类部门”，用于管理众多设备，一个部门负责一类设备。具体要怎么管理设备呢？你不妨带着这个问题，正式开始今天的学习！
这节课的代码，你可以从这里下载。
计算机的结构不知道你是否和我一样，经常把计算机的机箱打开，看看 CPU，看看内存条，看看显卡，看看主板上的各种芯片。
其实，这些芯片并非独立存在，而是以总线为基础连接在一起的，各自完成自己的工作，又能互相打配合，共同实现用户要求的功能。
为了帮你理清它们的连接关系，我为你画了一幅图，如下所示。
上图是一个典型的桌面系统，你先不用管是物理上怎么样连接的，逻辑上就是这样的。实际可能比图中有更多或者更少的总线。但是总线有层级关系，各种设备通过总线相连。这里我们只需要记住，计算机中有很多种类的设备，脑中有刚才这幅图就行了。
如何管理设备在前面的课程中，我们实现了管理内存和进程，其实进程从正面看它是管理应用程序的，反过来看它也是管理CPU的，它能使CPU的使用率达到最高。
管理内存和管理CPU是操作系统最核心的部分，但是这还不够，因为计算机不止有CPU，还有各种设备。
如果把计算机内部所有的设备和数据都描述成资源，操作系统内核无疑是这些资源的管理者。既然设备也是一种资源，如何高效管理它们，以便提供给应用进程使用和操作，就是操作系统内核的重要任务。
分权而治一个国家之所以有那么多部门，就是要把管理工作分开，专权专职专责，对于操作系统也是一样。
现代计算机早已不限于只处理计算任务，它还可以呈现图像、音频，和远程计算机通信，储存大量数据，以及和用户交互。所以，计算机内部需要处理图像、音频、网络、储存、交互的设备。这从上面的图中也可以看得出来。
操作系统内核要控制这些设备，就要包含每个设备的控制代码。如果操作系统内核被设计为通用可移植的内核，那是相当可怕的。试想一下，这个世界上有如此多的设备，操作系统内核代码得多庞大，越庞大就越危险，因为其中一行代码有问题，整个操作系统就崩溃了。
可是仅仅只有这些问题吗？当然不是，我们还要考虑到后面这几点。
1.操作系统内核开发人员，不可能罗列世界上所有的设备，并为其写一套控制代码。
2.为了商业目的，有很多设备厂商并不愿意公开设备的编程细节。就算内核开发人员想为其写控制代码，实际也不可行。
3.如果设备更新换代，就要重写设备的控制代码，然后重新编译操作系统内核，这样的话操作很麻烦，操作系统内核开发人员和用户都可能受不了。
以上三点，足于证明这种方案根本不可取。
既然操作系统内核无法包含所有的设备控制代码，那就索性不包含，或者只包含最基本、最通用的设备控制代码。这样操作系统内核就可以非常通用，非常精巧。
但是要控制设备就必须要有设备的相关控制代码才行，所以我们要把设备控制代码独立出来，与操作系统内核分开、独立开发，设备控制代码可由设备厂商人员开发。
每个设备对应一个设备控制代码模块，操作系统内核要控制哪个设备，就加载相应的设备代码模块，以后不使用这个设备了，就可以删除对应的设备控制代码模块。
这种方式，给操作系统内核带来了巨大的灵活性。设备厂商在发布新设备时，只要随之发布一个与此相关的设备控制代码模块就行了。
设备分类要想管理设备，先要对其分门别类，在开始分类之前，你不妨先思考一个问题：操作系统内核所感知的设备，一定要与物理设备一一对应吗？
举个例子，储存设备，其实不管它是机械硬盘，还是TF卡，或者是一个设备控制代码模块，它向操作系统内核表明它是储存设备，但它完全有可能分配一块内存空间来储存数据，不必访问真正的储存设备。所以，操作系统内核所感知的设备，并不需要和物理设备对应，这取决于设备控制代码自身的行为。
操作系统内核所定义的设备，可称为内核设备或者逻辑设备，其实这只是对物理计算平台中几种类型设备的一种抽象。下面，我们在cosmos/include/knlinc/krldevice_t.h文件中对设备进行分类定义，代码如下。
#define NOT_DEVICE 0 //不表示任何设备 #define BRIDGE_DEVICE 4 //总线桥接器设备 #define CPUCORE_DEVICE 5 //CPU设备，CPU也是设备 #define RAMCONTER_DEVICE 6 //内存控制器设备 #define RAM_DEVICE 7 //内存设备 #define USBHOSTCONTER_DEVICE 8 //USB主控制设备 #define INTUPTCONTER_DEVICE 9 //中断控制器设备 #define DMA_DEVICE 10 //DMA设备 #define CLOCKPOWER_DEVICE 11 //时钟电源设备 #define LCDCONTER_DEVICE 12 //LCD控制器设备 #define NANDFLASH_DEVICE 13 //nandflash设备 #define CAMERA_DEVICE 14 //摄像头设备 #define UART_DEVICE 15 //串口设备 #define TIMER_DEVICE 16 //定时器设备 #define USB_DEVICE 17 //USB设备 #define WATCHDOG_DEVICE 18 //看门狗设备 #define RTC_DEVICE 22 //实时时钟设备 #define SD_DEVICE 25 //SD卡设备 #define AUDIO_DEVICE 26 //音频设备 #define TOUCH_DEVICE 27 //触控设备 #define NETWORK_DEVICE 28 //网络设备 #define VIR_DEVICE 29 //虚拟设备 #define FILESYS_DEVICE 30 //文件系统设备 #define SYSTICK_DEVICE 31 //系统TICK设备 #define UNKNOWN_DEVICE 32 //未知设备，也是设备 #define HD_DEVICE 33 //硬盘设备 上面定义的这些类型的设备，都是Cosmos内核抽象出来的逻辑设备，例如NETWORK_DEVICE网络设备，不管它是有线网卡还是无线网卡，或者是设备控制代码虚拟出来的虚拟网卡。Cosmos内核都将认为它是一个网络设备，这就是设备的抽象，这样有利于我们灵活、简便管理设备。</description></item><item><title>28｜增加更丰富的类型第3步：支持数组</title><link>https://artisanbox.github.io/3/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/30/</guid><description>你好，我是宫文学。
前面我们已经给我们的语言，增加了两种数据类型：浮点数和字符串。那么，今天这一节课，我们再继续增加一种典型的数据类型：数组。
数组也是计算机语言中最重要的基础类型之一。像C、Java和JavaScript等各种语言，都提供了对数组的原生支持。
数组跟我们之前已经实现过的number、string类型的数据类型相比，有明显的差异。所以在这里，你也能学到一些新的知识点，包括，如何对数组做类型处理、如何设计数组的内存布局，如何正确访问数组元素，等等。
在完成这节课的任务后，我们的语言将支持在数组中保存number和string类型的数据，甚至还可以支持多维数组。是不是感觉很强大呢？那就赶紧动手试一试吧！
那么这实现的第一步，我们需要修改编译器前端的代码，来支持与数组处理有关的语法。
修改编译器前端在编译器前端，我们首先要增加与数组有关的语法规则。要增加哪些语法规则呢？我们来看看一个最常见的使用数组的例子中，都会涉及哪些语法特性。
let names:string[] = ["richard", "sam", "john"]; let ages:number[] = [8, 18, 28]; let a2:number[][] = [[1,2,3],[4,5]]; for (let i = 0; i&amp;lt; names.length; i++){ println(names[i]); } 在这个例子中，我们首先声明了一个字符串类型的数组，然后用一个数组字面量来初始化它。你还可以用同样的方法，声明并初始化一个number数组。最后，我们用names[i]这样的表达式来访问数组元素。
在这个例子中，你会发现三个与数组有关的语法现象，分别是数组类型、数组字面量和下标表达式。
首先是数组类型。在声明变量的时候，我们可以用string[]、number[]来表示一个数组类型。这个数组类型是一个基础类型再加上一对方括号[]。在这里，我们甚至还声明了一个二维数组。
所以，我们还需要扩展与类型有关的语法规则。你看看下面的语法规则，你可以这样表示数组类型：“primaryType ‘[’ ‘]’”。而primaryType本身也可以是一个数组类型，这样就能表达多维数组了，比如a[][]。
primaryType : predefinedType | literal | typeReference | '(' type_ ')' | primaryType '[' ']' ; 不过，这是一个左递归的文法，就会遇到我们之前学过的左递归问题。我们可以改写一下，变成下面的文法：
primaryType : primaryTypeLeft ('[' ']')* ; primaryTypeLeft : predefinedType | literal | typeReference | '(' type_ ')' | primaryType ; 这样的话，我们每次解析完毕一个primaryTypeLeft以后，再看看后面有没有跟着一对方括号就行了。如果出现多对方括号，就表示这是一个多维数组。</description></item><item><title>29_CISC和RISC：为什么手机芯片都是ARM？</title><link>https://artisanbox.github.io/4/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/29/</guid><description>我在第5讲讲计算机指令的时候，给你看过MIPS体系结构计算机的机器指令格式。MIPS的指令都是固定的32位长度，如果要用一个打孔卡来表示，并不复杂。
MIPS机器码的长度都是固定的32位第6讲的时候，我带你编译了一些简单的C语言程序，看了x86体系结构下的汇编代码。眼尖的话，你应该能发现，每一条机器码的长度是不一样的。
Intel x86的机器码的长度是可变的而CPU的指令集里的机器码是固定长度还是可变长度，也就是复杂指令集（Complex Instruction Set Computing，简称CISC）和精简指令集（Reduced Instruction Set Computing，简称RISC）这两种风格的指令集一个最重要的差别。那今天我们就来看复杂指令集和精简指令集之间的对比、差异以及历史纠葛。
CISC VS RISC：历史的车轮不总是向前的在计算机历史的早期，其实没有什么CISC和RISC之分。或者说，所有的CPU其实都是CISC。
虽然冯·诺依曼高屋建瓴地提出了存储程序型计算机的基础架构，但是实际的计算机设计和制造还是严格受硬件层面的限制。当时的计算机很慢，存储空间也很小。《人月神话》这本软件工程界的名著，讲的是花了好几年设计IBM 360这台计算机的经验。IBM 360的最低配置，每秒只能运行34500条指令，只有8K的内存。为了让计算机能够做尽量多的工作，每一个字节乃至每一个比特都特别重要。
所以，CPU指令集的设计，需要仔细考虑硬件限制。为了性能考虑，很多功能都直接通过硬件电路来完成。为了少用内存，指令的长度也是可变的。就像算法和数据结构里的赫夫曼编码（Huffman coding）一样，常用的指令要短一些，不常用的指令可以长一些。那个时候的计算机，想要用尽可能少的内存空间，存储尽量多的指令。
不过，历史的车轮滚滚向前，计算机的性能越来越好，存储的空间也越来越大了。到了70年代末，RISC开始登上了历史的舞台。当时，UC Berkeley的大卫·帕特森（David Patterson）教授发现，实际在CPU运行的程序里，80%的时间都是在使用20%的简单指令。于是，他就提出了RISC的理念。自此之后，RISC类型的CPU开始快速蓬勃发展。
我经常推荐的课后阅读材料，有不少是来自《计算机组成与设计：硬件/软件接口》和《计算机体系结构：量化研究方法》这两本教科书。大卫·帕特森教授正是这两本书的作者。此外，他还在2017年获得了图灵奖。
RISC架构的CPU究竟是什么样的呢？为什么它能在这么短的时间内受到如此大的追捧？
RISC架构的CPU的想法其实非常直观。既然我们80%的时间都在用20%的简单指令，那我们能不能只要那20%的简单指令就好了呢？答案当然是可以的。因为指令数量多，计算机科学家们在软硬件两方面都受到了很多挑战。
在硬件层面，我们要想支持更多的复杂指令，CPU里面的电路就要更复杂，设计起来也就更困难。更复杂的电路，在散热和功耗层面，也会带来更大的挑战。在软件层面，支持更多的复杂指令，编译器的优化就变得更困难。毕竟，面向2000个指令来优化编译器和面向500个指令来优化编译器的困难是完全不同的。
于是，在RISC架构里面，CPU选择把指令“精简”到20%的简单指令。而原先的复杂指令，则通过用简单指令组合起来来实现，让软件来实现硬件的功能。这样，CPU的整个硬件设计就会变得更简单了，在硬件层面提升性能也会变得更容易了。
RISC的CPU里完成指令的电路变得简单了，于是也就腾出了更多的空间。这个空间，常常被拿来放通用寄存器。因为RISC完成同样的功能，执行的指令数量要比CISC多，所以，如果需要反复从内存里面读取指令或者数据到寄存器里来，那么很多时间就会花在访问内存上。于是，RISC架构的CPU往往就有更多的通用寄存器。
除了寄存器这样的存储空间，RISC的CPU也可以把更多的晶体管，用来实现更好的分支预测等相关功能，进一步去提升CPU实际的执行效率。
总的来说，对于CISC和RISC的对比，我们可以一起回到第4讲讲的程序运行时间的公式：
程序的CPU执行时间=指令数 × CPI × Clock Cycle TimeCISC的架构，其实就是通过优化指令数，来减少CPU的执行时间。而RISC的架构，其实是在优化CPI。因为指令比较简单，需要的时钟周期就比较少。
因为RISC降低了CPU硬件的设计和开发难度，所以从80年代开始，大部分新的CPU都开始采用RISC架构。从IBM的PowerPC，到SUN的SPARC，都是RISC架构。所有人看到仍然采用CISC架构的Intel CPU，都可以批评一句“Complex and messy”。但是，为什么无论是在PC上，还是服务器上，仍然是Intel成为最后的赢家呢？
Intel的进化：微指令架构的出现面对这么多负面评价的Intel，自然也不能无动于衷。更何况，x86架构的问题并不能说明Intel的工程师不够厉害。事实上，在整个CPU设计的领域，Intel集中了大量优秀的人才。无论是成功的Pentium时代引入的超标量设计，还是失败的Pentium 4时代引入的超线程技术，都是异常精巧的工程实现。
而x86架构所面临的种种问题，其实都来自于一个最重要的考量，那就是指令集的向前兼容性。因为x86在商业上太成功了，所以市场上有大量的Intel CPU。而围绕着这些CPU，又有大量的操作系统、编译器。这些系统软件只支持x86的指令集，就比如著名的Windows 95。而在这些系统软件上，又有各种各样的应用软件。
如果Intel要放弃x86的架构和指令集，开发一个RISC架构的CPU，面临的第一个问题就是所有这些软件都是不兼容的。事实上，Intel并非没有尝试过在x86之外另起炉灶，这其实就是我在第26讲介绍的安腾处理器。当时，Intel想要在CPU进入64位的时代的时候，丢掉x86的历史包袱，所以推出了全新的IA-64的架构。但是，却因为不兼容x86的指令集，遭遇了重大的失败。
反而是AMD，趁着Intel研发安腾的时候，推出了兼容32位x86指令集的64位架构，也就是AMD64。如果你现在在Linux下安装各种软件包，一定经常会看到像下面这样带有AMD64字样的内容。这是因为x86下的64位的指令集x86-64，并不是Intel发明的，而是AMD发明的。
Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fontconfig amd64 2.12.6-0ubuntu2 [169 kB] 在Ubuntu下通过APT安装程序的时候，随处可见AMD64的关键字花开两朵，各表一枝。Intel在开发安腾处理器的同时，也在不断借鉴其他RISC处理器的设计思想。既然核心问题是要始终向前兼容x86的指令集，那么我们能不能不修改指令集，但是让CISC风格的指令集，用RISC的形式在CPU里面运行呢？
于是，从Pentium Pro时代开始，Intel就开始在处理器里引入了微指令（Micro-Instructions/Micro-Ops）架构。而微指令架构的引入，也让CISC和RISC的分界变得模糊了。
在微指令架构的CPU里面，编译器编译出来的机器码和汇编代码并没有发生什么变化。但在指令译码的阶段，指令译码器“翻译”出来的，不再是某一条CPU指令。译码器会把一条机器码，“翻译”成好几条“微指令”。这里的一条条微指令，就不再是CISC风格的了，而是变成了固定长度的RISC风格的了。
这些RISC风格的微指令，会被放到一个微指令缓冲区里面，然后再从缓冲区里面，分发给到后面的超标量，并且是乱序执行的流水线架构里面。不过这个流水线架构里面接受的，就不是复杂的指令，而是精简的指令了。在这个架构里，我们的指令译码器相当于变成了设计模式里的一个“适配器”（Adaptor）。这个适配器，填平了CISC和RISC之间的指令差异。
不过，凡事有好处就有坏处。这样一个能够把CISC的指令译码成RISC指令的指令译码器，比原来的指令译码器要复杂。这也就意味着更复杂的电路和更长的译码时间：本来以为可以通过RISC提升的性能，结果又有一部分浪费在了指令译码上。针对这个问题，我们有没有更好的办法呢？
我在前面说过，之所以大家认为RISC优于CISC，来自于一个数字统计，那就是在实际的程序运行过程中，有80%运行的代码用着20%的常用指令。这意味着，CPU里执行的代码有很强的局部性。而对于有着很强局部性的问题，常见的一个解决方案就是使用缓存。
所以，Intel就在CPU里面加了一层L0 Cache。这个Cache保存的就是指令译码器把CISC的指令“翻译”成RISC的微指令的结果。于是，在大部分情况下，CPU都可以从Cache里面拿到译码结果，而不需要让译码器去进行实际的译码操作。这样不仅优化了性能，因为译码器的晶体管开关动作变少了，还减少了功耗。
因为“微指令”架构的存在，从Pentium Pro开始，Intel处理器已经不是一个纯粹的CISC处理器了。它同样融合了大量RISC类型的处理器设计。不过，由于Intel本身在CPU层面做的大量优化，比如乱序执行、分支预测等相关工作，x86的CPU始终在功耗上还是要远远超过RISC架构的ARM，所以最终在智能手机崛起替代PC的时代，落在了ARM后面。
ARM和RISC-V：CPU的现在与未来2017年，ARM公司的CEO Simon Segards宣布，ARM累积销售的芯片数量超过了1000亿。作为一个从12个人起步，在80年代想要获取Intel的80286架构授权来制造CPU的公司，ARM是如何在移动端把自己的芯片塑造成了最终的霸主呢？</description></item><item><title>29_中端总结：不遗余力地进行代码优化</title><link>https://artisanbox.github.io/7/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/29/</guid><description>你好，我是宫文学。
今天这一讲，我继续带你来总结前面解析的7种真实的编译器中，中端部分的特征和编译技术。
在课程的第1讲，我也给你总结过编译器的中端的主要作用，就是实现各种优化。并且在中端实现的优化，基本上都是机器无关的。而优化是在IR上进行的。
所以，今天这一讲，我们主要来总结以下这两方面的问题：
第一，是对IR的总结。我在第6讲中曾经讲过，IR分为HIR、MIR和LIR三个层次，可以采用线性结构、图、树等多种数据结构。那么基于我们对实际编译器的研究，再一起来总结一下它们的IR的特点。 第二，是对优化算法的总结。在第7讲，我们把各种优化算法做了一个总体的梳理。现在就是时候，来总结一下编译器中的实际实现了。 通过今天的总结，你能够对中端的两大主题IR和优化，形成更加深入的理解，从而更有利于你熟练运用编译技术。
好了，我们先来把前面学到的IR的相关知识，来系统地梳理和印证一下吧。
对IR的总结通过对前面几个真实编译器的分析，我们会发现IR方面的几个重要特征：SSA已经成为主流；Sea of Nodes展现出令人瞩目的优势；另外，一个编译器中的IR，既要能表示抽象度比较高的操作，也要能表示抽象度比较低的、接近机器码的操作。
SSA成为主流通过学习前面的课程，我们会发现，符合SSA格式的IR成为了主流。Java和JavaScript的Sea of Nodes，是符合SSA的；Golang是符合SSA的；Julia自己的IR，虽然最早不是SSA格式的，但后来也改成了SSA；而Julia所采用的LLVM工具，其IR也是SSA格式的。
SSA意味着什么呢？源代码中的一个变量，会变成多个版本，每次赋值都形成一个新版本。在SSA中，它们都叫做一个值（Value），对变量的赋值就是对值的定义（def）。这个值定义出来之后，就可以在定义其他值的时候被使用（use），因此就形成了清晰的“使用-定义”链（use-def）。
这种清晰的use-def链会给优化算法提供很多的便利：
如果一个值定义了，但没有被使用，那就可以做死代码删除。 如果对某个值实现了常数折叠，那么顺着def-use链，我们就可以马上把该值替换成常数，从而实现常数传播。 如果两个值的定义是一样的，那么这两个值也一定是一样的，因此就可以去掉一个，从而实现公共子表达式消除；而如果不采取SSA，实现CSE（公共子表达式消除）需要做一个数据流分析，来确定表达式的变量值并没有发生变化。 针对最后一种情况，也就是公共子表达式消除，我再给你展开讲解一下，让你充分体会SSA和传统IR的区别。
我们知道，基于传统的IR，要做公共子表达式消除，就需要专门做一个“可用表达式”的分析。像下图展示的那样，每扫描一遍代码，就要往一个集合里增加一个可用的表达式。
为什么叫做可用表达式呢？因为变量可能被二次赋值，就像图中的变量c那样。在二次赋值以后，之前的表达式“c:=a+b”就不可用了。
图1：变量c二次赋值后，各个位置的可用表达式集合在后面，当替换公共子表达式的时候，我们可以把“e:=a+b”替换成“e:=d”，这样就可以少做一次计算，实现了优化的目标。
而如果采用SSA格式，上面这几行语句就可以改写为下图中的样子：
图2：用SSA格式的IR改写的程序可以看到，原来的变量c被替换成了c1和c2两个变量，而c1、d和e右边的表达式都是一样的，并且它们的值不会再发生变化。所以，我们可以马上消除掉这些公共子表达式，从而减少了两次计算，这就比采用SSA之前的优化效果更好了。最重要的是，整个过程根本不需要做数据流分析。
图3：把公共子表达式a+b消除掉好，在掌握了SSA格式的特点以后，我们还可以注意到，Java和JavaScript的两大编译器，在做优化时，竟然都不约而同地用到了Sea Of Nodes这种数据结构。它看起来非常重要，所以，我们再接着来总结一下，符合SSA格式的Sea of Nodes，都有什么特点。
Sea of Nodes的特点总结其实在解析Graal编译器的时候，我就提到过，Sea of Nodes的特点是把数据流图和控制流图合二为一，从而更容易实现全局优化。因为采用这种IR，代码并没有一开始就被限制在一个个的基本块中。直到最后生成LIR的环节，才会把图节点Schedule到各个基本块。作为对比，采用基于CFG的IR，优化算法需要让代码在基本块内部和基本块之间移动，处理起来会比较复杂。
在这里，我再带你把生成IR的过程推导一遍，你能从中体会到生成Sea of Nodes的思路，并且还会有一些惊喜的发现。
示例函数或方法是下面这样：
int foo(int b){ a = b; c = a + b; c = b; d = a + b; e = a + b; return e; } 那么，为它生成IR图的过程是怎么样的呢？</description></item><item><title>29_堆的应用：如何快速获取到Top10最热门的搜索关键词？</title><link>https://artisanbox.github.io/2/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/30/</guid><description>搜索引擎的热门搜索排行榜功能你用过吗？你知道这个功能是如何实现的吗？实际上，它的实现并不复杂。搜索引擎每天会接收大量的用户搜索请求，它会把这些用户输入的搜索关键词记录下来，然后再离线地统计分析，得到最热门的Top 10搜索关键词。
那请你思考下，假设现在我们有一个包含10亿个搜索关键词的日志文件，如何能快速获取到热门榜Top 10的搜索关键词呢？
这个问题就可以用堆来解决，这也是堆这种数据结构一个非常典型的应用。上一节我们讲了堆和堆排序的一些理论知识，今天我们就来讲一讲，堆这种数据结构几个非常重要的应用：优先级队列、求Top K和求中位数。
堆的应用一：优先级队列首先，我们来看第一个应用场景：优先级队列。
优先级队列，顾名思义，它首先应该是一个队列。我们前面讲过，队列最大的特性就是先进先出。不过，在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。
如何实现一个优先级队列呢？方法有很多，但是用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。
你可别小看这个优先级队列，它的应用场景非常多。我们后面要讲的很多数据结构和算法都要依赖它。比如，赫夫曼编码、图的最短路径、最小生成树算法等等。不仅如此，很多语言中，都提供了优先级队列的实现，比如，Java的PriorityQueue，C++的priority_queue等。
只讲这些应用场景比较空泛，现在，我举两个具体的例子，让你感受一下优先级队列具体是怎么用的。
1.合并有序小文件假设我们有100个小文件，每个文件的大小是100MB，每个文件中存储的都是有序的字符串。我们希望将这些100个小文件合并成一个有序的大文件。这里就会用到优先级队列。
整体思路有点像归并排序中的合并函数。我们从这100个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。
假设，这个最小的字符串来自于13.txt这个小文件，我们就再从这个小文件取下一个字符串，放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，将它从数组中删除。依次类推，直到所有的文件中的数据都放入到大文件为止。
这里我们用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。有没有更加高效方法呢？
这里就可以用到优先级队列，也可以说是堆。我们将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将100个小文件中的数据依次放入到大文件中。
我们知道，删除堆顶数据和往堆中插入数据的时间复杂度都是O(logn)，n表示堆中的数据个数，这里就是100。是不是比原来数组存储的方式高效了很多呢？
2.高性能定时器假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如1秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。
但是，这样每过1秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。
针对这些问题，我们就可以用优先级队列来解决。我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。
这样，定时器就不需要每隔1秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔T。
这个时间间隔T就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在T秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。
当T秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。
这样，定时器既不用间隔1秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。
堆的应用二：利用堆求Top K刚刚我们学习了优先级队列，我们现在来看，堆的另外一个非常重要的应用场景，那就是“求Top K问题”。
我把这种求Top K的问题抽象成两类。一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。
针对静态数据，如何在一个包含n个数据的数组中，查找前K大数据呢？我们可以维护一个大小为K的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前K大数据了。
遍历数组需要O(n)的时间复杂度，一次堆化操作需要O(logK)的时间复杂度，所以最坏情况下，n个元素都入堆一次，时间复杂度就是O(nlogK)。
针对动态数据求得Top K就是实时Top K。怎么理解呢？我举一个例子。一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前K大数据。
如果每次询问前K大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是O(nlogK)，n表示当前的数据的大小。实际上，我们可以一直都维护一个K大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前K大数据，我们都可以立刻返回给他。
堆的应用三：利用堆求中位数前面我们讲了如何求Top K的问题，现在我们来讲下，如何求动态数据集合中的中位数。
中位数，顾名思义，就是处在中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第$\frac{n}{2}+1$个数据就是中位数（注意：假设数据是从0开始编号的）；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第$\frac{n}{2}$个和第$\frac{n}{2}+1$个数据，这个时候，我们可以随意取一个作为中位数，比如取两个数中靠前的那个，就是第$\frac{n}{2}$个数据。
对于一组静态数据，中位数是固定的，我们可以先排序，第$\frac{n}{2}$个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。所以，尽管排序的代价比较大，但是边际成本会很小。但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。
借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。我们来看看，它是如何做到的？
我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。
也就是说，如果有n个数据，n是偶数，我们从小到大排序，那前$\frac{n}{2}$个数据存储在大顶堆中，后$\frac{n}{2}$个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果n是奇数，情况是类似的，大顶堆就存储$\frac{n}{2}+1$个数据，小顶堆中就存储$\frac{n}{2}$个数据。
我们前面也提到，数据是动态变化的，当新添加一个数据的时候，我们如何调整两个堆，让大顶堆中的堆顶元素继续是中位数呢？
如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。
这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果n是偶数，两个堆中的数据个数都是$\frac{n}{2}$；如果n是奇数，大顶堆有$\frac{n}{2}+1$个数据，小顶堆有$\frac{n}{2}$个数据。这个时候，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。
于是，我们就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间复杂度变成了O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是O(1)。
实际上，利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。还记得我们在“为什么要学习数据结构与算法”里的这个问题吗？“如何快速求接口的99%响应时间？”我们现在就来看下，利用两个堆如何来实现。
在开始这个问题的讲解之前，我先解释一下，什么是“99%响应时间”。
中位数的概念就是将数据从小到大排列，处于中间位置，就叫中位数，这个数据会大于等于前面50%的数据。99百分位数的概念可以类比中位数，如果将一组数据从小到大排列，这个99百分位数就是大于前面99%数据的那个数据。
如果你还是不太理解，我再举个例子。假设有100个数据，分别是1，2，3，……，100，那99百分位数就是99，因为小于等于99的数占总个数的99%。
弄懂了这个概念，我们再来看99%响应时间。如果有100个接口访问请求，每个接口请求的响应时间都不同，比如55毫秒、100毫秒、23毫秒等，我们把这100个接口的响应时间按照从小到大排列，排在第99的那个数据就是99%响应时间，也叫99百分位响应时间。
我们总结一下，如果有n个数据，将数据从小到大排列之后，99百分位数大约就是第n*99%个数据，同类，80百分位数大约就是第n*80%个数据。
弄懂了这些，我们再来看如何求99%响应时间。
我们维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是n，大顶堆中保存n*99%个数据，小顶堆中保存n*1%个数据。大顶堆堆顶的数据就是我们要找的99%响应时间。
每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。
但是，为了保持大顶堆中的数据占99%，小顶堆中的数据占1%，在每次新插入数据之后，我们都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合99:1这个比例。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法，这里我就不啰嗦了。
通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是O(logn)。每次求99%响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是O(1)。
解答开篇学懂了上面的一些应用场景的处理思路，我想你应该能解决开篇的那个问题了吧。假设现在我们有一个包含10亿个搜索关键词的日志文件，如何快速获取到Top 10最热门的搜索关键词呢？
处理这个问题，有很多高级的解决方法，比如使用MapReduce等。但是，如果我们将处理的场景限定为单机，可以使用的内存为1GB。那这个问题该如何解决呢？
因为用户搜索的关键词，有很多可能都是重复的，所以我们首先要统计每个搜索关键词出现的频率。我们可以通过散列表、平衡二叉查找树或者其他一些支持快速查找、插入的数据结构，来记录关键词及其出现的次数。
假设我们选用散列表。我们就顺序扫描这10亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为1。以此类推，等遍历完这10亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。
然后，我们再根据前面讲的用堆求Top K的方法，建立一个大小为10的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。
以此类推，当遍历完整个散列表中的搜索关键词之后，堆中的搜索关键词就是出现次数最多的Top 10搜索关键词了。
不知道你发现了没有，上面的解决思路其实存在漏洞。10亿的关键词还是很多的。我们假设10亿条搜索关键词中不重复的有1亿条，如果每个搜索关键词的平均长度是50个字节，那存储1亿个关键词起码需要5GB的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。而我们的机器只有1GB的可用内存空间，所以我们无法一次性将所有的搜索关键词加入到内存中。这个时候该怎么办呢？
我们在哈希算法那一节讲过，相同数据经过哈希算法得到的哈希值是一样的。我们可以根据哈希算法的这个特点，将10亿条搜索关键词先通过哈希算法分片到10个文件中。
具体可以这样做：我们创建10个空文件00，01，02，……，09。我们遍历这10亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同10取模，得到的结果就是这个搜索关键词应该被分到的文件编号。</description></item><item><title>29_如何判断一个数据库是不是出问题了？</title><link>https://artisanbox.github.io/1/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/29/</guid><description>我在第25和27篇文章中，和你介绍了主备切换流程。通过这些内容的讲解，你应该已经很清楚了：在一主一备的双M架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上。
主备切换有两种场景，一种是主动切换，一种是被动切换。而其中被动切换，往往是因为主库出问题了，由HA系统发起的。
这也就引出了我们今天要讨论的问题：怎么判断一个主库出问题了？
你一定会说，这很简单啊，连上MySQL，执行个select 1就好了。但是select 1成功返回了，就表示主库没问题吗？
select 1判断实际上，select 1成功返回，只能说明这个库的进程还在，并不能说明主库没问题。现在，我们来看一下这个场景。
set global innodb_thread_concurrency=3; CREATE TABLE t ( id int(11) NOT NULL, c int(11) DEFAULT NULL, PRIMARY KEY (id) ) ENGINE=InnoDB;
insert into t values(1,1) 图1 查询blocked我们设置innodb_thread_concurrency参数的目的是，控制InnoDB的并发线程上限。也就是说，一旦并发线程数达到这个值，InnoDB在接收到新请求的时候，就会进入等待状态，直到有线程退出。
这里，我把innodb_thread_concurrency设置成3，表示InnoDB只允许3个线程并行执行。而在我们的例子中，前三个session 中的sleep(100)，使得这三个语句都处于“执行”状态，以此来模拟大查询。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;你看到了， session D里面，select 1是能执行成功的，但是查询表t的语句会被堵住。也就是说，如果这时候我们用select 1来检测实例是否正常的话，是检测不出问题的。
在InnoDB中，innodb_thread_concurrency这个参数的默认值是0，表示不限制并发线程数量。但是，不限制并发线程数肯定是不行的。因为，一个机器的CPU核数有限，线程全冲进来，上下文切换的成本就会太高。
所以，通常情况下，我们建议把innodb_thread_concurrency设置为64~128之间的值。这时，你一定会有疑问，并发线程上限数设置为128够干啥，线上的并发连接数动不动就上千了。
产生这个疑问的原因，是搞混了并发连接和并发查询。
并发连接和并发查询，并不是同一个概念。你在show processlist的结果里，看到的几千个连接，指的就是并发连接。而“当前正在执行”的语句，才是我们所说的并发查询。
并发连接数达到几千个影响并不大，就是多占一些内存而已。我们应该关注的是并发查询，因为并发查询太高才是CPU杀手。这也是为什么我们需要设置innodb_thread_concurrency参数的原因。
然后，你可能还会想起我们在第7篇文章中讲到的热点更新和死锁检测的时候，如果把innodb_thread_concurrency设置为128的话，那么出现同一行热点更新的问题时，是不是很快就把128消耗完了，这样整个系统是不是就挂了呢？
实际上，在线程进入锁等待以后，并发线程的计数会减一，也就是说等行锁（也包括间隙锁）的线程是不算在128里面的。
MySQL这样设计是非常有意义的。因为，进入锁等待的线程已经不吃CPU了；更重要的是，必须这么设计，才能避免整个系统锁死。
为什么呢？假设处于锁等待的线程也占并发线程的计数，你可以设想一下这个场景：
线程1执行begin; update t set c=c+1 where id=1, 启动了事务trx1， 然后保持这个状态。这时候，线程处于空闲状态，不算在并发线程里面。
线程2到线程129都执行 update t set c=c+1 where id=1; 由于等行锁，进入等待状态。这样就有128个线程处于等待状态；</description></item><item><title>29_目标代码的生成和优化（一）：如何适应各种硬件架构？</title><link>https://artisanbox.github.io/6/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/29/</guid><description>在编译器的后端，我们要能够针对不同的计算机硬件，生成优化的代码。在23讲，我曾带你试着生成过汇编代码，但当时生成汇编代码的逻辑是比较幼稚的，一个正式的编译器后端，代码生成部分需要考虑得更加严密才可以。
那么具体要考虑哪些问题呢？其实主要有三点：
指令的选择。同样一个功能，可以用不同的指令或指令序列来完成，而我们需要选择比较优化的方案。
寄存器分配。每款CPU的寄存器都是有限的，我们要有效地利用它。
指令重排序。计算执行的次序会影响所生成的代码的效率。在不影响运行结果的情况下，我们要通过代码重排序获得更高的效率。
我会用两节课的时间，带你对这三点问题建立直观认识，然后，我还会介绍LLVM的实现策略。这样一来，你会对目标代码的生成，建立比较清晰的顶层认知，甚至可以尝试去实现自己的算法。
接下来，我们针对第一个问题，聊一聊为什么需要选择指令，以及如何选择指令。
选择正确的指令你可能会问：我们为什么非要关注指令的选择呢？我来做个假设。
如果我们不考虑目标代码的性能，可以按照非常机械的方式翻译代码。比如，我们可以制定一个代码翻译的模板，把形如“a := b + c”的代码都翻译成下面的汇编代码：
mov b, r0 //把b装入寄存器r0 add c, r0 //把c加到r0上 mov r0, a //把r0存入a 那么，下面两句代码：
a := b + c d := a + e 将被机械地翻译成：
mov b, r0 add c, r0 mov r0, a mov a, r0 add e, r0 mov r0, d 你可以从上面这段代码中看到，第4行其实是多余的，因为r0的值就是a，不用再装载一遍了。另外，如果后面的代码不会用到a（也就是说a只是个临时变量），那么第3行也是多余的。
这种算法很幼稚，正确性没有问题，但代码量太大，代价太高。所以我们最好用聪明一点儿的算法来生成更加优化的代码。这是我们要做指令选择的原因之一。
做指令选择的第二个原因是，实现同一种功能可以使用多种指令，特别是CISC指令集（可替代的选择很多，但各自有适用的场景）。
对于某个CPU来说，完成同样的任务可以采用不同的指令。比如，实现“a := a + 1”，可以生成三条代码：</description></item><item><title>29_部门建立：如何在内核中注册设备？</title><link>https://artisanbox.github.io/9/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/29/</guid><description>你好，我是LMOS。
在上节课里，我们对设备进行了分类，建立了设备与驱动的数据结构，同时也规定了一个驱动程序应该提供哪些标准操作方法，供操作系统内核调用。这相当于设计了行政部门的规章制度，一个部门叫什么，应该干什么，这些就确定好了。
今天我们来继续探索部门的建立，也就是设备在内核中是如何注册的。我们先从全局了解一下设备的注册流程，然后了解怎么加载驱动，最后探索怎么让驱动建立一个设备，并在内核中注册。让我们正式开始今天的学习吧！
这节课配套代码，你可以从这里下载。
设备的注册流程你是否想象过，你在电脑上插入一个USB鼠标时，操作系统会作出怎样的反应呢？
我来简单作个描述，这个过程可以分成这样五步。
1.操作系统会收到一个中断。
2.USB总线驱动的中断处理程序会执行。
3.调用操作系统内核相关的服务，查找USB鼠标对应的驱动程序。
4.操作系统加载驱动程序。
5.驱动程序开始执行，向操作系统内核注册一个鼠标设备。这就是一般操作系统加载驱动的粗略过程。对于安装在主板上的设备，操作系统会枚举设备信息，然后加载驱动程序，让驱动程序创建并注册相应的设备。当然，你还可以手动加载驱动程序。
为了简单起见，我们的Cosmos不会这样复杂，暂时也不支持设备热拨插功能。我们让Cosmos自动加载驱动，在驱动中向Cosmos注册相应的设备，这样就可以大大降低问题的复杂度，我们先从简单的做起嘛，相信你明白了原理之后，还可以自行迭代。
为了让你更清楚地了解这个过程，我为你画了一幅图，如下所示。
上图中，完整展示了Cosmos自动加载驱动的整个流程，Cosmos在初始化驱动时会扫描整个驱动表，然后加载表中每个驱动，分别调用各个驱动的入口函数，最后在驱动中建立设备并向内核注册。接下来，我们分别讨论这些流程的实现。
驱动程序表为了简化问题，便于你理解，我们把驱动程序和内核链接到一起，省略了加载驱动程序的过程，因为加载程序不仅仅是把驱动程序放在内存中就可以了，还要进行程序链接相关的操作，这个操作极其复杂，我们先不在这里研究，感兴趣的话你可以自行拓展。
既然我们把内核和驱动程序链接在了一起，就需要有个机制让内核知道驱动程序的存在。这个机制就是驱动程序表，它可以这样设计。
//cosmos/kernel/krlglobal.c KRL_DEFGLOB_VARIABLE(drventyexit_t,osdrvetytabl)[]={NULL}; drventyexit_t类型，在上一课中，我们已经了解过了。它就是一个函数指针类型，这里就是定义了一个函数指针数组，而这个函数指针数组中放的就是驱动程序的入口函数，而内核只需要扫描这个函数指针数组，就可以调用到每个驱动程序了。
有了这个函数指针数组，接着我们还需要写好这个驱动程序的初始化函数，代码如下。
void init_krldriver() { //遍历驱动程序表中的每个驱动程序入口函数 for (uint_t ei = 0; osdrvetytabl[ei] != NULL; ei++) { //运行一个驱动程序入口 if (krlrun_driverentry(osdrvetytabl[ei]) == DFCERRSTUS) { hal_sysdie(&amp;quot;init driver err&amp;quot;); } } return; } void init_krl() { init_krlmm(); init_krldevice(); init_krldriver(); //…… return; } 像上面代码这样，我们的初始化驱动的代码就写好了。init_krldriver函数主要的工作就是遍历驱动程序表中的每个驱动程序入口，并把它作为参数传给krlrun_driverentry函数。
有了init_krldriver函数，还要在init_krl函数中调用它，主要调用上述代码中的调用顺序，请注意，一定要先初始化设备表，然后才能初始化驱动程序，否则在驱动程序中建立的设备和驱动就无处安放了。
运行驱动程序我们使用驱动程序表，虽然省略了加载驱动程序的步骤，但是驱动程序必须要运行，才能工作。接下来我们就详细看看运行驱动程序的全过程。
调用驱动程序入口函数我们首先来解决怎么调用驱动程序入口函数。你要知道，我们直接调用驱动程序入口函数是不行的，要先给它准备一个重要的参数，也就是驱动描述符指针。
为了帮你进一步理解，我们来写一个函数描述内核加载驱动的过程，后面代码中drvp就是一个驱动描述符指针。
drvstus_t krlrun_driverentry(drventyexit_t drventry) { driver_t *drvp = new_driver_dsc();//建立driver_t实例变量 if (drvp == NULL) { return DFCERRSTUS; } if (drventry(drvp, 0, NULL) == DFCERRSTUS)//运行驱动程序入口函数 { return DFCERRSTUS; } if (krldriver_add_system(drvp) == DFCERRSTUS)//把驱动程序加入系统 { return DFCERRSTUS; } return DFCOKSTUS; } 上述代码中，我们先调用了 一个new_driver_dsc函数，用来建立一个driver_t结构实例变量，这个函数我已经帮你写好了。</description></item><item><title>29｜面向对象编程第1步：先把基础搭好</title><link>https://artisanbox.github.io/3/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/31/</guid><description>你好，我是宫文学。
到目前为止，我们的语言已经简单支持了number类型、string类型和数组。现在，我们终于要来实现期待已久的面向对象功能了。
在我们的课程中，为了实现编译器的功能，我们使用了大量自定义的类。最典型的就是各种AST节点，它们都有共同的基类，然后各自又有自己属性或方法。这就是TypeScript面向对象特性最直观的体现。
面向对象特性是一个比较大的体系，涉及了很多知识点。我们会花两节课的时间，实现其中最关键的那些技术点，比如声明自定义类、创建对象、访问对象的属性和方法，以及对象的继承和多态，等等，让你理解面向对象的基础原理。
首先，我们仍然从编译器的前端部分改起，让它支持面向对象特性的语法和语义处理工作。
修改编译器前端首先是对语法的增强。我们还是先来看一个例子，通过这个例子看看，我们到底需要增加哪些语法特性：
class Mammal{ weight:number; color:string; constructor(weight:number, color:string){ this.weight = weight; this.color = color; } speak(){ println("Hello!"); } } let mammal = new Mammal(20,&amp;ldquo;white&amp;rdquo;); println(mammal.color); println(mammal.weight); println(mammal.speak); &amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;在这个例子中，我们声明了一个class，Mammal。这个类描述了哺乳动物的一些基础属性，包括它的体重weight、颜色color。它还提供了哺乳动物的一些行为特征，比如提供了一个speak方法。
Mammal类还有一个特殊的方法，叫做构造方法。通过调用构造方法，可以创建类的实例，也就是对象。然后，我们可以访问对象的属性和方法。
其实TypeScript的类还有很多特性，包括私有成员、静态成员等等。这里我们还是先考虑一个最小的特性集合，先让语言支持最基础的类和对象特性。
看看这个示例程序，我们能总结出多个需要增强的语法点，包括类的声明、调用类的构造方法，this关键字，以及通过点符号来引用对象的属性和方法。
我们首先看看类的声明。我们提供了下面这些语法规则，来支持类的声明：
classDecl : Class Identifier classTail ; classTail : &amp;lsquo;{&amp;rsquo; classElement* &amp;lsquo;}&amp;rsquo; ; classElement : constructorDecl| propertyMemberDecl; constructorDecl : Constructor &amp;lsquo;(&amp;rsquo; parameterList? &amp;lsquo;)&amp;rsquo; &amp;lsquo;{&amp;rsquo; functionBody &amp;lsquo;}&amp;rsquo; ; propertyMemberDecl : Identifier typeAnnotation? (&amp;rsquo;=&amp;rsquo; expression)?</description></item><item><title>30_GPU（上）：为什么玩游戏需要使用GPU？</title><link>https://artisanbox.github.io/4/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/30/</guid><description>讲完了CPU，我带你一起来看一看计算机里的另外一个处理器，也就是被称之为GPU的图形处理器。过去几年里，因为深度学习的大发展，GPU一下子火起来了，似乎GPU成了一个专为深度学习而设计的处理器。那GPU的架构究竟是怎么回事儿呢？它最早是用来做什么而被设计出来的呢？
想要理解GPU的设计，我们就要从GPU的老本行图形处理说起。因为图形处理才是GPU设计用来做的事情。只有了解了图形处理的流程，我们才能搞明白，为什么GPU要设计成现在这样；为什么在深度学习上，GPU比起CPU有那么大的优势。
GPU的历史进程GPU是随着我们开始在计算机里面需要渲染三维图形的出现，而发展起来的设备。图形渲染和设备的先驱，第一个要算是SGI（Silicon Graphics Inc.）这家公司。SGI的名字翻译成中文就是“硅谷图形公司”。这家公司从80年代起就开发了很多基于Unix操作系统的工作站。它的创始人Jim Clark是斯坦福的教授，也是图形学的专家。
后来，他也是网景公司（Netscape）的创始人之一。而Netscape，就是那个曾经和IE大战300回合的浏览器公司，虽然最终败在微软的Windows免费捆绑IE的策略下，但是也留下了Firefox这个完全由开源基金会管理的浏览器。不过这个都是后话了。
到了90年代中期，随着个人电脑的性能越来越好，PC游戏玩家们开始有了“3D显卡”的需求。那个时代之前的3D游戏，其实都是伪3D。比如，大神卡马克开发的著名Wolfenstein 3D（德军总部3D），从不同视角看到的是8幅不同的贴图，实际上并不是通过图形学绘制渲染出来的多边形。
这样的情况下，游戏玩家的视角旋转个10度，看到的画面并没有变化。但是如果转了45度，看到的画面就变成了另外一幅图片。而如果我们能实时渲染基于多边形的3D画面的话，那么任何一点点的视角变化，都会实时在画面里面体现出来，就好像你在真实世界里面看到的一样。
而在90年代中期，随着硬件和技术的进步，我们终于可以在PC上用硬件直接实时渲染多边形了。“真3D”游戏开始登上历史舞台了。“古墓丽影”“最终幻想7”，这些游戏都是在那个时代诞生的。当时，很多国内的计算机爱好者梦寐以求的，是一块Voodoo FX的显卡。
那为什么CPU的性能已经大幅度提升了，但是我们还需要单独的GPU呢？想要了解这个问题，我们先来看一看三维图像实际通过计算机渲染出来的流程。
图形渲染的流程现在我们电脑里面显示出来的3D的画面，其实是通过多边形组合出来的。你可以看看下面这张图，你在玩的各种游戏，里面的人物的脸，并不是那个相机或者摄像头拍出来的，而是通过多边形建模（Polygon Modeling）创建出来的。
图片来源3D游戏里的人脸，其实是用多边形建模创建出来的而实际这些人物在画面里面的移动、动作，乃至根据光线发生的变化，都是通过计算机根据图形学的各种计算，实时渲染出来的。
这个对于图像进行实时渲染的过程，可以被分解成下面这样5个步骤：
顶点处理（Vertex Processing） 图元处理（Primitive Processing） 栅格化（Rasterization） 片段处理（Fragment Processing） 像素操作（Pixel Operations） 我们现在来一步一步看这5个步骤。
顶点处理图形渲染的第一步是顶点处理。构成多边形建模的每一个多边形呢，都有多个顶点（Vertex）。这些顶点都有一个在三维空间里的坐标。但是我们的屏幕是二维的，所以在确定当前视角的时候，我们需要把这些顶点在三维空间里面的位置，转化到屏幕这个二维空间里面。这个转换的操作，就被叫作顶点处理。
如果你稍微学过一点图形学的话，应该知道，这样的转化都是通过线性代数的计算来进行的。可以想见，我们的建模越精细，需要转换的顶点数量就越多，计算量就越大。而且，这里面每一个顶点位置的转换，互相之间没有依赖，是可以并行独立计算的。
顶点处理就是在进行线性变换图元处理在顶点处理完成之后呢，我们需要开始进行第二步，也就是图元处理。图元处理，其实就是要把顶点处理完成之后的各个顶点连起来，变成多边形。其实转化后的顶点，仍然是在一个三维空间里，只是第三维的Z轴，是正对屏幕的“深度”。所以我们针对这些多边形，需要做一个操作，叫剔除和裁剪（Cull and Clip），也就是把不在屏幕里面，或者一部分不在屏幕里面的内容给去掉，减少接下来流程的工作量。
栅格化在图元处理完成之后呢，渲染还远远没有完成。我们的屏幕分辨率是有限的。它一般是通过一个个“像素（Pixel）”来显示出内容的。所以，对于做完图元处理的多边形，我们要开始进行第三步操作。这个操作就是把它们转换成屏幕里面的一个个像素点。这个操作呢，就叫作栅格化。这个栅格化操作，有一个特点和上面的顶点处理是一样的，就是每一个图元都可以并行独立地栅格化。
片段处理在栅格化变成了像素点之后，我们的图还是“黑白”的。我们还需要计算每一个像素的颜色、透明度等信息，给像素点上色。这步操作，就是片段处理。这步操作，同样也可以每个片段并行、独立进行，和上面的顶点处理和栅格化一样。
像素操作最后一步呢，我们就要把不同的多边形的像素点“混合（Blending）”到一起。可能前面的多边形可能是半透明的，那么前后的颜色就要混合在一起变成一个新的颜色；或者前面的多边形遮挡住了后面的多边形，那么我们只要显示前面多边形的颜色就好了。最终，输出到显示设备。
经过这完整的5个步骤之后，我们就完成了从三维空间里的数据的渲染，变成屏幕上你可以看到的3D动画了。这样5个步骤的渲染流程呢，一般也被称之为图形流水线（Graphic Pipeline）。这个名字和我们讲解CPU里面的流水线非常相似，都叫Pipeline。
解放图形渲染的GPU我们可以想一想，如果用CPU来进行这个渲染过程，需要花上多少资源呢？我们可以通过一些数据来做个粗略的估算。
在上世纪90年代的时候，屏幕的分辨率还没有现在那么高。一般的CRT显示器也就是640×480的分辨率。这意味着屏幕上有30万个像素需要渲染。为了让我们的眼睛看到画面不晕眩，我们希望画面能有60帧。于是，每秒我们就要重新渲染60次这个画面。也就是说，每秒我们需要完成1800万次单个像素的渲染。从栅格化开始，每个像素有3个流水线步骤，即使每次步骤只有1个指令，那我们也需要5400万条指令，也就是54M条指令。
90年代的CPU的性能是多少呢？93年出货的第一代Pentium处理器，主频是60MHz，后续逐步推出了66MHz、75MHz、100MHz的处理器。以这个性能来看，用CPU来渲染3D图形，基本上就要把CPU的性能用完了。因为实际的每一个渲染步骤可能不止一个指令，我们的CPU可能根本就跑不动这样的三维图形渲染。
也就是在这个时候，Voodoo FX这样的图形加速卡登上了历史舞台。既然图形渲染的流程是固定的，那我们直接用硬件来处理这部分过程，不用CPU来计算是不是就好了？很显然，这样的硬件会比制造有同样计算性能的CPU要便宜得多。因为整个计算流程是完全固定的，不需要流水线停顿、乱序执行等等的各类导致CPU计算变得复杂的问题。我们也不需要有什么可编程能力，只要让硬件按照写好的逻辑进行运算就好了。
那个时候，整个顶点处理的过程还是都由CPU进行的，不过后续所有到图元和像素级别的处理都是通过Voodoo FX或者TNT这样的显卡去处理的。也就是从这个时代开始，我们能玩上“真3D”的游戏了。
不过，无论是Voodoo FX还是NVidia TNT。整个显卡的架构还不同于我们现代的显卡，也没有现代显卡去进行各种加速深度学习的能力。这个能力，要到NVidia提出Unified Shader Archicture才开始具备。这也是我们下一讲要讲的内容。
总结延伸这一讲里，我带你了解了一个基于多边形建模的三维图形的渲染过程。这个渲染过程需要经过顶点处理、图元处理、栅格化、片段处理以及像素操作这5个步骤。这5个步骤把存储在内存里面的多边形数据变成了渲染在屏幕上的画面。因为里面的很多步骤，都需要渲染整个画面里面的每一个像素，所以其实计算量是很大的。我们的CPU这个时候，就有点跑不动了。
于是，像3dfx和NVidia这样的厂商就推出了3D加速卡，用硬件来完成图元处理开始的渲染流程。这些加速卡和现代的显卡还不太一样，它们是用固定的处理流程来完成整个3D图形渲染的过程。不过，因为不用像CPU那样考虑计算和处理能力的通用性。我们就可以用比起CPU芯片更低的成本，更好地完成3D图形的渲染工作。而3D游戏的时代也是从这个时候开始的。
推荐阅读想要了解GPU的设计构造，一个有效的办法就是回头去看看GPU的历史。我建议你好好读一读Wikipedia里面，关于GPU的条目。另外，也可以看看Techspot上的The History of the Mordern Graphics Processor的系列文章。
课后思考我们上面说的图形加速卡，可以加速3D图形的渲染。那么，这些显卡对于传统的2D图形，也能够进行加速，让CPU摆脱这些负担吗？
欢迎留言和我分享你的疑惑和见解。你也可以把今天的内容，分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>30_后端总结：充分发挥硬件的能力</title><link>https://artisanbox.github.io/7/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/30/</guid><description>你好，我是宫文学。
后端的工作，主要是针对各种不同架构的CPU来生成机器码。在第8讲，我已经对编译器在生成代码的过程中，所做的主要工作进行了简单的概述，你现在应该对编译器的后端工作有了一个大致的了解，也知道了后端工作中的关键算法包括指令选择、寄存器分配和指令排序（又叫做指令调度）。
那么今天这一讲，我们就借助在第二个模块中解析过的真实编译器，来总结、梳理一下各种编译器的后端技术，再来迭代提升一下原有的认知，并加深对以下这些问题的理解：
首先，在第8讲中，我只讲了指令选择的必要性，但对于如何实现指令选择等步骤，我并没有展开介绍。今天这一讲，我就会带你探索一下指令选择的相关算法。 其次，关于寄存器分配算法，我们探索过的好几个编译器，比如Graal、gc编译器等，采用的都是线性扫描算法，那么这个算法的原理是什么呢？我们一起来探究一下。 最后，我们再回到计算机语言设计的主线上来，一起分析一下不同编译器的后端设计，是如何跟该语言的设计目标相匹配的。 OK，我们先来了解一下指令选择的算法。
指令选择算法回顾一下，我们主要是在Graal和Go语言的编译器中，分析了与指令选择有关的算法。它们都采用了一种模式匹配的DSL，只要找到了符合模式的指令组合，编译器就生成一条低端的、对应于机器码的指令。
那为什么这种算法是有效的呢？这种算法的原理是什么呢？都有哪些不同的算法实现？接下来，我就给你揭晓一下答案。
我先给你举个例子。针对表达式“a[i]=b”，它是对数组a的第i个元素赋值。假设a是一个整数数组，那么地址的偏移量就是a+4*i，所以，这个赋值表达式用C语言可以写成“*(a+4*i)=b”，把它表达成AST的话，就是下图所示的样子。其中，赋值表达式的左子树的计算结果，是一个内存地址。
图1：a[i]=b的AST那么，我们要如何给这个表达式生成指令呢？
如果你熟悉x86汇编，你就会知道，上述语句可以非常简单地表达出来，因为x86的指令对数组寻址做了优化（参见第8讲的内容）。
不过，这里为了让你更容易理解算法的原理，我设计了一个新的指令集。这个指令集中的每条指令，都对应了一棵AST的子树，我们把它叫做模式树（Pattern Tree）。在有的算法里，它们也被叫做瓦片（Tiling）。对一个AST生成指令，就是用这样的模式树或瓦片来覆盖整个AST的过程。所以，这样的算法也叫做基于模式匹配的指令生成算法。
图2：指令集中的指令和对应的模式树你可以看到，在图2中，对于每棵模式树，它的根节点是这个指令产生的结果的存放位置。比如，Load_Const指令执行完毕以后，常数会被保存到一个寄存器里。这个寄存器，又可以作为上一级AST节点的操作数来使用。
图2中的指令包含：把常数和内存中的值加载到寄存器、加法运算、乘法运算等。其中有两个指令是特殊设计的，目的就是为了让你更容易理解接下来要探究的各种算法。
第一个指令是#4（Store_Offset），它把值保存到内存的时候，可以在目的地址上加一个偏移量。你可以认为这是为某些场景做的一个优化，比如你在对象地址上加一个偏移量，就能获得成员变量的地址，并把数值保存到这个地址上。
第二个指令是#9（Lea），它相当于x86指令集中的Lea指令，能够计算一个地址值，特别是能够利用间接寻址模式，计算出一个数组元素的地址。它能通过一条指令完成一个乘法计算和一个加法计算。如果你忘记了Lea指令，可以重新看看第8讲的内容。
基于上述的指令和模式树，我们就可以尝试来做一下模式匹配，从而选择出合适的指令。那么都可以采用什么样的算法呢？
第一个算法，是一种比较幼稚的算法。我们采取深度优先的后序遍历，也就是按照“左子节点-&amp;gt;右子节点-&amp;gt;父节点”的顺序遍历，针对每个节点去匹配上面的模式。
第1步，采用模式#2，把内存中a的值，也就是数组的地址，加载到寄存器。因为无论加减乘除等任何运算，都是可以拿寄存器作为操作数的，所以做这个决策是很安全的。 第2步，同上，采用模式#1，把常量4加载到寄存器。 第3步，采用模式#2，把内存中i的值加载到寄存器。 第4步，采用模式#8，把两个寄存器的值相乘，得到（4*i）的值。 第5步，采用模式#5，把两个寄存器的值相加，得到a+4*i的值，也就是a[i]的地址。 第6步，采用模式#2，把内存中b的值加载到寄存器。 第7步，采用模式#3，把寄存器中b的值写入a[i]的地址。 图3：用比较幼稚的算法做模式匹配最后形成的汇编代码是这样的：
Load_Mem a, R1 Load_Const 4, R2 Load_Mem i, R3 Mul_Reg R2, R3 Add_Reg R3, R1 Load_Mem b, R2 Store R2, (R1) 这种方法，是自底向上的做树的重写。它的优点是特别简单，缺点是性能比较差。它一共生成了7条指令，代价是19（3+1+3+4+1+3+4）。
在上述步骤中，我们能看到很多可以优化的地方。比如，4*i这个子表达式，我们是用了3条指令来实现的，总的Cost是1+3+4=8，而如果改成两条指令，也就是使用Mul_mem指令，就不用先把i加载到寄存器，Cost可以是1+6=7。
Load_Const 4, R1 Mul_Mem i, R1 第二种方法，是类似Graal编译器所采用的方法，自顶向下的做模式匹配。比如，当我们处理赋值节点的时候，算法会尽量匹配更多的子节点。因为一条指令包含的子节点越多，那么通过一条指令完成的操作就越多，从而总的Cost就更低。
所以，算法的大致步骤是这样的：
第1步，在#3和#4两个模式中做选择的话，选中了#4号。 第2步，沿着AST继续所深度遍历，其中+号节点第1步被处理掉了，所以现在处理变量a，采用了模式#2，把变量加载到寄存器。 第3步，处理*节点。这个时候要在#7和#8之间做对比，最后选择了#7，因为它可以包含更多的节点。 第4步，处理常量4。因为上级节点在这里需要一个寄存器作为操作数，所以我们采用了模式#1，把常量加载到寄存器。 第5步，处理变量b。这里也要把它加载到寄存器，因此采用了模式#2。 图4：Maximal Munch算法的匹配结果到此为止，我们用了5条指令就做完了所有的运算，生成的汇编代码是：</description></item><item><title>30_图的表示：如何存储微博、微信等社交网络中的好友关系？</title><link>https://artisanbox.github.io/2/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/31/</guid><description>微博、微信、LinkedIn这些社交软件我想你肯定都玩过吧。在微博中，两个人可以互相关注；在微信中，两个人可以互加好友。那你知道，如何存储微博、微信等这些社交网络的好友关系吗？
这就要用到我们今天要讲的这种数据结构：图。实际上，涉及图的算法有很多，也非常复杂，比如图的搜索、最短路径、最小生成树、二分图等等。我们今天聚焦在图存储这一方面，后面会分好几节来依次讲解图相关的算法。
如何理解“图”？我们前面讲过了树这种非线性表数据结构，今天我们要讲另一种非线性表数据结构，图（Graph）。和树比起来，这是一种更加复杂的非线性表结构。
我们知道，树中的元素我们称为节点，图中的元素我们就叫做顶点（vertex）。从我画的图中可以看出来，图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫做边（edge）。
我们生活中就有很多符合图这种结构的例子。比如，开篇问题中讲到的社交网络，就是一个非常典型的图结构。
我们就拿微信举例子吧。我们可以把每个用户看作一个顶点。如果两个用户之间互加好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。其中，每个用户有多少个好友，对应到图中，就叫做顶点的度（degree），就是跟顶点相连接的边的条数。
实际上，微博的社交关系跟微信还有点不一样，或者说更加复杂一点。微博允许单向关注，也就是说，用户A关注了用户B，但用户B可以不关注用户A。那我们如何用图来表示这种单向的社交关系呢？
我们可以把刚刚讲的图结构稍微改造一下，引入边的“方向”的概念。
如果用户A关注了用户B，我们就在图中画一条从A到B的带箭头的边，来表示边的方向。如果用户A和用户B互相关注了，那我们就画一条从A指向B的边，再画一条从B指向A的边。我们把这种边有方向的图叫做“有向图”。以此类推，我们把边没有方向的图就叫做“无向图”。
我们刚刚讲过，无向图中有“度”这个概念，表示一个顶点有多少条边。在有向图中，我们把度分为入度（In-degree）和出度（Out-degree）。
顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。对应到微博的例子，入度就表示有多少粉丝，出度就表示关注了多少人。
前面讲到了微信、微博、无向图、有向图，现在我们再来看另一种社交软件：QQ。
QQ中的社交关系要更复杂一点。不知道你有没有留意过QQ亲密度这样一个功能。QQ不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经常往来，亲密度就比较低。如何在图中记录这种好友关系的亲密度呢？
这里就要用到另一种图，带权图（weighted graph）。在带权图中，每条边都有一个权重（weight），我们可以通过这个权重来表示QQ好友间的亲密度。
关于图的概念比较多，我今天也只是介绍了几个常用的，理解起来都不复杂，不知道你都掌握了没有？掌握了图的概念之后，我们再来看下，如何在内存中存储图这种数据结构呢？
邻接矩阵存储方法图最直观的一种存储方法就是，邻接矩阵（Adjacency Matrix）。
邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点i与顶点j之间有边，我们就将A[i][j]和A[j][i]标记为1；对于有向图来说，如果顶点i到顶点j之间，有一条箭头从顶点i指向顶点j的边，那我们就将A[i][j]标记为1。同理，如果有一条箭头从顶点j指向顶点i的边，我们就将A[j][i]标记为1。对于带权图，数组中就存储相应的权重。
用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。为什么这么说呢？
对于无向图来说，如果A[i][j]等于1，那A[j][i]也肯定等于1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。
还有，如果我们存储的是稀疏图（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。比如微信有好几亿的用户，对应到图上就是好几亿的顶点。但是每个用户的好友并不会很多，一般也就三五百个而已。如果我们用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。
但这也并不是说，邻接矩阵的存储方法就完全没有优点。首先，邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。其次，用邻接矩阵存储图的另外一个好处是方便计算。这是因为，用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。比如求解最短路径问题时会提到一个Floyd-Warshall算法，就是利用矩阵循环相乘若干次得到结果。
邻接表存储方法针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，邻接表（Adjacency List）。
我画了一张邻接表的图，你可以先看下。乍一看，邻接表是不是有点像散列表？每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。另外我需要说明一下，图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点，你可以自己画下。
还记得我们之前讲过的时间、空间复杂度互换的设计思想吗？邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储起来比较节省空间，但是使用起来就比较耗时间。
就像图中的例子，如果我们要确定，是否存在一条从顶点2到顶点4的边，那我们就要遍历顶点2对应的那条链表，看链表中是否存在顶点4。而且，我们前面也讲过，链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。
在散列表那几节里，我讲到，在基于链表法解决冲突的散列表中，如果链过长，为了提高查找效率，我们可以将链表换成其他更加高效的数据结构，比如平衡二叉查找树等。我们刚刚也讲到，邻接表长得很像散列。所以，我们也可以将邻接表同散列表一样进行“改进升级”。
我们可以将邻接表中的链表改成平衡二叉查找树。实际开发中，我们可以选择用红黑树。这样，我们就可以更加快速地查找两个顶点之间是否存在边了。当然，这里的二叉查找树可以换成其他动态数据结构，比如跳表、散列表等。除此之外，我们还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边。
解答开篇有了前面讲的理论知识，现在我们回过头来看开篇的问题，如何存储微博、微信等社交网络中的好友关系？
前面我们分析了，微博、微信是两种“图”，前者是有向图，后者是无向图。在这个问题上，两者的解决思路差不多，所以我只拿微博来讲解。
数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。针对微博用户关系，假设我们需要支持下面这样几个操作：
判断用户A是否关注了用户B；
判断用户A是否是用户B的粉丝；
用户A关注用户B；
用户A取消关注用户B；
根据用户名称的首字母排序，分页获取用户的粉丝列表；
根据用户名称的首字母排序，分页获取用户的关注列表。
关于如何存储一个图，前面我们讲到两种主要的存储方法，邻接矩阵和邻接表。因为社交网络是一张稀疏图，使用邻接矩阵存储比较浪费存储空间。所以，这里我们采用邻接表来存储。
不过，用一个邻接表来存储这种有向图是不够的。我们去查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就是用户的粉丝列表，是非常困难的。
基于此，我们需要一个逆邻接表。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。对应到图上，邻接表中，每个顶点的链表中，存储的就是这个顶点指向的顶点，逆邻接表中，每个顶点的链表中，存储的是指向这个顶点的顶点。如果要查找某个用户关注了哪些用户，我们可以在邻接表中查找；如果要查找某个用户被哪些用户关注了，我们从逆邻接表中查找。
基础的邻接表不适合快速判断两个用户之间是否是关注与被关注的关系，所以我们选择改进版本，将邻接表中的链表改为支持快速查找的动态数据结构。选择哪种动态数据结构呢？红黑树、跳表、有序动态数组还是散列表呢？
因为我们需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或者关注列表，用跳表这种结构再合适不过了。这是因为，跳表插入、删除、查找都非常高效，时间复杂度是O(logn)，空间复杂度上稍高，是O(n)。最重要的一点，跳表中存储的数据本来就是有序的了，分页获取粉丝列表或关注列表，就非常高效。
如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，我们可以将整个社交关系存储在内存中，上面的解决思路是没有问题的。但是如果像微博那样有上亿的用户，数据规模太大，我们就无法全部存储在内存中了。这个时候该怎么办呢？
我们可以通过哈希算法等数据分片方式，将邻接表存储在不同的机器上。你可以看下面这幅图，我们在机器1上存储顶点1，2，3的邻接表，在机器2上，存储顶点4，5的邻接表。逆邻接表的处理方式也一样。当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找。
除此之外，我们还有另外一种解决思路，就是利用外部存储（比如硬盘），因为外部存储的存储空间要比内存会宽裕很多。数据库是我们经常用来持久化存储关系数据的，所以我这里介绍一种数据库的存储方式。
我用下面这张表来存储这样一个图。为了高效地支持前面定义的操作，我们可以在表上建立多个索引，比如第一列、第二列，给这两列都建立索引。
内容小结今天我们学习了图这种非线性表数据结构，关于图，你需要理解这样几个概念：无向图、有向图、带权图、顶点、边、度、入度、出度。除此之外，我们还学习了图的两个主要的存储方式：邻接矩阵和邻接表。
邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。
课后思考 关于开篇思考题，我们只讲了微博这种有向图的解决思路，那像微信这种无向图，应该怎么存储呢？你可以照着我的思路，自己做一下练习。
除了我今天举的社交网络可以用图来表示之外，符合图这种结构特点的例子还有很多，比如知识图谱（Knowledge Graph）。关于图这种数据结构，你还能想到其他生活或者工作中的例子吗？</description></item><item><title>30_目标代码的生成和优化（二）：如何适应各种硬件架构？</title><link>https://artisanbox.github.io/6/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/30/</guid><description>前一讲，我带你了解了指令选择和寄存器分配，本节课我们继续讲解目标代码生成的，第三个需要考虑的因素：指令重排序（Instruction Scheduling）。
我们可以通过重新排列指令，让代码的整体执行效率加快。那你可能会问了：就算重新排序了，每一条指令还是要执行啊？怎么就会变快了呢？
别着急，本节课我就带你探究其中的原理和算法，来了解这个问题。而且，我还会带你了解LLVM是怎么把指令选择、寄存器分配、指令重排序这三项工作组织成一个完整流程，完成目标代码生成的任务的。这样，你会对编译器后端的代码生成过程形成完整的认知，为正式做一些后端工作打下良好的基础。
首先，我们来看看指令重排序的问题。
指令重排序如果你有上面的疑问，其实是很正常的。因为我们通常会把CPU看做一个整体，把CPU执行指令的过程想象成，依此检票进站的过程，改变不同乘客的次序，并不会加快检票的速度。所以，我们会自然而然地认为改变顺序并不会改变总时间。
但当我们进入CPU内部，会看到CPU是由多个功能部件构成的。下图是Ice Lake微架构的CPU的内部构成（从Intel公司的技术手册中获取）：
在这个结构中，一条指令执行时，要依次用到多个功能部件，分成多个阶段，虽然每条指令是顺序执行的，但每个部件的工作完成以后，就可以服务于下一条指令，从而达到并行执行的效果。这种结构叫做流水线（pipeline）结构。我举例子说明一下，比如典型的RISC指令在执行过程会分成前后共5个阶段。
IF：获取指令； ID（或RF）：指令解码和获取寄存器的值； EX：执行指令； ME（或MEM）：内存访问（如果指令不涉及内存访问，这个阶段可以省略）； WB：写回寄存器。 对于CISC指令，CPU的流水线会根据指令的不同，分成更多个阶段，比如7个、10个甚至更多。
在执行指令的阶段，不同的指令也会由不同的单元负责，我们可以把这些单元叫做执行单元，比如，Intel的Ice Lake架构的CPU有下面这些执行单元：
其他执行单元还有：BM、Vec ALU、Vec SHFT、Vec Add、Vec Mul、Shuffle等。
因为CPU内部存在着多个功能单元，所以在同一时刻，不同的功能单元其实可以服务于不同的指令，看看下面这个图；
这样的话，多条指令实质上是并行执行的，从而减少了总的执行时间，这种并行叫做指令级并行：
如果没有这种并行结构，或者由于指令之间存在依赖关系，无法并行，那么执行周期就会大大加长：
我们来看一个实际的例子。
为了举例子方便，我们做个假设：假设load和store指令需要3个时钟周期来读写数据，add指令需要1个时钟周期，mul指令需要2个时钟周期。
图中橙色的编号是原来的指令顺序，绿色的数字是每条指令开始时的时钟周期，你把每条指令的时钟周期累计一下就能算出来。最后一条指令开始的时钟周期是20，该条指令运行需要3个时钟周期，所以在第22个时钟周期执行完所有的指令。右边是重新排序后的指令，一共花了13个时钟周期。这个优化力度还是很大的！
仔细看一下左边前两条指令，这两条指令的意思是：先加载数据到寄存器，然后做一个加法。但加载需要3个时钟周期，所以add指令无法执行，只能干等着。
右列的前三条都是load指令，它们之间没有数据依赖关系，我们可以每个时钟周期启动一个，到了第四个时钟周期，每一条指令的数据已经加载完毕，所以就可以执行加法运算了。
我们可以把右边的内容画成下面的样子，你能看到，很多指令在时钟周期上是重叠的，这就是指令级并行的特点。
当然了，不是所有的指令都可以并行，最后的3条指令就是顺序执行的，导致无法并行的原因有几个：
数据依赖约束 如果后一条指令要用到前一条指令的结果，那必须排在它后面，比如下面两条指令：add和mul。
对于第二条指令来说，除了获取指令的阶段（IF）可以和第一条指令并行以外，其他阶段需要等第一条指令的结果写入r1，第二条指令才可以使用r1的值继续运行。
add r2, r1 mul r3, r1 功能部件约束 如果只有一个乘法计算器，那么一次只能执行一条乘法运算。
指令流出约束 指令流出部件一次流出n条指令。
寄存器约束 寄存器数量有限，指令并行时使用的寄存器不可以超标。
后三者也可以合并成为一类，称作资源约束。
在数据依赖约束中，如果有因为使用同一个存储位置，而导致不能并行的，可以用重命名变量的方式消除，这类约束被叫做伪约束。而先写再写，以及先读后写是伪约束的两种呈现方式：
先写再写：如果指令A写一个寄存器或内存位置，B也写同一个位置，就不能改变A和B的执行顺序，不过我们可以修改程序，让A和B写不同的位置。
先读后写：如果A必须在B写某个位置之前读某个位置，那么不能改变A和B的执行顺序。除非能够通过重命名让它们使用不同的位置。
以上就是指令重排序的原理，掌握这个原理你就明白为什么重排序可以提升性能了，不过明白原理之后，我们还有能够用算法实现出来才行。
用算法排序的关键点，是要找出代码之间的数据依赖关系。下图展现了示例中各行代码之间的数据依赖，可以叫做数据的依赖图（dependence graph）。它的边代表了值的流动，比如a行加载了一个数据到r1，b行利用r1来做计算，所以b行依赖a行，这个图也可以叫做优先图（precedence graph），因为a比b优先，b比d优先。
我们可以给图中的每个节点再加上两个属性，利用这两个属性，就可以对指令进行排序了：
一是操作类型，因为这涉及它所需要的功能单元。 二是时延属性，也就是每条指令所需的时钟周期。 图中的a、c、e、g是叶子，它们没有依赖任何其他的节点，所以尽量排在前面。b、d、f、h必须出现在各自所依赖的节点后面。而根节点i，总是要排在最后面。</description></item><item><title>30_答疑文章（二）：用动态的观点看加锁</title><link>https://artisanbox.github.io/1/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/30/</guid><description>在第20和21篇文章中，我和你介绍了InnoDB的间隙锁、next-key lock，以及加锁规则。在这两篇文章的评论区，出现了很多高质量的留言。我觉得通过分析这些问题，可以帮助你加深对加锁规则的理解。
所以，我就从中挑选了几个有代表性的问题，构成了今天这篇答疑文章的主题，即：用动态的观点看加锁。
为了方便你理解，我们再一起复习一下加锁规则。这个规则中，包含了两个“原则”、两个“优化”和一个“bug”：
原则1：加锁的基本单位是next-key lock。希望你还记得，next-key lock是前开后闭区间。 原则2：查找过程中访问到的对象才会加锁。 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 接下来，我们的讨论还是基于下面这个表t：
CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 不等号条件里的等值查询有同学对“等值查询”提出了疑问：等值查询和“遍历”有什么区别？为什么我们文章的例子里面，where条件是不等号，这个过程里也有等值查询？
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;我们一起来看下这个例子，分析一下这条查询语句的加锁范围：
begin; select * from t where id&amp;gt;9 and id&amp;lt;12 order by id desc for update; 利用上面的加锁规则，我们知道这个语句的加锁范围是主键索引上的 (0,5]、(5,10]和(10, 15)。也就是说，id=15这一行，并没有被加上行锁。为什么呢？
我们说加锁单位是next-key lock，都是前开后闭区间，但是这里用到了优化2，即索引上的等值查询，向右遍历的时候id=15不满足条件，所以next-key lock退化为了间隙锁 (10, 15)。</description></item><item><title>30_部门响应：设备如何处理内核IO包？</title><link>https://artisanbox.github.io/9/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/30/</guid><description>你好，我是LMOS。
在上一课中，我们实现了建立设备的接口，这相当于制定了部门的相关法规，只要遵守这些法规就能建立一个部门。当然，建立了一个部门，是为了干活的，吃空饷可不行。
其实一个部门的职责不难确定，它应该能对上级下发的任务作出响应，并完成相关工作，而这对应到设备，就是如何处理内核的I/O包，这节课我们就来解决这个问题。
首先，我们需要搞清楚什么是I/O包，然后实现内核向设备发送I/O包的工作。最后，我还会带你一起来完成一个驱动实例，用于处理I/O包，这样你就能真正理解这里的来龙去脉了。
好，让我们开始今天的学习吧！代码你可以从这里下载。
什么是I/O包就像你要给部门下达任务时，需要准备材料报表之类的东西。同样，内核要求设备做什么事情，完成什么功能，必须要告诉设备的驱动程序。
内核要求设备完成任务，无非是调用设备的驱动程序函数，把完成任务的细节用参数的形式传递给设备的驱动程序。
由于参数很多，而且各种操作所需的参数又不相同，所以我们就想到了更高效的管理方法，也就是把各种操作所需的各种参数封装在一个数据结构中，称为I/O包，这样就可以统一驱动程序功能函数的形式了。
思路理清以后，现在我们来设计这个数据结构，如下所示。
typedef struct s_OBJNODE { spinlock_t on_lock; //自旋锁 list_h_t on_list; //链表 sem_t on_complesem; //完成信号量 uint_t on_flgs; //标志 uint_t on_stus; //状态 sint_t on_opercode; //操作码 uint_t on_objtype; //对象类型 void* on_objadr; //对象地址 uint_t on_acsflgs; //访问设备、文件标志 uint_t on_acsstus; //访问设备、文件状态 uint_t on_currops; //对应于读写数据的当前位置 uint_t on_len; //对应于读写数据的长度 uint_t on_ioctrd; //IO控制码 buf_t on_buf; //对应于读写数据的缓冲区 uint_t on_bufcurops; //对应于读写数据的缓冲区的当前位置 size_t on_bufsz; //对应于读写数据的缓冲区的大小 uint_t on_count; //对应于对象节点的计数 void* on_safedsc; //对应于对象节点的安全描述符 void* on_fname; //对应于访问数据文件的名称 void* on_finode; //对应于访问数据文件的结点 void* on_extp; //用于扩展 }objnode_t; 现在你可能还无法从objnode_t这个名字看出它跟I/O包的关系。但你从刚才的代码里可以看出，objnode_t的数据结构中包括了各个驱动程序功能函数的所有参数。</description></item><item><title>30｜面向对象编程第2步：剖析一些技术细节</title><link>https://artisanbox.github.io/3/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/32/</guid><description>你好，我是宫文学。
在上一节课里，我们实现了基本的面向对象特性，包括声明类、创建对象、访问对象的属性和方法等等。
本来，我想马上进入对象的继承和多态的环节。但在准备示例程序的过程中，我发现有一些技术细节还是值得单独拿出来，和你剖析一下的，以免你在看代码的时候可能会抓不住关键点，不好消化。俗话说，魔鬼都在细节中。搞技术的时候，经常一个小细节就会成为拦路虎。
我想给你剖析的技术细节呢，主要是语义分析和AST解释器方面的。通过研究这些技术细节，你会对面向对象的底层实现技术有更加细致的了解。
技术细节：语义分析语义分析方面的技术细节包括：如何设计和保存class的符号、如何设计class对应的类型、如何给This表达式做引用消解、如何消解点符号表达式中变量等等。
首先看看第一个问题，就是如何在符号表里保存class的符号。
我们知道，符号表里存储的是我们自己在程序里声明出来的那些符号。在上一节课之前，我们在符号表里主要保存了两类数据：变量和函数。而class也是我们用程序声明出来的，所以也可以被纳入到符号表里保存。
你应该还记得，我们的符号表采用的是一种层次化的数据结构，也就是Scope的层层嵌套。而且，TypeScript只允许在顶层的作用域中声明class，不允许在class内部或函数内部嵌套声明class，所以class的符号总是被保存在顶层的Scope中。
其实在TypeScript中，我们还可以在一个文件（或模块）里引用另一个文件里定义的类，这样你就能在当前文件里使用这些外部的类了。但我们其实并不是把外部类的全部代码都导入进来，而是只需要引入它们的符号就行了。在class符号里有这些类的描述信息，这些信息叫做元数据。元数据里会包括它都有哪些属性、哪些方法，分别都是什么类型的，等等。这些保存在符号里的这些信息，其实足够我们使用这个类了，我们不用去管这个类的实现细节。
你也可以对比一下FunctionSymbol的设计。FunctionSymbol里会记录函数的名称、参数个数、参数类型和返回值类型。你通过这些信息就可以调用一个函数，完全不用管这个函数的实现细节，也不用区分它是内置函数，还是你自己写的函数，调用方式都是一样的。
说完了class的符号设计和保存，我们再进入第二个技术点，讨论一下class的类型问题。
我们说过，class给我们提供了一个自定义类型的能力。那这个自定义的类型如何表达呢？
在前面的课程中，我们已经形成了自己的一套类型体系，用于进行类型计算。而在这个类型体系中，有一种类型叫做NamedType。这些类型都有名称，并且还有父类型。我们用NamedType首先表示了Number、String、Boolean这些TypeScript官方规定的类型，还用它来表示了Integer和Decimal这两个Number类型的子类型，这两个类型是我们自己设计的。
那其实NamedType就可以用来表示一个class的类型信息，以便参与类型计算。
这里你可能会提出一个问题：class本身不就是类型吗？我们在ClassSymbol里已经保存了类的各种描述信息，为什么还要用到NamedType呢？
采用这样的设计有几个原因。首先，并不是所有的类型都是用class定义出来的。比如系统里有一些内置的类型。再比如，如果你用TypeScript调用其他语言编写的库，比如一些AI库，你可以把其他语言的类型映射成TypeScript语言的类型。所以说，类型的来源并不只有自定义的class。
第二个原因是由类型计算的性质导致的。在我们目前的类型计算中，我们基本只用到了类型的名称和父子类型关系这两个信息，其他信息都没有用到，所以就不需要在类型体系中涉及。
不过，使用NamedType这种设计其实有个潜台词，就是我们类型系统是Norminal的类型系统。这是什么意思呢？Norminal的意思是说，我们在做类型比较的时候，仅仅是通过类型的名称来确定两个类型是否相同，或者是否存在父子关系。与之对应的另一种类型系统是structural的，也就是只要两个类型拥有的方法是一致的，那就认为它们是相同的类型。像Java、C++这些语言，采用的是Nominal的类型系统，而TypeScript和Go等语言，采用的是Structural的类型系统。这个话题我们就不展开了，有兴趣你可以多查阅这方面的资料。
不过，为了简单，我们目前的实现暂且采用Norminal的类型，只通过名称来区分相互之间的关系。
在分析完了class的符号和类型之后，我们再来看看它的用途。这就进入了第三个技术点，也就是如何消解This表达式。
我们知道，this表达式的使用场景，是在类的方法中指代当前对象的数据。那么它的类型是什么呢？在做引用消解的时候，应该让它引用哪个符号呢？
this的类型，不用说，肯定就是指当前的这个class对应的类型，这个不会有疑问。
那它应该被关联到什么符号上呢？我们知道，当程序中出现某个变量名称或函数名称的时候，我们会把这些AST节点关联到符号表里的VarSymbol和FunctionSymbol上，this当然也不会例外。this在被用于程序中的时候，其用法跟普通的一个对象类型的变量是没有区别的。那我们是否应该在每个用到this的方法里，创建一个this变量呢？
这样当然可以，但其实也没有必要。因为每个函数都可能用到this关键字，所以如果在每个方法里都创建一个this变量有点啰嗦。我们只需要简单地把this变量跟ClassSymbol关联起来就行了，在使用的时候也没有什么不方便的。我们下面在讲AST解释器的实现机制里，会进一步看看如何通过this来访问对象数据。
接下来，我们再看看第四个技术点：对点符号表达式的引用消解。
在上一节课的示例程序中，我们可以通过“this.weight”、“mammal.color”、“mammal.speak()”这样的点符号表达式访问对象的属性和方法。
我们知道，在做引用消解的时候，需要把这里面的this、mammal、color、speak()都关联到相应的符号上，这样我们就知道这些标识符都是在哪里声明的了。
不过，之前我们不是已经都做过引用消解了吗？为什么这里又要把点符号的引用消解单独拎出来分析呢？
这是因为，之前我们做变量和函数的引用消解的时候，只需要利用变量和函数的名称信息就行了。但在点符号这边，只依赖名称是不行的，还必须依赖类型信息。
比如，对于mammal.color这个表达式。我们在上下文里，很容易找到mammal是在哪里声明的。但color就不一样了。这个color是在哪里声明的呢？这个时候，你就必须知道mammal的类型，然后再找到mammal的定义。这样，你才能知道mammal是否有一个叫做color的属性。
那你可能说，这很简单呀，我们只需要先计算出每个表达式的类型，然后再做引用消解就可以了呀。
没那么简单。为什么呢？因为类型计算的时候，也需要用到引用消解的结果。比如在mammal.color中，如果你不知道mammal是在哪里声明的，就不能知道它的类型，那也就更没有办法去消解color属性了。
所以，在语义分析中，我们需要把类型计算和引用消解交叉着进行才行，不能分成单独的两个阶段。在《编译原理实战课》中，我曾经分析过Java的前端编译器的特点。这种多个分析工作穿插执行的情况，是Java编译器代码中最难以阅读和跟踪的部分，但你要知道这背后的原因。
我还给你提供了一个更复杂一点的例子，你可以先看一下：
class Human{ swim(){ console.log("swim"); } } class Bird{ fly(){ console.log(&amp;ldquo;fly&amp;rdquo;); } }
function foo(animal:Human|Bird){ if (animal instanceof Human){ animal.swim(); } else{ animal.fly(); } } 这个例子里有Human和Bird两个类，Human有swim()方法，而Bird有fly()方法。不过，我们可以声明一个变量animal，是Human和Bird的联合类型。那么，你什么时候可以调用animal的swim()方法，什么时候可以调用它的fly()方法呢？这个时候你就要基于数据流分析方法，先进行类型的窄化，然后才能把swim()和fly()两个方法正确地消解。
好了，关于语义分析部分的一些技术点，我就先剖析到这里。接着我们看看AST解释器中的一些技术。
技术细节：Ast解释器实现Ast解释器的时候，我们也涉及了不少的技术细节，包括如何表示对象数据、对象数据在栈桢中的存储方式、如何以左值和右值的方式访问对象的属性等。
首先我们看看如何表示对象的数据。上一节课里，我们提到用一个Map&amp;lt;Symbol, any&amp;gt;来存储对象数据就行了。我们在类中声明的每一个属性，都对应着一个Symbol，所以我们就可以用Symbol作为key，来访问对象的数据。
其实，我们的栈桢也是这样设计的。每个栈桢也是一个Map&amp;lt;Symbol, any&amp;gt;。你如果想访问哪个变量的数据，就把变量的Symbol作为key，到Map里去查找就好了。
不过，如果只用一个Map来代表对象数据，数据的接收方可能不知道该数据是属于哪个类的，在实现一些功能的时候不方便。所以我们就专门设计了一个PlayObject对象，在对象里包含了ClassSymbol和对象数据两方面的信息，具体实现如下：
class PlayObject{ classSym:ClassSymbol; data:Map&amp;lt;Symbol,any&amp;gt; = new Map(); constructor(classSym:ClassSymbol){ this.</description></item><item><title>31_GPU（下）：为什么深度学习需要使用GPU？</title><link>https://artisanbox.github.io/4/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/31/</guid><description>上一讲，我带你一起看了三维图形在计算机里的渲染过程。这个渲染过程，分成了顶点处理、图元处理、 栅格化、片段处理，以及最后的像素操作。这一连串的过程，也被称之为图形流水线或者渲染管线。
因为要实时计算渲染的像素特别地多，图形加速卡登上了历史的舞台。通过3dFx的Voodoo或者NVidia的TNT这样的图形加速卡，CPU就不需要再去处理一个个像素点的图元处理、栅格化和片段处理这些操作。而3D游戏也是从这个时代发展起来的。
你可以看这张图，这是“古墓丽影”游戏的多边形建模的变化。这个变化，则是从1996年到2016年，这20年来显卡的进步带来的。
图片来源Shader的诞生和可编程图形处理器不知道你有没有发现，在Voodoo和TNT显卡的渲染管线里面，没有“顶点处理“这个步骤。在当时，把多边形的顶点进行线性变化，转化到我们的屏幕的坐标系的工作还是由CPU完成的。所以，CPU的性能越好，能够支持的多边形也就越多，对应的多边形建模的效果自然也就越像真人。而3D游戏的多边形性能也受限于我们CPU的性能。无论你的显卡有多快，如果CPU不行，3D画面一样还是不行。
所以，1999年NVidia推出的GeForce 256显卡，就把顶点处理的计算能力，也从CPU里挪到了显卡里。不过，这对于想要做好3D游戏的程序员们还不够，即使到了GeForce 256。整个图形渲染过程都是在硬件里面固定的管线来完成的。程序员们在加速卡上能做的事情呢，只有改配置来实现不同的图形渲染效果。如果通过改配置做不到，我们就没有什么办法了。
这个时候，程序员希望我们的GPU也能有一定的可编程能力。这个编程能力不是像CPU那样，有非常通用的指令，可以进行任何你希望的操作，而是在整个的渲染管线（Graphics Pipeline）的一些特别步骤，能够自己去定义处理数据的算法或者操作。于是，从2001年的Direct3D 8.0开始，微软第一次引入了可编程管线（Programable Function Pipeline）的概念。
早期的可编程管线的GPU，提供了单独的顶点处理和片段处理（像素处理）的着色器一开始的可编程管线呢，仅限于顶点处理（Vertex Processing）和片段处理（Fragment Processing）部分。比起原来只能通过显卡和Direct3D这样的图形接口提供的固定配置，程序员们终于也可以开始在图形效果上开始大显身手了。
这些可以编程的接口，我们称之为Shader，中文名称就是着色器。之所以叫“着色器”，是因为一开始这些“可编程”的接口，只能修改顶点处理和片段处理部分的程序逻辑。我们用这些接口来做的，也主要是光照、亮度、颜色等等的处理，所以叫着色器。
这个时候的GPU，有两类Shader，也就是Vertex Shader和Fragment Shader。我们在上一讲看到，在进行顶点处理的时候，我们操作的是多边形的顶点；在片段操作的时候，我们操作的是屏幕上的像素点。对于顶点的操作，通常比片段要复杂一些。所以一开始，这两类Shader都是独立的硬件电路，也各自有独立的编程接口。因为这么做，硬件设计起来更加简单，一块GPU上也能容纳下更多的Shader。
不过呢，大家很快发现，虽然我们在顶点处理和片段处理上的具体逻辑不太一样，但是里面用到的指令集可以用同一套。而且，虽然把Vertex Shader和Fragment Shader分开，可以减少硬件设计的复杂程度，但是也带来了一种浪费，有一半Shader始终没有被使用。在整个渲染管线里，Vertext Shader运行的时候，Fragment Shader停在那里什么也没干。Fragment Shader在运行的时候，Vertext Shader也停在那里发呆。
本来GPU就不便宜，结果设计的电路有一半时间是闲着的。喜欢精打细算抠出每一分性能的硬件工程师当然受不了了。于是，统一着色器架构（Unified Shader Architecture）就应运而生了。
既然大家用的指令集是一样的，那不如就在GPU里面放很多个一样的Shader硬件电路，然后通过统一调度，把顶点处理、图元处理、片段处理这些任务，都交给这些Shader去处理，让整个GPU尽可能地忙起来。这样的设计，就是我们现代GPU的设计，就是统一着色器架构。
有意思的是，这样的GPU并不是先在PC里面出现的，而是来自于一台游戏机，就是微软的XBox 360。后来，这个架构才被用到ATI和NVidia的显卡里。这个时候的“着色器”的作用，其实已经和它的名字关系不大了，而是变成了一个通用的抽象计算模块的名字。
正是因为Shader变成一个“通用”的模块，才有了把GPU拿来做各种通用计算的用法，也就是GPGPU（General-Purpose Computing on Graphics Processing Units，通用图形处理器）。而正是因为GPU可以拿来做各种通用的计算，才有了过去10年深度学习的火热。
现代GPU的三个核心创意讲完了现代GPU的进化史，那么接下来，我们就来看看，为什么现代的GPU在图形渲染、深度学习上能那么快。
芯片瘦身我们先来回顾一下，之前花了很多讲仔细讲解的现代CPU。现代CPU里的晶体管变得越来越多，越来越复杂，其实已经不是用来实现“计算”这个核心功能，而是拿来实现处理乱序执行、进行分支预测，以及我们之后要在存储器讲的高速缓存部分。
而在GPU里，这些电路就显得有点多余了，GPU的整个处理过程是一个流式处理（Stream Processing）的过程。因为没有那么多分支条件，或者复杂的依赖关系，我们可以把GPU里这些对应的电路都可以去掉，做一次小小的瘦身，只留下取指令、指令译码、ALU以及执行这些计算需要的寄存器和缓存就好了。一般来说，我们会把这些电路抽象成三个部分，就是下面图里的取指令和指令译码、ALU和执行上下文。
多核并行和SIMT这样一来，我们的GPU电路就比CPU简单很多了。于是，我们就可以在一个GPU里面，塞很多个这样并行的GPU电路来实现计算，就好像CPU里面的多核CPU一样。和CPU不同的是，我们不需要单独去实现什么多线程的计算。因为GPU的运算是天然并行的。
我们在上一讲里面其实已经看到，无论是对多边形里的顶点进行处理，还是屏幕里面的每一个像素进行处理，每个点的计算都是独立的。所以，简单地添加多核的GPU，就能做到并行加速。不过光这样加速还是不够，工程师们觉得，性能还有进一步被压榨的空间。
我们在第27讲里面讲过，CPU里有一种叫作SIMD的处理技术。这个技术是说，在做向量计算的时候，我们要执行的指令是一样的，只是同一个指令的数据有所不同而已。在GPU的渲染管线里，这个技术可就大有用处了。
无论是顶点去进行线性变换，还是屏幕上临近像素点的光照和上色，都是在用相同的指令流程进行计算。所以，GPU就借鉴了CPU里面的SIMD，用了一种叫作SIMT（Single Instruction，Multiple Threads）的技术。SIMT呢，比SIMD更加灵活。在SIMD里面，CPU一次性取出了固定长度的多个数据，放到寄存器里面，用一个指令去执行。而SIMT，可以把多条数据，交给不同的线程去处理。
各个线程里面执行的指令流程是一样的，但是可能根据数据的不同，走到不同的条件分支。这样，相同的代码和相同的流程，可能执行不同的具体的指令。这个线程走到的是if的条件分支，另外一个线程走到的就是else的条件分支了。
于是，我们的GPU设计就可以进一步进化，也就是在取指令和指令译码的阶段，取出的指令可以给到后面多个不同的ALU并行进行运算。这样，我们的一个GPU的核里，就可以放下更多的ALU，同时进行更多的并行运算了。
GPU里的“超线程”虽然GPU里面的主要以数值计算为主。不过既然已经是一个“通用计算”的架构了，GPU里面也避免不了会有if…else这样的条件分支。但是，在GPU里我们可没有CPU这样的分支预测的电路。这些电路在上面“芯片瘦身”的时候，就已经被我们砍掉了。
所以，GPU里的指令，可能会遇到和CPU类似的“流水线停顿”问题。想到流水线停顿，你应该就能记起，我们之前在CPU里面讲过超线程技术。在GPU上，我们一样可以做类似的事情，也就是遇到停顿的时候，调度一些别的计算任务给当前的ALU。
和超线程一样，既然要调度一个不同的任务过来，我们就需要针对这个任务，提供更多的执行上下文。所以，一个Core里面的执行上下文的数量，需要比ALU多。
GPU在深度学习上的性能差异在通过芯片瘦身、SIMT以及更多的执行上下文，我们就有了一个更擅长并行进行暴力运算的GPU。这样的芯片，也正适合我们今天的深度学习的使用场景。
一方面，GPU是一个可以进行“通用计算”的框架，我们可以通过编程，在GPU上实现不同的算法。另一方面，现在的深度学习计算，都是超大的向量和矩阵，海量的训练样本的计算。整个计算过程中，没有复杂的逻辑和分支，非常适合GPU这样并行、计算能力强的架构。
我们去看NVidia 2080显卡的技术规格，就可以算出，它到底有多大的计算能力。
2080一共有46个SM（Streaming Multiprocessor，流式处理器），这个SM相当于GPU里面的GPU Core，所以你可以认为这是一个46核的GPU，有46个取指令指令译码的渲染管线。每个SM里面有64个Cuda Core。你可以认为，这里的Cuda Core就是我们上面说的ALU的数量或者Pixel Shader的数量，46x64呢一共就有2944个Shader。然后，还有184个TMU，TMU就是Texture Mapping Unit，也就是用来做纹理映射的计算单元，它也可以认为是另一种类型的Shader。
图片来源2080 Super显卡有48个SM，比普通版的2080多2个。每个SM（SM也就是GPU Core）里有64个Cuda Core，也就是Shader2080的主频是1515MHz，如果自动超频（Boost）的话，可以到1700MHz。而NVidia的显卡，根据硬件架构的设计，每个时钟周期可以执行两条指令。所以，能做的浮点数运算的能力，就是：</description></item><item><title>31_内存计算：对海量数据做计算，到底可以有多快？</title><link>https://artisanbox.github.io/6/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/31/</guid><description>内存计算是近十几年来，在数据库和大数据领域的一个热点。随着内存越来越便宜，CPU的架构越来越先进，整个数据库都可以放在内存中，并通过SIMD和并行计算技术，来提升数据处理的性能。
我问你一个问题：做1.6亿条数据的汇总计算，需要花费多少时间呢？几秒？几十秒？还是几分钟？如果你经常使用数据库，肯定会知道，我们不会在数据库的一张表中保存上亿条的数据，因为处理速度会很慢。
但今天，我会带你采用内存计算技术，提高海量数据处理工作的性能。与此同时，我还会介绍SIMD指令、高速缓存和局部性、动态优化等知识点。这些知识点与编译器后端技术息息相关，掌握这些内容，会对你从事基础软件研发工作，有很大的帮助。
了解SIMD本节课所采用的CPU，支持一类叫做SIMD（Single Instruction Multiple Data）的指令，它的字面意思是：单条指令能处理多个数据。相应的，你可以把每次只处理一个数据的指令，叫做SISD（Single Instruction Single Data）。
SISD使用普通的寄存器进行操作，比如加法：
addl $10, %eax 这行代码是把一个32位的整型数字，加到%eax寄存器上（在x86-64架构下，这个寄存器一共有64位，但这个指令只用它的低32位，高32位是闲置的）。
这种一次只处理一个数据的计算，叫做标量计算；一次可以同时处理多个数据的计算，叫做矢量计算。它在一个寄存器里可以并排摆下4个、8个甚至更多标量，构成一个矢量。图中ymm寄存器是256位的，可以支持同时做4个64位数的计算（xmm寄存器是它的低128位）。
如果不做64位整数，而做32位整数计算，一次能计算8个，如果做单字节（8位）数字的计算，一次可以算32个！
1997年，Intel公司推出了奔腾处理器，带有MMX指令集，意思是多媒体扩展。当时，让计算机能够播放多媒体（比如播放视频），是一个巨大的进步。但播放视频需要大量的浮点计算，依靠原来CPU的浮点运算功能并不够。
所以，Intel公司就引入了MMX指令集，和容量更大的寄存器来支持一条指令，同时计算多个数据，这是在PC上最早的SIMD指令集。后来，SIMD又继续发展，陆续产生了SSE（流式SIMD扩展）、AVX（高级矢量扩展）指令集，处理能力越来越强大。
2017年，Intel公司发布了一款至强处理器，支持AVX-512指令（也就是它的一个寄存器有512位）。每次能处理8个64位整数，或16个32位整数，或者32个双精度数、64个单精度数。你想想，一条指令顶64条指令，几十倍的性能提升，是不是很厉害！
那么你的电脑是否支持SIMD指令？又支持哪些指令集呢？在命令行终端，打下面的命令，你可以查看CPU所支持的指令集。
sysctl -a | grep features | grep cpu //macOs操作系统 cat /proc/cpuinfo //Linux操作系统 现在，想必你已经知道了SIMD指令的强大之处了。而它的实际作用主要有以下几点：
SIMD有助于多媒体的处理，比如在电脑上流畅地播放视频，或者开视频会议；
在游戏领域，图形渲染主要靠GPU，但如果你没有强大的GPU，还是要靠CPU的SIMD指令来帮忙；
在商业领域，数据库系统会采用SIMD来快速处理海量的数据；
人工智能领域，机器学习需要消耗大量的计算量，SIMD指令可以提升机器学习的速度。
你平常写的程序，编译器也会优化成，尽量使用SIMD指令来提高性能。
所以，我们所用到的程序，其实天天在都在执行SIMD指令。
接下来，我来演示一下如何使用SIMD指令，与传统的数据处理技术做性能上的对比，并探讨如何在编译器中生成SIMD指令，这样你可以针对自己的项目充分发挥SIMD指令的优势。
Intel公司为SIMD指令提供了一个标准的库，可以生成SIMD的汇编指令。我们写一个简单的程序（参考simd1.c）来对两组数据做加法运算，每组8个整数：
#include &amp;lt;stdio.h&amp;gt; #include &amp;quot;immintrin.h&amp;quot; void sum(){ //初始化两个矢量 ，8个32位整数 __m256i a=_mm256_set_epi32(20,30,40,60,342,34523,474,123); __m256i b=_mm256_set_epi32(234,234,456,78,2345,213,76,88);
//矢量加法 __m256i sum=_mm256_add_epi32(a, b);</description></item><item><title>31_深度和广度优先搜索：如何找出社交网络中的三度好友关系？</title><link>https://artisanbox.github.io/2/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/32/</guid><description>上一节我们讲了图的表示方法，讲到如何用有向图、无向图来表示一个社交网络。在社交网络中，有一个六度分割理论，具体是说，你与世界上的另一个人间隔的关系不会超过六度，也就是说平均只需要六步就可以联系到任何两个互不相识的人。
一个用户的一度连接用户很好理解，就是他的好友，二度连接用户就是他好友的好友，三度连接用户就是他好友的好友的好友。在社交网络中，我们往往通过用户之间的连接关系，来实现推荐“可能认识的人”这么一个功能。今天的开篇问题就是，给你一个用户，如何找出这个用户的所有三度（其中包含一度、二度和三度）好友关系？
这就要用到今天要讲的深度优先和广度优先搜索算法。
什么是“搜索”算法？我们知道，算法是作用于具体数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。这是因为，图这种数据结构的表达能力很强，大部分涉及搜索的场景都可以抽象成“图”。
图上的搜索算法，最直接的理解就是，在图中找出从一个顶点出发，到另一个顶点的路径。具体方法有很多，比如今天要讲的两种最简单、最“暴力”的深度优先、广度优先搜索，还有A*、IDA*等启发式搜索算法。
我们上一节讲过，图有两种主要存储方法，邻接表和邻接矩阵。今天我会用邻接表来存储图。
我这里先给出图的代码实现。需要说明一下，深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。在今天的讲解中，我都针对无向图来讲解。
public class Graph { // 无向图 private int v; // 顶点的个数 private LinkedList&amp;lt;Integer&amp;gt; adj[]; // 邻接表 public Graph(int v) { this.v = v; adj = new LinkedList[v]; for (int i=0; i&amp;lt;v; ++i) { adj[i] = new LinkedList&amp;lt;&amp;gt;(); } }
public void addEdge(int s, int t) { // 无向图一条边存两次 adj[s].add(t); adj[t].add(s); } } 广度优先搜索（BFS）广度优先搜索（Breadth-First-Search），我们平常都简称BFS。直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。理解起来并不难，所以我画了一张示意图，你可以看下。
尽管广度优先搜索的原理挺简单，但代码实现还是稍微有点复杂度。所以，我们重点讲一下它的代码实现。
这里面，bfs()函数就是基于之前定义的，图的广度优先搜索的代码实现。其中s表示起始顶点，t表示终止顶点。我们搜索一条从s到t的路径。实际上，这样求得的路径就是从s到t的最短路径。
public void bfs(int s, int t) { if (s == t) return; boolean[] visited = new boolean[v]; visited[s]=true; Queue&amp;lt;Integer&amp;gt; queue = new LinkedList&amp;lt;&amp;gt;(); queue.</description></item><item><title>31_瞧一瞧Linux：如何获取所有设备信息？</title><link>https://artisanbox.github.io/9/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/31/</guid><description>你好，我是LMOS。
前面我们已经完成了Cosmos的驱动设备的建立，还写好了一个真实的设备驱动。
今天，我们就来看看Linux是如何管理设备的。我们将从Linux如何组织设备开始，然后研究设备驱动相关的数据结构，最后我们还是要一起写一个Linux设备驱动实例，这样才能真正理解它。
感受一下Linux下的设备信息Linux的设计哲学就是一切皆文件，各种设备在Linux系统下自然也是一个个文件。不过这个文件并不对应磁盘上的数据文件，而是对应着存在内存当中的设备文件。实际上，我们对设备文件进行操作，就等同于操作具体的设备。
既然我们了解万事万物，都是从最直观的感受开始的，想要理解Linux对设备的管理，自然也是同样的道理。那么Linux设备文件在哪个目录下呢？其实现在我们在/sys/bus目录下，就可以查看所有的设备了。
Linux用BUS（总线）组织设备和驱动，我们在/sys/bus目录下输入tree命令，就可以看到所有总线下的所有设备了，如下图所示。
上图中，显示了部分Linux设备文件，有些设备文件是链接到其它目录下文件，这不是重点，重点是你要在心中有这个目录层次结构，即总线目录下有设备目录，设备目录下是设备文件。
数据结构我们接着刚才的图往下说，我们能感觉到Linux的驱动模型至少有三个核心数据结构，分别是总线、设备和驱动，但是要像上图那样有层次化地组织它们，只有总线、设备、驱动这三个数据结构是不够的，还得有两个数据结构来组织它们，那就是kobject和kset，下面我们就去研究它们。
kobject与ksetkobject和kset是构成/sys目录下的目录节点和文件节点的核心，也是层次化组织总线、设备、驱动的核心数据结构，kobject、kset数据结构都能表示一个目录或者文件节点。下面我们先来研究一下kobject数据结构，代码如下所示。
struct kobject { const char *name; //名称，反映在sysfs中 struct list_head entry; //挂入kset结构的链表 struct kobject *parent; //指向父结构 struct kset *kset; //指向所属的kset struct kobj_type *ktype; struct kernfs_node *sd; //指向sysfs文件系统目录项 struct kref kref; //引用计数器结构 unsigned int state_initialized:1;//初始化状态 unsigned int state_in_sysfs:1; //是否在sysfs中 unsigned int state_add_uevent_sent:1; unsigned int state_remove_uevent_sent:1; unsigned int uevent_suppress:1; }; 每一个 kobject，都对应着 /sys目录下（其实是sysfs文件系统挂载在/sys目录下） 的一个目录或者文件，目录或者文件的名字就是kobject结构中的name。
我们从kobject结构中可以看出，它挂载在kset下，并且指向了kset，那kset是什么呢？我们来分析分析，它是kobject结构的容器吗？
其实是也不是，因为kset结构中本身又包含一个kobject结构，所以它既是kobject的容器，同时本身还是一个kobject。kset结构代码如下所示。
struct kset { struct list_head list; //挂载kobject结构的链表 spinlock_t list_lock; //自旋锁 struct kobject kobj;//自身包含一个kobject结构 const struct kset_uevent_ops *uevent_ops;//暂时不关注 } __randomize_layout; 看到这里你应该知道了，kset不仅仅自己是个kobject，还能挂载多个kobject，这说明kset是kobject的集合容器。在Linux内核中，至少有两个顶层kset，代码如下所示。</description></item><item><title>31_误删数据后除了跑路，还能怎么办？</title><link>https://artisanbox.github.io/1/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/31/</guid><description>今天我要和你讨论的是一个沉重的话题：误删数据。
在前面几篇文章中，我们介绍了MySQL的高可用架构。当然，传统的高可用架构是不能预防误删数据的，因为主库的一个drop table命令，会通过binlog传给所有从库和级联从库，进而导致整个集群的实例都会执行这个命令。
虽然我们之前遇到的大多数的数据被删，都是运维同学或者DBA背锅的。但实际上，只要有数据操作权限的同学，都有可能踩到误删数据这条线。
今天我们就来聊聊误删数据前后，我们可以做些什么，减少误删数据的风险，和由误删数据带来的损失。
为了找到解决误删数据的更高效的方法，我们需要先对和MySQL相关的误删数据，做下分类：
使用delete语句误删数据行；
使用drop table或者truncate table语句误删数据表；
使用drop database语句误删数据库；
使用rm命令误删整个MySQL实例。
误删行在第24篇文章中，我们提到如果是使用delete语句误删了数据行，可以用Flashback工具通过闪回把数据恢复回来。
Flashback恢复数据的原理，是修改binlog的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保binlog_format=row 和 binlog_row_image=FULL。
具体恢复数据时，对单个事务做如下处理：
对于insert语句，对应的binlog event类型是Write_rows event，把它改成Delete_rows event即可；
同理，对于delete语句，也是将Delete_rows event改为Write_rows event；
而如果是Update_rows的话，binlog里面记录了数据行修改前和修改后的值，对调这两行的位置即可。
如果误操作不是一个，而是多个，会怎么样呢？比如下面三个事务：
(A)delete ... (B)insert ... (C)update ... 现在要把数据库恢复回这三个事务操作之前的状态，用Flashback工具解析binlog后，写回主库的命令是：
(reverse C)update ... (reverse B)delete ... (reverse A)insert ... 也就是说，如果误删数据涉及到了多个事务的话，需要将事务的顺序调过来再执行。
需要说明的是，我不建议你直接在主库上执行这些操作。
恢复数据比较安全的做法，是恢复出一个备份，或者找一个从库作为临时库，在这个临时库上执行这些操作，然后再将确认过的临时库的数据，恢复回主库。
为什么要这么做呢？
这是因为，一个在执行线上逻辑的主库，数据状态的变更往往是有关联的。可能由于发现数据问题的时间晚了一点儿，就导致已经在之前误操作的基础上，业务代码逻辑又继续修改了其他数据。所以，如果这时候单独恢复这几行数据，而又未经确认的话，就可能会出现对数据的二次破坏。
当然，我们不止要说误删数据的事后处理办法，更重要是要做到事前预防。我有以下两个建议：
把sql_safe_updates参数设置为on。这样一来，如果我们忘记在delete或者update语句中写where条件，或者where条件里面没有包含索引字段的话，这条语句的执行就会报错。
代码上线前，必须经过SQL审计。</description></item><item><title>31_运行时（一）：从0到语言级的虚拟化</title><link>https://artisanbox.github.io/7/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/31/</guid><description>你好，我是宫文学。今天，我会带你去考察现代语言设计中的运行时特性，并讨论一下与标准库有关的话题。
你可能要问了，咱们这门课是要讲编译原理啊，为什么要学运行时呢。其实，对于一门语言来说，除了要提供编译器外，还必须提供运行时功能和标准库：一是，编译器生成的目标代码，需要运行时的帮助才能顺利运行；二是，我们写代码的时候，有一些标准的功能，像是读写文件的功能，自己实现起来太麻烦，或者根本不可能用这门语言本身来实现，这时就需要标准库的支持。
其实，我们也经常会接触到运行时和库，但可能只是停留在使用层面上，并不太会关注它们的原理等。如果真要细究起来、真要对编译原理有更透彻的理解的话，你可能就会有下面这些问题了：
到底什么是运行时？任何语言都有运行时吗？运行时和编译器是什么关系？ 什么是标准库？标准库和运行时库又是什么关系？库一般都包含什么功能？ 今天，我们就来探讨一下这些与运行时和标准库有关的话题。这样，你能更加充分地理解设计一门语言要完成哪些工作，以及这些工作跟编译技术又有什么关系，也就能对编译原理有更深一层的理解。
首先，我们来了解一下运行时，以及它和编译技术的关系。
什么是运行时（Runtime）？我们在第5讲说过，每种语言都有一个特定的执行模型（Execution Model）。而这个执行模型就需要运行时系统（Runtime System）的支持。我们把这种可以支撑程序运行的运行时系统，简称为运行时。
那运行时都包含什么功能呢？通常，我们最关心的是三方面的功能：程序运行机制、内存管理机制和并发机制。接下来，我就分别以Java、Python以及C、C++、Go语言的运行时机制为例，做一下运行时的分析，因为它们的使用者比较多，并且体现了一些有代表性的运行时特征。
Java的运行时我们先看看Java语言的运行时系统，也就是JVM。
其实，JVM不仅为Java提供了运行时环境，还为其他所有基于JVM的语言提供了支撑，包括Scala、Clojure、Groovy等。我们可以通过JVM的规范来学习一下它的主要特点。
第一，JVM规定了一套程序的运行机制。JVM支持基于字节码的解释执行机制，还包括了即时编译成机器码并执行的机制。
针对基于字节码的解释执行机制，JVM规范定义下面这些内容：
定义了一套字节码来运行程序。这些字节码能支持一些基本的运算。超出这些基本运算逻辑的，就要自己去实现。比如，idiv指令用于做整数的除法，当除数为零的时候，虚拟缺省的操作是抛出异常。如果你自己的语言是专注于数学计算的，想让整数除以零的结果为无穷大，那么你需要自己去实现这个逻辑。 规定了一套类型系统，包括基础数据类型、数组、引用类型等。所以说，任何运行在JVM上的语言，不管它设计的类型系统是什么样子，编译以后都会变成字节码规定的基础类型。 定义了class文件的结构。class文件规定了把某个类的符号表放在哪里、把字节码放在哪里，所以写编译器的时候要遵守这个规范才能生成正确的class文件。JVM在运行时会加载class文件并执行。 提供了一个基于栈的解释器，来解释执行字节码。编译器要根据这个执行模型来生成正确的字节码。 除了解释执行字节码的机制，JVM还支持即时编译成机器码并执行的机制。它可以调度多个编译器，生成不同优化级别的机器码，这就是分层编译机制。在需要的时候，还可以做逆优化，在不同版本的机器码以及解释执行模式之间做切换。
最后，Java程序之间的互相调用，需要遵循一定的调用约定或二进制标准，包括如何传参数等等。这也是运行机制的一部分。
总体来说，JVM代表了一种比较复杂的运行机制，既可以解释执行，又可以编译成机器码执行。V8的运行时机制也跟JVM也很类似。
第二，JVM对内存做了统一的管理。它把内存划分为程序计数器、虚拟机栈、堆、方法区、运行时常量池和本地方法栈等不同的区域。
对于栈来说，它的栈桢既可以服务于解释执行，又可以用于执行机器码，并且还可以在两种模式之间转换。在解释执行的时候，栈桢里会有一个操作数栈，服务于解释器。我们提到过OSR，也就是在运行一个方法的时候，把这个方法做即时编译，并且把它的栈桢从解释执行的状态切换成运行机器码的状态。而如果遇到逆优化的场景，栈桢又会从运行机器码的状态，切换成解释执行的状态。
对于堆来说，Java提供了垃圾收集器帮助进行内存的自动管理。减少整体的停顿时间，是垃圾收集器设计的重要目标。
第三，JVM封装了操作系统的线程模型，为应用程序提供了并发处理的机制。我会在讲并发机制的时候再展开。
以上就是JVM为运行在其上的任何程序提供的支撑了。在提供这些支撑的同时，运行时系统也给程序运行带来了一些限制。
第一，JVM实际上提供了一个基础的对象模型，JVM上的各种语言必须遵守。所以，虽然Clojure是一个函数式编程语言，但它在底层却不得不使用JVM规定的对象模型。
第二，基于JVM的语言程序要去调用C语言等生成的机器码的库，会比较难。不过，对于同样基于JVM的语言，则很容易实现相互之间的调用，因为它们底层都是类和字节码。
第三，在内存管理上，程序不能直接访问内存地址，也不能手动释放内存。
第四，在并发方面，JVM只提供了线程机制。如果你要使用其他并发模型，比如我们会在34讲中讲到的协程模型和35讲中的Actor模型，需要语言的实现者绕着弯去做，增加一些自己的运行时机制（我会在第34讲来具体介绍）。
好了，以上就是我要通过JVM的例子带你学习的Java的运行时，以及其编译器的影响了。我们再来看看Python的运行时。
Python的运行时在解析Python语言的时候，已经讲了Python的字节码和解释器，以及Python对象模型和程序调用的机制。这里，我再从程序运行机制、内存管理机制、并发机制这三个方面，给你梳理下。
第一，Python也提供了一套字节码，以及运行该字节码的解释器。这套字节码，也是跟Python的类型体系互相配合的。字节码中操作的那些标识符，都是Python的对象引用。
第二，在内存管理方面，Python也提供了自己的机制，包括对栈和堆的管理。
首先，我们看看栈。Python运行程序的时候，有些时候是运行机器码，比如内置函数，而有些时候是解释执行字节码。
运行机器码的时候，栈帧跟C语言程序的栈帧是没啥区别的。而在解释执行字节码的时候，栈帧里会包含一个操作数栈，这点跟JVM的栈机是一样的。如果你再进一步，去看看操作数栈的实现，会发现解释器本身主要就是一个C语言实现的函数，而操作数栈就是这个函数里用到的本地变量。因此操作数栈也会像其他本地变量一样，被优化成尽量使用物理寄存器，从而提高运行效率。这个知识点你要掌握，也就是说，栈桢中的操作数栈，其实是有可能基于物理寄存器的。
然后，Python还提供了对堆的管理机制。程序从堆里申请内存的时候，不是直接从操作系统申请，而是通过Python提供的一个Arena机制，使得内存的申请和释放更加高效、灵活。Python还提供了基于引用的垃圾收集机制（我会在下一讲为你总结垃圾收集机制）。
第三，是并发机制。Python把操作系统的线程进行了封装，让Python程序能支持基于线程的并发。同时，它也实现了协程机制（我会在34讲详细展开）。
好了，我们再继续看看第三类语言，也就是C、C++、Go这样的直接编译成二进制文件执行的语言的运行时。
C、C++、Go的运行时一个有意思的问题是，C语言有没有运行时呢？我们对C语言的印象，是一旦编译完成以后，就是一段完全可以自主运行的二进制代码了，你也可以看到输出的完整的汇编代码。除此之外没有其他，C语言似乎不需要运行时的支持。
所以，C语言最主要的运行时，实际上就是操作系统。C语言和现代的各种操作系统可以说是伴生关系，就像Java和JVM是伴生关系一样。所以，如果我们要深入使用C语言，某种意义上就是要深入了解操作系统的运行机制。
在程序执行机制方面，C语言编译完毕的程序是完全按照操作系统的运行机制来执行的。
在内存管理方面，C语言使用了操作系统提供的线程栈，操作系统能够自动帮助程序管理内存。程序也可以从堆里申请内存，但必须自己负责释放，没有自动内存管理机制。
在并发机制方面，当然也是直接用操作系统提供的线程机制。因为操作系统没有提供协程和Actor机制，所以C语言也没有提供这种并发机制。
不过有一个程序crt0.o，有时被称作是C语言的运行时。它是一段汇编代码（crt0.s），由链接器自动插入到程序里面，主要功能是在调用main函数之前做一些初始化工作，比如设置main函数的参数（argc和argv）、环境变量的地址、调用main函数、设置一些中断向量用于处理程序异常等。所以，这个所谓的运行时所做的工作也特别简单。
不同系统的crt0.s会不太一样，因为CPU架构和ABI是不同的。下面是一个crt0.s的示例代码：
.text .globl _start _start: # _start是链接器需要用到的入口 xor %ebp, %ebp # 让ebp置为0，标记栈帧的底部 mov (%rsp), %edi # 从栈里获得argc的值 lea 8(%rsp), %rsi # 从栈里获得argv的地址 lea 16(%rsp,%rdi,8), %rdx # 从栈里获得envp的地址 xor %eax, %eax # 按照ABI的要求把eax置为0，并与icc兼容 call main # 调用main函数，%edi, %rsi, %rdx是传给main函数的三个参数 mov %eax, %edi # 把main函数的返回值提供给_exit作为第一个参数 xor %eax, %eax # 按照ABI的要求把eax置为0，并与icc兼容 call _exit # 终止程序 可以说，C语言的运行时是一个极端，提供了最少的功能。反过来呢，这也就是给了程序员最大的自由度。C++语言的跟C是类似的，我就不再展开了。总的来说，它们都没有Java和Python那种意义上的运行时。</description></item><item><title>31｜面向对象编程第3步：支持继承和多态</title><link>https://artisanbox.github.io/3/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/33/</guid><description>你好，我是宫文学。
经过前面两节课对面向对象编程的学习，今天这节课，我们终于要来实现到面向对象中几个最核心的功能了。
面向对象编程是目前使用最广泛的编程范式之一。通常，我们说面向对象编程的核心特性有封装、继承和多态这几个方面。只要实现了这几点，就可以获得面向对象编程的各种优势，比如提高代码的可重用性、可扩展性、提高编程效率，等等。
这节课，我们就先探讨一下面向对象的这些核心特性是如何实现的，然后我会带着你动手实现一下，破解其中的技术秘密。了解了这些实现机制，能够帮助你深入理解现代计算机语言更深层次的机制。
首先，我们先来分析面向对象的几个核心特性，并梳理一下实现思路。
面向对象的核心特性及其实现机制第一，是封装特性。
封装是指我们可以把对象内部的数据和实现细节隐藏起来，只对外提供一些公共的接口。这样做的好处，是提高了代码的复用性和安全性，因为内部实现细节只有代码的作者才能够修改，并且这种修改不会影响到类的使用者。
其实封装特性，我们在上两节课已经差不多实现完了。因为我们提供了方法的机制，让方法可以访问对象的内部数据。之后，我们只需要给属性和方法添加访问权限的修饰成分就可以了。比如我们可以声明某些属性和方法是private的，这样，属性和方法就只能由内部的方法去访问了。而对访问权限的检查，我们在语义分析阶段就可以轻松做到。
上一节课，我们已经分析了如何处理点符号表达式。你在程序里可以分析出点号左边的表达式的类型信息，也可以获得对象的属性和方法。再进一步，我们可以给这些属性和方法添加上访问权限的信息，那么这些私有的属性就只可以在内部访问了，比如使用this.xxx表达式，等等。而公有的属性仍然可以在外部访问，跟现在的实现没有区别。
第二，我们看看继承。
用直白的话来说，继承指的是一个class，可以免费获得父类中的属性和方法，从而降低了开发工作量，提高了代码的复用度。
我写了一个示例程序，你可以看一下：
function println(data:any=&amp;quot;&amp;quot;){ console.log(data); } class Mammal{ weight:number = 0; // weight2; color:string; constructor(weight:number, color:string){ this.weight = weight;
this.color = color; } speak(){ println(&amp;quot;Hello, I&amp;rsquo;m a mammal, and my weight is &amp;quot; + this.weight + &amp;quot;.&amp;quot;); } }
class Human extends Mammal{ //新的语法要素：extends name:string; constructor(weight:number, color:string, name:string){ super(weight,color); //新的语法要素：super this.name = name; } swim(){ println(&amp;quot;My weight is &amp;quot; +this.</description></item><item><title>32_FPGA和ASIC：计算机体系结构的黄金时代</title><link>https://artisanbox.github.io/4/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/32/</guid><description>过去很长一段时间里，大家在讲到高科技、互联网、信息技术的时候，谈的其实都是“软件”。从1995年微软发布Windows 95开始，高科技似乎就等同于软件业和互联网。著名的风险投资基金Andreessen Horowitz的合伙人Marc Andreessen，在2011年发表了一篇博客，声称“Software is Eating the World”。Marc Andreessen，不仅是投资人，更是Netscape的创始人之一。他当时的搭档就是我们在前两讲提过的SGI创始人Jim Clark。
的确，过去20年计算机工业界的中心都在软件上。似乎硬件对大家来说，慢慢变成了一个黑盒子。虽然必要，但却显得有点无关紧要。
不过，在上世纪70～80年代，计算机的世界可不是这样的。那个时候，计算机工业届最激动人心的，是层出不穷的硬件。无论是Intel的8086，还是摩托罗拉的68000，这样用于个人电脑的CPU，还是直到今天大家还会提起的Macintosh，还有史上最畅销的计算机Commodore 64，都是在那个时代被创造出来的。
电视剧Halt and Catch Fire，灵感应该就是来自第一台笔记本电脑Compaq Portable的诞生不过，随着计算机主频提升越来越困难。这几年，计算机硬件又进入了一个新的、快速发展的时期。
从树莓派基金会这样的非盈利组织开发35美元的单片机，到Google这样的巨头为了深度学习专门开发出来的TPU，新的硬件层出不穷，也无怪乎David Patterson老爷爷，去年在拿图灵奖之后专门发表讲话，说计算机体系结构又进入了一个黄金时代。那今天我就带你一起来看看，FPGA和ASIC这两个最近比较时髦的硬件发展。
FPGA之前我们讲解CPU的硬件实现的时候说过，其实CPU其实就是一些简单的门电路像搭积木一样搭出来的。从最简单的门电路，搭建成半加器、全加器，然后再搭建成完整功能的ALU。这些电路里呢，有完成各种实际计算功能的组合逻辑电路，也有用来控制数据访问，创建出寄存器和内存的时序逻辑电路。如果你对这块儿内容印象不深，可以回顾一下第12讲到第14讲的内容，以及第17讲的内容。
好了，那现在我问你一个问题，在我们现代CPU里面，有多少个晶体管这样的电路开关呢？这个答案说出来有点儿吓人。一个四核i7的Intel CPU，上面的晶体管数量差不多有20亿个。那接着问题就来了，我们要想设计一个CPU，就要想办法连接这20亿个晶体管。
这已经够难了，后面还有更难的。就像我们写程序一样，连接晶体管不是一次就能完事儿了的。设计更简单一点儿的专用于特定功能的芯片，少不了要几个月。而设计一个CPU，往往要以“年”来计。在这个过程中，硬件工程师们要设计、验证各种各样的技术方案，可能会遇到各种各样的Bug。如果我们每次验证一个方案，都要单独设计生产一块芯片，那这个代价也太高了。
我们有没有什么办法，不用单独制造一块专门的芯片来验证硬件设计呢？能不能设计一个硬件，通过不同的程序代码，来操作这个硬件之前的电路连线，通过“编程”让这个硬件变成我们设计的电路连线的芯片呢？
图片来源XILINX的FPGA芯片这个，就是我们接下来要说的FPGA，也就是现场可编程门阵列（Field-Programmable Gate Array）。看到这个名字，你可能要说了，这里面每个单词单独我都认识，放到一起就不知道是什么意思了。
没关系，我们就从FPGA里面的每一个字符，一个一个来看看它到底是什么意思。
P代表Programmable，这个很容易理解。也就是说这是一个可以通过编程来控制的硬件。 G代表Gate也很容易理解，它就代表芯片里面的门电路。我们能够去进行编程组合的就是这样一个一个门电路。 A代表的Array，叫作阵列，说的是在一块FPGA上，密密麻麻列了大量Gate这样的门电路。 最后一个F，不太容易理解。它其实是说，一块FPGA这样的板子，可以在“现场”多次进行编程。它不像PAL（Programmable Array Logic，可编程阵列逻辑）这样更古老的硬件设备，只能“编程”一次，把预先写好的程序一次性烧录到硬件里面，之后就不能再修改了。 这么看来，其实“FPGA”这样的组合，基本上解决了我们前面说的想要设计硬件的问题。我们可以像软件一样对硬件编程，可以反复烧录，还有海量的门电路，可以组合实现复杂的芯片功能。
不过，相信你和我一样好奇，我们究竟怎么对硬件进行编程呢？我们之前说过，CPU其实就是通过晶体管，来实现各种组合逻辑或者时序逻辑。那么，我们怎么去“编程”连接这些线路呢？
FPGA的解决方案很精巧，我把它总结为这样三个步骤。
第一，用存储换功能实现组合逻辑。在实现CPU的功能的时候，我们需要完成各种各样的电路逻辑。在FPGA里，这些基本的电路逻辑，不是采用布线连接的方式进行的，而是预先根据我们在软件里面设计的逻辑电路，算出对应的真值表，然后直接存到一个叫作LUT（Look-Up Table，查找表）的电路里面。这个LUT呢，其实就是一块存储空间，里面存储了“特定的输入信号下，对应输出0还是1”。
如果还没理解，你可以想一下这个问题。假如现在我们要实现一个函数，这个函数需要返回斐波那契数列的第N项，并且限制这个N不会超过100。该怎么解决这个问题呢？
斐波那契数列的通项公式是 f(N) = f(N-1) + f(N-2) 。所以，我们的第一种办法，自然是写一个程序，从第1项开始算。但其实还有一种办法，就是我们预先用程序算好斐波那契数量前100项，然后把它预先放到一个数组里面。这个数组就像 [1, 1, 2, 3, 5…] 这样。当要计算第N项的时候呢，我们并不是去计算得到结果，而是直接查找这个数组里面的第N项。
这里面的关键就在于，这个查表的办法，不只能够提供斐波那契数列。如果我们要有一个获得N的5次方的函数，一样可以先计算好，放在表里面进行查询。这个“查表”的方法，其实就是FPGA通过LUT来实现各种组合逻辑的办法。
第二，对于需要实现的时序逻辑电路，我们可以在FPGA里面直接放上D触发器，作为寄存器。这个和CPU里的触发器没有什么本质不同。不过，我们会把很多个LUT的电路和寄存器组合在一起，变成一个叫作逻辑簇（Logic Cluster）的东西。在FPGA里，这样组合了多个LUT和寄存器的设备，也被叫做CLB（Configurable Logic Block，可配置逻辑块）。
我们通过配置CLB实现的功能有点儿像我们前面讲过的全加器。它已经在最基础的门电路上做了组合，能够提供更复杂一点的功能。更复杂的芯片功能，我们不用再从门电路搭起，可以通过CLB组合搭建出来。
第三，FPGA是通过可编程逻辑布线，来连接各个不同的CLB，最终实现我们想要实现的芯片功能。这个可编程逻辑布线，你可以把它当成我们的铁路网。整个铁路系统已经铺好了，但是整个铁路网里面，设计了很多个道岔。我们可以通过控制道岔，来确定不同的列车线路。在可编程逻辑布线里面，“编程”在做的，就是拨动像道岔一样的各个电路开关，最终实现不同CLB之间的连接，完成我们想要的芯片功能。
于是，通过LUT和寄存器，我们能够组合出很多CLB，而通过连接不同的CLB，最终有了我们想要的芯片功能。最关键的是，这个组合过程是可以“编程”控制的。而且这个编程出来的软件，还可以后续改写，重新写入到硬件里。让同一个硬件实现不同的芯片功能。从这个角度来说，FPGA也是“软件吞噬世界”的一个很好的例子。
ASIC除了CPU、GPU，以及刚刚的FPGA，我们其实还需要用到很多其他芯片。比如，现在手机里就有专门用在摄像头里的芯片；录音笔里会有专门处理音频的芯片。尽管一个CPU能够处理好手机拍照的功能，也能处理好录音的功能，但是我们直接在手机或者录音笔里塞上一个Intel CPU，显然比较浪费。
于是，我们就考虑为这些有专门用途的场景，单独设计一个芯片。这些专门设计的芯片呢，我们称之为ASIC（Application-Specific Integrated Circuit），也就是专用集成电路。事实上，过去几年，ASIC发展得特别快。因为ASIC是针对专门用途设计的，所以它的电路更精简，单片的制造成本也比CPU更低。而且，因为电路精简，所以通常能耗要比用来做通用计算的CPU更低。而我们上一讲所说的早期的图形加速卡，其实就可以看作是一种ASIC。
因为ASIC的生产制造成本，以及能耗上的优势，过去几年里，有不少公司设计和开发ASIC用来“挖矿”。这个“挖矿”，说的其实就是设计专门的数值计算芯片，用来“挖”比特币、ETH这样的数字货币。</description></item><item><title>32_为什么还有kill不掉的语句？</title><link>https://artisanbox.github.io/1/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/32/</guid><description>在MySQL中有两个kill命令：一个是kill query +线程id，表示终止这个线程中正在执行的语句；一个是kill connection +线程id，这里connection可缺省，表示断开这个线程的连接，当然如果这个线程有语句正在执行，也是要先停止正在执行的语句的。
不知道你在使用MySQL的时候，有没有遇到过这样的现象：使用了kill命令，却没能断开这个连接。再执行show processlist命令，看到这条语句的Command列显示的是Killed。
你一定会奇怪，显示为Killed是什么意思，不是应该直接在show processlist的结果里看不到这个线程了吗？
今天，我们就来讨论一下这个问题。
其实大多数情况下，kill query/connection命令是有效的。比如，执行一个查询的过程中，发现执行时间太久，要放弃继续查询，这时我们就可以用kill query命令，终止这条查询语句。
还有一种情况是，语句处于锁等待的时候，直接使用kill命令也是有效的。我们一起来看下这个例子：
图1 kill query 成功的例子可以看到，session C 执行kill query以后，session B几乎同时就提示了语句被中断。这，就是我们预期的结果。
收到kill以后，线程做什么？但是，这里你要停下来想一下：session B是直接终止掉线程，什么都不管就直接退出吗？显然，这是不行的。
我在第6篇文章中讲过，当对一个表做增删改查操作时，会在表上加MDL读锁。所以，session B虽然处于blocked状态，但还是拿着一个MDL读锁的。如果线程被kill的时候，就直接终止，那之后这个MDL读锁就没机会被释放了。
这样看来，kill并不是马上停止的意思，而是告诉执行线程说，这条语句已经不需要继续执行了，可以开始“执行停止的逻辑了”。
其实，这跟Linux的kill命令类似，kill -N pid并不是让进程直接停止，而是给进程发一个信号，然后进程处理这个信号，进入终止逻辑。只是对于MySQL的kill命令来说，不需要传信号量参数，就只有“停止”这个命令。
实现上，当用户执行kill query thread_id_B时，MySQL里处理kill命令的线程做了两件事：
把session B的运行状态改成THD::KILL_QUERY(将变量killed赋值为THD::KILL_QUERY)；
给session B的执行线程发一个信号。
为什么要发信号呢？
因为像图1的我们例子里面，session B处于锁等待状态，如果只是把session B的线程状态设置THD::KILL_QUERY，线程B并不知道这个状态变化，还是会继续等待。发一个信号的目的，就是让session B退出等待，来处理这个THD::KILL_QUERY状态。
上面的分析中，隐含了这么三层意思：
一个语句执行过程中有多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是THD::KILL_QUERY，才开始进入语句终止逻辑；
如果处于等待状态，必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处；
语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的。
到这里你就知道了，原来不是“说停就停的”。
接下来，我们再看一个kill不掉的例子，也就是我们在前面第29篇文章中提到的 innodb_thread_concurrency 不够用的例子。
首先，执行set global innodb_thread_concurrency=2，将InnoDB的并发线程上限数设置为2；然后，执行下面的序列：
图2 kill query 无效的例子可以看到：</description></item><item><title>32_仓库结构：如何组织文件</title><link>https://artisanbox.github.io/9/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/32/</guid><description>你好，我是LMOS。
你有没有想过，蜜蜂把劳动成果变成蜜糖存放在蜂巢中，人类把劳动成果量化成财富存放在银行，但一个进程的劳动成果放在哪里呢？
看到这里，你可能有疑问，进程有劳动成果吗？当然有，进程加工处理的数据就是进程的劳动成果，可是这个“劳动成果”，如何表示、如何组织，又放在哪里呢？这些问题都会在我们讲解文件系统的过程中一一得到解答。
那今天我们先来搞清楚什么是文件系统，然后解决文件系统如何组织文件，最后对我们文件系统进行设计并抽象成数据结构。好了，下面我们正式开始今天的学习吧。
这节课的配套代码，你可以从这里获取。
什么是文件系统我们经常在计算机上听APE音乐、看4K视频、阅读各种文档、浏览各种精美的网页，这些东西都是一些特定格式的数据，我们习惯把它们叫做文件，这些文件可能储存在HD机械硬盘、SSD固态硬盘、TF卡，甚至远程计算机上。
所以你可以这样理解，文件系统解决的就是如何把许多文件储存在某一种储存设备上，方便进程对各种文件执行打开、关闭、读写、增加和删除等操作。因为这些操作实际上非常复杂，所以操作系统中分出一个子系统专门处理这些问题，这个系统就叫文件系统。
文件系统的核心现在我们还没法直观地感受到，但是它在上层为用户或者进程提供了一个逻辑视图，也就是目录结构。
下图中就是典型的文件系统逻辑视图，从/（根）目录开始，就能找到每个文件、每个目录和每个目录下的所有文件。我们可以看出目录也是文件的一部分，它也扮演了“组织仓库管理员”的角色，可以对文件进行分层分类，以便用户对众多文件进行管理。
虽然这看上去好像有点复杂、是个技术活，但是别怕，毕竟我们不是干这事的第一批人，可以参考别人的设计与实现。好了，废话不多说，难不难，要做了才知道……
文件系统设计既然要实现一个文件系统，还是要好好设计一下，我们首先从三个问题出发对文件系统设计方面的思考。
文件系统为什么可以是一个设备开始，以及它在整个Cosmos内核中的位置格局？ 文件数据的格式以及储存介质的最小单位是什么？ 如何组织越来越多的文件。 搞清楚这三大问题的过程，就是设计文件系统的过程，这里是重点中的重点，你可以停下来好好揣摩，然后再继续往下学习。
文件系统只是一个设备HD机械硬盘、SSD固态硬盘、U盘、各种TF卡等都属于存储设备，这些设备上的文件储存格式都不相同，甚至同一个硬盘上不同的分区的储存格式也不同。这个储存格式就是相应文件系统在储存设备上组织储存文件的方式。
例如我们经常看到的：FAT32、NTFS、Ext4、Btrfs、ZFS、HPFS等，这些都是不同的文件系统建立的文件系统格式。
看到上面储存设备与文件系统多样性的情况之后，不难发现让文件系统成为Cosmos内核中一部分，是个非常愚蠢的想法。那怎么解决这个困难呢，你可以先自己想一想，然后再参考我后面的分析。
针对前面的困难，我们不难提出这样两点设想：第一，文件系统组件是独立的与内核分开的；第二，操作系统需要动态加载和删除不同的文件系统组件，这样就可以适应复杂的情况了。例如，硬盘上不同的分区有不同的文件系统格式，还可以拔插U盘、TF卡等。
你还记得前面Cosmos内核的设备驱动的设计吗？如果文件系统也是Cosmos内核下的一个设备，那就好办多了，因为不同的设备驱动程序可以动态加载，而且可以建立多个文件系统设备，而对各个文件系统设备驱动程序的实现，就是各个文件系统的实现。
刚好前面的驱动模型中（第30节课），定义了文件系统的设备类型。这个架构我给你画一幅图，你看一下就明白了。
这里我不仅给出了文件系统设备的架构，还简单地梳理了内核中其它组件与文件系统的关系。
如图所示，文件系统下面有诸如U盘、硬盘、SSD、CD、TF卡等储存设备。文件系统一定要有储存设备，这个储存设备可以是硬盘，也可以是TF卡，总之能储存数据的设备就行。
为了减小程序的复杂程度，我们使用一块4MB大小的内存空间来模拟储存设备，何况又不是我们第一次建造内存文件系统（ramfs），只是我们做得更小。在文件系统设备驱动程序的入口函数中，分配4MB大小的内存空间。
相信即使如此，也能让我们清楚地看到文件系统的实现。等哪天有时间了，写好了硬盘驱动程序，也可以让文件系统设备驱动程序处理好了数据，然后发送给硬盘设备驱动程序，让其写入到硬盘中去。
这在我们设计的驱动模型中是完全允许的，这就形成了储存系统的“I/O栈”。
文件格式与储存块通常说的文件，都是一堆数据，当我们把这堆数据组织成一个文件，储存在储存介质上时，就有了一个问题：我们按什么格式把这些数据存放在储存介质上。
当然，这个格式是指文件系统存放文件数据的格式。文件数据本身的格式，文件系统不该多管，例如MP3、Word文档的内部格式，各不相同。
关于文件系统存放文件数据的格式，类UNIX系统和Windows系统都采用了相同的方案，那就是逻辑上认为一个文件就是一个可以动态增加、减少的线性字节数组，即文件数据的每个字节都一一对应到这个线性数组中的每个元素。
那么我们也和它们一样，我来给你画个图梳理逻辑关系。
图中的文件数据字节数组，终究是逻辑上的，所以问题又来了，我们如何把这个逻辑上的文件数据字节数组，映射到具体的储存设备上呢？只有解决了这个问题，才能真正储存数据。
现在的机械硬盘、SSD固态硬盘、TF卡，它们都是以储存块为单位储存数据的，一个储存块的大小可以是512、1024、2048、4096字节，访问这些储存设备的最小单位也是一个储存块，不像内存设备可以最少访问一个字节。
文件系统把文件数据定义成一个动态的线性字节数组，可是一开始我们不知道这个数组是多大，需要分配多少个物理储存块，最好是把这个动态的线性字节数组分成一个个数据块。
然而，不同的储存设备的物理储存块的大小不同，有的是512字节，而有的是4096字节，我们为了文件系统能工作在不同的储存设备上，所以我们把这里的数据块定义为文件系统逻辑块，其大小为4096字节，最后把这个逻辑块映射到一个或多个物理储存块。
为了让你更好地理解这个过程，我为你准备了一幅图，如下所示。
从这幅图里，我们可以看到从文件这个抽象概念，它是如何一步步从文件字节数组，整合形成文件数据逻辑块，最后映射到储存介质上的物理储存块。你需要先掌握整个演变过程的逻辑，具体怎么实现我们后面继续讲。
如何组织文件现在PC机上的文件数量都已经上十万的数量级了，网络服务器上更是不止这个数量。
我们不难想到，如果把十万个文件顺序地排列在一起，要找出其中一个文件，那是非常困难的，即使是计算机程序查找起来也是相当慢的，加上硬盘、TF卡之类的储存设备比内存慢得多，因此会变得更慢。
所以，需要一个叫文件目录或者叫文件夹的东西，我们习惯称其为目录。这样我们就可以用不同的目录来归纳不同的文件，例如在MP3目录下存放MP3音乐文件，或者在MP4目录下存放视频文件。同时，目录之下还可以创建目录，这样就建立了非常好的层次关系。
你可能经常在LINUX系统中看到如：“/dev/kvm，/user/bin/gcc”之类的东西，其中dev、user、bin它们就是目录，kvm、gcc它们就是文件，“/”符号就是文件路径分隔符，它们合起来就是文件路径名。
可以看出，整个文件层次结构就像是一棵倒挂的树。前面那幅图已经显示出了这种结构。后面我们的文件系统也会采用目录来组织文件。这里你只要明白，文件数量多了就出现了目录，而目录是用来帮助用户组织或归纳文件的就行了。
文件系统数据结构一路走来，不难发现操作系统内核的任何组件的实现，都需要设计一套相应的数据结构，文件系统也不例外。
根据前面我们对文件系统的设计，我们至少需要表示文件和目录的数据结构，除此之外，还需要表示文件系统本身的一些数据结构，这些数据结构我们称为文件系统元数据。下面我们先从文件系统元数据开始吧！
设计超级块一个文件系统有很多重要的信息，例如文件系统标识、版本、状态，储存介质大小，文件系统逻辑储存块大小，位图所在的储存块，还有根目录等。因为这些信息很重要，没有它们就等于没有文件系统，所以包含这些信息的数据结构，就叫做文件系统的超级块或者文件系统描述块。
下面我们就来设计超级块的数据结构，先在cosmos/include/drvinc/目录下建立一个drvrfs_t.h文件，写下rfssublk_t结构，代码如下所示。
typedef struct s_RFSSUBLK { spinlock_t rsb_lock;//超级块在内存中使用的自旋锁 uint_t rsb_mgic;//文件系统标识 uint_t rsb_vec;//文件系统版本 uint_t rsb_flg;//标志 uint_t rsb_stus;//状态 size_t rsb_sz;//该数据结构本身的大小 size_t rsb_sblksz;//超级块大小 size_t rsb_dblksz;//文件系统逻辑储存块大小，我们这里用的是4KB uint_t rsb_bmpbks;//位图的开始逻辑储存块 uint_t rsb_bmpbknr;//位图占用多少个逻辑储存块 uint_t rsb_fsysallblk;//文件系统有多少个逻辑储存块 rfsdir_t rsb_rootdir;//根目录，后面会看到这个数据结构的 }rfssublk_t; 我们文件系统的超级块，保存在储存设备的第一个4KB大小的逻辑储存块中，但是它本身的大小没有4KB，多余的空间用于以后扩展。rfsdir_t数据结构是一个目录数据结构，你先有个印象，后面我们会有介绍的。</description></item><item><title>32_字符串匹配基础（上）：如何借助哈希算法实现高效字符串匹配？</title><link>https://artisanbox.github.io/2/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/33/</guid><description>从今天开始，我们来学习字符串匹配算法。字符串匹配这样一个功能，我想对于任何一个开发工程师来说，应该都不会陌生。我们用的最多的就是编程语言提供的字符串查找函数，比如Java中的indexOf()，Python中的find()函数等，它们底层就是依赖接下来要讲的字符串匹配算法。
字符串匹配算法很多，我会分四节来讲解。今天我会讲两种比较简单的、好理解的，它们分别是：BF算法和RK算法。下一节，我会讲两种比较难理解、但更加高效的，它们是：BM算法和KMP算法。
这两节讲的都是单模式串匹配的算法，也就是一个串跟一个串进行匹配。第三节、第四节，我会讲两种多模式串匹配算法，也就是在一个串中同时查找多个串，它们分别是Trie树和AC自动机。
今天讲的两个算法中，RK算法是BF算法的改进，它巧妙借助了我们前面讲过的哈希算法，让匹配的效率有了很大的提升。那RK算法是如何借助哈希算法来实现高效字符串匹配的呢？你可以带着这个问题，来学习今天的内容。
BF算法BF算法中的BF是Brute Force的缩写，中文叫作暴力匹配算法，也叫朴素匹配算法。从名字可以看出，这种算法的字符串匹配方式很“暴力”，当然也就会比较简单、好懂，但相应的性能也不高。
在开始讲解这个算法之前，我先定义两个概念，方便我后面讲解。它们分别是主串和模式串。这俩概念很好理解，我举个例子你就懂了。
比方说，我们在字符串A中查找字符串B，那字符串A就是主串，字符串B就是模式串。我们把主串的长度记作n，模式串的长度记作m。因为我们是在主串中查找模式串，所以n&amp;gt;m。
作为最简单、最暴力的字符串匹配算法，BF算法的思想可以用一句话来概括，那就是，我们在主串中，检查起始位置分别是0、1、2....n-m且长度为m的n-m+1个子串，看有没有跟模式串匹配的。我举一个例子给你看看，你应该可以理解得更清楚。
从上面的算法思想和例子，我们可以看出，在极端情况下，比如主串是“aaaaa....aaaaaa”（省略号表示有很多重复的字符a），模式串是“aaaaab”。我们每次都比对m个字符，要比对n-m+1次，所以，这种算法的最坏情况时间复杂度是O(n*m)。
尽管理论上，BF算法的时间复杂度很高，是O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。
第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把m个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。
第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有bug也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。
所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。
RK算法RK算法的全称叫Rabin-Karp算法，是由它的两位发明者Rabin和Karp的名字来命名的。这个算法理解起来也不是很难。我个人觉得，它其实就是刚刚讲的BF算法的升级版。
我在讲BF算法的时候讲过，如果模式串长度为m，主串长度为n，那在主串中，就会有n-m+1个长度为m的子串，我们只需要暴力地对比这n-m+1个子串与模式串，就可以找出主串与模式串匹配的子串。
但是，每次检查主串与子串是否匹配，需要依次比对每个字符，所以BF算法的时间复杂度就比较高，是O(n*m)。我们对朴素的字符串匹配算法稍加改造，引入哈希算法，时间复杂度立刻就会降低。
RK算法的思路是这样的：我们通过哈希算法对主串中的n-m+1个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。
不过，通过哈希算法计算子串的哈希值的时候，我们需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。有没有方法可以提高哈希算法计算子串哈希值的效率呢？
这就需要哈希算法设计的非常有技巧了。我们假设要匹配的字符串的字符集中只包含K个字符，我们可以用一个K进制数来表示一个子串，这个K进制数转化成十进制数，作为子串的哈希值。表述起来有点抽象，我举了一个例子，看完你应该就能懂了。
比如要处理的字符串只包含a～z这26个小写字母，那我们就用二十六进制来表示一个字符串。我们把a～z这26个字符映射到0～25这26个数字，a就表示0，b就表示1，以此类推，z表示25。
在十进制的表示法中，一个数字的值是通过下面的方式计算出来的。对应到二十六进制，一个包含a到z这26个字符的字符串，计算哈希的时候，我们只需要把进位从10改成26就可以。
这个哈希算法你应该看懂了吧？现在，为了方便解释，在下面的讲解中，我假设字符串中只包含a～z这26个小写字符，我们用二十六进制来表示一个字符串，对应的哈希值就是二十六进制数转化成十进制的结果。
这种哈希算法有一个特点，在主串中，相邻两个子串的哈希值的计算公式有一定关系。我这有个例子，你先找一下规律，再来看我后面的讲解。
从这里例子中，我们很容易就能得出这样的规律：相邻两个子串s[i-1]和s[i]（i表示子串在主串中的起始位置，子串的长度都为m），对应的哈希值计算公式有交集，也就是说，我们可以使用s[i-1]的哈希值很快的计算出s[i]的哈希值。如果用公式表示的话，就是下面这个样子：
不过，这里有一个小细节需要注意，那就是26^(m-1)这部分的计算，我们可以通过查表的方法来提高效率。我们事先计算好26^0、26^1、26^2……26^(m-1)，并且存储在一个长度为m的数组中，公式中的“次方”就对应数组的下标。当我们需要计算26的x次方的时候，就可以从数组的下标为x的位置取值，直接使用，省去了计算的时间。
我们开头的时候提过，RK算法的效率要比BF算法高，现在，我们就来分析一下，RK算法的时间复杂度到底是多少呢？
整个RK算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是O(n)。
模式串哈希值与每个子串哈希值之间的比较的时间复杂度是O(1)，总共需要比较n-m+1个子串的哈希值，所以，这部分的时间复杂度也是O(n)。所以，RK算法整体的时间复杂度就是O(n)。
这里还有一个问题就是，模式串很长，相应的主串中的子串也会很长，通过上面的哈希算法计算得到的哈希值就可能很大，如果超过了计算机中整型数据可以表示的范围，那该如何解决呢？
刚刚我们设计的哈希算法是没有散列冲突的，也就是说，一个字符串与一个二十六进制数一一对应，不同的字符串的哈希值肯定不一样。因为我们是基于进制来表示一个字符串的，你可以类比成十进制、十六进制来思考一下。实际上，我们为了能将哈希值落在整型数据范围内，可以牺牲一下，允许哈希冲突。这个时候哈希算法该如何设计呢？
哈希算法的设计方法有很多，我举一个例子说明一下。假设字符串中只包含a～z这26个英文字母，那我们每个字母对应一个数字，比如a对应1，b对应2，以此类推，z对应26。我们可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值。这种哈希算法产生的哈希值的数据范围就相对要小很多了。
不过，你也应该发现，这种哈希算法的哈希冲突概率也是挺高的。当然，我只是举了一个最简单的设计方法，还有很多更加优化的方法，比如将每一个字母从小到大对应一个素数，而不是1，2，3……这样的自然数，这样冲突的概率就会降低一些。
那现在新的问题来了。之前我们只需要比较一下模式串和子串的哈希值，如果两个值相等，那这个子串就一定可以匹配模式串。但是，当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身并不匹配。
实际上，解决方法很简单。当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。当然，如果子串的哈希值与模式串的哈希值不相等，那对应的子串和模式串肯定也是不匹配的，就不需要比对子串和模式串本身了。
所以，哈希算法的冲突概率要相对控制得低一些，如果存在大量冲突，就会导致RK算法的时间复杂度退化，效率下降。极端情况下，如果存在大量的冲突，每次都要再对比子串和模式串本身，那时间复杂度就会退化成O(n*m)。但也不要太悲观，一般情况下，冲突不会很多，RK算法的效率还是比BF算法高的。
解答开篇&amp;amp;内容小结今天我们讲了两种字符串匹配算法，BF算法和RK算法。
BF算法是最简单、粗暴的字符串匹配算法，它的实现思路是，拿模式串与主串中是所有子串匹配，看是否有能匹配的子串。所以，时间复杂度也比较高，是O(n*m)，n、m表示主串和模式串的长度。不过，在实际的软件开发中，因为这种算法实现简单，对于处理小规模的字符串匹配很好用。
RK算法是借助哈希算法对BF算法进行改造，即对每个子串分别求哈希值，然后拿子串的哈希值与模式串的哈希值比较，减少了比较的时间。所以，理想情况下，RK算法的时间复杂度是O(n)，跟BF算法相比，效率提高了很多。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为O(n*m)。
课后思考我们今天讲的都是一维字符串的匹配方法，实际上，这两种算法都可以类比到二维空间。假设有下面这样一个二维字符串矩阵（图中的主串），借助今天讲的处理思路，如何在其中查找另一个二维字符串矩阵（图中的模式串）呢？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>32_字节码生成：为什么Spring技术很强大？</title><link>https://artisanbox.github.io/6/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/32/</guid><description>Java程序员几乎都了解Spring。它的IoC（依赖反转）和AOP（面向切面编程）功能非常强大、易用。而它背后的字节码生成技术（在运行时，根据需要修改和生成Java字节码的技术）就是一项重要的支撑技术。
Java字节码能够在JVM（Java虚拟机）上解释执行，或即时编译执行。其实，除了Java，JVM上的Groovy、Kotlin、Closure、Scala等很多语言，也都需要生成字节码。另外，playscript也可以生成字节码，从而在JVM上高效地运行！
而且，字节码生成技术很有用。你可以用它将高级语言编译成字节码，还可以向原来的代码中注入新代码，来实现对性能的监测等功能。
目前，我就有一个实际项目的需求。我们的一个产品，需要一个规则引擎，解析自定义的DSL，进行规则的计算。这个规则引擎处理的数据量比较大，所以它的性能越高越好。因此，如果把DSL编译成字节码就最理想了。
既然字节码生成技术有很强的实用价值，那么本节课，我就带你掌握它。
我会先带你了解Java的虚拟机和字节码的指令，然后借助ASM这个工具，生成字节码，最后，再实现从AST编译成字节码。通过这样一个过程，你会加深对Java虚拟机的了解，掌握字节码生成技术，从而更加了解Spring的运行机制，甚至有能力编写这样的工具！
Java虚拟机和字节码字节码是一种二进制格式的中间代码，它不是物理机器的目标代码，而是运行在Java虚拟机上，可以被解释执行和即时编译执行。
在讲后端技术时，我强调的都是，如何生成直接在计算机上运行的二进制代码，这比较符合C、C++、Go等静态编译型语言。但如果想要解释执行，除了直接解释执行AST以外，我没有讲其他解释执行技术。
而目前更常见的解释执行的语言，是采用虚拟机，其中最典型的就是JVM，它能够解释执行Java字节码。
而虚拟机的设计又有两种技术：一是基于栈的虚拟机；二是基于寄存器的虚拟机。
标准的JVM是基于栈的虚拟机（后面简称“栈机”）。
每一个线程都有一个JVM栈，每次调用一个方法都会生成一个栈桢，来支持这个方法的运行。栈桢里面又包含了本地变量数组（包括方法的参数和本地变量）、操作数栈和这个方法所用到的常数。这种栈桢的设计跟之前我们学过C语言的栈桢的结构，其实有很大的相似性，你可以通过21讲回顾一下。
栈机是基于操作数栈做计算的。以“2+3”的计算为例，只要把它转化成逆波兰表达式，“2 3 +”，然后按照顺序执行就可以了。也就是：先把2入栈，再把3入栈，再执行加法指令，这时，要从栈里弹出2个操作数做加法计算，再把结果压入栈。
你可以看出，栈机的加法指令，是不需要带操作数的，就是简单的“iadd”就行，这跟你之前学过的IR都不一样。为什么呢？因为操作数都在栈里，加法操作需要2个操作数，从栈里弹出2个元素就行了。
也就是说，指令的操作数是由栈确定的，我们不需要为每个操作数显式地指定存储位置，所以指令可以比较短，这是栈机的一个优点。
接下来，我们聊聊字节码的特点。
字节码是什么样子的呢？我编写了一个简单的类MyClass.java，其中的foo()方法实现了一个简单的加法计算，你可以看看它对应的字节码是怎样的：
public class MyClass { public int foo(int a){ return a + 3; } } 在命令行终端敲入下面两行命令，生成文本格式的字节码文件：
javac MyClass.java javap -v MyClass &amp;gt; MyClass.bc 打开MyClass.bc文件，你会看到下面的内容片段：
public int foo(int); Code: 0: iload_1 //把下标为1的本地变量入栈 1: iconst_3 //把常数3入栈 2: iadd //执行加法操作 3: ireturn //返回 其中，foo()方法一共有四条指令，前三条指令是计算一个加法表达式a+3。这完全是按照逆波兰表达式的顺序来执行的：先把一个本地变量入栈，再把常数3入栈，再执行加法运算。
如果你细心的话，应该会发现：把参数a入栈的第一条指令，用的下标是1，而不是0。这是因为，每个方法的第一个参数（下标为0）是当前对象实例的引用（this）。
我提供了字节码中，一些常用的指令，增加你对字节码特点的直观认识，完整的指令集可以参见JVM的规格书：
其中，每个指令都是8位的，占一个字节，而且iload_0、iconst_0这种指令，甚至把操作数（变量的下标、常数的值）压缩进了操作码里，可以看出，字节码的设计很注重节省空间。
根据这些指令所对应的操作码的数值，MyClass.bc文件中，你所看到的那四行代码，变成二进制格式，就是下面的样子：
你可以用“hexdump MyClass.class”显示字节码文件的内容，从中可以发现这个片段（就是橙色框里的内容）：
现在，你已经初步了解了基于栈的虚拟机，与此对应的是基于寄存器的虚拟机。这类虚拟机的运行机制跟机器码的运行机制是差不多的，它的指令要显式地指出操作数的位置（寄存器或内存地址）。它的优势是：可以更充分地利用寄存器来保存中间值，从而可以进行更多的优化。
例如，当存在公共子表达式时，这个表达式的计算结果可以保存在某个寄存器中，另一个用到该公共子表达式的指令，就可以直接访问这个寄存器，不用再计算了。在栈机里是做不到这样的优化的，所以基于寄存器的虚拟机，性能可以更高。而它的典型代表，是Google公司为Android开发的Dalvik虚拟机和Lua语言的虚拟机。
这里你需要注意，栈机并不是不用寄存器，实际上，操作数栈是可以基于寄存器实现的，寄存器放不下的再溢出到内存里。只不过栈机的每条指令，只能操作栈顶部的几个操作数，所以也就没有办法访问其它寄存器，实现更多的优化。
现在，你应该对虚拟机以及字节码有了一定的了解了。那么，如何借助工具生成字节码呢？你可能会问了：为什么不纯手工生成字节码呢？当然可以，只不过借助工具会更快一些。</description></item><item><title>32_运行时（二）：垃圾收集与语言的特性有关吗？</title><link>https://artisanbox.github.io/7/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/32/</guid><description>你好，我是宫文学。今天，我们继续一起学习垃圾收集的实现机制以及与编译器的关系。
对于一门语言来说，垃圾收集机制能够自动管理从堆中申请的内存，从而大大降低程序员的负担。在这门课的第二大模块“真实编译器解析篇”中，我们学习Java、Python、Go、Julia和JavaScript这几门语言，都有垃圾收集机制。那在今天这一讲，我们就来学习一下，这些语言的垃圾收集机制到底有什么不同，跟语言特性的设计又是什么关系，以及编译器又是如何配合垃圾收集机制的。
这样如果我们以后要设计一门语言的话，也能清楚如何选择合适的垃圾收集机制，以及如何让编译器来配合选定的垃圾收集机制。
在讨论不同语言的垃圾收集机制之前，我们还是需要先了解一下，通常我们都会用到哪些垃圾收集算法，以及它们都有什么特点。这样，我们才能深入探讨应该在什么时候采用什么算法。如果你对各种垃圾收集算法已经很熟悉了，也可以从这一讲的“Python与引用计数算法”开始学习；如果你还想理解垃圾收集算法的更多细节，也可以去看看我的第一季课程《编译原理之美》的第33讲的内容。
垃圾收集算法概述垃圾收集主要有标记-清除（Mark and Sweep）、标记-整理（Mark and Compact）、停止-拷贝（Stop and Copy）、引用计数、分代收集、增量收集和并发收集等不同的算法，在这里我简要地和你介绍一下。
首先，我们先学习一下什么是内存垃圾。内存垃圾，其实就是一些保存在堆里的、已经无法从程序里访问的对象。
我们看一个具体的例子。
在堆中申请一块内存时（比如Java中的对象实例），我们会用一个变量指向这块内存。但是，如果给变量赋予一个新的地址，或者当栈桢弹出时，该栈桢的变量全部失效，这时，变量所指向的内存就没用了（如图中的灰色块）。
图1：A是内存垃圾另外，如果A对象有一个成员变量指向C对象，那么A不可达，C也会不可达，也就失效了。但D对象除了被A引用，还被B引用，仍然是可达的。
图2：A和C是内存垃圾那么，所有不可达的内存就是垃圾。所以，垃圾收集的重点就是找到并清除这些垃圾。接下来，我们就看看不同的算法是怎么完成这个任务的。
标记-清除标记-清除算法，是从GC根节点出发，顺着对象的引用关系，依次标记可达的对象。这里说的GC根节点，包括全局变量、常量、栈里的本地变量、寄存器里的本地变量等。从它们出发，就可以找到所有有用的对象。那么剩下的对象，就是内存垃圾，可以清除掉。
标记-整理采用标记-清除算法，运行时间长了以后，会形成内存碎片。这样在申请内存的时候，可能会失败。
图3：内存碎片导致内存申请失败为了避免内存碎片，你可以采用变化后的算法，也就是标记-整理算法：在做完标记以后，做一下内存的整理，让存活的对象都移动到一边，消除掉内存碎片。
图4：内存整理以后，可以更有效地利用内存停止-拷贝停止和拷贝算法把内存分为新旧空间两部分。你需要保持一个堆指针，指向自由空间开始的位置。申请内存时，把堆指针往右移动就行了。
图5：在旧空间中申请内存当旧空间内存不够了以后，就会触发垃圾收集。在收集时，会把可达的对象拷贝到新空间，然后把新旧空间互换。
图6：新旧空间互换停止-拷贝算法，在分配内存的时候，不需要去查找一块合适的空闲内存；在垃圾收集完毕以后，也不需要做内存整理，因此速度是最快的。但它也有缺点，就是总有一半内存是闲置的。
引用计数引用计数方法，是在对象里保存该对象被引用的数量。一旦这个引用数为零，那么就可以作为垃圾被收集走。
有时候，我们会把引用计数叫做自动引用计数（ARC），并把它作为跟垃圾收集（GC）相对立的一个概念。所以，如果你读到相关的文章，它把ARC和GC做了对比，也不要吃惊。
引用计数实现起来简单，并且可以边运行边做垃圾收集，不需要为了垃圾收集而专门停下程序。可是，它也有缺陷，就是不能处理循环引用（Reference Cycles）的情况。在下图中，四个对象循环引用，但没有GC根指向它们。它们已经是垃圾，但计数却都为1。
图7：循环引用另外，由于在程序引用一个对象的前后，都要修改引用计数，并且还有多线程竞争的可能性，所以引用计数法的性能开销比较大。
分代收集在程序中，新创建的对象往往会很快死去，比如，你在一个方法中，使用临时变量指向一些新创建的对象，这些对象大多数在退出方法时，就没用了。这些数据叫做新生代。而如果一个对象被扫描多次，发现它还没有成为垃圾，那就会标记它为比较老的时代。这些对象可能Java里的静态数据成员，或者调用栈里比较靠近根部的方法所引用的，不会很快成为垃圾。
对于新生代对象，可以更频繁地回收。而对于老一代的对象，则回收频率可以低一些。并且，对于不同世代的对象，还可以用不同的回收方法。比如，新生代比较适合复制式收集算法，因为大部分对象会被收集掉，剩下来的不多；而老一代的对象生存周期比较长，拷贝的话代价太大，比较适合标记-清除算法，或者标记-整理算法。
增量收集和并发收集垃圾收集算法在运行时，通常会把程序停下。因为在垃圾收集的过程中，如果程序继续运行，可能会出错。这种停下整个程序的现象，被形象地称作“停下整个世界（STW）”。
可是让程序停下来，会导致系统卡顿，用户的体验感会很不好。一些对实时性要求比较高的系统，根本不可能忍受这种停顿。
所以，在自动内存管理领域的一个研究的重点，就是如何缩短这种停顿时间。增量收集和并发收集算法，就是在这方面的有益探索：
增量收集可以每次只完成部分收集工作，没必要一次把活干完，从而减少停顿。 并发收集就是在不影响程序执行的情况下，并发地执行垃圾收集工作。 好了，理解了垃圾收集算法的核心原理以后，我们就可以继续去探索各门语言是怎么运用这些算法的了。
首先，我们从Python的垃圾收集算法学起。
Python与引用计数算法Python语言选择的是引用计数的算法。除此之外，Swift语言和方舟编译器，采用的也是引用计数，所以值得我们重视。
Python的内存管理和垃圾收集机制首先我们来复习一下Python内存管理的特征。在Python里，每个数据都是对象，而这些对象又都是在堆上申请的。对比一下，在C和Java这样的语言里，很多计算可以用本地变量实现，而本地变量是在栈上申请的。这样，你用到一个整数的时候，只占用4个字节，而不像Python那样有一个对象头的固定开销。栈的优势还包括：不会产生内存碎片，数据的局部性又好，申请和释放的速度又超快。而在堆里申请太多的小对象，则会走向完全的反面：太多次系统调用，性能开销大；产生内存碎片；数据的局部性也比较差。
所以说，Python的内存管理方案，就决定了它的内存占用大、性能低。这是Python内存管理的短板。而为了稍微改善一下这个短板，Python采用了一套基于区域（Region-based）的内存管理方法，能够让小块的内存管理更高效。简单地说，就是Python每次都申请一大块内存，这一大块内存叫做Arena。当需要较小的内存的时候，直接从Arena里划拨就好了，不用一次次地去操作系统申请。当用垃圾回收算法回收内存时，也不一定马上归还给操作系统，而是归还到Arena里，然后被循环使用。这个策略能在一定程度上提高内存申请的效率，并且减少内存碎片化。
接下来，我们就看看Python是如何做垃圾回收的。回忆一下，在第19讲分析Python的运行时机制时，其中提到了一些垃圾回收的线索。Python里每个对象都是一个PyObject，每个PyObject都有一个ob_refcnt字段用于记录被引用的数量。
在解释器执行字节码的时候，会根据不同的指令自动增加或者减少ob_refcnt的值。当一个PyObject对象的ob_refcnt的值为0的时候，意味着没有任何一个变量引用它，可以立即释放掉，回收对象所占用的内存。
现在你已经知道，采用引用计数方法，需要解决循环引用的问题。那Python是如何实现的呢？
Python在gc模块里提供了一个循环检测算法。接下来我们通过一个示例，来看看这个算法的原理。在这个例子中，有一个变量指向对象A。你能用肉眼看出，对象A、B、C不是垃圾，而D和E是垃圾。
图8：把容器对象加入待扫描列表在循环检测算法里，gc使用了两个列表。一个列表保存所有待扫描的对象，另一个列表保存可能的垃圾对象。注意，这个算法只检测容器对象，比如列表、用户自定义的类的实例等。而像整数对象这样的，就不用检测了，因为它们不可能持有对其他对象的引用，也就不会造成循环引用。
在这个算法里，我们首先让一个gc_ref变量等于对象的引用数。接着，算法假装去掉对象之间的引用。比如，去掉从A到B的引用，这使得B对象的gc_ref值变为了0。在遍历完整个列表以后，除了A对象以外，其他对象的gc_ref都变成了0。
图9：扫描列表，修改gc_ref的值gc_ref等于零的对象，有的可能是垃圾对象，比如D和E；但也有些可能不是，比如B和C。那要怎么区分呢？我们先把这些对象都挪到另一个列表中，怀疑它们可能是垃圾。
图10：认为gc_ref为0的对象可能是垃圾这个时候，待扫描对象区只剩下了对象A。它的gc_ref是大于零的，也就是从gc根是可到达的，因此肯定不是垃圾对象。那么顺着这个对象所直接引用和间接引用到的对象，也都不是垃圾。而剩下的对象，都是从gc根不可到达的，也就是真正的内存垃圾。
图11：去除其中可达的对象，剩下的是真正的垃圾另外，基于循环检测的垃圾回收算法是定期执行的，这会跟Java等语言的垃圾收集器一样，导致系统的停顿。所以，它也会像Java等语言的垃圾收集器一样，采用分代收集的策略来减少垃圾收集的工作量，以及由于垃圾收集导致的停顿。
好了，以上就是Python的垃圾收集算法。我们前面提过，除了Python以外，Swift和方舟编译器也使用了引用计数算法。另外，还有些分代的垃圾收集器，在处理老一代的对象时，也会采用引用计数的方法，这样就可以在引用计数为零的时候收回内存，而不需要一遍遍地扫描。
编译器如何配合引用计数算法？对于Python来说，引用计数的增加和减少，是由运行时来负责的，编译器并不需要做额外的工作。它只需要生成字节码就行了。而对于Python的解释器来说，在把一个对象赋值给一个变量的时候，要把对象的引用数加1；而当该变量超出作用域的时候，要把对象的引用数减1。
不过，对于编译成机器码的语言来说，就要由编译器介入了。它要负责生成相应的指令，来做引用数的增减。
不过，这只是高度简化的描述。实际实现时，还要解决很多细致的问题。比如，在多线程的环境下，对引用数的改变，必须要用到锁，防止超过一个线程同时修改引用数。这种频繁地对锁的使用，会导致性能的降低。这时候，我们之前学过的一些优化算法就可以派上用场了。比如，编译器可以做一下逃逸分析，对于没有逃逸或者只是参数逃逸的对象，就可以不使用锁，因为这些对象不可能被多个线程访问。这样就可以提高程序的性能。
除了通过逃逸分析优化对锁的使用，编译器还可以进一步优化。比如，在一段程序中，一个对象指针被调用者通过参数传递给某个函数使用。在函数调用期间，由于调用者是引用着这个对象的，所以这个对象不会成为垃圾。而这个时候，就可以省略掉进入和退出函数时给对象引用数做增减的代码。
还有不少类似上面的情况，需要编译器配合垃圾收集机制，生成高效的、正确的代码。你在研究Swift和方舟编译器时，可以多关注一下它们对引用计数做了哪些优化。
接下来，我们再看看其他语言是怎么做垃圾收集的。
其他语言是怎么做垃圾收集的？除了Python以外，我们在第二个模块研究的其他几门语言，包括Java、JavaScript（V8）和Julia，都没有采用引用计数算法（除了在分代算法中针对老一代的对象），它们基本都采用了分代收集的策略。针对新生代，通常是采用标记-清除或者停止拷贝算法。
它们不采用引用计数的原因，其实我们可以先猜测一下，那就是因为引用计数的缺点。比如增减引用计数所导致的计算量比较多，在多线程的情况下要用到锁，就更是如此；再比如会导致内存碎片化、局部性差等。
而采用像停止-拷贝这样的算法，在总的计算开销上会比引用计数的方法低。Java和Go语言主要是用于服务端程序开发的。尽量减少内存收集带来的性能损耗，当然是语言的设计者重点考虑的问题。
再进一步看，采用像停止-拷贝这样的算法，其实是用空间换时间，以更大的内存消耗换来性能的提升。如果你的程序需要100M内存，那么虚拟机需要给它准备200M内存，因为有一半空间是空着的。这其实也是为什么Android手机比iPhone更加消耗内存的原因之一。
在为iPhone开发程序的时候，无论是采用Objective C还是Swift，都是采用引用计数的技术。并且，程序员还负责采用弱引用等技术，来避免循环引用，从而进一步消除了在运行时进行循环引用检测的开销。
通过上面的分析，我们能发现移动端应用和服务端应用有不同的特点，因此也会导致采用不同的垃圾收集算法。那么方舟编译器采用引用计数的方法，来编译原来的Android应用，是否也是借鉴了iPhone的经验呢？我没有去求证过，所以不得而知。但我们可以根据自己的知识去做一些合理的猜测。
好，回过头来，我们继续分析一下用Java和Go语言来写服务端程序对垃圾收集的需求。对于服务器端程序来说，垃圾收集导致的停顿，是一个令程序员们头痛的问题。有时候，一次垃圾收集会让整个程序停顿一段非常可观的时间（比如上百毫秒，甚至达到秒级），这对于实时性要求较高或并发量较大的系统来说，就会引起很大的问题。也因此，一些很关键的系统很长时间内无法采用Java和Go语言编写。
所以，Java和Go语言一直在致力于减少由于垃圾收集而产生的停顿。最新的垃圾收集器，已经使得垃圾收集导致的停顿降低到了几毫秒内。
在这里，你需要理解的要点，是为什么在垃圾收集时，要停下整个程序？又有什么办法可以减少停顿的时间？
为什么在垃圾收集时，要停下整个程序？其实，对于引用计数算法来说，是不需要停下整个程序的，每个对象的内存在计数为零的时候就可以收回。
而采用标记-清除算法时，你就必须要停下程序：首先做标记，然后做清除。在做标记的时候，你必须从所有的GC根出发，去找到所有存活的对象，剩下的才是垃圾。所以，看上去，这是一项完整的工作，程序要一直停顿到这项完整的工作做完。
让事情更棘手的是，你不仅要停下当前的线程，扫描栈里的所有GC根，你还要停下其他的线程，因为其他线程栈里的对象，也可能引用了相同的对象。最后的结果，就是你停下了整个世界。</description></item><item><title>32｜函数式编程第1关：实现高阶函数</title><link>https://artisanbox.github.io/3/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/34/</guid><description>你好，我是宫文学。
前面三节课，我们探讨了怎么在现代语言中实现面向对象编程的特性。面向对象是一种重要的编程范式。还有另一种编程范式，也同样重要，并且近年来使用得很多，这就是函数式编程。从今天这节课开始，我们就来实现一下函数式编程。
函数式编程思想其实比面向对象编程思想的历史更长，早期的Lisp等语言都是函数式编程语言。像JavaScript等后来的语言，也继承了Lisp语言在函数式编程方面的思想，对函数式编程也有不错的支持。
近年，函数式编程思想得到了一定程度的复兴，部分原因是由于函数式编程能够更好地应对大规模的并发处理。我自己最近参与的项目，也在全面使用一门函数式编程语言，这也是对函数式编程的优势的认可。此外，像Erlang这种能够开发高可靠性系统的函数式编程语言，也一直是我感兴趣的研究对象。
对于函数式编程这个话题，很多书和文章都对它有过讲解。我在《编译原理实战课》的第39节，也对函数式编程特性的一些技术点做了分析。在我们的这门课里，因为要动手实现出来，所以目标不能太大，我们就挑几个最核心的技术点来实现一下，让你对函数式编程的底层机制有一次穿透性的了解。
今天这节课，我们主要来实现高阶函数的特性。对于函数式编程来说，高阶函数是实现其他功能的基础，属于最核心的技术点。那么，我们就先分析一下什么是高阶函数。
高阶函数的例子高阶函数的核心思想，是函数本身可以当做数据来使用，就像number数据和string数据那样。那既然可以当做数据使用，那自然可以用它来声明变量、作为参数传递给另一个函数，以及作为返回值从另一个函数中返回。如果一门计算机语言把函数和数据同等对待，这时候我们说函数是一等公民（First-class Citizen）。
我用TypeScript写了一个reduce函数的例子，带你来感受一下高阶函数的特性。这个函数能够遍历一个number数组，并且返回一个number值。
//reduce函数：遍历数组中的每个元素，最后返回一个值 function reduce(numbers:number[], fun:(prev:number,cur:number)=&amp;gt;number):number{ let prev:number = 0; for (let i = 0; i &amp;lt; numbers.length; i++){ prev = fun(prev, numbers[i]); } return prev; } //累计汇总值 function sum(prev:number, cur:number):number{ return prev + cur; }
//累计最大值 function max(prev:number, cur:number):number{ if (prev &amp;gt;= cur) return prev; else return cur; }
let numbers = [2,3,4,5,7,4,5,2];
println(reduce(numbers, sum)); println(reduce(numbers, max)); 这个reduce函数很有意思的一点，是它能接受一个函数作为参数。在每遍历一个数组元素的时候，都会调用这个传进来的函数。根据传入的函数不同，reduce函数能完成不同的功能。当传入max函数的时候，reduce函数能返回数组元素的最大值；而当传入sum函数的时候，则能返回数组元素的汇总值。
这个例子能够部分体现函数式编程的优势：把系统的功能拆解成函数，再灵活组合。
那这些高阶函数的特性具体怎么实现呢？按照惯例，我们还是先看看在编译器前端方面，我们要做什么工作。
修改编译器前端要实现上面的功能，编译器前端需要增加新的语法规则，并做一些与函数类型有关的语义处理工作。</description></item><item><title>33_仓库划分：文件系统的格式化操作</title><link>https://artisanbox.github.io/9/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/33/</guid><description>你好，我是LMOS。
上一节课中，我们已经设计好了文件系统数据结构，相当于建好了仓库的基本结构。
今天，我将和你一起探索仓库的划分，即什么地方存放仓库的管理信息，什么地方存放进程的“劳动成果”（也就是文件），对应于文件系统就是文件系统的格式化操作。
具体我是这样安排的，我们先来实现文件系统设备驱动，接着建立文件系统超级块，然后建立根目录，最后建立文件系统的位图。下面，我们先从建立文件系统设备开始。
这节课的配套代码，你可以从这里获取。
文件系统设备根据我们前面的设计，文件系统并不是Cosmos的一部分，它只是Cosmos下的一个设备。
既然是设备，那就要编写相应的设备驱动程序。我们首先得编写文件系统设备的驱动程序。由于前面已经写过驱动程序了，你应该对驱动程序框架已经很熟悉了。
我们先在cosmos/drivers/目录下建立一个drvrfs.c文件，在里面写下文件系统驱动程序框架代码，如下所示。
drvstus_t rfs_entry(driver_t* drvp,uint_t val,void* p){……} drvstus_t rfs_exit(driver_t* drvp,uint_t val,void* p){……} drvstus_t rfs_open(device_t* devp,void* iopack){……} drvstus_t rfs_close(device_t* devp,void* iopack){……} drvstus_t rfs_read(device_t* devp,void* iopack){……} drvstus_t rfs_write(device_t* devp,void* iopack){……} drvstus_t rfs_lseek(device_t* devp,void* iopack){……} drvstus_t rfs_ioctrl(device_t* devp,void* iopack){……} drvstus_t rfs_dev_start(device_t* devp,void* iopack){……} drvstus_t rfs_dev_stop(device_t* devp,void* iopack){……} drvstus_t rfs_set_powerstus(device_t* devp,void* iopack){……} drvstus_t rfs_enum_dev(device_t* devp,void* iopack){……} drvstus_t rfs_flush(device_t* devp,void* iopack){……} drvstus_t rfs_shutdown(device_t* devp,void* iopack){……} 这个框架代码我们已经写好了，是不是感觉特别熟悉？这就是我们开发驱动程序的规范操作。下面，我们来建立文件系统设备。
按照之前的设计（如果不熟悉可以回顾第32课），我们将使用4MB内存空间来模拟真实的储存设备，在建立文件系统设备的时候分配一块4MB大小的内存空间，这个内存空间我们用一个数据结构来描述，这个数据结构的分配内存空间的代码如下所示。
typedef struct s_RFSDEVEXT { spinlock_t rde_lock;//自旋锁 list_h_t rde_list;//链表 uint_t rde_flg;//标志 uint_t rde_stus;//状态 void* rde_mstart;//用于模拟储存介质的内存块的开始地址 size_t rde_msize;//内存块的大小 void* rde_ext;//扩展所用 }rfsdevext_t; drvstus_t new_rfsdevext_mmblk(device_t* devp,size_t blksz) { //分配模拟储存介质的内存空间，大小为4MB adr_t blkp= krlnew(blksz); //分配rfsdevext_t结构实例的内存空间 rfsdevext_t* rfsexp=(rfsdevext_t*)krlnew(sizeof(rfsdevext_t)); //初始化rfsdevext_t结构 rfsdevext_t_init(rfsexp); rfsexp-&amp;gt;rde_mstart=(void*)blkp; rfsexp-&amp;gt;rde_msize=blksz; //把rfsdevext_t结构的地址放入device_t 结构的dev_extdata字段中，这里dev_extdata字段就起作用了 devp-&amp;gt;dev_extdata=(void*)rfsexp;.</description></item><item><title>33_垃圾收集：能否不停下整个世界？</title><link>https://artisanbox.github.io/6/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/33/</guid><description>对于内存的管理，我们已经了解了栈和栈桢，在编译器和操作系统的配合下，栈里的内存可以实现自动管理。
不过，如果你熟悉C和C++，那么肯定熟悉在堆中申请内存，也知道要小心维护所申请的内存，否则很容易引起内存泄漏或奇怪的Bug。
其实，现代计算机语言大多数都带有自动内存管理功能，也就是垃圾收集（GC）。程序可以使用堆中的内存，但我们没必要手工去释放。垃圾收集器可以知道哪些内存是垃圾，然后归还给操作系统。
那么这里会有几个问题，也是本节课关注的重点：
自动内存管理有哪些不同的策略？这些策略各自有什么优缺点？ 为什么垃圾收集会造成系统停顿？工程师们又为什么特别在意这一点？ 相信学完这节课之后，你对垃圾收集的机制理解得会更加深刻，从而在使用Java、Go等带有垃圾收集功能的语言时，可以更好地提升回收效率，减少停顿，提高程序的运行效率。
当然，想要达到这个目的，你首先需要了解什么是内存垃圾，如何发现哪些内存是没用的？
什么是内存垃圾内存垃圾是一些保存在堆里的对象，但从程序里已经无法访问。
在堆中申请一块内存时（比如Java中的对象实例），我们会用一个变量指向这块内存。这个变量可能是：全局变量、常量、栈里的变量、寄存器里的变量。我们把这些变量叫做GC根节点。它指向的对象中，可能还包含指向其他对象的指针。
但是，如果给变量赋予一个新的地址，或者当栈桢弹出，该栈桢的变量全部失效，这时，变量所指向的内存就无用了（如图中的灰色块）。
另外，如果A对象有一个成员变量指向C对象，那么如果A不可达，C也会不可达，也就失效了。但D对象除了被A引用，还被B引用，仍然是可达的。
所以，所有可达的内存就不是垃圾，而计算可达性，重点在于知道哪些是根节点。在一个活动记录（栈桢）里，有些位置放的是指向堆中内存的指针，有的位置不是，比如，可能存放的是返回地址，或者是一个整数值。如果我们能够知道活动记录的布局，就可以找出所有的指针，然后就能计算寻找垃圾内存。
现在，你应该知道了内存垃圾的特点了，接下来，只要用算法找出哪些内存是不可达的，就能进行垃圾收集了。
标记和清除（Mark and Sweep）标记和清除算法是最为经典的垃圾收集算法，它分为标记阶段和清除阶段。
在标记阶段中，GC跟踪所有可达的对象并做标记。每个对象上有一个标记位，一开始置为0，如果发现这个对象是可达的，就置为1。这个过程其实就是图的遍历算法，我们把这个算法细化一下，写成伪代码如下：
把所有的根节点加入todo列表 只要todo列表不为空，就循环处理： 从todo列表里移走一个变量v 如果v的标记为0，那么 把v的标记置为1 假设v1...vn是v中包含的指针 那么把v1...vn加入todo列表(去除重复成员) 下面的示例图中，x和y是GC根节点，标记完毕以后，A、C和D是可达的，B、E和F是可收集的（我用不同的颜色做了标注）。
在清除阶段中，GC遍历所有从堆里申请的对象，把标记为0的对象收回，把标记为1的内存重新置为0，等待下次垃圾收集再做标记。
这个算法虽然看上去简单清晰，但存在一个潜在的问题。
在标记阶段，也就是遍历图的时候，必须要有一个列表作为辅助的数据结构，来保存所有待检查的对象。但这个列表要多大，只有运行时才清楚，所以没有办法提前预留出一块内存，用于清除算法。而一旦开始垃圾收集，那说明系统的内存已经比较紧张了，所以剩下的内存是否够这个辅助的数据结构用，是不确定的。
可能你会说：那我可以改成递归算法，递归地查找下级节点并做标记。这是不行的，因为每次递归调用都会增加一个栈桢，来保存递归调用的参数等信息，内存消耗有可能更大。
不过，方法总比问题多，针对算法的内存占用问题，你可以用指针逆转（pointer reversal）来解决。这个技术的思想是：把算法所需要的辅助数据，记录在内存对象自身的存储空间。具体做法是：顺着指针方向从A到达B时，我们把从A到B的指针逆转过来，改成从B到A。把B以及B的子节点标记完以后，再顺着这个指针找到回去的路，回到A，然后再把指针逆转回来。
整个标记过程的直观示意图如下：
关于这个技术，你需要注意其中一个技术细节：内存对象中，可能没有空间来存一个指针信息。比如下图中，B对象原来就有一个变量，用来保存指向C的指针。现在用这个变量的位置保存逆转指针，来指向A就行了。但到C的时候，发现C没有空间来存逆转到B的指针。
这时，借助寄存器就可以了。在设置从B到A的指针之前，要把B和C的地址，临时保存在寄存器里，避免地址的丢失。进入C以后，如果C没有存指针的空间，就证明C是个叶子节点，这时，用寄存器里保存的地址返回给B就行了。
采用标记和清除算法，你会记住所有收集了的内存（通常是存在一个双向列表里），在下次申请内存的时候，可以从中寻找大小合适的内存块。不过，这会导致一个问题：随着我们多次申请和释放内存，内存会变得碎片化。所以，在申请内存的时候，要寻找合适的内存块，算法会有点儿复杂。而且就算你努力去寻找，当申请稍微大一点儿的内存时，也会失败。
为了避免内存碎片，你可以采用变化后的算法，标记-整理算法：在做完标记以后，做一下内存的整理，让存活的对象都移动到一边，消除掉内存碎片。
除此之外，停止和拷贝算法，也能够避免内存碎片化。
停止和拷贝（Stop and Copy）采用这个算法后，内存被分成两块：
一块是旧空间，用于分配内存。 一块是新空间，用于垃圾收集。 停止和拷贝算法也可以叫做复制式收集（Coping Collection）。
你需要保持一个堆指针，指向自由空间开始的位置。申请内存时，把堆指针往右移动就行了，比标记-清除算法申请内存更简单。
这里需要注意，旧空间里有一些对象可能已经不可达了（图中的灰色块），但你不用管。当旧空间变满时，就把所有可达的对象，拷贝到新空间，并且把新旧空间互换。这时，新空间里所有对象整齐排列，没有内存碎片。
停止-拷贝算法被认为是最快的垃圾收集算法，有两点原因：
分配内存比较简单，只需要移动堆指针就可以了。 垃圾收集的代价也比较低，因为它只拷贝可达的对象。当垃圾对象所占比例较高的时候，这种算法的优势就更大。 不过，停止-拷贝算法还有缺陷：
有些语言不允许修改指针地址。 在拷贝内存之后，你需要修改所有指向这块内存的指针。像C、C++这样的语言，因为内存地址是对编程者可见的，所以没法采用停止和拷贝算法。
始终有一半内存是闲置的，所以内存利用率不高。 最后，它一次垃圾收集的工作量比较大，会导致系统停顿时间比较长，对于一些关键系统来说，这种较长时间的停顿是不可接受的。但这两个算法都是基础的算法，它们可以被组合进更复杂的算法中，比如分代和增量的算法中，就能避免这个问题。 引用计数（Reference Counting）引用计数支持增量的垃圾收集，可以避免较长时间的停顿。
它的原理是：在每个对象中，保存引用本对象的指针数量，每次做赋值操作时，都要修改这个引用计数。如果x和y分别指向A和B，当执行“x=y”这样的赋值语句时，要把A的引用计数减少，把B的引用计数增加。如果某个对象的引用计数变成了0，那就可以把它收集掉。
所以，引用计数算法非常容易实现，只需要在赋值时修改引用计数就可以了。
不过，引用计数方法也有缺陷：
首先，是不能收集循环引用的结构。比如图中的A、B、C和D的引用计数都是1，但它们只是互相引用，没有其他变量指向它们。而循环引用在面向对象编程里很常见，比如一棵树的结构中，父节点保存了子节点的引用，子节点也保存了父节点的引用，这会让整棵树都没有办法被收集。
如果你有C++工作经验，应该思考过，怎么自动管理内存。有一个思路是：实现智能指针，对指针的引用做计数。这种思路也有循环引用的问题，所以要用其他算法辅助，来解决这个问题。
其次，在每次赋值时，都要修改引用计数，开销大。何况修改引用计数涉及写内存的操作，而写内存是比较慢的，会导致性能的降低。
其实，这三个算法都是比较单一的算法，实际上，它们可以作为更复杂、更实用算法的组成部分，比如分代收集算法。</description></item><item><title>33_字符串匹配基础（中）：如何实现文本编辑器中的查找功能？</title><link>https://artisanbox.github.io/2/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/34/</guid><description>文本编辑器中的查找替换功能，我想你应该不陌生吧？比如，我们在Word中把一个单词统一替换成另一个，用的就是这个功能。你有没有想过，它是怎么实现的呢？
当然，你用上一节讲的BF算法和RK算法，也可以实现这个功能，但是在某些极端情况下，BF算法性能会退化的比较严重，而RK算法需要用到哈希算法，设计一个可以应对各种类型字符的哈希算法并不简单。
对于工业级的软件开发来说，我们希望算法尽可能的高效，并且在极端情况下，性能也不要退化的太严重。那么，对于查找功能是重要功能的软件来说，比如一些文本编辑器，它们的查找功能都是用哪种算法来实现的呢？有没有比BF算法和RK算法更加高效的字符串匹配算法呢？
今天，我们就来学习BM（Boyer-Moore）算法。它是一种非常高效的字符串匹配算法，有实验统计，它的性能是著名的KMP算法的3到4倍。BM算法的原理很复杂，比较难懂，学起来会比较烧脑，我会尽量给你讲清楚，同时也希望你做好打硬仗的准备。好，现在我们正式开始！
BM算法的核心思想我们把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF算法和RK算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。我举个例子解释一下，你可以看我画的这幅图。
在这个例子里，主串中的c，在模式串中是不存在的，所以，模式串向后滑动的时候，只要c与模式串没有重合，肯定无法匹配。所以，我们可以一次性把模式串往后多滑动几位，把模式串移动到c的后面。
由现象找规律，你可以思考一下，当遇到不匹配的字符时，有什么固定的规律，可以将模式串往后多滑动几位呢？这样一次性往后滑动好几位，那匹配的效率岂不是就提高了？
我们今天要讲的BM算法，本质上其实就是在寻找这种规律。借助这种规律，在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位。
BM算法原理分析BM算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）。我们下面依次来看，这两个规则分别都是怎么工作的。
1.坏字符规则前面两节讲的算法，在匹配的过程中，我们都是按模式串的下标从小到大的顺序，依次与主串中的字符进行匹配的。这种匹配顺序比较符合我们的思维习惯，而BM算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的。我画了一张图，你可以看下。
从模式串的末尾往前倒着匹配，当发现某个字符没法匹配的时候，我们把这个没有匹配的字符叫作坏字符（主串中的字符）。
我们拿坏字符c在模式串中查找，发现模式串中并不存在这个字符，也就是说，字符c与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位，将模式串滑动到c后面的位置，再从模式串的末尾字符开始比较。
这个时候，我们发现，模式串中最后一个字符d，还是无法跟主串中的a匹配，这个时候，还能将模式串往后滑动三位吗？答案是不行的。因为这个时候，坏字符a在模式串中是存在的，模式串中下标是0的位置也是字符a。这种情况下，我们可以将模式串往后滑动两位，让两个a上下对齐，然后再从模式串的末尾字符开始，重新匹配。
第一次不匹配的时候，我们滑动了三位，第二次不匹配的时候，我们将模式串后移两位，那具体滑动多少位，到底有没有规律呢？
当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作si。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作xi。如果不存在，我们把xi记作-1。那模式串往后移动的位数就等于si-xi。（注意，我这里说的下标，都是字符在模式串的下标）。
这里我要特别说明一点，如果坏字符在模式串里多处出现，那我们在计算xi的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。
利用坏字符规则，BM算法在最好情况下的时间复杂度非常低，是O(n/m)。比如，主串是aaabaaabaaabaaab，模式串是aaaa。每次比对，模式串都可以直接后移四位，所以，匹配具有类似特点的模式串和主串的时候，BM算法非常高效。
不过，单纯使用坏字符规则还是不够的。因为根据si-xi计算出来的移动位数，有可能是负数，比如主串是aaaaaaaaaaaaaaaa，模式串是baaa。不但不会向后滑动模式串，还有可能倒退。所以，BM算法还需要用到“好后缀规则”。
2.好后缀规则好后缀规则实际上跟坏字符规则的思路很类似。你看我下面这幅图。当模式串滑动到图中的位置的时候，模式串和主串有2个字符是匹配的，倒数第3个字符发生了不匹配的情况。
这个时候该如何滑动模式串呢？当然，我们还可以利用坏字符规则来计算模式串的滑动位数，不过，我们也可以使用好后缀处理规则。两种规则到底如何选择，我稍后会讲。抛开这个问题，现在我们来看，好后缀规则是怎么工作的？
我们把已经匹配的bc叫作好后缀，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。
如果在模式串中找不到另一个等于{u}的子串，我们就直接将模式串，滑动到主串中{u}的后面，因为之前的任何一次往后滑动，都没有匹配主串中{u}的情况。
不过，当模式串中不存在等于{u}的子串时，我们直接将模式串滑动到主串{u}的后面。这样做是否有点太过头呢？我们来看下面这个例子。这里面bc是好后缀，尽管在模式串中没有另外一个相匹配的子串{u*}，但是如果我们将模式串移动到好后缀的后面，如图所示，那就会错过模式串和主串可以匹配的情况。
如果好后缀在模式串中不存在可匹配的子串，那在我们一步一步往后滑动模式串的过程中，只要主串中的{u}与模式串有重合，那肯定就无法完全匹配。但是当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况。
所以，针对这种情况，我们不仅要看好后缀在模式串中，是否有另一个匹配的子串，我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。
所谓某个字符串s的后缀子串，就是最后一个字符跟s对齐的子串，比如abc的后缀子串就包括c, bc。所谓前缀子串，就是起始字符跟s对齐的子串，比如abc的前缀子串有a，ab。我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的，假设是{v}，然后将模式串滑动到如图所示的位置。
坏字符和好后缀的基本原理都讲完了，我现在回答一下前面那个问题。当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？
我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。
BM算法代码实现学习完了基本原理，我们再来看，如何实现BM算法？
“坏字符规则”本身不难理解。当遇到坏字符时，要计算往后移动的位数si-xi，其中xi的计算是重点，我们如何求得xi呢？或者说，如何查找坏字符在模式串中出现的位置呢？
如果我们拿坏字符，在模式串中顺序遍历查找，这样就会比较低效，势必影响这个算法的性能。有没有更加高效的方式呢？我们之前学的散列表，这里可以派上用场了。我们可以将模式串中的每个字符及其下标都存到散列表中。这样就可以快速找到坏字符在模式串的位置下标了。
关于这个散列表，我们只实现一种最简单的情况，假设字符串的字符集不是很大，每个字符长度是1字节，我们用大小为256的数组，来记录每个字符在模式串中出现的位置。数组的下标对应字符的ASCII码值，数组中存储这个字符在模式串中出现的位置。
如果将上面的过程翻译成代码，就是下面这个样子。其中，变量b是模式串，m是模式串的长度，bc表示刚刚讲的散列表。
private static final int SIZE = 256; // 全局变量或成员变量 private void generateBC(char[] b, int m, int[] bc) { for (int i = 0; i &amp;lt; SIZE; ++i) { bc[i] = -1; // 初始化bc } for (int i = 0; i &amp;lt; m; ++i) { int ascii = (int)b[i]; // 计算b[i]的ASCII值 bc[ascii] = i; } } 掌握了坏字符规则之后，我们先把BM算法代码的大框架写好，先不考虑好后缀规则，仅用坏字符规则，并且不考虑si-xi计算得到的移动位数可能会出现负数的情况。</description></item><item><title>33_并发中的编译技术（一）：如何从语言层面支持线程？</title><link>https://artisanbox.github.io/7/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/33/</guid><description>你好，我是宫文学。
现代的编程语言，开始越来越多地采用并发计算的模式。这也对语言的设计和编译技术提出了要求，需要能够更方便地利用计算机的多核处理能力。
并发计算需求的增长跟两个趋势有关：一是，CPU在制程上的挑战越来越大，逼近物理极限，主频提升也越来越慢，计算能力的提升主要靠核数的增加，比如现在的手机，核数越来越多，动不动就8核、12核，用于服务器的CPU核数则更多；二是，现代应用对并发处理的需求越来越高，云计算、人工智能、大数据和5G都会吃掉大量的计算量。
因此，在现代语言中，友好的并发处理能力是一项重要特性，也就需要编译技术进行相应的配合。现代计算机语言采用了多种并发技术，包括线程、协程、Actor模式等。我会用三讲来带你了解它们，从而理解编译技术要如何与这些并发计算模式相配合。
这一讲，我们重点探讨线程模式，它是现代计算机语言中支持并发的基础模式。它也是讨论协程和Actor等其他话题的基础。
不过在此之前，我们需要先了解一下并发计算的一点底层机制：并行与并发、进程和线程。
并发的底层机制：并行与并发、进程与线程我们先来学习一下硬件层面对并行计算的支持。
假设你的计算机有两颗CPU，每颗CPU有两个内核，那么在同一时间，至少可以有4个程序同时运行。
后来CPU厂商又发明了超线程（Hyper Threading）技术，让一个内核可以同时执行两个线程，增加对CPU内部功能单元的利用率，这有点像我们之前讲过的流水线技术。这样一来，在操作系统里就可以虚拟出8个内核（或者叫做操作系统线程），在同一时间可以有8个程序同时运行。这种真正的同时运行，我们叫做并行（parallelism）。
图1：虚拟内核与CPU真实内核的对应关系可是仅仅8路并行，也不够用呀。如果你去查看一下自己电脑里的进程数，会发现运行着几十个进程，而线程数就更多了。
所以，操作系统会用分时技术，让一个程序执行一段时间，停下来，再让另一个程序运行。由于时间片切得很短，对于每一个程序来说，感觉上似乎一直在运行。这种“同时”能处理多个任务，但实际上并不一定是真正同时执行的，就叫做并发（Concurrency）。
实际上，哪怕我们的计算机只有一个内核，我们也可以实现多个任务的并发执行。这通常是由操作系统的一个调度程序（Scheduler）来实现的。但是有一点，操作系统在调度多个任务的时候，是有一定开销的：
一开始是以进程为单位来做调度，开销比较大。 在切换进程的时候，要保存当前进程的上下文，加载下一个进程的上下文，也会有一定的开销。由于进程是一个比较大的单位，其上下文的信息也比较多，包括用户级上下文（程序代码、静态数据、用户堆栈等）、寄存器上下文（各种寄存器的值）和系统级上下文（操作系统中与该进程有关的信息，包括进程控制块、内存管理信息、内核栈等）。 相比于进程，线程技术就要轻量级一些。在一个进程内部，可以有多个线程，每个线程都共享进程的资源，包括内存资源（代码、静态数据、堆）、操作系统资源（如文件描述符、网络连接等）和安全属性（用户ID等），但拥有自己的栈和寄存器资源。这样一来，线程的上下文包含的信息比较少，所以切换起来开销就比较小，可以把宝贵的CPU时间用于执行用户的任务。
总结起来，线程是操作系统做并发调度的基本单位，并且可以跟同一个进程内的其他线程共享内存等资源。操作系统会让一个线程运行一段时间，然后把它停下来，把它所使用的寄存器保存起来，接着让另一个线程运行，这就是线程调度原理。你要在大脑里记下这个场景，这样对理解后面所探讨的所有并发技术都很有帮助。
图2：进程的共享资源和线程私有的资源我们通常把进程作为资源分配的基本单元，而把线程作为并发执行的基本单元。不过，有的时候，用进程作为并发的单元也是比较好的，比如谷歌浏览器每打开一个Tab页，就新启动一个进程。这是因为，浏览器中多个进程之间不需要有互动。并且，由于各个进程所使用的资源是独立的，所以一个进程崩溃也不会影响到另一个。
而如果采用线程模型的话，由于它比较轻量级，消耗的资源比较少，所以你可以在一个操作系统上启动几千个线程，这样就能执行更多的并发任务。所以，在一般的网络编程模型中，我们可以针对每个网络连接，都启动一条线程来处理该网络连接上的请求。在第二个模块中我们分析过的MySQL就是这样做的。你每次跟MySQL建立连接，它就会启动一条线程来响应你的查询请求。
采用线程模型的话，程序就可以在不同线程之间共享数据。比如，在数据库系统中，如果一个客户端提交了一条SQL，那么这个SQL的编译结果可以被缓存起来。如果另一个用户恰好也执行了同一个SQL，那么就可以不用再编译一遍，因为两条线程可以访问共享的内存。
但是共享内存也会带来一些问题。当多个线程访问同样的数据的时候，会出现数据处理的错误。如果使用并发程序会造成错误，那当然不是我们所希望的。所以，我们就要采用一定的技术去消除这些错误。
Java语言内置的并发模型就是线程模型，并且在语法层面为线程模型提供了一些原生的支持。所以接下来，我们先借助Java语言去了解一下，如何用编译技术来配合线程模型。
Java的并发机制Java从语言层面上对并发编程提供了支持，简化了程序的开发。
Java对操作系统的线程进行了封装，程序员使用Thread类或者让一个类实现Runnable接口，就可以作为一个线程运行。Thread类提供了一些方法，能够控制线程的运行，并能够在多个线程之间协作。
从语法角度，与并发有关的关键字有synchronized和volatile。它们就是用于解决多个线程访问共享内存的难题。
synchronized关键字：保证操作的原子性我们通过一个例子，来看看多个线程访问共享数据的时候，为什么会导致数据错误。
public class TestThread { public static void main(String[] args) { Num num = new Num(); for (int i = 0; i &amp;amp;lt; 3; i++) { new NewThread(num).start(); } } }
//线程类NewThread 对数字进行操作 class NewThread extends Thread { private Num num;</description></item><item><title>33_我查这么多数据，会不会把数据库内存打爆？</title><link>https://artisanbox.github.io/1/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/33/</guid><description>我经常会被问到这样一个问题：我的主机内存只有100G，现在要对一个200G的大表做全表扫描，会不会把数据库主机的内存用光了？
这个问题确实值得担心，被系统OOM（out of memory）可不是闹着玩的。但是，反过来想想，逻辑备份的时候，可不就是做整库扫描吗？如果这样就会把内存吃光，逻辑备份不是早就挂了？
所以说，对大表做全表扫描，看来应该是没问题的。但是，这个流程到底是怎么样的呢？
全表扫描对server层的影响假设，我们现在要对一个200G的InnoDB表db1. t，执行一个全表扫描。当然，你要把扫描结果保存在客户端，会使用类似这样的命令：
mysql -h$host -P$port -u$user -p$pwd -e &amp;quot;select * from db1.t&amp;quot; &amp;gt; $target_file 你已经知道了，InnoDB的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表t的主键索引。这条查询语句由于没有其他的判断条件，所以查到的每一行都可以直接放到结果集里面，然后返回给客户端。
那么，这个“结果集”存在哪里呢？
实际上，服务端并不需要保存一个完整的结果集。取数据和发数据的流程是这样的：
获取一行，写到net_buffer中。这块内存的大小是由参数net_buffer_length定义的，默认是16k。
重复获取行，直到net_buffer写满，调用网络接口发出去。
如果发送成功，就清空net_buffer，然后继续取下一行，并写入net_buffer。
如果发送函数返回EAGAIN或WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。
这个过程对应的流程图如下所示。
图1 查询结果发送流程从这个流程中，你可以看到：
一个查询在发送过程中，占用的MySQL内部的内存最大就是net_buffer_length这么大，并不会达到200G；
socket send buffer 也不可能达到200G（默认定义/proc/sys/net/core/wmem_default），如果socket send buffer被写满，就会暂停读数据的流程。
也就是说，MySQL是“边读边发的”，这个概念很重要。这就意味着，如果客户端接收得慢，会导致MySQL服务端由于结果发不出去，这个事务的执行时间变长。
比如下面这个状态，就是我故意让客户端不去读socket receive buffer中的内容，然后在服务端show processlist看到的结果。
图2 服务端发送阻塞如果你看到State的值一直处于“Sending to client”，就表示服务器端的网络栈写满了。
我在上一篇文章中曾提到，如果客户端使用–quick参数，会使用mysql_use_result方法。这个方法是读一行处理一行。你可以想象一下，假设有一个业务的逻辑比较复杂，每读一行数据以后要处理的逻辑如果很慢，就会导致客户端要过很久才会去取下一行数据，可能就会出现如图2所示的这种情况。
因此，对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，我都建议你使用mysql_store_result这个接口，直接把查询结果保存到本地内存。
当然前提是查询返回结果不多。在第30篇文章评论区，有同学说到自己因为执行了一个大查询导致客户端占用内存近20G，这种情况下就需要改用mysql_use_result接口了。
另一方面，如果你在自己负责维护的MySQL里看到很多个线程都处于“Sending to client”这个状态，就意味着你要让业务开发同学优化查询结果，并评估这么多的返回结果是否合理。
而如果要快速减少处于这个状态的线程的话，将net_buffer_length参数设置为一个更大的值是一个可选方案。
与“Sending to client”长相很类似的一个状态是“Sending data”，这是一个经常被误会的问题。有同学问我说，在自己维护的实例上看到很多查询语句的状态是“Sending data”，但查看网络也没什么问题啊，为什么Sending data要这么久？</description></item><item><title>33_解读TPU：设计和拆解一块ASIC芯片</title><link>https://artisanbox.github.io/4/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/33/</guid><description>过去几年，最知名、最具有实用价值的ASIC就是TPU了。各种解读TPU论文内容的文章网上也很多。不过，这些文章更多地是从机器学习或者AI的角度，来讲解TPU。
上一讲，我为你讲解了FPGA和ASIC，讲解了FPGA如何实现通过“软件”来控制“硬件”，以及我们可以进一步把FPGA设计出来的电路变成一块ASIC芯片。
不过呢，这些似乎距离我们真实的应用场景有点儿远。我们怎么能够设计出来一块有真实应用场景的ASIC呢？如果要去设计一块ASIC，我们应该如何思考和拆解问题呢？今天，我就带着你一起学习一下，如何设计一块专用芯片。
TPU V1想要解决什么问题？黑格尔说，“世上没有无缘无故的爱，也没有无缘无故的恨”。第一代TPU的设计并不是异想天开的创新，而是来自于真实的需求。
从2012年解决计算机视觉问题开始，深度学习一下子进入了大爆发阶段，也一下子带火了GPU，NVidia的股价一飞冲天。我们在第31讲讲过，GPU天生适合进行海量、并行的矩阵数值计算，于是它被大量用在深度学习的模型训练上。
不过你有没有想过，在深度学习热起来之后，计算量最大的是什么呢？并不是进行深度学习的训练，而是深度学习的推断部分。
所谓推断部分，是指我们在完成深度学习训练之后，把训练完成的模型存储下来。这个存储下来的模型，是许许多多个向量组成的参数。然后，我们根据这些参数，去计算输入的数据，最终得到一个计算结果。这个推断过程，可能是在互联网广告领域，去推测某一个用户是否会点击特定的广告；也可能是我们在经过高铁站的时候，扫一下身份证进行一次人脸识别，判断一下是不是你本人。
虽然训练一个深度学习的模型需要花的时间不少，但是实际在推断上花的时间要更多。比如，我们上面说的高铁，去年（2018年）一年就有20亿人次坐了高铁，这也就意味着至少进行了20亿次的人脸识别“推断“工作。
所以，第一代的TPU，首先优化的并不是深度学习的模型训练，而是深度学习的模型推断。这个时候你可能要问了，那模型的训练和推断有什么不同呢？主要有三个点。
第一点，深度学习的推断工作更简单，对灵活性的要求也就更低。模型推断的过程，我们只需要去计算一些矩阵的乘法、加法，调用一些Sigmoid或者RELU这样的激活函数。这样的过程可能需要反复进行很多层，但是也只是这些计算过程的简单组合。
第二点，深度学习的推断的性能，首先要保障响应时间的指标。我们在第4讲讲过，计算机关注的性能指标，有响应时间（Response Time）和吞吐率（Throughput）。我们在模型训练的时候，只需要考虑吞吐率问题就行了。因为一个模型训练少则好几分钟，多的话要几个月。而推断过程，像互联网广告的点击预测，我们往往希望能在几十毫秒乃至几毫秒之内就完成，而人脸识别也不希望会超过几秒钟。很显然，模型训练和推断对于性能的要求是截然不同的。
第三点，深度学习的推断工作，希望在功耗上尽可能少一些。深度学习的训练，对功耗没有那么敏感，只是希望训练速度能够尽可能快，多费点电就多费点儿了。这是因为，深度学习的推断，要7×24h地跑在数据中心里面。而且，对应的芯片，要大规模地部署在数据中心。一块芯片减少5%的功耗，就能节省大量的电费。而深度学习的训练工作，大部分情况下只是少部分算法工程师用少量的机器进行。很多时候，只是做小规模的实验，尽快得到结果，节约人力成本。少数几台机器多花的电费，比起算法工程师的工资来说，只能算九牛一毛了。
这三点的差别，也就带出了第一代TPU的设计目标。那就是，在保障响应时间的情况下，能够尽可能地提高能效比这个指标，也就是进行同样多数量的推断工作，花费的整体能源要显著低于CPU和GPU。
深入理解TPU V1快速上线和向前兼容，一个FPU的设计如果你来设计TPU，除了满足上面的深度学习的推断特性之外，还有什么是你要重点考虑的呢？你可以停下来思考一下，然后再继续往下看。
不知道你的答案是什么，我的第一反应是，有两件事情必须要考虑，第一个是TPU要有向前兼容性，第二个是希望TPU能够尽早上线。我下面说说我考虑这两点的原因。
图片来源第一代的TPU就像一块显卡一样，可以直接插在主板的PCI-E口上第一点，向前兼容。在计算机产业界里，因为没有考虑向前兼容，惨遭失败的产品数不胜数。典型的有我在第26讲提过的安腾处理器。所以，TPU并没有设计成一个独立的“CPU“，而是设计成一块像显卡一样，插在主板PCI-E接口上的板卡。更进一步地，TPU甚至没有像我们之前说的现代GPU一样，设计成自己有对应的取指令的电路，而是通过CPU，向TPU发送需要执行的指令。
这两个设计，使得我们的TPU的硬件设计变得简单了，我们只需要专心完成一个专用的“计算芯片”就好了。所以，TPU整个芯片的设计上线时间也就缩短到了15个月。不过，这样一个TPU，其实是第26讲里我们提过的387浮点数计算芯片，是一个像FPU（浮点数处理器）的协处理器（Coprocessor），而不是像CPU和GPU这样可以独立工作的Processor Unit。
专用电路和大量缓存，适应推断的工作流程明确了TPU整体的设计思路之后，我们可以来看一看，TPU内部有哪些芯片和数据处理流程。我在文稿里面，放了TPU的模块图和对应的芯片布局图，你可以对照着看一下。
图片来源模块图：整个TPU的硬件，完全是按照深度学习一个层（Layer）的计算流程来设计的你可以看到，在芯片模块图里面，有单独的矩阵乘法单元（Matrix Multiply Unit）、累加器（Accumulators）模块、激活函数（Activation）模块和归一化/池化（Normalization/Pool）模块。而且，这些模块是顺序串联在一起的。
这是因为，一个深度学习的推断过程，是由很多层的计算组成的。而每一个层（Layer）的计算过程，就是先进行矩阵乘法，再进行累加，接着调用激活函数，最后进行归一化和池化。这里的硬件设计呢，就是把整个流程变成一套固定的硬件电路。这也是一个ASIC的典型设计思路，其实就是把确定的程序指令流程，变成固定的硬件电路。
接着，我们再来看下面的芯片布局图，其中控制电路（Control）只占了2%。这是因为，TPU的计算过程基本上是一个固定的流程。不像我们之前讲的CPU那样，有各种复杂的控制功能，比如冒险、分支预测等等。
你可以看到，超过一半的TPU的面积，都被用来作为Local Unified Buffer（本地统一缓冲区）（29%）和矩阵乘法单元（Matrix Mutliply Unit）了。
相比于矩阵乘法单元，累加器、实现激活函数和后续的归一/池化功能的激活管线（Activation Pipeline）也用得不多。这是因为，在深度学习推断的过程中，矩阵乘法的计算量是最大的，计算也更复杂，所以比简单的累加器和激活函数要占用更多的晶体管。
而统一缓冲区（Unified Buffer），则由SRAM这样高速的存储设备组成。SRAM一般被直接拿来作为CPU的寄存器或者高速缓存。我们在后面的存储器部分会具体讲。SRAM比起内存使用的DRAM速度要快上很多，但是因为电路密度小，所以占用的空间要大很多。统一缓冲区之所以使用SRAM，是因为在整个的推断过程中，它会高频反复地被矩阵乘法单元读写，来完成计算。
图片来源芯片布局图：从尺寸可以看出，统一缓冲区和矩阵乘法单元是TPU的核心功能组件可以看到，整个TPU里面，每一个组件的设计，完全是为了深度学习的推断过程设计出来的。这也是我们设计开发ASIC的核心原因：用特制的硬件，最大化特定任务的运行效率。
细节优化，使用8 Bits数据除了整个TPU的模块设计和芯片布局之外，TPU在各个细节上也充分考虑了自己的应用场景，我们可以拿里面的矩阵乘法单元（Matrix Multiply Unit）来作为一个例子。
如果你仔细一点看的话，会发现这个矩阵乘法单元，没有用32 Bits来存放一个浮点数，而是只用了一个8 Bits来存放浮点数。这是因为，在实践的机器学习应用中，会对数据做归一化（Normalization）和正则化（Regularization）的处理。咱们毕竟不是一个机器学习课，所以我就不深入去讲什么是归一化和正则化了，你只需要知道，这两个操作呢，会使得我们在深度学习里面操作的数据都不会变得太大。通常来说呢，都能控制在-3到3这样一定的范围之内。
因为这个数值上的特征，我们需要的浮点数的精度也不需要太高了。我们在第16讲讲解浮点数的时候说过，32位浮点数的精度，差不多可以到1/1600万。如果我们用8位或者16位表示浮点数，也能把精度放到2^6或者2^12，也就是1/64或者1/4096。在深度学习里，常常够用了。特别是在模型推断的时候，要求的计算精度，往往可以比模型训练低。所以，8 Bits的矩阵乘法器，就可以放下更多的计算量，使得TPU的推断速度更快。
用数字说话，TPU的应用效果那么，综合了这么多优秀设计点的TPU，实际的使用效果怎么样呢？不管设计得有多好，最后还是要拿效果和数据说话。俗话说，是骡子是马，总要拿出来溜溜啊。
Google在TPU的论文里面给出了答案。一方面，在性能上，TPU比现在的CPU、GPU在深度学习的推断任务上，要快15～30倍。而在能耗比上，更是好出30～80倍。另一方面，Google已经用TPU替换了自家数据中心里95%的推断任务，可谓是拿自己的实际业务做了一个明证。
总结延伸这一讲，我从第一代TPU的设计目标讲起，为你解读了TPU的设计。你可以通过这篇文章，回顾我们过去32讲提到的各种知识点。
第一代TPU，是为了做各种深度学习的推断而设计出来的，并且希望能够尽早上线。这样，Google才能节约现有数据中心里面的大量计算资源。
从深度学习的推断角度来考虑，TPU并不需要太灵活的可编程能力，只要能够迭代完成常见的深度学习推断过程中一层的计算过程就好了。所以，TPU的硬件构造里面，把矩阵乘法、累加器和激活函数都做成了对应的专门的电路。
为了满足深度学习推断功能的响应时间短的需求，TPU设置了很大的使用SRAM的Unified Buffer（UB），就好像一个CPU里面的寄存器一样，能够快速响应对于这些数据的反复读取。
为了让TPU尽可能快地部署在数据中心里面，TPU采用了现有的PCI-E接口，可以和GPU一样直接插在主板上，并且采用了作为一个没有取指令功能的协处理器，就像387之于386一样，仅仅用来进行需要的各种运算。
在整个电路设计的细节层面，TPU也尽可能做到了优化。因为机器学习的推断功能，通常做了数值的归一化，所以对于矩阵乘法的计算精度要求有限，整个矩阵乘法的计算模块采用了8 Bits来表示浮点数，而不是像Intel CPU里那样用上了32 Bits。
最终，综合了种种硬件设计点之后的TPU，做到了在深度学习的推断层面更高的能效比。按照Google论文里面给出的官方数据，它可以比CPU、GPU快上15～30倍，能耗比更是可以高出30～80倍。而TPU，也最终替代了Google自己的数据中心里，95%的深度学习推断任务。
推荐阅读既然要深入了解TPU，自然要读一读关于TPU的论文In-Datacenter Performance Analysis of a Tensor Processing Unit。
除了这篇论文之外，你也可以读一读Google官方专门讲解TPU构造的博客文章 An in-depth look at Google’s first Tensor Processing Unit(TPU)。</description></item><item><title>33｜函数式编程第2关：实现闭包特性</title><link>https://artisanbox.github.io/3/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/35/</guid><description>你好，我是宫文学。
上节课，我们实现了函数式编程的一个重要特性：高阶函数。今天这节课，我们继续来实现函数式编程的另一个重要特性，也就是闭包。
闭包机制能实现信息的封装、缓存内部状态数据等等，所以被资深程序员所喜欢，并被用于实现各种框架、类库等等。相信很多程序员都了解过闭包的概念，但往往对它的内在机制不是十分清楚。
今天这节课，我会带你了解闭包的原理和实现机制。在这个过程中，你会了解到闭包数据的形成机制、词法作用域的概念、闭包和面向对象特性的相似性，以及如何访问位于其他栈桢中的数据。
首先，让我们了解一下闭包的技术实质，从而确定如何让我们的语言支持闭包特性。
理解闭包的实质我们先通过一个例子来了解闭包的特点。在下面的示例程序中有一个ID的生成器。这个生成器是一个函数，但它把一个内部函数作为返回值来返回。这个返回值被赋给了函数类型的变量id1，然后调用这个函数。
function idGenerator():number{//()=&amp;gt;number{ let nextId = 0; function getId(){ return nextId++; //访问了外部作用域的一个变量 } return getId; }
println(&amp;quot;\nid1:&amp;quot;); let id1 = idGenerator(); println(id1()); //0 println(id1()); //1
//新创建一个闭包，重新开始编号 println(&amp;quot;\nid2:&amp;quot;); let id2 = idGenerator(); println(id2()); //0 println(id2()); //1
//闭包可以通过赋值和参数传递，在没有任何变量引用它的时候，生命周期才会结束。 println(&amp;quot;\nid3:&amp;quot;); let id3 = id1; println(id3()); //2 &amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;然后神奇的事情就发生了。每次你调用id1()，它都会返回一个不同的值，依次是0、1、2……
为什么每次返回的值会不一样呢？
你看这个代码，内部函数getId()访问了外部函数的一个本地变量nextId。当外部函数退出以后，内部函数仍然可以使用这个本地变量，并且每次调用内部函数时，都让nextId的值加1。这个现象，就体现了闭包的特点。
总结起来，闭包是这么一种现象：一个函数可以返回一个内部函数，但这个内部函数使用了它的作用域之外的数据。这些作用域之外的数据会一直伴随着该内部函数的生命周期，内部函数一直可以访问它。
说得更直白一点，就是当内部函数被返回时，它把外部作用域中的一些数据打包带走了，随身携带，便于访问。
这样分析之后你就明白了。为了支持闭包，你需要让某些函数有一个私有的数据区，用于保存一些私有数据，供这个函数访问。在我们这门课里，我们可以把这个函数专有的数据，叫做闭包数据，或者叫做闭包对象。
然后我们再运行这个示例程序，并分析它的输出结果：
你会发现这样一个事实：id1和id2分别通过调用idGenerator()函数获得了一个内部函数，而它们各自拥有自己的闭包数据，是互不干扰的。
从这种角度看，闭包有点像面向对象特性。每次new一个对象的时候，都会生成不同的对象实例。实际上，在函数式语言里，我们确实可以用闭包来模拟某些面向对象编程特性。不过这里你要注意，并不是函数所引用的外部数据，都需要放到私有的数据区中的。我们可以再通过一个例子来看一下。
我把前面的示例程序做了一点修改。这一次，我们的内部函数可以访问两个变量了。
//编号的组成部分 let segment:number = 1000;
function idGenerator():()=&amp;gt;number{ let nextId = 0;</description></item><item><title>34_仓库管理：如何实现文件的六大基本操作？</title><link>https://artisanbox.github.io/9/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/34/</guid><description>你好，我是LMOS。
我们在上一节课中，已经建立了仓库，并对仓库进行了划分，就是文件系统的格式化。有了仓库就需要往里面存取东西，对于我们的仓库来说，就是存取应用程序的文件。
所以今天我们要给仓库增加一些相关的操作，这些操作主要用于新建、打开、关闭、读写文件，它们也是文件系统的标准功能，自然即使我们这个最小的文件系统，也必须要支持。
好了，话不多说，我们开始吧。这节课的配套代码，你可以从这里下载。
辅助操作通过上一节课的学习，我们了解了文件系统格式化操作，不难发现文件系统格式化并不复杂，但是它们需要大量的辅助函数。同样的，完成文件相关的操作，我们也需要大量的辅助函数。为了让你更加清楚每个实现细节，这里我们先来实现文件操作相关的辅助函数。
操作根目录文件根据我们文件系统的设计，不管是新建、删除、打开一个文件，首先都要找到与该文件对应的rfsdir_t结构。
在我们的文件系统中，一个文件的rfsdir_t结构就储存在根目录文件中，所以想要读取文件对应的rfsdir_t结构，首先就要获取和释放根目录文件。
下面我们来实现获取和释放根目录文件的函数，代码如下所示。
//获取根目录文件 void* get_rootdirfile_blk(device_t* devp) { void* retptr = NULL; rfsdir_t* rtdir = get_rootdir(devp);//获取根目录文件的rfsdir_t结构 //分配4KB大小的缓冲区并清零 void* buf = new_buf(FSYS_ALCBLKSZ); hal_memset(buf, FSYS_ALCBLKSZ, 0); //读取根目录文件的逻辑储存块到缓冲区中 read_rfsdevblk(devp, buf, rtdir-&amp;gt;rdr_blknr) retptr = buf;//设置缓冲区的首地址为返回值 goto errl1; errl: del_buf(buf, FSYS_ALCBLKSZ); errl1: del_rootdir(devp, rtdir);//释放根目录文件的rfsdir_t结构 return retptr; } //释放根目录文件 void del_rootdirfile_blk(device_t* devp,void* blkp) { //因为逻辑储存块的头512字节的空间中，保存的就是fimgrhd_t结构 fimgrhd_t* fmp = (fimgrhd_t*)blkp; //把根目录文件回写到储存设备中去，块号为fimgrhd_t结构自身所在的块号 write_rfsdevblk(devp, blkp, fmp-&amp;gt;fmd_sfblk) //释放缓冲区 del_buf(blkp, FSYS_ALCBLKSZ); return; } 上述代码中，get_rootdir函数的作用就是读取文件系统超级块中rfsdir_t结构到一个缓冲区中，del_rootdir函数则是用来释放这个缓冲区，其代码非常简单，我已经帮你写好了。
获取根目录文件的方法也很容易，根据超级块中的rfsdir_t结构中的信息，读取根目录文件的逻辑储存块就行了。而释放根目录文件，就是把根目录文件的储存块回写到储存设备中去，最后释放对应的缓冲区就可以了。</description></item><item><title>34_到底可不可以使用join？</title><link>https://artisanbox.github.io/1/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/34/</guid><description>在实际生产中，关于join语句使用的问题，一般会集中在以下两类：
我们DBA不让使用join，使用join有什么问题呢？
如果有两个大小不同的表做join，应该用哪个表做驱动表呢？
今天这篇文章，我就先跟你说说join语句到底是怎么执行的，然后再来回答这两个问题。
为了便于量化分析，我还是创建两个表t1和t2来和你说明。
CREATE TABLE `t2` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`) ) ENGINE=InnoDB; drop procedure idata; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=1000)do insert into t2 values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata();
create table t1 like t2; insert into t1 (select * from t2 where id&amp;lt;=100) 可以看到，这两个表都有一个主键索引id和一个索引a，字段b上无索引。存储过程idata()往表t2里插入了1000行数据，在表t1里插入的是100行数据。</description></item><item><title>34_字符串匹配基础（下）：如何借助BM算法轻松理解KMP算法？</title><link>https://artisanbox.github.io/2/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/35/</guid><description>上一节我们讲了BM算法，尽管它很复杂，也不好理解，但却是工程中非常常用的一种高效字符串匹配算法。有统计说，它是最高效、最常用的字符串匹配算法。不过，在所有的字符串匹配算法里，要说最知名的一种的话，那就非KMP算法莫属。很多时候，提到字符串匹配，我们首先想到的就是KMP算法。
尽管在实际的开发中，我们几乎不大可能自己亲手实现一个KMP算法。但是，学习这个算法的思想，作为让你开拓眼界、锻炼下逻辑思维，也是极好的，所以我觉得有必要拿出来给你讲一讲。不过，KMP算法是出了名的不好懂。我会尽力把它讲清楚，但是你自己也要多动动脑子。
实际上，KMP算法跟BM算法的本质是一样的。上一节，我们讲了好后缀和坏字符规则，今天，我们就看下，如何借助上一节BM算法的讲解思路，让你能更好地理解KMP算法？
KMP算法基本原理KMP算法是根据三位作者（D.E.Knuth，J.H.Morris和V.R.Pratt）的名字来命名的，算法的全称是Knuth Morris Pratt算法，简称为KMP算法。
KMP算法的核心思想，跟上一节讲的BM算法非常相近。我们假设主串是a，模式串是b。在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候，我们希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况。
还记得我们上一节讲到的好后缀和坏字符吗？这里我们可以类比一下，在模式串和主串匹配的过程中，把不能匹配的那个字符仍然叫作坏字符，把已经匹配的那段字符串叫作好前缀。
当遇到坏字符的时候，我们就要把模式串往后滑动，在滑动的过程中，只要模式串和好前缀有上下重合，前面几个字符的比较，就相当于拿好前缀的后缀子串，跟模式串的前缀子串在比较。这个比较的过程能否更高效了呢？可以不用一个字符一个字符地比较了吗？
KMP算法就是在试图寻找一种规律：在模式串和主串匹配的过程中，当遇到坏字符后，对于已经比对过的好前缀，能否找到一种规律，将模式串一次性滑动很多位？
我们只需要拿好前缀本身，在它的后缀子串中，查找最长的那个可以跟好前缀的前缀子串匹配的。假设最长的可匹配的那部分前缀子串是{v}，长度是k。我们把模式串一次性往后滑动j-k位，相当于，每次遇到坏字符的时候，我们就把j更新为k，i不变，然后继续比较。
为了表述起来方便，我把好前缀的所有后缀子串中，最长的可匹配前缀子串的那个后缀子串，叫作最长可匹配后缀子串；对应的前缀子串，叫作最长可匹配前缀子串。
如何来求好前缀的最长可匹配前缀和后缀子串呢？我发现，这个问题其实不涉及主串，只需要通过模式串本身就能求解。所以，我就在想，能不能事先预处理计算好，在模式串和主串匹配的过程中，直接拿过来就用呢？
类似BM算法中的bc、suffix、prefix数组，KMP算法也可以提前构建一个数组，用来存储模式串中每个前缀（这些前缀都有可能是好前缀）的最长可匹配前缀子串的结尾字符下标。我们把这个数组定义为next数组，很多书中还给这个数组起了一个名字，叫失效函数（failure function）。
数组的下标是每个前缀结尾字符下标，数组的值是这个前缀的最长可以匹配前缀子串的结尾字符下标。这句话有点拗口，我举了一个例子，你一看应该就懂了。
有了next数组，我们很容易就可以实现KMP算法了。我先假设next数组已经计算好了，先给出KMP算法的框架代码。
// a, b分别是主串和模式串；n, m分别是主串和模式串的长度。 public static int kmp(char[] a, int n, char[] b, int m) { int[] next = getNexts(b, m); int j = 0; for (int i = 0; i &amp;lt; n; ++i) { while (j &amp;gt; 0 &amp;amp;&amp;amp; a[i] != b[j]) { // 一直找到a[i]和b[j] j = next[j - 1] + 1; } if (a[i] == b[j]) { ++j; } if (j == m) { // 找到匹配模式串的了 return i - m + 1; } } return -1; } 失效函数计算方法KMP算法的基本原理讲完了，我们现在来看最复杂的部分，也就是next数组是如何计算出来的？</description></item><item><title>34_并发中的编译技术（二）：如何从语言层面支持协程？</title><link>https://artisanbox.github.io/7/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/34/</guid><description>你好，我是宫文学。
上一讲我们提到了线程模式是当前计算机语言支持并发的主要方式。
不过，在有些情况下，线程模式并不能满足要求。当需要运行大量并发任务的时候，线程消耗的内存、线程上下文切换的开销都太大。这就限制了程序所能支持的并发任务的数量。
在这个背景下，一个很“古老”的技术重新焕发了青春，这就是协程（Coroutine）。它能以非常低的代价、友好的编程方式支持大量的并发任务。像Go、Python、Kotlin、C#等语言都提供了对协程的支持。
今天这一讲，我们就来探究一下如何在计算机语言中支持协程的奇妙功能，它与编译技术又是怎样衔接的。
首先，我们来认识一下协程。
协程（Coroutine）的特点与使用场景我说协程“古老”，是因为这个概念是在1958年被马尔文 · 康威（Melvin Conway）提出来、在20世纪60年代又被高德纳（Donald Ervin Knuth）总结为两种子过程（Subroutine）的模式之一。一种是我们常见的函数调用的方式，而另一种就是协程。在当时，计算机的性能很低，完全没有现代的多核计算机。而采用协程就能够在这样低的配置上实现并发计算，可见它是多么的轻量级。
有的时候，协程又可能被称作绿色线程、纤程等，所采用的技术也各有不同。但总的来说，它们都有一些共同点。
首先，协程占用的资源非常少。你可以在自己的笔记本电脑上随意启动几十万个协程，而如果你启动的是几十万个线程，那结果就是不可想象的。比如，在JVM中，缺省会为每个线程分配1MB的内存，用于线程栈等。这样的话，几千个线程就要消耗掉几个GB的内存，而几十万个线程理论上需要消耗几百GB的内存，这还没算程序在堆中需要申请的内存。当然，由于底层操作系统和Java应用服务器的限制，你也无法启动这么多线程。
其次，协程是用户自己的程序所控制的并发。也就是说，协程模式，一般是程序交出运行权，之后又被另外的程序唤起继续执行，整个过程完全是由用户程序自己控制的。而线程模式就完全不同了，它是由操作系统中的调度器（Scheduler）来控制的。
我们看个Python的例子：
def running_avg(): total = 0.0 count = 0 avg = 0 while True: num = yield avg total += num count += 1 avg = total/count #生成协程，不会有任何输出 ra = running_avg() #运行到yield next(ra)
print(ra.send(2))
print(ra.send(3))
print(ra.send(4)) print(ra.send(7))
print(ra.send(9))
print(ra.send(11))
#关掉协程 ra.close 可以看到，使用协程跟我们平常使用函数几乎没啥差别，对编程人员很友好。实际上，它可以认为是跟函数并列的一种子程序形式。和函数的区别是，函数调用时，调用者跟被调用者之间像是一种上下级的关系；而在协程中，调用者跟被调用者更像是互相协作的关系，比如一个是生产者，一个是消费者。这也是“协程”这个名字直观反映出来的含义。
我们用一张图来对比下函数和协程中的调用关系。
图1：函数（左）与协程（右）的控制流细想一下，编程的时候，这种需要子程序之间互相协作的场景有很多，我们一起看两种比较常见的场景。
第一种比较典型的场景，就是生产者和消费者模式。如果你用过Unix管道或者消息队列编程的话，会非常熟悉这种模式。但那是在多个进程之间的协作。如果用协程的话，在一个进程内部就能实现这种协作，非常轻量级。
就拿编译器前端来说，词法分析器（Tokenizer）和语法分析器（Parser）就可以是这样的协作关系。也就是说，为了更好的性能，没有必要一次把词法分析完毕，而是语法分析器消费一个，就让词法分析器生产一个。因此，这个过程就没有必要做成两个线程了，否则就太重量级了。这种场景，我们可以叫做生成器（Generator）场景：主程序调用生成器，给自己提供数据。
特别适合使用协程的第二种场景是IO密集型的应用。比如做一个网络爬虫同时执行很多下载任务，或者做一个服务器同时响应很多客户端的请求，这样的任务大部分时间是在等待网络传输。
如果用同步阻塞的方式来做，一个下载任务在等待的时候就会把整个线程阻塞掉。而用异步的方式，协程在发起请求之后就把控制权交出，调度程序接收到数据之后再重新激活协程，这样就能高效地完成IO操作，同时看上去又是用同步的方式编程，不需要像异步编程那样写一大堆难以阅读的回调逻辑。
这样的场景在微服务架构的应用中很常见，我们来简化一个实际应用场景，分析下如何使用协程。
在下面的示例中，应用A从客户端接收大量的并发请求，而应用A需要访问应用B的服务接口，从中获得一些信息，然后返回给客户端。
图2：应用间通讯的场景要满足这样的场景，我们最容易想到的就是，编写同步通讯的程序，其实就是同步调用。
假设应用A对于每一个客户端的请求，都会起一个线程做处理。而你呢，则在这个线程里发起一个针对应用B的请求。在等待网络返回结果的时候，当前线程会被阻塞住。
图3：采用同步编程实现应用间的通讯这个架构是最简单的，你如果采用Java的Servlet容器来编写程序的话，很可能会采用这个结构。但它有一些缺陷：</description></item><item><title>34_理解虚拟机：你在云上拿到的计算机是什么样的？</title><link>https://artisanbox.github.io/4/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/34/</guid><description>上世纪60年代，计算机还是异常昂贵的设备，实际的计算机使用需求要面临两个挑战。第一，计算机特别昂贵，我们要尽可能地让计算机忙起来，一直不断地去处理一些计算任务。第二，很多工程师想要用上计算机，但是没有能力自己花钱买一台，所以呢，我们要让很多人可以共用一台计算机。
缘起分时系统为了应对这两个问题，分时系统的计算机就应运而生了。
无论是个人用户，还是一个小公司或者小机构，你都不需要花大价钱自己去买一台电脑。你只需要买一个输入输出的终端，就好像一套鼠标、键盘、显示器这样的设备，然后通过电话线，连到放在大公司机房里面的计算机就好了。这台计算机，会自动给程序或任务分配计算时间。你只需要为你花费的“计算时间”和使用的电话线路付费就可以了。比方说，比尔·盖茨中学时候用的学校的计算机，就是GE的分时系统。
图片来源图片里面的“计算机”其实只是一个终端而已，并没有计算能力，要通过电话线连接到实际的计算机上，才能完成运算从“黑色星期五”到公有云现代公有云上的系统级虚拟机能够快速发展，其实和分时系统的设计思路是一脉相承的，这其实就是来自于电商巨头亚马逊大量富余的计算能力。
和国内有“双十一”一样，美国会有感恩节的“黑色星期五（Black Friday）”和“网络星期一（Cyber Monday）”，这样一年一度的大型电商促销活动。几天的活动期间，会有大量的用户进入亚马逊这样的网站，看商品、下订单、买东西。这个时候，整个亚马逊需要的服务器计算资源可能是平时的数十倍。
于是，亚马逊会按照“黑色星期五”和“网络星期一”的用户访问量，来准备服务器资源。这个就带来了一个问题，那就是在一年的365天里，有360天这些服务器资源是大量空闲的。要知道，这个空闲的服务器数量不是一台两台，也不是几十几百台。根据媒体的估算，亚马逊的云服务器AWS在2014年就已经超过了150万台，到了2019年的今天，估计已经有超过千万台的服务器。
平时有这么多闲着的服务器实在是太浪费了，所以，亚马逊就想把这些服务器给租出去。出租物理服务器当然是可行的，但是却不太容易自动化，也不太容易面向中小客户。
直接出租物理服务器，意味着亚马逊只能进行服务器的“整租”，这样大部分中小客户就不愿意了。为了节约数据中心的空间，亚马逊实际用的物理服务器，大部分多半是强劲的高端8核乃至12核的服务器。想要租用这些服务器的中小公司，起步往往只需要1个CPU核心乃至更少资源的服务器。一次性要他们去租一整台服务器，就好像刚毕业想要租个单间，结果你非要整租个别墅给他。
这个“整租”的问题，还发生在“时间”层面。物理服务器里面装好的系统和应用，不租了而要再给其他人使用，就必须清空里面已经装好的程序和数据，得做一次“重装”。如果我们只是暂时不用这个服务器了，过一段时间又要租这个服务器，数据中心服务商就不得不先重装整个系统，然后租给别人。等别人不用了，再重装系统租给你，特别地麻烦。
其实，对于想要租用服务器的用户来说，最好的体验不是租房子，而是住酒店。我住一天，我就付一天的钱。这次是全家出门，一次多定几间酒店房间就好啦。
而这样的需求，用虚拟机技术来实现，再好不过了。虚拟机技术，使得我们可以在一台物理服务器上，同时运行多个虚拟服务器，并且可以动态去分配，每个虚拟服务器占用的资源。对于不运行的虚拟服务器，我们也可以把这个虚拟服务器“关闭”。这个“关闭”了的服务器，就和一个被关掉的物理服务器一样，它不会再占用实际的服务器资源。但是，当我们重新打开这个虚拟服务器的时候，里面的数据和应用都在，不需要再重新安装一次。
虚拟机的技术变迁那虚拟机技术到底是怎么一回事呢？下面我带你具体来看一看，它的技术变迁过程，好让你能更加了解虚拟机，从而更好地使用它。
虚拟机（Virtual Machine）技术，其实就是指在现有硬件的操作系统上，能够模拟一个计算机系统的技术。而模拟一个计算机系统，最简单的办法，其实不能算是虚拟机技术，而是一个模拟器（Emulator）。
解释型虚拟机要模拟一个计算机系统，最简单的办法，就是兼容这个计算机系统的指令集。我们可以开发一个应用程序，跑在我们的操作系统上。这个应用程序呢，可以识别我们想要模拟的、计算机系统的程序格式和指令，然后一条条去解释执行。
在这个过程中，我们把原先的操作系统叫作宿主机（Host），把能够有能力去模拟指令执行的软件，叫作模拟器（Emulator），而实际运行在模拟器上被“虚拟”出来的系统呢，我们叫客户机（Guest VM）。
这个方式，其实和运行Java程序的Java虚拟机很像。只不过，Java虚拟机运行的是Java自己定义发明的中间代码，而不是一个特定的计算机系统的指令。
这种解释执行另一个系统的方式，有没有真实的应用案例呢？当然是有的，如果你是一个Android开发人员，你在开发机上跑的Android模拟器，其实就是这种方式。如果你喜欢玩一些老游戏，可以注意研究一下，很多能在Windows下运行的游戏机模拟器，用的也是类似的方式。
这种解释执行方式的最大的优势就是，模拟的系统可以跨硬件。比如，Android手机用的CPU是ARM的，而我们的开发机用的是Intel X86的，两边的CPU指令集都不一样，但是一样可以正常运行。如果你想玩的街机游戏，里面的硬件早就已经停产了，那你自然只能选择MAME这样的模拟器。
图片来源MAME模拟器的界面不过这个方式也有两个明显的缺陷。第一个是，我们做不到精确的“模拟”。很多的老旧的硬件的程序运行，要依赖特定的电路乃至电路特有的时钟频率，想要通过软件达到100%模拟是很难做到的。第二个缺陷就更麻烦了，那就是这种解释执行的方式，性能实在太差了。因为我们并不是直接把指令交给CPU去执行的，而是要经过各种解释和翻译工作。
所以，虽然模拟器这样的形式有它的实际用途。甚至为了解决性能问题，也有类似于Java当中的JIT这样的“编译优化”的办法，把本来解释执行的指令，编译成Host可以直接运行的指令。但是，这个性能还是不能让人满意。毕竟，我们本来是想要把空余的计算资源租用出去的。如果我们空出来的计算能力算是个大平层，结果经过模拟器之后能够租出去的计算能力就变成了一个格子间，那我们就划不来了。
Type-1和Type-2：虚拟机的性能提升所以，我们希望我们的虚拟化技术，能够克服上面的模拟器方式的两个缺陷。同时，我们可以放弃掉模拟器方式能做到的跨硬件平台的这个能力。因为毕竟对于我们想要做的云服务里的“服务器租赁”业务来说，中小客户想要租的也是一个x86的服务器。而另外一方面，他们希望这个租用的服务器用起来，和直接买一台或者租一台物理服务器没有区别。作为出租方的我们，也希望服务器不要因为用了虚拟化技术，而在中间损耗掉太多的性能。
所以，首先我们需要一个“全虚拟化”的技术，也就是说，我们可以在现有的物理服务器的硬件和操作系统上，去跑一个完整的、不需要做任何修改的客户机操作系统（Guest OS）。那么，我们怎么在一个操作系统上，再去跑多个完整的操作系统呢？答案就是，我们自己做软件开发中很常用的一个解决方案，就是加入一个中间层。在虚拟机技术里面，这个中间层就叫作虚拟机监视器，英文叫VMM（Virtual Machine Manager）或者Hypervisor。
如果说我们宿主机的OS是房东的话，这个虚拟机监视器呢，就好像一个二房东。我们运行的虚拟机，都不是直接和房东打交道，而是要和这个二房东打交道。我们跑在上面的虚拟机呢，会把整个的硬件特征都映射到虚拟机环境里，这包括整个完整的CPU指令集、I/O操作、中断等等。
既然要通过虚拟机监视器这个二房东，我们实际的指令是怎么落到硬件上去实际执行的呢？这里有两种办法，也就是Type-1和Type-2这两种类型的虚拟机。
我们先来看Type-2类型的虚拟机。在Type-2虚拟机里，我们上面说的虚拟机监视器好像一个运行在操作系统上的软件。你的客户机的操作系统呢，把最终到硬件的所有指令，都发送给虚拟机监视器。而虚拟机监视器，又会把这些指令再交给宿主机的操作系统去执行。
那这时候你就会问了，这和上面的模拟器看起来没有那么大分别啊？看起来，我们只是把在模拟器里的指令翻译工作，挪到了虚拟机监视器里。没错，Type-2型的虚拟机，更多是用在我们日常的个人电脑里，而不是用在数据中心里。
在数据中心里面用的虚拟机，我们通常叫作Type-1型的虚拟机。这个时候，客户机的指令交给虚拟机监视器之后呢，不再需要通过宿主机的操作系统，才能调用硬件，而是可以直接由虚拟机监视器去调用硬件。
另外，在数据中心里面，我们并不需要在Intel x86上面去跑一个ARM的程序，而是直接在x86上虚拟一个x86硬件的计算机和操作系统。所以，我们的指令不需要做什么翻译工作，可以直接往下传递执行就好了，所以指令的执行效率也会很高。
所以，在Type-1型的虚拟机里，我们的虚拟机监视器其实并不是一个操作系统之上的应用层程序，而是一个嵌入在操作系统内核里面的一部分。无论是KVM、XEN还是微软自家的Hyper-V，其实都是系统级的程序。
因为虚拟机监视器需要直接和硬件打交道，所以它也需要包含能够直接操作硬件的驱动程序。所以Type-1的虚拟机监视器更大一些，同时兼容性也不能像Type-2型那么好。不过，因为它一般都是部署在我们的数据中心里面，硬件完全是统一可控的，这倒不是一个问题了。
Docker：新时代的最佳选择？虽然，Type-1型的虚拟机看起来已经没有什么硬件损耗。但是，这里面还是有一个浪费的资源。在我们实际的物理机上，我们可能同时运行了多个的虚拟机，而这每一个虚拟机，都运行了一个属于自己的单独的操作系统。
多运行一个操作系统，意味着我们要多消耗一些资源在CPU、内存乃至磁盘空间上。那我们能不能不要多运行的这个操作系统呢？
其实是可以的。因为我们想要的未必是一个完整的、独立的、全虚拟化的虚拟机。我们很多时候想要租用的不是“独立服务器”，而是独立的计算资源。在服务器领域，我们开发的程序都是跑在Linux上的。其实我们并不需要一个独立的操作系统，只要一个能够进行资源和环境隔离的“独立空间”就好了。那么，能够满足这个需求的解决方案，就是过去几年特别火热的Docker技术。使用Docker来搭建微服务，可以说是过去两年大型互联网公司的必经之路了。
在实践的服务器端的开发中，虽然我们的应用环境需要各种各样不同的依赖，可能是不同的PHP或者Python的版本，可能是操作系统里面不同的系统库，但是通常来说，我们其实都是跑在Linux内核上的。通过Docker，我们不再需要在操作系统上再跑一个操作系统，而只需要通过容器编排工具，比如Kubernetes或者Docker Swarm，能够进行各个应用之间的环境和资源隔离就好了。
这种隔离资源的方式呢，也有人称之为“操作系统级虚拟机”，好和上面的全虚拟化虚拟机对应起来。不过严格来说，Docker并不能算是一种虚拟机技术，而只能算是一种资源隔离的技术而已。
总结延伸这一讲，我从最古老的分时系统讲起，介绍了虚拟机的相关技术。我们现在的云服务平台上，你能够租到的服务器其实都是虚拟机，而不是物理机。而正是虚拟机技术的出现，使得整个云服务生态得以出现。
虚拟机是模拟一个计算机系统的技术，而其中最简单的办法叫模拟器。我们日常在PC上进行Android开发，其实就是在使用这样的模拟器技术。不过模拟器技术在性能上实在不行，所以我们才有了虚拟化这样的技术。
在宿主机的操作系统上，运行一个虚拟机监视器，然后再在虚拟机监视器上运行客户机的操作系统，这就是现代的虚拟化技术。这里的虚拟化技术可以分成Type-1和Type-2这两种类型。
Type-1类型的虚拟化机，实际的指令不需要再通过宿主机的操作系统，而可以直接通过虚拟机监视器访问硬件，所以性能比Type-2要好。而Type-2类型的虚拟机，所有的指令需要经历客户机操作系统、虚拟机监视器、宿主机操作系统，所以性能上要慢上不少。不过因为经历了宿主机操作系统的一次“翻译”过程，它的硬件兼容性往往会更好一些。
今天，即使是Type-1型的虚拟机技术，我们也会觉得有一些性能浪费。我们常常在同一个物理机上，跑上8个、10个的虚拟机。而且这些虚拟机的操作系统，其实都是同一个Linux Kernel的版本。于是，轻量级的Docker技术就进入了我们的视野。Docker也被很多人称之为“操作系统级”的虚拟机技术。不过Docker并没有再单独运行一个客户机的操作系统，而是直接运行在宿主机操作系统的内核之上。所以，Docker也是现在流行的微服务架构底层的基础设施。
推荐阅读又到了阅读英文文章的时间了。想要更多了解虚拟机、Docker这些相关技术的概念和知识，特别是进一步理解Docker的细节，你可以去读一读FreeCodeCamp里的A Beginner-Friendly Introduction to Containers, VMs and Docker这篇文章。
课后思考我们在程序开发过程中，除了会用今天讲到的系统级虚拟机之外，还会常常遇到Java虚拟机这样的进程级虚拟机。那么，JVM这个进程级虚拟机是为了解决什么问题而出现的呢？今天我们讲到的系统级虚拟机发展历程中的各种优化手段，有哪些是JVM中也可以通用的呢？
欢迎留言和我分享你的疑惑和见解。如果有收获，你也可以把今天的文章分享给你朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>34_运行时优化：即时编译的原理和作用</title><link>https://artisanbox.github.io/6/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/34/</guid><description>前面所讲的编译过程，都存在一个明确的编译期，编译成可执行文件后，再执行，这种编译方式叫做提前编译（AOT）。 与之对应的，另一个编译方式是即时编译（JIT），也就是，在需要运行某段代码的时候，再去编译。其实，Java、JavaScript等语言，都是通过即时编译来提高性能的。
那么问题来了：
什么时候用AOT，什么时候用JIT呢？ 在讲运行期原理时，我提到程序编译后，会生成二进制的可执行文件，加载到内存以后，目标代码会放到代码区，然后开始执行。那么即时编译时，对应的过程是什么？目标代码会存放到哪里呢？ 在什么情况下，我们可以利用即时编译技术，获得运行时的优化效果，又该如何实现呢？ 本节课，我会带你掌握，即时编译技术的特点，和它的实现机理，并通过一个实际的案例，探讨如何充分利用即时编译技术，让系统获得更好的优化。这样一来，你对即时编译技术的理解会更透彻，也会更清楚怎样利用即时编译技术，来优化自己的软件。
首先，来了解一下，即时编译的特点和原理。
了解即时编译的特点及原理根据计算机程序运行的机制，我们把，不需要编译成机器码的执行方式，叫做解释执行。解释执行，通常都会基于虚拟机来实现，比如，基于栈的虚拟机，和基于寄存器的虚拟机（在32讲中，我带你了解过）。
与解释执行对应的，是编译成目标代码，直接在CPU上运行。而根据编译时机的不同，又分为AOT和JIT。那么，JIT的特点，和使用场景是什么呢？
一般来说，一个稍微大点儿的程序，静态编译一次，花费的时间很长，而这个等待时间是很煎熬的。如果采用JIT机制，你的程序就可以像，解释执行的程序一样，马上运行，得到反馈结果。
其次，JIT能保证代码的可移植性。在某些场景下，我们没法提前知道，程序运行的目标机器，所以，也就没有办法提前编译。Java语言，先编译成字节码，到了具体运行的平台上，再即时编译成，目标代码来运行，这种机制，使Java程序具有很好的可移植性。
再比如，很多程序会用到GPU的功能，加速图像的渲染，但针对不同的GPU，需要编译成不同的目标代码，这里也会用到即时编译功能。
最后，JIT是编译成机器码的，在这个过程中，可以进行深度的优化，因此程序的性能要比解释执行高很多。
这样看来，JIT非常有优势。
而从实际应用来看，原来一些解释执行的语言，后来也采用JIT技术，优化其运行机制，在保留即时运行、可移植的优点的同时，又提升了运行效率，JavaScript就是其中的典型代表。基于谷歌推出的V8开源项目，JavaScript的性能获得了极大的提升，使基于Web的前端应用的体验，越来越好，这其中就有JIT的功劳。
而且据我了解，R语言也加入了JIT功能，从而提升了性能；Python的数据计算模块numba也采用了JIT。
在了解JIT的特点，和使用场景之后，你有必要对JIT和AOT在技术上的不同之处有所了解，以便掌握JIT的技术原理。
静态编译的时候，首先要把程序，编译成二进制目标文件，再链接形成可执行文件，然后加载到内存中运行。JIT也需要编译成二进制的目标代码，但是目标代码的加载和链接过程，就不太一样了。
首先说说目标代码的加载。
在静态编译的情况下，应用程序会被操作系统加载，目标代码被放到了代码区。从安全角度出发，操作系统给每个内存区域，设置了不同的权限，代码区具备可执行权限，所以我们才能运行程序。
在JIT的情况下，我们需要为这种动态生成的目标代码，申请内存，并给内存设置可执行权限。我写个实际的C语言程序，让你直观地理解一下这个过程。
我们在一个数组里，存一段小小的机器码，只有9个字节。这段代码的功能，相当于一个C语言函数的功能，也就是把输入的参数加上2，并返回。
/* * 机器码，对应下面函数的功能： * int foo(int a){ * return a + 2; * } */ uint8_t machine_code[] = { 0x55, 0x48, 0x89, 0xe5, 0x8d, 0x47, 0x02, 0x5d, 0xc3 }; 你可能问了：你怎么知道这个函数，对应的机器码是这9个字节呢？
这不难，你把foo.c编译成目标文件，然后查看里面的机器码就行了。
clang -c -O2 foo.c -o foo.o 或者 gcc -c -O2 foo.c -o foo.o objdump -d foo.</description></item><item><title>34｜内存管理第1关：Arena技术和元数据</title><link>https://artisanbox.github.io/3/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/36/</guid><description>你好，我是宫文学。
通过前面8节课的学习，我们实现了对浮点数、字符串、数组、自定义对象类型和函数类型的支持，涵盖了TypeScript的一些关键数据类型，也了解了实现这些语言特性所需要的一些关键技术。
在这些数据类型中，字符串、数组、class实例，还有闭包，都需要从堆中申请内存，但我们目前还没有实现内存回收机制。所以，如果用我们现在的版本，长时间运行某些需要在堆中申请内存的程序，可能很快会就把内存耗光。
所以，接下来的两节课，我们就来补上这个缺陷，实现一个简单的内存管理模块，支持内存的申请、内存垃圾的识别和回收功能。在这个过程中，你会对内存管理的原理产生更加清晰的认识，并且能够自己动手实现基本的内存管理功能。
那么，首先我们要分析一下内存管理涉及的技术点，以此来确定我们自己的技术方案。
内存管理中的技术点计算机语言中的内存管理模块，能够对内存从申请到回收进行全生命周期的管理。
内存的申请方面，一般不会为每个对象从操作系统申请内存资源，而是要提供自己的内存分配机制。
而垃圾回收技术则是内存管理中的难点。垃圾回收有很多个技术方案，包括标记-清除、标记-整理、停止-拷贝和自动引用计数这些基础的算法。在产品级的实现里，这些算法又被进一步复杂化。比如，你可以针对老的内存对象和新内存对象，使用不同的回收算法，从而形成分代管理的方案。又比如，为了充分减少由于垃圾收集所导致的程序停顿，发展出来了增量式回收和并行回收的技术。
关于这些算法的介绍，你可以参考《编译原理之美》的33节，里面介绍了各种垃圾收集算法。还有《编译原理实战课》的第32节，里面分析了Python、Java、JavaScript、Julia、Go、Swift、Objective-C等各种语言采用的内存管理技术的特点，也讨论了这些技术与语言特性的关系。在这节课里，我就不重复介绍这些内容了。
垃圾收集对语言运行的影响是很大的，因此我们希望垃圾回收导致的程序停顿越短越好，消耗的系统资源越少越好。这些苛刻的要求，导致在很多现代语言中，垃圾回收器（GC）成了运行时中技术挑战很高的一个模块。不过，再难的技术都是一口口吃下的。在这节课里，我们先不去挑战那些特别复杂的算法，而是选择一个最容易上手的、入门级的算法，标记-清除算法来做示范。
标记-清除算法的思路比较简单，只需要简单两步：
首先，我们要找出哪些内存对象不是垃圾，并进行标记； 第二，回收掉所有没做标记的对象，也就是垃圾对象。 我们通过一个例子来看一下。在下图中，x和y变量分别指向了两个内存对象，这两个内存对象可能是自定义类的实例，也有可能是闭包、字符串或数组。这些对象中的字段，又可能会引用另外的对象。
在图中，当变量x失效以后，它直接引用和间接引用的对象就会成为内存垃圾，你就可以回收掉它了。这就是标记-清除算法的原理，非常简单。
在这个图里，变量x和y叫做GC的根（GC root）。算法需要从这些根节点出发，去遍历它直接或间接引用的对象。这个过程，实际上就是图的遍历算法。
好了，算法上大的原理我们就搞清楚了。那接下来，我们需要讨论一些实现上的技术点，包括如何管理内存的申请和释放、如何遍历所有的栈帧和内存对象，等等。
首先说一下如何管理内存的申请和释放。
内存的申请和释放在我们前面实现的、C语言版本的字节码虚拟机中，我们就曾经讨论过如何高效申请内存的问题。我们发现，如果调用操作系统的接口频繁地申请和释放小的内存块，会大大降低系统的整体性能。所以，我们采用了Arena技术，也就是一次性地从操作系统中申请比较大块的内存，然后再自行把这块大内存划分成小块的内存，给自己的语言使用。
在今天这节课，我们仍然使用Arena技术来管理内存：当我们创建新的内存对象的时候，就从Arena中找一块未被占用的内容空间；而在回收内存对象的时候，就把内存对象占的内存区域标记成自由空间。
在这里你会发现，为了记住哪些内存是被分配出去的，那些内存是可用的，我们需要一个数据结构来保存这些信息。在我的参考实现里，我用了一个简单的链表来保存这些信息。每块被分配出去的内存，都是链表的一个节点。节点里保存了当前内存对象的大小，以及下一个节点的地址。
顺着这个链表，你可以查找出自由的内存。假设节点1的地址是80，对象大小是48字节，节点2地址是180，那么节点1和2之间就有52个字节的自由空间。
当我们要申请内存的时候，如果我们要申请的对象大小低于52个字节，那就可以把这块空间分配给它。这个时候，我们就要修改链表的指针，把新的节点插入到节点1和节点2之间。
如果要回收内存呢？也比较简单，我们就从链表中去掉这个节点就好了。
了解了内存申请和释放的内容后，接下来，我们就需要查找并标记哪些内存是仍然被使用的，从而识别出内存垃圾。这就需要程序遍历所有栈帧中的GC根引用的对象，以及这些对象引用的其他对象。而要完成这样的遍历，我们需要知道函数、类和闭包等的元数据信息才可以。
管理元数据我们前面说过，GC根就是那些引用了内存对象的变量。而我们知道，我们的程序中用到的变量，有可能是在栈中的，也有可能是在寄存器里的。那到底栈里的哪个位置是变量，哪个寄存器是变量呢？另外，如何遍历所有的栈帧呢？如何知道每个栈帧的开头和结尾位置？又如何知道哪个栈帧是第一个栈帧，从而结束遍历呢？这些都是需要解决的技术问题，我们一个一个来看。
首先，我们要确定栈帧和寄存器里，哪些是变量，也就是GC根。
这就需要我们保存变量在栈帧中的布局信息。对于每个函数来说，这些布局信息都是唯一的。这些信息可以看做是函数的元数据的一部分。其他元数据信息包括函数的名称，等等。
我们用一个例子来分析一下变量布局情况。下面的foo函数的栈帧里，包括几个本地变量和几个临时变量。基于我们的寄存器分配算法，这些变量有些会被Spill到栈帧中。比如，如果某个变量使用的寄存器是需要Caller保护的，那么在调用另一个函数的时候，这些变量就会被Spill到内存中。
function foo(b:number):number{ let a:number[] = [1,2,b]; let s:string = "Hello PlayScript!"; println(s); println(a[2]); return b*10; } println(foo(2)); 另外，如果一个函数用到了需要Callee保护的寄存器，那么这些寄存器的信息也会被写入到栈帧，这些寄存器的值也可能是调用者的某个变量。算法可以查询调用者的变量布局信息来确认这一点。
最终，对于foo函数来说，这些变量在栈帧中的布局如下：
那包含了变量布局的元数据信息，应该保存到哪里呢？你可能已经想到了，它们可以被保存在可执行文件的数据区呀，就像之前我们保存vtable那样。
在具体实现的时候，这个数据区可以分成多个组成部分。像vtable这样的数据，出于性能上的要求，我们最好能够比较快捷地访问，所以我们让程序通过“1跳”，也就是只做一次获取地址的操作，就能到查到方法的入口地址。而对于其他元数据信息，由于数据类型跟vtable的不一样，可以安排到另一个数据区中，并从第一个数据区链接过去。元数据在静态数据区的布局如下图所示：
它们在汇编代码中可以写成下面的样子：
.section __DATA,__const .globl _foo.meta ## can be accessed globally .p2align 3 ## 8 byte alignment _foo.meta: .</description></item><item><title>35_join语句怎么优化？</title><link>https://artisanbox.github.io/1/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/35/</guid><description>在上一篇文章中，我和你介绍了join语句的两种算法，分别是Index Nested-Loop Join(NLJ)和Block Nested-Loop Join(BNL)。
我们发现在使用NLJ算法的时候，其实效果还是不错的，比通过应用层拆分成多个语句然后再拼接查询结果更方便，而且性能也不会差。
但是，BNL算法在大表join的时候性能就差多了，比较次数等于两个表参与join的行数的乘积，很消耗CPU资源。
当然了，这两个算法都还有继续优化的空间，我们今天就来聊聊这个话题。
为了便于分析，我还是创建两个表t1、t2来和你展开今天的问题。
create table t1(id int primary key, a int, b int, index(a)); create table t2 like t1; drop procedure idata; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=1000)do insert into t1 values(i, 1001-i, i); set i=i+1; end while; set i=1; while(i&amp;lt;=1000000)do insert into t2 values(i, i, i); set i=i+1; end while;
end;; delimiter ; call idata(); 为了便于后面量化说明，我在表t1里，插入了1000行数据，每一行的a=1001-id的值。也就是说，表t1中字段a是逆序的。同时，我在表t2中插入了100万行数据。</description></item><item><title>35_Trie树：如何实现搜索引擎的搜索关键词提示功能？</title><link>https://artisanbox.github.io/2/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/36/</guid><description>搜索引擎的搜索关键词提示功能，我想你应该不陌生吧？为了方便快速输入，当你在搜索引擎的搜索框中，输入要搜索的文字的某一部分的时候，搜索引擎就会自动弹出下拉框，里面是各种关键词提示。你可以直接从下拉框中选择你要搜索的东西，而不用把所有内容都输入进去，一定程度上节省了我们的搜索时间。
尽管这个功能我们几乎天天在用，作为一名工程师，你是否思考过，它是怎么实现的呢？它底层使用的是哪种数据结构和算法呢？
像Google、百度这样的搜索引擎，它们的关键词提示功能非常全面和精准，肯定做了很多优化，但万变不离其宗，底层最基本的原理就是今天要讲的这种数据结构：Trie树。
什么是“Trie树”？Trie树，也叫“字典树”。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。
当然，这样一个问题可以有多种解决方法，比如散列表、红黑树，或者我们前面几节讲到的一些字符串匹配算法，但是，Trie树在这个问题的解决上，有它特有的优点。不仅如此，Trie树能解决的问题也不限于此，我们一会儿慢慢分析。
现在，我们先来看下，Trie树到底长什么样子。
我举个简单的例子来说明一下。我们有6个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这6个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？
这个时候，我们就可以先对这6个字符串做一下预处理，组织成Trie树的结构，之后每次查找，都是在Trie树中进行匹配查找。Trie树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。最后构造出来的就是下面这个图中的样子。
其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）。
为了让你更容易理解Trie树是怎么构造出来的，我画了一个Trie树构造的分解过程。构造过程的每一步，都相当于往Trie树中插入一个字符串。当所有字符串都插入完成之后，Trie树就构造好了。
当我们在Trie树中查找一个字符串的时候，比如查找字符串“her”，那我们将要查找的字符串分割成单个的字符h，e，r，然后从Trie树的根节点开始匹配。如图所示，绿色的路径就是在Trie树中匹配的路径。
如果我们要查找的是字符串“he”呢？我们还用上面同样的方法，从根节点开始，沿着某条路径来匹配，如图所示，绿色的路径，是字符串“he”匹配的路径。但是，路径的最后一个节点“e”并不是红色的。也就是说，“he”是某个字符串的前缀子串，但并不能完全匹配任何字符串。
如何实现一棵Trie树？知道了Trie树长什么样子，我们现在来看下，如何用代码来实现一个Trie树。
从刚刚Trie树的介绍来看，Trie树主要有两个操作，一个是将字符串集合构造成Trie树。这个过程分解开来的话，就是一个将字符串插入到Trie树的过程。另一个是在Trie树中查询一个字符串。
了解了Trie树的两个主要操作之后，我们再来看下，如何存储一个Trie树？
从前面的图中，我们可以看出，Trie树是一个多叉树。我们知道，二叉树中，一个节点的左右子节点是通过两个指针来存储的，如下所示Java代码。那对于多叉树来说，我们怎么存储一个节点的所有子节点的指针呢？
class BinaryTreeNode { char data; BinaryTreeNode left; BinaryTreeNode right; } 我先介绍其中一种存储方式，也是经典的存储方式，大部分数据结构和算法书籍中都是这么讲的。还记得我们前面讲到的散列表吗？借助散列表的思想，我们通过一个下标与字符一一映射的数组，来存储子节点的指针。这句话稍微有点抽象，不怎么好懂，我画了一张图你可以看看。
假设我们的字符串中只有从a到z这26个小写字母，我们在数组中下标为0的位置，存储指向子节点a的指针，下标为1的位置存储指向子节点b的指针，以此类推，下标为25的位置，存储的是指向的子节点z的指针。如果某个字符的子节点不存在，我们就在对应的下标的位置存储null。
class TrieNode { char data; TrieNode children[26]; } 当我们在Trie树中查找字符串的时候，我们就可以通过字符的ASCII码减去“a”的ASCII码，迅速找到匹配的子节点的指针。比如，d的ASCII码减去a的ASCII码就是3，那子节点d的指针就存储在数组中下标为3的位置中。
描述了这么多，有可能你还是有点懵，我把上面的描述翻译成了代码，你可以结合着一块看下，应该有助于你理解。
public class Trie { private TrieNode root = new TrieNode('/'); // 存储无意义字符 // 往Trie树中插入一个字符串 public void insert(char[] text) { TrieNode p = root; for (int i = 0; i &amp;lt; text.length; ++i) { int index = text[i] - &amp;lsquo;a&amp;rsquo;; if (p.</description></item><item><title>35_存储器层次结构全景：数据存储的大金字塔长什么样？</title><link>https://artisanbox.github.io/4/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/35/</guid><description>今天开始，我们要进入到计算机另一个重要的组成部分，存储器。
如果你自己组装过PC机，你肯定知道，想要CPU，我们只要买一个就好了，但是存储器，却有不同的设备要买。比方说，我们要买内存，还要买硬盘。买硬盘的时候，不少人会买一块SSD硬盘作为系统盘，还会买上一块大容量的HDD机械硬盘作为数据盘。内存和硬盘都是我们的存储设备。而且，像硬盘这样的持久化存储设备，同时也是一个I/O设备。
在实际的软件开发过程中，我们常常会遇到服务端的请求响应时间长，吞吐率不够的情况。在分析对应问题的时候，相信你没少听过类似“主要瓶颈不在CPU，而在I/O”的论断。可见，存储在计算机中扮演着多么重要的角色。那接下来这一整个章节，我会为你梳理和讲解整个存储器系统。
这一讲，我们先从存储器的层次结构说起，让你对各种存储器设备有一个整体的了解。
理解存储器的层次结构在有计算机之前，我们通常把信息和数据存储在书、文件这样的物理介质里面。有了计算机之后，我们通常把数据存储在计算机的存储器里面。而存储器系统是一个通过各种不同的方法和设备，一层一层组合起来的系统。下面，我们把计算机的存储器层次结构和我们日常生活里处理信息、阅读书籍做个对照，好让你更容易理解、记忆存储器的层次结构。
我们常常把CPU比喻成计算机的“大脑”。我们思考的东西，就好比CPU中的寄存器（Register）。寄存器与其说是存储器，其实它更像是CPU本身的一部分，只能存放极其有限的信息，但是速度非常快，和CPU同步。
而我们大脑中的记忆，就好比CPU Cache（CPU高速缓存，我们常常简称为“缓存”）。CPU Cache用的是一种叫作SRAM（Static Random-Access Memory，静态随机存取存储器）的芯片。
SRAMSRAM之所以被称为“静态”存储器，是因为只要处在通电状态，里面的数据就可以保持存在。而一旦断电，里面的数据就会丢失了。在SRAM里面，一个比特的数据，需要6～8个晶体管。所以SRAM的存储密度不高。同样的物理空间下，能够存储的数据有限。不过，因为SRAM的电路简单，所以访问速度非常快。
图片来源 6个晶体管组成SRAM的一个比特在CPU里，通常会有L1、L2、L3这样三层高速缓存。每个CPU核心都有一块属于自己的L1高速缓存，通常分成指令缓存和数据缓存，分开存放CPU使用的指令和数据。
不知道你还记不记得我们在第22讲讲过的哈佛架构，这里的指令缓存和数据缓存，其实就是来自于哈佛架构。L1的Cache往往就嵌在CPU核心的内部。
L2的Cache同样是每个CPU核心都有的，不过它往往不在CPU核心的内部。所以，L2 Cache的访问速度会比L1稍微慢一些。而L3 Cache，则通常是多个CPU核心共用的，尺寸会更大一些，访问速度自然也就更慢一些。
你可以把CPU中的L1 Cache理解为我们的短期记忆，把L2/L3 Cache理解成长期记忆，把内存当成我们拥有的书架或者书桌。 当我们自己记忆中没有资料的时候，可以从书桌或者书架上拿书来翻阅。这个过程中就相当于，数据从内存中加载到CPU的寄存器和Cache中，然后通过“大脑”，也就是CPU，进行处理和运算。
DRAM内存用的芯片和Cache有所不同，它用的是一种叫作DRAM（Dynamic Random Access Memory，动态随机存取存储器）的芯片，比起SRAM来说，它的密度更高，有更大的容量，而且它也比SRAM芯片便宜不少。
DRAM被称为“动态”存储器，是因为DRAM需要靠不断地“刷新”，才能保持数据被存储起来。DRAM的一个比特，只需要一个晶体管和一个电容就能存储。所以，DRAM在同样的物理空间下，能够存储的数据也就更多，也就是存储的“密度”更大。但是，因为数据是存储在电容里的，电容会不断漏电，所以需要定时刷新充电，才能保持数据不丢失。DRAM的数据访问电路和刷新电路都比SRAM更复杂，所以访问延时也就更长。
存储器的层级结构整个存储器的层次结构，其实都类似于SRAM和DRAM在性能和价格上的差异。SRAM更贵，速度更快。DRAM更便宜，容量更大。SRAM好像我们的大脑中的记忆，而DRAM就好像属于我们自己的书桌。
大脑（CPU）中的记忆（L1 Cache），不仅受成本层面的限制，更受物理层面的限制。这就好比L1 Cache不仅昂贵，其访问速度和它到CPU的物理距离有关。芯片造得越大，总有部分离CPU的距离会变远。电信号的传输速度又受物理原理的限制，没法超过光速。所以想要快，并不是靠多花钱就能解决的。
我们自己的书房和书桌（也就是内存）空间一般是有限的，没有办法放下所有书（也就是数据）。如果想要扩大空间的话，就相当于要多买几平方米的房子，成本就会很高。于是，想要放下更多的书，我们就要寻找更加廉价的解决方案。
没错，我们想到了公共图书馆。对于内存来说，SSD（Solid-state drive或Solid-state disk，固态硬盘）、HDD（Hard Disk Drive，硬盘）这些被称为硬盘的外部存储设备，就是公共图书馆。于是，我们就可以去家附近的图书馆借书了。图书馆有更多的空间（存储空间）和更多的书（数据）。
你应该也在自己的个人电脑上用过SSD硬盘。过去几年，SSD这种基于NAND芯片的高速硬盘，价格已经大幅度下降。
而HDD硬盘则是一种完全符合“磁盘”这个名字的传统硬件。“磁盘”的硬件结构，决定了它的访问速度受限于它的物理结构，是最慢的。
这些我们后面都会详细说，你可以对照下面这幅图了解一下，对存储器层次之间的作用和关联有个大致印象就可以了。
存储器的层次关系图从Cache、内存，到SSD和HDD硬盘，一台现代计算机中，就用上了所有这些存储器设备。其中，容量越小的设备速度越快，而且，CPU并不是直接和每一种存储器设备打交道，而是每一种存储器设备，只和它相邻的存储设备打交道。比如，CPU Cache是从内存里加载而来的，或者需要写回内存，并不会直接写回数据到硬盘，也不会直接从硬盘加载数据到CPU Cache中，而是先加载到内存，再从内存加载到Cache中。
这样，各个存储器只和相邻的一层存储器打交道，并且随着一层层向下，存储器的容量逐层增大，访问速度逐层变慢，而单位存储成本也逐层下降，也就构成了我们日常所说的存储器层次结构。
使用存储器的时候，该如何权衡价格和性能？存储器在不同层级之间的性能差异和价格差异，都至少在一个数量级以上。L1 Cache的访问延时是1纳秒（ns），而内存就已经是100纳秒了。在价格上，这两者也差出了400倍。
我这里放了一张各种存储器成本的对比表格，你可以看看。你也可以在点击这个链接，通过拖拉，查看1990～2020年随着硬件设备的进展，访问延时的变化。
因为这个价格和性能的差异，你会看到，我们实际在进行电脑硬件配置的时候，会去组合配置各种存储设备。
我们可以找一台现在主流的笔记本电脑来看看，比如，一款入门级的惠普战66的笔记本电脑。今天在京东上的价格是4999人民币。它的配置是下面这样的。
Intle i5-8265U的CPU（这是一块4核的CPU） 这块CPU每个核有32KB，一共128KB的L1指令Cache。 同样，每个核还有32KB，一共128KB的L1数据Cache，指令Cache和数据Cache都是采用8路组相连的放置策略。 每个核有256KB，一共1MB的L2 Cache。L2 Cache是用4路组相连的放置策略。 最后还有一块多个核心共用的12MB的L3 Cache，采用的是12路组相连的放置策略。 8GB的内存 一块128G的SSD硬盘 一块1T的HDD硬盘 你可以看到，在一台实际的计算机里面，越是速度快的设备，容量就越小。这里一共十多兆的Cache，成本只是几十美元。而8GB的内存、128G的SSD以及1T的HDD，大概零售价格加在一起，也就和我们的高速缓存的价格差不多。
总结延伸这节的内容不知道你掌握了多少呢？为了帮助你记忆，我这里再带你复习一下本节的重点。
我们常常把CPU比喻成高速运转的大脑，那么和大脑同步的寄存器（Register），就存放着我们当下正在思考和处理的数据。而L1-L3的CPU Cache，好比存放在我们大脑中的短期到长期的记忆。我们需要小小花费一点时间，就能调取并进行处理。
我们自己的书桌书架就好比计算机的内存，能放下更多的书也就是数据，但是找起来和看起来就要慢上不少。而图书馆更像硬盘这个外存，能够放下更多的数据，找起来也更费时间。从寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，速度越来越慢，空间越来越大，价格也越来越便宜。</description></item><item><title>35_并发中的编译技术（三）：Erlang语言厉害在哪里？</title><link>https://artisanbox.github.io/7/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/35/</guid><description>你好，我是宫文学。
在前面两讲，我们讨论了各门语言支持的并发计算的模型。线程比进程更加轻量级，上下文切换成本更低；协程则比线程更加轻量级，在一台计算机中可以轻易启动几十万、上百万个并发任务。
但不论是线程模型、还是协程模型，当涉及到多个线程访问共享数据的时候，都会出现竞争问题，从而需要用到锁。锁会让其他需要访问该数据的线程等待，从而导致系统整体处理能力的降低。
并且，编程人员还要特别注意，避免出现死锁。比如，线程A持有了锁x，并且想要获得锁y；而线程B持有了锁y，想要获得锁x，结果这两个线程就会互相等待，谁也进行不下去。像数据库这样的系统，检测和消除死锁是一项重要的功能，以防止互相等待的线程越来越多，对数据库操作不响应，并最终崩溃掉。
既然使用锁这么麻烦，那在并发计算中，能否不使用锁呢？这就出现了Actor模型。那么，什么是Actor模型？为什么它可以不用锁就实现并发？这个并发模型有什么特点？需要编译技术做什么配合？
今天这一讲，我们就从这几个问题出发，一起学习并理解Actor模型。借此，我们也可以把用编译技术支持不同的并发模型的机制，理解得更深刻。
首先，我们看一下什么是Actor模型。
什么是Actor模型？在线程和协程模型中，之所以用到锁，是因为两个线程共享了内存，而它们会去修改同一个变量的值。那，如果避免共享内存，是不是就可以消除这个问题了呢？
没错，这就是Actor模型的特点。Actor模型是1973年由Carl Hewitt提出的。在Actor模型中，并发的程序之间是不共享内存的。它们通过互相发消息来实现协作，很多个一起协作的Actor就构成了一个支持并发计算的系统。
我们看一个有三个Actor的例子。
图1：三个Actor的例子你会注意到，每个Actor都有一个邮箱，用来接收其他Actor发来的消息；每个Actor也都可以给其他Actor发送消息。这就是Actor之间交互的方式。Actor A给Actor B发完消息后就返回，并不会等着Actor B处理完毕，所以它们之间的交互是异步的。如果Actor B要把结果返回给A，也是通过发送消息的方式。
这就是Actor大致的工作原理了。因为Actor之间只是互发消息，没有共享的变量，当然也就不需要用到锁了。
但是，你可能会问：如果不共享内存，能解决传统上需要对资源做竞争性访问的需求吗？比如，卖电影票、卖火车票、秒杀或者转账的场景。我们以卖电影票为例讲解一下。
在用传统的线程或者协程来实现卖电影票功能的时候，对票的状态进行修改，需要用锁的机制实现同步互斥，以保证同一个时间段只有一个线程可以去修改票的状态、把它分配给某个用户，从而避免多个线程同时访问而出现一张票卖给多个人的情况。这种情况下，多个程序是串行执行的，所以系统的性能就很差。
如果用Actor模式会怎样呢？
你可以把电影院的前半个场地和后半个场地的票分别由Actor B和 C负责销售：Actor A在接收到定前半场座位的请求的时候，就发送给Actor B，后半场的就发送给Actor C，Actor B和C依次处理这些请求；如果Actor B或C接收到的两个信息都想要某个座位，那么针对第二个请求会返回订票失败的消息。
图2：Actor用于订票场景你发现没有？在这个场景中，Actor B和C仍然是顺序处理各个请求。但因为是两个Actor并发地处理请求，所以系统整体的性能会提升到原来的两倍。
甚至，你可以让每排座位、每个座位都由一个Actor负责，使得系统的性能更高。因为在系统中创建一个Actor的成本是很低的。Actor跟协程类似，很轻量级，一台服务器里创建几十万、上百万个Actor也没有问题。如果每个Actor负责一个座位，那一台服务器也能负责几十万、上百万个座位的销售，也是可以接受的。
当然，实际的场景要比这个复杂，比如一次购买多张相邻的票等，但原理是一样的。用这种架构，可以大大提高并发能力，处理海量订票、秒杀等场景不在话下。
其实，我个人比较喜欢Actor这种模式，因为它跟现实世界里的分工协作很相似。比如，餐厅里不同岗位的员工，他们通过互相发信息来实现协作，从而并发地服务很多就餐的顾客。
分析到这里，我再把Actor模式跟你非常熟悉的一个概念，面向对象编程（Object Oriented Programming，OOP）关联起来。你可能会问：Actor和面向对象怎么还有关联？
是的。面向对象语言之父阿伦 · 凯伊（Alan Kay），Smalltalk的发明人，在谈到面向对象时是这样说的：对象应该像生物的细胞，或者是网络上的计算机，它们只能通过消息互相通讯。对我来说OOP仅仅意味着消息传递、本地保留和保护以及隐藏状态过程，并且尽量推迟万物之间的绑定关系。
I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages (so messaging came at the very beginning – it took a while to see how to do messaging in a programming language efficiently enough to be useful)</description></item><item><title>35_案例总结与热点问题答疑：后端部分真的比前端部分难吗？</title><link>https://artisanbox.github.io/6/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/35/</guid><description>本节课，我会继续剖析一些，你们提出的，有代表性的问题（以后端问题为主），主要包括以下几个方面：
后端技术部分真的比前端技术部分难吗？ 怎样更好地理解栈和栈桢（有几个同学提出的问题很好，有必要在这里探究一下）？这样，你对栈桢的理解会更加扎实。 有关数据流分析框架。数据流分析是后端技术的几个重点之一，需要再细化一下。 关于Java的两个知识点：泛型和反射。我会从编译技术的角度讲一讲。 接下来，进入第一个问题：后端技术真的难吗？正确的学习路径是什么？
后端技术真的难吗？该怎么学？有同学觉得，一进到后端，难度马上加大了，你是不是也有这样的感觉？我承认，前端部分和后端部分确实不太相同。
前端部分偏纯逻辑，你只要把算法琢磨透就行了。而后端部分，开始用到计算机组成原理的知识，要考虑CPU、寄存器、内存和指令集，甚至还要深入到CPU内部，去看它的流水线结构，以便理解指令排序。当然，我们还要说清楚与操作系统的关系，操作系统是如何加载代码并运行的，如何帮你管理内存等等。另外，还涉及ABI和调用约定，NP完全的算法等等。看上去复杂了很多。
虽然比较复杂，但我认为，这并不意味着后端更难，只意味着知识点更多。可这些知识，往往你熟悉了就不难了。
比如，@风同学见到了汇编代码说：总算遇到了自己熟悉的内容了，不用天天看Java代码了。
我觉得，从算法的角度出发，后端部分的算法，至少没比前端的语法分析算法难。而且有些知识点，别的课程里应该讲过，如果你从以下三个方面多多积累，会更容易掌握后端内容：
计算机组成原理：CPU的运行原理、汇编指令等等。 数据结构和算法，特别是与树和图有关的算法：如果你之前了解过，与图有关的算法，了解旅行商问题，那么会发现，指令选择等算法似曾相识。自然会理解，我提到某些算法是NP完全的，是什么意思。 操作系统：大部分情况下，程序是在操作系统中运行的，所以，要搞清楚我们编译的程序是如何跟操作系统互动的。 @沉淀的梦想就对这些内容，发表过感触：感觉学编译原理，真的能够帮助我们贯通整个计算机科学，涉及到的东西好多。
确实如他所说，那么我也希望《编译原理之美》这门课，能促使你去学习另外几门基础课，把基础夯实。
后端技术的另一个特点，是它比较偏工程性，不像前端部分有很强的理论性，对于每个问题有清晰的答案。而后端技术部分，往往对同一个问题有多种解决思路和算法，不一定有统一的答案，甚至算法和术语的名称都不统一。
后端技术的工程性特点，还体现在它会涉及很多技术细节，这些细节信息往往在教科书上是找不到的，必须去查厂商（比如Intel）的手册，有时要到社区里问，有时要看论文，甚至有时候要看源代码。
总的来说，如何学好后端，我的建议主要有三个方面：
学习关联的基础课程，比如《数据结构与算法》，互相印证； 理解编译原理工程性的特点，接受术语、算法等信息的不一致，并从多渠道获得前沿信息，比如源代码、厂商的手册等等。 注重实操，亲自动手。比如，你在学优化算法时，即使没时间写算法，也要尽可能用LLVM的算法做做实验。 按照上面三条建议，你应该可以充分掌握后端技术了。当然，如果你只是想做一个概要的了解，那么阅读文稿也会有不错的收获，因为我已经把主线梳理出来了，能避免你摸不着头脑，不知如何入手。
接下来，我们进入第二个问题：再次审视一下栈桢。
再次认识栈桢@刘强同学问：操作系统在栈的管理中到底起不起作用？
这是操作系统方面的知识点，但可以跟编译技术中栈的管理联系在一起看。
我们应用程序能够访问很大的地址空间，但操作系统不会慷慨地，一下子分配很多真实的物理内存。操作系统会把内存分成很多页，一页一页地按需分配给应用程序。那么什么时候分配呢？
当应用访问自己内存空间中的一个地址，但实际上没有对应的物理内存时，就会导致CPU产生一个PageFault（在Intel手册中可以查到），这是一种异常（Exception）。
对异常的处理跟中断的处理很相似，会调用注册好的一个操作系统的例程，在内核态运行，来处理这个异常。这时候，操作系统就会实际分配物理内存。之后，回到用户态，继续执行你的程序，比如，一个push指令等等。整个过程对应用程序是透明的，其实背后有CPU和操作系统的参与。
@风提出了关于栈桢的第二个问题：看到汇编代码里管理栈桢的时候，用了rbp和rsp两个寄存器。是不是有点儿浪费？一个寄存器就够了啊。
确实是这样，用这种写法是习惯形成的，其实可以省略。而我在34讲里，用到的那个foo函数，根本没有使用栈，仅仅用寄存器就完成了工作。这时，可以把下面三行指令全部省掉：
pushq %rbp movq %rsp, %rbp popq %rbp 从而让产生的机器码少5个字节。最重要的是，还省掉两次内存读写操作（相比对寄存器的操作，对内存的操作是很费时间的）。
实际上，如果你用GCC编译的话，可以使用-fomit-frame-pointer参数来优化，会产生同样的效果，也就是不再使用rbp。在访问栈中的地址时，会采用4(%rsp)、8(%rsp)的方式，在rsp的基础上加某个值，来访问内存。
@沉淀的梦想提出了第三个问题：栈顶（也就是rsp的值）为什么要16字节对齐？
这其实是一个调用约定。是在GCC发展的过程中，形成的一个事实上的标准。不过，它也有一些好处，比如内存对齐后，某些指令读取数据的速度会更快，这会让你产生一个清晰的印象，每次用到栈桢，至少要占16个字节，也就是4个32位的整数的空间。那么，如果把一些尾递归转化为循环来执行，确实会降低系统的开销，包括内存开销和保存前一个桢的bsp、返回地址、寄存器的运行时间开销。
而@不的问了第四个问题： 为什么要设计成区分调用者、被调用者保护的寄存器，统一由被调用者或者调用者保护，有什么问题么？
这个问题是关于保护寄存器的，我没有仔细去研究它的根源。不过我想，这种策略是最经济的。
如果全部都是调用者保护，那么你调用的对象不会破坏你的寄存器的话，你也要保护起来，那就增加了成本；如果全部都是被调用者保护，也是一样的逻辑。如果调用者用了很少几个寄存器，被调用者却要保护很多，也不划算。
所以最优的方法，其实是比较中庸主义的，两边各负责保护一部分，不过，我觉得这可以用概率的方法做比较严谨的证明。
关于栈桢，我最后再补充一点。有的教材用活动记录这个术语，有的教材叫做栈桢。你要知道这两个概念的联系和区别。活动记录是比较抽象的概念，它可以表现为多种实际的实现方式。在我们的课程中，栈桢加上函数调用中所使用的寄存器，就相当于一个活动记录。
讲完栈桢之后，再来说说与数据流分析框架有关的问题。
细化数据流分析框架数据流分析本身，理解起来并不难，就算不引入半格这个数学工具，你也完全可以理解。
对于数据流分析方法，不同的文献也有不同的描述，有的说是3个要素，有的说是4个要素。而我在文稿里说的是5个要素：方向（D）、值（V）、转换函数（F）、相遇运算（meet operation, Λ）和初始值（I）。你只要把这几个问题弄清楚，就可以了。
引入半格理论，主要是进一步规范相遇运算，这也是近些年研究界的一个倾向。用数学做形式化地描述虽然简洁清晰，但会不小心提升学习门槛。如果你只是为了写算法，完全可以不理半格理论，但如果为了方便看这方面算法的论文，了解半格理论会更好。
首先，半格是一种偏序集。偏序集里，某些元素是可以比较大小的。但怎么比较大小呢？其实，有时是人为定的，比如，{a, b}和{a, b, c}的大小，就是人为定的。
那么，既然能比较大小，就有上界（Upper Bound）和下界（Lower Bound）的说法。给定偏序集P的一个子集A，如果A中的每个元素a，都小于等于一个值x（x属于P），那么x就是A的一个上界。反过来，你也知道什么是下界。
半格是偏序集中，一种特殊的类型，它要求偏序集中，每个非空有限的子集，要么有最小上界（并半格，join-semilattice），要么有最大下界（交半格，meet-semilattice）。
其实，如果你把一个偏序集排序的含义反过来，它就会从交半格转换成并半格，或者并半格转换成交半格。我们还定义了两个特殊值：Top、Bottom。在不同的文献里，Top和Bottom有时刚好是反着的，那是因为排序的方向是反着的。
因为交半格和并半格是可以相互转化的，所以有的研究者采用的框架，就只用交半格。交半格中，集合{x, y}的最大下界，就记做x Λ y。在做活跃性分析的时候，我们就规定{a, b} &amp;gt; {a, b, c}就行了，这样就是个交半格。如果按照这个规矩，我在28讲中举的那个常数传播的例子，应该把大小反过来，也做成个交半格。文稿中的写法，实际是个并半格，不过也不影响写算法。</description></item><item><title>35_瞧一瞧Linux：虚拟文件系统如何管理文件？</title><link>https://artisanbox.github.io/9/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/35/</guid><description>你好，我是LMOS。
在前面的课程中，我们已经实现了Cosmos下的文件系统rfs，相信你已经感受到了一个文件系统是如何管理文件的。今天我们一起来瞧一瞧Linux是如何管理文件，也验证一下Linux那句口号：一切皆为文件。
为此，我们需要首先搞清楚什么是VFS，接着理清为了实现VFS所用到的数据结构，然后看看一个文件的打开、读写、关闭的过程，最后我们还要亲自动手实践，在VFS下实现一个“小”且“能跑”的文件系统。
下面让我们开始吧！这节课的配套代码，你可以从这里下载。
什么是VFSVFS（Virtual Filesystem）就像伙伴系统、SLAB内存管理算法一样，也是SUN公司最早在Sloaris上实现的虚拟文件系统，也可以理解为通用文件系统抽象层。Linux又一次“白嫖”了Sun公司的技术。
在Linux中，支持EXT、XFS、JFS、BTRFS、FAT、NTFS等多达十几种不同的文件系统，但不管在什么储存设备上使用什么文件系统，也不管访问什么文件，都可以统一地使用一套open(), read()、write()、close()这样的接口。
这些接口看上去都很简单，但要基于不同的存储设备设计，还要适应不同的文件系统，这并不容易。这就得靠优秀的VFS了，它提供了一个抽象层，让不同的文件系统表现出一致的行为。
对于用户空间和内核空间的其他部分，这些文件系统看起来都是一样的：文件都有目录，都支持建立、打开，读写、关闭和删除操作，不用关注不同文件系统的细节。
我来给你画张图，你一看就明白了。
你有没有发现，在计算机科学领域的很多问题，都可以通过增加一个中间的抽象层来解决，上图中Linux的VFS层就是应用和许多文件系统之间的抽象层。VFS向上对应用提供了操作文件的标准接口，向下规范了一个文件系统要接入VFS必需要实现的机制。
后面我们就会看到，VFS提供一系列数据结构和具体文件系统应该实现的回调函数。这样，一个文件系统就可以被安装到VFS中了。操作具体文件时，VFS会根据需要调用具体文件系统的函数。从此文件系统的细节就被VFS屏蔽了，应用程序只需要调用标准的接口就行了。
VFS数据结构VFS为了屏蔽各个文件系统的差异，就必须要定义一组通用的数据结构，规范各个文件系统的实现，每种结构都对应一套回调函数集合，这是典型的面向对象的设计方法。
这些数据结构包含描述文件系统信息的超级块、表示文件名称的目录结构、描述文件自身信息的索引节点结构、表示打开一个文件的实例结构。下面我们依次探讨这些结构。
超级块结构首先我们来看一看超级块结构，这个结构用于一个具体文件系统的相关信息，其中包含了VFS规定的标准信息，也有具体文件系统的特有信息，Linux系统中的超级块结构是一个文件系统安装在VFS中的标识。我们来看看代码，如下所示。
struct super_block { struct list_head s_list; //超级块链表 dev_t s_dev; //设备标识 unsigned char s_blocksize_bits;//以位为单位的块大小 unsigned long s_blocksize;//以字节为单位的块大小 loff_t s_maxbytes; //一个文件最大多少字节 struct file_system_type *s_type; //文件系统类型 const struct super_operations *s_op;//超级块函数集合 const struct dquot_operations *dq_op;//磁盘限额函数集合 unsigned long s_flags;//挂载标志 unsigned long s_magic;//文件系统魔数 struct dentry *s_root;//挂载目录 struct rw_semaphore s_umount;//卸载信号量 int s_count;//引用计数 atomic_t s_active;//活动计数 struct block_device *s_bdev;//块设备 void *s_fs_info;//文件系统信息 time64_t s_time_min;//最小时间限制 time64_t s_time_max;//最大时间限制 char s_id[32]; //标识名称 uuid_t s_uuid; //文件系统的UUID struct list_lru s_dentry_lru;//LRU方式挂载的目录 struct list_lru s_inode_lru;//LRU方式挂载的索引结点 struct mutex s_sync_lock;//同步锁 struct list_head s_inodes; //所有的索引节点 spinlock_t s_inode_wblist_lock;//回写索引节点的锁 struct list_head s_inodes_wb; //挂载所有要回写的索引节点 } __randomize_layout; 上述代码中我删除了我们现在不用关注的代码，在文件系统被挂载到VFS的某个目录下时，VFS会调用获取文件系统自己的超级块的函数，用具体文件系统的信息构造一个上述结构的实例，有了这个结构实例，VFS就能感知到一个文件系统插入了。</description></item><item><title>35｜内存管理第2关：实现垃圾回收</title><link>https://artisanbox.github.io/3/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/37/</guid><description>你好，我是宫文学。
今天这节课，我们继续上一节课未完成的内容，完成垃圾回收功能。
在上一节课，我们已经实现了一个基于Arena做内存分配的模块。并且，我们还在可执行程序里保存了函数、类和闭包相关的元数据信息。
有了上一节课的基础之后，我们这节课就能正式编写垃圾回收的算法了。算法思路是这样的：
首先，我们要有一个种机制来触发垃圾回收，进入垃圾回收的处理程序； 第二，我们要基于元数据信息来遍历栈帧，找到所有的GC根； 第三，从每个GC根出发，我们需要去标记GC根直接和间接引用的内存对象； 最后，我们再基于对象的标记信息，来回收内存垃圾。 在今天这节课，你不仅仅会掌握标记-清除算法，其中涉及的知识点，也会让你能够更容易地实现其他垃圾回收算法，并且让我们的程序能更好地与运行时功能相配合。
那接下来，我们就顺着算法实现思路，看看如何启动垃圾回收机制。
启动垃圾回收机制在现代的计算机语言中，我们可以有各种策略来启动垃圾回收机制。比如，在申请内存时，如果内存不足，就可以触发垃圾回收。甚至，你也可以每隔一段时间就触发一下垃圾收集。不过不论采取哪种机制，我们首先要有办法从程序的正常执行流程，进入垃圾回收程序才行。
进入垃圾回收程序，其实有一个经常使用的时机，就是在函数返回的时候。这个时候，我们可以不像平常那样，使用retq跳回调用者，而是先去检查是否需要做垃圾回收：如果需要做垃圾回收，那就先回收完垃圾，再返回到原来函数的调用者；如果不需要做垃圾回收，那就直接跳转到函数的调用者。
实现这个功能很简单，只需要在return语句之前调用frame_walker这个内置函数，并把当前%rbp寄存器的值作为参数传进去就好了：
visitReturnStatement(rtnStmt:ReturnStatement):any{ if (rtnStmt.exp!=null){ let ret = this.visit(rtnStmt.exp) as Oprand; //调用一个内置函数，来做垃圾回收 this.callBuiltIns(&amp;quot;frame_walker&amp;quot;,[Register.rbp]);//把当前%rbp的值传进去 //把返回值赋给相应的寄存器 let dataType = getCpuDataType(rtnStmt.exp.theType as Type); this.movIfNotSame(dataType, ret, Register.returnReg(dataType)); &amp;hellip; } } 这样，我们就能获得调用GC程序的时机。
在这段代码中，frame_walker内置函数的功能是遍历整个调用栈。这就是我们启动垃圾回收机制后，要进行的下一个任务。接下来我们就来分析一下具体怎么做。
遍历栈帧和对象遍历栈帧其实很简单，因为我们能够知道每个栈帧的起始地址。从哪里知道呢？就是rbp寄存器。rbp寄存器里保存的是每个栈帧的底部地址。
每次新建立一个栈帧的时候，我们总是把前一个栈帧的rbp值保护起来，这就是你在每个函数开头看到的第一行指令：pushq %rbp。因此，我们从栈帧里的第一个8字节区域，就可以读出前一个栈帧的%rbp值，这就意味着我们得到了前一个栈帧的栈底。然后你可以到这个位置，再继续获取更前一个栈帧的地址。具体你可以看下面这张图：
那这个思路真的有用吗？我们直接动手试一下！
首先，我写了一个简单的测试程序。在这个程序里，main函数调用了foo，foo又调用了bar。这样，在foo和bar返回前，我们都可以启动垃圾回收，但从main函数返回的时候就没有必要启动了，因为这个时候进程结束，进程从操作系统申请的所有内存，都会还给操作系统。
function foo(a:number):string{ let s:string = &amp;ldquo;PlayScript!&amp;rdquo; let b:number = bar(a+5); return s; }
function bar(b:number):number{ let a:number[] = [1,2,b];
let s:string = &amp;ldquo;Hello&amp;rdquo;; println(s); println(a[2]); b = b*10; return b; }</description></item><item><title>36_AC自动机：如何用多模式串匹配实现敏感词过滤功能？</title><link>https://artisanbox.github.io/2/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/37/</guid><description>很多支持用户发表文本内容的网站，比如BBS，大都会有敏感词过滤功能，用来过滤掉用户输入的一些淫秽、反动、谩骂等内容。你有没有想过，这个功能是怎么实现的呢？
实际上，这些功能最基本的原理就是字符串匹配算法，也就是通过维护一个敏感词的字典，当用户输入一段文字内容之后，通过字符串匹配算法，来查找用户输入的这段文字，是否包含敏感词。如果有，就用“***”把它替代掉。
我们前面讲过好几种字符串匹配算法了，它们都可以处理这个问题。但是，对于访问量巨大的网站来说，比如淘宝，用户每天的评论数有几亿、甚至几十亿。这时候，我们对敏感词过滤系统的性能要求就要很高。毕竟，我们也不想，用户输入内容之后，要等几秒才能发送出去吧？我们也不想，为了这个功能耗费过多的机器吧？那如何才能实现一个高性能的敏感词过滤系统呢？这就要用到今天的多模式串匹配算法。
基于单模式串和Trie树实现的敏感词过滤我们前面几节讲了好几种字符串匹配算法，有BF算法、RK算法、BM算法、KMP算法，还有Trie树。前面四种算法都是单模式串匹配算法，只有Trie树是多模式串匹配算法。
我说过，单模式串匹配算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串。多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。
尽管，单模式串匹配算法也能完成多模式串的匹配工作。例如开篇的思考题，我们可以针对每个敏感词，通过单模式串匹配算法（比如KMP算法）与用户输入的文字内容进行匹配。但是，这样做的话，每个匹配过程都需要扫描一遍用户输入的内容。整个过程下来就要扫描很多遍用户输入的内容。如果敏感词很多，比如几千个，并且用户输入的内容很长，假如有上千个字符，那我们就需要扫描几千遍这样的输入内容。很显然，这种处理思路比较低效。
与单模式匹配算法相比，多模式匹配算法在这个问题的处理上就很高效了。它只需要扫描一遍主串，就能在主串中一次性查找多个模式串是否存在，从而大大提高匹配效率。我们知道，Trie树就是一种多模式串匹配算法。那如何用Trie树实现敏感词过滤功能呢？
我们可以对敏感词字典进行预处理，构建成Trie树结构。这个预处理的操作只需要做一次，如果敏感词字典动态更新了，比如删除、添加了一个敏感词，那我们只需要动态更新一下Trie树就可以了。
当用户输入一个文本内容后，我们把用户输入的内容作为主串，从第一个字符（假设是字符C）开始，在Trie树中匹配。当匹配到Trie树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符C的下一个字符开始，重新在Trie树中匹配。
基于Trie树的这种处理方法，有点类似单模式串匹配的BF算法。我们知道，单模式串匹配算法中，KMP算法对BF算法进行改进，引入了next数组，让匹配失败时，尽可能将模式串往后多滑动几位。借鉴单模式串的优化改进方法，能否对多模式串Trie树进行改进，进一步提高Trie树的效率呢？这就要用到AC自动机算法了。
经典的多模式串匹配算法：AC自动机AC自动机算法，全称是Aho-Corasick算法。其实，Trie树跟AC自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟KMP算法之间的关系一样，只不过前者针对的是多模式串而已。所以，AC自动机实际上就是在Trie树之上，加了类似KMP的next数组，只不过此处的next数组是构建在树上罢了。如果代码表示，就是下面这个样子：
public class AcNode { public char data; public AcNode[] children = new AcNode[26]; // 字符集只包含a~z这26个字符 public boolean isEndingChar = false; // 结尾字符为true public int length = -1; // 当isEndingChar=true时，记录模式串长度 public AcNode fail; // 失败指针 public AcNode(char data) { this.data = data; } } 所以，AC自动机的构建，包含两个操作：
将多个模式串构建成Trie树；
在Trie树上构建失败指针（相当于KMP中的失效函数next数组）。
关于如何构建Trie树，我们上一节已经讲过了。所以，这里我们就重点看下，构建好Trie树之后，如何在它之上构建失败指针？
我用一个例子给你讲解。这里有4个模式串，分别是c，bc，bcd，abcd；主串是abcd。
Trie树中的每一个节点都有一个失败指针，它的作用和构建过程，跟KMP算法中的next数组极其相似。所以要想看懂这节内容，你要先理解KMP算法中next数组的构建过程。如果你还有点不清楚，建议你先回头去弄懂KMP算法。
假设我们沿Trie树走到p节点，也就是下图中的紫色节点，那p的失败指针就是从root走到紫色节点形成的字符串abc，跟所有模式串前缀匹配的最长可匹配后缀子串，就是箭头指的bc模式串。
这里的最长可匹配后缀子串，我稍微解释一下。字符串abc的后缀子串有两个bc，c，我们拿它们与其他模式串匹配，如果某个后缀子串可以匹配某个模式串的前缀，那我们就把这个后缀子串叫作可匹配后缀子串。
我们从可匹配后缀子串中，找出最长的一个，就是刚刚讲到的最长可匹配后缀子串。我们将p节点的失败指针指向那个最长匹配后缀子串对应的模式串的前缀的最后一个节点，就是下图中箭头指向的节点。
计算每个节点的失败指针这个过程看起来有些复杂。其实，如果我们把树中相同深度的节点放到同一层，那么某个节点的失败指针只有可能出现在它所在层的上一层。</description></item><item><title>36_为什么临时表可以重名？</title><link>https://artisanbox.github.io/1/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/36/</guid><description>今天是大年三十，在开始我们今天的学习之前，我要先和你道一声春节快乐！
在上一篇文章中，我们在优化join查询的时候使用到了临时表。当时，我们是这么用的：
create temporary table temp_t like t1; alter table temp_t add index(b); insert into temp_t select * from t2 where b&amp;gt;=1 and b&amp;lt;=2000; select * from t1 join temp_t on (t1.b=temp_t.b); 你可能会有疑问，为什么要用临时表呢？直接用普通表是不是也可以呢？
今天我们就从这个问题说起：临时表有哪些特征，为什么它适合这个场景？
这里，我需要先帮你厘清一个容易误解的问题：有的人可能会认为，临时表就是内存表。但是，这两个概念可是完全不同的。
内存表，指的是使用Memory引擎的表，建表语法是create table … engine=memory。这种表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在。除了这两个特性看上去比较“奇怪”外，从其他的特征上看，它就是一个正常的表。
而临时表，可以使用各种引擎类型 。如果是使用InnoDB引擎或者MyISAM引擎的临时表，写数据的时候是写到磁盘上的。当然，临时表也可以使用Memory引擎。
弄清楚了内存表和临时表的区别以后，我们再来看看临时表有哪些特征。
临时表的特性为了便于理解，我们来看下下面这个操作序列：
图1 临时表特性示例可以看到，临时表在使用上有以下几个特点：
建表语法是create temporary table …。
一个临时表只能被创建它的session访问，对其他线程不可见。所以，图中session A创建的临时表t，对于session B就是不可见的。
临时表可以与普通表同名。
session A内有同名的临时表和普通表的时候，show create语句，以及增删改查语句访问的是临时表。</description></item><item><title>36_从URL到网卡：如何全局观察网络数据流动？</title><link>https://artisanbox.github.io/9/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/36/</guid><description>你好，我是 LMOS。
从这节课起，我们就要开始学习网络篇的内容了。网络是一个极其宏大的知识结构，我会通过五节课带你了解计算机网络的关键内容。
具体我们是这样安排的。作为网络篇的开始，今天这节课我会从一个面试中高频出现的问题切入，带你梳理从输入URL到网卡的网络数据流动过程中都发生了什么事。如果你真正理解了这个过程，相信你对整个网络架构的认知也会有质的飞跃。
网络篇的第二节课，我会带你分析网络数据包在内核中如何流转；第三节课，我们一起探讨互联网架构演进过程，并动手做一次协议栈移植；最后两节课，我还是照例带你看看Linux，让你理解套接字在Linux内核中怎样实现。
从一道经典面试题说起下面我们一起来看看一个问题，估计你多多少少会觉得熟悉。
输入URL，从一个请求到响应都发生了什么事？
没错，这是一道非常经典的面试题，你在网上随便一搜，也会找到各种各样的资料解答这道题。
不过啊，那些答案都有一些笼统，今天我会尽量详细地为你梳理一下这个过程。跟着我学完这节课，你就能明白，为什么面试官对这道题青睐有加了。
这里我先给你概括一下全过程，让你有个整体印象。
1.常规的网络交互过程是从客户端发起网络请求，用户态的应用程序（浏览器）会生成HTTP请求报文、并通过DNS协议查找到对应的远端IP地址。
2.在套接字生成之后进入内核态，浏览器会委托操作系统内核协议栈中的上半部分，也就是TCP/UDP协议发起连接请求。
3.然后经由协议栈下半部分的IP协议进行封装，使数据包具有远程定位能力。
4.经过MAC层处理，找到接收方的目标MAC地址。
5.最终数据包在经过网卡转化成电信号经过交换机、路由器发送到服务端，服务端经过处理拿到数据，再通过各种网络协议把数据响应给客户端。
6.客户端拿到数据进行渲染。
7.客户端和服务端之间反复交换数据，客户端的页面数据就会发生变化。
你有没有发现，刚才的过程中，我们提到了多个层级和网络协议，那么网络为什么要分层呢？网络协议又是什么呢？请听我给你一一道来。
前置知识：网络分层和网络协议在计算机网络时代初期，各大厂商推出了不同的网络架构和标准，为统一标准，国际标准化组织ISO推出了统一的OSI参考模型。
当前网络主要遵循的IEEE 802.3标准，就是基于OSI模型提出的，主要定义的是物理层和数据链路层有线物理数据流传输的标准。
那么问题来了，网络为什么要分层呢？
我们都知道网络是复杂的。对于复杂的问题，我们自然要通过分层处理简化问题难度，降低复杂度，由于分层后的各层之间相互独立，我们可以把大问题分割成小问题。同样，分层也保证了网络的松耦合和相对的灵活，分层拆分后易于各层的实现和维护，也方便了各层的后续扩展。
网络分层解决了网络复杂的问题，在网络中传输数据中，我们对不同设备之间的传输数据的格式，需要定义一个数据标准，所以就有了网络协议。
网络协议是双方通信的一种约定，以便双方都可以理解对方的信息。接下来我们就用OSI协议体系中广泛应用的TCP/IP层的体系结构来分析整个过程，你重点需要关注的是数据处理的过程和网络协议。
发起请求阶段（应用层）下面我们首先来看看网络应用层，它是最上层的，也是我们能直接接触到的。
我们的电脑或⼿机使⽤的应⽤软件都是在应⽤层实现，所以应⽤层只需要专注于为⽤户提供应⽤功能，不⽤去关⼼数据是如何传输的。你可以这样理解，应⽤层是⼯作在操作系统中的⽤户态。
我们依然从浏览器中输入URL，开始了解网络应用层的工作流程。
用户输入：在浏览器中输入URL我们在浏览器中输入URL的时候，浏览器已经开始工作了。浏览器会根据我们的输入内容，先匹配对应的URL以及关键词，给出输入建议，同时校验URL的合法性，并且会在URL前后补全URL。
为了帮你更好地理解，我给你举个例子说明。我们以输入cosmos.com为例，首先浏览器会判断出这是一个合法的URL，并且会补全为http://www.cosmos.com。
其中http为协议，cosmos.com为网络地址，每个网络栏的地址都符合通用 URI 的语法。URI 一般语法由五个分层序列组成。后面的第一行内容我给你列了URL的格式，第二行做了行为说明。
URI = scheme:[//authority]path[?query][#fragment] URI = 方案:[//授权]路径[?查询][#片段ID] 接着，浏览器从URL中会提取出网络的地址，也叫做主机名（host），一般主机名可以为域名或IP地址，此处使用域名。
对URL进行解析之后，浏览器确定了服务器的主机名和请求路径，接下来就是根据这些信息来生成HTTP请求消息了，那么到现在为止，我们的HTTP请求是否已经发出了呢？并不是这样的，我们接着往下看。
网络请求前：查看浏览器缓存浏览器在HTTP报文生成完成后，它并不是马上就开始网络请求的。
在请求发出之前，浏览器首先会检查保存在本地计算机中的缓存，如果访问过当前的URL，会先进入缓存中查询是否有要请求的文件。此时存在的缓存有路由器缓存、DNS缓存、浏览器缓存、Service Worker、Memory Cache、Disk Cache、Push Cache、系统缓存等。
在这里我们看一下系统缓存，如果在浏览器缓存里没有命中缓存，浏览器会做一个系统调用获得系统缓存中的记录，就是我们的gethostbyname方法，它的作用是通过域名获取IP地址。这个方法会返回如下结构。
struct hostent { char *h_name;// 主机的别名.www.cosmos.com就是google他自己的别名
char **h_aliases;// 主机ip地址的类型，到底是ipv4(AF_INET)，还是pv6(AF_INET6) int h_addrtype;// 主机ip地址的长度 int h_length;// 主机ip地址的长度 char **h_addr_list; // 主机的ip地址，注意，这个是以网络字节序存储的 #define h_addr h_addr_list[0] 这个函数，是将类型为af的网络地址结构src，转换成主机序的字符串形式，存放在长度为cnt的字符串中。返回指向dst的一个指针。如果函数调用错误，返回值是NULL }; 如果没有访问过当前的URL，就会跳过缓存这一步，这时我们就会进入网络操作了。</description></item><item><title>36_局部性原理：数据库性能跟不上，加个缓存就好了？</title><link>https://artisanbox.github.io/4/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/36/</guid><description>平时进行服务端软件开发的时候，我们通常会把数据存储在数据库里。而服务端系统遇到的第一个性能瓶颈，往往就发生在访问数据库的时候。这个时候，大部分工程师和架构师会拿出一种叫作“缓存”的武器，通过使用Redis或者Memcache这样的开源软件，在数据库前面提供一层缓存的数据，来缓解数据库面临的压力，提升服务端的程序性能。
在数据库前添加数据缓存是常见的性能优化方式那么，不知道你有没有想过，这种添加缓存的策略一定是有效的吗？或者说，这种策略在什么情况下是有效的呢？如果从理论角度去分析，添加缓存一定是我们的最佳策略么？进一步地，如果我们对于访问性能的要求非常高，希望数据在1毫秒，乃至100微秒内完成处理，我们还能用这个添加缓存的策略么？
理解局部性原理我们先来回顾一下，上一讲的这张不同存储器的性能和价目表。可以看到，不同的存储器设备之间，访问速度、价格和容量都有几十乃至上千倍的差异。
以上一讲的Intel 8265U的CPU为例，它的L1 Cache只有256K，L2 Cache有个1MB，L3 Cache有12MB。一共13MB的存储空间，如果按照7美元/1MB的价格计算，就要91美元。
我们的内存有8GB，容量是CPU Cache的600多倍，按照表上的价格差不多就是120美元。如果按照今天京东上的价格，恐怕不到40美元。128G的SSD和1T的HDD，现在的价格加起来也不会超过100美元。虽然容量是内存的16倍乃至128倍，但是它们的访问速度却不到内存的1/1000。
性能和价格的巨大差异，给我们工程师带来了一个挑战：我们能不能既享受CPU Cache的速度，又享受内存、硬盘巨大的容量和低廉的价格呢？你可以停下来自己思考一下，或者点击文章右上方的“请朋友读”，邀请你的朋友一起来思考这个问题。然后，再一起听我的讲解。
好了，现在我公布答案。想要同时享受到这三点，前辈们已经探索出了答案，那就是，存储器中数据的局部性原理（Principle of Locality）。我们可以利用这个局部性原理，来制定管理和访问数据的策略。这个局部性原理包括时间局部性（temporal locality）和空间局部性（spatial locality）这两种策略。
我们先来看时间局部性。这个策略是说，如果一个数据被访问了，那么它在短时间内还会被再次访问。这么看这个策略有点奇怪是吧？我用一个简单的例子给你解释下，你一下就能明白了。
比如说，《哈利波特与魔法石》这本小说，我今天读了一会儿，没读完，明天还会继续读。同理，在一个电子商务型系统中，如果一个用户打开了App，看到了首屏。我们推断他应该很快还会再次访问网站的其他内容或者页面，我们就将这个用户的个人信息，从存储在硬盘的数据库读取到内存的缓存中来。这利用的就是时间局部性。
同一份数据在短时间内会反复多次被访问我们再来看空间局部性。这个策略是说，如果一个数据被访问了，那么和它相邻的数据也很快会被访问。
我们还拿刚才读《哈利波特与魔法石》的例子来说。我读完了这本书之后，感觉这书不错，所以就会借阅整套“哈利波特”。这就好比我们的程序，在访问了数组的首项之后，多半会循环访问它的下一项。因为，在存储数据的时候，数组内的多项数据会存储在相邻的位置。这就好比图书馆会把“哈利波特”系列放在一个书架上，摆放在一起，加载的时候，也会一并加载。我们去图书馆借书，往往会一次性把7本都借回来。
相邻的数据会被连续访问有了时间局部性和空间局部性，我们不用再把所有数据都放在内存里，也不用都放在HDD硬盘上，而是把访问次数多的数据，放在贵但是快一点的存储器里，把访问次数少的数据，放在慢但是大一点的存储器里。这样组合使用内存、SSD硬盘以及HDD硬盘，使得我们可以用最低的成本提供实际所需要的数据存储、管理和访问的需求。
如何花最少的钱，装下亚马逊的所有商品？了解了局部性原理，下面我用一些真实世界中的数据举个例子，带你做个小小的思维体操，来看一看通过局部性原理，利用不同层次存储器的组合，究竟会有什么样的好处。
我们现在要提供一个亚马逊这样的电商网站。我们假设里面有6亿件商品，如果每件商品需要4MB的存储空间（考虑到商品图片的话，4MB已经是一个相对较小的估计了），那么一共需要2400TB（ = 6亿 × 4MB）的数据存储。
如果我们把数据都放在内存里面，那就需要3600万美元（ = 2400TB/1MB × 0.015美元 = 3600万美元）。但是，这6亿件商品中，不是每一件商品都会被经常访问。比如说，有Kindle电子书这样的热销商品，也一定有基本无人问津的商品，比如偏门的缅甸语词典。
如果我们只在内存里放前1%的热门商品，也就是600万件热门商品，而把剩下的商品，放在机械式的HDD硬盘上，那么，我们需要的存储成本就下降到45.6万美元（ = 3600 万美元 × 1% + 2400TB / 1MB × 0.00004 美元），是原来成本的1.3%左右。
这里我们用的就是时间局部性。我们把有用户访问过的数据，加载到内存中，一旦内存里面放不下了，我们就把最长时间没有在内存中被访问过的数据，从内存中移走，这个其实就是我们常用的LRU（Least Recently Used）缓存算法。热门商品被访问得多，就会始终被保留在内存里，而冷门商品被访问得少，就只存放在HDD硬盘上，数据的读取也都是直接访问硬盘。即使加载到内存中，也会很快被移除。越是热门的商品，越容易在内存中找到，也就更好地利用了内存的随机访问性能。
那么，只放600万件商品真的可以满足我们实际的线上服务请求吗？这个就要看LRU缓存策略的缓存命中率（Hit Rate/Hit Ratio）了，也就是访问的数据中，可以在我们设置的内存缓存中找到的，占有多大比例。
内存的随机访问请求需要100ns。这也就意味着，在极限情况下，内存可以支持1000万次随机访问。我们用了24TB内存，如果8G一条的话，意味着有3000条内存，可以支持每秒300亿次（ = 24TB/8GB × 1s/100ns）访问。以亚马逊2017年3亿的用户数来看，我们估算每天的活跃用户为1亿，这1亿用户每人平均会访问100个商品，那么平均每秒访问的商品数量，就是12万次。
但是如果数据没有命中内存，那么对应的数据请求就要访问到HDD磁盘了。刚才的图表中，我写了，一块HDD硬盘只能支撑每秒100次的随机访问，2400TB的数据，以4TB一块磁盘来计算，有600块磁盘，也就是能支撑每秒 6万次（ = 2400TB/4TB × 1s/10ms ）的随机访问。
这就意味着，所有的商品访问请求，都直接到了HDD磁盘，HDD磁盘支撑不了这样的压力。我们至少要50%的缓存命中率，HDD磁盘才能支撑对应的访问次数。不然的话，我们要么选择添加更多数量的HDD硬盘，做到每秒12万次的随机访问，或者将HDD替换成SSD硬盘，让单个硬盘可以支持更多的随机访问请求。
当然，这里我们只是一个简单的估算。在实际的应用程序中，查看一个商品的数据可能意味着不止一次的随机内存或者随机磁盘的访问。对应的数据存储空间也不止要考虑数据，还需要考虑维护数据结构的空间，而缓存的命中率和访问请求也要考虑均值和峰值的问题。
通过这个估算过程，你需要理解，如何进行存储器的硬件规划。你需要考虑硬件的成本、访问的数据量以及访问的数据分布，然后根据这些数据的估算，来组合不同的存储器，能用尽可能低的成本支撑所需要的服务器压力。而当你用上了数据访问的局部性原理，组合起了多种存储器，你也就理解了怎么基于存储器层次结构，来进行硬件规划了。</description></item><item><title>36_当前技术的发展趋势以及其对编译技术的影响</title><link>https://artisanbox.github.io/6/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/36/</guid><description>在IT领域，技术一直在飞速的进步，而每次进步，都会带来新的业态和新的发展机遇。
退回到10年前，移动互联网刚兴起不久，谁也没想到它会催生现在这么多的业态。而云计算还在酝酿期，腾讯和百度的创始人都觉得它走不远，现在竟然这么普及。
退回到20年前，互联网刚兴起，上网都要拨号。互联网的几个巨头，像阿里巴巴、百度、腾讯、新浪，还有网易，都是在那个时代展露头角的。毫不夸张地说，如果你在那个时代搞技术，懂Web编程的话，那绝对是人人争抢的“香饽饽”，毕竟那时，Web编程是前沿技术，懂这个领域的人，凤毛麟角。
退回到30年前，微软等公司才刚开始展露头角，雷军、求伯君等老一代程序员也正在发力，WPS的第一个版本运行在DOS操作系统上。我还记得，95年的时候，我在大学的阶梯教室里，看了比尔盖茨曾发表的，关于未来技术方向的演讲。当时，他预测了未来的科技成果，比如移动智能设备，听上去像天方夜谭，但现在移动互联网、人工智能和5G的发展，早已超出了他当时的想象。
那么你有理由相信，未来10年、20年、30年，会发生同样天翻地覆的变化。这种变化所造成的的影响，你我哪怕大开“脑洞”都无法预料。而你在这种趋势下，所能做的就是，把握当下，并为未来的职业生涯做好准备。这是一件认真且严肃的事情，值得你用心对待。
当然，洞悉未来很难，但你可以根据当前了解到的信息，捕捉一些发展趋势，看看这些发展趋势，让编译技术的发展方向有了哪些不同，跟你又有什么关系。
本节课，我想与你分享3个方面的技术发展趋势，以及它们对编译技术的影响：
人工智能，以及如何让编程和编译技术变得更智能？ 云计算，以及是否需要云原生的语言？ 前端技术，以及能否出现统一各个平台的大前端技术？ 期望这些内容，能让你看到一些不同的思考视角，获得一些新的信息。
趋势1：让编程更智能人工智能是当前发展最迅速的技术之一了。这几年，它的发展速度超过了人们的预期。那么你知道，它对编译技术和计算机语言的影响是什么吗？
首先，它需要编译器能够支撑，机器学习对庞大计算力的需求，同时兼容越来越多新的硬件架构。
由于机器学习的过程需要大量的计算，仅仅采用CPU效率很低，所以GPU和TPU等新的硬件架构得到了迅速的发展。对于编译技术来说，首要的任务，是要充分发挥这些新硬件的能力；因为AI的算法要能跑在各种后端架构上，包括CPU、GPU和TPU，也包括仍然要采用SIMD等技术，所以后端技术就会变得比较复杂。同时，前端也有不同的技术框架，比如谷歌的TensorFlow、Facebooke的PyTorch等。那么编译器怎样更好地支持多种前端和多种后端呢？
根据在24讲学到的知识，你应该会想到要借助中间代码。所以，MLIR应运而生。这里要注意，ML是Multi-Level（多层次）的意思，而不是Machine Learning的缩写。我还想告诉你，MLIR的作者，也是LLVM的核心作者、Swift语言的发明人，Chris Lattner（他目前在谷歌TensorFlow项目中）。而当你看到MLIR的格式，也许会觉得跟LLVM的IR很像，那么你其实可以用更短的学习周期来掌握这个IR。
其次，AI还可能让现有的编译技术发生较大的改变。
实际上，把AI和编译技术更好地结合，是已经持续研究了20年左右的，一个研究方向。不过，没有很大的发展。因为之前，人工智能技术的进步不像这几年这么快。近几年，随着人工智能技术快速进步，在人脸识别、自动驾驶等各个领域产生了相当实用的成果，人们对人工智能可能给编译技术带来的改变，产生了更大的兴趣。这给了研究者们研究的动力，他们开始从各个角度探索变革的可能性。
比如说，在后端技术部分，很多算法都是NP完全的。这就是说，如果你用穷举的方法找出最优解，成本非常高。这个时候，就会用启发式（heuristic）的算法，也就是凭借直观或经验构造的算法，能够在可接受的花费下找出一个可行的解。那么采用人工智能技术，通过大数据的训练，有可能找出更好的启发式算法，供我们选择。这是人工智能和编译技术结合的一个方向。
Milepost GCC项目早在2009年就发布了，它是一款开源的，人工智能编译器。它能够通过自动学习来确定去优化哪些代码，以便让程序的性能更高。据IBM的测试数据，某些嵌入式软件的性能因此提升了18%。
另一个讨论度比较高的方向就是人工智能编程（或者叫自动编程）。从某种意义上看，从计算机诞生到现在，我们编写程序的方式一直没有太大的进步。最早，是通过在纸带或卡片上打孔，来写程序；后来产生了汇编语言和高级语言。但是，写程序的本质没有变化，我们只是在用更高级的方式打孔。
讽刺的是，在计算机语言的帮助下，很多领域都出现了非常好的工具，比如CAD完全改变了建筑设计行业。但计算机行业本身用的工具仍然是比较原始的，还是在一个编辑器中，用文本的方式输入代码。
而人工智能技术可能让我们习惯已久的编程模式发生改变。比如，现在的编译器只是检查错误并生成代码，带有AI功能的编译器呢，有可能不仅检查出比较明显的错误，甚至还会对你的编码方式提出建议。假设你用一个循环去做某个数组的计算，带有AI功能的编译器会告诉你，用函数式编程做向量计算性能更高，并提供一键式替换功能。
这里延伸一下，有可能，未来写程序的方式都会改变。微软收购GitHub以后，运用大量的代码作为训练数据，正在改进IDE，提供智能提示功能。而这还只是开始。目前，AI其实已经能帮你做UI的设计：你画一个草图，AI给你生成对应的Web页面。
而且在AI辅助设计领域，算法还能根据你的需要，帮你生成平面或三维的设计方案。我能想象，未来，你告诉AI给你生成一个电商APP，它就能给你生成出来。你再告诉它做什么样的修改，它都会立即修改。在未来，应用开发中，最令人头疼的需求变化的问题，在AI看来根本不是事。
那么，如果这个前景是真实的，对于你而言，需要掌握什么技能呢？
我建议你了解，编译技术和人工智能这两个领域的知识。那些计算机的基础知识会一直有用，你可以参与到编程范式迁移，这样一个伟大的进程中。现有程序开发中的那些简单枯燥，又不需要多少创造力的工作，也就是大家通常所说的“搬砖”工作，可能会被AI代替。而我猜测，未来的机会可能会留给两类人：
一类是具备更加深入的计算机基础技能，能应对未来挑战的，计算机技术人才，他们为新的计算基础设施的发展演化，贡献自己的力量。
另一类是应用领域的专家和人才。他们通过更富有创造力的工作，利用新的编程技术实现各种应用。编写应用程序的重点，可能不再是写代码，而是通过人工智能，训练出能够反映领域特点的模型。
当然，向自动编程转移的过程肯定是逐步实现的：AI先是能帮一些小忙，解放我们一部分工作量，比如辅助做界面设计、智能提示；接着是能够自动生成某些小的、常用的模块；最后是能够生成和管理复杂的系统。
总而言之，AI技术给编译技术，和编程模式带来了各种可能性，而你会见证这种转变。除此之外，云计算技术的普及和深化，也可能给编译技术和编程模式带来改变。
趋势2：云原生的开发语言云计算技术现在的普及度很广，所有应用的后端部分，缺省情况下都是跑在云平台上的，云就是应用的运行环境。
在课程里，我带你了解过程序的运行环境。那时，我们的关注点，还是在一个单机的环境上，包括CPU和内存这些硬件，以及操作系统这个软件，弄清楚程序跟它们互动的关系。比如，操作系统能够加载程序，能够帮程序管理内存，能够为程序提供一些系统功能（把数据写到磁盘上等等）。
然而，在云计算时代，云就是应用的运行环境。一个应用程序不是仅仅加载到一台物理机上，而是要能够根据需要，加载很多实例到很多机器上，实现横向扩展。当然了，云也给应用程序提供各种系统功能，比如云存储功能，它就像一台单独的服务器，会给程序提供读写磁盘的能力一样。
除此之外，在单机环境下，传统的应用程序，是通过函数或方法，来调用另一个模块的功能，函数调用的层次体现为栈里一个个栈桢的叠加，编译器要能够形成正确的栈桢，实现自动的内存管理。而在云环境下，软件模块以服务的形式存在，也就是说，一个模块通过RESTful接口或各种RPC协议，调用另外的模块的功能。程序还需要处理通讯失败的情况，甚至要在调用多个微服务时，保证分布式事务特性。而我们却没从编译技术的角度，帮助程序员减轻这个负担。
导致的结果是：现在后端的程序特别复杂。你要写很多代码，来处理RPC、消息、分布式事务、数据库分片等逻辑，还要跟各种库、框架、通讯协议等等打交道。更糟糕的是，这些技术性的逻辑跟应用逻辑，混杂在一起，让系统的复杂度迅速提高，开发成本迅速提升，维护难度也增加。很多试图采用微服务架构的项目因此受到挫折，甚至回到单一应用的架构。
所以，一个有意义的问题是：能否在语言设计的时候，就充分利用云的基础设施，实现云原生（Cloud Native）的应用？也就是说，我们的应用，能够透明地利用好云计算的能力，并能兼容各种不同厂商的云计算平台，就像传统应用程序，能够编译成，不同操作系统的可执行文件一样。
好消息是，云计算的服务商在不断地升级技术，希望能帮助应用程序，更好地利用云计算的能力。而无服务器（Serverless）架构就是最新的成果之一。采用无服务器架构，你的程序都不需要知道容器的存在，也不需要关心虚拟机和物理机器，你只需要写一个个的函数，来完成功能就可以了。至于这个函数所需要的计算能力、存储能力，想要多少就有多少。
但是，云计算厂商提供的服务和接口缺少标准化，当你把大量应用都部署到某个云平台的时候，可能会被厂商锁定。如果有一门面向云原生应用的编程语言，和相应的开发平台，能帮助人们简化云应用的开发，同时又具备跨不同云平台的能力，那就最理想了。
实际上，已经有几个创业项目在做这个方向做探索了，比如 Ballerina、Pulumi和Dark，你可以看一下。
当然了，云计算和编程结合起来，就是另一个有趣的话题：云编程。我会在下一讲，与你进一步讨论这个话题。
趋势3：大前端技术栈上面所讲的云计算，针对的是后端编程，而与此对应的，是前端编程工作。
后端工作的特点，是越来越云化，让工程师们头疼的问题，是处理分布式计算环境下，软件系统的复杂性。当然，前端的挑战也不少。
我们开发一款应用，通常需要支持Web、IOS和Android三种平台，有时候，甚至需要提供Windows和macOS的桌面客户端。不同的平台会需要不同的技术栈，从而导致一款应用的开发成本很高，这也是前端工程师们不太满意的地方。
所以，前端工程师们一直希望能用一套技术栈，搞定多个平台。比如，尝试用Web开发的技术栈完成Android、IOS和桌面应用的开发。React Native、Electron等框架是这个方面的有益探索；Flutter项目也做了一些更大胆的尝试。
Flutter采用Dart开发语言，可以在Android和IOS上生成高质量的原生界面，甚至还可以支持macOS、Windows和Linux上的原生界面。另外，它还能编译成Web应用。所以，本质上，你可以用同一套代码，给移动端、桌面端和Web端开发UI。
你可以把这种技术思路叫做大前端：同一套代码，支持多个平台。
从Flutter框架中，你可以看出编译技术起到的作用。首先，Dart语言也是基于虚拟机的，编译方式支持AOT和JIT，能够运行在移动端和桌面端，能够调用本地操作系统的功能。对于Web应用则编译成JavaScript、CSS和HTML。这个项目的创新力度已经超过了React Native这些项目，工程师们已经不满足于，在现有的语言（JavaScript）基础上编写框架，而是用一门新的语言去整合多个技术栈。
当然，提到前端技术，就不能不提Web Assembly（WASM）。WASM是一种二进制的字节码，也就是一种新的IR，能够在浏览器里运行。相比JavaScript，它有以下特点：
静态类型； 性能更高； 支持C/C++/Rust等各种语言生成WASM，LLVM也给了WASM很好的支持； 字节码尺寸比较少，减少了下载时间； 因为提前编译成字节码，因此相比JavaScript减少了代码解析的时间。 由于这些特点，WASM可以在浏览器里，更高效地运行，比如可以支持更复杂的游戏效果。我猜想，未来可能出现，基于浏览器的、性能堪比本地应用的字处理软件、电子表格软件。基于云的文档软件（比如Google Doc）会得到再一次升级，使用者也将获得更好的体验。</description></item><item><title>36_高级特性（一）：揭秘元编程的实现机制</title><link>https://artisanbox.github.io/7/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/36/</guid><description>你好，我是宫文学。
作为一名技术人员，我想你肯定知道什么是编程，那你有没有听说过“元编程（Meta-Programming）”这个概念呢？
元编程是计算机语言提供的一项重要能力。这么说吧，如果你要编写一些比较厉害的程序，像是Java世界里的Spring、Hibernate这样的库，以及C++的STL库等这样级别的程序，也就是那些通用性很强、功能强大的库，元编程功能通常会给予你巨大的帮助。
我还可以从另一个角度来评价元编程功能。那就是善用计算机语言的元编程功能，某种意义上能让你修改这门语言，让它更满足你的个性化需求，为你量身打造！
是不是觉得元编程还挺有意思的？今天这一讲，我就带你来理解元编程的原理，并一起探讨如何用编译技术来支持元编程功能的实现。
首先，我们需要透彻地了解一下什么是元编程。
什么是元编程（Meta-Programming）？元编程是一种把程序当做数据来处理的技术。因此，采用元编程技术，你可以把一个程序变换成另一个程序。
图1：元编程处理的对象是程序那你可能要问了，既然把程序作为处理对象的技术就是元编程技术，那么编译器不就是把程序作为处理对象的吗？经过处理，编译器会把源代码转换成目标代码。类似的还有对源代码的静态分析工具、代码生成工具等，都算是采用了元编程技术。
不过，我们在计算机语言里说的元编程技术，通常是指用这门语言本身提供的功能，就能处理它自己的程序。
比如说，在C语言中，你可以用宏功能。经过C语言的预处理器处理以后，那些宏就被转换成了另外的代码。下面的MUL宏，用起来像一个函数，但其实它只是做了一些字符串的替换工作。它可以说是最原始的元编程功能了。你在阅读像Python和Julia的编译器时，就会发现有不少地方采用了宏的功能，能让代码更简洁、可读性更好。
#define MUL(a,b) (a*b) MUL(2,3) //预处理后变成(2*3) 再拿Java语言举个例子。Java语言对元编程提供了多重支持，其中之一是注解功能。我们在解析Java编译器的时候已经发现，Java编译器会把所编译的程序表示成一个对象模型。而注解程序可以通过这个对象模型访问被注解的程序，并进行一些处理，比如生成新的程序。所以，这也是把程序作为数据来处理。
除了注解以外，Java还提供了反射机制。通过反射机制，Java程序可以在运行时获取某个类有哪些方法、哪些属性等信息，并可以动态地运行该程序。你看，这同样是把程序作为数据来处理。
像Python和JavaScript这样的脚本语言，其元编程能力就更强了。比如说，你用程序可以很容易地查询出某个对象都有哪些属性和方法，甚至可以给它们添加新的属性和方法。换句话说，你可以很容易地把程序作为数据进行各种变换，从而轻松地实现一些灵活的功能。这种灵活性，是很多程序员特别喜欢Python和JavaScript这样的语言的原因。
图2：各种不同的元编程技术起作用的时机好了，到现在为止，你已经了解了元编程的基本特征：把程序当做数据去处理。接下来，我再带你更深入地了解一下元编程，并把不同的元编程技术做做分类。
理解Meta的含义、层次以及作用首先，我们来注意一下Meta这个词缀的意思。维基百科中的解释是，Meta来自希腊文，意思是“在……之后（after）”和“超越……（beyond）”。加上这个词缀后，Meta-Somthing所形成的新概念就会比原来的Somthing的概念的抽象度上升一层。
举例来说，Physics是物理学的意思，表示看得见摸得着的物理现象。而Metaphysics就代表超越了物理现象的学问，也就是形而上学。Data是数据，而Metadata是元数据，是指对数据特性的描述，比如它是什么数据类型、取值范围是什么，等等。
还有，一门语言我们叫做Language，而语法规则（Grammar）是对一门语言的特点的描述，所以语法规则可以看做是Metalanguage。
其次，在理解了Meta的概念以后，我再进一步告诉你，Meta是可以分层次的。你可以对Meta再超越一层、抽象一层，就是Meta-Meta。理解Meta的层次，对于你深入理解元编程的概念非常重要。
拿你很熟悉的关系数据库来举个例子吧，看看不同的Meta层次都是什么意思。
首先是M0层，也就是关系数据库中的数据。比如一条人员数据，编号是“001”，姓名是“宫文学”等。一个数据库的使用者，从数据库中查出了这条数据，我们说这个人是工作在M0层的。
比M0抽象一层的是M1层，也就是Metadata，它描述了数据库中表的结构。比如，它定义了一张人员表，并且规定里面有编号、姓名等字段，以及每个字段的数据类型等信息。这样看来，元数据实际上是描述了一个数据模型，所以它也被叫做Model。一个工程师设计了这个数据库表的结构，我们说这个工程师是工作在M1层的。基于该工程师设计的数据库表，你可以保存很多M0层的人员数据：张三、李四、王五，等等。
比M1再抽象一层的是M2层。因为M1层可以叫做Model，所以M2层可以叫做Metamodel，也就是元模型。在这个例子中，Metamodel描述的是关系数据模型：它是由一张张的表（Table）构成的；而每张表，都是由字段构成的；每个字段，都可以有数据类型、是否可空等信息。发明关系数据模型，以及基于这个模型设计出关系数据库的大师，是工作在M2层的。基于关系模型，你可以设计出很多M1层的数据库表：人员表、订单表、商品表，等等。
那么有没有比Metamodel更抽象的层次呢？有的。这就是M3层，叫做Meta-Metamodel。这一层要解决的问题是，如何去描述关系数据模型和其他的元模型？在UML标准中，有一个MOF（Meta Object Facility）的规范，可以用来描述关系数据库、数据仓库等元模型。它用类、关联、数据类型和包这些基本要素来描述一个元模型。
好，通过关系数据库这个例子，现在你应该理解了不同的Meta层次是什么概念。那我们再把这个概念应用到计算机语言领域，也是一样的。
假设你使用一门面向对象的语言写了一个程序。这个程序运行时，在内存里创建了一个Person对象。那这个对象属于M0层。
而为了创建这个Person对象，你需要用程序设计一个Person类。从这个意义上来看，我们平常写的程序属于M1层，也就是相当于建立了一个模型来描述现实世界。你编写的订票程序就是对真实世界中的购票行为建立了一个模型，而你编写的游戏当然也是建立了一个逼真的游戏模型。
那么，你要如何才能设计一个Person类，以及一个完整的程序呢？这就需要用到计算机语言。计算机语言对应着M2层。它提供了类、成员变量、方法、数据类型、本地变量等元素，用于设计你的程序。我们对一门计算机语言的词法规则、语法规则和语义规则等方面的描述，就属于M2层，也就是一门计算机语言的元模型。而编译器就是工作在M2层的程序，它会根据元模型，也就是词法规则、语法规则等，来做程序的翻译工作。
我们在描述词法规则、语法规则的时候，曾经用到产生式、EBNF这些工具。这些工具是属于M3层的。你可以用我们前面说过的一个词，也就是Metalanguage来称呼这一层次。
这里我用了一个表格，来给你展示下关系数据模型与Java程序中不同的Meta层次。
元编程技术的分类理解了Meta层次的概念以后，我们再来总结一下元编程技术都有哪些分类。
第一，元编程可以通过生成语义层对象来生成程序。
当我们操纵M1层的程序时，我们通常需要透过M2层的对象来完成，比如读取类和方法的定义信息。类和方法就是M2层的对象。Java的注解功能和反射机制，就是通过读取和操纵M2层的对象来完成的。
在学习编译原理的过程中，你知道了类、方法这些都是语义层次的概念，编译器保证了编译后的程序在语义上的正确性，所以你可以大胆地使用这些信息，不容易出错。如果你要在运行时动态地调用方法，运行时也会提供一定的检查机制，减少出错的可能性。
第二，元编程可以通过生成AST来生成程序。
你同样知道，一个程序也可以用AST来表达。所以，我们能不能让程序直接读取、修改和生成AST呢？这样对AST的操纵，就等价于对程序的操纵。
答案是可以的。所有Lisp家族的语言都采用了这种元数据技术，Julia就是其中之一。Lisp语言可以用S表达式来表示程序。S表达式是那种括号嵌套括号的数据结构，其实就是一棵AST。你可以用宏来生成S表达式，也就是生成AST。
不过，让程序直接操作比较底层的数据结构，其代价是可能生成的AST不符合语义规则。毕竟，AST只表达了语法规则。所以，用这种方式做元编程需要小心一些，不要生成错误的程序。同时，这种元编程技术对程序员来说，学习的成本也更高，因为他们要在比较低的概念层次上工作。
第三，元编程可以通过文本字符串来生成程序。
当然，你还可以把程序表达成更加低端的格式，就是一些文本字符串而已。我们前面说过，C语言的宏，其实就是做字符串的替换。而一些脚本语言，通常也能接受一个文本字符串作为程序来运行，比如JavaScript的eval()函数就可以接受一个字符串作为参数，然后把字符串作为程序来运行。所以，在JavaScript里的一项非常灵活的功能，就是用程序生成一些字符串，然后用eval()函数来运行。当然你也能预料到，用越原始的模型来表示程序，出错的可能性就越大。所以有经验的程序员，都会很谨慎地使用类似eval()这样的功能。但无论如何，这也确实是一种元编程技术。
第四，元编程可以通过字节码操纵技术来生成字节码。
那么，除了通过生成语义层对象、AST和文本来生成程序以外，对于Java这种能够运行字节码的语言来说，你还可以通过字节码操纵技术来生成字节码。这种技术一般不是由语言本身提供的能力，而是由第三方工具来实现的，典型的就是Spring。
好，到这里，我们就探讨完了通过元编程技术由程序生成程序的各种方式。下面我们再通过另一个维度来讨论一下元编程技术。这个维度是元编程技术起作用的时机，我们可以据此分为静态元编程和动态元编程。
静态元编程技术只在编译期起作用。比如C++的模板技术和把Java注解技术用在编译期的情况（在下面会具体介绍这两种技术）。一旦编译完毕以后，元程序跟普通程序一样，都会变成机器码。
动态元编程技术会在运行期起作用。这方面的例子是Java的反射机制。你可以在运行期加载一个类，来查看它的名称、都有哪些方法，然后打印出来。而为了实现这种功能，Java程序必须在class文件里保存这个类的Model，比如符号表，并通过M2层的接口，来查询类的信息。Java程序能在运行期进行类型判断，也是基于同样的原理。
好，通过上面的介绍，我想你对元编程的概念应该有比较清晰的理解了。那接下来，我们就来看看不同语言具体实现元编程的方式，并且一起探讨下在这个过程中应该如何运用编译技术。
不同语言的元编程技术我们讨论的语言包括几大类，首先是Java，接着是Python和JavaScript这样的脚本语言，然后是Julia这样的Lisp语言，最后是C++的模板技术等一些很值得探讨的元编程技术。
Java的元编程技术在分析Java的编译器的时候，我们已经解析了它是如何处理注解的，注解就是一种元编程技术。在我们举的例子中，注解是在编译期就被处理掉了。
@Retention(RetentionPolicy.SOURCE) //注解用于编译期处理 @Target(ElementType.TYPE) //注解是针对类型的 public @interface HelloWorld { } 当时我们写了一个简单的注解处理程序，这个程序，能够获取被注解的代码的元数据（M1层的信息），比如类名称、方法名称等。这些元数据是由编译器提供的。然后，注解处理程序会基于这些元数据生成一个新的Java源代码，紧接着该源代码就会被编译器发现并编译掉。
通过这个分析，你会发现注解处理过程自始至终都借助了编译器提供的能力：先是通过编译器查询被注解的程序的元数据，然后生成的新程序也会被编译器编译掉。所以你能得出一个结论：所谓元编程，某种意义上就是由程序来调用编译器提供的能力。
刚刚我们探究的是在编译期使用元编程技术。那么在运行期，Java提供了反射机制，来动态地获取程序的元数据，并操纵程序的执行。
举个例子。假设你写了一个简单的ORM（Object-Relational Mapping）程序，能够把Java对象自动保存到数据库中。那么你就可以通过反射机制，来获取这个对象都有哪些属性，然后读取这些属性的值，并生成一个正确的SQL语句来完成对象的保存动作。比如，对于一个Person对象，ORM程序通过反射机制会得知它有name和country两个字段，再从对象里读取name和字段的值，就会生成类似"Insert into Person (name, age), values(“Richard”, “China”)"</description></item><item><title>36｜节点之海：怎么生成基于图的IR？</title><link>https://artisanbox.github.io/3/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/38/</guid><description>你好，我是宫文学。
从今天这节课开始，我们就要学习我们这门课的最后一个主题，也就是优化篇。
在前面的起步篇和进阶篇，我们基本上把编译器前端、后端和运行时的主要技术点都过了一遍。虽然到现在，我们语言支持的特性还不够丰富，但基本上都是工作量的问题了。当然，每个技术点我们还可以继续深挖下去，比如我们可以在类型计算中增加泛型计算的内容，可以把上两节课的垃圾收集算法改成更实用的版本，等等。在这个过程中，你还需要不断克服新冒出来的各种技术挑战。不过，基本上，你已经算入了门了，已经把主要的知识脉络都打通了。
而第三部分的内容，是我们整个知识体系中相对独立、相对完整的一部分，也是我们之前屡次提起过，但一直没有真正深化的内容，这就是优化。
优化是现代语言的编译器最重要的工作之一。像V8和其他JavaScript虚拟机的速度，比早期的JavaScript引擎提升了上百倍，让运行在浏览器中的应用可以具备强大的处理能力，这都是优化技术的功劳。
所以，在这第三部分，我会带你涉猎优化技术中的一些基础话题，让你能够理解优化是怎么回事，并能够上手真正做一些优化。
那在这第一节课，我会带你总体了解优化技术的作用、相关算法和所采用的数据结构。接着，我会介绍本课程所采用的一个行业内前沿的数据结构，基于图的IR，又叫节点之海，从而为后面具体的优化任务奠定一个基础。
那首先，我们先简单介绍一下与优化有关的背景知识。
有关优化的背景知识如果我要把优化的内容和算法都大致介绍一下，可能也需要好几节课的篇幅。不过，我在《编译原理之美》的第27节和28节，对优化算法的场景和分类，做了一些通俗的介绍。对于优化算法，特别是基于数据流分析的优化算法，也做了一些介绍。
而在《编译原理实战课》中，我在第14节、15节、21节、23节、24节分别涉及了Java、JavaScript、Julia和Go语言的编译器中的优化技术。所以，我这里就不重复那些内容了，只提炼几个要点，重点和你说一下优化的目标、分类、算法，以及数据结构，让你做好讨论优化技术的知识准备。
优化的目标优化工作最常见的目标，是提高代码运行的性能。在有些场景下，我们还会关注降低目标代码的大小、优化IO次数等其他方面。
优化工作的分类优化技术的种类非常多，我们很难用一个分类标准把各种优化工作都涵盖进去。但通常，我们会按照几个不同的维度来进行分类。
从优化算法的作用范围（或者空间维度）来说，可以分为局部优化（针对基本块的优化）、全局优化（针对整个函数）和过程间优化（多个函数一起统筹优化）。
从优化的时机（也就是时间维度）来说，我们在编译和运行的各个阶段都可以做优化。所以llvm的主要发起人Chris Lattner曾经发表了一篇论文，主题就是全生命周期优化。在编译期呢，编译器的前端就可以做优化，比如我们已经做过一些常数折叠工作。在后端也可以做一些优化，比如我们前面讲过的尾递归和尾调用优化。
但大部分优化是发生在前端和后端中间的过渡阶段，这个阶段有时候也被叫做中端。除了这些，还有运行时的优化。对于V8这种JIT的引擎，在运行时还可以收集程序运行时的一些统计信息，对程序做进一步的优化编译，在某些场景下，甚至比静态编译的效果还好。
优化的算法优化涉及的算法也有很多。比如，前面我们做常量折叠的时候，基本上遍历一下AST，进行属性计算就行了 ，但在做尾递归和尾调用优化的时候，我们就需要基于栈桢的知识对生成的汇编代码做调整，这里面就涉及到了一些优化的算法。但其中最有用的，则是控制流和数据流分析。
对于数据流分析，我们已经讲过不少了。那控制流分析是怎么回事呢？控制流分析的重点是分析程序跳转的模式，比如识别出来哪些是循环语句、哪些是条件分支语句等等，从而找到可以优化的地方。
比如，如果一个循环内部的变量，是跟循环无关的。那我们就可以把它提到循环外面，避免重复计算该变量的值，这种优化叫做“循环无关变量外提”。比如下面的示例程序中，变量c的值跟循环是无关的，所以我们就没必要每次循环都去计算它了。而要实现这种优化，需要优化算法把程序的控制流分析清楚。
function foo(a:number):number{ let b = 0; for (let i = 0; i&amp;lt; a; i++){ let c = a*a; //变量c的值与循环无关，导致重复计算！ b = i + c; } return b; } 优化算法所依托的数据结构针对中端的优化工作，我们最经常采用的数据结构是控制流图，也就是CFG。在生成汇编代码的时候，我们已经接触过控制流图了。当时我们把代码划分成一个个的基本块，每个基本块都保存一些汇编代码，基本块之间形成控制流的跳转。控制流图的数据结构用得很广泛，比如llvm编译器就是基于CFG的，这也意味着像C、C++、Rust、Julia这些基于llvm的语言都受益于CFG数据结构。另外，虽然Go语言并不是基于llvm编译器的，但也采用了CFG。
控制流图最大的优点，当然是能够非常清楚地显示出控制流来，也就是程序的全局结构。而我们做数据流分析的时候，通常也要基于这样一个控制流的大框架来进行。比如，我们在做变量活跃性分析的时候，就是先分析了在单个的基本块里的变量活跃性，然后再扩展到基于CFG，在多个基本块之间做数据流分析。
不过，虽然CFG的应用很普遍，但它并不是唯一用于优化的数据结构。特别是，像Java编译器Graal和JavaScript的V8引擎，都采用了另一种基于图的IR。不过构成这个图的节点并不是基本块。我在这节课后面会重点介绍这个数据结构，并且说明为什么采用这个数据结构的原因。
刚才我挑重点介绍了与优化有关的背景知识。不过，我用短短的篇幅浓缩了太多的干货，你可能会觉得过于抽象。所以，我还是举几个例子更加直观地说明一下与优化有关的知识点，借此我们也可以继续讨论下面关于IR的话题。
一个优化的例子我们先来看这个代码片段，这段代码中，x和y都被赋值成了a+b。
x = a + b y = a + b z = y - x 你用肉眼就能看出来，第二行代码是可以被优化的，因为x和y的值是一样的，所以在第二行代码中，我们就不需要再计算一遍a+b了，直接把x赋值给y就行。这种优化，叫做“公共子表达式删除（Common Subexpression Elimination）”：</description></item><item><title>37_云编程：云计算会如何改变编程模式？</title><link>https://artisanbox.github.io/6/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/37/</guid><description>上一讲中，我分享了当前3个技术发展趋势，以及其对编译技术的影响。今天我们把其中的云计算和编程模式、编译技术的之间的关系、前景再展开探讨一下。
总的来说，现在编写程序是越来越云化了，所以，我们简单地称作云编程就好了。
关于云编程，有很多有趣的问题：
1.编程本身是否也能上云？在云上编程会跟本地开发有什么不同？
2.如何编写云应用，来充分发挥云平台的能力？分为哪些不同的模式？
3.为什么编写云应用那么复杂？如何降低这些复杂度？云原生应用的开发平台，能否解决这些问题？
本节课，我就带你深入讨论这些问题，希望借此帮助你对编程和云计算技术的关系做一个梳理，促使你更好地利用云计算技术。
首先，来看看如何实现云上编程。
实现云上编程90年代初，我在大学学习编程，宿舍几个人合买了一台386电脑。那个时候，我记得自己不太喜欢微软提供的MFC编程框架，这和386电脑没有浮点运算器，编译起来比较慢有关，编译一次使用MFC框架的，C++程序的时间，足够我看一页报纸的了。
喜欢编程的人，为了获得流畅的性能，电脑配置总是很高，虽然这足以满足C/C++时代的编程需要，但进入Java时代后，因为应用结构越来越复杂，工程师们有时需要在笔记本或桌面电脑上，安装各种复杂的中间件，甚至还要安装数据库软件，这时，电脑的配置即便再高，也很难安装和配置好这么复杂的环境。那么到了云计算时代，挑战就更大了，比如，你能想象在电脑上安装Hadoop等软件，来做大数据功能的开发吗？
其实，编写一个小的应用还好，但现在的应用越来越复杂，所需的服务端资源越来越多。以我最近参与的一个项目为例，这个项目是采用微服务架构的一个企业应用，要想实现可扩展的性能、更好的功能复用，就要用到数据库、消息队列、容器服务、RPC服务、分布式事务服务、API服务等等很多基础设施，在自己的电脑上配置所有这些环境，是不大可能的。
因此，工程师们已经习惯于，在云上搭建开发和测试环境，这样，可以随需获取各种云端资源。
因为编程跟云的关系越发紧密，有些开发工具已经跟云平台有了一定的整合，方便开发者按需获取云端资源。比如，微软的Visual Studio支持直接使用Azure云上的资源。
再进一步，IDE本身也可以云化，我们可以把它叫做“云IDE”。你的电脑只负责代码编辑的工作，代码本身放在云上，编译过程以及所需的类库也放在云上。Visual Studio Code就具备UI和服务端分离的能力。还有一些服务商提供基于浏览器的IDE，也是实现了前后端的分离。
我认为，未来的IDE可能会越来越云化，因为云IDE有很多优势，能给你带来很多好处。
1.易于管理的编程环境
编程环境完全配置在云上，不用在本地配置各种依赖项。
这一点，会给编程教育这个领域，提供很大的帮助。因为，学习编程的人能够根据需要，打开不同的编程环境，立即投入学习。反之，如果要先做很多复杂的配置才能开始学习，学习热情就会减退，一些人也就因此止步了。
其实，在软件开发团队中，你经常会看到这样一个现象：新加入项目组的成员，要花很长的时间，才能把开发环境搭建起来。因为他们需要安装各种软件，开通各种账号等等。那么，如果是基于云IDE开发的，这些麻烦都可以省掉。
2.支持跨平台编程
有些编程所需要的环境，在本地很难配置，在云中开发就很简单。比如，可以用Windows电脑为Linux平台开发程序，甚至你可以在云上，为你的无人机开发程序，并下载到无人机上。
在为手机编程时，比较复杂的一项工作是，适配各种不同型号的手机。这时，你只需要通过云IDE，整合同样基于云的移动应用测试环境，就可以在成百上千种型号的手机上测试你的应用了。
3.更强的计算能力
有些软件的编译非常消耗CPU，比如，完整编译LLVM可能需要一两个小时，而充分利用服务器的资源可以让编译速度更快。如果你从事AI方面的开发，体会会更深，AI需要大量的算力，并且GPU和TPU都很昂贵，我们很难自己去搭建这样的开发环境。而基于云开发，你可以按需使用云上的GPU、TPU和CPU的计算能力。
4.有利于开发过程的管理
开发活动集中到云上以后，会有利于各种管理工作。比如，很多软件项目是外包开发的，那么你可以想象，基于云编程的平台，甲乙双方的项目管理者，都可以获得更多关于开发过程的大数据，也更容易做好源代码的保护。
5.更好的团队协作
越来越多的人已经习惯在网上编写文档，平心而论，线上文档工具并没有本地的Office软件功能强大，是什么因素让我们更加偏爱线上文档工具呢？就是它的协作功能。团队中的成员可以同时编辑一个文档，还可以方便地将这个文档在团队中分享。
而我比较希望见到这样的场景，那就是，程序员们可以基于同一个代码文件，进行点评和交互式的修改，这相当于基于云的结对编程，对于加强团队的知识分享、提升软件质量都会有好处。
基于上述几点，我个人猜测：编程这项工作，会越来越与云紧密结合。这样一来，不仅仅能方便地调取云端的资源，越来越多的编程环境也会迁移到云上。
既然提到了在云上编程的方式，那么接下来，我们从编译技术的视角，来探讨一下，如何编写能充分运用云计算强大威力的应用，这样，你会对云计算有一个更加全面的认知。
如何编写云应用？学习编译原理，你可能会有一个感受，那就是编程可以在不同的抽象层次上进行。也就是说，你可以通过抽象，把底层复杂的技术细节转换成上层简单的语义。
程序员最早是直接编写机器码，指令和寄存器都要直接用0101来表示。后来，冯·诺依曼的一个学生，发明了用助记符的方法（也就是汇编语言）简化机器码的编写。用汇编语言编程的时候，你仍然要使用指令和寄存器，但可以通过名称来引用，比如34讲中，用pushq %rbp这样的汇编指令来表示机器码0x55。这就增加了一个抽象层次，用名称代替了指令和寄存器的编码。
而高级语言出现后，我们不再直接访问寄存器，而是使用变量、过程和作用域，抽象程度进一步增加。
总结起来，就是我们使用的语言抽象程度越来越高，每一次抽象对下一层的复杂性做了屏蔽，因此使用起来越来越友好。而编译技术，则帮你一层层地还原这个抽象过程，重新转换成复杂的底层实现。
云计算的发展过程跟编译技术也很类似。云计算服务商们希望通过层层的抽象，来屏蔽底层的复杂性，让云计算变得更易用。
而且，通常来说，在较低的抽象层次上，你可以有更大的掌控力，而在更高的抽象层次上，则会获得更好的方便性。
虚拟机是人们最早使用云资源的方式，一台物理服务器可以分割成多个虚拟机。在需要的时候，可以创建同一个虚拟机镜像的多个实例，形成集群。因为虚拟机包含了一套完整的操作系统，所以占据空间比较大，启动一个实例的速度比较慢。
我们一般是通过编写脚本来管理软件的部署，每种软件的安装部署方式都不相同，系统管理的负担比较重。
最近几年，容器技术变得流行起来。容器技术可以用更轻量级的方式，分配和管理计算资源。一台物理服务器可以运行几十、上百个容器，启动新容器的速度也比虚拟机快了很多。
跟虚拟机模式相比，容器部署和管理软件模块的方式标准化了，我们通过Kubernetes这样的软件，编写配置文件来管理容器。从编译原理的角度出发，这些配置文件就是容器管理的DSL，它用标准化的方式，取代了原来对软件配置进行管理的各种脚本。
无服务器（Serverless）架构，或者叫做FaaS（Function as a Service），做了进一步的抽象。你只要把一个个功能写成函数，就能被平台调用，来完成Web服务、消息队列处理等工作。这些函数可能是运行在容器中的，通过Kubernetes管理的，并且按照一定的架构来协调各种服务功能。
但这些技术细节都不需要你关心，你会因此丧失一些掌控力，比如，你不能自己去生成很多个线程做并行计算。不过，也因为需要你关心的技术细节变少了，编程效率会提高很多。
上面三个层次，每一级都比上一级的抽象层次更高。就像编译技术中，高级语言比汇编语言简单一样，使用无服务架构要比直接使用虚拟机和容器更简单、更方便。
但即使到了FaaS这个层次，编写一个云应用仍然不是一件简单的事情，你还是要面临很多复杂性，比如，处理应用程序与大容量数据库的关系，实现跨公有云和私有云的应用等等。那么能否再进一步抽象并简化云应用的开发？是否能通过针对云原生应用的编程平台，来实现这个目标呢？
为了探究这个问题，我们需要进一步审视一下，现在云编程仍然有哪些，需要被新的抽象层次消除掉的复杂性。
对云原生编程平台的需求：能否解决云应用的复杂性？在《人月神话》里，作者把复杂性分为两种：
一种叫做本质复杂性（Essential Complexity），指的是你要解决的问题本身的复杂性，是无法避免的。 一种叫做附属复杂性（Accidental Complexity），是指我们在解决本质问题时，所采用的解决方案而引入的复杂性。在我们现在的系统中，90%的工作量都是用来解决附属复杂性的。 我经常会被问到这样的问题：做一个电商系统，成本是多少？而我给出的回答是：可能几千块，也可能很多亿。
如果你理解我的答案，那意味着比较理解当前软件编程的复杂性问题。因为软件系统的复杂性会随着规模急剧上升。
像阿里那样的电商系统，需要成千上万位工程师来维护。它在双11的时候，一天的成交量要达到几千亿，接受几亿用户的访问，在性能、可靠性、安全性、数据一致性等维度，都面临巨大的挑战。最重要的是，复杂性不是线性叠加的，可能是相乘的。
比如，当一个软件服务1万个用户的时候，增加一个功能可能需要100人天的话；针对服务于1百万用户的系统，增加同样的功能，可能需要几千到上万人天。同样的，如果功能不变，只是用户规模增加，你同样要花费很多人天来修改系统。那么你可以看出，整体的复杂性是多个因素相乘的结果，而不是简单相加。
这跟云计算的初衷是相悖的。云计算最早承诺，当我们需要更多计算资源的时候，简单增加一下就行了。然而，现有软件的架构，其实离这个目标还很远。那有没有可能把这些复杂性解耦，使得复杂性的增长变成线性或多项式级别（这里是借助算法复杂性的理论）的呢？
我再带你细化地看一下附属复杂性的一些构成，以便加深你对造成复杂性的根源的理解。
1.基础设施的复杂性
编写一个简单的程序，你只需要写写业务逻辑、处理少量数据，采用很简单的架构就行了。但是编写大型应用，你必须关心软件运行的基础设施，比如，你是用虚拟机还是容器？你还要关心很多技术构成部分，比如Kubernetes、队列、负载均衡器、网络、防火墙、服务发现、系统监控、安全、数据库、分片、各种优化，等等。
这些基础设施产生的复杂性，要花费你很多时间。像无服务器架构这样的技术，已经能够帮你屏蔽部分的复杂性，但还不够，仍然有很多复杂性因素需要找到解决方案。举个例子。
大多数商业应用都要很小心地处理跟数据库的关系，因为一旦数据出错（比如电商平台上的商品价格出错），就意味着重大的商业损失。你要根据应用需求设计数据库结构；要根据容量设计数据库分片的方案；要根据数据分析的需求设计数据仓库方案，以及对应的ETL程序。</description></item><item><title>37_什么时候会使用内部临时表？</title><link>https://artisanbox.github.io/1/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/37/</guid><description>今天是大年初二，在开始我们今天的学习之前，我要先和你道一声春节快乐！
在第16和第34篇文章中，我分别和你介绍了sort buffer、内存临时表和join buffer。这三个数据结构都是用来存放语句执行过程中的中间数据，以辅助SQL语句的执行的。其中，我们在排序的时候用到了sort buffer，在使用join语句的时候用到了join buffer。
然后，你可能会有这样的疑问，MySQL什么时候会使用内部临时表呢？
今天这篇文章，我就先给你举两个需要用到内部临时表的例子，来看看内部临时表是怎么工作的。然后，我们再来分析，什么情况下会使用内部临时表。
union 执行流程为了便于量化分析，我用下面的表t1来举例。
create table t1(id int primary key, a int, b int, index(a)); delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=1000)do insert into t1 values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 然后，我们执行下面这条语句：
(select 1000 as f) union (select id from t1 order by id desc limit 2); 这条语句用到了union，它的语义是，取这两个子查询结果的并集。并集的意思就是这两个集合加起来，重复的行只保留一行。
下图是这个语句的explain结果。
图1 union语句explain 结果可以看到：</description></item><item><title>37_从AST到IR：体会数据流和控制流思维</title><link>https://artisanbox.github.io/3/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/39/</guid><description>你好，我是宫文学。
在上一节课，我们已经初步认识了基于图的IR。那接下来，我们就直接动手来实现它，这需要我们修改之前的编译程序，基于AST来生成IR，然后再基于IR生成汇编代码。
过去，我们语言的编译器只有前端和后端。加上这种中间的IR来过渡以后，我们就可以基于这个IR添加很多优化算法，形成编译器的中端。这样，我们编译器的结构也就更加完整了。
今天这节课，我先带你熟悉这个IR，让你能够以数据流和控制流的思维模式来理解程序的运行逻辑。之后，我还会带你设计IR的数据结构，并介绍HIR、MIR和LIR的概念。最后，我们再来讨论如何基于AST生成IR，从而为基于IR做优化、生成汇编代码做好铺垫。
首先，我还是以上一节课的示例程序为础，介绍一下程序是如何基于这个IR来运行的，加深你对控制流和数据流的理解。
理解基于图的运行逻辑下面是上节课用到的示例程序，一个带有if语句的函数，它能够比较充分地展示数据流和控制流的特点：
function foo(a:number, b:number):number{ let x:number; if (a&amp;gt;10){ x = a + b; } else{ x = a - b; } return x; } 我们把这个程序转化成图，是这样的：
我们之前说了，这个图能够忠实地反映源代码的逻辑。那如果程序是基于这个图来解释执行的，它应该如何运行呢？我们来分析一下。
第1步，从start节点进入程序。
第2步，程序顺着控制流，遇到if节点，并且要在if节点这里产生分支。但为了确定如何产生分支，if节点需要从数据流中获取一个值，这个值是由“&amp;gt;”运算符节点提供的。所以，“a&amp;gt;10”这个表达式，必须要在if节点之前运行完毕，来产生if节点需要的值。
第3步，我们假设a&amp;gt;10返回的是true，那么控制流就会走最左边的分支，也就是if块，直到这个块运行结束。而如果返回的是false，那么就走右边的分支，也就是else块，直到这个块运行结束。这里，if块和else块都是以Begin节点开始，以End节点结束。如果块中有if或for循环这样导致控制流变化的语句，那么它们对应的控制流就会出现在Begin和End之间，作为子图。
第4步，在if块或else执行结束后，控制流又会汇聚到一起。所以图中这里就出现了一个Merge节点。这个节点把两个分支的End节点作为输入，这样我们就能知道实际程序执行的时候，是从哪个分支过来的。
第5步，控制流到达Return节点。Return节点需要返回x的值，所以这就要求数据流必须在Return之前把x的值提供出来。那到底是x1的值，还是x2的值呢？这需要由Phi节点来确定。而Phi节点会从控制流的Merge节点获取控制流路径的信息，决定到底采用x1还是x2。
最后，return语句会把所获取的x值返回，程序结束。
在我这个叙述过程中，你有没有发现一个重要的特点，就是程序的控制流和数据流是相对独立的，只是在个别地方有交互。这跟我们平常写程序的思维方式是很不一样的。在写程序的时候，我们是把数据流与控制流混合在一起的，不加以区分。
比如，针对当前我们的示例程序，我们的源代码里一个if语句，然后在if块和else块中分别写一些代码。这似乎意味着，只能在进入if块的时候，才运行x1=a+b的代码，而在进入else块的时候，才可以运行x2=a-b的逻辑。
但如果你把数据流和控制流分开来思考，你会发现，其实我们在任何时候都可以去计算x1和x2的值，只要在return语句之前计算完就行。比如说，你可以把x1和x2的计算挪到if语句前面去，相当于把程序改成下面的样子：
function foo(a:number, b:number):number{ x1 = a + b; x2 = a - b; if (a&amp;gt;10){ x = x1; } else{ x = x2; } return x; } 当然，针对我们现在的例子，把x1和x2提前计算并没有什么好处，反倒增加了计算量。我的用意在于说明，其实数据流和控制流之间可以不必耦合得那么紧，可以相对独立。
我们可以用这种思想再来分析下我们上节课提到的几个优化技术。</description></item><item><title>37_从内核到应用：网络数据在内核中如何流转</title><link>https://artisanbox.github.io/9/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/37/</guid><description>你好，我是 LMOS。
上节课我们对一次请求到响应的过程积累了一些宏观认识，相信你已经对整个网络架构有了一个整体蓝图。这节课，让我们来仔细研究一下网络数据是如何在内核中流转的，让你开阔视野，真正理解底层工程的实现思路。
凡事先问目的，在网络数据在内核中的流转，最终要服务于网络收发功能。所以，我会先带你了解一次具体的网络发收过程，然后带你了解lwIP的网络数据收发。有了这些基础，我还会示范一下如何实现协议栈移植，你可以在课后自行动手拓展。
好，让我们正式开始今天的学习吧。课程配套代码，你可以点击这里获取。
先看看一次具体的网络发收过程理解软件的设计思想，最重要的是先要理解需求。而内核中的数据流转也只是为了满足网络收发的需求而进行的设计。
发送过程总览下面我们一起来看看应用程序通过网络发送数据的全过程。
应用程序首先会准备好数据，调用用户态下的库函数。接着调用系统API接口函数，进入到内核态。
内核态对应的系统服务函数会复制应用程序的数据到内核的内存空间中，然后将数据移交给网络协议栈，在网络协议栈中将数据层层打包。
最后，包装好的数据会交给网卡驱动，网卡驱动程序负责将打包好的数据写入网卡并让其发送出去。
我为你准备了一张流程图供你参考，如下所示。
上图中，只是展示了大致流程，其中还有DMA处理、CRC校验、出错处理等细节，但对于我们理解梳理发送流程，这些就够了。
接收过程总览了解了发送数据的过程以后，掌握接收数据的过程就更容易了，因为它就是发送数据的逆过程。
首先，网卡接收到数据，通过DMA复制到指定的内存，接着发送中断，以便通知网卡驱动，由网卡驱动处理中断复制数据。然后网络协议收到网卡驱动传过来的数据，层层解包，获取真正的有效数据。最后，这个数据会发送给用户态监听的应用进程。
为了让你更加直观地了解这一过程，我特意准备了一张流程图供你参考，如下所示。
前面只是帮你梳理一下数据的发送与接收的流程，其实我们真正要关注的是网络协议。可是我们若手动实现一个完整的网络协议，不太现实，网络协议的复杂度大到也许要重新开一门课程，才可以完全解决，所以下面我们借用一下lwIP项目，以这个为基础来讨论网络协议。
认识一下lwIP架构现在我们清楚了一次具体网络发收过程是怎么回事，那怎么让Cosmos实现网络通信呢？这里我们选择lwIP这个TCP/IP协议的轻量级开源项目，让它成为Cosmos的网络部的战略合作伙伴。
lwIP是由瑞典计算机科学研究院（SICS）的Adam Dunkels开发的小型开源TCP/IP协议栈。它是一个用C语言实现的软件组件，一共有两套接口层，向下是操作系统要提供的，向上是提供给应用程序的。这样lwIP就能嵌入到任何操作系统之中工作，并为这个操作系统上的应用软件提供网络功能支持了。
为啥说lwIP是轻量级的呢？很简单，跟Linux比，从代码行数上就能看得出。lwIP的设计目标就是尽量用少量资源消耗，实现一个相对完整的TCP/IP协议栈。
这里的“完整性”主要是指TCP协议的完整性，实现的关键点就是在保持TCP协议主要功能的基础上减少对RAM的占用。同时，lwIP还支持IPv6的标准实现，这也让我们与现代交换设备的对接变得非常方便。
这里额外提供你一份扩展阅读资料，lwIP的项目主页链接，这里包含了大量相关资料，感兴趣的同学可以课后深入了解。另外，lwIP既可以移植到操作系统上运行，也可以在没有操作系统的情况下独立运行。
lwIP在结构上可分为四层：OS层、API层、核心层、硬件驱动层，如下图所示。
第一层
MCU的业务层是lwIP的服务对象，也是其自身代码使用lwIP的地方。大部分时候我们都是从这里入手，通过netconn或lwip_api使用lwIP的各种功能函数。
在典型的TCP通信的客户端应用程序中，一般先要通过netconn_new创建一个struct netconn对象，然后调用netconn_connect连接到服务器，并返回成功或失败。成功后，可以调用netconn_write向服务器发送数据，也可以调用netconn_recv接收数据。最后，关闭连接并通过netconn_close释放资源。
第二层
lwIP的api层是netconn的功能代码所在的层，负责为上层代码提供netconn的api。习惯使用socket的同学也可以使用lwip_socket等函数，以标准的socket方式调用lwIP。新版本增加了http、mqtt等应用的代码，这些额外的应用对目前的物联网通信来说确实很方便。
第三层
lwIP的核心层存放了TCP/IP协议栈的核心代码，它不仅实现了大部分的TCP和UDP功能，还实现了DNS、ICMP、IGMP等协议，同时也实现了内存管理和网络接口功能。
该层提供了sys_arch模块设计，便于将lwIP移植到不同的操作系统，如线程创建、信号量、消息队列等功能。和操作系统相关的真正定义写在了lwip/include/sys.h文件中。
第四层
硬件驱动层提供PHY芯片驱动，用来匹配lwIP的使用。lwIP会调用该层的代码将组装好的数据包发送到网络，同时从网络接收数据包并进行分析，实现通信功能。
lwIP的三套应用程序编程接口理清了架构，我们再说一说lwIP的应用程序编程接口，一共有三套。
原始API：原始的lwIP API。它通过事件回调机制开发应用程序。该应用编程接口提供了最佳的性能和优化的代码长度，但它增加了应用程序开发的复杂性。
Netconn API：是高级的有序API、需要实时操作系统（RTOS）的支持（提供进程间通信的方法）。Netconn API支持多线程。
BSD套接字API：类似伯克利的套接字API（在Netconn API上开发，需要注意NETCONN API 即为 Sequential API）。
对于以上三种接口，前者只需要裸机调用，后两种需要操作系统调用。因此，移植lwIP有两种方法，一种是只移植内核，不过这样之后只能基于RAW/Callback API编写应用程序。第二种是移植内核和上层API。这时应用程序编程可以使用三种API，即RAW/Callback API、顺序API和Socket API。
lwIP执行流程现在，想必聪明的你已经理解了前文中的网络收发过程。
接下来，让我们顺着之前的思路来对应到lwIP在收发过程中的核心函数，具体过程我同样梳理了流程图。你可以结合图里关键的函数名以及步骤顺序，按这个顺序在IwIP代码中检索阅读。
数据发送首先要说的是数据发送过程。
由于我们把lwIP作为Cosmos的一个内核组件来工作，自然要由lwIP接收来自内核上层发来的数据。内核上层首先会调用lwIP的netconn层的接口函数netconn_write，通过这个函数，数据正式流进lwIP组件层。
接着，netconn层调用lwIP组件的TCP层的接口函数tcp_write，在TCP层对数据首次进行打包。然后，TCP层将打包好的数据通过调用io_output函数，向下传递给lwIP组件的IP层，进行打包。
最后，IP层将打包好的数据发送给网卡驱动接口层netif，这里调用了实际的网卡驱动程序，将数据发送出去。
数据接收数据接收的步骤相比数据发送稍微多一些，但也不用害怕，跟住我的讲解思路一定可以理清这个过程。
数据接收需要应用程序首先调用lwIP的netconn层的netconn_recv接口。然后由netconn层调用sys_arch_mbox_fetch函数，进入监听等待相关的mbox。
接着，数据会进入网卡，驱动程序相关的函数负责把它复制到内存。再然后是调用ethernet_input函数，进入ethernet层。完成相关处理后，调用ip4_input函数，数据在lwIP组件的IP层对数据解包，进行相应处理之后，还会调用tcp_input函数，进入lwIP组件的TCP层对数据解包。
最后，调用sys_mbox_trypost函数把数据放入特定的mbox，也就是消息盒子里，这样等待监听的应用程序就能得到数据了。
在了解了lwIP组件收发数据的过程之后，就可以进行移植的相关工作了。lwIP的结构设计非常优秀，这让移植工作变得很容易。我们这里只要了解lwIP组件的sys_arch层的接口函数即可。
下面我们一起了解lwIP的移植细节。
协议栈移植lwIP有两种移植模式，一种是NO_SYS，无操作系统模式，一种是有操作系统模式。用NO_SYS模式比较简单，你可以自行探索。
操作系统模式主要需要基于操作系统的 IPC 机制，对网络连接进行了抽象（信号量、邮箱/队列、互斥体等机制），从而保证内核与应用层API的通讯，这样做的好处是lwIP 内核线程可以只负责数据包的 TCP/IP 封装和拆封，而不用进行数据的应用层处理，从而极大地提高系统对网络数据包的处理效率。</description></item><item><title>37_贪心算法：如何用贪心算法实现Huffman压缩编码？</title><link>https://artisanbox.github.io/2/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/38/</guid><description>基础的数据结构和算法我们基本上学完了，接下来几节，我会讲几种更加基本的算法。它们分别是贪心算法、分治算法、回溯算法、动态规划。更加确切地说，它们应该是算法思想，并不是具体的算法，常用来指导我们设计具体的算法和编码等。
贪心、分治、回溯、动态规划这4个算法思想，原理解释起来都很简单，但是要真正掌握且灵活应用，并不是件容易的事情。所以，接下来的这4个算法思想的讲解，我依旧不会长篇大论地去讲理论，而是结合具体的问题，让你自己感受这些算法是怎么工作的，是如何解决问题的，带你在问题中体会这些算法的本质。我觉得，这比单纯记忆原理和定义要更有价值。
今天，我们先来学习一下贪心算法（greedy algorithm）。贪心算法有很多经典的应用，比如霍夫曼编码（Huffman Coding）、Prim和Kruskal最小生成树算法、还有Dijkstra单源最短路径算法。最小生成树算法和最短路径算法我们后面会讲到，所以我们今天讲下霍夫曼编码，看看它是如何利用贪心算法来实现对数据压缩编码，有效节省数据存储空间的。
如何理解“贪心算法”？关于贪心算法，我们先看一个例子。
假设我们有一个可以容纳100kg物品的背包，可以装各种物品。我们有以下5种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？
实际上，这个问题很简单，我估计你一下子就能想出来，没错，我们只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆，所以，我们可以往背包里装20kg黑豆、30kg绿豆、50kg红豆。
这个问题的解决思路显而易见，它本质上借助的就是贪心算法。结合这个例子，我总结一下贪心算法解决问题的步骤，我们一起来看看。
第一步，当我们看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。
类比到刚刚的例子，限制值就是重量不能超过100kg，期望值就是物品的总价值。这组数据就是5种豆子。我们从中选出一部分，满足重量不超过100kg，并且总价值最大。
第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。
类比到刚刚的例子，我们每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。
第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。
实际上，用贪心算法解决问题的思路，并不总能给出最优解。
我来举一个例子。在一个有权图中，我们从顶点S开始，找一条到顶点T的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点T。按照这种思路，我们求出的最短路径是S-&amp;gt;A-&amp;gt;E-&amp;gt;T，路径长度是1+4+4=9。
但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径S-&amp;gt;B-&amp;gt;D-&amp;gt;T才是最短路径，因为这条路径的长度是2+2+2=6。为什么贪心算法在这个问题上不工作了呢？
在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果我们第一步从顶点S走到顶点A，那接下来面对的顶点和边，跟第一步从顶点S走到顶点B，是完全不同的。所以，即便我们第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。
贪心算法实战分析对于贪心算法，你是不是还有点懵？如果死抠理论的话，确实很难理解透彻。掌握贪心算法的关键是多练习。只要多练习几道题，自然就有感觉了。所以，我带着你分析几个具体的例子，帮助你深入理解贪心算法。
1.分糖果我们有m个糖果和n个孩子。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m&amp;lt;n），所以糖果只能分配给一部分孩子。
每个糖果的大小不等，这m个糖果的大小分别是s1，s2，s3，……，sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这n个孩子对糖果大小的需求分别是g1，g2，g3，……，gn。
我的问题是，如何分配糖果，能尽可能满足最多数量的孩子？
我们可以把这个问题抽象成，从n个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数（期望值）是最大的。这个问题的限制值就是糖果个数m。
我们现在来看看如何用贪心算法来解决。对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。
我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。
2.钱币找零这个问题在我们的日常生活中更加普遍。假设我们有1元、2元、5元、10元、20元、50元、100元这些面额的纸币，它们的张数分别是c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付K元，最少要用多少张纸币呢？
在生活中，我们肯定是先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用1元来补齐。
在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。直觉告诉我们，这种处理方法就是最好的。实际上，要严谨地证明这种贪心算法的正确性，需要比较复杂的、有技巧的数学推导，我不建议你花太多时间在上面，不过如果感兴趣的话，可以自己去研究下。
3.区间覆盖假设我们有n个区间，区间的起始端点和结束端点分别是[l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。我们从这n个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？
这个问题的处理思路稍微不是那么好懂，不过，我建议你最好能弄懂，因为这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。
这个问题的解决思路是这样的：我们假设这n个区间中最左端点是lmin，最右端点是rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将[lmin, rmax]覆盖上。我们按照起始端点从小到大的顺序对这n个区间排序。
我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。
解答开篇今天的内容就讲完了，我们现在来看开篇的问题，如何用贪心算法实现霍夫曼编码？
假设我有一个包含1000个字符的文件，每个字符占1个byte（1byte=8bits），存储这1000个字符就一共需要8000bits，那有没有更加节省空间的存储方式呢？
假设我们通过统计分析发现，这1000个字符中只包含6种不同字符，假设它们分别是a、b、c、d、e、f。而3个二进制位（bit）就可以表示8个不同的字符，所以，为了尽量减少存储空间，每个字符我们用3个二进制位来表示。那存储这1000个字符只需要3000bits就可以了，比原来的存储方式节省了很多空间。不过，还有没有更加节省空间的存储方式呢？
a(000)、b(001)、c(010)、d(011)、e(100)、f(101) 霍夫曼编码就要登场了。霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在20%～90%之间。
霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。
对于等长的编码来说，我们解压缩起来很简单。比如刚才那个例子中，我们用3个bit表示一个字符。在解压缩的时候，我们每次从文本中读取3位二进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取1位还是2位、3位等等来解压缩呢？这个问题就导致霍夫曼编码解压缩起来比较复杂。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。
假设这6个字符出现的频率从高到低依次是a、b、c、d、e、f。我们把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀，在解压缩的时候，我们每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这1000个字符只需要2100bits就可以了。
尽管霍夫曼编码的思想并不难理解，但是如何根据字符出现频率的不同，给不同的字符进行不同长度的编码呢？这里的处理稍微有些技巧。
我们把每个字符看作一个节点，并且附带着把频率放到优先级队列中。我们从队列中取出频率最小的两个节点A、B，然后新建一个节点C，把频率设置为两个节点的频率之和，并把这个新节点C作为节点A、B的父节点。最后再把C节点放入到优先级队列中。重复这个过程，直到队列中没有数据。
现在，我们给每一条边加上画一个权值，指向左子节点的边我们统统标记为0，指向右子节点的边，我们统统标记为1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。
内容小结今天我们学习了贪心算法。
实际上，贪心算法适用的场景比较有限。这种算法思想更多的是指导设计基础算法。比如最小生成树算法、单源最短路径算法，这些算法都用到了贪心算法。从我个人的学习经验来讲，不要刻意去记忆贪心算法的原理，多练习才是最有效的学习方法。
贪心算法的最难的一块是如何将要解决的问题抽象成贪心算法模型，只要这一步搞定之后，贪心算法的编码一般都很简单。贪心算法解决问题的正确性虽然很多时候都看起来是显而易见的，但是要严谨地证明算法能够得到最优解，并不是件容易的事。所以，很多时候，我们只需要多举几个例子，看一下贪心算法的解决方案是否真的能得到最优解就可以了。
课后思考 在一个非负整数a中，我们希望从中移除k个数字，让剩下的数字值最小，如何选择移除哪k个数字呢？
假设有n个人等待被服务，但是服务窗口只有一个，每个人需要被服务的时间长度是不同的，如何安排被服务的先后顺序，才能让这n个人总的等待时间最短？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>37_高级特性（二）：揭秘泛型编程的实现机制</title><link>https://artisanbox.github.io/7/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/37/</guid><description>你好，我是宫文学。
对泛型的支持，是现代语言中的一个重要特性。它能有效地降低程序员编程的工作量，避免重复造轮子，写很多雷同的代码。像C++、Java、Scala、Kotlin、Swift和Julia这些语言都支持泛型。至于Go语言，它的开发团队也对泛型技术方案讨论了很久，并可能会在2021年的版本中正式支持泛型。可见，泛型真的是成为各种强类型语言的必备特性了。
那么，泛型有哪些特点？在设计和实现上有哪些不同的方案？编译器应该进行什么样的配合呢？今天这一讲，我就带你一起探讨泛型的实现原理，借此加深你对编译原理相关知识点的认知，让你能够在自己的编程中更好地使用泛型技术。
首先，我们来了解一下什么是泛型。
什么是泛型？在日常编程中，我们经常会遇到一些代码逻辑，它们除了类型不同，其他逻辑是完全一样的。你可以看一下这段示例代码，里面有两个类，其中一个类是保存Integer的列表，另一个类是保存Student对象的列表。
public class IntegerList{ List data = new ArrayList(); public void add(Integer elem){ data.add(elem); } public Integer get(int index){ return (Integer) data.get(index); } } public class StudentList{ List data = new ArrayList(); public void add(Student elem){ data.add(elem); } public Student get(int index){ return (Student) data.get(index); } } 我们都知道，程序员是很不喜欢重复的代码的。像上面这样的代码，如果要为每种类型都重新写一遍，简直会把人逼疯！
泛型的典型用途是针对集合类型，能够更简单地保存各种类型的数据，比如List、Map这些。在Java语言里，如果用通用的集合类来保存特定类型的对象，就要做很多强制转换工作。而且，我们还要小心地做类型检查。比如：
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;List strList = new ArrayList(); //字符串列表 strList.add(&amp;quot;Richard&amp;quot;); String name = (String)strList.get(i); //类型转换 for (Object obj in strList){ String str = (String)obj; //类型转换 &amp;hellip; }</description></item><item><title>37_高速缓存（上）：“4毫秒”究竟值多少钱？</title><link>https://artisanbox.github.io/4/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/37/</guid><description>在这一节内容开始之前，我们先来看一个3行的小程序。你可以猜一猜，这个程序里的循环1和循环2，运行所花费的时间会差多少？你可以先思考几分钟，然后再看我下面的解释。
int[] arr = new int[64 * 1024 * 1024]; // 循环1 for (int i = 0; i &amp;lt; arr.length; i++) arr[i] *= 3;
// 循环2 for (int i = 0; i &amp;lt; arr.length; i += 16) arr[i] *= 3 在这段Java程序中，我们首先构造了一个64×1024×1024大小的整型数组。在循环1里，我们遍历整个数组，将数组中每一项的值变成了原来的3倍；在循环2里，我们每隔16个索引访问一个数组元素，将这一项的值变成了原来的3倍。
按道理来说，循环2只访问循环1中1/16的数组元素，只进行了循环1中1/16的乘法计算，那循环2花费的时间应该是循环1的1/16左右。但是实际上，循环1在我的电脑上运行需要50毫秒，循环2只需要46毫秒。这两个循环花费时间之差在15%之内。
为什么会有这15%的差异呢？这和我们今天要讲的CPU Cache有关。之前我们看到了内存和硬盘之间存在的巨大性能差异。在CPU眼里，内存也慢得不行。于是，聪明的工程师们就在CPU里面嵌入了CPU Cache（高速缓存），来解决这一问题。
我们为什么需要高速缓存?按照摩尔定律，CPU的访问速度每18个月便会翻一番，相当于每年增长60%。内存的访问速度虽然也在不断增长，却远没有这么快，每年只增长7%左右。而这两个增长速度的差异，使得CPU性能和内存访问性能的差距不断拉大。到今天来看，一次内存的访问，大约需要120个CPU Cycle，这也意味着，在今天，CPU和内存的访问速度已经有了120倍的差距。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;如果拿我们现实生活来打个比方的话，CPU的速度好比风驰电掣的高铁，每小时350公里，然而，它却只能等着旁边腿脚不太灵便的老太太，也就是内存，以每小时3公里的速度缓慢步行。因为CPU需要执行的指令、需要访问的数据，都在这个速度不到自己1%的内存里。
随着时间变迁，CPU和内存之间的性能差距越来越大为了弥补两者之间的性能差异，我们能真实地把CPU的性能提升用起来，而不是让它在那儿空转，我们在现代CPU中引入了高速缓存。
从CPU Cache被加入到现有的CPU里开始，内存中的指令、数据，会被加载到L1-L3 Cache中，而不是直接由CPU访问内存去拿。在95%的情况下，CPU都只需要访问L1-L3 Cache，从里面读取指令和数据，而无需访问内存。要注意的是，这里我们说的CPU Cache或者L1/L3 Cache，不是一个单纯的、概念上的缓存（比如之前我们说的拿内存作为硬盘的缓存），而是指特定的由SRAM组成的物理芯片。
这里是一张Intel CPU的放大照片。这里面大片的长方形芯片，就是这个CPU使用的20MB的L3 Cache。
现代CPU中大量的空间已经被SRAM占据，图中用红色框出的部分就是CPU的L3 Cache芯片在这一讲一开始的程序里，运行程序的时间主要花在了将对应的数据从内存中读取出来，加载到CPU Cache里。CPU从内存中读取数据到CPU Cache的过程中，是一小块一小块来读取数据的，而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在CPU Cache里面，我们把它叫作Cache Line（缓存块）。
在我们日常使用的Intel服务器或者PC里，Cache Line的大小通常是64字节。而在上面的循环2里面，我们每隔16个整型数计算一次，16个整型数正好是64个字节。于是，循环1和循环2，需要把同样数量的Cache Line数据从内存中读取到CPU Cache中，最终两个程序花费的时间就差别不大了。</description></item><item><title>38_从单排到团战：详解操作系统的宏观网络架构</title><link>https://artisanbox.github.io/9/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/38/</guid><description>你好，我是 LMOS。
上节课我们学习了单机状态下网络数据在内核中流转的全过程，并且带你一起梳理了网络栈移植的关键步骤。
这节课我会带你看看，现实世界中网络请求是如何穿过重重网络设备，实现大规模组网的。同时，我还会给你讲解网络架构的过去、现在，并展望一下将来的发展趋势。最后我会带你动手搭建一个现代互联网实验环境，通过实际的组网实践加深对网络架构的理解。
从传统网络架构聊起你是否好奇过，我们目前用的互联网是如何做到互联互通的呢？
让我们先来看看传统的三层网络架构，著名的通信设备厂商思科把这种架构叫做分级的互联网络模型（Hierarchical Inter-networking Model）。这种架构的优点是，可以把复杂的网络设计问题抽象为几个层面来解决，每个层面又聚焦于某些特定的功能。这样就能把复杂而庞大的网络问题拆解成比较好解决的子问题。
如下图所示，三层网络架构设计主要包括核心层、汇聚层、接入层这三个层。下面我分别给你说一说。
首先是核心层。交换层的核心交换机为进出数据中心的数据包提供高速转发的功能，为多个汇聚层提供连通性，同时也为整个网络提供灵活的L3路由网络。
然后是汇聚层。汇聚交换机与接入交换机相连，提供防火墙、SSL卸载、入侵检测、网络分析等其他服务。
最后我们来看接入层。接入交换机通常位于机架的顶部，因此它们也被称为ToR交换机，并且它们与服务器物理连接。
当然，观察这个架构我们可以发现，核心层和汇聚层这种骨干网络需要承担的流量是蛮大的，流量大意味着对交换性能、效率有更高的要求。所以为了解决性能、效率等问题，我们需要在OSI的1、2、3层上分别做优化。
这里要说到传统网络架构的不足之处，我们发现经典的IP网络是逐跳转发数据的。转发数据时，每台路由器都要根据包头的目的地址查询路由表，以获得下一跳的出口。这个过程显然是繁琐低效的。
另外，转发路径也不够灵活，为了加以改善，我们在第二层之上、第三层之下引入一个2.5层的技术方案，即多协议标签交换（MPLS）技术。
优化与迭代：MPLS技术目前MPLS技术在国内应用广泛，无论是BAT等互联网巨头，还是运营商建设骨干网都在应用这种技术。MPLS的核心结构如下。
MPLS通过LDP标签分发协议。我来举个例子吧，这相当于把快递标签“贴在”了快递盒子上了，后续只需要读取标签，就能知道这个数据要转发到哪里去了。这样就避免了传统路由网络中每路过一个经手人（每一跳），都要把快递盒子打开看一看的额外开销。
而路径计算元素协议（RSVP-TE）最大的优点是收集整个网络的拓扑和链路状态信息。通过扩展的资源预留协议，可以实现灵活的转发路径选择和规划。这就好比双十一了，物流公司根据物流大数据收集到的路网和拥堵状态等信息，自动规划出性价比最高的路径，显然快递配送效率会得到很大提升。
当然，只在OSI的2、3层之间做优化是远远不够的，为了满足动辄数百G传输需求，物理层也经历了从DWDM（Dense Wavelength Division Multiplexing）波分复用系统这种波分复用技术到OTN（Iptical Transport Network，光传送网）的技术演进。感兴趣的同学可以搜索光传送网和波分复用相关的资料，这里我就不展开了。
根据前面的讲解我们发现，传统网络基础架构确实可以解决不少问题，但这样真的完美了么？其实不然，比如前面的MPLS技术虽然也解决了问题，但也加重了耦合，并且存在资源利用率低、复杂度高、价格昂贵等缺点。
所以后来SR（Segment Routing）技术又应运而生，而随着IPv6的演进，我们用SRv6替代MPLS技术也是大势所趋。
另外，我们还要注意到业务需求的变化。比如随着云与5G等移动通信的发展，流量除了以前客户端和服务端的南北向通信之外，服务端分布式服务之间也会引入了大量的通信流量。甚至随着云与容器的演进，服务端会存在大量的虚拟机迁移等动作。这些对传统网络中STP拓扑变化、收敛以及网络规模都带来了巨大的挑战。
那么如何解决传统三层网络架构带来的挑战呢？答案其实在贝尔实验室的Charles Clos博士在1953年的《无阻塞交换网络研究》之中。论文中提到的核心思想是：用多个小规模、低成本的单元，构建复杂、大规模的网络。
论文中提到的简单的CLOW网络是包含输入级别、中间级别和输出级别的三级互连体系结构。
下图中的矩形表示规模较小的转发单元，其成本显然也相对较低。CLOS的本质可以简单理解为是一种多级交换的架构思想，并且这种架构很适合在输入和输出持续增加的情况下将中间交叉数降至最低。
下图中，m是每个子模块的输入端口数，n是每个子模块的输出端口数，r是每一级的子模块数，经过合理的重排，只要满足公式：
r2≥max(m1,n3) 那么，对于任意的输入到输出，总是能找到一条无阻塞的通路。
直到1990年代，CLOS架构被应用到Switch Fabric。应用CLOS架构的交换机的开关密度，与交换机端口数量N的关系如下。
O(N^(3/2)) 可以看到，在N较大时，CLOS模型能降低交换机内部的开关密度。由此可见，越来越多的人发现了传统三层网络架构下的痛点，于是一种叫做胖树的网络架构应运而生（感兴趣的同学可以在搜索《A Scalable, Commodity Data Center Network Architecture》这篇论文）。
而借鉴Fattree和CLOS模型的思想，目前业界衍生出了叶脊（Spine-Leaf）网络架构。目前通过FaceBook、Google等公司大量实践的事实已经证明，Spine-Leaf网络架构可以提供高带宽、低延迟、非阻塞、可扩展的服务器到服务器连接。
这种新一代架构在工程实践中的代表之一，则正是Google的B4网络，接下来就让我们一起看一下Google B4网络的架构。
谈谈Google B4Google的研究员Amin Vahdat曾经说过：“如果没有软件定义网络，那Google就不会是今天的Google。”
为了实现实现数据中心的互联互通，谷歌设计并搭建了B4网络，实现了数据在各个公司园区之间的实时复制。
B4网络的核心架构由Google设计的控制软件和白盒交换机构成。谷歌的目标是建立一个类似于广域网的镜像网络，随着网络规模的不断扩展，目前谷歌的大部分业务都已经运行在B4上了。
接下来让我们来看一下Google Google B4的架构图（下面4张图出自Google B4网络论文）：
B4网络的其实也是由三层构成，但这个和传统网络的“三层架构”又不太一样。这里指的是物理设备层（Switch Hardware）、局部网络控制层（Site Controllers）和全局控制层（Global）。
全局控制层中的SDN网关和TE服务器会在全局进行统一控制，而每个数据中心（Site）则会通过Site Controller来控制物理交换机，从而实现将网络的控制面和数据面分离的效果。
第一层：物理设备层我们首先来看第一层的物理交换设备，它是Google自研并请ODM厂商代工的白盒交换机。这个自研的交换机使用了24颗16×10Gb的芯片，还携带了128个10Gb网口。
交换机里面运行的是OpenFlow协议。但众所周知，交换机内的专用芯片从研发设计到最终流片其实周期和成本还是很高的。
那如何让专用的交换机芯片跟OpenFlow更好地进行协同呢？为了解决这个问题，Google采用了TTP方案。实际运行时交换机则会把像访问控制列表（ACL）、路由表、隧道表之类的关键数据通过BGP/IS-IS协议报文送到Controller，由Controller进行处理。
第二层：局部网络控制层B4网络中，一个Controller服务可以控制多个交换机。而为了保证可用性，一个交换机是可以连接多个Controller服务的，而同一时间只会有一个Controller服务为这台交换机提供服务，并且一个数据中心中会包含由多个Controller服务实例构成的服务集群。
在局部网络控制层中，还会使用Paxos协议负责所有控制功能的领导者（leader）选举。
具体过程是这样的，每个节点上的Paxos实例对给定控制功能的可用副本集做应用程序级别的健康检测。当大多数的Paxos实例检测到故障时，他们就会从剩余的可用服务器集中选出一个新的负责人。然后，Paxos会将递增的ID号回调给当选的leader。leader使用这个ID来向客户表明自己的身份。
第三层全局控制层（Global）负责全局控制的TE Server通过SDN Gateway从各个数据中心的控制器收集链路信息，从而掌握路径状态。这些路径以IP-In-IP隧道的方式创建，通过SDN网关到达Onix控制器，最后下达到交换机。</description></item><item><title>38_元编程：一边写程序，一边写语言</title><link>https://artisanbox.github.io/6/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/38/</guid><description>今天，我再带你讨论一个很有趣的话题：元编程。把这个话题放在这一篇的压轴位置，也暗示了这个话题的重要性。
我估计很多同学会觉得元编程（Meta Programming）很神秘。编程，你不陌生，但什么是元编程呢？
元编程是这样一种技术：你可以让计算机程序来操纵程序，也就是说，用程序修改或生成程序。另一种说法是，具有元编程能力的语言，能够把程序当做数据来处理，从而让程序产生程序。
而元编程也有传统编程所不具备的好处：比如，可以用更简单的编码来实现某个功能，以及可以按需产生、完成某个功能的代码，从而让系统更有灵活性。
某种意义上，元编程让程序员拥有了语言设计者的一些权力。是不是很酷？你甚至可以说，普通程序员自己写程序，文艺程序员让程序写程序。
那么本节课，我会带你通过实际的例子，详细地来理解什么是元编程，然后探讨带有元编程能力的语言的特性，以及与编译技术的关系。通过这样的讨论，我希望你能理解元编程的思维，并利用编译技术和元编程的思维，提升自己的编程水平。
从Lisp语言了解元编程说起元编程，追溯源头，应该追到Lisp语言。这门语言其实没有复杂的语法结构，仅有的语法结构就是一个个函数嵌套的调用，就像下面的表达式，其中“+”和“*”也是函数，并不是其他语言中的操作符：
(+ 2 (* 3 5)) //对2和3求和，这里+是一个函数，并不是操作符 你会发现，如果解析Lisp语言形成AST，是特别简单的事情，基本上括号嵌套的结构，就是AST的树状结构（其实，你让Antlr打印输出AST的时候，它缺省就是按照Lisp的格式输出的，括号嵌套括号）。这也是Lisp容易支持元编程的根本原因，你实际上可以通过程序来生成，或修改AST。
我采用了Common Lisp的一个实现，叫做SBCL。在macOS下，你可以用“brew install sbcl”来安装它；而在Windows平台，你需要到sbcl.org去下载安装。在命令行输入sbcl，就可以进入它的REPL，你可以试着输入刚才的代码运行一下。
在Lisp中，你可以把(+ 2 (* 3 5))看做一段代码，也可以看做是一个列表数据。所以，你可以生成这样一组数据，然后作为代码执行。这就是Lisp的宏功能。
我们通过一个例子来看一下，宏跟普通的函数有什么不同。下面两段代码分别是用Java和Common Lisp写的，都是求一组数据的最大值。
Java版本：
public static int max(int[] numbers) { int rtn = numbers[0]; for (int i = 1;i &amp;lt; numbers.length; i++){ if (numbers[i] &amp;gt; rtn) rtn = numbers[i]; } return rtn; } Common Lisp版本：
(defun mymax1 (list) (let ((rtn (first list))) ;让rtn等于list的第一个元素 (do ((i 1 (1+ i))) ;做一个循环，让i从1开始，每次加1 ((&amp;gt;= i (length list)) rtn) ;循环终止条件：i&amp;gt;=list的长度 (when (&amp;gt; (nth i list) rtn) ;如果list的第i个元素 &amp;gt; rtn (setf rtn (nth i list)))))) ;让rtn等于list的第i个元素 那么，如果写一个函数，去求一组数据的最小值，你该怎么做呢？采用普通的编程方法，你会重写一个函数，里面大部分代码都跟求最大值的代码一样，只是把其中的一个“&amp;gt;”改为"</description></item><item><title>38_分治算法：谈一谈大规模计算框架MapReduce中的分治思想</title><link>https://artisanbox.github.io/2/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/39/</guid><description>MapReduce是Google大数据处理的三驾马车之一，另外两个是GFS和Bigtable。它在倒排索引、PageRank计算、网页分析等搜索引擎相关的技术中都有大量的应用。
尽管开发一个MapReduce看起来很高深，感觉跟我们遥不可及。实际上，万变不离其宗，它的本质就是我们今天要学的这种算法思想，分治算法。
如何理解分治算法？为什么说MapRedue的本质就是分治算法呢？我们先来看，什么是分治算法？
分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成n个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。
这个定义看起来有点类似递归的定义。关于分治和递归的区别，我们在排序（下）的时候讲过，分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：
分解：将原问题分解成一系列子问题；
解决：递归地求解各个子问题，若子问题足够小，则直接求解；
合并：将子问题的结果合并成原问题。
分治算法能解决的问题，一般需要满足下面这几个条件：
原问题与分解成的小问题具有相同的模式；
原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；
具有分解终止条件，也就是说，当问题足够小时，可以直接求解；
可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。
分治算法应用举例分析理解分治算法的原理并不难，但是要想灵活应用并不容易。所以，接下来，我会带你用分治算法解决我们在讲排序的时候涉及的一个问题，加深你对分治算法的理解。
还记得我们在排序算法里讲的数据的有序度、逆序度的概念吗？我当时讲到，我们用有序度来表示一组数据的有序程度，用逆序度表示一组数据的无序程度。
假设我们有n个数据，我们期望数据从小到大排列，那完全有序的数据的有序度就是n(n-1)/2，逆序度等于0；相反，倒序排列的数据的有序度就是0，逆序度是n(n-1)/2。除了这两种极端情况外，我们通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。
我现在的问题是，如何编程求出一组数据的有序对个数或者逆序对个数呢？因为有序对个数和逆序对个数的求解方式是类似的，所以你可以只思考逆序对个数的求解方法。
最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的k值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是O(n^2)。那有没有更加高效的处理方法呢？
我们用分治算法来试试。我们套用分治的思想来求数组A的逆序对个数。我们可以将数组分成前后两半A1和A2，分别计算A1和A2的逆序对个数K1和K2，然后再计算A1与A2之间的逆序对个数K3。那数组A的逆序对个数就等于K1+K2+K3。
我们前面讲过，使用分治算法其中一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了。那回到这个问题，如何快速计算出两个子问题A1与A2之间的逆序对个数呢？
这里就要借助归并排序算法了。你可以先试着想想，如何借助归并排序算法来解决呢？
归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。实际上，在这个合并的过程中，我们就可以计算这两个小数组的逆序对个数了。每次合并操作，我们都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。
尽管我画了张图来解释，但是我个人觉得，对于工程师来说，看代码肯定更好理解一些，所以我们把这个过程翻译成了代码，你可以结合着图和文字描述一起看下。
private int num = 0; // 全局变量或者成员变量 public int count(int[] a, int n) { num = 0; mergeSortCounting(a, 0, n-1); return num; }
private void mergeSortCounting(int[] a, int p, int r) { if (p &amp;gt;= r) return; int q = (p+r)/2; mergeSortCounting(a, p, q); mergeSortCounting(a, q+1, r); merge(a, p, q, r); }</description></item><item><title>38_综合实现（一）：如何实现面向对象编程？</title><link>https://artisanbox.github.io/7/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/38/</guid><description>你好，我是宫文学。
从20世纪90年代起，面向对象编程的范式逐渐成为了主流。目前流行度比较高的几种语言，比如Java、JavaScript、Go、C++和Python等，都支持面向对象编程。
那么，为了支持面向对象编程，我们需要在语言的设计上，以及编译器和运行时的实现上，考虑到哪些问题呢？
这一讲，我就带你来探讨一下如何在一门语言里支持面向对象特性。这是一个很综合的话题，会涉及很多的知识点，所以很有助于帮你梳理和贯通与编译原理有关的知识。
那么，我们就先来分析一下，面向对象特性都包括哪些内容。
面向对象语言的特性日常中，虽然我们经常会使用面向对象的语言，但如果要问，到底什么才是面向对象？我们通常会说得含含糊糊。最常见的情况，就是会拿自己所熟悉的某种语言的面向对象特性，想当然地认为这就是面向对象语言的全部特性。
不过，在我们的课程里，我想从计算机语言设计的角度，带你重新梳理和认识一下面向对象的编程语言，把面向对象按照清晰的逻辑解构，这样也便于讨论它的实现策略。在这个过程中，你可能会对面向对象产生新的认识。
特征1：对象面向对象编程语言的核心，是把世界看成了一个个的对象，比如汽车、动物等。这些对象包含了数据和代码。数据被叫做字段或属性，而代码通常又被叫做是方法。
此外，这些对象之间还会有一定的关系。比如，汽车是由轮子、发动机等构成的，这叫做聚合关系。而某个班级会有一个班主任，那么班级和作为班主任的老师之间，会有一种引用关系。
对象之间还可以互相发送消息。比如，司机会“通知”汽车，让它加速或者减速。在面向对象的语言中，这通常是通过方法调用来实现的。但也并不局限于这种方式，比如对象之间还可以通过异步的消息进行互相通讯，不过一般的编程语言都没有原生支持这种通讯方式。我们在讨论Actor模式的时候，曾经提到过Actor之间互相通讯的方式，就有点像对象之间互发消息。
特征2：类和类型体系很多面向对象的语言都是基于类（class）的，并且类也是一种自定义的类型。这个类型是对象的模板。而对象呢，则是类的实例。我们还可以再印证一下，前面在探究元编程的实现机制时，学过的Meta层次的概念。对象属于M0层，而类属于M1层，它为对象制定了一个标准，也就是对象中都包含了什么数据和方法。
其实，面向对象的语言并不一定需要类这个概念，这个概念更多是来自于类型理论，而非面向对象的语言一样可以支持类型和子类型。类型的好处主要是针对静态编译的语言的，因为这样就可以通过类型，来限制可以访问的对象属性和方法，从而减少程序的错误。
而有些面向对象的语言，比如JavaScript并没有类的概念。也有的像Python，虽然有类的概念，但你可以随时修改对象的属性和方法。
特征3：重用–继承（Inheritance）和组合（Composition）在软件工程里，我们总是希望能重用已有的功能。像Java、C++这样的语言，能够让子类型重用父类型的一些数据和逻辑，这叫做继承。比如Animal有speak()方法，Cat是Animal的子类，那么Cat就可以继承这个speak()方法。Cat也可以重新写一个方法，把父类的方法覆盖掉，让叫声更像猫叫。
不过，并不是所有的面向对象编程语言都喜欢通过继承的方式来实现重用。你在网上可以找到很多文章，都在分析继承模式的缺陷。像Go语言，采用的是组合方式来实现重用。在这里，我引用了一篇文章中的例子。在这个例子中，作者首先定义了一个author的结构体，并给这个结构体定义了一些方法：
type author struct { //结构体：author(作者) firstName string //作者的名称 lastName string bio string //作者简介 } func (a author) fullName() string { //author的方法：获取全名 return fmt.Sprintf(&amp;quot;%s %s&amp;quot;, a.firstName, a.lastName) }
type post struct { //结构体：文章 title string //文章标题 content string //文章内容 author //文章作者 }
func (p post) details() { //文章的方法：获取文章的详细内容。 fmt.Println(&amp;quot;Title: &amp;quot;, p.title) fmt.Println(&amp;quot;Content: &amp;quot;, p.content) fmt.Println(&amp;quot;Author: &amp;quot;, p.</description></item><item><title>38_都说InnoDB好，那还要不要使用Memory引擎？</title><link>https://artisanbox.github.io/1/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/38/</guid><description>我在上一篇文章末尾留给你的问题是：两个group by 语句都用了order by null，为什么使用内存临时表得到的语句结果里，0这个值在最后一行；而使用磁盘临时表得到的结果里，0这个值在第一行？
今天我们就来看看，出现这个问题的原因吧。
内存表的数据组织结构为了便于分析，我来把这个问题简化一下，假设有以下的两张表t1 和 t2，其中表t1使用Memory 引擎， 表t2使用InnoDB引擎。
create table t1(id int primary key, c int) engine=Memory; create table t2(id int primary key, c int) engine=innodb; insert into t1 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); insert into t2 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); 然后，我分别执行select * from t1和select * from t2。
图1 两个查询结果-0的位置可以看到，内存表t1的返回结果里面0在最后一行，而InnoDB表t2的返回结果里0在第一行。
出现这个区别的原因，要从这两个引擎的主键索引的组织方式说起。
表t2用的是InnoDB引擎，它的主键索引id的组织方式，你已经很熟悉了：InnoDB表的数据就放在主键索引树上，主键索引是B+树。所以表t2的数据组织方式如下图所示：
图2 表t2的数据组织主键索引上的值是有序存储的。在执行select *的时候，就会按照叶子节点从左到右扫描，所以得到的结果里，0就出现在第一行。
与InnoDB引擎不同，Memory引擎的数据和索引是分开的。我们来看一下表t1中的数据内容。
图3 表t1 的数据组织可以看到，内存表的数据部分以数组的方式单独存放，而主键id索引里，存的是每个数据的位置。主键id是hash索引，可以看到索引上的key并不是有序的。
在内存表t1中，当我执行select *的时候，走的是全表扫描，也就是顺序扫描这个数组。因此，0就是最后一个被读到，并放入结果集的数据。
可见，InnoDB和Memory引擎的数据组织方式是不同的：
InnoDB引擎把数据放在主键索引上，其他索引上保存的是主键id。这种方式，我们称之为索引组织表（Index Organizied Table）。 而Memory引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为堆组织表（Heap Organizied Table）。 从中我们可以看出，这两个引擎的一些典型不同：
InnoDB表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的；</description></item><item><title>38_高速缓存（下）：你确定你的数据更新了么？</title><link>https://artisanbox.github.io/4/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/38/</guid><description>在我工作的十几年里，写了很多Java的程序。同时，我也面试过大量的Java工程师。对于一些表示自己深入了解和擅长多线程的同学，我经常会问这样一个面试题：“volatile这个关键字有什么作用？”如果你或者你的朋友写过Java程序，不妨来一起试着回答一下这个问题。
就我面试过的工程师而言，即使是工作了多年的Java工程师，也很少有人能准确说出volatile这个关键字的含义。这里面最常见的理解错误有两个，一个是把volatile当成一种锁机制，认为给变量加上了volatile，就好像是给函数加了sychronized关键字一样，不同的线程对于特定变量的访问会去加锁；另一个是把volatile当成一种原子化的操作机制，认为加了volatile之后，对于一个变量的自增的操作就会变成原子性的了。
// 一种错误的理解，是把volatile关键词，当成是一个锁，可以把long/double这样的数的操作自动加锁 private volatile long synchronizedValue = 0; // 另一种错误的理解，是把volatile关键词，当成可以让整数自增的操作也变成原子性的 private volatile int atomicInt = 0; amoticInt++; 事实上，这两种理解都是完全错误的。很多工程师容易把volatile关键字，当成和锁或者数据数据原子性相关的知识点。而实际上，volatile关键字的最核心知识点，要关系到Java内存模型（JMM，Java Memory Model）上。
虽然JMM只是Java虚拟机这个进程级虚拟机里的一个内存模型，但是这个内存模型，和计算机组成里的CPU、高速缓存和主内存组合在一起的硬件体系非常相似。理解了JMM，可以让你很容易理解计算机组成里CPU、高速缓存和主内存之间的关系。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;“隐身”的变量我们先来一起看一段Java程序。这是一段经典的volatile代码，来自知名的Java开发者网站dzone.com，后续我们会修改这段代码来进行各种小实验。
public class VolatileTest { private static volatile int COUNTER = 0;
public static void main(String[] args) { new ChangeListener().start(); new ChangeMaker().start(); } static class ChangeListener extends Thread { @Override public void run() { int threadValue = COUNTER; while ( threadValue &amp;amp;lt; 5){ if( threadValue!</description></item><item><title>38｜中端优化第1关：实现多种本地优化</title><link>https://artisanbox.github.io/3/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/40/</guid><description>你好，我是宫文学。
上一节课，我们设计了IR的数据结构，并且分析了如何从AST生成IR。并且，这些IR还可以生成.dot文件，以直观的图形化的方式显示出来。
不过，我们上一节课只分析了if语句，这还远远不够。这节课，我会先带你分析for循环语句，加深对你控制流和数据流的理解。接着，我们就会开始享受这个IR带来的红利，用它来完成一些基本的本地优化工作，包括公共子表达式删除、拷贝传播和死代码删除，让你初步体会基于IR做优化的感觉。
那么，我们先接着上一节课，继续把for循环从AST转换成IR。
把For循环转换成IR同样地，我们还是借助一个例子来做分析。这个例子是一个实现累加功能的函数，bar函数接受一个参数a，然后返回从1到a的累加值。
function bar(a:number):number{ let sum:number = 0; for(let i = 1; i &amp;lt;= a; i++){ sum = sum + i; } return sum; } 这里，我先直接画出最后生成的IR图的样子：
你一看这个图，肯定会觉得有点眼花缭乱，摸不清头绪。不过没关系，这里面是有着清晰的逻辑的。
第一步，我们先来看控制流的部分。
在程序开头的时候，依然还是一个Start节点。
而下面的LoopBegin节点，则代表了整个for循环语句的开始。开始后，它会根据for循环的条件，确定是否进入循环体。这里，我们引入了一个If节点，来代表循环条件。If节点要依据一个if条件，所以这里有一条黑线指向一个条件表达式节点。
当循环条件为true的时候，程序就进入循环体。循环体以Begin开头，以LoopEnd结尾。而当循环条件为false的时候，程序则要通过LoopExit来退出循环。最后再通过Return语句从函数中返回。
并且，LoopEnd和LoopExit各自都有一条输入边，连接到LoopBegin。这样，循环的开始和结束就能正确地配对，不至于搞混。
不过，你可能注意到了一个现象，Start节点的后序节点并不马上是循环的开始LoopBegin。为什么呢？因为其实有两条控制流能够到达LoopBegin：一条是从程序开始的上方进去，另一条是在每次循环结束以后，又重新开始循环。所以LoopBegin相当于我们上一节见过的Merge节点，两条控制流在这里汇聚。而我们在控制流中，如果用一条蓝线往下连接其他节点，只适用于单一控制流和流程分叉的情况，不包括流程汇聚的情况。我们上节课也说过，每个ControlNode最多只有一个前序节点。
那控制流的部分就说清楚了。第二步，我们就来看一下数据流。
在数据流中，我们需要计算i和sum这两个变量。我们先看i：
function bar(a:number):number{ let sum1:number = 0; for(let i1 = 1; i &amp;lt;= a; i2 = i + 1){ sum2 = sum + i; } return sum; } 这里，变量i被静态赋值了两次。一开始被赋值为1，后来又通过i++来递增。为了符合SSA格式，我们要把它拆分成i1和i2两个变量，然后再用Phi节点把它们聚合起来，用于循环条件的判断。
我们把与i有关的数据流加入到图中，就是下面这样：
我再解释一下这张图。i1=1这个表达式，在刚进入循环时被触发，一次循环结束后，会触发i2 = i + 1。所以，在i&amp;lt;=a这个条件中的i，在刚进入循环的时候，会选择i1；而在循环体中循环过一次以后，会选择i2。因此，我们图中这个phi节点有一条输入边指向LoopBegin，用于判断控制流到底是从上面那条边进入的，还是从LoopEnd返回的。</description></item><item><title>39_MESI协议：如何让多核CPU的高速缓存保持一致？</title><link>https://artisanbox.github.io/4/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/39/</guid><description>你平时用的电脑，应该都是多核的CPU。多核CPU有很多好处，其中最重要的一个就是，它使得我们在不能提升CPU的主频之后，找到了另一种提升CPU吞吐率的办法。
不知道上一讲的内容你还记得多少？上一节，我们讲到，多核CPU里的每一个CPU核，都有独立的属于自己的L1 Cache和L2 Cache。多个CPU之间，只是共用L3 Cache和主内存。
我们说，CPU Cache解决的是内存访问速度和CPU的速度差距太大的问题。而多核CPU提供的是，在主频难以提升的时候，通过增加CPU核心来提升CPU的吞吐率的办法。我们把多核和CPU Cache两者一结合，就给我们带来了一个新的挑战。因为CPU的每个核各有各的缓存，互相之间的操作又是各自独立的，就会带来缓存一致性（Cache Coherence）的问题。
缓存一致性问题那什么是缓存一致性呢？我们拿一个有两个核心的CPU，来看一下。你可以看这里这张图，我们结合图来说。
在这两个CPU核心里，1号核心要写一个数据到内存里。这个怎么理解呢？我拿一个例子来给你解释。
比方说，iPhone降价了，我们要把iPhone最新的价格更新到内存里。为了性能问题，它采用了上一讲我们说的写回策略，先把数据写入到L2 Cache里面，然后把Cache Block标记成脏的。这个时候，数据其实并没有被同步到L3 Cache或者主内存里。1号核心希望在这个Cache Block要被交换出去的时候，数据才写入到主内存里。
如果我们的CPU只有1号核心这一个CPU核，那这其实是没有问题的。不过，我们旁边还有一个2号核心呢！这个时候，2号核心尝试从内存里面去读取iPhone的价格，结果读到的是一个错误的价格。这是因为，iPhone的价格刚刚被1号核心更新过。但是这个更新的信息，只出现在1号核心的L2 Cache里，而没有出现在2号核心的L2 Cache或者主内存里面。这个问题，就是所谓的缓存一致性问题，1号核心和2号核心的缓存，在这个时候是不一致的。
为了解决这个缓存不一致的问题，我们就需要有一种机制，来同步两个不同核心里面的缓存数据。那这样的机制需要满足什么条件呢？我觉得能够做到下面两点就是合理的。
第一点叫写传播（Write Propagation）。写传播是说，在一个CPU核心里，我们的Cache数据更新，必须能够传播到其他的对应节点的Cache Line里。
第二点叫事务的串行化（Transaction Serialization），事务串行化是说，我们在一个CPU核心里面的读取和写入，在其他的节点看起来，顺序是一样的。
第一点写传播很容易理解。既然我们数据写完了，自然要同步到其他CPU核的Cache里。但是第二点事务的串行化，可能没那么好理解，我这里仔细解释一下。
我们还拿刚才修改iPhone的价格来解释。这一次，我们找一个有4个核心的CPU。1号核心呢，先把iPhone的价格改成了5000块。差不多在同一个时间，2号核心把iPhone的价格改成了6000块。这里两个修改，都会传播到3号核心和4号核心。
然而这里有个问题，3号核心先收到了2号核心的写传播，再收到1号核心的写传播。所以3号核心看到的iPhone价格是先变成了6000块，再变成了5000块。而4号核心呢，是反过来的，先看到变成了5000块，再变成6000块。虽然写传播是做到了，但是各个Cache里面的数据，是不一致的。
事实上，我们需要的是，从1号到4号核心，都能看到相同顺序的数据变化。比如说，都是先变成了5000块，再变成了6000块。这样，我们才能称之为实现了事务的串行化。
事务的串行化，不仅仅是缓存一致性中所必须的。比如，我们平时所用到的系统当中，最需要保障事务串行化的就是数据库。多个不同的连接去访问数据库的时候，我们必须保障事务的串行化，做不到事务的串行化的数据库，根本没法作为可靠的商业数据库来使用。
而在CPU Cache里做到事务串行化，需要做到两点，第一点是一个CPU核心对于数据的操作，需要同步通信给到其他CPU核心。第二点是，如果两个CPU核心里有同一个数据的Cache，那么对于这个Cache数据的更新，需要有一个“锁”的概念。只有拿到了对应Cache Block的“锁”之后，才能进行对应的数据更新。接下来，我们就看看实现了这两个机制的MESI协议。
总线嗅探机制和MESI协议要解决缓存一致性问题，首先要解决的是多个CPU核心之间的数据传播问题。最常见的一种解决方案呢，叫作总线嗅探（Bus Snooping）。这个名字听起来，你多半会很陌生，但是其实特很好理解。
这个策略，本质上就是把所有的读写请求都通过总线（Bus）广播给所有的CPU核心，然后让各个核心去“嗅探”这些请求，再根据本地的情况进行响应。
总线本身就是一个特别适合广播进行数据传输的机制，所以总线嗅探这个办法也是我们日常使用的Intel CPU进行缓存一致性处理的解决方案。关于总线这个知识点，我们会放在后面的I/O部分更深入地进行讲解，这里你只需要了解就可以了。
基于总线嗅探机制，其实还可以分成很多种不同的缓存一致性协议。不过其中最常用的，就是今天我们要讲的MESI协议。和很多现代的CPU技术一样，MESI协议也是在Pentium时代，被引入到Intel CPU中的。
MESI协议，是一种叫作写失效（Write Invalidate）的协议。在写失效协议里，只有一个CPU核心负责写入数据，其他的核心，只是同步读取到这个写入。在这个CPU核心写入Cache之后，它会去广播一个“失效”请求告诉所有其他的CPU核心。其他的CPU核心，只是去判断自己是否也有一个“失效”版本的Cache Block，然后把这个也标记成失效的就好了。
相对于写失效协议，还有一种叫作写广播（Write Broadcast）的协议。在那个协议里，一个写入请求广播到所有的CPU核心，同时更新各个核心里的Cache。
写广播在实现上自然很简单，但是写广播需要占用更多的总线带宽。写失效只需要告诉其他的CPU核心，哪一个内存地址的缓存失效了，但是写广播还需要把对应的数据传输给其他CPU核心。
MESI协议的由来呢，来自于我们对Cache Line的四个不同的标记，分别是：
M：代表已修改（Modified） E：代表独占（Exclusive） S：代表共享（Shared） I：代表已失效（Invalidated） 我们先来看看“已修改”和“已失效”，这两个状态比较容易理解。所谓的“已修改”，就是我们上一讲所说的“脏”的Cache Block。Cache Block里面的内容我们已经更新过了，但是还没有写回到主内存里面。而所谓的“已失效“，自然是这个Cache Block里面的数据已经失效了，我们不可以相信这个Cache Block里面的数据。
然后，我们再来看“独占”和“共享”这两个状态。这就是MESI协议的精华所在了。无论是独占状态还是共享状态，缓存里面的数据都是“干净”的。这个“干净”，自然对应的是前面所说的“脏”的，也就是说，这个时候，Cache Block里面的数据和主内存里面的数据是一致的。
那么“独占”和“共享”这两个状态的差别在哪里呢？这个差别就在于，在独占状态下，对应的Cache Line只加载到了当前CPU核所拥有的Cache里。其他的CPU核，并没有加载对应的数据到自己的Cache里。这个时候，如果要向独占的Cache Block写入数据，我们可以自由地写入数据，而不需要告知其他CPU核。
在独占状态下的数据，如果收到了一个来自于总线的读取对应缓存的请求，它就会变成共享状态。这个共享状态是因为，这个时候，另外一个CPU核心，也把对应的Cache Block，从内存里面加载到了自己的Cache里来。
而在共享状态下，因为同样的数据在多个CPU核心的Cache里都有。所以，当我们想要更新Cache里面的数据的时候，不能直接修改，而是要先向所有的其他CPU核心广播一个请求，要求先把其他CPU核心里面的Cache，都变成无效的状态，然后再更新当前Cache里面的数据。这个广播操作，一般叫作RFO（Request For Ownership），也就是获取当前对应Cache Block数据的所有权。</description></item><item><title>39_回溯算法：从电影《蝴蝶效应》中学习回溯算法的核心思想</title><link>https://artisanbox.github.io/2/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/40/</guid><description>我们在第31节提到，深度优先搜索算法利用的是回溯算法思想。这个算法思想非常简单，但是应用却非常广泛。它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。
除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1背包、图的着色、旅行商问题、全排列等等。既然应用如此广泛，我们今天就来学习一下这个算法思想，看看它是如何指导我们解决问题的。
如何理解“回溯算法”？在我们的一生中，会遇到很多重要的岔路口。在岔路口上，每个选择都会影响我们今后的人生。有的人在每个岔路口都能做出最正确的选择，最后生活、事业都达到了一个很高的高度；而有的人一路选错，最后碌碌无为。如果人生可以量化，那如何才能在岔路口做出最正确的选择，让自己的人生“最优”呢？
我们可以借助前面学过的贪心算法，在每次面对岔路口的时候，都做出看起来最优的选择，期望这一组选择可以使得我们的人生达到“最优”。但是，我们前面也讲过，贪心算法并不一定能得到最优解。那有没有什么办法能得到最优解呢？
2004年上映了一部非常著名的电影《蝴蝶效应》，讲的就是主人公为了达到自己的目标，一直通过回溯的方法，回到童年，在关键的岔路口，重新做选择。当然，这只是科幻电影，我们的人生是无法倒退的，但是这其中蕴含的思想其实就是回溯算法。
笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。
回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。
理论的东西还是过于抽象，老规矩，我还是举例说明一下。我举一个经典的回溯例子，我想你可能已经猜到了，那就是八皇后问题。
我们有一个8x8的棋盘，希望往里放8个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。你可以看我画的图，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。
我们把这个问题划分成8个阶段，依次将8个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前放法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种放法，继续尝试。
回溯算法非常适合用递归代码实现，所以，我把八皇后的算法翻译成代码。我在代码里添加了详细的注释，你可以对比着看下。如果你之前没有接触过八皇后问题，建议你自己用熟悉的编程语言实现一遍，这对你理解回溯思想非常有帮助。
int[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列 public void cal8queens(int row) { // 调用方式：cal8queens(0); if (row == 8) { // 8个棋子都放置好了，打印结果 printQueens(result); return; // 8行棋子都放好了，已经没法再往下递归了，所以就return } for (int column = 0; column &amp;lt; 8; ++column) { // 每一行都有8中放法 if (isOk(row, column)) { // 有些放法不满足要求 result[row] = column; // 第row行的棋子放到了column列 cal8queens(row+1); // 考察下一行 } } } private boolean isOk(int row, int column) {//判断row行column列放置是否合适 int leftup = column - 1, rightup = column + 1; for (int i = row-1; i &amp;gt;= 0; &amp;ndash;i) { // 逐行往上考察每一行 if (result[i] == column) return false; // 第i行的column列有棋子吗？ if (leftup &amp;gt;= 0) { // 考察左上对角线：第i行leftup列有棋子吗？ if (result[i] == leftup) return false; } if (rightup &amp;lt; 8) { // 考察右上对角线：第i行rightup列有棋子吗？ if (result[i] == rightup) return false; } &amp;ndash;leftup; ++rightup; } return true; }</description></item><item><title>39_瞧一瞧Linux：详解socket实现与网络编程接口</title><link>https://artisanbox.github.io/9/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/39/</guid><description>你好，我是LMOS。
前面我们了解了网络的宏观架构，建立了网络模块知识的大局观，也进行了实际的组网实践。现在我们来瞧一瞧Linux的网络程序，不过想要入门Linux的网络编程，套接字也是一个绕不开的重要知识点，正是有了套接字，Linux系统才拥有了网络通信的能力。而且网络协议的最底层也是套接字，有了这个基础，你再去看相关的网络协议的时候也会更加轻松。
我会通过两节课的内容带你了解套接字的原理和具体实现。这节课，我们先来了解套接字的作用、工作原理和关键数据结构。下一节课，我们再一起研究它在Linux内核中的设计与实现。
好，让我们开始今天的学习吧。
如何理解套接字根据底层网络机制的差异，计算机网络世界中定义了不同协议族的套接字（socket），比如DARPA Internet地址（Internet套接字）、本地节点的路径名（Unix套接字）、CCITT X.25地址（X.25 套接字）等。
我们会重点讲解跟网络子系统和TCP/IP协议栈息息相关的一种套接字——Internet 套接字。如果你对其他类型的套接字有兴趣，可以自行阅读这里的资料。
Internet套接字是TCP/IP协议栈中传输层协议的接口，也是传输层以上所有协议的实现。
同时，套接字接口在网络程序功能中是内核与应用层之间的接口。TCP/IP协议栈的所有数据和控制功能都来自于套接字接口，与OSI网络分层模型相比，TCP/IP协议栈本身在传输层以上就不包含任何其他协议。
在Linux操作系统中，替代传输层以上协议实体的标准接口，称为套接字，它负责实现传输层以上所有的功能，可以说套接字是TCP/IP协议栈对外的窗口。
Linux套接字API适合所有的应用标准，现在的应用层协议也全部移植到了Linux系统中。但请你注意，在套接字层下的基础体系结构实现却是Linux系统独有的，Linux内核支持的套接字结构如图所示。
我们创建套接字时，可以通过参数选择协议族，为应用程序指定不同的网络机制。如果指定为PF_INET协议族，这里的套接字就叫做INET套接字，它的套接字接口函数提供了TCP/IP网络服务功能。现在我先带你了解一下套接字的数据结构。
套接字的数据结构在Linux操作系统下，对套接字、套接字的属性、套接字传输的数据格式还有管理套接字连接状态的数据结构分别做了一系列抽象定义。
每个程序使用的套接字都有一个struct socket数据结构与struct sock数据结构的实例。
Linux内核在套接字层定义了包含套接字通用属性的数据结构，分别是struct socket与struct sock，它们独立于具体协议；而具体的协议族与协议实例继承了通用套接字的属性，加入协议相关属性，就形成了管理协议本身套接字的结构。
struct socket数据结构struct socket是套接字结构类型，每个套接字在内核中都对应唯一的struct socket结构（用户程序通过唯一的套接字描述符来表示套接字，且描述符与struct socket结构一一对应）。
我们来看看struct socket数据结构是什么样，代码如下，我相信配合注释你有能力理解它。
struct socket { socket_state state; // 套接字的状态 unsigned long flags; // 套接字的设置标志。存放套接字等待缓冲区的状态信息，其值的形式如SOCK_ASYNC_NOSPACE等 struct fasync_struct *fasync_list; // 等待被唤醒的套接字列表，该链表用于异步文件调用 struct file *file; // 套接字所属的文件描述符 struct sock *sk; // 指向存放套接字属性的结构指针 wait_queue_head_t wait; //套接字的等待队列 short type; // 套接字的类型。其取值为SOCK_XXXX形式 const struct proto_ops *ops; // 套接字层的操作函数块 } struct sock数据结构在Linux内核的早期版本中，struct sock数据结构非常复杂。从Linux2.</description></item><item><title>39_综合实现（二）：如何实现函数式编程？</title><link>https://artisanbox.github.io/7/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/39/</guid><description>你好，我是宫文学。
近些年，函数式编程正在复兴。除了一些纯函数式编程语言，比如Lisp、Clojure、Erlang等，众多的主流编程语言，如Python、JavaScript、Go甚至Java，它们都有对函数式编程的支持。
你应该会发现，现在人们对于函数式编程的讨论有很多，比如争论函数式编程和面向对象编程到底哪个更强，在语言里提供混合的编程模式到底对不对等等。
这些论战一时半会儿很难停息。不过我们的这一讲，不会涉及这些有争议的话题，而是试图从编译技术的角度，来探讨如何支持函数式编程，包括如何让函数作为一等公民、如何针对函数式编程的特点做优化、如何处理不变性，等等。通过函数式编程这个综合的主题，我们也再一次看看，如何在实现一门语言时综合运用编译原理的各种知识点，同时在这个探究的过程中，也会加深你对函数式编程语言的理解。
好，我们先来简单了解一下函数式编程的特点。
函数式编程的特点我想，你心里可能多多少少都会有一点疑问，为什么函数式编程开始变得流行了呢？为什么我在开篇的时候，说函数式编程正在“复兴”，而没有说正在兴起？为什么围绕函数式编程会有那么多的争论？
要回答这几个问题，我会建议你先去了解一点历史。
我们都知道，计算机发展历史上有一个重要的人物是阿兰 · 图灵（Alan Turing）。他在1936年提出了一种叫做图灵机的抽象模型，用来表达所有的计算。图灵机有一个无限长的纸带，还有一个读写头，能够读写数据并根据规则左右移动。这种计算过程跟我们在现代的计算机中，用一条条指令驱动计算机运行的方式很相似。
不过，计算模型其实不仅仅可以用图灵机来表达。早在图灵机出现之前，阿隆佐 · 邱奇（Alonzo Church）就提出了一套Lambda演算的模型。并且，计算机科学领域中的很多人，其实都认为用Lambda演算来分析可计算性、计算复杂性，以及用来编程，会比采用图灵机模型更加简洁。而Lambda演算，就是函数式编程的数学基础。
补充：实际上，邱奇是图灵的导师。当年图灵发表他的论文的时候，编辑看不懂，所以找邱奇帮忙，并推荐图灵成为他的学生，图灵机这个词也是邱奇起的。所以师生二人，对计算机科学的发展都做出了很大的贡献。
因为有Lambda演算的数学背景，所以函数式编程范式的历史很早。上世纪50年代出现的Lisp语言，就是函数式编程语言。Lisp的发明人约翰 · 麦卡锡（John McCarthy）博士，是一位数学博士。所以你用Lisp语言和其他函数式编程语言的时候，都会感觉到有一种数学思维的味道。
也正因如此，与函数式编程有关的理论和术语其实是有点抽象的，比如函子（Functor）、单子（Monad）、柯里化（Currying）等。当然，对它们的深入研究不是我们这门课的任务。这里我想带你先绕过这些理论和术语，从我们日常的编程经验出发，来回顾一下函数式编程的特点，反倒更容易一些。
我前面也说过，目前流行的很多语言，虽然不是纯粹的函数式编程语言，但多多少少都提供了对函数式编程的一些支持，比如JavaScript、Python和Go等。就连Java语言，也在Java8中加入了对函数式编程的支持，很多同学可能已经尝试过了。
我们使用函数式编程最多的场景，恐怕是对集合的处理了。举个例子，假设你有一个JavaScript的数组a，你想基于这个数组计算出另一个数组b，其中b的每个元素是a中对应元素的平方。如果用普通的方式写程序，你可能会用一个循环语句，遍历数组a，然后针对每个数组元素做处理：
var b = []; for (var i = 0; i&amp;lt; a.length; i++){ //遍历数组a b.push(a[i]*a[i]); //把计算结果加到数组b中 } 不过你也可以采用更简单的实现方法。
这次我们使用了map方法，并给它传了一个回调函数。map方法会针对数组的每个元素执行这个回调函数，并把计算结果组合成一个新的数组。
function sq(item){ //计算平方值的函数 return item*item; } var b = a.map(sq); //把函数作为参数传递 它还可以写成一种更简化的方式，也就是Lambda表达式的格式：
var b = a.map(item=&amp;gt;item*item); 通过这个简单的例子，我们可以体会出函数式编程的几个特点：
1.函数作为一等公民也就是说，函数可以像一个数值一样，被赋给变量，也可以作为函数参数。如果一个函数能够接受其他函数作为参数，或者能够把一个函数作为返回值，那么它就是高阶函数。像示例程序中的map就是高阶函数。
那函数式编程语言的优势来自于哪里呢？就在于它可以像数学那样使用函数和变量，这会让软件的结构变得特别简单、清晰，运行结果可预测，不容易出错。
根据这个特点，我们先来看看函数式编程语言中的函数，跟其他编程语言中的函数有什么不同。
2.纯函数（Pure Function）在函数式编程里面，有一个概念叫做纯函数。纯函数是这样一种函数，即相同的输入，永远会得到相同的输出。
其实你对纯函数应该并不陌生。你在中学时学到的函数，就是纯函数。比如对于f(x)=ax+b，对于同样的x，所得到的函数值肯定是一样的。所以说，纯函数不应该算是个新概念，而是可以回归到你在学习计算机语言之前的那个旧概念。
在C语言、Java等语言当中，由于函数或方法里面可以引用外面的变量，比如全局变量、对象的成员变量，使得其返回值与这些变量有关。因此，如果有其他软件模块修改了这些变量的值，那么该函数或方法的返回值也会受到影响。这就会让多个模块之间基于共享的变量耦合在一起，这种耦合也使得软件模块的依赖关系变得复杂、隐秘，容易出错，牵一发而动全身。这也是像面向对象语言这些命令式编程语言最令人诟病的一点。
而对于纯函数来说，它不依赖外部的变量，这个叫做引用透明（Reference Transparency）。纯函数的这种“靠谱”、可预测的特征，就给我们的编程工作带来了很多的好处。
举个例子。既然函数的值只依赖输入，那么就跟调用时间无关了。假设有一个函数式g(f(x))，如果按照传统的求值习惯，我们应该先把f(x)的值求出来，再传递给g()。但如果f(x)是纯函数，那么早求值和晚求值其实是无所谓的，所以我们可以延迟求值（Lazy Evaluation）。</description></item><item><title>39_自增主键为什么不是连续的？</title><link>https://artisanbox.github.io/1/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/39/</guid><description>在第4篇文章中，我们提到过自增主键，由于自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑。
之前我见过有的业务设计依赖于自增主键的连续性，也就是说，这个设计假设自增主键是连续的。但实际上，这样的假设是错的，因为自增主键不能保证连续递增。
今天这篇文章，我们就来说说这个问题，看看什么情况下自增主键会出现 “空洞”？
为了便于说明，我们创建一个表t，其中id是自增主键字段、c是唯一索引。
CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `c` (`c`) ) ENGINE=InnoDB; 自增值保存在哪儿？在这个空表t里面执行insert into t values(null, 1, 1);插入一行数据，再执行show create table命令，就可以看到如下图所示的结果：
图1 自动生成的AUTO_INCREMENT值可以看到，表定义里面出现了一个AUTO_INCREMENT=2，表示下一次插入数据时，如果需要自动生成自增值，会生成id=2。
其实，这个输出结果容易引起这样的误解：自增值是保存在表结构定义里的。实际上，表的结构定义存放在后缀名为.frm的文件中，但是并不会保存自增值。
不同的引擎对于自增值的保存策略不同。
MyISAM引擎的自增值保存在数据文件中。 InnoDB引擎的自增值，其实是保存在了内存里，并且到了MySQL 8.0版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为MySQL重启前的值”，具体情况是： 在MySQL 5.7及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值max(id)，然后将max(id)+1作为这个表当前的自增值。﻿
举例来说，如果一个表当前数据行里最大的id是10，AUTO_INCREMENT=11。这时候，我们删除id=10的行，AUTO_INCREMENT还是11。但如果马上重启实例，重启后这个表的AUTO_INCREMENT就会变成10。﻿
也就是说，MySQL重启可能会修改一个表的AUTO_INCREMENT的值。 在MySQL 8.0版本，将自增值的变更记录在了redo log中，重启的时候依靠redo log恢复重启之前的值。 理解了MySQL对自增值的保存策略以后，我们再看看自增值修改机制。
自增值修改机制在MySQL里面，如果字段id被定义为AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下：
如果插入数据时id字段指定为0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT值填到自增字段；
如果插入数据时id字段指定了具体的值，就直接使用语句里指定的值。
根据要插入的值和当前自增值的大小关系，自增值的变更结果也会有所不同。假设，某次要插入的值是X，当前的自增值是Y。</description></item><item><title>39｜中端优化第2关：全局优化要怎么搞？</title><link>https://artisanbox.github.io/3/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/41/</guid><description>你好，我是宫文学。
上一节课，我们用了一些例子，讨论了如何用基于图的IR来实现一些优化，包括公共子表达式删除、拷贝传播和死代码删除。但这些例子，都属于本地优化的场景。也就是说，在未来生成的汇编代码中，这些代码其实都位于同一个基本块。
不过，复杂一点的程序，都会有if语句和循环语句这种流程控制语句，所以程序就会存在多个基本块。那么就会存在跨越多个基本块的优化工作，也就是全局优化。
所以，今天这节课，我们就来讨论一下如何基于当前的IR做全局优化。同时，为了达到优化效果，我们这一节课还需要把浮动的数据节点划分到具体的基本块中去，实现指令的调度。
但在讨论全局优化的场景之前，我还要先给你补充一块知识点，就是变量的版本和控制流的关系，让你能更好地理解全局优化。
变量的版本和控制流的关系通过前几节课我们已经知道，我们的IR生成算法能够对一个变量产生多个版本的定义，从而让IR符合SSA格式。可是，我们是如何来表示不同版本的定义的，又是如何确定程序中到底引用的是变量的哪个版本呢？
在IR的模型中，我引入了一个VarProxy类，来引用变量的一个版本，就像d0、d1和d2，也有的文献把变量的一个定义叫做变量的一个定值。VarProxy里面保存了一个VarSymbol，还包括了一个下标：
//代表了变量的一次定义。每次变量重新定义，都会生成一个新的Proxy，以便让IR符合SSA格式 class VarProxy{ varSym:VarSymbol; index:number; //变量的第几个定义 constructor(varSym:VarSymbol, index:number){ this.varSym = varSym; this.index = index; } get label():string{ return this.varSym.name+this.index; } } 每次遇到变量声明、变量赋值，以及像i++这样能够导致变量值改变的语句时，我们就会产生一个新的变量定义，也就是一个VarProxy。这个VarProxy会被绑定到一个具体的DataNode上。所以，我在IR中显示DataNode节点的时候，也会把绑定在这个节点上的变量定义一并显示出来。
那当我们在程序中遇到一个变量的时候，如何确定它采用的是哪个版本呢？
这就需要我们在生成IR的过程中，把VarProxy与当前的控制流绑定。每个控制流针对每个变量，只有一个确定的版本。
//把每个变量绑定到控制流，从而知道当前代码用到的是变量的哪个定义 //在同一个控制流里，如果有多个定义，则后面的定义会替换掉前面的。 varProxyMap:Map&amp;lt;AbstractBeginNode,Map&amp;lt;VarSymbol,VarProxy&amp;gt;&amp;gt; = new Map(); 在这里，我们还用了一个AbstractBeginNode节点来标识一个控制流。因为每个控制流都是存在一个起点的。而每个控制流节点，透过它的predecessor链，总能找到自己这条控制流的开始节点。
//获取这条控制流的开头节点 get beginNode():AbstractBeginNode{ if (this instanceof AbstractBeginNode){ return this; } else{ return (this.predecessor as UniSuccessorNode).beginNode; } } 但是，如果变量不是在当前控制流中定义的，而是在前面的控制流中定义的，那我们可以递归地往前查找。这里具体的实现，你可以参考一下getVarProxyFromFlow()。
最后，如果控制流的起点是一个merge节点，那这个变量就可能是在分支语句中定义的，那我们就要生成一个Phi节点，并把这个Phi节点也看成是变量定义的一个版本，方便我们在后续程序中引用。
好了，相信现在你已经可以更清晰地理解变量版本与控制流之间的关系了。现在我们基于这些前置知识，就可以开始讨论全局优化的场景了。
全局的死代码删除上一节课，我们实现了基本块中的死代码删除功能。那个时候，我们基本上只需要考虑数据流的特点，把uses属性为空的节点删除掉就行了。因为这些节点对应的变量定义没有被引用，所以它们就是死代码。
那么，现在考虑带有程序分支的情况，会怎么样呢？
我们还是通过一个例子来分析一下。你可以先停下来两分钟，用肉眼看一下，看看哪些代码可以删除：
function deadCode2(b:number,c:number){ let a:number = b+c; let d:number; let y:number; if (b &amp;gt; 0){ b = a+b; d = a+b; } else{ d = a+c; y = b+d; } let x = a+b; y = c + d; return x; } 我也把答案写出来了，看看跟你想的是否一样。在整个代码优化完毕以后，其实只剩下很少的代码了。变量c、d和y的定义都被优化掉了。</description></item><item><title>40_insert语句的锁为什么这么多？</title><link>https://artisanbox.github.io/1/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/40/</guid><description>在上一篇文章中，我提到MySQL对自增主键锁做了优化，尽量在申请到自增id以后，就释放自增锁。
因此，insert语句是一个很轻量的操作。不过，这个结论对于“普通的insert语句”才有效。也就是说，还有些insert语句是属于“特殊情况”的，在执行过程中需要给其他资源加锁，或者无法在申请到自增id以后就立马释放自增锁。
那么，今天这篇文章，我们就一起来聊聊这个话题。
insert … select 语句我们先从昨天的问题说起吧。表t和t2的表结构、初始化数据语句如下，今天的例子我们还是针对这两个表展开。
CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(null, 1,1); insert into t values(null, 2,2); insert into t values(null, 3,3); insert into t values(null, 4,4);
create table t2 like t 现在，我们一起来看看为什么在可重复读隔离级别下，binlog_format=statement时执行：
insert into t2(c,d) select c,d from t; 这个语句时，需要对表t的所有行和间隙加锁呢？
其实，这个问题我们需要考虑的还是日志和数据的一致性。我们看下这个执行序列：
图1 并发insert场景实际的执行效果是，如果session B先执行，由于这个语句对表t主键索引加了(-∞,1]这个next-key lock，会在语句执行完成后，才允许session A的insert语句执行。</description></item><item><title>40_初识动态规划：如何巧妙解决“双十一”购物时的凑单问题？</title><link>https://artisanbox.github.io/2/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/41/</guid><description>淘宝的“双十一”购物节有各种促销活动，比如“满200元减50元”。假设你女朋友的购物车中有n个（n&amp;gt;100）想买的商品，她希望从里面选几个，在凑够满减条件的前提下，让选出来的商品价格总和最大程度地接近满减条件（200元），这样就可以极大限度地“薅羊毛”。作为程序员的你，能不能编个代码来帮她搞定呢？
要想高效地解决这个问题，就要用到我们今天讲的动态规划（Dynamic Programming）。
动态规划学习路线动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。不过，它也是出了名的难学。它的主要学习难点跟递归类似，那就是，求解问题的过程不太符合人类常规的思维方式。对于新手来说，要想入门确实不容易。不过，等你掌握了之后，你会发现，实际上并没有想象中那么难。
为了让你更容易理解动态规划，我分了三节给你讲解。这三节分别是，初识动态规划、动态规划理论、动态规划实战。
第一节，我会通过两个非常经典的动态规划问题模型，向你展示我们为什么需要动态规划，以及动态规划解题方法是如何演化出来的。实际上，你只要掌握了这两个例子的解决思路，对于其他很多动态规划问题，你都可以套用类似的思路来解决。
第二节，我会总结动态规划适合解决的问题的特征，以及动态规划解题思路。除此之外，我还会将贪心、分治、回溯、动态规划这四种算法思想放在一起，对比分析它们各自的特点以及适用的场景。
第三节，我会教你应用第二节讲的动态规划理论知识，实战解决三个非常经典的动态规划问题，加深你对理论的理解。弄懂了这三节中的例子，对于动态规划这个知识点，你就算是入门了。
0-1背包问题我在讲贪心算法、回溯算法的时候，多次讲到背包问题。今天，我们依旧拿这个问题来举例。
对于一组不同重量、不可分割的物品，我们需要选择一些装入背包，在满足背包最大重量限制的前提下，背包中物品总重量的最大值是多少呢？
关于这个问题，我们上一节讲了回溯的解决方法，也就是穷举搜索所有可能的装法，然后找出满足条件的最大值。不过，回溯算法的复杂度比较高，是指数级别的。那有没有什么规律，可以有效降低时间复杂度呢？我们一起来看看。
// 回溯算法实现。注意：我把输入的变量都定义成了成员变量。 private int maxW = Integer.MIN_VALUE; // 结果放到maxW中 private int[] weight = {2，2，4，6，3}; // 物品重量 private int n = 5; // 物品个数 private int w = 9; // 背包承受的最大重量 public void f(int i, int cw) { // 调用f(0, 0) if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了 if (cw &amp;gt; maxW) maxW = cw; return; } f(i+1, cw); // 选择不装第i个物品 if (cw + weight[i] &amp;lt;= w) { f(i+1,cw + weight[i]); // 选择装第i个物品 } } 规律是不是不好找？那我们就举个例子、画个图看看。我们假设背包的最大承载重量是9。我们有5个不同的物品，每个物品的重量分别是2，2，4，6，3。如果我们把这个例子的回溯求解过程，用递归树画出来，就是下面这个样子：</description></item><item><title>40_成果检验：方舟编译器的优势在哪里？</title><link>https://artisanbox.github.io/7/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/40/</guid><description>你好，我是宫文学。到这里，咱们的课程就已经进入尾声了。在这门课程里，通过查看真实的编译器，你应该已经积累了不少对编译器的直观认识。前面我们研究的各种编译器，都是国外的产品或项目。而这一讲呢，我们则要看看一个有中国血统的编译器：方舟编译器。
通过阅读方舟编译器已经公开的代码和文档，在解析它的过程中，你可以检验一下自己的所学，谈谈你对它的认识。比如，跟你了解的其他编译器相比，它有什么特点？先进性如何？你是否有兴趣利用方舟编译器做点实际项目？等等。
不过，到目前为止，由于方舟编译器开源的部分仍然比较有限，所以这一讲我们只根据已经掌握的信息做一些分析。其中涉及两个大的话题，一是对方舟编译器的定位和设计思路的分析，二是对方舟编译器所使用的Maple IR的介绍。
好，首先，我借助Android对应用开发支持的缺陷，来谈一下为什么方舟编译器是必要的。
Android的不足为什么要研发一款自己的编译器？对于一个大的技术生态而言，语言的编译和运行体系非常重要。它处在上层应用和下层硬件之间，直接决定了应用软件能否充分地发挥出硬件的性能。对于移动应用生态而言，我国拥有体量最大的移动用户和领先的移动应用，也有着最大的手机制造量。可是，对于让上层应用和底层硬件得以发挥最大能力的编译器和运行时，我们却缺少话语权。
实际上，我认为Android对应用开发的支持并不够好。我猜测，掌控Android生态的谷歌公司，对于移动应用开发和手机制造都没有关系到切身利益，因此创新的动力不足。
我之所以说Android对应用开发的支持不够好，这其实跟苹果的系统稍加对比就很清楚了。同样的应用，在苹果手机上会运行得更流畅，且消耗的内存也更低。所以Android手机只好增加更多的CPU内核和更多的内存。
你可能会问，谷歌不是也有自己的应用吗？对应用的支持也关系到谷歌自己的利益呀。那我这里其实要补充一下，我说的应用开发，指的是用Java和Kotlin开发的应用，这也是大部分Android平台上的应用开发者所采用的语言。而像谷歌这样拥有强大技术力量的互联网巨头们，通常对于性能要求比较高的代码，是用C开发的。比如微信的关键逻辑就是用C编写的；像手机游戏这种对性能要求比较高的应用，底层的游戏引擎也是基于C/C++实现的。
这些开发者们不采用Java的原因，是因为Java在Android平台上的编译和运行方式有待提高。Android为了提升应用的运行速度，一直在尝试升级其应用运行机制。从最早的仅仅解释执行字节码，到引入JIT编译机制，到当前版本的ART（Android Runtime）支持AOT、JIT和基于画像的编译机制。尽管如此，Android对应用的支持仍然存在明显的短板。
第一个短板，是垃圾收集机制。我们知道，Java基于标记-拷贝算法的垃圾收集机制有两个缺陷。一是要占据更多的内存，二是在垃圾收集的时候会有停顿，导致应用不流畅。在系统资源紧张的时候，更是会强制做内存收集，引起整个系统的卡顿。
实际上，Java的内存管理机制使得它一直不太适合编写客户端应用。就算在台式机上，用Java编写的客户端应用同样会占用很大的内存，并且时不时会有卡顿。你如果使用过Eclipse和IDEA，应该就会有这样的体会。
第二个短板，是不同语言的融合问题。Android系统中大量的底层功能都是C/C++实现，而Java应用只是去调用它们。比如，图形界面的绘制和刷新，是由一个叫做Skia的库来实现的，这个库是用C/C++编写的，各种窗口控件都是在Skia的基础上封装出来的。所以，用户在界面上的操作，背后就有大量的JNI调用。
问题是，Java通过JNI调用C语言的库的时候，实现成本是很高的，因为两种不同语言的数据类型、调用约定完全不同，又牵涉到跨语言的异常传播和内存管理，所以Java不得不通过虚拟机进行昂贵的处理，效率十分低下。
据调查，95%的顶级移动应用都是用Java和C、C++等混合开发的。所以，让不同语言开发的功能能够更好地互相调用，是一个具有普遍意义的问题。
第三个短板，就是Android的运行时一直还是受Java虚拟机思路的影响，一直摆脱不了虚拟机。虚拟机本身要占据内存资源和CPU资源。在做即时编译的时候，也要消耗额外的资源。
那么如何解决这些问题呢？我们来看看方舟编译器的解决方案。
方舟编译器的解决方案方舟编译器的目标并不仅仅是为了替代Android上的应用开发和运行环境。但我们可以通过方舟是如何解决Android应用开发的问题，来深入了解一下方舟编译器。
我们先来看看，方舟编译器是怎么解决垃圾收集的问题的。
不过，在讨论方舟的方案之前，我们不妨先参考一下苹果的方案做个对照。苹果采用的开发语言，无论是Objective-C，还是后来的Swift，都是采用引用计数技术。引用计数可以实时回收内存垃圾，所以没有卡顿。并且它也不用像标记-拷贝算法那样，需要保留额外的内存。而方舟编译器，采用的是跟苹果一样的思路，同样采用了引用计数技术。
当然，这里肯定会有孰优孰劣的争论。我们之前也讲过，采用引用计数法，每次在变量引用对象的时候都要增加引用计数，而在退出变量的作用域或者变量不再指向该对象时，又要减少引用计数，这会导致一些额外的性能开销。当对象在多个线程之间共享的时候，增减引用计数的操作还要加锁，从而进一步导致了性能的降低。
不过，针对引用计数对性能的损耗，我们可以在编译器中通过多种优化算法得到改善，尽量减少不必要的增减计数的操作，也减少不必要的锁操作。另外，有些语言在设计上也会做一些限制，比如引入弱引用机制，从而降低垃圾收集的负担。
无论如何，在全面考察了引用计数方法的优缺点以后，你仍然会发现它其实更适合开发客户端应用。
关于第二个问题，也就是不同语言的融合问题。华为采取的方法是，让Java语言的程序和基于C、C++等语言的程序按照同一套框架做编译。无论前端是什么语言，都统一编译成机器码，同时不同语言的程序互相调用的时候，也没有额外的开销。
下图是方舟编译器的文档中所使用的架构图。你能看到它的设计目标是支持多种语言，都统一转换成方舟IR，然后进行统一的优化处理，再生成机器码的可执行文件。
方舟编译器架构示意图这个技术方案其实非常大胆。它不仅解决了不同语言之间的互相调用问题，也彻底抛弃了植根于JVM的虚拟机思路。方舟编译器新的思路是不要虚拟机，最大程度地以机器码的方式运行，再加上一个非常小的运行时。
我说这个技术方案大胆，是因为方舟编译器彻底抛弃了Java原有的运行方案，包括内存布局、调用约定、对象结构、分层编译机制等。我们在第二个模块讲过Graal，在仍然基于JVM运行的情况下，JIT只是尽力做改良，它随时都有一个退路，就是退到用字节码解释器去执行。就算采用AOT以后，运行时可以变得小一些，但Java运行机制的大框架仍然是不变的。
我也介绍过，GraalVM支持对多种语言做统一编译，其中也包含了对C语言的支持，并且也支持语言之间的互相调用。但即便如此，它仍是改良主义，它不会抛弃Java原来的技术积累。
而方舟编译器不是在做改良，而是在做革命。它对Java的编译更像是对C/C++等语言的编译，抛弃了JVM的那一套思路。
这个方案不仅大胆，而且难度更高。因为这样就不再像分层编译那样有退路，方舟编译器需要把所有的Java语义都静态编译成机器码。而对于那些比较动态的语义，比如运行时的动态绑定、Reflection机制等，是挑战比较大的。
那方舟编译器目前的成果如何呢？根据华为官方的介绍，方舟编译器可以使安卓系统的操作流畅度提升24%，响应速度提升44%，第三方应用操作流畅度提升高达60%。这就是方舟编译器的厉害之处，这也证明方舟编译器的大胆革新之路是走对了的。
我们目前只讨论了方舟编译器对Android平台的改进。其实，方舟编译器的目标操作系统不仅仅是Android平台，它本质上可移植所有的操作系统，也包括华为自己的鸿蒙操作系统。对于硬件平台也一样，它可以支持从手机到物联网设备的各种硬件架构。
所以，你能看出，方舟编译器真的是志存高远。它不是为了解决某一个小问题，而是致力于打造一套新的应用开发生态。
好了，通过上面的介绍，你应该对方舟编译器的定位有了一个了解。接下来的问题是，方舟编译器的内部到底是怎样的呢？
方舟编译器的开源项目要深入了解方舟编译器，还是必须要从它的源代码入手。从去年9月份开源以来，方舟编译器吸引了很多人的目光。不过方舟编译器是逐步开源的，由于开放出来的源代码必须在知识产权等方面能够经得起严格的审查，因此到现在为止，我们能看到的开源版本号还只是0.2版，开放出来的功能并不多。
我参照方舟的环境配置文档，在Ubuntu 16.04上做好了环境配置。
注意：请尽量完全按照文档的要求来配置环境，避免出现不必要的错误。不要嫌某些软件的版本不够新。
接着，你可以继续根据开发者指南来编译方舟编译器本身。方舟编译器本身的代码是用C++写的，需要用LLVM加Clang编译，这说明它到目前还没有实现自举。然后，你可以编译一下示例程序。比如，用下面的四个命令，可以编译出HelloWorld样例。
source build/envsetup.sh; make; cd samples/helloworld/; make 这个“hellowold”目录原来只有一个HelloWorld.java源代码，经过编译后，形成了下面的文件：
如果你跟踪查看编译过程，你会发现中间有几步的操作：
第一步，执行java2jar，这一步是调用Java的编译器，把Java文件先编译成class文件，然后打包成jar文件。
补充：java2jar实际上是一个简单的脚本文件，你可以查看里面的内容。
第二步，执行jbc2mpl，也就是把Java字节码转换成Maple IR。Maple IR是方舟编译器的IR，我下面会展开介绍。编译后生成的Maple IR保存到了HelloWorld.mpl中。
第三步，通过maple命令，执行mpl2mpl和mplme这两项对Maple IR做分析和优化的工作。这其中，很重要的一个步骤，就是把Java方法的动态绑定，用vtable做了实现，并生成了一个新的Maple IR文件：HelloWorld.VtableImpl.mpl。
最后一步，调用mplcg命令，将Maple IR转换成汇编代码，保存到一个以.s结尾的文件里面。
注意，我们目前还没有办法编译成直接可以执行的文件。当前开源的版本，既没有编译器前端部分的代码，也没有后端部分的代码，甚至基于Maple IR的一些常见的优化，比如内联、公共子表达式消除、常量传播等等，都是没有的。目前开源的版本主要展现了Maple IR，以及对Maple IR做的一些变换，比如转换成SSA格式，以便进行后续的分析处理。
到这里，你可能会有一点失望，因为当前开放出来的东西确实有点少。但是不要紧，方舟编译器既然选择首先开放Maple IR的设计，这说明Maple IR在整个方舟编译器的体系中是很重要的。
事实也确实如此。方舟编译器的首席科学家，Fred Chow（周志德）先生，曾发表过一篇论文：The increasing significance of intermediate representations in compilers。他指出，IR的设计会影响优化的效果；IR的调整，会导致编译器实现的重大调整。他还提出：如果不同体系的IR可以实现转换的话，就可以加深编译器之间的合作。</description></item><item><title>40_理解内存（上）：虚拟内存和内存保护是什么？</title><link>https://artisanbox.github.io/4/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/40/</guid><description>我们在专栏一开始说过，计算机有五大组成部分，分别是：运算器、控制器、存储器、输入设备和输出设备。如果说计算机最重要的组件，是承担了运算器和控制器作用的CPU，那内存就是我们第二重要的组件了。内存是五大组成部分里面的存储器，我们的指令和数据，都需要先加载到内存里面，才会被CPU拿去执行。
专栏第9讲，我们讲了程序装载到内存的过程。可以知道，在我们日常使用的Linux或者Windows操作系统下，程序并不能直接访问物理内存。
我们的内存需要被分成固定大小的页（Page），然后再通过虚拟内存地址（Virtual Address）到物理内存地址（Physical Address）的地址转换（Address Translation），才能到达实际存放数据的物理内存位置。而我们的程序看到的内存地址，都是虚拟内存地址。
既然如此，这些虚拟内存地址究竟是怎么转换成物理内存地址的呢？这一讲里，我们就来看一看。
简单页表想要把虚拟内存地址，映射到物理内存地址，最直观的办法，就是来建一张映射表。这个映射表，能够实现虚拟内存里面的页，到物理内存里面的页的一一映射。这个映射表，在计算机里面，就叫作页表（Page Table）。
页表这个地址转换的办法，会把一个内存地址分成页号（Directory）和偏移量（Offset）两个部分。这么说太理论了，我以一个32位的内存地址为例，帮你理解这个概念。
其实，前面的高位，就是内存地址的页号。后面的低位，就是内存地址里面的偏移量。做地址转换的页表，只需要保留虚拟内存地址的页号和物理内存地址的页号之间的映射关系就可以了。同一个页里面的内存，在物理层面是连续的。以一个页的大小是4K字节（4KB）为例，我们需要20位的高位，12位的低位。
总结一下，对于一个内存地址转换，其实就是这样三个步骤：
把虚拟内存地址，切分成页号和偏移量的组合； 从页表里面，查询出虚拟页号，对应的物理页号； 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。 看起来这个逻辑似乎很简单，很容易理解，不过问题马上就来了。你能算一算，这样一个页表需要多大的空间吗？我们以32位的内存地址空间为例，你可以暂停一下，拿出纸笔算一算。
不知道你算出的数字是多少？32位的内存地址空间，页表一共需要记录2^20个到物理页号的映射关系。这个存储关系，就好比一个2^20大小的数组。一个页号是完整的32位的4字节（Byte），这样一个页表就需要4MB的空间。听起来4MB的空间好像还不大啊，毕竟我们现在的内存至少也有4GB，服务器上有个几十GB的内存和很正常。
不过，这个空间可不是只占用一份哦。我们每一个进程，都有属于自己独立的虚拟内存地址空间。这也就意味着，每一个进程都需要这样一个页表。不管我们这个进程，是个本身只有几KB大小的程序，还是需要几GB的内存空间，都需要这样一个页表。如果你用的是Windows，你可以打开你自己电脑上的任务管理器看看，现在你的计算机里同时在跑多少个进程，用这样的方式，页表需要占用多大的内存。
这还只是32位的内存地址空间，现在大家用的内存，多半已经超过了4GB，也已经用上了64位的计算机和操作系统。这样的话，用上面这个数组的数据结构来保存页面，内存占用就更大了。那么，我们有没有什么更好的解决办法呢？你可以先仔细思考一下。
多级页表仔细想一想，我们其实没有必要存下这2^20个物理页表啊。大部分进程所占用的内存是有限的，需要的页也自然是很有限的。我们只需要去存那些用到的页之间的映射关系就好了。如果你对数据结构比较熟悉，你可能要说了，那我们是不是应该用哈希表（Hash Map）这样的数据结构呢？
很可惜你猜错了：）。在实践中，我们其实采用的是一种叫作多级页表（Multi-Level Page Table）的解决方案。这是为什么呢？为什么我们不用哈希表而用多级页表呢？别着急，听我慢慢跟你讲。
我们先来看一看，一个进程的内存地址空间是怎么分配的。在整个进程的内存地址空间，通常是“两头实、中间空”。在程序运行的时候，内存地址从顶部往下，不断分配占用的栈的空间。而堆的空间，内存地址则是从底部往上，是不断分配占用的。
所以，在一个实际的程序进程里面，虚拟内存占用的地址空间，通常是两段连续的空间。而不是完全散落的随机的内存地址。而多级页表，就特别适合这样的内存地址分布。
我们以一个4级的多级页表为例，来看一下。同样一个虚拟内存地址，偏移量的部分和上面简单页表一样不变，但是原先的页号部分，我们把它拆成四段，从高到低，分成4级到1级这样4个页表索引。
对应的，一个进程会有一个4级页表。我们先通过4级页表索引，找到4级页表里面对应的条目（Entry）。这个条目里存放的是一张3级页表所在的位置。4级页面里面的每一个条目，都对应着一张3级页表，所以我们可能有多张3级页表。
找到对应这张3级页表之后，我们用3级索引去找到对应的3级索引的条目。3级索引的条目再会指向一个2级页表。同样的，2级页表里我们可以用2级索引指向一个1级页表。
而最后一层的1级页表里面的条目，对应的数据内容就是物理页号了。在拿到了物理页号之后，我们同样可以用“页号+偏移量”的方式，来获取最终的物理内存地址。
我们可能有很多张1级页表、2级页表，乃至3级页表。但是，因为实际的虚拟内存空间通常是连续的，我们很可能只需要很少的2级页表，甚至只需要1张3级页表就够了。
事实上，多级页表就像一个多叉树的数据结构，所以我们常常称它为页表树（Page Table Tree）。因为虚拟内存地址分布的连续性，树的第一层节点的指针，很多就是空的，也就不需要有对应的子树了。所谓不需要子树，其实就是不需要对应的2级、3级的页表。找到最终的物理页号，就好像通过一个特定的访问路径，走到树最底层的叶子节点。
以这样的分成4级的多级页表来看，每一级如果都用5个比特表示。那么每一张某1级的页表，只需要2^5=32个条目。如果每个条目还是4个字节，那么一共需要128个字节。而一个1级索引表，对应32个4KB的也就是128KB的大小。一个填满的2级索引表，对应的就是32个1级索引表，也就是4MB的大小。
我们可以一起来测算一下，一个进程如果占用了8MB的内存空间，分成了2个4MB的连续空间。那么，它一共需要2个独立的、填满的2级索引表，也就意味着64个1级索引表，2个独立的3级索引表，1个4级索引表。一共需要69个索引表，每个128字节，大概就是9KB的空间。比起4MB来说，只有差不多1/500。
不过，多级页表虽然节约了我们的存储空间，却带来了时间上的开销，所以它其实是一个“以时间换空间”的策略。原本我们进行一次地址转换，只需要访问一次内存就能找到物理页号，算出物理内存地址。但是，用了4级页表，我们就需要访问4次内存，才能找到物理页号了。
我们在前面两讲讲过，内存访问其实比Cache要慢很多。我们本来只是要做一个简单的地址转换，反而是一下子要多访问好多次内存。对于这个时间层面的性能损失，我们有没有什么更好的解决办法呢？那请你一定要关注下一讲的内容哦！
总结延伸好了，这一讲的内容差不多了，我们来总结一下。
我们从最简单的进行虚拟页号一一映射的简单页表说起，仔细讲解了现在实际应用的多级页表。多级页表就像是一颗树。因为一个进程的内存地址相对集中和连续，所以采用这种页表树的方式，可以大大节省页表所需要的空间。而因为每个进程都需要一个独立的页表，这个空间的节省是非常可观的。
在优化页表的过程中，我们可以观察到，数组这样的紧凑的数据结构，以及树这样稀疏的数据结构，在时间复杂度和空间复杂度的差异。另外，纯粹理论软件的数据结构和硬件的设计也是高度相关的。
推荐阅读对于虚拟内存的知识点，你可以再深入读一读《计算机组成与设计：硬件/软件接口》的第5.7章节。如果你觉得还不过瘾，可以进一步去读一读《What Every Programmer Should Know About Memory》的第4部分，也就是Virtual Memory。
课后思考在实际的虚拟内存地址到物理内存地址的地址转换的过程里，我们没有采用哈希表，而是采用了多级页表的解决方案。你能想一想，使用多级页表，对于哈希表有哪些优点，又有哪些缺点吗？
欢迎留言和我分享你的想法，如果觉得有收获，你也可以把这篇文章分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>40_瞧一瞧Linux：详解socket的接口实现</title><link>https://artisanbox.github.io/9/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/40/</guid><description>你好，我是LMOS。
上节课，我们一起了解了套接字的工作机制和数据结构，但套接字有哪些基本接口实现呢？相信学完这节课，你就能够解决这个问题了。
今天我会和你探讨套接字从创建、协议接口注册与初始化过程，还会为你深入分析套接字系统，是怎样调用各个功能函数的。通过这节课，相信你可以学会基于套接字来编写网络应用程序。有了之前的基础，想理解这节课并不难，让我们正式开始吧。
套接字接口套接字接口最初是BSD操作系统的一部分，在应用层与TCP/IP协议栈之间接供了一套标准的独立于协议的接口。
Linux内核实现的套接字接口，将UNIX的“一切都是文件操作”的概念应用在了网络连接访问上，让应用程序可以用常规文件操作API访问网络连接。
从TCP/IP协议栈的角度来看，传输层以上的都是应用程序的一部分，Linux与传统的UNIX类似，TCP/IP协议栈驻留在内核中，与内核的其他组件共享内存。传输层以上执行的网络功能，都是在用户地址空间完成的。
Linux使用内核套接字概念与用户空间套接字通信，这样可以让实现和操作变得更简单。Linux提供了一套API和套接字数据结构，这些服务向下与内核接口，向上与用户空间接口，应用程序正是使用这一套API访问内核中的网络功能。
套接字的创建在应用程序使用TCP/IP协议栈的功能之前，我们必须调用套接字库函数API创建一个新的套接字，创建好以后，对库函数创建套接字的调用，就会转换为内核套接字创建函数的系统调用。
这时，完成的是通用套接字创建的初始化功能，跟具体的协议族并不相关。
这个过程具体是这样的，在应用程序中执行socket函数，socket产生系统调用中断执行内核的套接字分路函数sys_socketcall，在sys_socketcall套接字函数分路器中将调用传送到sys_socket函数，由sys_socket函数调用套接字的通用创建函数sock_create。
sock_create函数完成通用套接字创建、初始化任务后，再调用特定协议族的套接字创建函数。
这样描述你可能还没有直观感受，我特意画了图，帮你梳理socket创建的流程，你可以对照图片仔细体会调用过程。
结合图解，我再用一个具体例子帮你加深理解，比如由AF_INET协议族的inet_create函数完成套接字与特定协议族的关联。
一个新的struct socket数据结构起始由sock_create函数创建，该函数直接调用__sock_create函数，__sock_create函数的任务是为套接字预留需要的内存空间，由sock_alloc函数完成这项功能。
这个sock_alloc函数不仅会为struct socket数据结构实例预留空间，也会为struct inode数据结构实例分配需要的内存空间，这样可以使两个数据结构的实例相关联。__sock_create函数代码如下。
static int __sock_create(struct net *net, int family, int type, int protocol, struct socket **res, int kern) { int err; struct socket *sock; const struct net_proto_family *pf; // 首先检验是否支持协议族 /* * 检查是否在内核支持的socket范围内 */ if (family &amp;lt; 0 || family &amp;gt;= NPROTO) return -EAFNOSUPPORT; if (type &amp;lt; 0 || type &amp;gt;= SOCK_MAX) return -EINVAL; /* * 为新的套接字分配内存空间，分配成功后返回新的指针 */ sock = sock_alloc(); } sock_alloc函数如下所示。</description></item><item><title>40｜中端优化第3关：一起来挑战过程间优化</title><link>https://artisanbox.github.io/3/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/42/</guid><description>你好，我是宫文学。
在前面两节课，我们分析了本地优化和全局优化的场景。我们发现，由于基于图IR的优点，也就是控制流和数据流之间耦合度比较低的这个特点，我们很多优化算法的实现都变得更简单了。
那么，对于过程间优化的场景，我们这个基于图IR是否也会带来类似的便利呢？
过程间优化（Inter-procedural Optimization）指的是跨越多个函数（或者叫做过程），对程序进行多方面的分析，包括过程间的控制流分析和数据流分析，从而找出可以优化的机会。
今天这节课，我们就来分析两种常用的过程间优化技术，也就是内联优化和全局的逃逸分析，让你能了解过程间优化的思路，也能明白如何基于我们的IR来实现这些优化。之后，我还会给你补充另一个优化技术方面的知识点，也就是规范化。
内联优化内联优化是最常见到的一个过程间优化场景，说的就是当一个函数调用一个子函数时，干脆把子函数的代码拷贝到调用者中，从而减少由于函数调用导致的开销。
特别是，如果调用者是在一个循环中调用子函数，那么由很多次循环累积而导致的性能开销是很大的。内联优化的优势在这时就会得到体现。
而在面向对象编程中，我们通常会写很多很简短的setter和getter方法，并且在程序里频繁调用。如果编译器能自动把这些短方法做内联优化，我们就可以放心大胆地写这些短方法，而不用担心由此导致的性能开销了。
现在我们就举一个非常简单的、可以做内联的例子看看。在这个示例中，inline函数是调用者，它调用了add函数。
//内联 function inline(x:number):number{ return add(x, x+1); } function add(x:number, y:number):number{ return x + y; } 显然，在编译inline函数的时候，我们没必要额外多产生一次对add函数的调用，而是把add函数内联进来就行了，形成下面这些优化后的代码：
//内联 function inline(x:number):number{ return x + (x+1); } 那要如何基于我们的IR实现内联优化呢？
首先，我们还是看看在没有优化以前，inline和add两个函数的IR：
在inline函数的IR里，你能发现两个新的节点：一个是Invoke节点，代表函数调用的控制流；另一个是CallTarget节点，代表函数调用的数据流。
而内联优化就是要把这两个IR图合并，形成一个大的IR。如下图所示：
具体来说，要实现上面的合并，我们需要完成两个任务：
首先，把inline函数中的函数调用的节点替换成add函数中的加法节点； 第二，将加法节点中的x和y两个形式参数，替换成inline函数里的两个实际参数。 总的来说，整个算法都是去做节点的替换和重新连接，思路还是很清晰的。
我们之前说过，编译器在做了一种优化以后，经常可以给其他优化制造机会。在这里，内联优化不仅仅减少了函数调用导致的开销，它还会导致一些其他优化。比如说，我们在Inline函数里调用add函数的时候，传入两个参数x和-x，如下面的示例代码：
//内联 function inline2(x:number):number{ return add(x, -x); } function add(x:number, y:number):number{ return x + y; } 那么内联之后，这里就相当于计算x+(-x)的值，那也能计算出一个常量0。至于如何把x+(-x)化简成0，我先留个悬念，你先自己思考一下，我们这节课后面会介绍到。
//内联 function inline(x:number):number{ return x + (-x); //常量0 } 再比如，我们在主函数里调用add的时候，传的参数是常量。那么内联以后，我们就可以进行常量传播和常量折叠的优化，在编译期就能计算出结果为5：</description></item><item><title>41_动态规划理论：一篇文章带你彻底搞懂最优子结构、无后效性和重复子问题</title><link>https://artisanbox.github.io/2/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/42/</guid><description>上一节，我通过两个非常经典的问题，向你展示了用动态规划解决问题的过程。现在你对动态规划应该有了一个初步的认识。
今天，我主要讲动态规划的一些理论知识。学完这节内容，可以帮你解决这样几个问题：什么样的问题可以用动态规划解决？解决动态规划问题的一般思考过程是什么样的？贪心、分治、回溯、动态规划这四种算法思想又有什么区别和联系？
理论的东西都比较抽象，不过你不用担心，我会结合具体的例子来讲解，争取让你这次就能真正理解这些知识点，也为后面的应用和实战做好准备。
“一个模型三个特征”理论讲解什么样的问题适合用动态规划来解决呢？换句话说，动态规划能解决的问题有什么规律可循呢？实际上，动态规划作为一个非常成熟的算法思想，很多人对此已经做了非常全面的总结。我把这部分理论总结为“一个模型三个特征”。
首先，我们来看，什么是“一个模型”？它指的是动态规划适合解决的问题的模型。我把这个模型定义为“多阶段决策最优解模型”。下面我具体来给你讲讲。
我们一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。
现在，我们再来看，什么是“三个特征”？它们分别是最优子结构、无后效性和重复子问题。这三个概念比较抽象，我来逐一详细解释一下。
1.最优子结构最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，后面阶段的状态可以通过前面阶段的状态推导出来。
2.无后效性无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。
3.重复子问题这个概念比较好理解。前面一节，我已经多次提过。如果用一句话概括一下，那就是，不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。
“一个模型三个特征”实例剖析“一个模型三个特征”这部分是理论知识，比较抽象，你看了之后可能还是有点懵，有种似懂非懂的感觉，没关系，这个很正常。接下来，我结合一个具体的动态规划问题，来给你详细解释。
假设我们有一个n乘以n的矩阵w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？
我们先看看，这个问题是否符合“一个模型”？
从(0, 0)走到(n-1, n-1)，总共要走2*(n-1)步，也就对应着2*(n-1)个阶段。每个阶段都有向右走或者向下走两种决策，并且每个阶段都会对应一个状态集合。
我们把状态定义为min_dist(i, j)，其中i表示行，j表示列。min_dist表达式的值表示从(0, 0)到达(i, j)的最短路径长度。所以，这个问题是一个多阶段决策最优解问题，符合动态规划的模型。
我们再来看，这个问题是否符合“三个特征”？
我们可以用回溯算法来解决这个问题。如果你自己写一下代码，画一下递归树，就会发现，递归树中有重复的节点。重复的节点表示，从左上角到节点对应的位置，有多种路线，这也能说明这个问题中存在重复子问题。
如果我们走到(i, j)这个位置，我们只能通过(i-1, j)，(i, j-1)这两个位置移动过来，也就是说，我们想要计算(i, j)位置对应的状态，只需要关心(i-1, j)，(i, j-1)两个位置对应的状态，并不关心棋子是通过什么样的路线到达这两个位置的。而且，我们仅仅允许往下和往右移动，不允许后退，所以，前面阶段的状态确定之后，不会被后面阶段的决策所改变，所以，这个问题符合“无后效性”这一特征。
刚刚定义状态的时候，我们把从起始位置(0, 0)到(i, j)的最小路径，记作min_dist(i, j)。因为我们只能往右或往下移动，所以，我们只有可能从(i, j-1)或者(i-1, j)两个位置到达(i, j)。也就是说，到达(i, j)的最短路径要么经过(i, j-1)，要么经过(i-1, j)，而且到达(i, j)的最短路径肯定包含到达这两个位置的最短路径之一。换句话说就是，min_dist(i, j)可以通过min_dist(i, j-1)和min_dist(i-1, j)两个状态推导出来。这就说明，这个问题符合“最优子结构”。
min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j)) 两种动态规划解题思路总结刚刚我讲了，如何鉴别一个问题是否可以用动态规划来解决。现在，我再总结一下，动态规划解题的一般思路，让你面对动态规划问题的时候，能够有章可循，不至于束手无策。
我个人觉得，解决动态规划问题，一般有两种思路。我把它们分别叫作，状态转移表法和状态转移方程法。
1.状态转移表法一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。所以，当我们拿到问题的时候，我们可以先用简单的回溯算法解决，然后定义状态，每个状态表示一个节点，然后对应画出递归树。从递归树中，我们很容易可以看出来，是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。
找到重复子问题之后，接下来，我们有两种处理思路，第一种是直接用回溯加“备忘录”的方法，来避免重复子问题。从执行效率上来讲，这跟动态规划的解决思路没有差别。第二种是使用动态规划的解决方法，状态转移表法。第一种思路，我就不讲了，你可以看看上一节的两个例子。我们重点来看状态转移表法是如何工作的。
我们先画出一个状态表。状态表一般都是二维的，所以你可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。
尽管大部分状态表都是二维的，但是如果问题的状态比较复杂，需要很多变量来表示，那对应的状态表可能就是高维的，比如三维、四维。那这个时候，我们就不适合用状态转移表法来解决了。一方面是因为高维状态转移表不好画图表示，另一方面是因为人脑确实很不擅长思考高维的东西。
现在，我们来看一下，如何套用这个状态转移表法，来解决之前那个矩阵最短路径的问题？
从起点到终点，我们有很多种不同的走法。我们可以穷举所有走法，然后对比找出一个最短走法。不过如何才能无重复又不遗漏地穷举出所有走法呢？我们可以用回溯算法这个比较有规律的穷举算法。
回溯算法的代码实现如下所示。代码很短，而且我前面也分析过很多回溯算法的例题，这里我就不多做解释了，你自己来看看。
private int minDist = Integer.MAX_VALUE; // 全局变量或者成员变量 // 调用方式：minDistBacktracing(0, 0, 0, w, n); public void minDistBT(int i, int j, int dist, int[][] w, int n) { // 到达了n-1, n-1这个位置了，这里看着有点奇怪哈，你自己举个例子看下 if (i == n &amp;amp;&amp;amp; j == n) { if (dist &amp;lt; minDist) minDist = dist; return; } if (i &amp;lt; n) { // 往下走，更新i=i+1, j=j minDistBT(i + 1, j, dist+w[i][j], w, n); } if (j &amp;lt; n) { // 往右走，更新i=i, j=j+1 minDistBT(i, j+1, dist+w[i][j], w, n); } } 有了回溯代码之后，接下来，我们要画出递归树，以此来寻找重复子问题。在递归树中，一个状态（也就是一个节点）包含三个变量(i, j, dist)，其中i，j分别表示行和列，dist表示从起点到达(i, j)的路径长度。从图中，我们看出，尽管(i, j, dist)不存在重复的，但是(i, j)重复的有很多。对于(i, j)重复的节点，我们只需要选择dist最小的节点，继续递归求解，其他节点就可以舍弃了。</description></item><item><title>41_怎么最快地复制一张表？</title><link>https://artisanbox.github.io/1/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/41/</guid><description>我在上一篇文章最后，给你留下的问题是怎么在两张表中拷贝数据。如果可以控制对源表的扫描行数和加锁范围很小的话，我们简单地使用insert … select 语句即可实现。
当然，为了避免对源表加读锁，更稳妥的方案是先将数据写到外部文本文件，然后再写回目标表。这时，有两种常用的方法。接下来的内容，我会和你详细展开一下这两种方法。
为了便于说明，我还是先创建一个表db1.t，并插入1000行数据，同时创建一个相同结构的表db2.t。
create database db1; use db1; create table t(id int primary key, a int, b int, index(a))engine=innodb; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=1000)do insert into t values(i,i,i); set i=i+1; end while; end;; delimiter ; call idata();
create database db2; create table db2.t like db1.t 假设，我们要把db1.t里面a&amp;gt;900的数据行导出来，插入到db2.t中。
mysqldump方法一种方法是，使用mysqldump命令将数据导出成一组INSERT语句。你可以使用下面的命令：
mysqldump -h$host -P$port -u$user &amp;ndash;add-locks=0 &amp;ndash;no-create-info &amp;ndash;single-transaction &amp;ndash;set-gtid-purged=OFF db1 t &amp;ndash;where=&amp;quot;a&amp;gt;900&amp;quot; &amp;ndash;result-file=/client_tmp/t.sql 把结果输出到临时文件。</description></item><item><title>41_服务接口：如何搭建沟通桥梁？</title><link>https://artisanbox.github.io/9/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/41/</guid><description>你好，我是LMOS。
一路走来，咱们的Cosmos系统已经有内存管理，进程、文件、I/O了，这些重要的组件已经建立了，也就是说它们可以向应用程序提供服务了。
但就好像你去各政府部门办理业务证件一样，首先是前台工作人员接待你，对你的业务需求进行初级预判，然后后台人员进行审核并进行业务办理，最后由前台人员回复，并且给你开好相关业务证件。
今天，我们就来实现Cosmos下的“前台工作人员”，我们称之为服务接口，也可以说是Cosmos的API。代码你可以从这里下载。
服务接口的结构我们先来设计一下服务接口的整体结构，即Cosmos的API结构。因为Cosmos的API数量很多，所以我们先来分个类，它们分别是进程类、内存类、文件类和时间类的API。这些API还会被上层C库封装，方便应用程序调用。
为了帮你理解它们之间的关系，我为你准备了一幅图，如下所示。
结合上图可以看到，我们的应用程序库分为时间库、进程库、内存库、文件库这几种类型。
通常情况下，应用程序中调用的是一些库函数。库函数是对系统服务的封装，有的库函数是直接调用相应的系统服务；而有的库函数为了完成特定的功能，则调用了几个相应的系统服务；还有一些库函数完成的功能不需要调用相应的系统调用，这时前台接待人员也就是“库函数”，可以自行处理。
如何进入内核由上图我们还可以看出，应用程序和库函数都在用户空间中，而系统服务却在内核空间中，想要让代码控制流从用户空间进入到内核空间中，如何穿过CPU保护模式的“铜墙铁壁”才是关键。下面我们就一起来探索这个问题。
软中断指令请你回忆下，CPU长模式下如何处理中断的（不熟悉的可以回看第5课和第13课）？
设备向CPU发送一个中断信号，CPU接受到这个电子信号后，在允许响应中断的情况下，就会中断当前正在运行的程序，自动切换到相应的CPU R0特权级，并跳转到中断门描述符中相应的地址上运行中断处理代码。
当然，这里的中断处理代码就是操作系统内核的代码，这样CPU的控制权就转到操作系统内核的手中了。
其实，应用软件也可以给CPU发送中断。现代CPU设计时都会设计这样一条指令，一旦执行该指令，CPU就要中断当前正在运行的程序，自动跳转到相应的固定地址上运行代码。当然这里的代码也就是操作系统内核的代码，就这样CPU的控制权同样会回到操作系统内核的手中。
因为这条指令模拟了中断的电子信号，所以称为软中断指令。在x86 CPU上这条指令是int指令。例如int255。int指令后面需要跟一个常数，这个常数表示CPU从中断表描述符表中取得第几个中断描述符进入内核。
传递参数虽然int指令提供了应用程序进入操作系统内核函数的底层机制，但是我们还需要解决参数传递的问题。
因为你必须要告诉操作系统你要干什么，系统才能做出相应的反馈。比如你要分配内存，分配多大的内存，这些信息必须要以参数的形式传递给操作系统内核。
因为应用程序运行在用户空间时，用的是用户栈，当它切换到内核空间时，用的是内核栈。所以参数的传递，就需要硬性地规定一下，要么所有的参数都用寄存器传递，要么所有的参数都保存在用户栈中。
显然，第一种用寄存器传递所有参数的方法要简单得多，事实上有很多操作系统就是用寄存器传递参数的。
我们使用RBX、RCX、RDX、RDI、RSI这5个寄存器来传递参数，事实上一个系统服务接口函数不会超过5个参数，所以这是足够的。而RAX寄存器中保存着一个整数，称为系统服务号。在系统服务分发器中，会根据这个系统服务号调用相应的函数。
因为C编译器不能处理这种参数传递形式，另外C编译器也不支持int指令，所以要用汇编代码来处理这种问题。
下面我们来建立一个cosmos/include/libinc/lapinrentry.h文件，在这里写上后面的代码。
//传递一个参数所用的宏 #define API_ENTRY_PARE1(intnr,rets,pval1) \ __asm__ __volatile__(\ &amp;quot;movq %[inr],%%rax\n\t&amp;quot;\//系统服务号 &amp;quot;movq %[prv1],%%rbx\n\t&amp;quot;\//第一个参数 &amp;quot;int $255 \n\t&amp;quot;\//触发中断 &amp;quot;movq %%rax,%[retval] \n\t&amp;quot;\//处理返回结果 :[retval] &amp;quot;=r&amp;quot; (rets)\ :[inr] &amp;quot;r&amp;quot; (intnr),[prv1]&amp;quot;r&amp;quot; (pval1)\ :&amp;quot;rax&amp;quot;,&amp;quot;rbx&amp;quot;,&amp;quot;cc&amp;quot;,&amp;quot;memory&amp;quot;\ ) //传递四个参数所用的宏 #define API_ENTRY_PARE4(intnr,rets,pval1,pval2,pval3,pval4) \ __asm__ __volatile__(\ &amp;quot;movq %[inr],%%rax \n\t&amp;quot;\//系统服务号 &amp;quot;movq %[prv1],%%rbx \n\t&amp;quot;\//第一个参数 &amp;quot;movq %[prv2],%%rcx \n\t&amp;quot;\//第二个参数 &amp;quot;movq %[prv3],%%rdx \n\t&amp;quot;\//第三个参数 &amp;quot;movq %[prv4],%%rsi \n\t&amp;quot;\//第四个参数 &amp;quot;int $255 \n\t&amp;quot;\//触发中断 &amp;quot;movq %%rax,%[retval] \n\t&amp;quot;\//处理返回结果 :[retval] &amp;quot;=r&amp;quot; (rets)\ :[inr] &amp;quot;r&amp;quot; (intnr),[prv1]&amp;quot;g&amp;quot; (pval1),\ [prv2] &amp;quot;g&amp;quot; (pval2),[prv3]&amp;quot;g&amp;quot; (pval3),\ [prv4] &amp;quot;g&amp;quot; (pval4)\ :&amp;quot;rax&amp;quot;,&amp;quot;rbx&amp;quot;,&amp;quot;rcx&amp;quot;,&amp;quot;rdx&amp;quot;,&amp;quot;rsi&amp;quot;,&amp;quot;cc&amp;quot;,&amp;quot;memory&amp;quot;\ ) 上述代码中只展示了两个宏。其实是有四个，在代码文件中我已经帮你写好了，主要功能是用来解决传递参数和触发中断问题，并且还需要处理系统返回的结果。这些都是用C语言中嵌入汇编代码的方式来实现的。</description></item><item><title>41_理解内存（下）：解析TLB和内存保护</title><link>https://artisanbox.github.io/4/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/41/</guid><description>机器指令里面的内存地址都是虚拟内存地址。程序里面的每一个进程，都有一个属于自己的虚拟内存地址空间。我们可以通过地址转换来获得最终的实际物理地址。我们每一个指令都存放在内存里面，每一条数据都存放在内存里面。因此，“地址转换”是一个非常高频的动作，“地址转换”的性能就变得至关重要了。这就是我们今天要讲的第一个问题，也就是性能问题。
因为我们的指令、数据都存放在内存里面，这里就会遇到我们今天要谈的第二个问题，也就是内存安全问题。如果被人修改了内存里面的内容，我们的CPU就可能会去执行我们计划之外的指令。这个指令可能是破坏我们服务器里面的数据，也可能是被人获取到服务器里面的敏感信息。
现代的CPU和操作系统，会通过什么样的方式来解决这两个问题呢？别着急，等讲完今天的内容，你就知道答案了。
加速地址转换：TLB上一节我们说了，从虚拟内存地址到物理内存地址的转换，我们通过页表这个数据结构来处理。为了节约页表的内存存储空间，我们会使用多级页表数据结构。
不过，多级页表虽然节约了我们的存储空间，但是却带来了时间上的开销，变成了一个“以时间换空间”的策略。原本我们进行一次地址转换，只需要访问一次内存就能找到物理页号，算出物理内存地址。但是用了4级页表，我们就需要访问4次内存，才能找到物理页号。
我们知道，内存访问其实比Cache要慢很多。我们本来只是要做一个简单的地址转换，现在反而要一下子多访问好多次内存。这种情况该怎么处理呢？你是否还记得之前讲过的“加个缓存”的办法呢？我们来试一试。
程序所需要使用的指令，都顺序存放在虚拟内存里面。我们执行的指令，也是一条条顺序执行下去的。也就是说，我们对于指令地址的访问，存在前面几讲所说的“空间局部性”和“时间局部性”，而需要访问的数据也是一样的。我们连续执行了5条指令。因为内存地址都是连续的，所以这5条指令通常都在同一个“虚拟页”里。
因此，这连续5次的内存地址转换，其实都来自于同一个虚拟页号，转换的结果自然也就是同一个物理页号。那我们就可以用前面几讲说过的，用一个“加个缓存”的办法。把之前的内存转换地址缓存下来，使得我们不需要反复去访问内存来进行内存地址转换。
于是，计算机工程师们专门在CPU里放了一块缓存芯片。这块缓存芯片我们称之为TLB，全称是地址变换高速缓冲（Translation-Lookaside Buffer）。这块缓存存放了之前已经进行过地址转换的查询结果。这样，当同样的虚拟地址需要进行地址转换的时候，我们可以直接在TLB里面查询结果，而不需要多次访问内存来完成一次转换。
TLB和我们前面讲的CPU的高速缓存类似，可以分成指令的TLB和数据的TLB，也就是ITLB和DTLB。同样的，我们也可以根据大小对它进行分级，变成L1、L2这样多层的TLB。
除此之外，还有一点和CPU里的高速缓存也是一样的，我们需要用脏标记这样的标记位，来实现“写回”这样缓存管理策略。
为了性能，我们整个内存转换过程也要由硬件来执行。在CPU芯片里面，我们封装了内存管理单元（MMU，Memory Management Unit）芯片，用来完成地址转换。和TLB的访问和交互，都是由这个MMU控制的。
安全性与内存保护讲完了虚拟内存和物理内存的转换，我们来看看内存保护和安全性的问题。
进程的程序也好，数据也好，都要存放在内存里面。实际程序指令的执行，也是通过程序计数器里面的地址，去读取内存内的内容，然后运行对应的指令，使用相应的数据。
虽然我们现代的操作系统和CPU，已经做了各种权限的管控。正常情况下，我们已经通过虚拟内存地址和物理内存地址的区分，隔离了各个进程。但是，无论是CPU这样的硬件，还是操作系统这样的软件，都太复杂了，难免还是会被黑客们找到各种各样的漏洞。
就像我们在软件开发过程中，常常会有一个“兜底”的错误处理方案一样，在对于内存的管理里面，计算机也有一些最底层的安全保护机制。这些机制统称为内存保护（Memory Protection）。我这里就为你简单介绍两个。
可执行空间保护第一个常见的安全机制，叫可执行空间保护（Executable Space Protection）。
这个机制是说，我们对于一个进程使用的内存，只把其中的指令部分设置成“可执行”的，对于其他部分，比如数据部分，不给予“可执行”的权限。因为无论是指令，还是数据，在我们的CPU看来，都是二进制的数据。我们直接把数据部分拿给CPU，如果这些数据解码后，也能变成一条合理的指令，其实就是可执行的。
这个时候，黑客们想到了一些搞破坏的办法。我们在程序的数据区里，放入一些要执行的指令编码后的数据，然后找到一个办法，让CPU去把它们当成指令去加载，那CPU就能执行我们想要执行的指令了。对于进程里内存空间的执行权限进行控制，可以使得CPU只能执行指令区域的代码。对于数据区域的内容，即使找到了其他漏洞想要加载成指令来执行，也会因为没有权限而被阻挡掉。
其实，在实际的应用开发中，类似的策略也很常见。我下面给你举两个例子。
比如说，在用PHP进行Web开发的时候，我们通常会禁止PHP有eval函数的执行权限。这个其实就是害怕外部的用户，所以没有把数据提交到服务器，而是把一段想要执行的脚本提交到服务器。服务器里在拼装字符串执行命令的时候，可能就会执行到预计之外被“注入”的破坏性脚本。这里我放了一个例子，用这个办法可以去删除服务器上的数据。
script.php?param1=xxx //我们的PHP接受一个传入的参数，这个参数我们希望提供计算功能 $code = eval($_GET[&amp;quot;param1&amp;quot;]); // 我们直接通过 eval 计算出来对应的参数公式的计算结果 script.php?param1=&amp;quot;;%20echo%20exec(&amp;lsquo;rm -rf ~/&amp;rsquo;);%20// // 用户传入的参数里面藏了一个命令 $code = &amp;quot;&amp;quot;; echo exec(&amp;lsquo;rm -rf ~/&amp;rsquo;); //&amp;quot;; // 执行的结果就变成了删除服务器上的数据 还有一个例子就是SQL注入攻击。如果服务端执行的SQL脚本是通过字符串拼装出来的，那么在Web请求里面传输的参数就可以藏下一些我们想要执行的SQL，让服务器执行一些我们没有想到过的SQL语句。这样的结果就是，或者破坏了数据库里的数据，或者被人拖库泄露了数据。
地址空间布局随机化第二个常见的安全机制，叫地址空间布局随机化（Address Space Layout Randomization）。
内存层面的安全保护核心策略，是在可能有漏洞的情况下进行安全预防。上面的可执行空间保护就是一个很好的例子。但是，内存层面的漏洞还有其他的可能性。
这里的核心问题是，其他的人、进程、程序，会去修改掉特定进程的指令、数据，然后，让当前进程去执行这些指令和数据，造成破坏。要想修改这些指令和数据，我们需要知道这些指令和数据所在的位置才行。
原先我们一个进程的内存布局空间是固定的，所以任何第三方很容易就能知道指令在哪里，程序栈在哪里，数据在哪里，堆又在哪里。这个其实为想要搞破坏的人创造了很大的便利。而地址空间布局随机化这个机制，就是让这些区域的位置不再固定，在内存空间随机去分配这些进程里不同部分所在的内存空间地址，让破坏者猜不出来。猜不出来呢，自然就没法找到想要修改的内容的位置。如果只是随便做点修改，程序只会crash掉，而不会去执行计划之外的代码。
这样的“随机化”策略，其实也是我们日常应用开发中一个常见的策略。一个大家都应该接触过的例子就是密码登陆功能。网站和App都会需要你设置用户名和密码，之后用来登陆自己的账号。然后，在服务器端，我们会把用户名和密码保存下来，在下一次用户登陆的时候，使用这个用户名和密码验证。
我们的密码当然不能明文存储在数据库里，不然就会有安全问题。如果明文存储在数据库里，意味着能拿到数据库访问权限的人，都能看到用户的明文密码。这个可能是因为安全漏洞导致被人拖库，而且网站的管理员也能直接看到所有的用户名和密码信息。
比如，前几年CSDN就发生过被人拖库的事件。虽然用户名和密码都是明文保存的，别人如果只是拿到了CSDN网站的用户名密码，用户的损失也不会太大。但是很多用户可能会在不同的网站使用相同的密码，如果拿到这些用户名和密码的人，能够成功登录用户的银行、支付、社交等等其他网站的话，用户损失就大了去了。
于是，大家会在数据库里存储密码的哈希值，比如用现在常用的SHA256，生成一一个验证的密码哈希值。但是这个往往还是不够的。因为同样的密码，对应的哈希值都是相同的，大部分用户的密码又常常比较简单。于是，拖库成功的黑客可以通过彩虹表的方式，来推测出用户的密码。
这个时候，我们的“随机化策略”就可以用上了。我们可以在数据库里，给每一个用户名生成一个随机的、使用了各种特殊字符的盐值（Salt）。这样，我们的哈希值就不再是仅仅使用密码来生成的了，而是密码和盐值放在一起生成的对应的哈希值。哈希值的生成中，包括了一些类似于“乱码”的随机字符串，所以通过彩虹表碰撞来猜出密码的办法就用不了了。
$password = &amp;quot;goodmorning12345&amp;quot;; // 我们的密码是明文存储的</description></item><item><title>41｜后端优化：生成LIR和指令选择</title><link>https://artisanbox.github.io/3/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/43/</guid><description>你好，我是宫文学。
前面几节课中，我们讨论的主要是中端的优化。中端优化是跟具体硬件关系并不大，但由于我们还要生成针对具体CPU的汇编代码或机器码，所以做完中端优化之后，我们还要针对具体CPU的特性来做一些优化，也就是后端优化。
其实，我们已经接触过一些后端优化技术了。比如，之前我们已经讲过寄存器分配算法、尾调用和尾递归的优化，这些基本上都属于后端优化。不过，那个时候，我们是从AST直接生成汇编代码，然后在这个过程中做一些后端优化的。
在第三部分优化篇中，我们引入了新的、基于图的IR，进行了很多与硬件无关的优化。用这个基于图的IR来做后端优化，效果又怎样呢？接下来，我们就要修改以前的生成汇编代码的逻辑，改成从这个IR来生成汇编代码，并在这个过程中做一些优化。
今天这一节课，我就带你从中端过渡到后端，看看如何实现后端优化，并生成目标代码。这其中包括IR的Lower、生成LIR和指令选择，以及寄存器分配、指令重排序、窥孔优化和汇编代码的生成，等等工作。不过有些知识点我之前已经讲过了，还有一些知识点不是我们这门课的重点，我就不再展开讲了，重点帮你贯穿一下整个过程。
首先，我们先了解一下给IR做Lower的过程。
Lower过程HIR要经过一系列Lower过程，最后变成LIR。我先举一个例子，让你理解一下在Lower过程中会发生什么事情。这是一个简单的例子，它只实现了给mammal对象weight字段赋值的功能：
function accessField(mammal:Mammal, weight:number){ &amp;nbsp; &amp;nbsp; mammal.weight = weight; } 针对对象属性的赋值，通常编译器要生成写内存的指令。这是因为，对象通常使用的是在堆里申请的内存。而且，由于一个对象可能会由多个线程访问，所以只有把对象属性写到内存里，另一个线程才能访问到更新后的属性。
当然，我们前一节课也说过，如果这个对象并没有逃逸，那就是另一种情况了。我们先假设该对象是逃逸的，那么我们要给对象属性赋值，首先就需要进行写内存的操作。
对于这个简单的场景，一开始这个程序的IR是下面这样：
这里，我们使用了一个抽象度比较高的节点，叫做StoreField。它接受两个输入：一个输入是对象的引用，也就是对象地址；第二个输入是weight属性的值。
在编译的过程中，这个IR会被做Lower处理。StoreField节点会被一个Write节点代替，Write节点是一个写内存的操作。而内存地址呢，用OffsetAddress表示，也就是一个基地址加上一定的偏移量。基地址就是对象的地址，偏移量是对象头的大小。在PlayScript的设计中，它是16个字节。Lower过一次的IR图是这样的：
到目前为止，这个IR还是跟具体CPU无关的。因为按照这张IR图，无论针对什么CPU，你都可以通过在某个地址的基础上加上一个偏移量，来获得新的地址。
然后这个IR还会进一步被Lower，让地址的表示方式更贴近x86-64（或AMD64）架构的具体寻址方式。我们曾经学过x86-64的寻址方式，它的完整形式包括基地址、偏移量、下标值、元素字节数等多个参数。但这里我们只需要它的简化方式，也就是基地址加上一个偏移量就行。进一步Lower的IR变成了这样：
到了这一步，我们的IR已经变得跟具体CPU架构相关了。接下来，我们就把它彻底转化成LIR的格式。
生成LIR和指令选择那LIR又是什么样子的呢？你可以思考一下，如果你来设计编译器，应该如何设计LIR呢？
LIR的目的，是进行机器相关的优化，并最后生成汇编代码。所以，大部分LIR的设计，都是跟汇编代码是同构的。也就是说，LIR是由一条条指令构成的，指令是放在基本块中的，而基本块之间存在跳转关系。
从这个意义上说，我们之前生成汇编代码的时候，已经设计过这样的LIR。而Graal、LLVM、Go语言的gc编译器，在生成汇编代码或机器码之前也都有类似的LIR设计。这个数据结构看上去仍然是基于CFG的，但我们目前已经不需要分析它的控制流和数据流，并调整里面的代码了。这些工作，我们在中端优化的时候都已经完成了。现在，基于这个LIR，我们关心的主要是指令选择、寄存器分配和指令重排序（或者叫做指令调度）这样的话题。
由于我们的IR设计借鉴了Graal编译器，那么同样的，我们继续跟着Graal看看它是怎么处理LIR的。图中是Graal编译器中生成的LIR的例子，你可以通过它建立对LIR的直观感觉：
这张图中一行行的文本，是为了显示LIR中内容，便于调试，实际上的LIR都是内存里的一条条的指令对象。在这个图中，你还能看到Graal编译器的后端处理过程，包括生成LIR、寄存器分配，一直到生成目标代码。我们自己实现的编译器，也需要完成类似的功能。
好了，我们现在已经理解了LIR是什么样子了。那我们现在就从HIR生成LIR，并在这个过程中进行指令的选择，这又可以分成几项子任务。
首先，我们要把HIR中的不同节点分配到不同的基本块中，这被叫做调度算法（Schedule）。
我们在39节已经介绍过，由于很多数据流节点是浮动的，我们可以自由地选择在什么时候进行计算。但我们在生成汇编代码之前，还是要把确定这些数据节点的计算时机，因此要把它们分配到具体的基本块中。
而且，我们需要基于一些规则来完成这个分配工作，比如对于循环无关的代码，我们会提到循环外边；而基于控制流来求值的代码，比如if语句的两个不同分支的代码，我们尽量分配到这两个分支对应的基本块中。制定这些规则的出发点，是尽可能地提升程序的性能，但其实并不能完全保证。在比较AOT和JIT时，我们已经讲过这点了。
在划分好基本块以后，我们再做第二项工作，把IR图转化成LIR的指令，并在这个过程中进行指令的选择。
如果细讲起来，指令选择有两层含义，而这两层含义的工作经常是一起实现的。指令选择的第一层含义，是把抽象的运算，准确地Lower到硬件的具体指令上。比如说，我们可以从比较抽象的层次，对整数和浮点数都执行加法运算。但到了CPU层面，整数的加法指令和浮点数的加法指令就不一样了。其他指令，比如比较运算、数据拷贝的指令，也是跟数据类型和所采用的指令集相关的，编译器要确定出正确的指令。
指令选择的第二层含义，指的是相同的功能，可以用不同的指令组合来实现，而我们要尽量选择让整体性能最优的那组指令。这实际上是一个最优化问题。
关于指令选择的算法，我在《编译原理之美》的29节做过一些理论性的介绍，在《编译原理实战课》的16节，我也介绍过Graal编译器的具体实现。这门课，我也会参考Graal的思路，做一个比较简化的实现。
在我看来，要理解指令选择，除了学习算法，更重要的是要了解很多具体的指令选择场景。下面，我们就以x86-64架构的指令来举几个例子，帮助你建立直观理解。经过这些讲解后，你就能理解那些抽象的算法到底在说些什么了。
第一个例子，是经常出现在if语句中的条件跳转指令：
if(a&amp;gt;b){ //somecode } else{ //some other code } 回忆一下，我们在这门课的第9节、为字节码虚拟机生成字节码的时候，if条件和跳转相关的字节码是分两步来生成的：第一步，处理if条件，计算条件表达式"a&amp;lt;b"，并生成1或0两个值，代表true和false；第二步，处理if节点，根据&amp;lt;节点的值来生成跳转指令。跳转指令使用JE或JNE就行了，也就是比较if条件是不是1。
用这个方式生成指令比较简单。算法上说，就是对每个AST节点依次进行处理。像字面量、变量这样的节点，我们会返回一个Operand。而对于计算性节点，我们就要生成指令，并把指令运行的结果作为Operand来返回。
不过，大部分CPU或虚拟机都提供了更丰富的条件跳转指令，比如JL指令就可以用于在a&amp;lt;b的时候做跳转，而JG指令就可以用于在a&amp;gt;b的时候跳转。这个时候，我们需要同时处理if和&amp;lt;号两个节点，确定采用什么指令，你可以看一下这张示意图：
这就是指令选择算法的特点，我们需要一次性地考虑AST或IR中的多个节点，并生成合适的指令，只要最后算法确实覆盖了所有节点就行。
第二类经常需要做指令选择的情况，是对内存的访问。我还是用这节课开头这个、给mammal对象的weight属性赋值的例子来做说明：
function accessField(mammal:Mammal, weight:number){ &amp;nbsp; &amp;nbsp; mammal.weight = weight; } 在生成指令的时候，我们需要在对象的基地址的基础上，添加一个偏移量，获得weight属性的地址，然后再给这个地址赋值。
要完成这个操作，我们有两个办法。第一个办法是分成两步来生成指令，第一步是先计算出weight属性的地址，第二步是往内存地址写weight的值：
这两步对应的LIR相当于下面两条代码：
add $16, p0 #把p0,也就是对象的地址加上偏移量 mov p1, (p0) #把p1赋给p0指向的内存地址 不过，我们还有第二个方法来生成指令，这就是直接使用x86-64的寻址方式，用一条指令就能完成地址计算和写内存这两个操作。我们的指令选择算法需要一次性处理Write和AMD64Address两个节点：</description></item><item><title>42_grant之后要跟着flushprivileges吗？</title><link>https://artisanbox.github.io/1/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/42/</guid><description>在MySQL里面，grant语句是用来给用户赋权的。不知道你有没有见过一些操作文档里面提到，grant之后要马上跟着执行一个flush privileges命令，才能使赋权语句生效。我最开始使用MySQL的时候，就是照着一个操作文档的说明按照这个顺序操作的。
那么，grant之后真的需要执行flush privileges吗？如果没有执行这个flush命令的话，赋权语句真的不能生效吗？
接下来，我就先和你介绍一下grant语句和flush privileges语句分别做了什么事情，然后再一起来分析这个问题。
为了便于说明，我先创建一个用户：
create user 'ua'@'%' identified by 'pa'; 这条语句的逻辑是创建一个用户’ua’@’%’，密码是pa。注意，在MySQL里面，用户名(user)+地址(host)才表示一个用户，因此 ua@ip1 和 ua@ip2代表的是两个不同的用户。
这条命令做了两个动作：
磁盘上，往mysql.user表里插入一行，由于没有指定权限，所以这行数据上所有表示权限的字段的值都是N；
内存里，往数组acl_users里插入一个acl_user对象，这个对象的access字段值为0。
图1就是这个时刻用户ua在user表中的状态。
图1 mysql.user 数据行在MySQL中，用户权限是有不同的范围的。接下来，我就按照用户权限范围从大到小的顺序依次和你说明。
全局权限全局权限，作用于整个MySQL实例，这些权限信息保存在mysql库的user表里。如果我要给用户ua赋一个最高权限的话，语句是这么写的：
grant all privileges on *.* to 'ua'@'%' with grant option; 这个grant命令做了两个动作：
磁盘上，将mysql.user表里，用户’ua’@’%'这一行的所有表示权限的字段的值都修改为‘Y’；
内存里，从数组acl_users中找到这个用户对应的对象，将access值（权限位）修改为二进制的“全1”。
在这个grant命令执行完成后，如果有新的客户端使用用户名ua登录成功，MySQL会为新连接维护一个线程对象，然后从acl_users数组里查到这个用户的权限，并将权限值拷贝到这个线程对象中。之后在这个连接中执行的语句，所有关于全局权限的判断，都直接使用线程对象内部保存的权限位。
基于上面的分析我们可以知道：
grant 命令对于全局权限，同时更新了磁盘和内存。命令完成后即时生效，接下来新创建的连接会使用新的权限。
对于一个已经存在的连接，它的全局权限不受grant命令的影响。
需要说明的是，一般在生产环境上要合理控制用户权限的范围。我们上面用到的这个grant语句就是一个典型的错误示范。如果一个用户有所有权限，一般就不应该设置为所有IP地址都可以访问。
如果要回收上面的grant语句赋予的权限，你可以使用下面这条命令：
revoke all privileges on *.* from 'ua'@'%'; 这条revoke命令的用法与grant类似，做了如下两个动作：
磁盘上，将mysql.</description></item><item><title>42_动态规划实战：如何实现搜索引擎中的拼写纠错功能？</title><link>https://artisanbox.github.io/2/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/43/</guid><description>在Trie树那节我们讲过，利用Trie树，可以实现搜索引擎的关键词提示功能，这样可以节省用户输入搜索关键词的时间。实际上，搜索引擎在用户体验方面的优化还有很多，比如你可能经常会用的拼写纠错功能。
当你在搜索框中，一不小心输错单词时，搜索引擎会非常智能地检测出你的拼写错误，并且用对应的正确单词来进行搜索。作为一名软件开发工程师，你是否想过，这个功能是怎么实现的呢？
如何量化两个字符串的相似度？计算机只认识数字，所以要解答开篇的问题，我们就要先来看，如何量化两个字符串之间的相似程度呢？有一个非常著名的量化方法，那就是编辑距离（Edit Distance）。
顾名思义，编辑距离指的就是，将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。编辑距离越大，说明两个字符串的相似程度越小；相反，编辑距离就越小，说明两个字符串的相似程度越大。对于两个完全相同的字符串来说，编辑距离就是0。
根据所包含的编辑操作种类的不同，编辑距离有多种不同的计算方式，比较著名的有莱文斯坦距离（Levenshtein distance）和最长公共子串长度（Longest common substring length）。其中，莱文斯坦距离允许增加、删除、替换字符这三个编辑操作，最长公共子串长度只允许增加、删除字符这两个编辑操作。
而且，莱文斯坦距离和最长公共子串长度，从两个截然相反的角度，分析字符串的相似程度。莱文斯坦距离的大小，表示两个字符串差异的大小；而最长公共子串的大小，表示两个字符串相似程度的大小。
关于这两个计算方法，我举个例子给你说明一下。这里面，两个字符串mitcmu和mtacnu的莱文斯坦距离是3，最长公共子串长度是4。
了解了编辑距离的概念之后，我们来看，如何快速计算两个字符串之间的编辑距离？
如何编程计算莱文斯坦距离？之前我反复强调过，思考过程比结论更重要，所以，我现在就给你展示一下，解决这个问题，我的完整的思考过程。
这个问题是求把一个字符串变成另一个字符串，需要的最少编辑次数。整个求解过程，涉及多个决策阶段，我们需要依次考察一个字符串中的每个字符，跟另一个字符串中的字符是否匹配，匹配的话如何处理，不匹配的话又如何处理。所以，这个问题符合多阶段决策最优解模型。
我们前面讲了，贪心、回溯、动态规划可以解决的问题，都可以抽象成这样一个模型。要解决这个问题，我们可以先看一看，用最简单的回溯算法，该如何来解决。
回溯是一个递归处理的过程。如果a[i]与b[j]匹配，我们递归考察a[i+1]和b[j+1]。如果a[i]与b[j]不匹配，那我们有多种处理方式可选：
可以删除a[i]，然后递归考察a[i+1]和b[j]；
可以删除b[j]，然后递归考察a[i]和b[j+1]；
可以在a[i]前面添加一个跟b[j]相同的字符，然后递归考察a[i]和b[j+1];
可以在b[j]前面添加一个跟a[i]相同的字符，然后递归考察a[i+1]和b[j]；
可以将a[i]替换成b[j]，或者将b[j]替换成a[i]，然后递归考察a[i+1]和b[j+1]。
我们将上面的回溯算法的处理思路，翻译成代码，就是下面这个样子：
private char[] a = &amp;quot;mitcmu&amp;quot;.toCharArray(); private char[] b = &amp;quot;mtacnu&amp;quot;.toCharArray(); private int n = 6; private int m = 6; private int minDist = Integer.MAX_VALUE; // 存储结果 // 调用方式 lwstBT(0, 0, 0); public lwstBT(int i, int j, int edist) { if (i == n || j == m) { if (i &amp;lt; n) edist += (n-i); if (j &amp;lt; m) edist += (m - j); if (edist &amp;lt; minDist) minDist = edist; return; } if (a[i] == b[j]) { // 两个字符匹配 lwstBT(i+1, j+1, edist); } else { // 两个字符不匹配 lwstBT(i + 1, j, edist + 1); // 删除a[i]或者b[j]前添加一个字符 lwstBT(i, j + 1, edist + 1); // 删除b[j]或者a[i]前添加一个字符 lwstBT(i + 1, j + 1, edist + 1); // 将a[i]和b[j]替换为相同字符 } } 根据回溯算法的代码实现，我们可以画出递归树，看是否存在重复子问题。如果存在重复子问题，那我们就可以考虑能否用动态规划来解决；如果不存在重复子问题，那回溯就是最好的解决方法。</description></item><item><title>42_总线：计算机内部的高速公路</title><link>https://artisanbox.github.io/4/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/42/</guid><description>专栏讲到现在，如果我再问你，计算机五大组成部分是什么，应该没有人不知道了吧？我们这一节要讲的内容，依然要围绕这五大部分，控制器、运算器、存储器、输入设备和输出设备。
CPU所代表的控制器和运算器，要和存储器，也就是我们的主内存，以及输入和输出设备进行通信。那问题来了，CPU从我们的键盘、鼠标接收输入信号，向显示器输出信号，这之间究竟是怎么通信的呢？换句话说，计算机是用什么样的方式来完成，CPU和内存、以及外部输入输出设备的通信呢？
这个问题就是我们今天要讲的主题，也就是总线。之前很多同学留言问，我什么时候会讲一讲总线。那这一讲，你就要听仔细了。
降低复杂性：总线的设计思路来源计算机里其实有很多不同的硬件设备，除了CPU和内存之外，我们还有大量的输入输出设备。可以说，你计算机上的每一个接口，键盘、鼠标、显示器、硬盘，乃至通过USB接口连接的各种外部设备，都对应了一个设备或者模块。
如果各个设备间的通信，都是互相之间单独进行的。如果我们有$N$个不同的设备，他们之间需要各自单独连接，那么系统复杂度就会变成$N^2$。每一个设备或者功能电路模块，都要和其他$N-1$个设备去通信。为了简化系统的复杂度，我们就引入了总线，把这个$N^2$的复杂度，变成一个$N$的复杂度。
那怎么降低复杂度呢？与其让各个设备之间互相单独通信，不如我们去设计一个公用的线路。CPU想要和什么设备通信，通信的指令是什么，对应的数据是什么，都发送到这个线路上；设备要向CPU发送什么信息呢，也发送到这个线路上。这个线路就好像一个高速公路，各个设备和其他设备之间，不需要单独建公路，只建一条小路通向这条高速公路就好了。
这个设计思路，就是我们今天要说的总线（Bus）。
总线，其实就是一组线路。我们的CPU、内存以及输入和输出设备，都是通过这组线路，进行相互间通信的。总线的英文叫作Bus，就是一辆公交车。这个名字很好地描述了总线的含义。我们的“公交车”的各个站点，就是各个接入设备。要想向一个设备传输数据，我们只要把数据放上公交车，在对应的车站下车就可以了。
其实，对应的设计思路，在软件开发中也是非常常见的。我们在做大型系统开发的过程中，经常会用到一种叫作事件总线（Event Bus）的设计模式。
进行大规模应用系统开发的时候，系统中的各个组件之间也需要相互通信。模块之间如果是两两之间单独去定义协议，这个软件系统一样会遇到一个复杂度变成了$N^2$的问题。所以常见的一个解决方案，就是事件总线这个设计模式。
在事件总线这个设计模式里，各个模块触发对应的事件，并把事件对象发送到总线上。也就是说，每个模块都是一个发布者（Publisher）。而各个模块也会把自己注册到总线上，去监听总线上的事件，并根据事件的对象类型或者是对象内容，来决定自己是否要进行特定的处理或者响应。
这样的设计下，注册在总线上的各个模块就是松耦合的。模块互相之间并没有依赖关系。无论代码的维护，还是未来的扩展，都会很方便。
理解总线：三种线路和多总线架构理解了总线的设计概念，我们来看看，总线在实际的计算机硬件里面，到底是什么样。
现代的Intel CPU的体系结构里面，通常有好几条总线。
首先，CPU和内存以及高速缓存通信的总线，这里面通常有两种总线。这种方式，我们称之为双独立总线（Dual Independent Bus，缩写为DIB）。CPU里，有一个快速的本地总线（Local Bus），以及一个速度相对较慢的前端总线（Front-side Bus）。
我们在前面几讲刚刚讲过，现代的CPU里，通常有专门的高速缓存芯片。这里的高速本地总线，就是用来和高速缓存通信的。而前端总线，则是用来和主内存以及输入输出设备通信的。有时候，我们会把本地总线也叫作后端总线（Back-side Bus），和前面的前端总线对应起来。而前端总线也有很多其他名字，比如处理器总线（Processor Bus）、内存总线（Memory Bus）。
除了前端总线呢，我们常常还会听到PCI总线、I/O总线或者系统总线（System Bus）。看到这么多总线的名字，你是不是已经有点晕了。这些名词确实容易混为一谈。其实各种总线的命名一直都很混乱，我们不如直接来看一看CPU的硬件架构图。对照图来看，一切问题就都清楚了。
CPU里面的北桥芯片，把我们上面说的前端总线，一分为二，变成了三个总线。
我们的前端总线，其实就是系统总线。CPU里面的内存接口，直接和系统总线通信，然后系统总线再接入一个I/O桥接器（I/O Bridge）。这个I/O桥接器，一边接入了我们的内存总线，使得我们的CPU和内存通信；另一边呢，又接入了一个I/O总线，用来连接I/O设备。
事实上，真实的计算机里，这个总线层面拆分得更细。根据不同的设备，还会分成独立的PCI总线、ISA总线等等。
在物理层面，其实我们完全可以把总线看作一组“电线”。不过呢，这些电线之间也是有分工的，我们通常有三类线路。
数据线（Data Bus），用来传输实际的数据信息，也就是实际上了公交车的“人”。 地址线（Address Bus），用来确定到底把数据传输到哪里去，是内存的某个位置，还是某一个I/O设备。这个其实就相当于拿了个纸条，写下了上面的人要下车的站点。 控制线（Control Bus），用来控制对于总线的访问。虽然我们把总线比喻成了一辆公交车。那么有人想要做公交车的时候，需要告诉公交车司机，这个就是我们的控制信号。 尽管总线减少了设备之间的耦合，也降低了系统设计的复杂度，但同时也带来了一个新问题，那就是总线不能同时给多个设备提供通信功能。
我们的总线是很多个设备公用的，那多个设备都想要用总线，我们就需要有一个机制，去决定这种情况下，到底把总线给哪一个设备用。这个机制，就叫作总线裁决（Bus Arbitraction）。总线裁决的机制有很多种不同的实现，如果你对这个实现的细节感兴趣，可以去看一看Wiki里面关于裁决器的对应条目，这里我们就不多说了。
总结延伸好了，你现在明白计算机里的总线、各种不同的总线到底是什么意思了吧？希望这一讲能够帮你厘清计算机总线的知识点。现在我们一起来总结梳理一下这节的内容。
这一讲，我为你讲解了计算机里各个不同的组件之间用来通信的渠道，也就是总线。总线的设计思路，核心是为了减少多个模块之间交互的复杂性和耦合度。实际上，总线这个设计思路在我们的软件开发过程中也经常会被用到。事件总线就是我们常见的一个设计模式，通常事件总线也会和订阅者发布者模式结合起来，成为大型系统的各个松耦合的模块之间交互的一种主要模式。
在实际的硬件层面，总线其实就是一组连接电路的线路。因为不同设备之间的速度有差异，所以一台计算机里面往往会有多个总线。常见的就有在CPU内部和高速缓存通信的本地总线，以及和外部I/O设备以及内存通信的前端总线。
前端总线通常也被叫作系统总线。它可以通过一个I/O桥接器，拆分成两个总线，分别来和I/O设备以及内存通信。自然，这样拆开的两个总线，就叫作I/O总线和内存总线。总线本身的电路功能，又可以拆分成用来传输数据的数据线、用来传输地址的地址线，以及用来传输控制信号的控制线。
总线是一个各个接入的设备公用的线路，所以自然会在各个设备之间争夺总线所有权的情况。于是，我们需要一个机制来决定让谁来使用总线，这个决策机制就是总线裁决。
推荐阅读总线是一个抽象的设计模式，它不仅在我们计算机的硬件设计里出现。在日常的软件开发中，也是一个常见的设计模式，你可以去读一读Google开源的Java的一个常用的工具库Guava的相关资料和代码，进一步理解事件总线的设计模式，看看在软件层面怎么实现它。
对于计算机硬件层面的总线，很多教科书里讲得都比较少，你可以去读一读Wiki里面总线和系统总线的相关条目。
课后思考2008年之后，我们的Intel CPU其实已经没有前端总线了。Intel发明了快速通道互联（Intel Quick Path Interconnect，简称为QPI）技术，替代了传统的前端总线。这个QPI技术，你可以搜索和翻阅一下相关资料，了解一下它引入了什么新的设计理念。
欢迎在留言区分享你查阅到的资料，以及阅读之后的思考总结，和大家一起交流。如果有收获，你也可以把这篇文章分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>42_瞧一瞧Linux：如何实现系统API？</title><link>https://artisanbox.github.io/9/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/42/</guid><description>你好，我是LMOS。
上节课，我们通过实现一个获取时间的系统服务，学习了Cosmos里如何建立一个系统服务接口。Cosmos为应用程序提供服务的过程大致是这样的：应用程序先设置服务参数，然后通过int指令进入内核，由Cosmos内核运行相应的服务函数，最后为应用程序提供所需服务。
不知道你是否好奇过业内成熟的Linux内核，又是怎样为应用程序提供服务的呢？
这节课我们就来看看Linux内核是如何实现这一过程的，我们首先了解一下Linux内核有多少API接口，然后了解一下Linux内核API接口的架构，最后，我们动手为Linux内核增加一个全新的API，并实现相应的功能。
下面让我们开始吧！这节课的配套代码你可以从这里下载。
Linux内核API接口的架构在上节课中，我们已经熟悉了我们自己的Cosmos内核服务接口的架构，由应用程序调用库函数，再由库函数调用API入口函数，进入内核函数执行系统服务。
其实对于Linux内核也是一样，应用程序会调用库函数，在库函数中调用API入口函数，触发中断进入Linux内核执行系统调用，完成相应的功能服务。
在Linux内核之上，使用最广泛的C库是glibc，其中包括C标准库的实现，也包括所有和系统API对应的库接口函数。几乎所有C程序都要调用glibc的库函数，所以glibc是Linux内核上C程序运行的基础。
下面我们以open库函数为例分析一下，看看open是如何进入Linux内核调用相关的系统调用的。glibc虽然开源了，但是并没有在Linux内核代码之中，你需要从这里下载并解压，open函数代码如下所示。
//glibc/intl/loadmsgcat.c #ifdef _LIBC # define open(name, flags) __open_nocancel (name, flags) # define close(fd) __close_nocancel_nostatus (fd) #endif //glibc/sysdeps/unix/sysv/linux/open_nocancel.c int __open_nocancel (const char *file, int oflag, ...) { int mode = 0; if (__OPEN_NEEDS_MODE (oflag)) { va_list arg; va_start (arg, oflag);//解决可变参数 mode = va_arg (arg, int); va_end (arg); } return INLINE_SYSCALL_CALL (openat, AT_FDCWD, file, oflag, mode); } //glibc/sysdeps/unix/sysdep.h //这是为了解决不同参数数量的问题 #define __INLINE_SYSCALL0(name) \ INLINE_SYSCALL (name, 0) #define __INLINE_SYSCALL1(name, a1) \ INLINE_SYSCALL (name, 1, a1) #define __INLINE_SYSCALL2(name, a1, a2) \ INLINE_SYSCALL (name, 2, a1, a2) #define __INLINE_SYSCALL3(name, a1, a2, a3) \ INLINE_SYSCALL (name, 3, a1, a2, a3) #define __INLINE_SYSCALL_NARGS_X(a,b,c,d,e,f,g,h,n,.</description></item><item><title>42｜到这里，我们的收获和未尽的工作有哪些？</title><link>https://artisanbox.github.io/3/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/44/</guid><description>你好，我是宫文学。
到今天这节课为止，我们已经把这门课程的主要内容都学完了。感谢你一路的坚持！
所以，在今天这节课，我想做一个简单的总结。我想先带你回顾一下我们一起闯过的那些技术关卡，以及取得的成果。接下来，我还想梳理一下我们尚未完成的工作，也对我们后续作为开源项目的PlayScript语言做一下规划。
在这个过程中，你可以暂时从技术细节中解脱出来，站在一个语言的架构师的角度，一起做一些高层面的思考，锻炼一下架构思维。
首先，我们简单总结一下当前已经完成的工作。
当前的收获到目前为止，我们在40多节课的内容里，塞进了大量的知识点。我们按课程顺序来梳理一下。
基础篇：三大关卡在第一部分基础篇中，我带你连续闯了三个关卡。
第一个关卡，是编译器前端技术，包括词法分析、语法分析和语义分析技术。
在词法和语法分析方面，我们这门课没有带你进入相关算法的迷魂阵，而是带你去掌握最佳实践。一方面，这些算法我在《编译原理之美》课程中已经讲过了。另一方面，如果你只是写个编译器，而不是写个像Yacc、Antlr这样的编译器生成工具，其实不需要深究那些算法，只要大概明白原理就行了。
即使是这样，对于递归下降中的左递归问题这样偏理论性的知识点，很多同学免不了还是有疑惑。比如，有同学会问我，我在课程里用到的有些文法，为什么仍然有一些是左递归的呢？这里其实涉及到PEG文法的一个知识点，我会在后面的加餐里讲一下PEG。其实，并不是所有的左递归都没有办法处理。关于左递归，直到现在仍然是做算法研究的人感兴趣的一个领域。
在语义分析方面，我们体会了如何建立符号表、如何做引用消解、如何检查一些语义错误的过程，这样，你会对课本中讲到的一些抽象概念建立具象的理解。
为了编译TypeScript语言，编译器在语义分析阶段最重要的功能是进行类型的处理，其中的关键点又是类型的联合和窄化。在这门课里，我们主要采用了集合运算的手法来处理类型。在PlayScript开源项目中，我计划把这部分进一步优化，让类型计算更简洁、更精准。
对很多同学来说，闯过编译器前端的这个关卡，其实已经收获满满，可以在自己的项目里大展身手了。不过，如果你喜欢钻研底层实现，显然还不会满足止步于此，那你就可以继续闯第二关。
在第二关，我们实现了两个版本的字节码虚拟机。一个版本是用TypeScript实现的，另一个版本使用C语言实现的。
通过这个实现过程，你会了解到像Java这样的成熟语言的字节码是如何设计的，又是如何实际运行的。这样，在需要的时候，我希望你能够敢于自己生成Java或.NET的字节码，实现自己想要的软件编程功能。
并且，通过实现C语言版本的虚拟机，我们也能初步了解运行时的功能。特别是，你要知道，如果我们不能做好内存的管理，系统运行的性能就会大受影响。而且，通过我们多次的性能测试，你应该已经对一个解释器中影响性能的因素产生了直观的理解，这样你自己写程序的时候，也能够进行更明智的决策。
在初步实现了字节码解释器以后，我们又进入了第三关，挑战把源代码编译成二进制的可执行程序。
为了完成第三关的任务，我们必须对程序的运行机制有深入的了解，包括程序运行跟硬件是什么关系，跟操作系统和ABI又是什么关系。再进一步，我们还需要了解CPU架构和它支持的指令集，学会阅读甚至手写汇编代码。
生成汇编代码有两个关键点：第一，要熟悉ABI，正确地维护栈桢和寄存器的状态，否则程序运行时就会报segment错误；第二，就是要实现寄存器分配算法。你要知道，不同的指令集会使用不同的寄存器，并且我们在函数调用前后要保护某些寄存器。做好这些以后，程序就可以充分利用寄存器，飞一般地运行了。
在闯完这三关以后，你已经从前端到后端打通了技术路线。接下来在第二部分进阶篇中，我们把这条路线做了拓宽。
进阶篇：拓宽路径怎么去做拓宽呢？我们的主线是支持更多的数据类型，包括浮点数、字符串和数组这几个基础类型，还包括自定义对象、高阶函数这种高级的类型。
为了支持这些类型，我们必须增强运行时的功能，需要设计对象的内存布局。在访问对象属性、数组成员的时候，我们也要能够正确计算出内存地址来。
进阶篇最难的部分，是自动内存管理功能，包括基于Arena的内存申请机制，以及垃圾回收机制。而实现垃圾回收的关键点，在于找到GC根，并顺着GC根去找到直接引用和间接引用的对象。因此，我们需要保存栈桢的布局信息、对象的元数据信息、闭包的元数据信息等各种静态信息。
如果你充分掌握了内存管理涉及的技术点，那么你在后面实现很多高级功能的时候都能用上。比如对程序做调试、支持运行时的类型判断和元编程功能，都需要用到我们提前保存的元数据信息。
学完第二部分以后，你对实现像面向对象、函数式编程等各种语言特性基本上心里有数了。在第三部分优化篇，我们就专注于解决一个问题，就是优化问题。
优化篇：基于图的IR优化是编译器最重要的功能之一。优化可以发生在全生命周期，包括前端、中端、后端和运行过程中。第三部分的核心，就是一个基于图的IR。这个IR被Java和JavaScript的编译器采用，你可以想象一定有它的优势。
在使用这个IR的过程中，我们确实发现，我们很多优化的实现都变得很简单了。像公共子表达式删除，我们在生成IR的过程中顺带着就能完成。还有，像拷贝传播、死代码删除、值编号等这些优化，实现起来也很简单。最重要的是，这个IR更容易打通本地优化、全局优化和过程间优化三者的边界，让代码更容易在不同的基本块中移动，获取我们想要的优化效果。
并且，我们还了解了基于这个IR不断地做lower，直到生成LIR，然后基于LIR做指令选择、寄存器分配等后端优化的完整过程。通过这一部分的学习，你对于前端、中端、后端优化要做的工作就都比较清晰了。
好了，以上就是这门课中我们领略的各种技术风光，我希望你能够充分掌握，一方面这能开阔你的技术思路，另一方面这些技术也能用到你的实际项目中。
不过，受限于时间，我还没有把一门完整的语言完全实现完。所以，我后面会把这门课的示例代码，作为一个开源项目继续迭代下去，并形成完整的、实用的版本。至于当前已有的基础，我们就把它作为0.1版本吧！
那如果我们要实现一个实用的版本，还有哪些工作要做呢？
后续工作第一，我们要对编译器前端做比较大的增强和重构。
首先，当前我们已有的词法分析、语法分析和语义分析功能，都要支持更多的特性，比如，除了我们已经支持的for循环和if分支语句外，还有while循环、switch语句，等等。
而且，我们对编译错误的处理要更加友好。你应该也感受到了，目前我们的编译器，在遇到某些语法错误的时候，会持续不停地尝试，不断打印错误信息。这显然太不友好了，要做优化。
然后还有一个比较大的工作，就是对类型系统进行升级。我们要重构一下我们之前类型计算的算法，让它变得更加简洁和准确。我们目前使用的Nominal的类型系统，也要修改成支持structural的类型系统，并且我们还要让我们的类型计算支持泛型。
另外，我们在升级编译器前端的时候，对AST和符号表这两个重要的数据结构，也需要重构和优化一下。
第二，我们要把面向对象和函数式编程的特性实现完整。
比如，面向对象方面，我们需要实现严格的对象的初始化流程，需要支持访问权限，还要支持接口。在函数式编程方面，怎么着也要把Lambda表达式这些基础功能实现。这主要是工作量的问题，但需要前端、中端、后端和运行时各方面的配合。
第三，是升级编译器的中端优化功能。
基于目前的IR，我们只实现了少量的优化，并且还没有支持面向对象等复杂的语言特性，这些都需要进行扩展和支持。在这个实现的过程中，我们IR的数据结构也会得到丰富和完善。
第四，是升级编译器的后端功能。
我们之前的编译器后端主要是基于AST来生成汇编代码。所以，在引入IR以后，我们编译器的后端也需要重构一下。从AST生成IR后，再基于HIR生成LIR，然后在LIR的基础上重新实现指令选择和寄存器分配。
另外，我们目前只支持x86-64架构，并且也没有在多个操作系统上做测试。在后面，我们要支持至少两种CPU架构，我计划先支持的是x86-64和Aarch64。前者被广泛用于PC和服务器中，后者被广泛用于智能手机和苹果新一代Mac电脑中。而且，我们还要兼容多种操作系统。
第五，是内存管理方面的升级。
在垃圾收集方面，我们的GC还是很基础的，达不到实用级别。那么，接下来我们首先要完善基本的标记-清除算法。之后，我计划实现一个自动引用计数的（ARC）的机制。
ARC的原理是记录每个对象的引用数，当引用数为零的时候，就自动作为垃圾清理掉。ARC的好处就是垃圾回收不会引起大的停顿，能让系统的响应比较平缓。苹果的Objective-C和Swift都采用了ARC，这也是苹果的系统很少卡顿的原因之一。
第六，实现并发机制。
你应该也注意到了，目前我们这门课中并没有涉及并发机制。但如果不实现并发机制，显然会是一个遗憾。所以后面，我会给出协程功能的一个参考实现。至于另外的并发功能的设想，我接下来还会介绍到。
好了，上面就是要实现一个完整的、实用的、静态编译的语言会涉及到的工作量，细细看一下，还真是挺大的。不过，你也不用怵，这里并没有太多技术点是我们这门课没有涵盖到的，更多的是工作量和工程化的问题。
花费这么多的工作量，我并不完全是为了兴趣爱好，或者是做技术验证，还是想未来有一天，能把它用于一些实际的应用场景中。那接下来，我就谈谈对开源的PlayScript语言项目的一些规划。
对PlayScript的规划为什么我会产生自己动手实现一门语言的想法呢？这其实是出于一些实际的需求。现有的语言，或者已有语言的现有实现，有时会让我很不满意。所以我就想，与其等着别人来满足我的需求，不如自己动手试试看。
我就分享一下我的这几个需求，看看你是否也遇到过类似的问题。
首先，我对后端编程语言不满意。
你知道吗？要实现像微信这样的应用的后端，你只能使用C++这样的语言。并且，微信团队还开发了自己的协程库，才能应对海量并发的需求。我们每天用微信，觉得它总是会实时响应，其实后端的挑战是巨大的。你想想看就知道，成亿的用户，加上成亿的并发，绝对是顶级的技术挑战。
Java、Go等典型的后端语言，都不能满足这种场景的需求。Java的内存占用太大，自带的并发机制只有线程。虽然Go语言好一点，但它和Java都有一个致命的弱点，就是垃圾回收导致的停顿是不可控的。这对于微信这种大型的、高并发的平台，会带来灾难性的后果。但是，让普通的技术团队用C++开发应用，门槛有点高。
而且，我对现有后端语言提供的可靠性也不满意。在这方面，我比较喜欢Erlang，它的并发机制和其他特性结合起来，能提供9个9的可靠性。我觉得，如果每个应用都能实现这么高的可靠性就好了。但是，我对Erlang的性能又不满意，而且它的语法对于大多数程序员来说，也不是太友好。
实现一门高可靠性的语言，其实有隐含一个需求，就是语言中的功能是能够在运行时被动态替换的，因为你没有办法停下整个系统。所以，我们不能仅仅实现AOT的功能，还要有JIT、动态优化、动态部署、动态Dispatch的功能。
所以，我理想的后端语言，是能够用比较低的成本，开发出高并发、高可靠性、资源消耗低的应用。不知道你是不是也有类似的需求呢？
第二，我对前端编程环境也不太满意。
我感觉，现在的前端编程环境太碎片化了，包括浏览器、Android、IOS、Windows、macOS、Linux等不同的平台，而且国内还有好几个不同的小程序平台。
所以，我想要是有一个语言或者工具，能够开发一次，部署到很多客户端，那就好了。
第三，我对企业应用的编程语言也不满意。
我曾经参与过很多企业应用的开发工作。在企业应用的开发中，很多时候我们要更关注业务逻辑。但是，现在很多应用的业务逻辑和技术逻辑都是混杂的，企业应用的开发成本太高了。
我一直认为，如果你是做企业软件的厂商，那应该有相应的开发语言才好，比如，德国的SAP就有自己的ABAP语言。不过，这个语言在现代的应用架构下已经过时了，用ABAP开发不出很容易横向扩展的应用。再有一点，这个语言是企业私有的，不是公共的。
当然，现在我们已经迎来了低代码开发的一波浪潮。但是，如果低代码工具的开发者不是像微软这么有实力的厂商，很难维护一个完全私有的生态。这样的情况下，客户在你私有的平台上开发应用，就是比较有风险的，所以还是应该有一个公共开放的语言。
而且，就算是低代码开发，我也希望是基于某个语言的，而不是仅仅提供一些图形化的定制工具。最好呢，是语言可以转化为图形，图形也可以转化为代码。这种代码和图形化表达双向转化的能力，我在华为的HarmonyOS开发工具上看到了，感觉很喜欢。其实我觉得，低代码开发工具也应该实现类似的功能才算合格，这种能把应用表达为代码的能力，是保证应用的可移植性、保护企业投资的关键。
第四，如有可能，我希望能让物联网应用的开发变得更简单一点。
我在之前的一篇加餐里，介绍过工控领域软件开发的情况。工控领域的技术原来叫做OT，它们的技术跟IT是不一样的。但在我看来，现在很多IT技术可以进入OT领域。比如，现在儿童编程都可以用一个图形化开发工具控制机器人，这跟控制发电机、控制高铁，其实没有本质的区别。所以，我们应该也可以把这两个领域的开发工具打通才对。就算是OT强调高可靠性，但其实，IT里现在就有更高可靠性的技术，比如我前面提到过的Erlang的9个9的可靠性。
另外，OT的技术生态，原来都把控在少数外国的企业手里。我觉得，如有可能，我们最好可以搞搞破坏，把它搞成一个开放的生态。</description></item><item><title>43_拓扑排序：如何确定代码源文件的编译依赖关系？</title><link>https://artisanbox.github.io/2/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/44/</guid><description>从今天开始，我们就进入了专栏的高级篇。相对基础篇，高级篇涉及的知识点，都比较零散，不是太系统。所以，我会围绕一个实际软件开发的问题，在阐述具体解决方法的过程中，将涉及的知识点给你详细讲解出来。
所以，相较于基础篇“开篇问题-知识讲解-回答开篇-总结-课后思考”这样的文章结构，高级篇我稍作了些改变，大致分为这样几个部分：“问题阐述-算法解析-总结引申-课后思考”。
好了，现在，我们就进入高级篇的第一节，如何确定代码源文件的编译依赖关系？
我们知道，一个完整的项目往往会包含很多代码源文件。编译器在编译整个项目的时候，需要按照依赖关系，依次编译每个源文件。比如，A.cpp依赖B.cpp，那在编译的时候，编译器需要先编译B.cpp，才能编译A.cpp。
编译器通过分析源文件或者程序员事先写好的编译配置文件（比如Makefile文件），来获取这种局部的依赖关系。那编译器又该如何通过源文件两两之间的局部依赖关系，确定一个全局的编译顺序呢？
算法解析这个问题的解决思路与“图”这种数据结构的一个经典算法“拓扑排序算法”有关。那什么是拓扑排序呢？这个概念很好理解，我们先来看一个生活中的拓扑排序的例子。
我们在穿衣服的时候都有一定的顺序，我们可以把这种顺序想成，衣服与衣服之间有一定的依赖关系。比如说，你必须先穿袜子才能穿鞋，先穿内裤才能穿秋裤。假设我们现在有八件衣服要穿，它们之间的两两依赖关系我们已经很清楚了，那如何安排一个穿衣序列，能够满足所有的两两之间的依赖关系？
这就是个拓扑排序问题。从这个例子中，你应该能想到，在很多时候，拓扑排序的序列并不是唯一的。你可以看我画的这幅图，我找到了好几种满足这些局部先后关系的穿衣序列。
弄懂了这个生活中的例子，开篇的关于编译顺序的问题，你应该也有思路了。开篇问题跟这个问题的模型是一样的，也可以抽象成一个拓扑排序问题。
拓扑排序的原理非常简单，我们的重点应该放到拓扑排序的实现上面。
我前面多次讲过，算法是构建在具体的数据结构之上的。针对这个问题，我们先来看下，如何将问题背景抽象成具体的数据结构？
我们可以把源文件与源文件之间的依赖关系，抽象成一个有向图。每个源文件对应图中的一个顶点，源文件之间的依赖关系就是顶点之间的边。
如果a先于b执行，也就是说b依赖于a，那么就在顶点a和顶点b之间，构建一条从a指向b的边。而且，这个图不仅要是有向图，还要是一个有向无环图，也就是不能存在像a-&amp;gt;b-&amp;gt;c-&amp;gt;a这样的循环依赖关系。因为图中一旦出现环，拓扑排序就无法工作了。实际上，拓扑排序本身就是基于有向无环图的一个算法。
public class Graph { private int v; // 顶点的个数 private LinkedList&amp;lt;Integer&amp;gt; adj[]; // 邻接表 public Graph(int v) { this.v = v; adj = new LinkedList[v]; for (int i=0; i&amp;lt;v; ++i) { adj[i] = new LinkedList&amp;lt;&amp;gt;(); } }
public void addEdge(int s, int t) { // s先于t，边s-&amp;gt;t adj[s].add(t); } } 数据结构定义好了，现在，我们来看，如何在这个有向无环图上，实现拓扑排序？
拓扑排序有两种实现方法，都不难理解。它们分别是Kahn算法和DFS深度优先搜索算法。我们依次来看下它们都是怎么工作的。
1.Kahn算法Kahn算法实际上用的是贪心算法思想，思路非常简单、好懂。
定义数据结构的时候，如果s需要先于t执行，那就添加一条s指向t的边。所以，如果某个顶点入度为0， 也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。
我们先从图中，找出一个入度为0的顶点，将其输出到拓扑排序的结果序列中（对应代码中就是把它打印出来），并且把这个顶点从图中删除（也就是把这个顶点可达的顶点的入度都减1）。我们循环执行上面的过程，直到所有的顶点都被输出。最后输出的序列，就是满足局部依赖关系的拓扑排序。
我把Kahn算法用代码实现了一下，你可以结合着文字描述一块看下。不过，你应该能发现，这段代码实现更有技巧一些，并没有真正删除顶点的操作。代码中有详细的注释，你自己来看，我就不多解释了。</description></item><item><title>43_虚拟机内核：KVM是什么？</title><link>https://artisanbox.github.io/9/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/43/</guid><description>你好，我是LMOS。
上节课，我们理解了Linux里要如何实现系统API。可是随着云计算、大数据和分布式技术的演进，我们需要在一台服务器上虚拟化出更多虚拟机，还要让这些虚拟机能够弹性伸缩，实现跨主机的迁移。
而虚拟化技术正是这些能力的基石。这一节课，就让我们一起探索一下，亚马逊、阿里、腾讯等知名公司用到的云虚拟主机，看看其中的核心技术——KVM虚拟化技术。
理解虚拟化的定义什么是虚拟化？在我看来，虚拟化的本质是一种资源管理的技术，它可以通过各种技术手段把计算机的实体资源（如：CPU、RAM、存储、网络、I/O等等）进行转换和抽象，让这些资源可以重新分割、排列与组合，实现最大化使用物理资源的目的。
虚拟化的核心思想学习了前面的课程我们发现，操作系统的设计很高明，已经帮我们实现了单机的资源配置需求，具体就是在一台物理机上把CPU、内存资源抽象成进程，把磁盘等资源抽象出存储、文件、I/O等特性，方便之后的资源调度和管理工作。
但随着时间的推移，我们做个统计就会发现，其实现在的PC机平常可能只有50%的时间处于工作状态，剩下的一半时间都是在闲置资源，甚至要被迫切换回低功耗状态。这显然是对资源的严重浪费，那么我们如何解决资源复用的问题呢？
这个问题确实很复杂，但根据我们的工程经验，但凡遇到不太好解决的问题，我们就可以考虑抽象出一个新的层次来解决。于是我们在已有的OS经验之上，进行了后面这样的设计。
结合图解，可以看出最大的区别就是后者额外引入了一个叫Hypervisor/Virtual Machine Monitor（VMM）的层。在这个层里面我们就可以做一些“无中生有”的事情，向下统一管理和调度真实的物理资源，向上“骗”虚拟机，让每个虚拟机都以为自己都独享了独立的资源。
而在这个过程中，我们既然作为一个“两头骗的中间商”，显然要做一些瞒天过海的事情（访问资源的截获与重定向）。那么让我们先暂停两分钟，思考一下具体如何设计，才能实现这个“两头骗”的目标呢？
用赵高矫诏谈理解虚拟化说起欺上瞒下，有个历史人物很有代表性，他就是赵高。始皇三十七年（前210年），统一了天下的秦始皇（OS）在生平最后一次出巡路上去世了，管理诏书的赵高（Hypervisor/VMM）却趁机发动了阴谋，威胁丞相李斯，矫诏处死扶苏与蒙恬。
赵高隐瞒秦始皇死讯，还伪造了诏书，回到了咸阳最终一顿忽悠立了胡亥为为帝。这段故事后世称为沙丘之变。
作为一个成功瞒天过海，实现了偷梁换柱的中间人赵高，他成事的关键要点包括这些，首先要像咸阳方向伪造一切正常的假象（让被虚拟化的机器看起来和平常一样），其次还要把真正核心的权限获取到手（Hypervisor/VMM要想办法调度真正的物理资源）。
所以以史为鉴。在具体实现的层面，我们会发现，这个瞒天过海的目标其实有几种实现方式。
一种思路是赵高一个人全权代理，全部模拟和代理出所有的资源（软件虚拟化技术），另一种思路是朝中有人（胡亥）配合赵高控制、调度各种资源的使用，真正执行的时候，再转发给胡亥去处理（硬件虚拟化技术）。
我们发现如果如果是前者，显然赵高会消耗大量资源，并且还可能会遇到一些安全问题，所以他选择了后者。
历史总是惊人地相似，在软件虚拟化遇到了无法根治的性能瓶颈和安全等问题的时候，软件工程师就开始给硬件工程师提需求了，需求背后的核心想法是这样的：能不能让朝中有人，有问题交给他，软件中间层只管调度资源之类的轻量级工作呢？
KVM架构梳理答案显然是可以的，根据我们对计算机的了解就会发现，计算机最重要几种资源分别是：计算（CPU）、存储（RAM、ROM），以及为了连接各种设备抽象出的I/O资源。
所以Intel分别设计出了VT-x指令集、VT-d指令集、VT-c指令集等技术来实现硬件虚拟化，让CPU配合我们来实现这个目标，了解了核心思想之后，让我们来看一看KVM的架构图。（图片出自论文《Residency-Aware Virtual Machine Communication Optimization: Design Choices and Techniques》）
是不是看起来比较复杂？别担心，我用大白话帮你梳理一下。
首先，客户机（咸阳）看到的硬件资源基本都是由Hypervisor（赵高）模拟出来的。当客户机对模拟设备进行操作时，命令就会被截获并转发给实际设备/内核模块（胡亥）去处理。
通过这种架构设计Hypervisor层，最终实现了把一个客户机映射到宿主机OS系统的一个进程，而一个客户机的vCPU则映射到这个进程下的独立的线程中。同理，I/O也可以映射到同一个线程组内的独立线程中。
这样，我们就可以基于物理机OS的进程等资源调度能力，实现不同虚拟机的权限限定、优先级管理等功能了。
KVM核心原理通过前面的知识，我们发现，要实现成功的虚拟化，核心是要对资源进行“欺上瞒下”。而对应到我们计算机内的最重要的资源，可以简单抽象成为三大类，分别是：CPU、内存、I/O。接下来，我们就来看看如何让这三大类资源做好虚拟化。
CPU虚拟化原理众所周知，CPU是我们计算机最重要的模块，让我们先看看Intel CPU是如何跟Hypervisor/VMM“里应外合”的。
Intel定义了Virtual Machine Extension（VMX）这个处理器特性，也就是传说中的VT-x指令集，开启了这个特性之后，就会存在两种操作模式。它们分别是：根操作（VMX root operation）和非根操作（VMX non-root operation）。
我们之前说的Hypervisor/VMM，其实就运行在根操作模式下，这种模式下的系统对处理器和平台硬件具有完全的控制权限。
而客户软件（Guest software）包括虚拟机内的操作系统和应用程序，则运行在非根操作模式下。当客户软件执行一些特殊的敏感指令或者一些异常（如CPUID、INVD、INVEPT指令，中断、故障、或者一些寄存器操作等）时，则会触发VM-Exit指令切换回根操作模式，从而让Hypervisor/VMM完全接管控制权限。
下面这张图画出了模式切换的过程，想在这两种模式之间切换，就要通过VM-Entry和VM-Exit实现进入和退出。而在这个切换过程中，你要留意一个非常关键的数据结构，它就是VMCS（Virtual Machine Control Structure）数据结构控制（下文也会讲到）。
内存虚拟化原理内存虚拟化的核心目的是“骗”客户机，给每个虚拟客户机都提供一个从0开始的连续的物理内存空间的假象，同时又要保障各个虚拟机之间内存的隔离和调度能力。
可能有同学已经联想到，我们之前实现实现虚拟内存的时候，不也是在“骗”应用程序每个程序都有连续的物理内存，为此还设计了一大堆“转换表”的数据结构和转换、调度机制么？
没错，其实内存虚拟化也借鉴了相同的思想，只不过问题更复杂些，因为我们发现我们的内存从原先的虚拟地址、物理地址突然变成了后面这四种内存地址。
1.客户机虚拟地址GVA（Guest Virtual Address）
2.客户机物理地址GPA（Guest Physical Address）
3.宿主机虚拟地址HVA（Host Virtual Address）
4.宿主机物理地址HPA（Host Physical Address）
一看到有这么多种地址，又需要进行地址转换，想必转换时的映射关系表是少不掉的。
确实，早期我们主要是基于影子页表（Shadow Page Table）来进行转换的，缺点就是性能有不小的损耗。所以，后来Intel在硬件上就设计了EPT（Extended Page Tables）机制，用来提升内存地址转换效率。</description></item><item><title>43_要不要使用分区表？</title><link>https://artisanbox.github.io/1/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/43/</guid><description>我经常被问到这样一个问题：分区表有什么问题，为什么公司规范不让使用分区表呢？今天，我们就来聊聊分区表的使用行为，然后再一起回答这个问题。
分区表是什么？为了说明分区表的组织形式，我先创建一个表t：
CREATE TABLE `t` ( `ftime` datetime NOT NULL, `c` int(11) DEFAULT NULL, KEY (`ftime`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 PARTITION BY RANGE (YEAR(ftime)) (PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB, PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB, PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB, PARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB); insert into t values('2017-4-1',1),('2018-4-1',1); 图1 表t的磁盘文件我在表t中初始化插入了两行记录，按照定义的分区规则，这两行记录分别落在p_2018和p_2019这两个分区上。
可以看到，这个表包含了一个.frm文件和4个.ibd文件，每个分区对应一个.ibd文件。也就是说：
对于引擎层来说，这是4个表； 对于Server层来说，这是1个表。 你可能会觉得这两句都是废话。其实不然，这两句话非常重要，可以帮我们理解分区表的执行逻辑。</description></item><item><title>43_输入输出设备：我们并不是只能用灯泡显示“0”和“1”</title><link>https://artisanbox.github.io/4/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/43/</guid><description>我们在前面的章节搭建了最简单的电路，在这里面，计算机的输入设备就是一个一个开关，输出设备呢，是一个一个灯泡。的确，早期发展的时候，计算机的核心是做“计算”。我们从“计算机”这个名字上也能看出这一点。不管是中文名字“计算机”，还是英文名字“Computer”，核心都是在”计算“这两个字上。不过，到了今天，这些“计算”的工作，更多的是一个幕后工作。
我们无论是使用自己的PC，还是智能手机，大部分时间都是在和计算机进行各种“交互操作”。换句话说，就是在和输入输出设备打交道。这些输入输出设备也不再是一个一个开关，或者一个一个灯泡。你在键盘上直接敲击的都是字符，而不是“0”和“1”，你在显示器上看到的，也是直接的图形或者文字的画面，而不是一个一个闪亮或者关闭的灯泡。想要了解这其中的关窍，那就请你和我一起来看一看，计算机里面的输入输出设备。
接口和设备：经典的适配器模式我们在前面讲解计算机的五大组成部分的时候，我看到这样几个留言。
一个同学问，像蓝牙、WiFi无线网卡这样的设备也是输入输出设备吗？还有一个同学问，我们的输入输出设备的寄存器在哪里？到底是在主板上，还是在硬件设备上？
这两个问题问得很好。其实你只要理解了这两个问题，也就理解输入输出设备是怎么回事儿了。
实际上，输入输出设备，并不只是一个设备。大部分的输入输出设备，都有两个组成部分。第一个是它的接口（Interface），第二个才是实际的I/O设备（Actual I/O Device）。我们的硬件设备并不是直接接入到总线上和CPU通信的，而是通过接口，用接口连接到总线上，再通过总线和CPU通信。
图片来源SATA硬盘，上面的整个绿色电路板和黄色的齿状部分就是接口电路，黄色齿状的就是和主板对接的接口，绿色的电路板就是控制电路你平时听说的并行接口（Parallel Interface）、串行接口（Serial Interface）、USB接口，都是计算机主板上内置的各个接口。我们的实际硬件设备，比如，使用并口的打印机、使用串口的老式鼠标或者使用USB接口的U盘，都要插入到这些接口上，才能和CPU工作以及通信的。
接口本身就是一块电路板。CPU其实不是和实际的硬件设备打交道，而是和这个接口电路板打交道。我们平时说的，设备里面有三类寄存器，其实都在这个设备的接口电路上，而不在实际的设备上。
那这三类寄存器是哪三类寄存器呢？它们分别是状态寄存器（Status Register）、 命令寄存器（Command Register）以及数据寄存器（Data Register），
除了内置在主板上的接口之外，有些接口可以集成在设备上。你可能都没有见过老一点儿的硬盘，我来简单给你介绍一下。
上世纪90年代的时候，大家用的硬盘都叫作IDE硬盘。这个IDE不是像IntelliJ或者WebStorm这样的软件开发集成环境（Integrated Development Environment）的IDE，而是代表着集成设备电路（Integrated Device Electronics）。也就是说，设备的接口电路直接在设备上，而不在主板上。我们需要通过一个线缆，把集成了接口的设备连接到主板上去。
我自己使用的PC的设备管理器把接口和实际设备分离，这个做法实际上来自于计算机走向开放架构（Open Architecture）的时代。
当我们要对计算机升级，我们不会扔掉旧的计算机，直接买一台全新的计算机，而是可以单独升级硬盘这样的设备。我们把老硬盘从接口上拿下来，换一个新的上去就好了。各种输入输出设备的制造商，也可以根据接口的控制协议，来设计和制造硬盘、鼠标、键盘、打印机乃至其他种种外设。正是这样的分工协作，带来了PC时代的繁荣。
其实，在软件的设计模式里也有这样的思路。面向对象里的面向接口编程的接口，就是Interface。如果你做iOS的开发，Objective-C里面的Protocol其实也是这个意思。而Adaptor设计模式，更是一个常见的、用来解决不同外部应用和系统“适配”问题的方案。可以看到，计算机的软件和硬件，在逻辑抽象上，其实是相通的。
如果你用的是Windows操作系统，你可以打开设备管理器，里面有各种各种的Devices（设备）、Controllers（控制器）、Adaptors（适配器）。这些，其实都是对于输入输出设备不同角度的描述。被叫作Devices，看重的是实际的I/O设备本身。被叫作Controllers，看重的是输入输出设备接口里面的控制电路。而被叫作Adaptors，则是看重接口作为一个适配器后面可以插上不同的实际设备。
CPU是如何控制I/O设备的？无论是内置在主板上的接口，还是集成在设备上的接口，除了三类寄存器之外，还有对应的控制电路。正是通过这个控制电路，CPU才能通过向这个接口电路板传输信号，来控制实际的硬件。
我们先来看一看，硬件设备上的这些寄存器有什么用。这里，我拿我们平时用的打印机作为例子。
首先是数据寄存器（Data Register）。CPU向I/O设备写入需要传输的数据，比如要打印的内容是“GeekTime”，我们就要先发送一个“G”给到对应的I/O设备。 然后是命令寄存器（Command Register）。CPU发送一个命令，告诉打印机，要进行打印工作。这个时候，打印机里面的控制电路会做两个动作。第一个，是去设置我们的状态寄存器里面的状态，把状态设置成not-ready。第二个，就是实际操作打印机进行打印。 而状态寄存器（Status Register），就是告诉了我们的CPU，现在设备已经在工作了，所以这个时候，CPU你再发送数据或者命令过来，都是没有用的。直到前面的动作已经完成，状态寄存器重新变成了ready状态，我们的CPU才能发送下一个字符和命令。 当然，在实际情况中，打印机里通常不只有数据寄存器，还会有数据缓冲区。我们的CPU也不是真的一个字符一个字符这样交给打印机去打印的，而是一次性把整个文档传输到打印机的内存或者数据缓冲区里面一起打印的。不过，通过上面这个例子，相信你对CPU是怎么操作I/O设备的，应该有所了解了。
信号和地址：发挥总线的价值搞清楚了实际的I/O设备和接口之间的关系，一个新的问题就来了。那就是，我们的CPU到底要往总线上发送一个什么样的命令，才能和I/O接口上的设备通信呢？
CPU和I/O设备的通信，一样是通过CPU支持的机器指令来执行的。
如果你回头去看一看第5讲，MIPS的机器指令的分类，你会发现，我们并没有一种专门的和I/O设备通信的指令类型。那么，MIPS的CPU到底是通过什么样的指令来和I/O设备来通信呢？
答案就是，和访问我们的主内存一样，使用“内存地址”。为了让已经足够复杂的CPU尽可能简单，计算机会把I/O设备的各个寄存器，以及I/O设备内部的内存地址，都映射到主内存地址空间里来。主内存的地址空间里，会给不同的I/O设备预留一段一段的内存地址。CPU想要和这些I/O设备通信的时候呢，就往这些地址发送数据。这些地址信息，就是通过上一讲的地址线来发送的，而对应的数据信息呢，自然就是通过数据线来发送的了。
而我们的I/O设备呢，就会监控地址线，并且在CPU往自己地址发送数据的时候，把对应的数据线里面传输过来的数据，接入到对应的设备里面的寄存器和内存里面来。CPU无论是向I/O设备发送命令、查询状态还是传输数据，都可以通过这样的方式。这种方式呢，叫作内存映射IO（Memory-Mapped I/O，简称MMIO）。
那么，MMIO是不是唯一一种CPU和设备通信的方式呢？答案是否定的。精简指令集MIPS的CPU特别简单，所以这里只有MMIO。而我们有2000多个指令的Intel X86架构的计算机，自然可以设计专门的和I/O设备通信的指令，也就是 in 和 out 指令。
Intel CPU虽然也支持MMIO，不过它还可以通过特定的指令，来支持端口映射I/O（Port-Mapped I/O，简称PMIO）或者也可以叫独立输入输出（Isolated I/O）。
其实PMIO的通信方式和MMIO差不多，核心的区别在于，PMIO里面访问的设备地址，不再是在内存地址空间里面，而是一个专门的端口（Port）。这个端口并不是指一个硬件上的插口，而是和CPU通信的一个抽象概念。
无论是PMIO还是MMIO，CPU都会传送一条二进制的数据，给到I/O设备的对应地址。设备自己本身的接口电路，再去解码这个数据。解码之后的数据呢，就会变成设备支持的一条指令，再去通过控制电路去操作实际的硬件设备。对于CPU来说，它并不需要关心设备本身能够支持哪些操作。它要做的，只是在总线上传输一条条数据就好了。
这个，其实也有点像我们在设计模式里面的Command模式。我们在总线上传输的，是一个个数据对象，然后各个接受这些对象的设备，再去根据对象内容，进行实际的解码和命令执行。
这是我计算机上，设备管理器里显卡设备的资源信息这是一张我自己的显卡，在设备管理器里面的资源（Resource）信息。你可以看到，里面既有Memory Range，这个就是设备对应映射到的内存地址，也就是我们上面所说的MMIO的访问方式。同样的，里面还有I/O Range，这个就是我们上面所说的PMIO，也就是通过端口来访问I/O设备的地址。最后，里面还有一个IRQ，也就是会来自于这个设备的中断信号了。
总结延伸好了，讲到这里，不知道，现在你是不是可以把CPU的指令、总线和I/O设备之间的关系彻底串联起来了呢？我来带你回顾一下。
CPU并不是发送一个特定的操作指令来操作不同的I/O设备。因为如果是那样的话，随着新的I/O设备的发明，我们就要去扩展CPU的指令集了。
在计算机系统里面，CPU和I/O设备之间的通信，是这么来解决的。
首先，在I/O设备这一侧，我们把I/O设备拆分成，能和CPU通信的接口电路，以及实际的I/O设备本身。接口电路里面有对应的状态寄存器、命令寄存器、数据寄存器、数据缓冲区和设备内存等等。接口电路通过总线和CPU通信，接收来自CPU的指令和数据。而接口电路中的控制电路，再解码接收到的指令，实际去操作对应的硬件设备。
而在CPU这一侧，对CPU来说，它看到的并不是一个个特定的设备，而是一个个内存地址或者端口地址。CPU只是向这些地址传输数据或者读取数据。所需要的指令和操作内存地址的指令其实没有什么本质差别。通过软件层面对于传输的命令数据的定义，而不是提供特殊的新的指令，来实际操作对应的I/O硬件。
推荐阅读想要进一步了解CPU和I/O设备交互的技术细节，我推荐你去看一看北京大学在Coursera上的视频课程，《计算机组成》第10周的内容。这个课程在Coursera上是中文的，而且可以免费观看。相信这一个小时的视频课程，对于你深入理解输入输出设备，会很有帮助。
课后思考我们还是回到，这节开始的时候同学留言的问题。如果你买的是一个带无线接收器的蓝牙鼠标，你需要把蓝牙接收器插在电脑的USB接口上，然后你的鼠标会和这个蓝牙接收器进行通信。那么，你能想一下，我们的CPU和蓝牙鼠标这个输入设备之间的通信是怎样的吗？
你可以好好思考一下，然后在留言区写下你的想法。当然，你也可以把这个问题分享给你的朋友，拉上他一起学习。</description></item><item><title>44_容器：如何理解容器的实现机制？</title><link>https://artisanbox.github.io/9/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/44/</guid><description>你好，我是LMOS。
上节课我带你通过KVM技术打开了计算机虚拟化技术的大门，KVM技术是基于内核的虚拟机，同样的KVM和传统的虚拟化技术一样，需要虚拟出一台完整的计算机，对于某些场景来说成本会比较高，其实还有比KVM更轻量化的虚拟化技术，也就是今天我们要讲的容器。
这节课我会先带你理解容器的概念，然后把它跟虚拟机作比较，之后为你讲解容器的基础架构跟基础技术，虽然这样安排有点走马观花，但这些内容都是我精选的核心知识，相信会为你以后继续探索容器打下一个良好的基础。
什么是容器容器的名词源于container，但不得不说我们再次被翻译坑了。相比“容器”，如果翻译成“集装箱”会更加贴切。为啥这么说呢？
我们先从“可复用”说起，现实里我们如果有一个集装箱的模具和原材料，很容易就能批量生产出多个规格相同的集装箱。从功能角度看，集装箱可以用来打包和隔离物品。不同类型的物品放在不同的集装箱里，这样东西就不会混在一起。
而且，集装箱里的物品在运输过程中不易损坏，具体说就是不管集装箱里装了什么东西，被送到哪里，只要集装箱没破坏，再次开箱时放在里面的东西就是完好无损的。
因此，我们可以这样来理解，容器是这样一种工作模式：轻量、拥有一个模具（镜像），既可以规模生产出多个相同集装箱（运行实例），又可以和外部环境（宿主机）隔离，最终实现对“内容”的打包隔离，方便其运输传送。
如果把容器看作集装箱，那内部运行的进程/应用就应该是集装箱里的物品了，类比来看，容器的目的就是提供一个独立的运行环境。
和虚拟机的对比我们传统的虚拟化技术可以通过硬件模拟来实现，也可以通过操作系统软件来实现，比如上节课提到的KVM。
为了让虚拟的应用程序达到和物理机相近的效果，我们使用了Hypervisor/VMM（虚拟机监控器），它允许多个操作系统共享一个或多个CPU，但是却带来了很大的开销，由于虚拟机中包括全套的OS，调度与资源占用都非常重。
容器（container）是一种更加轻量级的操作系统虚拟化技术，它将应用程序，依赖包，库文件等运行依赖环境打包到标准化的镜像中，通过容器引擎提供进程隔离、资源可限制的运行环境，实现应用与OS平台及底层硬件的解耦。
为了大大降低我们的计算成本，节省物理资源，提升计算机资源的利用率，让虚拟技术更加轻量化，容器技术应运而生。那么如何实现一个容器程序呢？我们需要先看看容器的基础架构。
看一看容器基础架构容器概念的起源是哪里？其实是从UNIX系统的chroot这个系统调用开始的。
在Linux系统上，LXC是第一个比较完整的容器，但是功能上还存在一定的不足，例如缺少可移植性，不够标准化。后面Docker的出现解决了容器标准化与可移植性问题，成为现在应用最广泛的容器技术。
Docker是最经典，使用范围最广，最具有代表性的容器技术。所以我们就以它为例，先对容器架构进行分析，Docker应用是一种C/S架构，包括3个核心部分。
容器客户端（Client）首先来看Docker的客户端，其主要任务是接收并解析用户的操作指令和执行参数，收集所需要的配置信息，根据相应的Docker命令通过HTTP或REST API等方式与Docker daemon（守护进程）进行交互，并将处理结果返回给用户，实现Docker服务使用与管理。
当然我们也可以使用其他工具通过Docker提供的API与daemon通信。
容器镜像仓库（Registry）Registry就是存储容器镜像的仓库，在容器的运行过程中，Client在接受到用户的指令后转发给Host下的Daemon，它会通过网络与Registry进行通信，例如查询镜像（search），下载镜像（pull），推送镜像（push）等操作。
镜像仓库可以部署在公网环境，如Docker Hub，我们也可以私有化部署到内网，通过局域网对镜像进行管理。
容器管理引擎进程（Host）容器引擎进程是Docker架构的核心，包括运行Docker Daemon（守护进程）、Image（镜像）、驱动（Driver）、Libcontainer（容器管理）等。
接下来，我们详细说说守护进程、镜像、驱动和容器管理这几个模块的运作机制/实现原理。
Docker Daemon详解
首先来看Docker Daemon进程，它是一个常驻后台的系统进程，也是Docker架构中非常重要的一环。Docker Daemon负责监听客户端请求，然后执行后续的对应逻辑，还能管理Docker对象（容器、镜像、网络、磁盘等）。
我们可以把Daemon分为三大部分，分别是Server、Job、Engine。
Server负责接收客户端发来的请求（由Daemon在后台启动Server）。接受请求以后Server通过路由与分发调度找到相应的Handler执行请求，然后与容器镜像仓库交互（查询、拉取、推送）镜像并将结果返回给Docker Client。
而Engine是Daemon架构中的运行引擎，同时也是Docker运行的核心模块。Engine扮演了Docker container存储仓库的角色。Engine执行的每一项工作，都可以拆解成多个最小动作——Job，这是Engine最基本的工作执行单元。
其实，Job不光能用在Engine内部，Docker内部每一步操作，都可以抽象为一个Job。Job负责执行各项操作时，如储存拉取的镜像，配置容器网络环境等，会使用下层的Driver（驱动）来完成。
Docker Driver
Driver顾名思义就是Docker中的驱动。设计驱动这一层依旧是解耦，将容器管理的镜像、网络和隔离执行逻辑从Docker Daemon的逻辑中剥离。
在Docker Driver的实现中，可以分为以下三类驱动。
graphdriver负责容器镜像的管理，主要就是镜像的存储和获取，当镜像下载的时候，会将镜像持久化存储到本地的指定目录； networkdriver主要负责Docker容器网络环境的配置，如Docker运行时进行IP分配端口映射以及启动时创建网桥和虚拟网卡； execdriver是Docker的执行驱动，通过操作Lxc或者libcontainer实现资源隔离。它负责创建管理容器运行命名空间、管理分配资源和容器内部真实进程的运行； libcontainer
上面我们提到execdriver，通过调用libcontainer来完成对容器的操作，加载容器配置container，继而创建真正的Docker容器。libcontainer提供了访问内核中和容器相关的API，负责对容器进行具体操作。
容器可以创建出一个相对隔离的环境，就容器技术本身来说，容器的核心部分是利用了我们操作系统内核的虚拟化技术，那么libcontainer中到底用到了哪些操作系统内核中的基础能力呢？
容器基础技术我们经常听到，Docker是一个基于Linux操作系统下的Namespace和Cgroups和UnionFS的虚拟化工具，下面我带你看一下这几个容器用到的内核中的基础能力。
Linux NameSpace容器的一大特点就是创造出一个相对隔离的环境。在Linux内核中，实现各种资源隔离功能的技术就叫Linux Namespace，它可以隔离一系列的系统资源，比如PID（进程ID）、UID（用户ID）、Network等。
看到这里，你很容易就会想到开头讲的chroot系统调用。类似于chroot把当前目录变成被隔离出的根目录，使得当前目录无法访问到外部的内容，Namespace在基于chroot扩展升级的基础上，也可以分别将一些资源隔离起来，限制每个进程能够访问的资源。
Linux内核提供了7类Namespace，以下是不同Namespace的隔离资源和系统调用参数。
1.PID Namespace：保障进程隔离，每个容器都以PID=1的init进程来启动。PID Namespace使用了的参数CLONE_NEWPID。类似于单独的Linux系统一样，每个NameSpace都有自己的初始化进程，PID为1，作为所有进程的父进程，父进程拥有很多特权。其他进程的PID会依次递增，子NameSpace的进程映射到父NameSpace的进程上，父NameSpace可以拿到全部子NameSpace的状态，但是每个子NameSpace之间是互相隔离的。
2.User Namespace：用于隔离容器中UID、GID以及根目录等。User Namespace使用了CLONE_NEWUSER的参数，可配置映射宿主机和容器中的UID、GID。某一个UID的用户，虚拟化出来一个Namespace，在当前的Namespace下，用户是具有root权限的。但是，在宿主机上面，他还是那个用户，这样就解决了用户之间隔离的问题。
3.UTS Namespace：保障每个容器都有独立的主机名或域名。UTS Namespace使用了的参数CLONE_NEWUTS，用来隔离hostname 和 NIS Domain name 两个系统标识，在UTS Namespace里面，每个Namespace允许有自己的主机名，作用就是可以让不同namespace中的进程看到不同的主机名。</description></item><item><title>44_最短路径：地图软件是如何计算出最优出行路径的？</title><link>https://artisanbox.github.io/2/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/45/</guid><description>基础篇的时候，我们学习了图的两种搜索算法，深度优先搜索和广度优先搜索。这两种算法主要是针对无权图的搜索算法。针对有权图，也就是图中的每条边都有一个权重，我们该如何计算两点之间的最短路径（经过的边的权重和最小）呢？今天，我就从地图软件的路线规划问题讲起，带你看看常用的最短路径算法（Shortest Path Algorithm）。
像Google地图、百度地图、高德地图这样的地图软件，我想你应该经常使用吧？如果想从家开车到公司，你只需要输入起始、结束地址，地图就会给你规划一条最优出行路线。这里的最优，有很多种定义，比如最短路线、最少用时路线、最少红绿灯路线等等。作为一名软件开发工程师，你是否思考过，地图软件的最优路线是如何计算出来的吗？底层依赖了什么算法呢？
算法解析我们刚提到的最优问题包含三个：最短路线、最少用时和最少红绿灯。我们先解决最简单的，最短路线。
解决软件开发中的实际问题，最重要的一点就是建模，也就是将复杂的场景抽象成具体的数据结构。针对这个问题，我们该如何抽象成数据结构呢？
我们之前也提到过，图这种数据结构的表达能力很强，显然，把地图抽象成图最合适不过了。我们把每个岔路口看作一个顶点，岔路口与岔路口之间的路看作一条边，路的长度就是边的权重。如果路是单行道，我们就在两个顶点之间画一条有向边；如果路是双行道，我们就在两个顶点之间画两条方向不同的边。这样，整个地图就被抽象成一个有向有权图。
具体的代码实现，我放在下面了。于是，我们要求解的问题就转化为，在一个有向有权图中，求两个顶点间的最短路径。
public class Graph { // 有向有权图的邻接表表示 private LinkedList&amp;lt;Edge&amp;gt; adj[]; // 邻接表 private int v; // 顶点个数 public Graph(int v) { this.v = v; this.adj = new LinkedList[v]; for (int i = 0; i &amp;lt; v; ++i) { this.adj[i] = new LinkedList&amp;lt;&amp;gt;(); } }
public void addEdge(int s, int t, int w) { // 添加一条边 this.adj[s].add(new Edge(s, t, w)); }
private class Edge { public int sid; // 边的起始顶点编号 public int tid; // 边的终止顶点编号 public int w; // 权重 public Edge(int sid, int tid, int w) { this.</description></item><item><title>44_理解IO_WAIT：IO性能到底是怎么回事儿？</title><link>https://artisanbox.github.io/4/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/44/</guid><description>在专栏一开始的时候，我和你说过，在计算机组成原理这门课里面，很多设计的核心思路，都来源于性能。在前面讲解CPU的时候，相信你已经有了切身的感受了。
大部分程序员开发的都是应用系统。在开发应用系统的时候，我们遇到的性能瓶颈大部分都在I/O上。在第36讲讲解局部性原理的时候，我们一起看了通过把内存当作是缓存，来提升系统的整体性能。在第37讲讲解CPU Cache的时候，我们一起看了CPU Cache和主内存之间性能的巨大差异。
然而，我们知道，并不是所有问题都能靠利用内存或者CPU Cache做一层缓存来解决。特别是在这个“大数据”的时代。我们在硬盘上存储了越来越多的数据，一个MySQL数据库的单表有个几千万条记录，早已经不算是什么罕见现象了。这也就意味着，用内存当缓存，存储空间是不够用的。大部分时间，我们的请求还是要打到硬盘上。那么，这一讲我们就来看看硬盘I/O性能的事儿。
IO性能、顺序访问和随机访问如果去看硬盘厂商的性能报告，通常你会看到两个指标。一个是响应时间（Response Time），另一个叫作数据传输率（Data Transfer Rate）。没错，这个和我们在专栏的一开始讲的CPU的性能一样，前面那个就是响应时间，后面那个就是吞吐率了。
我们先来看一看后面这个指标，数据传输率。
我们现在常用的硬盘有两种。一种是HDD硬盘，也就是我们常说的机械硬盘。另一种是SSD硬盘，一般也被叫作固态硬盘。现在的HDD硬盘，用的是SATA 3.0的接口。而SSD硬盘呢，通常会用两种接口，一部分用的也是SATA 3.0的接口；另一部分呢，用的是PCI Express的接口。
现在我们常用的SATA 3.0的接口，带宽是6Gb/s。这里的“b”是比特。这个带宽相当于每秒可以传输768MB的数据。而我们日常用的HDD硬盘的数据传输率，差不多在200MB/s左右。
这是在我自己的电脑上，运行AS SSD测算SATA接口SSD硬盘性能的结果，第一行的Seq就是顺序读写硬盘得到的数据传输率的实际结果当我们换成SSD的硬盘，性能自然会好上不少。比如，我最近刚把自己电脑的HDD硬盘，换成了一块Crucial MX500的SSD硬盘。它的数据传输速率能到差不多500MB/s，比HDD的硬盘快了一倍不止。不过SATA接口的硬盘，差不多到这个速度，性能也就到顶了。因为SATA接口的速度也就这么快。
不过，实际SSD硬盘能够更快，所以我们可以换用PCI Express的接口。我自己电脑的系统盘就是一块使用了PCI Express的三星SSD硬盘。它的数据传输率，在读取的时候就能做到2GB/s左右，差不多是HDD硬盘的10倍，而在写入的时候也能有1.2GB/s。
除了数据传输率这个吞吐率指标，另一个我们关心的指标响应时间，其实也可以在AS SSD的测试结果里面看到，就是这里面的Acc.Time指标。
这个指标，其实就是程序发起一个硬盘的写入请求，直到这个请求返回的时间。可以看到，在上面的两块SSD硬盘上，大概时间都是在几十微秒这个级别。如果你去测试一块HDD的硬盘，通常会在几毫秒到十几毫秒这个级别。这个性能的差异，就不是10倍了，而是在几十倍，乃至几百倍。
光看响应时间和吞吐率这两个指标，似乎我们的硬盘性能很不错。即使是廉价的HDD硬盘，接收一个来自CPU的请求，也能够在几毫秒时间返回。一秒钟能够传输的数据，也有200MB左右。你想一想，我们平时往数据库里写入一条记录，也就是1KB左右的大小。我们拿200MB去除以1KB，那差不多每秒钟可以插入20万条数据呢。但是这个计算出来的数字，似乎和我们日常的经验不符合啊？这又是为什么呢？
答案就来自于硬盘的读写。在顺序读写和随机读写的情况下，硬盘的性能是完全不同的。
我们回头看一下上面的AS SSD的性能指标。你会看到，里面有一个“4K”的指标。这个指标是什么意思呢？它其实就是我们的程序，去随机读取磁盘上某一个4KB大小的数据，一秒之内可以读取到多少数据。
你会发现，在这个指标上，我们使用SATA 3.0接口的硬盘和PCI Express接口的硬盘，性能差异变得很小。这是因为，在这个时候，接口本身的速度已经不是我们硬盘访问速度的瓶颈了。更重要的是，你会发现，即使我们用PCI Express的接口，在随机读写的时候，数据传输率也只能到40MB/s左右，是顺序读写情况下的几十分之一。
我们拿这个40MB/s和一次读取4KB的数据算一下。
40MB / 4KB = 10,000也就是说，一秒之内，这块SSD硬盘可以随机读取1万次的4KB的数据。如果是写入的话呢，会更多一些，90MB /4KB 差不多是2万多次。
这个每秒读写的次数，我们称之为IOPS，也就是每秒输入输出操作的次数。事实上，比起响应时间，我们更关注IOPS这个性能指标。IOPS和DTR（Data Transfer Rate，数据传输率）才是输入输出性能的核心指标。
这是因为，我们在实际的应用开发当中，对于数据的访问，更多的是随机读写，而不是顺序读写。我们平时所说的服务器承受的“并发”，其实是在说，会有很多个不同的进程和请求来访问服务器。自然，它们在硬盘上访问的数据，是很难顺序放在一起的。这种情况下，随机读写的IOPS才是服务器性能的核心指标。
好了，回到我们引出IOPS这个问题的HDD硬盘。我现在要问你了，那一块HDD硬盘能够承受的IOPS是多少呢？其实我们应该已经在第36讲说过答案了。
HDD硬盘的IOPS通常也就在100左右，而不是在20万次。在后面讲解机械硬盘的原理和性能优化的时候，我们还会再来一起看一看，这个100是怎么来的，以及我们可以有哪些优化的手段。
如何定位IO_WAIT？我们看到，即使是用上了PCI Express接口的SSD硬盘，IOPS也就是在2万左右。而我们的CPU的主频通常在2GHz以上，也就是每秒可以做20亿次操作。
即使CPU向硬盘发起一条读写指令，需要很多个时钟周期，一秒钟CPU能够执行的指令数，和我们硬盘能够进行的操作数，也有好几个数量级的差异。这也是为什么，我们在应用开发的时候往往会说“性能瓶颈在I/O上”。因为很多时候，CPU指令发出去之后，不得不去“等”我们的I/O操作完成，才能进行下一步的操作。
那么，在实际遇到服务端程序的性能问题的时候，我们怎么知道这个问题是不是来自于CPU等I/O来完成操作呢？别着急，我们接下来，就通过top和iostat这些命令，一起来看看CPU到底有没有在等待io操作。
# top 你一定在Linux下用过 top 命令。对于很多刚刚入门Linux的同学，会用top去看服务的负载，也就是load average。不过，在top命令里面，我们一样可以看到CPU是否在等待IO操作完成。
top - 06:26:30 up 4 days, 53 min, 1 user, load average: 0.</description></item><item><title>44_答疑文章（三）：说一说这些好问题</title><link>https://artisanbox.github.io/1/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/44/</guid><description>这是我们专栏的最后一篇答疑文章，今天我们来说说一些好问题。
在我看来，能够帮我们扩展一个逻辑的边界的问题，就是好问题。因为通过解决这样的问题，能够加深我们对这个逻辑的理解，或者帮我们关联到另外一个知识点，进而可以帮助我们建立起自己的知识网络。
在工作中会问好问题，是一个很重要的能力。
经过这段时间的学习，从评论区的问题我可以感觉出来，紧跟课程学习的同学，对SQL语句执行性能的感觉越来越好了，提出的问题也越来越细致和精准了。
接下来，我们就一起看看同学们在评论区提到的这些好问题。在和你一起分析这些问题的时候，我会指出它们具体是在哪篇文章出现的。同时，在回答这些问题的过程中，我会假设你已经掌握了这篇文章涉及的知识。当然，如果你印象模糊了，也可以跳回文章再复习一次。
join的写法在第35篇文章《join语句怎么优化？》中，我在介绍join执行顺序的时候，用的都是straight_join。@郭健 同学在文后提出了两个问题：
如果用left join的话，左边的表一定是驱动表吗？
如果两个表的join包含多个条件的等值匹配，是都要写到on里面呢，还是只把一个条件写到on里面，其他条件写到where部分？
为了同时回答这两个问题，我来构造两个表a和b：
create table a(f1 int, f2 int, index(f1))engine=innodb; create table b(f1 int, f2 int)engine=innodb; insert into a values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6); insert into b values(3,3),(4,4),(5,5),(6,6),(7,7),(8,8); 表a和b都有两个字段f1和f2，不同的是表a的字段f1上有索引。然后，我往两个表中都插入了6条记录，其中在表a和b中同时存在的数据有4行。
@郭健 同学提到的第二个问题，其实就是下面这两种写法的区别：
select * from a left join b on(a.f1=b.f1) and (a.f2=b.f2); /*Q1*/ select * from a left join b on(a.f1=b.f1) where (a.f2=b.f2);/*Q2*/ 我把这两条语句分别记为Q1和Q2。
首先，需要说明的是，这两个left join语句的语义逻辑并不相同。我们先来看一下它们的执行结果。
图1 两个join的查询结果可以看到：
语句Q1返回的数据集是6行，表a中即使没有满足匹配条件的记录，查询结果中也会返回一行，并将表b的各个字段值填成NULL。 语句Q2返回的是4行。从逻辑上可以这么理解，最后的两行，由于表b中没有匹配的字段，结果集里面b.f2的值是空，不满足where 部分的条件判断，因此不能作为结果集的一部分。 接下来，我们看看实际执行这两条语句时，MySQL是怎么做的。</description></item><item><title>45_ARM新宠：苹果的M1芯片因何而快？</title><link>https://artisanbox.github.io/9/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/45/</guid><description>你好，我是 LMOS。
前面两节课，我们一起学习了虚拟机和容器的原理，这些知识属于向上延展。而这节课我们要向下深挖，看看操作系统下面的硬件层面，重点研究一下CPU的原理和它的加速套路。
有了这些知识的加持，我还会给你说说，为什么去年底发布的苹果M1芯片可以实现高性能、低功耗。你会发现，掌握了硬件的知识，很多“黑科技”就不再那么神秘了。
好，让我们正式开始今天的学习！
CPU的原理初探经过前面的学习，我们已经对操作系统原理建立了一定认知。从操作系统的位置来看，它除了能够向上封装，为软件调用提供API（也就是系统调用），向下又对硬件资源进行了调度和抽象。我们通常更为关注系统调用，但为了更好地设计实现一个OS，我们当然也要对硬件足够了解。
接下来，我们一起看一看硬件中最重要的一个硬件——CPU是怎么工作的。让我们拆开CPU这个黑盒子，看一看一个最小的CPU应该包含哪些部分。不同架构的CPU，具体设计还是有很大差异的。为了方便你理解，我这里保留了CPU里的共性部分，给你抽象出了CPU的最小组成架构。
对照上图描绘的基本模块，我们可以把CPU运行过程抽象成这样6步。
1.众所周知，CPU的指令是以二进制形式存储在存储器中的（这里把寄存器、RAM统一抽象成了存储器），所以当CPU执行指令的时候，第一步就要先从存储器中取出（fetch）指令。
2.CPU将取出的指令通过硬件的指令解码器进行解码。
3.CPU根据指令解码出的功能，决定是否还要从存储器中取出需要处理的数据。
4.控制单元（CU）根据解码出的指令决定要进行哪些相应的计算，这部分工作由算术逻辑单元（ALU）完成。
5.控制单元（CU）根据前边解码出的指令决定是否将计算结果存入存储器。
6.修改程序计数器（PC）的指针，为下一次取指令做准备，以上整体执行过程由控制单元（CU）在时钟信号的驱动之下，周而复始地有序运行。
看了CPU核心组件执行的这6个步骤，不知道你有没有联想到第一节课的图灵机的执行原理？没错，现代CPU架构与实现虽然千差万别，但核心思想都是一致的。
ALU的需求梳理与方案设计通过研究CPU核心组件的运行过程，我们发现，原来CPU也可以想象成我们熟悉的软件，一样能抽象成几大模块，然后再进行模块化开发。
因为从零开始实现一款CPU的工程量还是不小的，所以在这里我带你使用Verilog语言实现一个可以运行简单计算的ALU，从而对CPU具体模块的设计与实现加深一下认知。
首先，我们来思考一下，对于一个最简单的ALU这个模块，我们的核心需求是什么？
没错，聪明的你可能已经脱口而出了，我需要能对两个N位的二进制数进行加减、比较运算。等等，为啥这里没有乘除？还记得学生时代初学乘除法的时候，老师也同样先简化为加减法，方便我们理解。
这里也一样，因为乘除也可以转换为循环的加减运算，比如2*3可以转换成2+2+2，6/2可以转换成6-2-2-2。所以，只需要实现了加减运算之后，我们就可以通过软件操作CPU，让它实现更复杂的运算了，这也正是软件扩展硬件能力的魅力。
好了，搞清楚需求之后，先不用着急编码，我们先来根据需求梳理一下ALU模块功能简图。
首先，我们在模块左侧（也就是输入侧）抽象出了5根引脚，这五根引脚的作用分别是：
ena：表示使能信号，它的取值是0或1可以分别控制ALU关闭或开启。 clk：表示时钟信号，时钟信号也是01交替运行的方波，时钟信号会像人的心跳一样驱动ALU的电路稳定可靠地运行。 opcode：表示操作码，取值范围是00、01、10这三种值，用来区分这一次计算到底是加法、减法还是比较运算。 data1、data2：表示参与运算的两个N位数据总线。 现在我们再来看图片右侧，也就是输出侧的y，它表示输出结果，如果是加减运算，则直接输出运算后的数值，而比较运算，则要输出0、1、2，分别表示等于、大于、小于。
好了，有了方案，接下来就让我们想办法把方案变成可落地的实践吧。
自己动手用Verilog实现一个ALUVerilog是一种优秀的硬件描述语言，它可以用类似C语言的高级语言设计芯片，从而免去了徒手画门电路的烦恼。
目前Intel等很多著名芯片公司都在使用Verilog进行芯片设计。我们为了和业界保持一致，也采用了这种Verilog来设计我们的ALU。
在开发之前，你需要先进行一些准备工作，安装VSCode的Verilog语言支持插件、iverilog、gtkwave，这些工具安装比较简单，你可以自行Google搜索。
接下来，我们就来实现一下ALU的代码，也就是alu.v，代码如下。
/*---------------------------------------------------------------- Filename: alu.v Function: 设计一个N位的ALU(实现两个N位有符号整数加 减 比较运算) -----------------------------------------------------------------*/ module alu(ena, clk, opcode, data1, data2, y); //定义alu位宽 parameter N = 32; //输入范围[-128, 127] //定义输入输出端口 input ena, clk; input [1 : 0] opcode; input signed [N - 1 : 0] data1, data2; //输入有符号整数范围为[-128, 127] output signed [N : 0] y; //输出范围有符号整数范围为[-255, 255] //内部寄存器定义 reg signed [N : 0] y; //状态编码 parameter ADD = 2'b00, SUB = 2'b01, COMPARE = 2'b10; //逻辑实现 always@(posedge clk) begin if(ena) begin casex(opcode) ADD: y &amp;amp;lt;= data1 + data2; //实现有符号整数加运算 SUB: y &amp;amp;lt;= data1 - data2; //实现有符号数减运算 COMPARE: y &amp;amp;lt;= (data1 &amp;amp;gt; data2) ?</description></item><item><title>45_位图：如何实现网页爬虫中的URL去重功能？</title><link>https://artisanbox.github.io/2/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/46/</guid><description>网页爬虫是搜索引擎中的非常重要的系统，负责爬取几十亿、上百亿的网页。爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。而同一个网页链接有可能被包含在多个页面中，这就会导致爬虫在爬取的过程中，重复爬取相同的网页。如果你是一名负责爬虫的工程师，你会如何避免这些重复的爬取呢？
最容易想到的方法就是，我们记录已经爬取的网页链接（也就是URL），在爬取一个新的网页之前，我们拿它的链接，在已经爬取的网页链接列表中搜索。如果存在，那就说明这个网页已经被爬取过了；如果不存在，那就说明这个网页还没有被爬取过，可以继续去爬取。等爬取到这个网页之后，我们将这个网页的链接添加到已经爬取的网页链接列表了。
思路非常简单，我想你应该很容易就能想到。不过，我们该如何记录已经爬取的网页链接呢？需要用什么样的数据结构呢？
算法解析关于这个问题，我们可以先回想下，是否可以用我们之前学过的数据结构来解决呢？
这个问题要处理的对象是网页链接，也就是URL，需要支持的操作有两个，添加一个URL和查询一个URL。除了这两个功能性的要求之外，在非功能性方面，我们还要求这两个操作的执行效率要尽可能高。除此之外，因为我们处理的是上亿的网页链接，内存消耗会非常大，所以在存储效率上，我们要尽可能地高效。
我们回想一下，满足这些条件的数据结构有哪些呢？显然，散列表、红黑树、跳表这些动态数据结构，都能支持快速地插入、查找数据，但是在内存消耗方面，是否可以接受呢？
我们拿散列表来举例。假设我们要爬取10亿个网页（像Google、百度这样的通用搜索引擎，爬取的网页可能会更多），为了判重，我们把这10亿网页链接存储在散列表中。你来估算下，大约需要多少内存？
假设一个URL的平均长度是64字节，那单纯存储这10亿个URL，需要大约60GB的内存空间。因为散列表必须维持较小的装载因子，才能保证不会出现过多的散列冲突，导致操作的性能下降。而且，用链表法解决冲突的散列表，还会存储链表指针。所以，如果将这10亿个URL构建成散列表，那需要的内存空间会远大于60GB，有可能会超过100GB。
当然，对于一个大型的搜索引擎来说，即便是100GB的内存要求，其实也不算太高，我们可以采用分治的思想，用多台机器（比如20台内存是8GB的机器）来存储这10亿网页链接。这种分治的处理思路，我们讲过很多次了，这里就不详细说了。
对于爬虫的URL去重这个问题，刚刚讲到的分治加散列表的思路，已经是可以实实在在工作的了。不过，作为一个有追求的工程师，我们应该考虑，在添加、查询数据的效率以及内存消耗方面，是否还有进一步的优化空间呢？
你可能会说，散列表中添加、查找数据的时间复杂度已经是O(1)，还能有进一步优化的空间吗？实际上，我们前面也讲过，时间复杂度并不能完全代表代码的执行时间。大O时间复杂度表示法，会忽略掉常数、系数和低阶，并且统计的对象是语句的频度。不同的语句，执行时间也是不同的。时间复杂度只是表示执行时间随数据规模的变化趋势，并不能度量在特定的数据规模下，代码执行时间的多少。
如果时间复杂度中原来的系数是10，我们现在能够通过优化，将系数降为1，那在时间复杂度没有变化的情况下，执行效率就提高了10倍。对于实际的软件开发来说，10倍效率的提升，显然是一个非常值得的优化。
如果我们用基于链表的方法解决冲突问题，散列表中存储的是URL，那当查询的时候，通过哈希函数定位到某个链表之后，我们还需要依次比对每个链表中的URL。这个操作是比较耗时的，主要有两点原因。
一方面，链表中的结点在内存中不是连续存储的，所以不能一下子加载到CPU缓存中，没法很好地利用到CPU高速缓存，所以数据访问性能方面会打折扣。
另一方面，链表中的每个数据都是URL，而URL不是简单的数字，是平均长度为64字节的字符串。也就是说，我们要让待判重的URL，跟链表中的每个URL，做字符串匹配。显然，这样一个字符串匹配操作，比起单纯的数字比对，要慢很多。所以，基于这两点，执行效率方面肯定是有优化空间的。
对于内存消耗方面的优化，除了刚刚这种基于散列表的解决方案，貌似没有更好的法子了。实际上，如果要想内存方面有明显的节省，那就得换一种解决方案，也就是我们今天要着重讲的这种存储结构，布隆过滤器（Bloom Filter）。
在讲布隆过滤器前，我要先讲一下另一种存储结构，位图（BitMap）。因为，布隆过滤器本身就是基于位图的，是对位图的一种改进。
我们先来看一个跟开篇问题非常类似、但比那个稍微简单的问题。我们有1千万个整数，整数的范围在1到1亿之间。如何快速查找某个整数是否在这1千万个整数中呢？
当然，这个问题还是可以用散列表来解决。不过，我们可以使用一种比较“特殊”的散列表，那就是位图。我们申请一个大小为1亿、数据类型为布尔类型（true或者false）的数组。我们将这1千万个整数作为数组下标，将对应的数组值设置成true。比如，整数5对应下标为5的数组值设置为true，也就是array[5]=true。
当我们查询某个整数K是否在这1千万个整数中的时候，我们只需要将对应的数组值array[K]取出来，看是否等于true。如果等于true，那说明1千万整数中包含这个整数K；相反，就表示不包含这个整数K。
不过，很多语言中提供的布尔类型，大小是1个字节的，并不能节省太多内存空间。实际上，表示true和false两个值，我们只需要用一个二进制位（bit）就可以了。那如何通过编程语言，来表示一个二进制位呢？
这里就要用到位运算了。我们可以借助编程语言中提供的数据类型，比如int、long、char等类型，通过位运算，用其中的某个位表示某个数字。文字描述起来有点儿不好理解，我把位图的代码实现写了出来，你可以对照着代码看下，应该就能看懂了。
public class BitMap { // Java中char类型占16bit，也即是2个字节 private char[] bytes; private int nbits; public BitMap(int nbits) { this.nbits = nbits; this.bytes = new char[nbits/16+1]; }
public void set(int k) { if (k &amp;gt; nbits) return; int byteIndex = k / 16; int bitIndex = k % 16; bytes[byteIndex] |= (1 &amp;lt;&amp;lt; bitIndex); }</description></item><item><title>45_机械硬盘：Google早期用过的“黑科技”</title><link>https://artisanbox.github.io/4/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/45/</guid><description>在1991年，我刚接触计算机的时候，很多计算机还没有硬盘。整个操作系统都安装在5寸或者3.5寸的软盘里。不过，很快大部分计算机都开始用上了直接安装在主板上的机械硬盘。到了今天，更早的软盘早已经被淘汰了。在个人电脑和服务器里，更晚出现的光盘也已经很少用了。
机械硬盘的生命力仍然非常顽强。无论是作为个人电脑的数据盘，还是在数据中心里面用作海量数据的存储，机械硬盘仍然在被大量使用。不仅如此，随着成本的不断下降，机械硬盘还替代掉了很多传统的存储设备，比如，以前常常用来备份冷数据的磁带。
那这一讲里，我们就从机械硬盘的物理构造开始，从原理到应用剖析一下，看看我们可以怎么样用好机械硬盘。
拆解机械硬盘上一讲里，我们提到过机械硬盘的IOPS。我们说，机械硬盘的IOPS，大概只能做到每秒100次左右。那么，这个100次究竟是怎么来的呢？
我们把机械硬盘拆开来看一看，看看它的物理构造是怎么样的，你就自然知道为什么它的IOPS是100左右了。
我们之前看过整个硬盘的构造，里面有接口，有对应的控制电路版，以及实际的I/O设备（也就是我们的机械硬盘）。这里，我们就拆开机械硬盘部分来看一看。
图片来源一块机械硬盘是由盘面、磁头和悬臂三个部件组成的。下面我们一一来看每一个部件。
首先，自然是盘面（Disk Platter）。盘面其实就是我们实际存储数据的盘片。如果你剪开过软盘的外壳，或者看过光盘DVD，那你看到盘面应该很熟悉。盘面其实和它们长得差不多。
盘面本身通常是用的铝、玻璃或者陶瓷这样的材质做成的光滑盘片。然后，盘面上有一层磁性的涂层。我们的数据就存储在这个磁性的涂层上。盘面中间有一个受电机控制的转轴。这个转轴会控制我们的盘面去旋转。
我们平时买硬盘的时候经常会听到一个指标，叫作这个硬盘的转速。我们的硬盘有5400转的、7200转的，乃至10000转的。这个多少多少转，指的就是盘面中间电机控制的转轴的旋转速度，英文单位叫RPM，也就是每分钟的旋转圈数（Rotations Per Minute）。所谓7200转，其实更准确地说是7200RPM，指的就是一旦电脑开机供电之后，我们的硬盘就可以一直做到每分钟转上7200圈。如果折算到每一秒钟，就是120圈。
说完了盘面，我们来看磁头（Drive Head）。我们的数据并不能直接从盘面传输到总线上，而是通过磁头，从盘面上读取到，然后再通过电路信号传输给控制电路、接口，再到总线上的。
通常，我们的一个盘面上会有两个磁头，分别在盘面的正反面。盘面在正反两面都有对应的磁性涂层来存储数据，而且一块硬盘也不是只有一个盘面，而是上下堆叠了很多个盘面，各个盘面之间是平行的。每个盘面的正反两面都有对应的磁头。
最后我们来看悬臂（Actutor Arm）。悬臂链接在磁头上，并且在一定范围内会去把磁头定位到盘面的某个特定的磁道（Track）上。这个磁道是怎么来呢？想要了解这个问题，我们要先看一看我们的数据是怎么存放在盘面上的。
一个盘面通常是圆形的，由很多个同心圆组成，就好像是一个个大小不一样的“甜甜圈”嵌套在一起。每一个“甜甜圈”都是一个磁道。每个磁道都有自己的一个编号。悬臂其实只是控制，到底是读最里面那个“甜甜圈”的数据，还是最外面“甜甜圈”的数据。
图片来源知道了我们硬盘的物理构成，现在我们就可以看一看，这样的物理结构，到底是怎么来读取数据的。
我们刚才说的一个磁道，会分成一个一个扇区（Sector）。上下平行的一个一个盘面的相同扇区呢，我们叫作一个柱面（Cylinder）。
读取数据，其实就是两个步骤。一个步骤，就是把盘面旋转到某一个位置。在这个位置上，我们的悬臂可以定位到整个盘面的某一个子区间。这个子区间的形状有点儿像一块披萨饼，我们一般把这个区间叫作几何扇区（Geometrical Sector），意思是，在“几何位置上”，所有这些扇区都可以被悬臂访问到。另一个步骤，就是把我们的悬臂移动到特定磁道的特定扇区，也就在这个“几何扇区”里面，找到我们实际的扇区。找到之后，我们的磁头会落下，就可以读取到正对着扇区的数据。
所以，我们进行一次硬盘上的随机访问，需要的时间由两个部分组成。
第一个部分，叫作平均延时（Average Latency）。这个时间，其实就是把我们的盘面旋转，把几何扇区对准悬臂位置的时间。这个时间很容易计算，它其实就和我们机械硬盘的转速相关。随机情况下，平均找到一个几何扇区，我们需要旋转半圈盘面。上面7200转的硬盘，那么一秒里面，就可以旋转240个半圈。那么，这个平均延时就是
1s / 240 = 4.17ms第二个部分，叫作平均寻道时间（Average Seek Time），也就是在盘面选转之后，我们的悬臂定位到扇区的的时间。我们现在用的HDD硬盘的平均寻道时间一般在4-10ms。
这样，我们就能够算出来，如果随机在整个硬盘上找一个数据，需要 8-14 ms。我们的硬盘是机械结构的，只有一个电机转轴，也只有一个悬臂，所以我们没有办法并行地去定位或者读取数据。那一块7200转的硬盘，我们一秒钟随机的IO访问次数，也就是
1s / 8 ms = 125 IOPS 或者 1s / 14ms = 70 IOPS现在，你明白我们上一讲所说的，HDD硬盘的IOPS每秒100次左右是怎么来的吧？好了，现在你再思考一个问题。如果我们不是去进行随机的数据访问，而是进行顺序的数据读写，我们应该怎么最大化读取效率呢？
我们可以选择把顺序存放的数据，尽可能地存放在同一个柱面上。这样，我们只需要旋转一次盘面，进行一次寻道，就可以去写入或者读取，同一个垂直空间上的多个盘面的数据。如果一个柱面上的数据不够，我们也不要去动悬臂，而是通过电机转动盘面，这样就可以顺序读完一个磁道上的所有数据。所以，其实对于HDD硬盘的顺序数据读写，吞吐率还是很不错的，可以达到200MB/s左右。
Partial Stroking：根据场景提升性能只有100的IOPS，其实很难满足现在互联网海量高并发的请求。所以，今天的数据库，都会把数据存储在SSD硬盘上。不过，如果我们把时钟倒播20年，那个时候，我们可没有现在这么便宜的SSD硬盘。数据库里面的数据，只能存放在HDD硬盘上。
今天，即便是数据中心用的HDD硬盘，一般也是7200转的，因为如果要更快的随机访问速度，我们会选择用SSD硬盘。但是在当时，SSD硬盘价格非常昂贵，还没有能够商业化。硬盘厂商们在不断地研发转得更快的硬盘。在数据中心里，往往我们会用上10000转，乃至15000转的硬盘。甚至直到2010年，SSD硬盘已经开始逐步进入市场了，西数还在尝试研发20000转的硬盘。转速更高、寻道时间更短的机械硬盘，才能满足实际的数据库需求。
不过，10000转，乃至15000转的硬盘也更昂贵。如果你想要节约成本，提高性价比，那就得想点别的办法。你应该听说过，Google早年用家用PC乃至二手的硬件，通过软件层面的设计来解决可靠性和性能的问题。那么，我们是不是也有什么办法，能提高机械硬盘的IOPS呢？
还真的有。这个方法，就叫作Partial Stroking或者Short Stroking。我没有看到过有中文资料给这个方法命名。在这里，我就暂时把它翻译成“缩短行程”技术。
其实这个方法的思路很容易理解，我一说你就明白了。既然我们访问一次数据的时间，是“平均延时+寻道时间”，那么只要能缩短这两个之一，不就可以提升IOPS了吗？
一般情况下，硬盘的寻道时间都比平均延时要长。那么我们自然就可以想一下，有什么办法可以缩短平均的寻道时间。最极端的办法就是我们不需要寻道，也就是说，我们把所有数据都放在一个磁道上。比如，我们始终把磁头放在最外道的磁道上。这样，我们的寻道时间就基本为0，访问时间就只有平均延时了。那样，我们的IOPS，就变成了
1s / 4ms = 250 IOPS不过呢，只用一个磁道，我们能存的数据就比较有限了。这个时候，可能我们还不如把这些数据直接都放到内存里面呢。所以，实践当中，我们可以只用1/2或者1/4的磁道，也就是最外面1/4或者1/2的磁道。这样，我们硬盘可以使用的容量可能变成了1/2或者1/4。但是呢，我们的寻道时间，也变成了1/4或者1/2，因为悬臂需要移动的“行程”也变成了原来的1/2或者1/4，我们的IOPS就能够大幅度提升了。
比如说，我们一块7200转的硬盘，正常情况下，平均延时是4.17ms，而寻道时间是9ms。那么，它原本的IOPS就是
1s / (4.</description></item><item><title>45_自增id用完怎么办？</title><link>https://artisanbox.github.io/1/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/45/</guid><description>MySQL里有很多自增的id，每个自增id都是定义了初始值，然后不停地往上加步长。虽然自然数是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无符号整型(unsigned int)是4个字节，上限就是232-1。
既然自增id有上限，就有可能被用完。但是，自增id用完了会怎么样呢？
今天这篇文章，我们就来看看MySQL里面的几种自增id，一起分析一下它们的值达到上限以后，会出现什么情况。
表定义自增值id说到自增id，你第一个想到的应该就是表结构定义里的自增字段，也就是我在第39篇文章《自增主键为什么不是连续的？》中和你介绍过的自增主键id。
表定义的自增值达到上限后的逻辑是：再申请下一个id时，得到的值保持不变。
我们可以通过下面这个语句序列验证一下：
create table t(id int unsigned auto_increment primary key) auto_increment=4294967295; insert into t values(null); //成功插入一行 4294967295 show create table t; /* CREATE TABLE `t` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4294967295; */ insert into t values(null); //Duplicate entry &amp;lsquo;4294967295&amp;rsquo; for key &amp;lsquo;PRIMARY&amp;rsquo; 可以看到，第一个insert语句插入数据成功后，这个表的AUTO_INCREMENT没有改变（还是4294967295），就导致了第二个insert语句又拿到相同的自增id值，再试图执行插入语句，报主键冲突错误。
232-1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创建成8个字节的bigint unsigned。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;InnoDB系统自增row_id如果你创建的InnoDB表没有指定主键，那么InnoDB会给你创建一个不可见的，长度为6个字节的row_id。InnoDB维护了一个全局的dict_sys.row_id值，所有无主键的InnoDB表，每插入一行数据，都将当前的dict_sys.row_id值作为要插入数据的row_id，然后把dict_sys.row_id的值加1。
实际上，在代码实现时row_id是一个长度为8字节的无符号长整型(bigint unsigned)。但是，InnoDB在设计时，给row_id留的只是6个字节的长度，这样写到数据表中时只放了最后6个字节，所以row_id能写到数据表中的值，就有两个特征：
row_id写入表中的值范围，是从0到248-1；
当dict_sys.row_id=248时，如果再有插入数据的行为要来申请row_id，拿到以后再取最后6个字节的话就是0。
也就是说，写入表的row_id是从0开始到248-1。达到上限后，下一个值就是0，然后继续循环。</description></item><item><title>46_AArch64体系：ARM最新编程架构模型剖析</title><link>https://artisanbox.github.io/9/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/46/</guid><description>你好，我是LMOS。
在今天，Andriod+ARM已经成了移动领域的霸主，这与当年的Windows+Intel何其相似。之前我们已经在Intel的x86 CPU上实现了Cosmos，今天我会给你讲讲ARM的AArch64体系结构，带你扩展一下视野。
首先，我们来看看什么是AArch64体系，然后分析一下AArch64体系有什么特点，最后了解一下AArch64体系下运行程序的基础，包括AArch64体系下的寄存器、运行模式、异常与中断处理，以及AArch64体系的地址空间与内存模型。
话不多说，下面我们进入正题。
什么是AArch64体系ARM架构在不断发展，现在它在各个领域都得到了非常广泛地应用。
自从Acorn公司于1983年开始发布第一个版本，到目前为止，有九个主要版本，版本号由1到9表示。2011年，Acorn公司发布了ARMv8版本。
ARMv8是首款支持64位指令集的ARM处理器架构，它兼容了ARMv7与之前处理器的技术基础，同样它也兼容现有的A32（ARM 32bit）指令集，还扩充了基于64bit的AArch64架构。
下面我们一起来看看ARMv8一共定义了哪几种架构，一共有三种。
1.ARMv8-A（Application）架构，支持基于内存管理的虚拟内存系统体系结构（VMSA），支持A64、A32和T32指令集，主打高性能，在我们的移动智能设备中广泛应用。
2.ARMv8-R（Real-time）架构，支持基于内存保护的受保护内存系统架构（PMSA），支持A32和T32指令集，一般用于实时计算系统。
3.ARMv8-M（Microcontroller架构），是一个压缩成本的嵌入式架构，而且需要极低延迟中断处理。它支持T32指令集的变体，主打低功耗，一般用于物联网设备。
今天我们要讨论的AArch64，它只是ARMv8-A架构下的一种执行状态，“64”表示内存或者数据都保存在64位的寄存器中，并且它的基本指令集可以用64位寄存器进行数据运算处理。
AArch64体系的寄存器一款处理器要运行程序和处理数据，必须要有一定数量的寄存器。特别是基于RISC（精简指令集）架构的ARM处理器，寄存器数量非常之多，因为大量的指令操作的就是寄存器。
ARMv8-AArch64体系下的寄存器简单可以分为以下几类。
1.通用寄存器
2.特殊寄存器
3.系统寄存器
下面我们分别来看看这三类寄存器。
通用寄存器R0-R30首先来看通用寄存器（general-purpose registers），通用寄存器一共为31个，从R0到R30，这个31个寄存器可以作为全64位使用，也可以只使用其中的低32位。
全64位的寄存器以x0到x30名称进行引用，用于32位或者64位的整数运算或者64位的寻址；低32位寄存器以W0到W30名称进行引用，只能用于32位的整数运算或者32位的寻址。为了帮你理解，我还在后面画了示意图。
通用寄存器中还有32个向量寄存器（SIMD），编号从V0到V31。因为向量计算依然是数据运算类的，所以要把它们归纳到通用寄存器中。每个向量寄存器都是128位的，但是它们可以单独使用其中的8位、16位、32位、64位，它们的访问方式和索引名称如下所示。
Q0到Q31为一个128-bit的向量寄存器 ； D0到D31为一个64-bit的向量寄存器； S0到S31为一个32-bit的向量寄存器； H0到H31为一个16-bit的向量寄存器； B0到B31为一个8-bit的向量寄存器； 特殊寄存器特殊寄存器（spseical registers）比通用寄存器稍微复杂一些，它还可以细分，包括程序计数寄存器（PC），栈指针寄存器（SP），异常链接寄存器（ELR_ELx），程序状态寄存器（PSTATE、SPSR_ELx）等。
PC寄存器
PC寄存器，保存当前指令地址的64位程序计数器，指向即将要执行的下一条指令，CPU正是在这个寄存器的指引下，一条一条地运行代码指令。在ARMv7上，PC寄存器就是通用寄存器R15，而在ARMv8上，PC寄存器不再是通用寄存器，不能直接被修改，只可以通过隐式的指令来改变，例如PC-relative load。
SP寄存器
SP是64位的栈指针寄存器，可以通过WSP寄存器访问低32位，在指令中使用SP作为操作数，表示使用当前栈指针。C语言调用函数和分配局部变量都需要用栈，栈是一种后进先出的内存空间，而SP寄存器中保存的就是栈顶的内存地址。
ELR_ELx异常链接寄存器
每个异常状态下都有一个ELR_EL寄存器，ELR_ELx 寄存器是异常综合寄存器或者异常状态寄存器 ，负责保存异常进入Elx的地址和发生异常的原因等信息。
该寄存器只有ELR_EL1、ELR_EL2、ELR_EL3这几种，没用ELR_EL0寄存器，因为异常不会routing(target)到EL0。例如：16bit指令的异常、32bit指令的异常、simd浮点运算的异常、MSR/MRS的异常。
PSTATE
PSTATE不是单独的一个寄存器，而是保存当前PE（Processing Element）状态的一组寄存器统称，其中可访问寄存器有：NZCV、DAIF、CurrentEL（）、SPSel。这些属于ARMv8新增内容，在64bit下可以代替CPSR（32位系统下的PE信息）。
type ProcState is ( // PSTATE.{N, Z, C, V}： 条件标志位，这些位的含义跟之前AArch32位一样，分别表示补码标志，运算结果为0标志，进位标志，带符号位溢出标志 bits (1) N, // Negative condition flag bits (1) Z, // Zero condition flag bits (1) C, // Carry condition flag bits (1) V, // oVerflow condition flag // D表示debug异常产生，比如软件断点指令/断点/观察点/向量捕获/软件单步 等； // A, I, F表示异步异常标志，异步异常会有两种类型：一种是物理中断产生的，包括SError（系统错误类型，包括外部数据终止），IRQ或者FIQ； // 另一种是虚拟中断产生的，这种中断发生在运行在EL2管理者enable的情况下：vSError，vIRQ，vFIQ； bits (1) D, // Debug mask bit [AArch64 only] bits (1) A, // Asynchronous abort mask bit bits (1) I, // IRQ mask bit bits (1) F, // FIQ mask bit // 异常发生的时候，通过设置MDSCR_EL1.</description></item><item><title>46_SSD硬盘（上）：如何完成性能优化的KPI？</title><link>https://artisanbox.github.io/4/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/46/</guid><description>随着3D垂直封装技术和QLC技术的出现，今年的“618”，SSD硬盘的价格进一步大跳水，趁着这个机会，我把自己电脑上的仓库盘，从HDD换成了SSD硬盘。我的个人电脑彻底摆脱了机械硬盘。
随着智能手机的出现，互联网用户在2008年之后开始爆发性增长，大家在网上花的时间也越来越多。这也就意味着，隐藏在精美App和网页之后的服务端数据请求量，呈数量级的上升。
无论是用10000转的企业级机械硬盘，还是用Short Stroking这样的方式进一步提升IOPS，HDD硬盘已经满足不了我们的需求了。上面这些优化措施，无非就是，把IOPS从100提升到300、500也就到头了。
于是，SSD硬盘在2010年前后，进入了主流的商业应用。我们在第44讲看过，一块普通的SSD硬盘，可以轻松支撑10000乃至20000的IOPS。那个时候，不少互联网公司想要完成性能优化的KPI，最后的解决方案都变成了换SSD的硬盘。如果这还不够，那就换上使用PCI Express接口的SSD。
不过，只是简单地换一下SSD硬盘，真的最大限度地用好了SSD硬盘吗？另外，即便现在SSD硬盘很便宜了，大部分公司的批量数据处理系统，仍然在用传统的机械硬盘，这又是为什么呢？
那么接下来这两讲，就请你和我一起来看一看，SSD硬盘的工作原理，以及怎么最大化利用SSD的工作原理，使得访问的速度最快，硬盘的使用寿命最长。
SSD的读写原理SSD没有像机械硬盘那样的寻道过程，所以它的随机读写都更快。我在下面列了一个表格，对比了一下SSD和机械硬盘的优缺点。
你会发现，不管是机械硬盘不擅长的随机读写，还是它本身已经表现不错的顺序写入，SSD在这些方面都要比HDD强。不过，有一点，机械硬盘要远强于SSD，那就是耐用性。如果我们需要频繁地重复写入删除数据，那么机械硬盘要比SSD性价比高很多。
要想知道为什么SSD的耐用性不太好，我们先要理解SSD硬盘的存储和读写原理。我们之前说过，CPU Cache用的SRAM是用一个电容来存放一个比特的数据。对于SSD硬盘，我们也可以先简单地认为，它是由一个电容加上一个电压计组合在一起，记录了一个或者多个比特。
SLC、MLC、TLC和QLC能够记录一个比特很容易理解。给电容里面充上电有电压的时候就是1，给电容放电里面没有电就是0。采用这样方式存储数据的SSD硬盘，我们一般称之为使用了SLC的颗粒，全称是Single-Level Cell，也就是一个存储单元中只有一位数据。
但是，这样的方式会遇到和CPU Cache类似的问题，那就是，同样的面积下，能够存放下的元器件是有限的。如果只用SLC，我们就会遇到，存储容量上不去，并且价格下不来的问题。于是呢，硬件工程师们就陆续发明了MLC（Multi-Level Cell）、TLC（Triple-Level Cell）以及QLC（Quad-Level Cell），也就是能在一个电容里面存下2个、3个乃至4个比特。
只有一个电容，我们怎么能够表示更多的比特呢？别忘了，这里我们还有一个电压计。4个比特一共可以从0000-1111表示16个不同的数。那么，如果我们能往电容里面充电的时候，充上15个不同的电压，并且我们电压计能够区分出这15个不同的电压。加上电容被放空代表的0，就能够代表从0000-1111这样4个比特了。
不过，要想表示15个不同的电压，充电和读取的时候，对于精度的要求就会更高。这会导致充电和读取的时候都更慢，所以QLC的SSD的读写速度，要比SLC的慢上好几倍。如果你想要知道是什么样的物理原理导致这个QLC更慢，可以去读一读这篇文章。
P/E擦写问题如果我们去看一看SSD硬盘的硬件构造，可以看到，它大概是自顶向下是这么构成的。
首先，自然和其他的I/O设备一样，它有对应的接口和控制电路。现在的SSD硬盘用的是SATA或者PCI Express接口。在控制电路里，有一个很重要的模块，叫作FTL（Flash-Translation Layer），也就是闪存转换层。这个可以说是SSD硬盘的一个核心模块，SSD硬盘性能的好坏，很大程度上也取决于FTL的算法好不好。现在容我卖个关子，我们晚一会儿仔细讲FTL的功能。
接下来是实际I/O设备，它其实和机械硬盘很像。现在新的大容量SSD硬盘都是3D封装的了，也就是说，是由很多个裸片（Die）叠在一起的，就好像我们的机械硬盘把很多个盘面（Platter）叠放再一起一样，这样可以在同样的空间下放下更多的容量。
接下来，一张裸片上可以放多个平面（Plane），一般一个平面上的存储容量大概在GB级别。一个平面上面，会划分成很多个块（Block），一般一个块（Block）的存储大小， 通常几百KB到几MB大小。一个块里面，还会区分很多个页（Page），就和我们内存里面的页一样，一个页的大小通常是4KB。
在这一层一层的结构里面，处在最下面的两层块和页非常重要。
对于SSD硬盘来说，数据的写入叫作Program。写入不能像机械硬盘一样，通过覆写（Overwrite）来进行的，而是要先去擦除（Erase），然后再写入。
SSD的读取和写入的基本单位，不是一个比特（bit）或者一个字节（byte），而是一个页（Page）。SSD的擦除单位就更夸张了，我们不仅不能按照比特或者字节来擦除，连按照页来擦除都不行，我们必须按照块来擦除。
而且，你必须记住的一点是，SSD的使用寿命，其实是每一个块（Block）的擦除的次数。你可以把SSD硬盘的一个平面看成是一张白纸。我们在上面写入数据，就好像用铅笔在白纸上写字。如果想要把已经写过字的地方写入新的数据，我们先要用橡皮把已经写好的字擦掉。但是，如果频繁擦同一个地方，那这个地方就会破掉，之后就没有办法再写字了。
我们上面说的SLC的芯片，可以擦除的次数大概在10万次，MLC就在1万次左右，而TLC和QLC就只在几千次了。这也是为什么，你去购买SSD硬盘，会看到同样的容量的价格差别很大，因为它们的芯片颗粒和寿命完全不一样。
SSD读写的生命周期下面我们来实际看一看，一块SSD硬盘在日常是怎么被用起来的。
我用三种颜色分别来表示SSD硬盘里面的页的不同状态，白色代表这个页从来没有写入过数据，绿色代表里面写入的是有效的数据，红色代表里面的数据，在我们的操作系统看来已经是删除的了。
一开始，所有块的每一个页都是白色的。随着我们开始往里面写数据，里面的有些页就变成了绿色。
然后，因为我们删除了硬盘上的一些文件，所以有些页变成了红色。但是这些红色的页，并不能再次写入数据。因为SSD硬盘不能单独擦除一个页，必须一次性擦除整个块，所以新的数据，我们只能往后面的白色的页里面写。这些散落在各个绿色空间里面的红色空洞，就好像硬盘碎片。
如果有哪一个块的数据一次性全部被标红了，那我们就可以把整个块进行擦除。它就又会变成白色，可以重新一页一页往里面写数据。这种情况其实也会经常发生。毕竟一个块不大，也就在几百KB到几MB。你删除一个几MB的文件，数据又是连续存储的，自然会导致整个块可以被擦除。
随着硬盘里面的数据越来越多，红色空洞占的地方也会越来越多。于是，你会发现，我们就要没有白色的空页去写入数据了。这个时候，我们要做一次类似于Windows里面“磁盘碎片整理”或者Java里面的“内存垃圾回收”工作。找一个红色空洞最多的块，把里面的绿色数据，挪到另一个块里面去，然后把整个块擦除，变成白色，可以重新写入数据。
不过，这个“磁盘碎片整理”或者“内存垃圾回收”的工作，我们不能太主动、太频繁地去做。因为SSD的擦除次数是有限的。如果动不动就搞个磁盘碎片整理，那么我们的SSD硬盘很快就会报废了。
说到这里，你可能要问了，这是不是说，我们的SSD硬盘的容量是用不满的？因为我们总会遇到一些红色空洞？
没错，一块SSD的硬盘容量，是没办法完全用满的。不过，为了不得罪消费者，生产SSD硬盘的厂商，其实是预留了一部分空间，专门用来做这个“磁盘碎片整理”工作的。一块标成240G的SSD硬盘，往往实际有256G的硬盘空间。SSD硬盘通过我们的控制芯片电路，把多出来的硬盘空间，用来进行各种数据的闪转腾挪，让你能够写满那240G的空间。这个多出来的16G空间，叫作预留空间（Over Provisioning），一般SSD的硬盘的预留空间都在7%-15%左右。
总结延伸到这里，相信你对SSD硬盘的写入和擦除的原理已经清楚了，也明白了SSD硬盘的使用寿命受限于可以擦除的次数。
仔细想一想，你会发现SSD硬盘，特别适合读多写少的应用。在日常应用里面，我们的系统盘适合用SSD。但是，如果我们用SSD做专门的下载盘，一直下载各种影音数据，然后刻盘备份就不太好了，特别是现在QLC颗粒的SSD，它只有几千次可擦写的寿命啊。
在数据中心里面，SSD的应用场景也是适合读多写少的场景。我们拿SSD硬盘用来做数据库，存放电商网站的商品信息很合适。但是，用来作为Hadoop这样的Map-Reduce应用的数据盘就不行了。因为Map-Reduce任务会大量在任务中间向硬盘写入中间数据再删除掉，这样用不了多久，SSD硬盘的寿命就会到了。
好了，最后让我们总结一下。
这一讲，我们从SSD的物理原理，也就是“电容+电压计”的组合，向你介绍了SSD硬盘存储数据的原理，以及从SLC、MLC、TLC，直到今天的QLC颗粒是怎么回事儿。
然后，我们一起看了SSD硬盘的物理构造，也就是裸片、平面、块、页的层次结构。我们对于数据的写入，只能是一页一页的，不能对页进行覆写。对于数据的擦除，只能整块进行。所以，我们需要用一个，类似“磁盘碎片整理”或者“内存垃圾回收”这样的机制，来清理块当中的数据空洞。而SSD硬盘也会保留一定的预留空间，避免出现硬盘无法写满的情况。
到了这里，我们SSD硬盘在硬件层面的写入机制就介绍完了。不过，更有挑战的一个问题是，在这样的机制下，我们怎么尽可能延长SSD的使用寿命呢？如果要开发一个跑在SSD硬盘上的数据库，我们可以利用SSD的哪些特性呢？想要知道这些，请你一定要记得回来听下一讲。
推荐阅读想要对于SSD的硬件实现原理有所了解，我推荐你去读一读这一篇Understand TLC NAND。
课后思考现在大家使用的数据系统里，往往会有日志系统。你觉得日志系统适合存放在SSD硬盘上吗？
欢迎在留言区写下你的思考。如果有收获，你也可以把这篇文章分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>46_概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？</title><link>https://artisanbox.github.io/2/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/47/</guid><description>上一节我们讲到，如何用位图、布隆过滤器，来过滤重复的数据。今天，我们再讲一个跟过滤相关的问题，如何过滤垃圾短信？
垃圾短信和骚扰电话，我想每个人都收到过吧？买房、贷款、投资理财、开发票，各种垃圾短信和骚扰电话，不胜其扰。如果你是一名手机应用开发工程师，让你实现一个简单的垃圾短信过滤功能以及骚扰电话拦截功能，该用什么样的数据结构和算法实现呢？
算法解析实际上，解决这个问题并不会涉及很高深的算法。今天，我就带你一块看下，如何利用简单的数据结构和算法，解决这种看似非常复杂的问题。
1.基于黑名单的过滤器我们可以维护一个骚扰电话号码和垃圾短信发送号码的黑名单。这个黑名单的收集，有很多途径，比如，我们可以从一些公开的网站上下载，也可以通过类似“360骚扰电话拦截”的功能，通过用户自主标记骚扰电话来收集。对于被多个用户标记，并且标记个数超过一定阈值的号码，我们就可以定义为骚扰电话，并将它加入到我们的黑名单中。
如果黑名单中的电话号码不多的话，我们可以使用散列表、二叉树等动态数据结构来存储，对内存的消耗并不会很大。如果我们把每个号码看作一个字符串，并且假设平均长度是16个字节，那存储50万个电话号码，大约需要10MB的内存空间。即便是对于手机这样的内存有限的设备来说，这点内存的消耗也是可以接受的。
但是，如果黑名单中的电话号码很多呢？比如有500万个。这个时候，如果再用散列表存储，就需要大约100MB的存储空间。为了实现一个拦截功能，耗费用户如此多的手机内存，这显然有点儿不合理。
上一节我们讲了，布隆过滤器最大的特点就是比较省存储空间，所以，用它来解决这个问题再合适不过了。如果我们要存储500万个手机号码，我们把位图大小设置为10倍数据大小，也就是5000万，那也只需要使用5000万个二进制位（5000万bits），换算成字节，也就是不到7MB的存储空间。比起散列表的解决方案，内存的消耗减少了很多。
实际上，我们还有一种时间换空间的方法，可以将内存的消耗优化到极致。
我们可以把黑名单存储在服务器端上，把过滤和拦截的核心工作，交给服务器端来做。手机端只负责将要检查的号码发送给服务器端，服务器端通过查黑名单，判断这个号码是否应该被拦截，并将结果返回给手机端。
用这个解决思路完全不需要占用手机内存。不过，有利就有弊。我们知道，网络通信是比较慢的，所以，网络延迟就会导致处理速度降低。而且，这个方案还有个硬性要求，那就是只有在联网的情况下，才能正常工作。
基于黑名单的过滤器我就讲完了，不过，你可能还会说，布隆过滤器会有判错的概率呀！如果它把一个重要的电话或者短信，当成垃圾短信或者骚扰电话拦截了，对于用户来说，这是无法接受的。你说得没错，这是一个很大的问题。不过，我们现在先放一放，等三种过滤器都讲完之后，我再来解答。
2.基于规则的过滤器刚刚讲了一种基于黑名单的垃圾短信过滤方法，但是，如果某个垃圾短信发送者的号码并不在黑名单中，那这种方法就没办法拦截了。所以，基于黑名单的过滤方式，还不够完善，我们再继续看一种基于规则的过滤方式。
对于垃圾短信来说，我们还可以通过短信的内容，来判断某条短信是否是垃圾短信。我们预先设定一些规则，如果某条短信符合这些规则，我们就可以判定它是垃圾短信。实际上，规则可以有很多，比如下面这几个：
短信中包含特殊单词（或词语），比如一些非法、淫秽、反动词语等；
短信发送号码是群发号码，非我们正常的手机号码，比如+60389585；
短信中包含回拨的联系方式，比如手机号码、微信、QQ、网页链接等，因为群发短信的号码一般都是无法回拨的；
短信格式花哨、内容很长，比如包含各种表情、图片、网页链接等；
符合已知垃圾短信的模板。垃圾短信一般都是重复群发，对于已经判定为垃圾短信的短信，我们可以抽象成模板，将获取到的短信与模板匹配，一旦匹配，我们就可以判定为垃圾短信。
当然，如果短信只是满足其中一条规则，如果就判定为垃圾短信，那会存在比较大的误判的情况。我们可以综合多条规则进行判断。比如，满足2条以上才会被判定为垃圾短信；或者每条规则对应一个不同的得分，满足哪条规则，我们就累加对应的分数，某条短信的总得分超过某个阈值，才会被判定为垃圾短信。
不过，我只是给出了一些制定规则的思路，具体落实到执行层面，其实还有很大的距离，还有很多细节需要处理。比如，第一条规则中，我们该如何定义特殊单词；第二条规则中，我们该如何定义什么样的号码是群发号码等等。限于篇幅，我就不一一详细展开来讲了。我这里只讲一下，如何定义特殊单词？
如果我们只是自己拍脑袋想，哪些单词属于特殊单词，那势必有比较大的主观性，也很容易漏掉某些单词。实际上，我们可以基于概率统计的方法，借助计算机强大的计算能力，找出哪些单词最常出现在垃圾短信中，将这些最常出现的单词，作为特殊单词，用来过滤短信。
不过这种方法的前提是，我们有大量的样本数据，也就是说，要有大量的短信（比如1000万条短信），并且我们还要求，每条短信都做好了标记，它是垃圾短信还是非垃圾短信。
我们对这1000万条短信，进行分词处理（借助中文或者英文分词算法），去掉“的、和、是”等没有意义的停用词（Stop words），得到n个不同的单词。针对每个单词，我们统计有多少个垃圾短信出现了这个单词，有多少个非垃圾短信会出现这个单词，进而求出每个单词出现在垃圾短信中的概率，以及出现在非垃圾短信中的概率。如果某个单词出现在垃圾短信中的概率，远大于出现在非垃圾短信中的概率，那我们就把这个单词作为特殊单词，用来过滤垃圾短信。
文字描述不好理解，我举个例子来解释一下。
3.基于概率统计的过滤器基于规则的过滤器，看起来很直观，也很好理解，但是它也有一定的局限性。一方面，这些规则受人的思维方式局限，规则未免太过简单；另一方面，垃圾短信发送者可能会针对规则，精心设计短信，绕过这些规则的拦截。对此，我们再来看一种更加高级的过滤方式，基于概率统计的过滤方式。
这种基于概率统计的过滤方式，基础理论是基于朴素贝叶斯算法。为了让你更好地理解下面的内容，我们先通过一个非常简单的例子来看下，什么是朴素贝叶斯算法？
假设事件A是“小明不去上学”，事件B是“下雨了”。我们现在统计了一下过去10天的下雨情况和小明上学的情况，作为样本数据。
我们来分析一下，这组样本有什么规律。在这10天中，有4天下雨，所以下雨的概率P(B)=4/10。10天中有3天，小明没有去上学，所以小明不去上学的概率P(A)=3/10。在4个下雨天中，小明有2天没去上学，所以下雨天不去上学的概率P(A|B)=2/4。在小明没有去上学的3天中，有2天下雨了，所以小明因为下雨而不上学的概率是P(B|A)=2/3。实际上，这4个概率值之间，有一定的关系，这个关系就是朴素贝叶斯算法，我们用公式表示出来，就是下面这个样子。
朴素贝叶斯算法是不是非常简单？我们用一个公式就可以将它概括。弄懂了朴素贝叶斯算法，我们再回到垃圾短信过滤这个问题上，看看如何利用朴素贝叶斯算法，来做垃圾短信的过滤。
基于概率统计的过滤器，是基于短信内容来判定是否是垃圾短信。而计算机没办法像人一样理解短信的含义。所以，我们需要把短信抽象成一组计算机可以理解并且方便计算的特征项，用这一组特征项代替短信本身，来做垃圾短信过滤。
我们可以通过分词算法，把一个短信分割成n个单词。这n个单词就是一组特征项，全权代表这个短信。因此，判定一个短信是否是垃圾短信这样一个问题，就变成了，判定同时包含这几个单词的短信是否是垃圾短信。
不过，这里我们并不像基于规则的过滤器那样，非黑即白，一个短信要么被判定为垃圾短信、要么被判定为非垃圾短息。我们使用概率，来表征一个短信是垃圾短信的可信程度。如果我们用公式将这个概率表示出来，就是下面这个样子：
尽管我们有大量的短信样本，但是我们没法通过样本数据统计得到这个概率。为什么不可以呢？你可能会说，我只需要统计同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信有多少个（我们假设有x个），然后看这里面属于垃圾短信的有几个（我们假设有y个），那包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信是垃圾短信的概率就是y/x。
理想很丰满，但现实往往很骨感。你忽视了非常重要的一点，那就是样本的数量再大，毕竟也是有限的，样本中不会有太多同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$的短信的，甚至很多时候，样本中根本不存在这样的短信。没有样本，也就无法计算概率。所以这样的推理方式虽然正确，但是实践中并不好用。
这个时候，朴素贝叶斯公式就可以派上用场了。我们通过朴素贝叶斯公式，将这个概率的求解，分解为其他三个概率的求解。你可以看我画的图。那转化之后的三个概率是否可以通过样本统计得到呢？
P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中 | 短信是垃圾短信）这个概率照样无法通过样本来统计得到。但是我们可以基于下面这条著名的概率规则来计算。
独立事件发生的概率计算公式：P(A*B) = P(A)*P(B)
如果事件A和事件B是独立事件，两者的发生没有相关性，事件A发生的概率P(A)等于p1，事件B发生的概率P(B)等于p2，那两个同时发生的概率P(A*B)就等于P(A)*P(B)。
基于这条独立事件发生概率的计算公式，我们可以把P（W1，W2，W3，…，Wn同时出现在一条短信中 | 短信是垃圾短信）分解为下面这个公式：
其中，P（$W_{i}$出现在短信中 | 短信是垃圾短信）表示垃圾短信中包含$W_{i}$这个单词的概率有多大。这个概率值通过统计样本很容易就能获得。我们假设垃圾短信有y个，其中包含$W_{i}$的有x个，那这个概率值就等于x/y。
P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中 | 短信是垃圾短信）这个概率值，我们就计算出来了，我们再来看下剩下两个。
P（短信是垃圾短信）表示短信是垃圾短信的概率，这个很容易得到。我们把样本中垃圾短信的个数除以总样本短信个数，就是短信是垃圾短信的概率。
不过，P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中）这个概率还是不好通过样本统计得到，原因我们前面说过了，样本空间有限。不过，我们没必要非得计算这一部分的概率值。为什么这么说呢？
实际上，我们可以分别计算同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信，是垃圾短信和非垃圾短信的概率。假设它们分别是p1和p2。我们并不需要单纯地基于p1值的大小来判断是否是垃圾短信，而是通过对比p1和p2值的大小，来判断一条短信是否是垃圾短信。更细化一点讲，那就是，如果p1是p2的很多倍（比如10倍），我们才确信这条短信是垃圾短信。
基于这两个概率的倍数来判断是否是垃圾短信的方法，我们就可以不用计算P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中）这一部分的值了，因为计算p1与p2的时候，都会包含这个概率值的计算，所以在求解p1和p2倍数（p1/p2）的时候，我们也就不需要这个值。</description></item><item><title>47_SSD硬盘（下）：如何完成性能优化的KPI？</title><link>https://artisanbox.github.io/4/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/47/</guid><description>如果你平时用的是Windows电脑，你会发现，用了SSD的系统盘，就不能用磁盘碎片整理功能。这是因为，一旦主动去运行磁盘碎片整理功能，就会发生一次块的擦除，对应块的寿命就少了一点点。这个SSD的擦除寿命的问题，不仅会影响像磁盘碎片整理这样的功能，其实也很影响我们的日常使用。
我们的操作系统上，并没有SSD硬盘上各个块目前已经擦写的情况和寿命，所以它对待SSD硬盘和普通的机械硬盘没有什么区别。
我们日常使用PC进行软件开发的时候，会先在硬盘上装上操作系统和常用软件，比如Office，或者工程师们会装上VS Code、WebStorm这样的集成开发环境。这些软件所在的块，写入一次之后，就不太会擦除了，所以就只有读的需求。
一旦开始开发，我们就会不断添加新的代码文件，还会不断修改已经有的代码文件。因为SSD硬盘没有覆写（Override）的功能，所以，这个过程中，其实我们是在反复地写入新的文件，然后再把原来的文件标记成逻辑上删除的状态。等SSD里面空的块少了，我们会用“垃圾回收”的方式，进行擦除。这样，我们的擦除会反复发生在这些用来存放数据的地方。
有一天，这些块的擦除次数到了，变成了坏块。但是，我们安装操作系统和软件的地方还没有坏，而这块硬盘的可以用的容量却变小了。
磨损均衡、TRIM和写入放大效应FTL和磨损均衡那么，我们有没有什么办法，不让这些坏块那么早就出现呢？我们能不能，匀出一些存放操作系统的块的擦写次数，给到这些存放数据的地方呢？
相信你一定想到了，其实我们要的就是想一个办法，让SSD硬盘各个块的擦除次数，均匀分摊到各个块上。这个策略呢，就叫作磨损均衡（Wear-Leveling）。实现这个技术的核心办法，和我们前面讲过的虚拟内存一样，就是添加一个间接层。这个间接层，就是我们上一讲给你卖的那个关子，就是FTL这个闪存转换层。
就像在管理内存的时候，我们通过一个页表映射虚拟内存页和物理页一样，在FTL里面，存放了逻辑块地址（Logical Block Address，简称LBA）到物理块地址（Physical Block Address，简称PBA）的映射。
操作系统访问的硬盘地址，其实都是逻辑地址。只有通过FTL转换之后，才会变成实际的物理地址，找到对应的块进行访问。操作系统本身，不需要去考虑块的磨损程度，只要和操作机械硬盘一样来读写数据就好了。
操作系统所有对于SSD硬盘的读写请求，都要经过FTL。FTL里面又有逻辑块对应的物理块，所以FTL能够记录下来，每个物理块被擦写的次数。如果一个物理块被擦写的次数多了，FTL就可以将这个物理块，挪到一个擦写次数少的物理块上。但是，逻辑块不用变，操作系统也不需要知道这个变化。
这也是我们在设计大型系统中的一个典型思路，也就是各层之间是隔离的，操作系统不需要考虑底层的硬件是什么，完全交由硬件的控制电路里面的FTL，来管理对于实际物理硬件的写入。
TRIM指令的支持不过，操作系统不去关心实际底层的硬件是什么，在SSD硬盘的使用上，也会带来一个问题。这个问题就是，操作系统的逻辑层和SSD的逻辑层里的块状态，是不匹配的。
我们在操作系统里面去删除一个文件，其实并没有真的在物理层面去删除这个文件，只是在文件系统里面，把对应的inode里面的元信息清理掉，这代表这个inode还可以继续使用，可以写入新的数据。这个时候，实际物理层面的对应的存储空间，在操作系统里面被标记成可以写入了。
所以，其实我们日常的文件删除，都只是一个操作系统层面的逻辑删除。这也是为什么，很多时候我们不小心删除了对应的文件，我们可以通过各种恢复软件，把数据找回来。同样的，这也是为什么，如果我们想要删除干净数据，需要用各种“文件粉碎”的功能才行。
这个删除的逻辑在机械硬盘层面没有问题，因为文件被标记成可以写入，后续的写入可以直接覆写这个位置。但是，在SSD硬盘上就不一样了。我在这里放了一张详细的示意图。我们下面一起来看看具体是怎么回事儿。
一开始，操作系统里面有好几个文件，不同的文件我用不同的颜色标记出来了。下面的SSD的逻辑块里面占用的页，我们也用同样的颜色标记出来文件占用的对应页。
当我们在操作系统里面，删除掉一个刚刚下载的文件，比如标记成黄色 openjdk.exe 这样一个jdk的安装文件，在操作系统里面，对应的inode里面，就没有文件的元信息。
但是，这个时候，我们的SSD的逻辑块层面，其实并不知道这个事情。所以在，逻辑块层面，openjdk.exe 仍然是占用了对应的空间。对应的物理页，也仍然被认为是被占用了的。
这个时候，如果我们需要对SSD进行垃圾回收操作，openjdk.exe 对应的物理页，仍然要在这个过程中，被搬运到其他的Block里面去。只有当操作系统，再在刚才的inode里面写入数据的时候，我们才会知道原来的些黄色的页，其实都已经没有用了，我们才会把它标记成废弃掉。
所以，在使用SSD的硬盘情况下，你会发现，操作系统对于文件的删除，SSD硬盘其实并不知道。这就导致，我们为了磨损均衡，很多时候在都在搬运很多已经删除了的数据。这就会产生很多不必要的数据读写和擦除，既消耗了SSD的性能，也缩短了SSD的使用寿命。
为了解决这个问题，现在的操作系统和SSD的主控芯片，都支持TRIM命令。这个命令可以在文件被删除的时候，让操作系统去通知SSD硬盘，对应的逻辑块已经标记成已删除了。现在的SSD硬盘都已经支持了TRIM命令。无论是Linux、Windows还是MacOS，这些操作系统也都已经支持了TRIM命令了。
写入放大其实，TRIM命令的发明，也反应了一个使用SSD硬盘的问题，那就是，SSD硬盘容易越用越慢。
当SSD硬盘的存储空间被占用得越来越多，每一次写入新数据，我们都可能没有足够的空白。我们可能不得不去进行垃圾回收，合并一些块里面的页，然后再擦除掉一些页，才能匀出一些空间来。
这个时候，从应用层或者操作系统层面来看，我们可能只是写入了一个4KB或者4MB的数据。但是，实际通过FTL之后，我们可能要去搬运8MB、16MB甚至更多的数据。
我们通过“实际的闪存写入的数据量 / 系统通过FTL写入的数据量 = 写入放大”，可以得到，写入放大的倍数越多，意味着实际的SSD性能也就越差，会远远比不上实际SSD硬盘标称的指标。
而解决写入放大，需要我们在后台定时进行垃圾回收，在硬盘比较空闲的时候，就把搬运数据、擦除数据、留出空白的块的工作做完，而不是等实际数据写入的时候，再进行这样的操作。
AeroSpike：如何最大化SSD的使用效率？讲到这里，相信你也发现了，想要把SSD硬盘用好，其实没有那么简单。如果我们只是简单地拿一块SSD硬盘替换掉原来的HDD硬盘，而不是从应用层面考虑任何SSD硬盘特性的话，我们多半还是没法获得想要的性能提升。
不过，既然清楚了SSD硬盘的各种特性，我们就可以依据这些特性，来设计我们的应用。接下来，我就带你一起看一看，AeroSpike这个专门针对SSD硬盘特性设计的Key-Value数据库（键值对数据库），是怎么利用这些物理特性的。
首先，AeroSpike操作SSD硬盘，并没有通过操作系统的文件系统。而是直接操作SSD里面的块和页。因为操作系统里面的文件系统，对于KV数据库来说，只是让我们多了一层间接层，只会降低性能，对我们没有什么实际的作用。
其次，AeroSpike在读写数据的时候，做了两个优化。在写入数据的时候，AeroSpike尽可能去写一个较大的数据块，而不是频繁地去写很多小的数据块。这样，硬盘就不太容易频繁出现磁盘碎片。并且，一次性写入一个大的数据块，也更容易利用好顺序写入的性能优势。AeroSpike写入的一个数据块，是128KB，远比一个页的4KB要大得多。
另外，在读取数据的时候，AeroSpike倒是可以读取512字节（Bytes）这样的小数据。因为SSD的随机读取性能很好，也不像写入数据那样有擦除寿命问题。而且，很多时候我们读取的数据是键值对里面的值的数据，这些数据要在网络上传输。如果一次性必须读出比较大的数据，就会导致我们的网络带宽不够用。
因为AeroSpike是一个对于响应时间要求很高的实时KV数据库，如果出现了严重的写放大效应，会导致写入数据的响应时间大幅度变长。所以AeroSpike做了这样几个动作：
第一个是持续地进行磁盘碎片整理。AeroSpike用了所谓的高水位（High Watermark）算法。其实这个算法很简单，就是一旦一个物理块里面的数据碎片超过50%，就把这个物理块搬运压缩，然后进行数据擦除，确保磁盘始终有足够的空间可以写入。
第二个是在AeroSpike给出的最佳实践中，为了保障数据库的性能，建议你只用到SSD硬盘标定容量的一半。也就是说，我们人为地给SSD硬盘预留了50%的预留空间，以确保SSD硬盘的写放大效应尽可能小，不会影响数据库的访问性能。
正是因为做了这种种的优化，在NoSQL数据库刚刚兴起的时候，AeroSpike的性能把Cassandra、MongoDB这些数据库远远甩在身后，和这些数据库之间的性能差距，有时候会到达一个数量级。这也让AeroSpike成为了当时高性能KV数据库的标杆。你可以看一看InfoQ出的这个Benchmark，里面有2013年的时候，这几个NoSQL数据库巨大的性能差异。
总结延伸好了，现在让我们一起来总结一下今天的内容。
因为SSD硬盘的使用寿命，受限于块的擦除次数，所以我们需要通过一个磨损均衡的策略，来管理SSD硬盘的各个块的擦除次数。我们通过在逻辑块地址和物理块地址之间，引入FTL这个映射层，使得操作系统无需关心物理块的擦写次数，而是由FTL里的软件算法，来协调到底每一次写入应该磨损哪一块。
除了磨损均衡之外，操作系统和SSD硬件的特性还有一个不匹配的地方。那就是，操作系统在删除数据的时候，并没有真的删除物理层面的数据，而只是修改了inode里面的数据。这个“伪删除”，使得SSD硬盘在逻辑和物理层面，都没有意识到有些块其实已经被删除了。这就导致在垃圾回收的时候，会浪费很多不必要的读写资源。
SSD这个需要进行垃圾回收的特性，使得我们在写入数据的时候，会遇到写入放大。明明我们只是写入了4MB的数据，可能在SSD的硬件层面，实际写入了8MB、16MB乃至更多的数据。
针对这些特性，AeroSpike，这个专门针对SSD硬盘特性的KV数据库，设计了很多的优化点，包括跳过文件系统直写硬盘、写大块读小块、用高水位算法持续进行磁盘碎片整理，以及只使用SSD硬盘的一半空间。这些策略，使得AeroSpike的性能，在早年间远远超过了Cassandra等其他NoSQL数据库。
可以看到，针对硬件特性设计的软件，才能最大化发挥我们的硬件性能。
推荐阅读如果你想要基于SSD硬盘本身的特性来设计开发你的系统，我推荐你去读一读AeroSpike的这个PPT。AeroSpike是市面上最优秀的KV数据库之一，通过深入地利用了SSD本身的硬件特性，最大化提升了作为一个KV数据库的性能。真正在进行系统软件开发的时候，了解硬件是必不可少的一个环节。
课后思考在SSD硬盘的价格大幅度下降了之后，LFS，也就是Log-Structured File System，在业界出现了第二春。你可以去了解一下什么是LFS，以及为什么LFS特别适合SSD硬盘。
欢迎在留言区分享你了解到的信息，和大家一起交流。如果有收获，你可以把这篇文章分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>47_向量空间：如何实现一个简单的音乐推荐系统？</title><link>https://artisanbox.github.io/2/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/48/</guid><description>很多人都喜爱听歌，以前我们用MP3听歌，现在直接通过音乐App在线就能听歌。而且，各种音乐App的功能越来越强大，不仅可以自己选歌听，还可以根据你听歌的口味偏好，给你推荐可能会喜爱的音乐，而且有时候，推荐的音乐还非常适合你的口味，甚至会惊艳到你！如此智能的一个功能，你知道它是怎么实现的吗？
算法解析实际上，要解决这个问题，并不需要特别高深的理论。解决思路的核心思想非常简单、直白，用两句话就能总结出来。
找到跟你口味偏好相似的用户，把他们爱听的歌曲推荐给你；
找出跟你喜爱的歌曲特征相似的歌曲，把这些歌曲推荐给你。
接下来，我就分别讲解一下这两种思路的具体实现方法。
1.基于相似用户做推荐如何找到跟你口味偏好相似的用户呢？或者说如何定义口味偏好相似呢？实际上，思路也很简单，我们把跟你听类似歌曲的人，看作口味相似的用户。你可以看我下面画的这个图。我用“1”表示“喜爱”，用“0”笼统地表示“不发表意见”。从图中我们可以看出，你跟小明共同喜爱的歌曲最多，有5首。于是，我们就可以说，小明跟你的口味非常相似。
我们只需要遍历所有的用户，对比每个用户跟你共同喜爱的歌曲个数，并且设置一个阈值，如果你和某个用户共同喜爱的歌曲个数超过这个阈值，我们就把这个用户看作跟你口味相似的用户，把这个用户喜爱但你还没听过的歌曲，推荐给你。
不过，刚刚的这个解决方案中有一个问题，我们如何知道用户喜爱哪首歌曲呢？也就是说，如何定义用户对某首歌曲的喜爱程度呢？
实际上，我们可以通过用户的行为，来定义这个喜爱程度。我们给每个行为定义一个得分，得分越高表示喜爱程度越高。
还是刚刚那个例子，我们如果把每个人对每首歌曲的喜爱程度表示出来，就是下面这个样子。图中，某个人对某首歌曲是否喜爱，我们不再用“1”或者“0”来表示，而是对应一个具体的分值。
有了这样一个用户对歌曲的喜爱程度的对应表之后，如何来判断两个用户是否口味相似呢？
显然，我们不能再像之前那样，采用简单的计数来统计两个用户之间的相似度。还记得我们之前讲字符串相似度度量时，提到的编辑距离吗？这里的相似度度量，我们可以使用另外一个距离，那就是欧几里得距离（Euclidean distance）。欧几里得距离是用来计算两个向量之间的距离的。这个概念中有两个关键词，向量和距离，我来给你解释一下。
一维空间是一条线，我们用1，2，3……这样单个的数，来表示一维空间中的某个位置；二维空间是一个面，我们用（1，3）（4，2）（2，2）……这样的两个数，来表示二维空间中的某个位置；三维空间是一个立体空间，我们用（1，3，5）（3，1，7）（2，4，3）……这样的三个数，来表示三维空间中的某个位置。一维、二维、三维应该都不难理解，那更高维中的某个位置该如何表示呢？
类比一维、二维、三维的表示方法，K维空间中的某个位置，我们可以写作（$X_{1}$，$X_{2}$，$X_{3}$，…，$X_{K}$）。这种表示方法就是向量（vector）。我们知道，二维、三维空间中，两个位置之间有距离的概念，类比到高纬空间，同样也有距离的概念，这就是我们说的两个向量之间的距离。
那如何计算两个向量之间的距离呢？我们还是可以类比到二维、三维空间中距离的计算方法。通过类比，我们就可以得到两个向量之间距离的计算公式。这个计算公式就是欧几里得距离的计算公式：
我们把每个用户对所有歌曲的喜爱程度，都用一个向量表示。我们计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量。从图中的计算可以看出，小明与你的欧几里得距离距离最小，也就是说，你俩在高维空间中靠得最近，所以，我们就断定，小明跟你的口味最相似。
2.基于相似歌曲做推荐刚刚我们讲了基于相似用户的歌曲推荐方法，但是，如果用户是一个新用户，我们还没有收集到足够多的行为数据，这个时候该如何推荐呢？我们现在再来看另外一种推荐方法，基于相似歌曲的推荐方法，也就是说，如果某首歌曲跟你喜爱的歌曲相似，我们就把它推荐给你。
如何判断两首歌曲是否相似呢？对于人来说，这个事情可能会比较简单，但是对于计算机来说，判断两首歌曲是否相似，那就需要通过量化的数据来表示了。我们应该通过什么数据来量化两个歌曲之间的相似程度呢？
最容易想到的是，我们对歌曲定义一些特征项，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等。类似基于相似用户的推荐方法，我们给每个歌曲的每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量。我们可以基于这个特征项向量，来计算两个歌曲之间的欧几里得距离。欧几里得距离越小，表示两个歌曲的相似程度越大。
但是，要实现这个方案，需要有一个前提，那就是我们能够找到足够多，并且能够全面代表歌曲特点的特征项，除此之外，我们还要人工给每首歌标注每个特征项的得分。对于收录了海量歌曲的音乐App来说，这显然是一个非常大的工程。此外，人工标注有很大的主观性，也会影响到推荐的准确性。
既然基于歌曲特征项计算相似度不可行，那我们就换一种思路。对于两首歌，如果喜欢听的人群都是差不多的，那侧面就可以反映出，这两首歌比较相似。如图所示，每个用户对歌曲有不同的喜爱程度，我们依旧通过上一个解决方案中定义得分的标准，来定义喜爱程度。
你有没有发现，这个图跟基于相似用户推荐中的图几乎一样。只不过这里把歌曲和用户主次颠倒了一下。基于相似用户的推荐方法中，针对每个用户，我们将对各个歌曲的喜爱程度作为向量。基于相似歌曲的推荐思路中，针对每个歌曲，我们将每个用户的打分作为向量。
有了每个歌曲的向量表示，我们通过计算向量之间的欧几里得距离，来表示歌曲之间的相似度。欧几里得距离越小，表示两个歌曲越相似。然后，我们就在用户已经听过的歌曲中，找出他喜爱程度较高的歌曲。然后，我们找出跟这些歌曲相似度很高的其他歌曲，推荐给他。
总结引申实际上，这个问题是推荐系统（Recommendation System）里最典型的一类问题。之所以讲这部分内容，主要还是想给你展示，算法的强大之处，利用简单的向量空间的欧几里得距离，就能解决如此复杂的问题。不过，今天，我只给你讲解了基本的理论，实践中遇到的问题还有很多，比如冷启动问题，产品初期积累的数据不多，不足以做推荐等等。这些更加深奥的内容，你可以之后自己在实践中慢慢探索。
课后思考关于今天讲的推荐算法，你还能想到其他应用场景吗？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>48_B+树：MySQL数据库索引是如何实现的？</title><link>https://artisanbox.github.io/2/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/49/</guid><description>作为一个软件开发工程师，你对数据库肯定再熟悉不过了。作为主流的数据存储系统，它在我们的业务开发中，有着举足轻重的地位。在工作中，为了加速数据库中数据的查找速度，我们常用的处理思路是，对表中数据创建索引。那你是否思考过，数据库索引是如何实现的呢？底层使用的是什么数据结构和算法呢？
算法解析思考的过程比结论更重要。跟着我学习了这么多节课，很多同学已经意识到这一点，比如Jerry银银同学。我感到很开心。所以，今天的讲解，我会尽量还原这个解决方案的思考过程，让你知其然，并且知其所以然。
1.解决问题的前提是定义清楚问题如何定义清楚问题呢？除了对问题进行详细的调研，还有一个办法，那就是，通过对一些模糊的需求进行假设，来限定要解决的问题的范围。
如果你对数据库的操作非常了解，针对我们现在这个问题，你就能把索引的需求定义得非常清楚。但是，对于大部分软件工程师来说，我们可能只了解一小部分常用的SQL语句，所以，这里我们假设要解决的问题，只包含这样两个常用的需求：
根据某个值查找数据，比如select * from user where id=1234；
根据区间值来查找某些数据，比如select * from user where id &amp;gt; 1234 and id &amp;lt; 2345。
除了这些功能性需求之外，这种问题往往还会涉及一些非功能性需求，比如安全、性能、用户体验等等。限于专栏要讨论的主要是数据结构和算法，对于非功能性需求，我们着重考虑性能方面的需求。性能方面的需求，我们主要考察时间和空间两方面，也就是执行效率和存储空间。
在执行效率方面，我们希望通过索引，查询数据的效率尽可能地高；在存储空间方面，我们希望索引不要消耗太多的内存空间。
2.尝试用学过的数据结构解决这个问题问题的需求大致定义清楚了，我们现在回想一下，能否利用已经学习过的数据结构解决这个问题呢？支持快速查询、插入等操作的动态数据结构，我们已经学习过散列表、平衡二叉查找树、跳表。
我们先来看散列表。散列表的查询性能很好，时间复杂度是O(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。
我们再来看平衡二叉查找树。尽管平衡二叉查找树查询的性能也很高，时间复杂度是O(logn)。而且，对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但这仍然不足以支持按照区间快速查找数据。
我们再来看跳表。跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是O(logn)。并且，跳表也支持按照区间快速地查找数据。我们只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止，这期间遍历得到的数据就是满足区间值的数据。
这样看来，跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫作B+树。不过，它是通过二叉查找树演化过来的，而非跳表。为了给你还原发明B+树的整个思考过程，所以，接下来，我还要从二叉查找树讲起，看它是如何一步一步被改造成B+树的。
3.改造二叉查找树来解决这个问题为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。经过改造之后的二叉树，就像图中这样，看起来是不是很像跳表呢？
改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。
但是，我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。
比如，我们给一亿个数据构建二叉查找树索引，那索引中会包含大约1亿个节点，每个节点假设占用16个字节，那就需要大约1GB的内存空间。给一张表建立索引，我们需要1GB的内存空间。如果我们要给10张表建立索引，那对内存的需求是无法满足的。如何解决这个索引占用太多内存的问题呢？
我们可以借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。
这种将索引存储在硬盘中的方案，尽管减少了内存消耗，但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。
二叉查找树，经过改造之后，支持区间查找的功能就实现了。不过，为了节省内存，如果把树存储在硬盘中，那么每个节点的读取（或者访问），都对应一次磁盘IO操作。树的高度就等于每次查询数据时磁盘IO操作的次数。
我们前面讲到，比起内存读写操作，磁盘IO操作非常耗时，所以我们优化的重点就是尽量减少磁盘IO操作，也就是，尽量降低树的高度。那如何降低树的高度呢？
我们来看下，如果我们把索引构建成m叉树，高度是不是比二叉树要小呢？如图所示，给16个数据构建二叉树索引，树的高度是4，查找一个数据，就需要4个磁盘IO操作（如果根节点存储在内存中，其他节点存储在磁盘中），如果对16个数据构建五叉树索引，那高度只有2，查找一个数据，对应只需要2次磁盘操作。如果m叉树中的m是100，那对一亿个数据构建索引，树的高度也只是3，最多只要3次磁盘IO就能获取到数据。磁盘IO变少了，查找数据的效率也就提高了。
如果我们将m叉树实现B+树索引，用代码实现出来，就是下面这个样子（假设我们给int类型的数据库字段添加索引，所以代码中的keywords是int类型的）：
/** * 这是B+树非叶子节点的定义。 * * 假设keywords=[3, 5, 8, 10] * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF) * 5个区间分别对应：children[0]...children[4] * * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = (m-1)*4[keywordss大小]+m*8[children大小] */ public class BPlusTreeNode { public static int m = 5; // 5叉树 public int[] keywords = new int[m-1]; // 键值，用来划分数据区间 public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针 } /**</description></item><item><title>48_DMA：为什么Kafka这么快？</title><link>https://artisanbox.github.io/4/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/48/</guid><description>过去几年里，整个计算机产业界，都在尝试不停地提升I/O设备的速度。把HDD硬盘换成SSD硬盘，我们仍然觉得不够快；用PCI Express接口的SSD硬盘替代SATA接口的SSD硬盘，我们还是觉得不够快，所以，现在就有了傲腾（Optane）这样的技术。
但是，无论I/O速度如何提升，比起CPU，总还是太慢。SSD硬盘的IOPS可以到2万、4万，但是我们CPU的主频有2GHz以上，也就意味着每秒会有20亿次的操作。
如果我们对于I/O的操作，都是由CPU发出对应的指令，然后等待I/O设备完成操作之后返回，那CPU有大量的时间其实都是在等待I/O设备完成操作。
但是，这个CPU的等待，在很多时候，其实并没有太多的实际意义。我们对于I/O设备的大量操作，其实都只是把内存里面的数据，传输到I/O设备而已。在这种情况下，其实CPU只是在傻等而已。特别是当传输的数据量比较大的时候，比如进行大文件复制，如果所有数据都要经过CPU，实在是有点儿太浪费时间了。
因此，计算机工程师们，就发明了DMA技术，也就是直接内存访问（Direct Memory Access）技术，来减少CPU等待的时间。
理解DMA，一个协处理器其实DMA技术很容易理解，本质上，DMA技术就是我们在主板上放一块独立的芯片。在进行内存和I/O设备的数据传输的时候，我们不再通过CPU来控制数据传输，而直接通过DMA控制器（DMA Controller，简称DMAC）。这块芯片，我们可以认为它其实就是一个协处理器（Co-Processor）。
DMAC最有价值的地方体现在，当我们要传输的数据特别大、速度特别快，或者传输的数据特别小、速度特别慢的时候。
比如说，我们用千兆网卡或者硬盘传输大量数据的时候，如果都用CPU来搬运的话，肯定忙不过来，所以可以选择DMAC。而当数据传输很慢的时候，DMAC可以等数据到齐了，再发送信号，给到CPU去处理，而不是让CPU在那里忙等待。
好了，现在你应该明白DMAC的价值，知道了它适合用在什么情况下。那我们现在回过头来看。我们上面说，DMAC是一块“协处理器芯片”，这是为什么呢？
注意，这里面的“协”字。DMAC是在“协助”CPU，完成对应的数据传输工作。在DMAC控制数据传输的过程中，我们还是需要CPU的。
除此之外，DMAC其实也是一个特殊的I/O设备，它和CPU以及其他I/O设备一样，通过连接到总线来进行实际的数据传输。总线上的设备呢，其实有两种类型。一种我们称之为主设备（Master），另外一种，我们称之为从设备（Slave）。
想要主动发起数据传输，必须要是一个主设备才可以，CPU就是主设备。而我们从设备（比如硬盘）只能接受数据传输。所以，如果通过CPU来传输数据，要么是CPU从I/O设备读数据，要么是CPU向I/O设备写数据。
这个时候你可能要问了，那我们的I/O设备不能向主设备发起请求么？可以是可以，不过这个发送的不是数据内容，而是控制信号。I/O设备可以告诉CPU，我这里有数据要传输给你，但是实际数据是CPU拉走的，而不是I/O设备推给CPU的。
不过，DMAC就很有意思了，它既是一个主设备，又是一个从设备。对于CPU来说，它是一个从设备；对于硬盘这样的IO设备来说呢，它又变成了一个主设备。那使用DMAC进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。
1.首先，CPU还是作为一个主设备，向DMAC设备发起请求。这个请求，其实就是在DMAC里面修改配置寄存器。
2.CPU修改DMAC的配置的时候，会告诉DMAC这样几个信息：
首先是源地址的初始值以及传输时候的地址增减方式。
所谓源地址，就是数据要从哪里传输过来。如果我们要从内存里面写入数据到硬盘上，那么就是要读取的数据在内存里面的地址。如果是从硬盘读取数据到内存里，那就是硬盘的I/O接口的地址。
我们讲过总线的时候说过，I/O的地址可以是一个内存地址，也可以是一个端口地址。而地址的增减方式就是说，数据是从大的地址向小的地址传输，还是从小的地址往大的地址传输。 其次是目标地址初始值和传输时候的地址增减方式。目标地址自然就是和源地址对应的设备，也就是我们数据传输的目的地。 第三个自然是要传输的数据长度，也就是我们一共要传输多少数据。 3.设置完这些信息之后，DMAC就会变成一个空闲的状态（Idle）。
4.如果我们要从硬盘上往内存里面加载数据，这个时候，硬盘就会向DMAC发起一个数据传输请求。这个请求并不是通过总线，而是通过一个额外的连线。
5.然后，我们的DMAC需要再通过一个额外的连线响应这个申请。
6.于是，DMAC这个芯片，就向硬盘的接口发起要总线读的传输请求。数据就从硬盘里面，读到了DMAC的控制器里面。
7.然后，DMAC再向我们的内存发起总线写的数据传输请求，把数据写入到内存里面。
8.DMAC会反复进行上面第6、7步的操作，直到DMAC的寄存器里面设置的数据长度传输完成。
9.数据传输完成之后，DMAC重新回到第3步的空闲状态。
所以，整个数据传输的过程中，我们不是通过CPU来搬运数据，而是由DMAC这个芯片来搬运数据。但是CPU在这个过程中也是必不可少的。因为传输什么数据，从哪里传输到哪里，其实还是由CPU来设置的。这也是为什么，DMAC被叫作“协处理器”。
现在的外设里面，很多都内置了DMAC最早，计算机里是没有DMAC的，所有数据都是由CPU来搬运的。随着人们对于数据传输的需求越来越多，先是出现了主板上独立的DMAC控制器。到了今天，各种I/O设备越来越多，数据传输的需求越来越复杂，使用的场景各不相同。加之显示器、网卡、硬盘对于数据传输的需求都不一样，所以各个设备里面都有自己的DMAC芯片了。
为什么那么快？一起来看Kafka的实现原理了解了DMAC是怎么回事儿，那你可能要问了，这和我们实际进行程序开发有什么关系呢？有什么API，我们直接调用一下，就能加速数据传输，减少CPU占用吗？
你还别说，过去几年的大数据浪潮里面，还真有一个开源项目很好地利用了DMA的数据传输方式，通过DMA的方式实现了非常大的性能提升。这个项目就是Kafka。下面我们就一起来看看它究竟是怎么利用DMA的。
Kafka是一个用来处理实时数据的管道，我们常常用它来做一个消息队列，或者用来收集和落地海量的日志。作为一个处理实时数据和日志的管道，瓶颈自然也在I/O层面。
Kafka里面会有两种常见的海量数据传输的情况。一种是从网络中接收上游的数据，然后需要落地到本地的磁盘上，确保数据不丢失。另一种情况呢，则是从本地磁盘上读取出来，通过网络发送出去。
我们来看一看后一种情况，从磁盘读数据发送到网络上去。如果我们自己写一个简单的程序，最直观的办法，自然是用一个文件读操作，从磁盘上把数据读到内存里面来，然后再用一个Socket，把这些数据发送到网络上去。
File.read(fileDesc, buf, len); Socket.send(socket, buf, len); 代码来源这段伪代码，来自IBM Developer Works上关于Zero Copy的文章在这个过程中，数据一共发生了四次传输的过程。其中两次是DMA的传输，另外两次，则是通过CPU控制的传输。下面我们来具体看看这个过程。
第一次传输，是从硬盘上，读到操作系统内核的缓冲区里。这个传输是通过DMA搬运的。
第二次传输，需要从内核缓冲区里面的数据，复制到我们应用分配的内存里面。这个传输是通过CPU搬运的。
第三次传输，要从我们应用的内存里面，再写到操作系统的Socket的缓冲区里面去。这个传输，还是由CPU搬运的。
最后一次传输，需要再从Socket的缓冲区里面，写到网卡的缓冲区里面去。这个传输又是通过DMA搬运的。
这个时候，你可以回过头看看这个过程。我们只是要“搬运”一份数据，结果却整整搬运了四次。而且这里面，从内核的读缓冲区传输到应用的内存里，再从应用的内存里传输到Socket的缓冲区里，其实都是把同一份数据在内存里面搬运来搬运去，特别没有效率。
像Kafka这样的应用场景，其实大部分最终利用到的硬件资源，其实又都是在干这个搬运数据的事儿。所以，我们就需要尽可能地减少数据搬运的需求。
事实上，Kafka做的事情就是，把这个数据搬运的次数，从上面的四次，变成了两次，并且只有DMA来进行数据搬运，而不需要CPU。
@Override public long transferFrom(FileChannel fileChannel, long position, long count) throws IOException { return fileChannel.</description></item><item><title>49_搜索：如何用Ax搜索算法实现游戏中的寻路功能？</title><link>https://artisanbox.github.io/2/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/50/</guid><description>魔兽世界、仙剑奇侠传这类MMRPG游戏，不知道你有没有玩过？在这些游戏中，有一个非常重要的功能，那就是人物角色自动寻路。当人物处于游戏地图中的某个位置的时候，我们用鼠标点击另外一个相对较远的位置，人物就会自动地绕过障碍物走过去。玩过这么多游戏，不知你是否思考过，这个功能是怎么实现的呢？
算法解析实际上，这是一个非常典型的搜索问题。人物的起点就是他当下所在的位置，终点就是鼠标点击的位置。我们需要在地图中，找一条从起点到终点的路径。这条路径要绕过地图中所有障碍物，并且看起来要是一种非常聪明的走法。所谓“聪明”，笼统地解释就是，走的路不能太绕。理论上讲，最短路径显然是最聪明的走法，是这个问题的最优解。
不过，在第44节最优出行路线规划问题中，我们也讲过，如果图非常大，那Dijkstra最短路径算法的执行耗时会很多。在真实的软件开发中，我们面对的是超级大的地图和海量的寻路请求，算法的执行效率太低，这显然是无法接受的。
实际上，像出行路线规划、游戏寻路，这些真实软件开发中的问题，一般情况下，我们都不需要非得求最优解（也就是最短路径）。在权衡路线规划质量和执行效率的情况下，我们只需要寻求一个次优解就足够了。那如何快速找出一条接近于最短路线的次优路线呢？
这个快速的路径规划算法，就是我们今天要学习的A*算法。实际上，A*算法是对Dijkstra算法的优化和改造。如何将Dijkstra算法改造成A*算法呢？为了更好地理解接下来要讲的内容，我建议你先温习下第44节中的Dijkstra算法的实现原理。
Dijkstra算法有点儿类似BFS算法，它每次找到跟起点最近的顶点，往外扩展。这种往外扩展的思路，其实有些盲目。为什么这么说呢？我举一个例子来给你解释一下。下面这个图对应一个真实的地图，每个顶点在地图中的位置，我们用一个二维坐标（x，y）来表示，其中，x表示横坐标，y表示纵坐标。
在Dijkstra算法的实现思路中，我们用一个优先级队列，来记录已经遍历到的顶点以及这个顶点与起点的路径长度。顶点与起点路径长度越小，就越先被从优先级队列中取出来扩展，从图中举的例子可以看出，尽管我们找的是从s到t的路线，但是最先被搜索到的顶点依次是1，2，3。通过肉眼来观察，这个搜索方向跟我们期望的路线方向（s到t是从西向东）是反着的，路线搜索的方向明显“跑偏”了。
之所以会“跑偏”，那是因为我们是按照顶点与起点的路径长度的大小，来安排出队列顺序的。与起点越近的顶点，就会越早出队列。我们并没有考虑到这个顶点到终点的距离，所以，在地图中，尽管1，2，3三个顶点离起始顶点最近，但离终点却越来越远。
如果我们综合更多的因素，把这个顶点到终点可能还要走多远，也考虑进去，综合来判断哪个顶点该先出队列，那是不是就可以避免“跑偏”呢？
当我们遍历到某个顶点的时候，从起点走到这个顶点的路径长度是确定的，我们记作g(i)（i表示顶点编号）。但是，从这个顶点到终点的路径长度，我们是未知的。虽然确切的值无法提前知道，但是我们可以用其他估计值来代替。
这里我们可以通过这个顶点跟终点之间的直线距离，也就是欧几里得距离，来近似地估计这个顶点跟终点的路径长度（注意：路径长度跟直线距离是两个概念）。我们把这个距离记作h(i)（i表示这个顶点的编号），专业的叫法是启发函数（heuristic function）。因为欧几里得距离的计算公式，会涉及比较耗时的开根号计算，所以，我们一般通过另外一个更加简单的距离计算公式，那就是曼哈顿距离（Manhattan distance）。曼哈顿距离是两点之间横纵坐标的距离之和。计算的过程只涉及加减法、符号位反转，所以比欧几里得距离更加高效。
int hManhattan(Vertex v1, Vertex v2) { // Vertex表示顶点，后面有定义 return Math.abs(v1.x - v2.x) + Math.abs(v1.y - v2.y); } 原来只是单纯地通过顶点与起点之间的路径长度g(i)，来判断谁先出队列，现在有了顶点到终点的路径长度估计值，我们通过两者之和f(i)=g(i)+h(i)，来判断哪个顶点该最先出队列。综合两部分，我们就能有效避免刚刚讲的“跑偏”。这里f(i)的专业叫法是估价函数（evaluation function）。
从刚刚的描述，我们可以发现，A*算法就是对Dijkstra算法的简单改造。实际上，代码实现方面，我们也只需要稍微改动几行代码，就能把Dijkstra算法的代码实现，改成A*算法的代码实现。
在A*算法的代码实现中，顶点Vertex类的定义，跟Dijkstra算法中的定义，稍微有点儿区别，多了x，y坐标，以及刚刚提到的f(i)值。图Graph类的定义跟Dijkstra算法中的定义一样。为了避免重复，我这里就没有再贴出来了。
private class Vertex { public int id; // 顶点编号ID public int dist; // 从起始顶点，到这个顶点的距离，也就是g(i) public int f; // 新增：f(i)=g(i)+h(i) public int x, y; // 新增：顶点在地图中的坐标（x, y） public Vertex(int id, int x, int y) { this.id = id; this.</description></item><item><title>49_数据完整性（上）：硬件坏了怎么办？</title><link>https://artisanbox.github.io/4/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/49/</guid><description>2012年的时候，我第一次在工作中，遇到一个因为硬件的不可靠性引发的Bug。正是因为这个Bug，让我开始逐步花很多的时间，去复习回顾整个计算机系统里面的底层知识。
当时，我正在MediaV带领一个20多人的团队，负责公司的广告数据和机器学习算法。其中有一部分工作，就是用Hadoop集群处理所有的数据和报表业务。当时我们的业务增长很快，所以会频繁地往Hadoop集群里面添置机器。2012年的时候，国内的云计算平台还不太成熟，所以我们都是自己采购硬件，放在托管的数据中心里面。
那个时候，我们的Hadoop集群服务器，在从100台服务器往1000台服务器走。我们觉得，像Dell这样品牌厂商的服务器太贵了，而且能够提供的硬件配置和我们的期望也有差异。于是，运维的同学开始和OEM厂商合作，自己定制服务器，批量采购硬盘、内存。
那个时候，大家都听过Google早期发展时，为了降低成本买了很多二手的硬件来降低成本，通过分布式的方式来保障系统的可靠性的办法。虽然我们还没有抠门到去买二手硬件，不过当时，我们选择购买了普通的机械硬盘，而不是企业级的、用在数据中心的机械硬盘；采购了普通的内存条，而不是带ECC纠错的服务器内存条，想着能省一点儿是一点儿。
单比特翻转：软件解决不了的硬件错误忽然有一天，我们最大的、每小时执行一次的数据处理报表应用，完成时间变得比平时晚了不少。一开始，我们并没有太在意，毕竟当时数据量每天都在增长，慢一点就慢一点了。但是，接着糟糕的事情开始发生了。
一方面，我们发现，报表任务有时候在一个小时之内执行不完，接着，偶尔整个报表任务会执行失败。于是，我们不得不停下手头开发的工作，开始排查这个问题。
用过Hadoop的话，你可能知道，作为一个分布式的应用，考虑到硬件的故障，Hadoop本身会在特定节点计算出错的情况下，重试整个计算过程。之前的报表跑得慢，就是因为有些节点的计算任务失败过，只是在重试之后又成功了。进一步分析，我们发现，程序的错误非常奇怪。有些数据计算的结果，比如“34+23”，结果应该是“57”，但是却变成了一个美元符号“$”。
前前后后折腾了一周，我们发现，从日志上看，大部分出错的任务都在几个固定的硬件节点上。
另一方面，我们发现，问题出现在我们新的一批自己定制的硬件上架之后。于是，和运维团队的同事沟通近期的硬件变更，并且翻阅大量Hadoop社区的邮件组列表之后，我们有了一个大胆的推测。
我们推测，这个错误，来自我们自己定制的硬件。定制的硬件没有使用ECC内存，在大量的数据中，内存中出现了单比特翻转（Single-Bit Flip）这个传说中的硬件错误。
那这个符号是怎么来的呢？是由于内存中的一个整数字符，遇到了一次单比特翻转转化而来的。 它的ASCII码二进制表示是0010 0100，所以它完全可能来自 0011 0100 遇到一次在第4个比特的单比特翻转，也就是从整数“4”变过来的。但是我们也只能推测是这个错误，而不能确信是这个错误。因为单比特翻转是一个随机现象，我们没法稳定复现这个问题。
ECC内存的全称是Error-Correcting Code memory，中文名字叫作纠错内存。顾名思义，就是在内存里面出现错误的时候，能够自己纠正过来。
在和运维同学沟通之后，我们把所有自己定制的服务器的内存替换成了ECC内存，之后这个问题就消失了。这也使得我们基本确信，问题的来源就是因为没有使用ECC内存。我们所有工程师的开发用机在2012年，也换成了32G内存。是的，换下来的内存没有别的去处，都安装到了研发团队的开发机上。
奇偶校验和校验位：捕捉错误的好办法其实，内存里面的单比特翻转或者错误，并不是一个特别罕见的现象。无论是因为内存的制造质量造成的漏电，还是外部的射线，都有一定的概率，会造成单比特错误。而内存层面的数据出错，软件工程师并不知道，而且这个出错很有可能是随机的。遇上随机出现难以重现的错误，大家肯定受不了。我们必须要有一个办法，避免这个问题。
其实，在ECC内存发明之前，工程师们已经开始通过奇偶校验的方式，来发现这些错误。
奇偶校验的思路很简单。我们把内存里面的N位比特当成是一组。常见的，比如8位就是一个字节。然后，用额外的一位去记录，这8个比特里面有奇数个1还是偶数个1。如果是奇数个1，那额外的一位就记录为1；如果是偶数个1，那额外的一位就记录成0。那额外的一位，我们就称之为校验码位。
如果在这个字节里面，我们不幸发生了单比特翻转，那么数据位计算得到的校验码，就和实际校验位里面的数据不一样。我们的内存就知道出错了。
除此之外，校验位有一个很大的优点，就是计算非常快，往往只需要遍历一遍需要校验的数据，通过一个O(N)的时间复杂度的算法，就能把校验结果计算出来。
校验码的思路，在很多地方都会用到。
比方说，我们下载一些软件的时候，你会看到，除了下载的包文件，还会有对应的MD5这样的哈希值或者循环冗余编码（CRC）的校验文件。这样，当我们把对应的软件下载下来之后，我们可以计算一下对应软件的校验码，和官方提供的校验码去做个比对，看看是不是一样。
如果不一样，你就不能轻易去安装这个软件了。因为有可能，这个软件包是坏的。但是，还有一种更危险的情况，就是你下载的这个软件包，可能是被人植入了后门的。安装上了之后，你的计算机的安全性就没有保障了。
不过，使用奇偶校验，还是有两个比较大的缺陷。
第一个缺陷，就是奇偶校验只能解决遇到单个位的错误，或者说奇数个位的错误。如果出现2个位进行了翻转，那么这个字节的校验位计算结果其实没有变，我们的校验位自然也就不能发现这个错误。
第二个缺陷，是它只能发现错误，但是不能纠正错误。所以，即使在内存里面发现数据错误了，我们也只能中止程序，而不能让程序继续正常地运行下去。如果这个只是我们的个人电脑，做一些无关紧要的应用，这倒是无所谓了。
但是，你想一下，如果你在服务器上进行某个复杂的计算任务，这个计算已经跑了一周乃至一个月了，还有两三天就跑完了。这个时候，出现内存里面的错误，要再从头跑起，估计你内心是崩溃的。
所以，我们需要一个比简单的校验码更好的解决方案，一个能够发现更多位的错误，并且能够把这些错误纠正过来的解决方案，也就是工程师们发明的ECC内存所使用的解决方案。
我们不仅能捕捉到错误，还要能够纠正发生的错误。这个策略，我们通常叫作纠错码（Error Correcting Code）。它还有一个升级版本，叫作纠删码（Erasure Code），不仅能够纠正错误，还能够在错误不能纠正的时候，直接把数据删除。无论是我们的ECC内存，还是网络传输，乃至硬盘的RAID，其实都利用了纠错码和纠删码的相关技术。
想要看看我们怎么通过算法，怎么配置硬件，使得我们不仅能够发现单个位的错误，而能发现更多位的错误，你一定要记得跟上下一讲的内容。
总结延伸好了，让我们一起来总结一下今天的内容。
我给你介绍了我自己亲身经历的一个硬件错误带来的Bug。由于没有采用ECC内存，导致我们的数据处理中，出现了大量的单比特数据翻转的错误。这些硬件带来的错误，其实我们没有办法在软件层面解决。
如果对于硬件以及硬件本身的原理不够熟悉，恐怕这个问题的解决方案还是遥遥无期。如果你对计算机组成原理有所了解，并能够意识到，在硬件的存储层有着数据验证和纠错的需求，那你就能在有限的时间内定位到问题所在。
进一步地，我为你简单介绍了奇偶校验，也就是如何通过冗余的一位数据，发现在硬件层面出现的位错误。但是，奇偶校验以及其他的校验码，只能发现错误，没有办法纠正错误。所以，下一讲，我们一起来看看，怎么利用纠错码这样的方式，来解决问题。
推荐阅读我推荐你去深入阅读一下Wikipedia里面，关于CRC的内容，了解一下，这样的校验码的详细算法。
课后思考有人说，奇偶校验只是循环冗余编码（CRC）的一种特殊情况。在读完推荐阅读里面的CRC算法的实现之后，你能分析一下为什么奇偶校验只是CRC的一种特殊情况吗？
欢迎把你阅读和分析的内容写在留言区，和大家一起分享。如果觉得有帮助，你也可以把今天的内容分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>50_数据完整性（下）：如何还原犯罪现场？</title><link>https://artisanbox.github.io/4/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/50/</guid><description>讲完校验码之后，你现在应该知道，无论是奇偶校验码，还是CRC这样的循环校验码，都只能告诉我们一个事情，就是你的数据出错了。所以，校验码也被称为检错码（Error Detecting Code）。
不管是校验码，还是检错码，在硬件出错的时候，只能告诉你“我错了”。但是，下一个问题，“错哪儿了”，它是回答不了的。这就导致，我们的处理方式只有一种，那就是当成“哪儿都错了”。如果是下载一个文件，发现校验码不匹配，我们只能重新去下载；如果是程序计算后放到内存里面的数据，我们只能再重新算一遍。
这样的效率实在是太低了，所以我们需要有一个办法，不仅告诉我们“我错了”，还能告诉我们“错哪儿了”。于是，计算机科学家们就发明了纠错码。纠错码需要更多的冗余信息，通过这些冗余信息，我们不仅可以知道哪里的数据错了，还能直接把数据给改对。这个是不是听起来很神奇？接下来就让我们一起来看一看。
海明码：我们需要多少信息冗余？最知名的纠错码就是海明码。海明码（Hamming Code）是以他的发明人Richard Hamming（理查德·海明）的名字命名的。这个编码方式早在上世纪四十年代就被发明出来了。而直到今天，我们上一讲所说到的ECC内存，也还在使用海明码来纠错。
最基础的海明码叫7-4海明码。这里的“7”指的是实际有效的数据，一共是7位（Bit）。而这里的“4”，指的是我们额外存储了4位数据，用来纠错。
首先，你要明白一点，纠错码的纠错能力是有限的。不是说不管错了多少位，我们都能给纠正过来。不然我们就不需要那7个数据位，只需要那4个校验位就好了，这意味着我们可以不用数据位就能传输信息了。这就不科学了。事实上，在7-4海明码里面，我们只能纠正某1位的错误。这是怎么做到的呢？我们一起来看看。
4位的校验码，一共可以表示 2^4 = 16 个不同的数。根据数据位计算出来的校验值，一定是确定的。所以，如果数据位出错了，计算出来的校验码，一定和确定的那个校验码不同。那可能的值，就是在 2^4 - 1 = 15 那剩下的15个可能的校验值当中。
15个可能的校验值，其实可以对应15个可能出错的位。这个时候你可能就会问了，既然我们的数据位只有7位，那为什么我们要用4位的校验码呢？用3位不就够了吗？2^3 - 1 = 7，正好能够对上7个不同的数据位啊！
你别忘了，单比特翻转的错误，不仅可能出现在数据位，也有可能出现在校验位。校验位本身也是可能出错的。所以，7位数据位和3位校验位，如果只有单比特出错，可能出错的位数就是10位，2^3 - 1 = 7 种情况是不能帮我们找到具体是哪一位出错的。
事实上，如果我们的数据位有K位，校验位有N位。那么我们需要满足下面这个不等式，才能确保我们能够对单比特翻转的数据纠错。这个不等式就是：
K + N + 1 &amp;lt;= 2^N在有7位数据位，也就是K=7的情况下，N的最小值就是4。4位校验位，其实最多可以支持到11位数据位。我在下面列了一个简单的数据位数和校验位数的对照表，你可以自己算一算，理解一下上面的公式。
海明码的纠错原理现在你应该搞清楚了，在数据位数确定的情况下，怎么计算需要的校验位。那接下来，我们就一起看看海明码的编码方式是怎么样的。
为了算起来简单一点，我们少用一些位数，来算一个4-3海明码（也就是4位数据位，3位校验位）。我们把4位数据位，分别记作d1、d2、d3、d4。这里的d，取的是数据位data bits的首字母。我们把3位校验位，分别记作p1、p2、p3。这里的p，取的是校验位parity bits的首字母。
从4位的数据位里面，我们拿走1位，然后计算出一个对应的校验位。这个校验位的计算用之前讲过的奇偶校验就可以了。比如，我们用d1、d2、d4来计算出一个校验位p1；用d1、d3、d4计算出一个校验位p2；用d2、d3、d4计算出一个校验位p3。就像下面这个对应的表格一样：
这个时候，你去想一想，如果d1这一位的数据出错了，会发生什么情况？我们会发现，p1和p2和校验的计算结果不一样。d2出错了，是因为p1和p3的校验的计算结果不一样；d3出错了，则是因为p2和p3；如果d4出错了，则是p1、p2、p3都不一样。你会发现，当数据码出错的时候，至少会有2位校验码的计算是不一致的。
那我们倒过来，如果是p1的校验码出错了，会发生什么情况呢？这个时候，只有p1的校验结果出错。p2和p3的出错的结果也是一样的，只有一个校验码的计算是不一致的。
所以校验码不一致，一共有 2^3-1=7种情况，正好对应了7个不同的位数的错误。我把这个对应表格也放在下面了，你可以理解一下。
可以看到，海明码这样的纠错过程，有点儿像电影里面看到的推理探案的过程。通过出错现场的额外信息，一步一步条分缕析地找出，到底是哪一位的数据出错，还原出错时候的“犯罪现场”。
看到这里，相信你一方面会觉得海明码特别神奇，但是同时也会冒出一个新的疑问，我们怎么才能用一套程序或者规则来生成海明码呢？其实这个步骤并不复杂，接下来我们就一起来看一下。
首先，我们先确定编码后，要传输的数据是多少位。比如说，我们这里的7-4海明码，就是一共11位。
然后，我们给这11位数据从左到右进行编号，并且也把它们的二进制表示写出来。
接着，我们先把这11个数据中的二进制的整数次幂找出来。在这个7-4海明码里面，就是1、2、4、8。这些数，就是我们的校验码位，我们把他们记录做p1～p4。如果从二进制的角度看，它们是这11个数当中，唯四的，在4个比特里面只有一个比特是1的数值。
那么剩下的7个数，就是我们d1-d7的数据码位了。
然后，对于我们的校验码位，我们还是用奇偶校验码。但是每一个校验码位，不是用所有的7位数据来计算校验码。而是p1用3、5、7、9、11来计算。也就是，在二进制表示下，从右往左数的第一位比特是1的情况下，用p1作为校验码。
剩下的p2，我们用3、6、10、11来计算校验码，也就是在二进制表示下，从右往左数的第二位比特是1的情况下，用p2。那么，p3自然是从右往左数，第三位比特是1的情况下的数字校验码。而p4则是第四位比特是1的情况下的校验码。
这个时候，你会发现，任何一个数据码出错了，就至少会有对应的两个或者三个校验码对不上，这样我们就能反过来找到是哪一个数据码出错了。如果校验码出错了，那么只有校验码这一位对不上，我们就知道是这个校验码出错了。
上面这个方法，我们可以用一段确定的程序表示出来，意味着无论是几位的海明码，我们都不再需要人工去精巧地设计编码方案了。
海明距离：形象理解海明码的作用其实，我们还可以换一个角度来理解海明码的作用。对于两个二进制表示的数据，他们之间有差异的位数，我们称之为海明距离。比如 1001 和 0001 的海明距离是1，因为他们只有最左侧的第一位是不同的。而1001 和 0000 的海明距离是2，因为他们最左侧和最右侧有两位是不同的。
于是，你很容易可以想到，所谓的进行一位纠错，也就是所有和我们要传输的数据的海明距离为1的数，都能被纠正回来。
而任何两个实际我们想要传输的数据，海明距离都至少要是3。你可能会问了，为什么不能是2呢？因为如果是2的话，那么就会有一个出错的数，到两个正确的数据的海明距离都是1。当我们看到这个出错的数的时候，我们就不知道究竟应该纠正到那一个数了。</description></item><item><title>50_索引：如何在海量数据中快速查找某个数据？</title><link>https://artisanbox.github.io/2/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/51/</guid><description>在第48节中，我们讲了MySQL数据库索引的实现原理。MySQL底层依赖的是B+树这种数据结构。留言里有同学问我，那类似Redis这样的Key-Value数据库中的索引，又是怎么实现的呢？底层依赖的又是什么数据结构呢？
今天，我就来讲一下索引这种常用的技术解决思路，底层往往会依赖哪些数据结构。同时，通过索引这个应用场景，我也带你回顾一下，之前我们学过的几种支持动态集合的数据结构。
为什么需要索引？在实际的软件开发中，业务纷繁复杂，功能千变万化，但是，万变不离其宗。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为“对数据的存储和计算”。对应到数据结构和算法中，那“存储”需要的就是数据结构，“计算”需要的就是算法。
对于存储的需求，功能上无外乎增删改查。这其实并不复杂。但是，一旦存储的数据很多，那性能就成了这些系统要关注的重点，特别是在一些跟存储相关的基础系统（比如MySQL数据库、分布式文件系统等）、中间件（比如消息中间件RocketMQ等）中。
“如何节省存储空间、如何提高数据增删改查的执行效率”，这样的问题就成了设计的重点。而这些系统的实现，都离不开一个东西，那就是索引。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。
索引这个概念，非常好理解。你可以类比书籍的目录来理解。如果没有目录，我们想要查找某个知识点的时候，就要一页一页翻。通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。
索引的需求定义索引的概念不难理解，我想你应该已经搞明白。接下来，我们就分析一下，在设计索引的过程中，需要考虑到的一些因素，换句话说就是，我们该如何定义清楚需求呢？
对于系统设计需求，我们一般可以从功能性需求和非功能性需求两方面来分析，这个我们之前也说过。因此，这个问题也不例外。
1.功能性需求对于功能性需求需要考虑的点，我把它们大致概括成下面这几点。
数据是格式化数据还是非格式化数据？要构建索引的原始数据，类型有很多。我把它分为两类，一类是结构化数据，比如，MySQL中的数据；另一类是非结构化数据，比如搜索引擎中网页。对于非结构化数据，我们一般需要做预处理，提取出查询关键词，对关键词构建索引。
数据是静态数据还是动态数据？如果原始数据是一组静态数据，也就是说，不会有数据的增加、删除、更新操作，所以，我们在构建索引的时候，只需要考虑查询效率就可以了。这样，索引的构建就相对简单些。不过，大部分情况下，我们都是对动态数据构建索引，也就是说，我们不仅要考虑到索引的查询效率，在原始数据更新的同时，我们还需要动态地更新索引。支持动态数据集合的索引，设计起来相对也要更加复杂些。
索引存储在内存还是硬盘？如果索引存储在内存中，那查询的速度肯定要比存储在磁盘中的高。但是，如果原始数据量很大的情况下，对应的索引可能也会很大。这个时候，因为内存有限，我们可能就不得不将索引存储在磁盘中了。实际上，还有第三种情况，那就是一部分存储在内存，一部分存储在磁盘，这样就可以兼顾内存消耗和查询效率。
单值查找还是区间查找？所谓单值查找，也就是根据查询关键词等于某个值的数据。这种查询需求最常见。所谓区间查找，就是查找关键词处于某个区间值的所有数据。你可以类比MySQL数据库的查询需求，自己想象一下。实际上，不同的应用场景，查询的需求会多种多样。
单关键词查找还是多关键词组合查找？比如，搜索引擎中构建的索引，既要支持一个关键词的查找，比如“数据结构”，也要支持组合关键词查找，比如“数据结构 AND 算法”。对于单关键词的查找，索引构建起来相对简单些。对于多关键词查询来说，要分多种情况。像MySQL这种结构化数据的查询需求，我们可以实现针对多个关键词的组合，建立索引；对于像搜索引擎这样的非结构数据的查询需求，我们可以针对单个关键词构建索引，然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。
实际上，不同的场景，不同的原始数据，对于索引的需求也会千差万别。我这里只列举了一些比较有共性的需求。
2.非功能性需求讲完了功能性需求，我们再来看，索引设计的非功能性需求。
不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非常有限，一个中间件启动后就占用几个GB的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会超过原始数据。
在考虑索引查询效率的同时，我们还要考虑索引的维护成本。索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，我们还要考虑到，索引的维护成本。因为在原始数据动态增删改的同时，我们也需要动态地更新索引。而索引的更新势必会影响到增删改操作的性能。
构建索引常用的数据结构有哪些？我刚刚从很宏观的角度，总结了在索引设计的过程中，需要考虑的一些共性因素。现在，我们就来看，对于不同需求的索引结构，底层一般使用哪种数据结构。
实际上，常用来构建索引的数据结构，就是我们之前讲过的几种支持动态数据集合的数据结构。比如，散列表、红黑树、跳表、B+树。除此之外，位图、布隆过滤器可以作为辅助索引，有序数组可以用来对静态数据构建索引。
我们知道，散列表增删改查操作的性能非常好，时间复杂度是O(1)。一些键值数据库，比如Redis、Memcache，就是使用散列表来构建索引的。这类索引，一般都构建在内存中。
红黑树作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是O(logn)，也非常适合用来构建内存索引。Ext文件系统中，对磁盘块的索引，用的就是红黑树。
B+树比起红黑树来说，更加适合构建存储在磁盘中的索引。B+树是一个多叉树，所以，对相同个数的数据构建索引，B+树的高度要低于红黑树。当借助索引查询数据的时候，读取B+树索引，需要的磁盘IO次数会更少。所以，大部分关系型数据库的索引，比如MySQL、Oracle，都是用B+树来实现的。
跳表也支持快速添加、删除、查找数据。而且，我们通过灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率。Redis中的有序集合，就是用跳表来构建的。
除了散列表、红黑树、B+树、跳表之外，位图和布隆过滤器这两个数据结构，也可以用于索引中，辅助存储在磁盘中的索引，加速数据查找的效率。我们来看下，具体是怎么做的？
我们知道，布隆过滤器有一定的判错率。但是，我们可以规避它的短处，发挥它的长处。尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在。而且，布隆过滤器还有一个更大的特点，那就是内存占用非常少。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。
实际上，有序数组也可以被作为索引。如果数据是静态的，也就是不会有插入、删除、更新操作，那我们可以把数据的关键词（查询用的）抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。
总结引申今天这节算是一节总结课。我从索引这个非常常用的技术方案，给你展示了散列表、红黑树、跳表、位图、布隆过滤器、有序数组这些数据结构的应用场景。学习完这节课之后，不知道你对这些数据结构以及索引，有没有更加清晰的认识呢？
从这一节内容中，你应该可以看出，架构设计离不开数据结构和算法。要想成长为一个优秀的业务架构师、基础架构师，数据结构和算法的根基一定要打稳。因为，那些看似很惊艳的架构设计思路，实际上，都是来自最常用的数据结构和算法。
课后思考你知道基础系统、中间件、开源软件等系统中，有哪些用到了索引吗？这些系统的索引是如何实现的呢？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>51_分布式计算：如果所有人的大脑都联网会怎样？</title><link>https://artisanbox.github.io/4/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/51/</guid><description>今天是原理篇的最后一篇。过去50讲，我们一起看了抽象概念上的计算机指令，看了这些指令怎么拆解成一个个简单的电路，以及CPU是怎么通过一个一个的电路组成的。我们还一起看了高速缓存、内存、SSD硬盘和机械硬盘，以及这些组件又是怎么通过总线和CPU连在一起相互通信的。
把计算机这一系列组件组合起来，我们就拿到了一台完整的计算机。现在我们每天在用的个人PC、智能手机，乃至云上的服务器，都是这样一台计算机。
但是，一台计算机在数据中心里是不够的。因为如果只有一台计算机，我们会遇到三个核心问题。第一个核心问题，叫作垂直扩展和水平扩展的选择问题，第二问题叫作如何保持高可用性（High Availability），第三个问题叫作一致性问题（Consistency）。
围绕这三个问题，其实就是我们今天要讲的主题，分布式计算。当然，短短的一讲肯定讲不完这么大一个主题。分布式计算拿出来单开一门专栏也绰绰有余。我们今天这一讲的目标，是让你能理解水平扩展、高可用性这两个核心问题。对于分布式系统带来的一致性问题，我们会留在我们的实战篇里面，再用案例来为大家分析。
从硬件升级到水平扩展从技术开发的角度来讲，想要在2019年创业真的很幸福。只要在AWS或者阿里云这样的云服务上注册一个账号，一个月花上一两百块钱，你就可以有一台在数据中心里面的服务器了。而且这台服务器，可以直接提供给世界各国人民访问。如果你想要做海外市场，你可以把这个服务器放在美国、欧洲、东南亚，任何一个你想要去的市场的数据中心里，然后把自己的网站部署在这台服务器里面就可以了。
现在在云服务商购买服务器的成本和方便程度都已经很高了当然，这台服务器就是我们在第34讲里说的虚拟机。不过因为只是个业余时间的小项目，一开始这台服务器的配置也不会太高。我以我现在公司所用的Google Cloud为例。最低的配置差不多是1个CPU核心、3.75G内存以及一块10G的SSD系统盘。这样一台服务器每个月的价格差不多是28美元。
幸运的是，你的网站很受大家欢迎，访问量也上来了。这个时候，这台单核心的服务器的性能有点不够用了。这个时候，你需要升级你的服务器。于是，你就会面临两个选择。
第一个选择是升级现在这台服务器的硬件，变成2个CPU核心、7.5G内存。这样的选择我们称之为垂直扩展（Scale Up）。第二个选择则是我们再租用一台和之前一样的服务器。于是，我们有了2台1个CPU核心、3.75G内存的服务器。这样的选择我们称之为水平扩展（Scale Out）。
在这个阶段，这两个选择，从成本上看起来没有什么差异。2核心、7.5G内存的服务器，成本是56.61美元，而2台1核心、3.75G内存的服务器价格，成本是57美元，这之间的价格差异不到1%。
不过，垂直扩展和水平扩展看似是两个不同的选择，但是随着流量不断增长。到最后，只会变成一个选择。那就是既会垂直扩展，又会水平扩展，并且最终依靠水平扩展，来支撑Google、Facebook、阿里、腾讯这样体量的互联网服务。
垂直扩展背后的逻辑和优势都很简单。一般来说，垂直扩展通常不需要我们去改造程序，也就是说，我们没有研发成本。那为什么我们最终还是要用水平扩展呢？你可以先自己想一想。
原因其实很简单，因为我们没有办法不停地去做垂直扩展。我们在Google Cloud上现在能够买到的性能最好的服务器，是96个CPU核心、1.4TB的内存。如果我们的访问量逐渐增大，一台96核心的服务器也支撑不了了，那么我们就没有办法再去做垂直扩展了。这个时候，我们就不得不采用水平扩展的方案了。
96个CPU核心看起来是个很强大的服务器，但是你算一算就知道，其实它的计算资源并没有多大。你现在多半在用一台4核心，或者至少也是2核心的CPU。96个CPU也就是30～50台日常使用的开发机的计算性能。而我们今天在互联网上遇到的问题，是每天数亿的访问量，靠30～50台个人电脑的计算能力想要支撑这样的计算需求，可谓是天方夜谭了。
然而，一旦开始采用水平扩展，我们就会面临在软件层面改造的问题了。也就是我们需要开始进行分布式计算了。我们需要引入负载均衡（Load Balancer）这样的组件，来进行流量分配。我们需要拆分应用服务器和数据库服务器，来进行垂直功能的切分。我们也需要不同的应用之间通过消息队列，来进行异步任务的执行。
所有这些软件层面的改造，其实都是在做分布式计算的一个核心工作，就是通过消息传递（Message Passing）而不是共享内存（Shared Memory）的方式，让多台不同的计算机协作起来共同完成任务。
而因为我们最终必然要进行水平扩展，我们需要在系统设计的早期就基于消息传递而非共享内存来设计系统。即使这些消息只是在同一台服务器上进行传递。
事实上，有不少增长迅猛的公司，早期没有准备好通过水平扩展来支撑访问量的情况，而一味通过提升硬件配置Scale Up，来支撑更大的访问量，最终影响了公司的存亡。最典型的例子，就是败在Facebook手下的MySpace。
理解高可用性和单点故障尽管在1个CPU核心的服务器支撑不了我们的访问量的时候，选择垂直扩展是一个最简单的办法。不过如果是我的话，第一次扩展我会选择水平扩展。
选择水平扩展的一个很好的理由，自然是可以“强迫”从开发的角度，尽早地让系统能够支持水平扩展，避免在真的流量快速增长的时候，垂直扩展的解决方案跟不上趟。不过，其实还有一个更重要的理由，那就是系统的可用性问题。
上面的1核变2核的垂直扩展的方式，扩展完之后，我们还是只有1台服务器。如果这台服务器出现了一点硬件故障，比如，CPU坏了，那我们的整个系统就坏了，就不可用了。
如果采用了水平扩展，即便有一台服务器的CPU坏了，我们还有另外一台服务器仍然能够提供服务。负载均衡能够通过健康检测（Health Check）发现坏掉的服务器没有响应了，就可以自动把所有的流量切换到第2台服务器上，这个操作就叫作故障转移（Failover），我们的系统仍然是可用的。
系统的可用性（Avaiability）指的就是，我们的系统可以正常服务的时间占比。无论是因为软硬件故障，还是需要对系统进行停机升级，都会让我们损失系统的可用性。可用性通常是用一个百分比的数字来表示，比如99.99%。我们说，系统每个月的可用性要保障在99.99%，也就是意味着一个月里，你的服务宕机的时间不能超过4.32分钟。
有些系统可用性的损失，是在我们计划内的。比如上面说的停机升级，这个就是所谓的计划内停机时间（Scheduled Downtime）。有些系统可用性的损失，是在我们计划外的，比如一台服务器的硬盘忽然坏了，这个就是所谓的计划外停机时间（Unscheduled Downtime）。
我们的系统是一定不可能做到100%可用的，特别是计划外的停机时间。从简单的硬件损坏，到机房停电、光缆被挖断，乃至于各种自然灾害，比如地震、洪水、海啸，都有可能使得我们的系统不可用。作为一个工程师和架构师，我们要做的就是尽可能低成本地提高系统的可用性。
咱们的专栏是要讲计算机组成原理，那我们先来看一看硬件服务器的可用性。
现在的服务器的可用性都已经很不错了，通常都能保障99.99%的可用性了。如果我们有一个小小的三台服务器组成的小系统，一台部署了Nginx来作为负载均衡和反向代理，一台跑了PHP-FPM作为Web应用服务器，一台用来作为MySQL数据库服务器。每台服务器的可用性都是99.99%。那么我们整个系统的可用性是多少呢？你可以先想一想。
答案是99.99% × 99.99% × 99.99% = 99.97%。在这个系统当中，这个数字看起来似乎没有那么大区别。不过反过来看，我们是从损失了0.01%的可用性，变成了损失0.03%的可用性，不可用的时间变成了原来的3倍。
如果我们有1000台服务器，那么整个的可用性，就会变成 99.99% ^ 1000 = 90.5%。也就是说，我们的服务一年里有超过一个月是不可用的。这可怎么办呀？
我们先来分析一下原因。之所以会出现这个问题，是因为在这个场景下，任何一台服务器出错了，整个系统就没法用了。这个问题就叫作单点故障问题（Single Point of Failure，SPOF）。我们这里的这个假设特别糟糕。我们假设这1000台服务器，每一个都存在单点故障问题。所以，我们的服务也就特别脆弱，随便哪台出现点风吹草动，整个服务就挂了。
要解决单点故障问题，第一点就是要移除单点。其实移除单点最典型的场景，在我们水平扩展应用服务器的时候就已经看到了，那就是让两台服务器提供相同的功能，然后通过负载均衡把流量分发到两台不同的服务器去。即使一台服务器挂了，还有一台服务器可以正常提供服务。
不过光用两台服务器是不够的，单点故障其实在数据中心里面无处不在。我们现在用的是云上的两台虚拟机。如果这两台虚拟机是托管在同一台物理机上的，那这台物理机本身又成为了一个单点。那我们就需要把这两台虚拟机分到两台不同的物理机上。
不过这个还是不够。如果这两台物理机在同一个机架（Rack）上，那机架上的交换机（Switch）就成了一个单点。即使放到不同的机架上，还是有可能出现整个数据中心遭遇意外故障的情况。
去年我自己就遇到过，部署在Azure上的服务所在的数据中心，因为散热问题触发了整个数据中心所有服务器被关闭的问题。面对这种情况，我们就需要设计进行异地多活的系统设计和部署。所以，在现代的云服务，你在买服务器的时候可以选择服务器的area（地区）和zone（区域），而要不要把服务器放在不同的地区或者区域里，也是避免单点故障的一个重要因素。
只是能够去除单点，其实我们的可用性问题还没有解决。比如，上面我们用负载均衡把流量均匀地分发到2台服务器上，当一台应用服务器挂掉的时候，我们的确还有一台服务器在提供服务。但是负载均衡会把一半的流量发到已经挂掉的服务器上，所以这个时候只能算作一半可用。
想要让整个服务完全可用，我们就需要有一套故障转移（Failover）机制。想要进行故障转移，就首先要能发现故障。
以我们这里的PHP-FPM的Web应用为例，负载均衡通常会定时去请求一个Web应用提供的健康检测（Health Check）的地址。这个时间间隔可能是5秒钟，如果连续2～3次发现健康检测失败，负载均衡就会自动将这台服务器的流量切换到其他服务器上。于是，我们就自动地产生了一次故障转移。故障转移的自动化在大型系统里是很重要的，因为服务器越多，出现故障基本就是个必然发生的事情。而自动化的故障转移既能够减少运维的人手需求，也能够缩短从故障发现到问题解决的时间周期，提高可用性。
我们在Web应用上设置了一个Heartbeat接口，每20秒检查一次，出现问题的时候可以进行故障转移切换那么，让我们算一算，通过水平扩展相同功能的服务器来去掉单点故障，并且通过健康检查机制来触发自动的故障转移，这样的可用性会变成多少呢？你可以拿出纸和笔来试一下。
不知道你想明白应该怎么算了没有，在这种情况下，我们其实只要有任何一台服务器能够正常运转，就能正常提供服务。那么，我们的可用性就是：
100% - (100% - 99.</description></item><item><title>51_并行算法：如何利用并行处理提高算法的执行效率？</title><link>https://artisanbox.github.io/2/52/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/52/</guid><description>时间复杂度是衡量算法执行效率的一种标准。但是，时间复杂度并不能跟性能划等号。在真实的软件开发中，即便在不降低时间复杂度的情况下，也可以通过一些优化手段，提升代码的执行效率。毕竟，对于实际的软件开发来说，即便是像10%、20%这样微小的性能提升，也是非常可观的。
算法的目的就是为了提高代码执行的效率。那当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢？我们今天就讲一种非常简单但又非常好用的优化方法，那就是并行计算。今天，我就通过几个例子，给你展示一下，如何借助并行计算的处理思想对算法进行改造？
并行排序假设我们要给大小为8GB的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据。对于排序来说，最常用的就是时间复杂度为O(nlogn)的三种排序算法，归并排序、快速排序、堆排序。从理论上讲，这个排序问题，已经很难再从算法层面优化了。而利用并行的处理思想，我们可以很轻松地将这个给8GB数据排序问题的执行效率提高很多倍。具体的实现思路有下面两种。
第一种是对归并排序并行化处理。我们可以将这8GB的数据划分成16个小的数据集合，每个集合包含500MB的数据。我们用16个线程，并行地对这16个500MB的数据集合进行排序。这16个小集合分别排序完成之后，我们再将这16个有序集合合并。
第二种是对快速排序并行化处理。我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成16个小区间。我们将8GB的数据划分到对应的区间中。针对这16个小区间的数据，我们启动16个线程，并行地进行排序。等到16个线程都执行结束之后，得到的数据就是有序数据了。
对比这两种处理思路，它们利用的都是分治的思想，对数据进行分片，然后并行处理。它们的区别在于，第一种处理思路是，先随意地对数据分片，排序之后再合并。第二种处理思路是，先对数据按照大小划分区间，然后再排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。
这里我还要多说几句，如果要排序的数据规模不是8GB，而是1TB，那问题的重点就不是算法的执行效率了，而是数据的读取效率。因为1TB的数据肯定是存在硬盘中，无法一次性读取到内存中，这样在排序的过程中，就会有频繁地磁盘数据的读取和写入。如何减少磁盘的IO操作，减少磁盘数据读取和写入的总量，就变成了优化的重点。不过这个不是我们这节要讨论的重点，你可以自己思考下。
并行查找我们知道，散列表是一种非常适合快速查找的数据结构。
如果我们是给动态数据构建索引，在数据不断加入的时候，散列表的装载因子就会越来越大。为了保证散列表性能不下降，我们就需要对散列表进行动态扩容。对如此大的散列表进行动态扩容，一方面比较耗时，另一方面比较消耗内存。比如，我们给一个2GB大小的散列表进行扩容，扩展到原来的1.5倍，也就是3GB大小。这个时候，实际存储在散列表中的数据只有不到2GB，所以内存的利用率只有60%，有1GB的内存是空闲的。
实际上，我们可以将数据随机分割成k份（比如16份），每份中的数据只有原来的1/k，然后我们针对这k个小数据集合分别构建散列表。这样，散列表的维护成本就变低了。当某个小散列表的装载因子过大的时候，我们可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。
还是刚才那个例子，假设现在有2GB的数据，我们放到16个散列表中，每个散列表中的数据大约是150MB。当某个散列表需要扩容的时候，我们只需要额外增加150*0.5=75MB的内存（假设还是扩容到原来的1.5倍）。无论从扩容的执行效率还是内存的利用率上，这种多个小散列表的处理方法，都要比大散列表高效。
当我们要查找某个数据的时候，我们只需要通过16个线程，并行地在这16个散列表中查找数据。这样的查找性能，比起一个大散列表的做法，也并不会下降，反倒有可能提高。
当往散列表中添加数据的时候，我们可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。
并行字符串匹配我们前面学过，在文本中查找某个关键词这样一个功能，可以通过字符串匹配算法来实现。我们之前学过的字符串匹配算法有KMP、BM、RK、BF等。当在一个不是很长的文本中查找关键词的时候，这些字符串匹配算法中的任何一个，都可以表现得非常高效。但是，如果我们处理的是超级大的文本，那处理的时间可能就会变得很长，那有没有办法加快匹配速度呢？
我们可以把大的文本，分割成k个小文本。假设k是16，我们就启动16个线程，并行地在这16个小文本中查找关键词，这样整个查找的性能就提高了16倍。16倍效率的提升，从理论的角度来说并不多。但是，对于真实的软件开发来说，这显然是一个非常可观的优化。
不过，这里还有一个细节要处理，那就是原本包含在大文本中的关键词，被一分为二，分割到两个小文本中，这就会导致尽管大文本中包含这个关键词，但在这16个小文本中查找不到它。实际上，这个问题也不难解决，我们只需要针对这种特殊情况，做一些特殊处理就可以了。
我们假设关键词的长度是m。我们在每个小文本的结尾和开始各取m个字符串。前一个小文本的末尾m个字符和后一个小文本的开头m个字符，组成一个长度是2m的字符串。我们再拿关键词，在这个长度为2m的字符串中再重新查找一遍，就可以补上刚才的漏洞了。
并行搜索前面我们学习过好几种搜索算法，它们分别是广度优先搜索、深度优先搜索、Dijkstra最短路径算法、A*启发式搜索算法。对于广度优先搜索算法，我们也可以将其改造成并行算法。
广度优先搜索是一种逐层搜索的搜索策略。基于当前这一层顶点，我们可以启动多个线程，并行地搜索下一层的顶点。在代码实现方面，原来广度优先搜索的代码实现，是通过一个队列来记录已经遍历到但还没有扩展的顶点。现在，经过改造之后的并行广度优先搜索算法，我们需要利用两个队列来完成扩展顶点的工作。
假设这两个队列分别是队列A和队列B。多线程并行处理队列A中的顶点，并将扩展得到的顶点存储在队列B中。等队列A中的顶点都扩展完成之后，队列A被清空，我们再并行地扩展队列B中的顶点，并将扩展出来的顶点存储在队列A。这样两个队列循环使用，就可以实现并行广度优先搜索算法。
总结引申上一节，我们通过实际软件开发中的“索引”这一技术点，回顾了之前学过的一些支持动态数据集合的数据结构。今天，我们又通过“并行算法”这个话题，回顾了之前学过的一些算法。
今天的内容比较简单，没有太复杂的知识点。我通过一些例子，比如并行排序、查找、搜索、字符串匹配，给你展示了并行处理的实现思路，也就是对数据进行分片，对没有依赖关系的任务，并行地执行。
并行计算是一个工程上的实现思路，尽管跟算法关系不大，但是，在实际的软件开发中，它确实可以非常巧妙地提高程序的运行效率，是一种非常好用的性能优化手段。
特别是，当要处理的数据规模达到一定程度之后，我们无法通过继续优化算法，来提高执行效率 的时候，我们就需要在实现的思路上做文章，利用更多的硬件资源，来加快执行的效率。所以，在很多超大规模数据处理中，并行处理的思想，应用非常广泛，比如MapReduce实际上就是一种并行计算框架。
课后思考假设我们有n个任务，为了提高执行的效率，我们希望能并行执行任务，但是各个任务之间又有一定的依赖关系，如何根据依赖关系找出可以并行执行的任务？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>52_算法实战（一）：剖析Redis常用数据类型对应的数据结构</title><link>https://artisanbox.github.io/2/53/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/53/</guid><description>到此为止，专栏前三部分我们全部讲完了。从今天开始，我们就正式进入实战篇的部分。这部分我主要通过一些开源项目、经典系统，真枪实弹地教你，如何将数据结构和算法应用到项目中。所以这部分的内容，更多的是知识点的回顾，相对于基础篇、高级篇的内容，其实这部分会更加容易看懂。
不过，我希望你不要只是看懂就完了。你要多举一反三地思考，自己接触过的开源项目、基础框架、中间件中，都用过哪些数据结构和算法。你也可以想一想，在自己做的项目中，有哪些可以用学过的数据结构和算法进一步优化。这样的学习效果才会更好。
好了，今天我就带你一块儿看下，经典数据库Redis中的常用数据类型，底层都是用哪种数据结构实现的？
Redis数据库介绍Redis是一种键值（Key-Value）数据库。相对于关系型数据库（比如MySQL），Redis也被叫作非关系型数据库。
像MySQL这样的关系型数据库，表的结构比较复杂，会包含很多字段，可以通过SQL语句，来实现非常复杂的查询需求。而Redis中只包含“键”和“值”两部分，只能通过“键”来查询“值”。正是因为这样简单的存储结构，也让Redis的读写效率非常高。
除此之外，Redis主要是作为内存数据库来使用，也就是说，数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中。这一点，我们后面会介绍。
Redis中，键的数据类型是字符串，但是为了丰富数据存储的方式，方便开发者使用，值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。
“字符串（string）”这种数据类型非常简单，对应到数据结构里，就是字符串。你应该非常熟悉，这里我就不多介绍了。我们着重看下，其他四种比较复杂点的数据类型，看看它们底层都依赖了哪些数据结构。
列表（list）我们先来看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是压缩列表（ziplist），另一种是双向循环链表。
当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件：
列表中保存的单个数据（有可能是字符串类型的）小于64字节；
列表中数据个数少于512个。
关于压缩列表，我这里稍微解释一下。它并不是基础数据结构，而是Redis自己设计的一种数据存储结构。它有点儿类似数组，通过一片连续的内存空间，来存储数据。不过，它跟数组不同的一点是，它允许存储的数据大小不同。具体的存储结构也非常简单，你可以看我下面画的这幅图。
现在，我们来看看，压缩列表中的“压缩”两个字该如何理解？
听到“压缩”两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存，是相较于数组的存储思路而言的。我们知道，数组要求每个元素的大小相同，如果我们要存储不同长度的字符串，那我们就需要用最大长度的字符串大小作为元素的大小（假设是20个字节）。那当我们存储小于20个字节长度的字符串的时候，便会浪费部分存储空间。听起来有点儿拗口，我画个图解释一下。
压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。而且，因为数据存储在一片连续的内存空间，通过键来获取值为列表类型的数据，读取的效率也非常高。
当列表中存储的数据量比较大的时候，也就是不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。
在链表里，我们已经讲过双向循环链表这种数据结构了，如果不记得了，你可以先回去复习一下。这里我们着重看一下Redis中双向链表的编码实现方式。
Redis的这种双向链表的实现方式，非常值得借鉴。它额外定义一个list结构体，来组织链表的首、尾指针，还有长度等信息。这样，在使用的时候就会非常方便。
// 以下是C语言代码，因为Redis是用C语言实现的。 typedef struct listnode { struct listNode *prev; struct listNode *next; void *value; } listNode; typedef struct list { listNode *head; listNode *tail; unsigned long len; // &amp;hellip;.省略其他定义 } list; 字典（hash）字典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的压缩列表，另一种是散列表。
同样，只有当存储的数据量比较小的情况下，Redis才使用压缩列表来实现字典类型。具体需要满足两个条件：
字典中保存的键和值的大小都要小于64字节；
字典中键值对的个数要小于512个。
当不能同时满足上面两个条件的时候，Redis就使用散列表来实现字典类型。Redis使用MurmurHash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，Redis使用链表法来解决。除此之外，Redis还支持散列表的动态扩容、缩容。
当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于1的时候，Redis会触发扩容，将散列表扩大为原来大小的2倍左右（具体值需要计算才能得到，如果感兴趣，你可以去阅读源码）。
当数据动态减少之后，为了节省内存，当装载因子小于0.1的时候，Redis就会触发缩容，缩小为字典中数据个数的大约2倍大小（这个值也是计算得到的，如果感兴趣，你也可以去阅读源码）。
我们前面讲过，扩容缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。针对这个问题，Redis使用我们在散列表（中）讲的渐进式扩容缩容策略，将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。
集合（set）集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。</description></item><item><title>52_设计大型DMP系统（上）：MongoDB并不是什么灵丹妙药</title><link>https://artisanbox.github.io/4/52/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/52/</guid><description>如果你一讲一讲跟到现在，那首先要恭喜你，马上就看到胜利的曙光了。过去的50多讲里，我把计算机组成原理中的各个知识点，一点一点和你拆解了。对于其中的很多知识点，我也给了相应的代码示例和实际的应用案例。
不过呢，相信你和我一样，觉得只了解这样一个个零散的知识点和案例还不过瘾。那么从今天开始，我们就进入应用篇。我会通过两个应用系统的案例，串联起计算机组成原理的两大块知识点，一个是我们的整个存储器系统，另一个自然是我们的CPU和指令系统了。
我们今天就先从搭建一个大型的DMP系统开始，利用组成原理里面学到的存储器知识，来做选型判断，从而更深入地理解计算机组成原理。
DMP：数据管理平台我们先来看一下什么是DMP系统。DMP系统的全称叫作数据管理平台（Data Management Platform），目前广泛应用在互联网的广告定向（Ad Targeting）、个性化推荐（Recommendation）这些领域。
通常来说，DMP系统会通过处理海量的互联网访问数据以及机器学习算法，给一个用户标注上各种各样的标签。然后，在我们做个性化推荐和广告投放的时候，再利用这些这些标签，去做实际的广告排序、推荐等工作。无论是Google的搜索广告、淘宝里千人千面的商品信息，还是抖音里面的信息流推荐，背后都会有一个DMP系统。
那么，一个DMP系统应该怎么搭建呢？对于外部使用DMP的系统或者用户来说，可以简单地把DMP看成是一个键-值对（Key-Value）数据库。我们的广告系统或者推荐系统，可以通过一个客户端输入用户的唯一标识（ID），然后拿到这个用户的各种信息。
这些信息中，有些是用户的人口属性信息（Demographic），比如性别、年龄；有些是非常具体的行为（Behavior），比如用户最近看过的商品是什么，用户的手机型号是什么；有一些是我们通过算法系统计算出来的兴趣（Interests），比如用户喜欢健身、听音乐；还有一些则是完全通过机器学习算法得出的用户向量，给后面的推荐算法或者广告算法作为数据输入。
基于此，对于这个KV数据库，我们的期望也很清楚，那就是：低响应时间（Low Response Time）、高可用性（High Availability）、高并发（High Concurrency）、海量数据（Big Data），同时我们需要付得起对应的成本（Affordable Cost）。如果用数字来衡量这些指标，那么我们的期望就会具体化成下面这样。
低响应时间：一般的广告系统留给整个广告投放决策的时间也就是10ms左右，所以对于访问DMP获取用户数据，预期的响应时间都在1ms之内。 高可用性：DMP常常用在广告系统里面。DMP系统出问题，往往就意味着我们整个的广告收入在不可用的时间就没了，所以我们对于可用性的追求可谓是没有上限的。Google 2018年的广告收入是1160亿美元，折合到每一分钟的收入是22万美元。即使我们做到 99.99% 的可用性，也意味着每个月我们都会损失100万美元。 高并发：还是以广告系统为例，如果每天我们需要响应100亿次的广告请求，那么我们每秒的并发请求数就在 100亿 / (86400) ~= 12K 次左右，所以我们的DMP需要支持高并发。 数据量：如果我们的产品针对中国市场，那么我们需要有10亿个Key，对应的假设每个用户有500个标签，标签有对应的分数。标签和分数都用一个4字节（Bytes）的整数来表示，那么一共我们需要 10亿 x 500 x (4 + 4) Bytes = 4 TB 的数据了。 低成本：我们还是从广告系统的角度来考虑。广告系统的收入通常用CPM（Cost Per Mille），也就是千次曝光来统计。如果千次曝光的利润是 0.10美元，那么每天100亿次的曝光就是100万美元的利润。这个利润听起来非常高了。但是反过来算一下，你会发现，DMP每1000次的请求的成本不能超过 0.10美元。最好只有0.01美元，甚至更低，我们才能尽可能多赚到一点广告利润。 这五个因素一结合，听起来是不是就不那么简单了？不过，更复杂的还在后面呢。
虽然从外部看起来，DMP特别简单，就是一个KV数据库，但是生成这个数据库需要做的事情更多。我们下面一起来看一看。
在这个系统中，我们关心的是蓝色的数据管道、绿色的数据仓库和KV数据库为了能够生成这个KV数据库，我们需要有一个在客户端或者Web端的数据采集模块，不断采集用户的行为，向后端的服务器发送数据。服务器端接收到数据，就要把这份数据放到一个数据管道（Data Pipeline）里面。数据管道的下游，需要实际将数据落地到数据仓库（Data Warehouse），把所有的这些数据结构化地存储起来。后续，我们就可以通过程序去分析这部分日志，生成报表或者或者利用数据运行各种机器学习算法。
除了这个数据仓库之外，我们还会有一个实时数据处理模块（Realtime Data Processing），也放在数据管道的下游。它同样会读取数据管道里面的数据，去进行各种实时计算，然后把需要的结果写入到DMP的KV数据库里面去。
MongoDB真的万能吗？面对这里的KV数据库、数据管道以及数据仓库，这三个不同的数据存储的需求，最合理的技术方案是什么呢？你可以先自己思考一下，我这里先卖个关子。
我共事过的不少不错的Web程序员，面对这个问题的时候，常常会说：“这有什么难的，用MongoDB就好了呀！”如果你也选择了MongoDB，那最终的结果一定是一场灾难。我为什么这么说呢？
MongoDB的设计听起来特别厉害，不需要预先数据Schema，访问速度很快，还能够无限水平扩展。作为KV数据库，我们可以把MongoDB当作DMP里面的KV数据库；除此之外，MongoDB还能水平扩展、跑MQL，我们可以把它当作数据仓库来用。至于数据管道，只要我们能够不断往MongoDB里面，插入新的数据就好了。从运维的角度来说，我们只需要维护一种数据库，技术栈也变得简单了。看起来，MongoDB这个选择真是相当完美！
但是，作为一个老程序员，第一次听到MongoDB这样“万能”的解决方案，我的第一反应是，“天底下哪有这样的好事”。所有的软件系统，都有它的适用场景，想通过一种解决方案适用三个差异非常大的应用场景，显然既不合理，又不现实。接下来，我们就来仔细看一下，这个“不合理”“不现实”在什么地方。
上面我们已经讲过DMP的KV数据库期望的应用场景和性能要求了，这里我们就来看一下数据管道和数据仓库的性能取舍。
对于数据管道来说，我们需要的是高吞吐量，它的并发量虽然和KV数据库差不多，但是在响应时间上，要求就没有那么严格了，1-2秒甚至再多几秒的延时都是可以接受的。而且，和KV数据库不太一样，数据管道的数据读写都是顺序读写，没有大量的随机读写的需求。
数据仓库就更不一样了，数据仓库的数据读取的量要比管道大得多。管道的数据读取就是我们当时写入的数据，一天有10TB日志数据，管道只会写入10TB。下游的数据仓库存放数据和实时数据模块读取的数据，再加上个2倍的10TB，也就是20TB也就够了。
但是，数据仓库的数据分析任务要读取的数据量就大多了。一方面，我们可能要分析一周、一个月乃至一个季度的数据。这一次分析要读取的数据可不是10TB，而是100TB乃至1PB。我们一天在数据仓库上跑的分析任务也不是1个，而是成千上万个，所以数据的读取量是巨大的。另一方面，我们存储在数据仓库里面的数据，也不像数据管道一样，存放几个小时、最多一天的数据，而是往往要存上3个月甚至是1年的数据。所以，我们需要的是1PB乃至5PB这样的存储空间。
我把KV数据库、数据管道和数据仓库的应用场景，总结成了一个表格，放在这里。你可以对照着看一下，想想为什么MongoDB在这三个应用场景都不合适。
在KV数据库的场景下，需要支持高并发。那么MongoDB需要把更多的数据放在内存里面，但是这样我们的存储成本就会特别高了。
在数据管道的场景下，我们需要的是大量的顺序读写，而MongoDB则是一个文档数据库系统，并没有为顺序写入和吞吐量做过优化，看起来也不太适用。
而在数据仓库的场景下，主要的数据读取时顺序读取，并且需要海量的存储。MongoDB这样的文档式数据库也没有为海量的顺序读做过优化，仍然不是一个最佳的解决方案。而且文档数据库里总是会有很多冗余的字段的元数据，还会浪费更多的存储空间。
那我们该选择什么样的解决方案呢？</description></item><item><title>53_算法实战（二）：剖析搜索引擎背后的经典数据结构和算法</title><link>https://artisanbox.github.io/2/54/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/54/</guid><description>像百度、Google这样的搜索引擎，在我们平时的工作、生活中，几乎天天都会用到。如果我们把搜索引擎也当作一个互联网产品的话，那它跟社交、电商这些类型的产品相比，有一个非常大的区别，那就是，它是一个技术驱动的产品。所谓技术驱动是指，搜索引擎实现起来，技术难度非常大，技术的好坏直接决定了这个产品的核心竞争力。
在搜索引擎的设计与实现中，会用到大量的算法。有很多针对特定问题的算法，也有很多我们专栏中讲到的基础算法。所以，百度、Google这样的搜索引擎公司，在面试的时候，会格外重视考察候选人的算法能力。
今天我就借助搜索引擎，这样一个非常有技术含量的产品，来给你展示一下，数据结构和算法是如何应用在其中的。
整体系统介绍像Google这样的大型商用搜索引擎，有成千上万的工程师，十年如一日地对它进行优化改进，所以，它所包含的技术细节非常多。我很难、也没有这个能力，通过一篇文章把所有细节都讲清楚，当然这也不是我们专栏所专注的内容。
所以，接下来的讲解，我主要给你展示，如何在一台机器上（假设这台机器的内存是8GB， 硬盘是100多GB），通过少量的代码，实现一个小型搜索引擎。不过，麻雀虽小，五脏俱全。跟大型搜索引擎相比，实现这样一个小型搜索引擎所用到的理论基础是相通的。
搜索引擎大致可以分为四个部分：搜集、分析、索引、查询。其中，搜集，就是我们常说的利用爬虫爬取网页。分析，主要负责网页内容抽取、分词，构建临时索引，计算PageRank值这几部分工作。索引，主要负责通过分析阶段得到的临时索引，构建倒排索引。查询，主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户。
接下来，我就按照网页处理的生命周期，从这四个阶段，依次来给你讲解，一个网页从被爬取到最终展示给用户，这样一个完整的过程。与此同时，我会穿插讲解，这个过程中需要用到哪些数据结构和算法。
搜集现在，互联网越来越发达，网站越来越多，对应的网页也就越来越多。对于搜索引擎来说，它事先并不知道网页都在哪里。打个比方来说就是，我们只知道海里面有很多鱼，但却并不知道鱼在哪里。那搜索引擎是如何爬取网页的呢？
搜索引擎把整个互联网看作数据结构中的有向图，把每个页面看作一个顶点。如果某个页面中包含另外一个页面的链接，那我们就在两个顶点之间连一条有向边。我们可以利用图的遍历搜索算法，来遍历整个互联网中的网页。
我们前面介绍过两种图的遍历方法，深度优先和广度优先。搜索引擎采用的是广度优先搜索策略。具体点讲的话，那就是，我们先找一些比较知名的网页（专业的叫法是权重比较高）的链接（比如新浪主页网址、腾讯主页网址等），作为种子网页链接，放入到队列中。爬虫按照广度优先的策略，不停地从队列中取出链接，然后去爬取对应的网页，解析出网页里包含的其他网页链接，再将解析出来的链接添加到队列中。
基本的原理就是这么简单。但落实到实现层面，还有很多技术细节。我下面借助搜集阶段涉及的几个重要文件，来给你解释一下搜集工程都有哪些关键技术细节。
1.待爬取网页链接文件：links.bin在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。于是，队列中的链接就会越来越多，可能会多到内存放不下。所以，我们用一个存储在磁盘中的文件（links.bin）来作为广度优先搜索中的队列。爬虫从links.bin文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到links.bin文件中。
这样用文件来存储网页链接的方式，还有其他好处。比如，支持断点续爬。也就是说，当机器断电之后，网页链接不会丢失；当机器重启之后，还可以从之前爬取到的位置继续爬取。
关于如何解析页面获取链接，我额外多说几句。我们可以把整个页面看作一个大的字符串，然后利用字符串匹配算法，在这个大字符串中，搜索&amp;lt;link&amp;gt;这样一个网页标签，然后顺序读取&amp;lt;link&amp;gt;&amp;lt;/link&amp;gt;之间的字符串。这其实就是网页链接。
2.网页判重文件：bloom_filter.bin如何避免重复爬取相同的网页呢？这个问题我们在位图那一节已经讲过了。使用布隆过滤器，我们就可以快速并且非常节省内存地实现网页的判重。
不过，还是刚刚那个问题，如果我们把布隆过滤器存储在内存中，那机器宕机重启之后，布隆过滤器就被清空了。这样就可能导致大量已经爬取的网页会被重复爬取。
这个问题该怎么解决呢？我们可以定期地（比如每隔半小时）将布隆过滤器持久化到磁盘中，存储在bloom_filter.bin文件中。这样，即便出现机器宕机，也只会丢失布隆过滤器中的部分数据。当机器重启之后，我们就可以重新读取磁盘中的bloom_filter.bin文件，将其恢复到内存中。
3.原始网页存储文件：doc_raw.bin爬取到网页之后，我们需要将其存储下来，以备后面离线分析、索引之用。那如何存储海量的原始网页数据呢？
如果我们把每个网页都存储为一个独立的文件，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。常用的文件系统显然不适合存储如此多的文件。所以，我们可以把多个网页存储在一个文件中。每个网页之间，通过一定的标识进行分隔，方便后续读取。具体的存储格式，如下图所示。其中，doc_id这个字段是网页的编号，我们待会儿再解释。
当然，这样的一个文件也不能太大，因为文件系统对文件的大小也有一定的限制。所以，我们可以设置每个文件的大小不能超过一定的值（比如1GB）。随着越来越多的网页被添加到文件中，文件的大小就会越来越大，当超过1GB的时候，我们就创建一个新的文件，用来存储新爬取的网页。
假设一台机器的硬盘大小是100GB左右，一个网页的平均大小是64KB。那在一台机器上，我们可以存储100万到200万左右的网页。假设我们的机器的带宽是10MB，那下载100GB的网页，大约需要10000秒。也就是说，爬取100多万的网页，也就是只需要花费几小时的时间。
4.网页链接及其编号的对应文件：doc_id.bin刚刚我们提到了网页编号这个概念，我现在解释一下。网页编号实际上就是给每个网页分配一个唯一的ID，方便我们后续对网页进行分析、索引。那如何给网页编号呢？
我们可以按照网页被爬取的先后顺序，从小到大依次编号。具体是这样做的：我们维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，我们将网页链接跟编号之间的对应关系，存储在另一个doc_id.bin文件中。
爬虫在爬取网页的过程中，涉及的四个重要的文件，我就介绍完了。其中，links.bin和bloom_filter.bin这两个文件是爬虫自身所用的。另外的两个（doc_raw.bin、doc_id.bin）是作为搜集阶段的成果，供后面的分析、索引、查询用的。
分析网页爬取下来之后，我们需要对网页进行离线分析。分析阶段主要包括两个步骤，第一个是抽取网页文本信息，第二个是分词并创建临时索引。我们逐一来讲解。
1.抽取网页文本信息网页是半结构化数据，里面夹杂着各种标签、JavaScript代码、CSS样式。对于搜索引擎来说，它只关心网页中的文本信息，也就是，网页显示在浏览器中时，能被用户肉眼看到的那部分信息。我们如何从半结构化的网页中，抽取出搜索引擎关系的文本信息呢？
我们之所以把网页叫作半结构化数据，是因为它本身是按照一定的规则来书写的。这个规则就是HTML语法规范。我们依靠HTML标签来抽取网页中的文本信息。这个抽取的过程，大体可以分为两步。
第一步是去掉JavaScript代码、CSS格式以及下拉框中的内容（因为下拉框在用户不操作的情况下，也是看不到的）。也就是&amp;lt;style&amp;gt;&amp;lt;/style&amp;gt;，&amp;lt;script&amp;gt;&amp;lt;/script&amp;gt;，&amp;lt;option&amp;gt;&amp;lt;/option&amp;gt;这三组标签之间的内容。我们可以利用AC自动机这种多模式串匹配算法，在网页这个大字符串中，一次性查找&amp;lt;style&amp;gt;, &amp;lt;script&amp;gt;, &amp;lt;option&amp;gt;这三个关键词。当找到某个关键词出现的位置之后，我们只需要依次往后遍历，直到对应结束标签（&amp;lt;/style&amp;gt;, &amp;lt;/script&amp;gt;, &amp;lt;/option）为止。而这期间遍历到的字符串连带着标签就应该从网页中删除。
第二步是去掉所有HTML标签。这一步也是通过字符串匹配算法来实现的。过程跟第一步类似，我就不重复讲了。
2.分词并创建临时索引经过上面的处理之后，我们就从网页中抽取出了我们关心的文本信息。接下来，我们要对文本信息进行分词，并且创建临时索引。
对于英文网页来说，分词非常简单。我们只需要通过空格、标点符号等分隔符，将每个单词分割开来就可以了。但是，对于中文来说，分词就复杂太多了。我这里介绍一种比较简单的思路，基于字典和规则的分词方法。
其中，字典也叫词库，里面包含大量常用的词语（我们可以直接从网上下载别人整理好的）。我们借助词库并采用最长匹配规则，来对文本进行分词。所谓最长匹配，也就是匹配尽可能长的词语。我举个例子解释一下。
比如要分词的文本是“中国人民解放了”，我们词库中有“中国”“中国人”“中国人民”“中国人民解放军”这几个词，那我们就取最长匹配，也就是“中国人民”划为一个词，而不是把“中国”、“中国人”划为一个词。具体到实现层面，我们可以将词库中的单词，构建成Trie树结构，然后拿网页文本在Trie树中匹配。
每个网页的文本信息在分词完成之后，我们都得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件。临时索引文件的格式如下：
在临时索引文件中，我们存储的是单词编号，也就是图中的term_id，而非单词本身。这样做的目的主要是为了节省存储的空间。那这些单词的编号是怎么来的呢？
给单词编号的方式，跟给网页编号类似。我们维护一个计数器，每当从网页文本信息中分割出一个新的单词的时候，我们就从计数器中取一个编号，分配给它，然后计数器加一。
在这个过程中，我们还需要使用散列表，记录已经编过号的单词。在对网页文本信息分词的过程中，我们拿分割出来的单词，先到散列表中查找，如果找到，那就直接使用已有的编号；如果没有找到，我们再去计数器中拿号码，并且将这个新单词以及编号添加到散列表中。
当所有的网页处理（分词及写入临时索引）完成之后，我们再将这个单词跟编号之间的对应关系，写入到磁盘文件中，并命名为term_id.bin。
经过分析阶段，我们得到了两个重要的文件。它们分别是临时索引文件（tmp_index.bin）和单词编号文件（term_id.bin）。
索引索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引。倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表。文字描述比较难理解，我画了一张倒排索引的结构图，你一看就明白。
我们刚刚讲到，在临时索引文件中，记录的是单词跟每个包含它的文档之间的对应关系。那如何通过临时索引文件，构建出倒排索引文件呢？这是一个非常典型的算法问题，你可以先自己思考一下，再看我下面的讲解。
解决这个问题的方法有很多。考虑到临时索引文件很大，无法一次性加载到内存中，搜索引擎一般会选择使用多路归并排序的方法来实现。
我们先对临时索引文件，按照单词编号的大小进行排序。因为临时索引很大，所以一般基于内存的排序算法就没法处理这个问题了。我们可以用之前讲到的归并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起。当然，实际的软件开发中，我们其实可以直接利用MapReduce来处理。
临时索引文件排序完成之后，相同的单词就被排列到了一起。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，然后把它们存储在倒排索引文件中。具体的处理过程，我画成了一张图。通过图，你应该更容易理解。
除了倒排文件之外，我们还需要一个文件，来记录每个单词编号在倒排索引文件中的偏移位置。我们把这个文件命名为term_offset.bin。这个文件的作用是，帮助我们快速地查找某个单词编号在倒排索引中存储的位置，进而快速地从倒排索引中读取单词编号对应的网页编号列表。
经过索引阶段的处理，我们得到了两个有价值的文件，它们分别是倒排索引文件（index.bin）和记录单词编号在索引文件中的偏移位置的文件（term_offset.bin）。
查询前面三个阶段的处理，只是为了最后的查询做铺垫。因此，现在我们就要利用之前产生的几个文件，来实现最终的用户搜索功能。
doc_id.bin：记录网页链接和编号之间的对应关系。
term_id.bin：记录单词和编号之间的对应关系。
index.bin：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。
term_offsert.bin：记录每个单词编号在倒排索引文件中的偏移位置。
这四个文件中，除了倒排索引文件（index.bin）比较大之外，其他的都比较小。为了方便快速查找数据，我们将其他三个文件都加载到内存中，并且组织成散列表这种数据结构。
当用户在搜索框中，输入某个查询文本的时候，我们先对用户输入的文本进行分词处理。假设分词之后，我们得到k个单词。</description></item><item><title>53_设计大型DMP系统（下）：SSD拯救了所有的DBA</title><link>https://artisanbox.github.io/4/53/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/53/</guid><description>上一讲里，根据DMP系统的各个应用场景，我们从抽象的原理层面，选择了AeroSpike作为KV数据库，Kafka作为数据管道，Hadoop/Hive来作为数据仓库。
不过呢，肯定有不信邪的工程师会问，为什么MongoDB，甚至是MySQL这样的文档数据库或者传统的关系型数据库不适用呢？为什么不能通过优化SQL、添加缓存这样的调优手段，解决这个问题呢？
今天DMP的下半场，我们就从数据库实现的原理，一起来看一看，这背后的原因。如果你能弄明白今天的这些更深入、更细节的原理，对于什么场景使用什么数据库，就会更加胸有成竹，而不是只有跑了大量的性能测试才知道。下次做数据库选型的时候，你就可以“以理服人”了。
关系型数据库：不得不做的随机读写我们先来想一想，如果现在让你自己写一个最简单的关系型数据库，你的数据要怎么存放在硬盘上？
最简单最直观的想法是，用一个CSV文件格式。一个文件就是一个数据表。文件里面的每一行就是这个表里面的一条记录。如果要修改数据库里面的某一条记录，那么我们要先找到这一行，然后直接去修改这一行的数据。读取数据也是一样的。
要找到这样数据，最笨的办法自然是一行一行读，也就是遍历整个CSV文件。不过这样的话，相当于随便读取任何一条数据都要扫描全表，太浪费硬盘的吞吐量了。那怎么办呢？我们可以试试给这个CSV文件加一个索引。比如，给数据的行号加一个索引。如果你学过数据库原理或者算法和数据结构，那你应该知道，通过B+树多半是可以来建立这样一个索引的。
索引里面没有一整行的数据，只有一个映射关系，这个映射关系可以让行号直接从硬盘的某个位置去读。所以，索引比起数据小很多。我们可以把索引加载到内存里面。即使不在内存里面，要找数据的时候快速遍历一下整个索引，也不需要读太多的数据。
加了索引之后，我们要读取特定的数据，就不用去扫描整个数据表文件了。直接从特定的硬盘位置，就可以读到想要的行。索引不仅可以索引行号，还可以索引某个字段。我们可以创建很多个不同的独立的索引。写SQL的时候，where子句后面的查询条件可以用到这些索引。
不过，这样的话，写入数据的时候就会麻烦一些。我们不仅要在数据表里面写入数据，对于所有的索引也都需要进行更新。这个时候，写入一条数据就要触发好几个随机写入的更新。
在这样一个数据模型下，查询操作很灵活。无论是根据哪个字段查询，只要有索引，我们就可以通过一次随机读，很快地读到对应的数据。但是，这个灵活性也带来了一个很大的问题，那就是无论干点什么，都有大量的随机读写请求。而随机读写请求，如果请求最终是要落到硬盘上，特别是HDD硬盘的话，我们就很难做到高并发了。毕竟HDD硬盘只有100左右的QPS。
而这个随时添加索引，可以根据任意字段进行查询，这样表现出的灵活性，又是我们的DMP系统里面不太需要的。DMP的KV数据库主要的应用场景，是根据主键的随机查询，不需要根据其他字段进行筛选查询。数据管道的需求，则只需要不断追加写入和顺序读取就好了。即使进行数据分析的数据仓库，通常也不是根据字段进行数据筛选，而是全量扫描数据进行分析汇总。
后面的两个场景还好说，大不了我们让程序去扫描全表或者追加写入。但是，在KV数据库这个需求上，刚才这个最简单的关系型数据库的设计，就会面临大量的随机写入和随机读取的挑战。
所以，在实际的大型系统中，大家都会使用专门的分布式KV数据库，来满足这个需求。那么下面，我们就一起来看一看，Facebook开源的Cassandra的数据存储和读写是怎么做的，这些设计是怎么解决高并发的随机读写问题的。
Cassandra：顺序写和随机读Cassandra的数据模型作为一个分布式的KV数据库，Cassandra的键一般被称为Row Key。其实就是一个16到36个字节的字符串。每一个Row Key对应的值其实是一个哈希表，里面可以用键值对，再存入很多你需要的数据。
Cassandra本身不像关系型数据库那样，有严格的Schema，在数据库创建的一开始就定义好了有哪些列（Column）。但是，它设计了一个叫作列族（Column Family）的概念，我们需要把经常放在一起使用的字段，放在同一个列族里面。比如，DMP里面的人口属性信息，我们可以把它当成是一个列族。用户的兴趣信息，可以是另外一个列族。这样，既保持了不需要严格的Schema这样的灵活性，也保留了可以把常常一起使用的数据存放在一起的空间局部性。
往Cassandra的里面读写数据，其实特别简单，就好像是在一个巨大的分布式的哈希表里面写数据。我们指定一个Row Key，然后插入或者更新这个Row Key的数据就好了。
Cassandra的写操作Cassandra只有顺序写入，没有随机写入Cassandra解决随机写入数据的解决方案，简单来说，就叫作“不随机写，只顺序写”。对于Cassandra数据库的写操作，通常包含两个动作。第一个是往磁盘上写入一条提交日志（Commit Log）。另一个操作，则是直接在内存的数据结构上去更新数据。后面这个往内存的数据结构里面的数据更新，只有在提交日志写成功之后才会进行。每台机器上，都有一个可靠的硬盘可以让我们去写入提交日志。写入提交日志都是顺序写（Sequential Write），而不是随机写（Random Write），这使得我们最大化了写入的吞吐量。
如果你不明白这是为什么，可以回到第47讲，看看硬盘的性能评测。无论是HDD硬盘还是SSD硬盘，顺序写入都比随机写入要快得多。
内存的空间比较有限，一旦内存里面的数据量或者条目超过一定的限额，Cassandra就会把内存里面的数据结构dump到硬盘上。这个Dump的操作，也是顺序写而不是随机写，所以性能也不会是一个问题。除了Dump的数据结构文件，Cassandra还会根据row key来生成一个索引文件，方便后续基于索引来进行快速查询。
随着硬盘上的Dump出来的文件越来越多，Cassandra会在后台进行文件的对比合并。在很多别的KV数据库系统里面，也有类似这种的合并动作，比如AeroSpike或者Google的BigTable。这些操作我们一般称之为Compaction。合并动作同样是顺序读取多个文件，在内存里面合并完成，再Dump出来一个新的文件。整个操作过程中，在硬盘层面仍然是顺序读写。
Cassandra的读操作Cassandra的读请求，会通过缓存、BloomFilter进行两道过滤，尽可能避免数据请求命中硬盘当我们要从Cassandra读数据的时候，会从内存里面找数据，再从硬盘读数据，然后把两部分的数据合并成最终结果。这些硬盘上的文件，在内存里面会有对应的Cache，只有在Cache里面找不到，我们才会去请求硬盘里面的数据。
如果不得不访问硬盘，因为硬盘里面可能Dump了很多个不同时间点的内存数据的快照。所以，找数据的时候，我们也是按照时间从新的往旧的里面找。
这也就带来另外一个问题，我们可能要查询很多个Dump文件，才能找到我们想要的数据。所以，Cassandra在这一点上又做了一个优化。那就是，它会为每一个Dump的文件里面所有Row Key生成一个BloomFilter，然后把这个BloomFilter放在内存里面。这样，如果想要查询的Row Key在数据文件里面不存在，那么99%以上的情况下，它会被BloomFilter过滤掉，而不需要访问硬盘。
这样，只有当数据在内存里面没有，并且在硬盘的某个特定文件上的时候，才会触发一次对于硬盘的读请求。
SSD：DBA们的大救星Cassandra是Facebook在2008年开源的。那个时候，SSD硬盘还没有那么普及。可以看到，它的读写设计充分考虑了硬件本身的特性。在写入数据进行持久化上，Cassandra没有任何的随机写请求，无论是Commit Log还是Dump，全部都是顺序写。
在数据读的请求上，最新写入的数据都会更新到内存。如果要读取这些数据，会优先从内存读到。这相当于是一个使用了LRU的缓存机制。只有在万般无奈的情况下，才会有对于硬盘的随机读请求。即使在这样的情况下，Cassandra也在文件之前加了一层BloomFilter，把本来因为Dump文件带来的需要多次读硬盘的问题，简化成多次内存读和一次硬盘读。
这些设计，使得Cassandra即使是在HDD硬盘上，也能有不错的访问性能。因为所有的写入都是顺序写或者写入到内存，所以，写入可以做到高并发。HDD硬盘的吞吐率还是很不错的，每秒可以写入100MB以上的数据，如果一条数据只有1KB，那么10万的WPS（Writes per seconds）也是能够做到的。这足够支撑我们DMP期望的写入压力了。
而对于数据的读，就有一些挑战了。如果数据读请求有很强的局部性，那我们的内存就能搞定DMP需要的访问量。
但是，问题就出在这个局部性上。DMP的数据访问分布，其实是缺少局部性的。你仔细想一想DMP的应用场景就明白了。DMP里面的Row Key都是用户的唯一标识符。普通用户的上网时长怎么会有局部性呢？每个人上网的时间和访问网页的次数就那么多。上网多的人，一天最多也就24小时。大部分用户一天也要上网2～3小时。我们没办法说，把这些用户的数据放在内存里面，那些用户不放。
DMP系统，只有根据国家和时区不同有比较明显的局部性，是局部性不强的系统那么，我们可不可能有一定的时间局部性呢？如果是Facebook那样的全球社交网络，那可能还有一定的时间局部性。毕竟不同国家的人的时区不一样。我们可以说，在印度人民的白天，把印度人民的数据加载到内存里面，美国人民的数据就放在硬盘上。到了印度人民的晚上，再把美国人民的数据换到内存里面来。
如果你的主要业务是在国内，那这个时间局部性就没有了。大家的上网高峰时段，都是在早上上班路上、中午休息的时候以及晚上下班之后的时间，没有什么区分度。
面临这个情况，如果你们的CEO或者CTO问你，是不是可以通过优化程序来解决这个问题？如果你没有仔细从数据分布和原理的层面思考这个问题，而直接一口答应下来，那你可能之后要头疼了，因为这个问题很有可能是搞不定的。
因为缺少了时间局部性，我们内存的缓存能够起到的作用就很小了，大部分请求最终还是要落到HDD硬盘的随机读上。但是，HDD硬盘的随机读的性能太差了，我们在第45讲看过，也就是100QPS左右。而如果全都放内存，那就太贵了，成本在HDD硬盘100倍以上。
不过，幸运的是，从2010年开始，SSD硬盘的大规模商用帮助我们解决了这个问题。它的价格在HDD硬盘的10倍，但是随机读的访问能力在HDD硬盘的百倍以上。也就是说，用上了SSD硬盘，我们可以用1/10的成本获得和内存同样的QPS。同样的价格的SSD硬盘，容量则是内存的几十倍，也能够满足我们的需求，用较低的成本存下整个互联网用户信息。
不夸张地说，过去十年的“大数据”“高并发”“千人千面”，有一半的功劳应该归在让SSD容量不断上升、价格不断下降的硬盘产业上。
回到我们看到的Cassandra的读写设计，你会发现，Cassandra的写入机制完美匹配了我们在第46和47讲所说的SSD硬盘的优缺点。
在数据写入层面，Cassandra的数据写入都是Commit Log的顺序写入，也就是不断地在硬盘上往后追加内容，而不是去修改现有的文件内容。一旦内存里面的数据超过一定的阈值，Cassandra又会完整地Dump一个新文件到文件系统上。这同样是一个追加写入。
数据的对比和紧凑化（Compaction），同样是读取现有的多个文件，然后写一个新的文件出来。写入操作只追加不修改的特性，正好天然地符合SSD硬盘只能按块进行擦除写入的操作。在这样的写入模式下，Cassandra用到的SSD硬盘，不需要频繁地进行后台的Compaction，能够最大化SSD硬盘的使用寿命。这也是为什么，Cassandra在SSD硬盘普及之后，能够获得进一步快速发展。
总结延伸好了，关于DMP和存储器的内容，讲到这里就差不多了。希望今天的这一讲，能够让你从Cassandra的数据库实现的细节层面，彻底理解怎么运用好存储器的性能特性和原理。
传统的关系型数据库，我们把一条条数据存放在一个地方，同时再把索引存放在另外一个地方。这样的存储方式，其实很方便我们进行单次的随机读和随机写，数据的存储也可以很紧凑。但是问题也在于此，大部分的SQL请求，都会带来大量的随机读写的请求。这使得传统的关系型数据库，其实并不适合用在真的高并发的场景下。
我们的DMP需要的访问场景，其实没有复杂的索引需求，但是会有比较高的并发性。我带你一看了Facebook开源的Cassandra这个分布式KV数据库的读写设计。通过在追加写入Commit Log和更新内存，Cassandra避开了随机写的问题。内存数据的Dump和后台的对比合并，同样也都避开了随机写的问题，使得Cassandra的并发写入性能极高。
在数据读取层面，通过内存缓存和BloomFilter，Cassandra已经尽可能地减少了需要随机读取硬盘里面数据的情况。不过挑战在于，DMP系统的局部性不强，使得我们最终的随机读的请求还是要到硬盘上。幸运的是，SSD硬盘在数据海量增长的那几年里价格不断下降，使得我们最终通过SSD硬盘解决了这个问题。
而SSD硬盘本身的擦除后才能写入的机制，正好非常适合Cassandra的数据读写模式，最终使得Cassandra在SSD硬盘普及之后得到了更大的发展。
推荐阅读今天的推荐阅读，是一篇相关的论文。我推荐你去读一读Cassandra - A Decentralized Structured Storage System。读完这篇论文，一方面你会对分布式KV数据库的设计原则有所了解，了解怎么去做好数据分片、故障转移、数据复制这些机制；另一方面，你可以看到基于内存和硬盘的不同存储设备的特性，Cassandra是怎么有针对性地设计数据读写和持久化的方式的。
课后思考除了MySQL这样的关系型数据库，还有Cassandra这样的分布式KV数据库。实际上，在海量数据分析的过程中，还有一种常见的数据库，叫作列式存储的OLAP的数据库，比如Clickhouse。你可以研究一下，Clickhouse这样的数据库里面的数据是怎么存储在硬盘上的。
欢迎把你研究的结果写在留言区，和大家一起分享、交流。如果觉得有帮助，你也可以把这篇文章分享给你的朋友，和他一起讨论、学习。</description></item><item><title>54_理解Disruptor（上）：带你体会CPU高速缓存的风驰电掣</title><link>https://artisanbox.github.io/4/54/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/54/</guid><description>坚持到底就是胜利，终于我们一起来到了专栏的最后一个主题。让我一起带你来看一看，CPU到底能有多快。在接下来的两讲里，我会带你一起来看一个开源项目Disruptor。看看我们怎么利用CPU和高速缓存的硬件特性，来设计一个对于性能有极限追求的系统。
不知道你还记不记得，在第37讲里，为了优化4毫秒专门铺设光纤的故事。实际上，最在意极限性能的并不是互联网公司，而是高频交易公司。我们今天讲解的Disruptor就是由一家专门做高频交易的公司LMAX开源出来的。
有意思的是，Disruptor的开发语言，并不是很多人心目中最容易做到性能极限的C/C++，而是性能受限于JVM的Java。这到底是怎么一回事呢？那通过这一讲，你就能体会到，其实只要通晓硬件层面的原理，即使是像Java这样的高级语言，也能够把CPU的性能发挥到极限。
Padding Cache Line，体验高速缓存的威力我们先来看看Disruptor里面一段神奇的代码。这段代码里，Disruptor在RingBufferPad这个类里面定义了p1，p2一直到p7 这样7个long类型的变量。
abstract class RingBufferPad { protected long p1, p2, p3, p4, p5, p6, p7; } 我在看到这段代码的第一反应是，变量名取得不规范，p1-p7这样的变量名没有明确的意义啊。不过，当我深入了解了Disruptor的设计和源代码，才发现这些变量名取得恰如其分。因为这些变量就是没有实际意义，只是帮助我们进行缓存行填充（Padding Cache Line），使得我们能够尽可能地用上CPU高速缓存（CPU Cache）。那么缓存行填充这个黑科技到底是什么样的呢？我们接着往下看。
不知道你还记不记得，我们在35讲里面的这个表格。如果访问内置在CPU里的L1 Cache或者L2 Cache，访问延时是内存的1/15乃至1/100。而内存的访问速度，其实是远远慢于CPU的。想要追求极限性能，需要我们尽可能地多从CPU Cache里面拿数据，而不是从内存里面拿数据。
CPU Cache装载内存里面的数据，不是一个一个字段加载的，而是加载一整个缓存行。举个例子，如果我们定义了一个长度为64的long类型的数组。那么数据从内存加载到CPU Cache里面的时候，不是一个一个数组元素加载的，而是一次性加载固定长度的一个缓存行。
我们现在的64位Intel CPU的计算机，缓存行通常是64个字节（Bytes）。一个long类型的数据需要8个字节，所以我们一下子会加载8个long类型的数据。也就是说，一次加载数组里面连续的8个数值。这样的加载方式使得我们遍历数组元素的时候会很快。因为后面连续7次的数据访问都会命中缓存，不需要重新从内存里面去读取数据。这个性能层面的好处，我在第37讲的第一个例子里面为你演示过，印象不深的话，可以返回去看看。
但是，在我们不使用数组，而是使用单独的变量的时候，这里就会出现问题了。在Disruptor的RingBuffer（环形缓冲区）的代码里面，定义了一个RingBufferFields类，里面有indexMask和其他几个变量，用来存放RingBuffer的内部状态信息。
CPU在加载数据的时候，自然也会把这个数据从内存加载到高速缓存里面来。不过，这个时候，高速缓存里面除了这个数据，还会加载这个数据前后定义的其他变量。这个时候，问题就来了。Disruptor是一个多线程的服务器框架，在这个数据前后定义的其他变量，可能会被多个不同的线程去更新数据、读取数据。这些写入以及读取的请求，会来自于不同的 CPU Core。于是，为了保证数据的同步更新，我们不得不把CPU Cache里面的数据，重新写回到内存里面去或者重新从内存里面加载数据。
而我们刚刚说过，这些CPU Cache的写回和加载，都不是以一个变量作为单位的。这些动作都是以整个Cache Line作为单位的。所以，当INITIAL_CURSOR_VALUE 前后的那些变量被写回到内存的时候，这个字段自己也写回到了内存，这个常量的缓存也就失效了。当我们要再次读取这个值的时候，要再重新从内存读取。这也就意味着，读取速度大大变慢了。
...... abstract class RingBufferPad { protected long p1, p2, p3, p4, p5, p6, p7; }
abstract class RingBufferFields&amp;lt;E&amp;gt; extends RingBufferPad { &amp;hellip;&amp;hellip; private final long indexMask; private final Object[] entries; protected final int bufferSize; protected final Sequencer sequencer; &amp;hellip;&amp;hellip; }</description></item><item><title>54_算法实战（三）：剖析高性能队列Disruptor背后的数据结构和算法</title><link>https://artisanbox.github.io/2/55/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/55/</guid><description>Disruptor你是否听说过呢？它是一种内存消息队列。从功能上讲，它其实有点儿类似Kafka。不过，和Kafka不同的是，Disruptor是线程之间用于消息传递的队列。它在Apache Storm、Camel、Log4j 2等很多知名项目中都有广泛应用。
之所以如此受青睐，主要还是因为它的性能表现非常优秀。它比Java中另外一个非常常用的内存消息队列ArrayBlockingQueue（ABS）的性能，要高一个数量级，可以算得上是最快的内存消息队列了。它还因此获得过Oracle官方的Duke大奖。
如此高性能的内存消息队列，在设计和实现上，必然有它独到的地方。今天，我们就来一块儿看下，Disruptor是如何做到如此高性能的？其底层依赖了哪些数据结构和算法？
基于循环队列的“生产者-消费者模型”什么是内存消息队列？对很多业务工程师或者前端工程师来说，可能会比较陌生。不过，如果我说“生产者-消费者模型”，估计大部分人都知道。在这个模型中，“生产者”生产数据，并且将数据放到一个中心存储容器中。之后，“消费者”从中心存储容器中，取出数据消费。
这个模型非常简单、好理解，那你有没有思考过，这里面存储数据的中心存储容器，是用什么样的数据结构来实现的呢？
实际上，实现中心存储容器最常用的一种数据结构，就是我们在第9节讲的队列。队列支持数据的先进先出。正是这个特性，使得数据被消费的顺序性可以得到保证，也就是说，早被生产的数据就会早被消费。
我们在第9节讲过，队列有两种实现思路。一种是基于链表实现的链式队列，另一种是基于数组实现的顺序队列。不同的需求背景下，我们会选择不同的实现方式。
如果我们要实现一个无界队列，也就是说，队列的大小事先不确定，理论上可以支持无限大。这种情况下，我们适合选用链表来实现队列。因为链表支持快速地动态扩容。如果我们要实现一个有界队列，也就是说，队列的大小事先确定，当队列中数据满了之后，生产者就需要等待。直到消费者消费了数据，队列有空闲位置的时候，生产者才能将数据放入。
实际上，相较于无界队列，有界队列的应用场景更加广泛。毕竟，我们的机器内存是有限的。而无界队列占用的内存数量是不可控的。对于实际的软件开发来说，这种不可控的因素，就会有潜在的风险。在某些极端情况下，无界队列就有可能因为内存持续增长，而导致OOM（Out of Memory）错误。
在第9节中，我们还讲过一种特殊的顺序队列，循环队列。我们讲过，非循环的顺序队列在添加、删除数据的工程中，会涉及数据的搬移操作，导致性能变差。而循环队列正好可以解决这个数据搬移的问题，所以，性能更加好。所以，大部分用到顺序队列的场景中，我们都选择用顺序队列中的循环队列。
实际上，循环队列这种数据结构，就是我们今天要讲的内存消息队列的雏形。我借助循环队列，实现了一个最简单的“生产者-消费者模型”。对应的代码我贴到这里，你可以看看。
为了方便你理解，对于生产者和消费者之间操作的同步，我并没有用到线程相关的操作。而是采用了“当队列满了之后，生产者就轮训等待；当队列空了之后，消费者就轮训等待”这样的措施。
public class Queue { private Long[] data; private int size = 0, head = 0, tail = 0; public Queue(int size) { this.data = new Long[size]; this.size = size; } public boolean add(Long element) { if ((tail + 1) % size == head) return false; data[tail] = element; tail = (tail + 1) % size; return true; }</description></item><item><title>55_理解Disruptor（下）：不需要换挡和踩刹车的CPU，有多快？</title><link>https://artisanbox.github.io/4/55/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/55/</guid><description>上一讲，我们学习了一个精妙的想法，Disruptor通过缓存行填充，来利用好CPU的高速缓存。不知道你做完课后思考题之后，有没有体会到高速缓存在实践中带来的速度提升呢？
不过，利用CPU高速缓存，只是Disruptor“快”的一个因素，那今天我们就来看一看Disruptor快的另一个因素，也就是“无锁”，而尽可能发挥CPU本身的高速处理性能。
缓慢的锁Disruptor作为一个高性能的生产者-消费者队列系统，一个核心的设计就是通过RingBuffer实现一个无锁队列。
上一讲里我们讲过，Java里面的基础库里，就有像LinkedBlockingQueue这样的队列库。但是，这个队列库比起Disruptor里用的RingBuffer要慢上很多。慢的第一个原因我们说过，因为链表的数据在内存里面的布局对于高速缓存并不友好，而RingBuffer所使用的数组则不然。
LinkedBlockingQueue慢，有另外一个重要的因素，那就是它对于锁的依赖。在生产者-消费者模式里，我们可能有多个消费者，同样也可能有多个生产者。多个生产者都要往队列的尾指针里面添加新的任务，就会产生多个线程的竞争。于是，在做这个事情的时候，生产者就需要拿到对于队列尾部的锁。同样地，在多个消费者去消费队列头的时候，也就产生竞争。同样消费者也要拿到锁。
那只有一个生产者，或者一个消费者，我们是不是就没有这个锁竞争的问题了呢？很遗憾，答案还是否定的。一般来说，在生产者-消费者模式下，消费者要比生产者快。不然的话，队列会产生积压，队列里面的任务会越堆越多。
一方面，你会发现越来越多的任务没有能够及时完成；另一方面，我们的内存也会放不下。虽然生产者-消费者模型下，我们都有一个队列来作为缓冲区，但是大部分情况下，这个缓冲区里面是空的。也就是说，即使只有一个生产者和一个消费者者，这个生产者指向的队列尾和消费者指向的队列头是同一个节点。于是，这两个生产者和消费者之间一样会产生锁竞争。
在LinkedBlockingQueue上，这个锁机制是通过ReentrantLock这个Java 基础库来实现的。这个锁是一个用Java在JVM上直接实现的加锁机制，这个锁机制需要由JVM来进行裁决。这个锁的争夺，会把没有拿到锁的线程挂起等待，也就需要经过一次上下文切换（Context Switch）。
不知道你还记不记得，我们在第28讲讲过的异常和中断，这里的上下文切换要做的和异常和中断里的是一样的。上下文切换的过程，需要把当前执行线程的寄存器等等的信息，保存到线程栈里面。而这个过程也必然意味着，已经加载到高速缓存里面的指令或者数据，又回到了主内存里面，会进一步拖慢我们的性能。
我们可以按照Disruptor介绍资料里提到的Benchmark，写一段代码来看看，是不是真是这样的。这里我放了一段Java代码，代码的逻辑很简单，就是把一个long类型的counter，从0自增到5亿。一种方式是没有任何锁，另外一个方式是每次自增的时候都要去取一个锁。
你可以在自己的电脑上试试跑一下这个程序。在我这里，两个方式执行所需要的时间分别是207毫秒和9603毫秒，性能差出了将近50倍。
package com.xuwenhao.perf.jmm; import java.util.concurrent.atomic.AtomicLong; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock;
public class LockBenchmark{
public static void runIncrement() { long counter = 0; long max = 500000000L; long start = System.currentTimeMillis(); while (counter &amp;amp;lt; max) { counter++; } long end = System.currentTimeMillis(); System.out.println(&amp;amp;quot;Time spent is &amp;amp;quot; + (end-start) + &amp;amp;quot;ms without lock&amp;amp;quot;); } public static void runIncrementWithLock() { Lock lock = new ReentrantLock(); long counter = 0; long max = 500000000L; long start = System.</description></item><item><title>55_算法实战（四）：剖析微服务接口鉴权限流背后的数据结构和算法</title><link>https://artisanbox.github.io/2/56/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/56/</guid><description>微服务是最近几年才兴起的概念。简单点讲，就是把复杂的大应用，解耦拆分成几个小的应用。这样做的好处有很多。比如，这样有利于团队组织架构的拆分，毕竟团队越大协作的难度越大；再比如，每个应用都可以独立运维，独立扩容，独立上线，各个应用之间互不影响。不用像原来那样，一个小功能上线，整个大应用都要重新发布。
不过，有利就有弊。大应用拆分成微服务之后，服务之间的调用关系变得更复杂，平台的整体复杂熵升高，出错的概率、debug问题的难度都高了好几个数量级。所以，为了解决这些问题，服务治理便成了微服务的一个技术重点。
所谓服务治理，简单点讲，就是管理微服务，保证平台整体正常、平稳地运行。服务治理涉及的内容比较多，比如鉴权、限流、降级、熔断、监控告警等等。这些服务治理功能的实现，底层依赖大量的数据结构和算法。今天，我就拿其中的鉴权和限流这两个功能，来带你看看，它们的实现过程中都要用到哪些数据结构和算法。
鉴权背景介绍以防你之前可能对微服务没有太多了解，所以我对鉴权的背景做了简化。
假设我们有一个微服务叫用户服务（User Service）。它提供很多用户相关的接口，比如获取用户信息、注册、登录等，给公司内部的其他应用使用。但是，并不是公司内部所有应用，都可以访问这个用户服务，也并不是每个有访问权限的应用，都可以访问用户服务的所有接口。
我举了一个例子给你讲解一下，你可以看我画的这幅图。这里面，只有A、B、C、D四个应用可以访问用户服务，并且，每个应用只能访问用户服务的部分接口。
要实现接口鉴权功能，我们需要事先将应用对接口的访问权限规则设置好。当某个应用访问其中一个接口的时候，我们就可以拿应用的请求URL，在规则中进行匹配。如果匹配成功，就说明允许访问；如果没有可以匹配的规则，那就说明这个应用没有这个接口的访问权限，我们就拒绝服务。
如何实现快速鉴权？接口的格式有很多，有类似Dubbo这样的RPC接口，也有类似Spring Cloud这样的HTTP接口。不同接口的鉴权实现方式是类似的，我这里主要拿HTTP接口给你讲解。
鉴权的原理比较简单、好理解。那具体到实现层面，我们该用什么数据结构来存储规则呢？用户请求URL在规则中快速匹配，又该用什么样的算法呢？
实际上，不同的规则和匹配模式，对应的数据结构和匹配算法也是不一样的。所以，关于这个问题，我继续细化为三个更加详细的需求给你讲解。
1.如何实现精确匹配规则？我们先来看最简单的一种匹配模式。只有当请求URL跟规则中配置的某个接口精确匹配时，这个请求才会被接受、处理。为了方便你理解，我举了一个例子，你可以看一下。
不同的应用对应不同的规则集合。我们可以采用散列表来存储这种对应关系。我这里着重讲下，每个应用对应的规则集合，该如何存储和匹配。
针对这种匹配模式，我们可以将每个应用对应的权限规则，存储在一个字符串数组中。当用户请求到来时，我们拿用户的请求URL，在这个字符串数组中逐一匹配，匹配的算法就是我们之前学过的字符串匹配算法（比如KMP、BM、BF等）。
规则不会经常变动，所以，为了加快匹配速度，我们可以按照字符串的大小给规则排序，把它组织成有序数组这种数据结构。当要查找某个URL能否匹配其中某条规则的时候，我们可以采用二分查找算法，在有序数组中进行匹配。
而二分查找算法的时间复杂度是O(logn)（n表示规则的个数），这比起时间复杂度是O(n)的顺序遍历快了很多。对于规则中接口长度比较长，并且鉴权功能调用量非常大的情况，这种优化方法带来的性能提升还是非常可观的 。
2.如何实现前缀匹配规则？我们再来看一种稍微复杂的匹配模式。只要某条规则可以匹配请求URL的前缀，我们就说这条规则能够跟这个请求URL匹配。同样，为了方便你理解这种匹配模式，我还是举一个例子说明一下。
不同的应用对应不同的规则集合。我们采用散列表来存储这种对应关系。我着重讲一下，每个应用的规则集合，最适合用什么样的数据结构来存储。
在Trie树那节，我们讲到，Trie树非常适合用来做前缀匹配。所以，针对这个需求，我们可以将每个用户的规则集合，组织成Trie树这种数据结构。
不过，Trie树中的每个节点不是存储单个字符，而是存储接口被“/”分割之后的子目录（比如“/user/name”被分割为“user”“name”两个子目录）。因为规则并不会经常变动，所以，在Trie树中，我们可以把每个节点的子节点们，组织成有序数组这种数据结构。在匹配的过程中，我们可以利用二分查找算法，决定从一个节点应该跳到哪一个子节点。
3.如何实现模糊匹配规则？如果我们的规则更加复杂，规则中包含通配符，比如“**”表示匹配任意多个子目录，“*”表示匹配任意一个子目录。只要用户请求URL可以跟某条规则模糊匹配，我们就说这条规则适用于这个请求。为了方便你理解，我举一个例子来解释一下。
不同的应用对应不同的规则集合。我们还是采用散列表来存储这种对应关系。这点我们刚才讲过了，这里不再重复说了。我们着重看下，每个用户对应的规则集合，该用什么数据结构来存储？针对这种包含通配符的模糊匹配，我们又该使用什么算法来实现呢？
还记得我们在回溯算法那节讲的正则表达式的例子吗？我们可以借助正则表达式那个例子的解决思路，来解决这个问题。我们采用回溯算法，拿请求URL跟每条规则逐一进行模糊匹配。如何用回溯算法进行模糊匹配，这部分我就不重复讲了。你如果忘记了，可以回到相应章节复习一下。
不过，这个解决思路的时间复杂度是非常高的。我们需要拿每一个规则，跟请求URL匹配一遍。那有没有办法可以继续优化一下呢？
实际上，我们可以结合实际情况，挖掘出这样一个隐形的条件，那就是，并不是每条规则都包含通配符，包含通配符的只是少数。于是，我们可以把不包含通配符的规则和包含通配符的规则分开处理。
我们把不包含通配符的规则，组织成有序数组或者Trie树（具体组织成什么结构，视具体的需求而定，是精确匹配，就组织成有序数组，是前缀匹配，就组织成Trie树），而这一部分匹配就会非常高效。剩下的是少数包含通配符的规则，我们只要把它们简单存储在一个数组中就可以了。尽管匹配起来会比较慢，但是毕竟这种规则比较少，所以这种方法也是可以接受的。
当接收到一个请求URL之后，我们可以先在不包含通配符的有序数组或者Trie树中查找。如果能够匹配，就不需要继续在通配符规则中匹配了；如果不能匹配，就继续在通配符规则中查找匹配。
限流背景介绍讲完了鉴权的实现思路，我们再来看一下限流。
所谓限流，顾名思义，就是对接口调用的频率进行限制。比如每秒钟不能超过100次调用，超过之后，我们就拒绝服务。限流的原理听起来非常简单，但它在很多场景中，发挥着重要的作用。比如在秒杀、大促、双11、618等场景中，限流已经成为了保证系统平稳运行的一种标配的技术解决方案。
按照不同的限流粒度，限流可以分为很多种类型。比如给每个接口限制不同的访问频率，或者给所有接口限制总的访问频率，又或者更细粒度地限制某个应用对某个接口的访问频率等等。
不同粒度的限流功能的实现思路都差不多，所以，我今天主要针对限制所有接口总的访问频率这样一个限流需求来讲解。其他粒度限流需求的实现思路，你可以自己思考。
如何实现精准限流？最简单的限流算法叫固定时间窗口限流算法。这种算法是如何工作的呢？首先我们需要选定一个时间起点，之后每当有接口请求到来，我们就将计数器加一。如果在当前时间窗口内，根据限流规则（比如每秒钟最大允许100次访问请求），出现累加访问次数超过限流值的情况时，我们就拒绝后续的访问请求。当进入下一个时间窗口之后，计数器就清零重新计数。
这种基于固定时间窗口的限流算法的缺点是，限流策略过于粗略，无法应对两个时间窗口临界时间内的突发流量。这是怎么回事呢？我举一个例子给你解释一下。
假设我们的限流规则是，每秒钟不能超过100次接口请求。第一个1s时间窗口内，100次接口请求都集中在最后10ms内。在第二个1s的时间窗口内，100次接口请求都集中在最开始的10ms内。虽然两个时间窗口内流量都符合限流要求（≤100个请求），但在两个时间窗口临界的20ms内，会集中有200次接口请求。固定时间窗口限流算法并不能对这种情况做限制，所以，集中在这20ms内的200次请求就有可能压垮系统。
为了解决这个问题，我们可以对固定时间窗口限流算法稍加改造。我们可以限制任意时间窗口（比如1s）内，接口请求数都不能超过某个阈值（ 比如100次）。因此，相对于固定时间窗口限流算法，这个算法叫滑动时间窗口限流算法。
流量经过滑动时间窗口限流算法整形之后，可以保证任意一个1s的时间窗口内，都不会超过最大允许的限流值，从流量曲线上来看会更加平滑。那具体到实现层面，我们该如何来做呢？
我们假设限流的规则是，在任意1s内，接口的请求次数都不能大于K次。我们就维护一个大小为K+1的循环队列，用来记录1s内到来的请求。注意，这里循环队列的大小等于限流次数加一，因为循环队列存储数据时会浪费一个存储单元。
当有新的请求到来时，我们将与这个新请求的时间间隔超过1s的请求，从队列中删除。然后，我们再来看循环队列中是否有空闲位置。如果有，则把新请求存储在队列尾部（tail指针所指的位置）；如果没有，则说明这1秒内的请求次数已经超过了限流值K，所以这个请求被拒绝服务。
为了方便你理解，我举一个例子，给你解释一下。在这个例子中，我们假设限流的规则是，任意1s内，接口的请求次数都不能大于6次。
即便滑动时间窗口限流算法可以保证任意时间窗口内，接口请求次数都不会超过最大限流值，但是仍然不能防止，在细时间粒度上访问过于集中的问题。
比如我刚刚举的那个例子，第一个1s的时间窗口内，100次请求都集中在最后10ms中，也就是说，基于时间窗口的限流算法，不管是固定时间窗口还是滑动时间窗口，只能在选定的时间粒度上限流，对选定时间粒度内的更加细粒度的访问频率不做限制。
实际上，针对这个问题，还有很多更加平滑的限流算法，比如令牌桶算法、漏桶算法等。如果感兴趣，你可以自己去研究一下。
总结引申今天，我们讲解了跟微服务相关的接口鉴权和限流功能的实现思路。现在，我稍微总结一下。
关于鉴权，我们讲了三种不同的规则匹配模式。不管是哪种匹配模式，我们都可以用散列表来存储不同应用对应的不同规则集合。对于每个应用的规则集合的存储，三种匹配模式使用不同的数据结构。
对于第一种精确匹配模式，我们利用有序数组来存储每个应用的规则集合，并且通过二分查找和字符串匹配算法，来匹配请求URL与规则。对于第二种前缀匹配模式，我们利用Trie树来存储每个应用的规则集合。对于第三种模糊匹配模式，我们采用普通的数组来存储包含通配符的规则，通过回溯算法，来进行请求URL与规则的匹配。
关于限流，我们讲了两种限流算法，第一种是固定时间窗口限流算法，第二种是滑动时间窗口限流算法。对于滑动时间窗口限流算法，我们用了之前学习过的循环队列来实现。比起固定时间窗口限流算法，它对流量的整形效果更好，流量更加平滑。
从今天的学习中，我们也可以看出，对于基础架构工程师来说，如果不精通数据结构和算法，我们就很难开发出性能卓越的基础架构、中间件。这其实就体现了数据结构和算法的重要性。
课后思考 除了用循环队列来实现滑动时间窗口限流算法之外，我们是否还可以用其他数据结构来实现呢？请对比一下这些数据结构跟循环队列在解决这个问题时的优劣之处。
分析一下鉴权那部分内容中，前缀匹配算法的时间复杂度和空间复杂度。
最后，有个消息提前通知你一下。本节是专栏的倒数第二节课了，不知道学到现在，你掌握得怎么样呢？为了帮你复习巩固，做到真正掌握这些知识，我针对专栏涉及的数据结构和算法，精心编制了一套练习题。从正月初一到初七，每天发布一篇。你要做好准备哦！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>56_算法实战（五）：如何用学过的数据结构和算法实现一个短网址系统？</title><link>https://artisanbox.github.io/2/57/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/57/</guid><description>短网址服务你用过吗？如果我们在微博里发布一条带网址的信息，微博会把里面的网址转化成一个更短的网址。我们只要访问这个短网址，就相当于访问原始的网址。比如下面这两个网址，尽管长度不同，但是都可以跳转到我的一个GitHub开源项目里。其中，第二个网址就是通过新浪提供的短网址服务生成的。
原始网址：https://github.com/wangzheng0822/ratelimiter4j 短网址：http://t.cn/EtR9QEG 从功能上讲，短网址服务其实非常简单，就是把一个长的网址转化成一个短的网址。作为一名软件工程师，你是否思考过，这样一个简单的功能，是如何实现的呢？底层都依赖了哪些数据结构和算法呢？
短网址服务整体介绍刚刚我们讲了，短网址服务的一个核心功能，就是把原始的长网址转化成短网址。除了这个功能之外，短网址服务还有另外一个必不可少的功能。那就是，当用户点击短网址的时候，短网址服务会将浏览器重定向为原始网址。这个过程是如何实现的呢？
为了方便你理解，我画了一张对比图，你可以看下。
从图中我们可以看出，浏览器会先访问短网址服务，通过短网址获取到原始网址，再通过原始网址访问到页面。不过这部分功能并不是我们今天要讲的重点。我们重点来看，如何将长网址转化成短网址？
如何通过哈希算法生成短网址？我们前面学过哈希算法。哈希算法可以将一个不管多长的字符串，转化成一个长度固定的哈希值。我们可以利用哈希算法，来生成短网址。
前面我们已经提过一些哈希算法了，比如MD5、SHA等。但是，实际上，我们并不需要这些复杂的哈希算法。在生成短网址这个问题上，毕竟，我们不需要考虑反向解密的难度，所以我们只需要关心哈希算法的计算速度和冲突概率。
能够满足这样要求的哈希算法有很多，其中比较著名并且应用广泛的一个哈希算法，那就是MurmurHash算法。尽管这个哈希算法在2008年才被发明出来，但现在它已经广泛应用到Redis、MemCache、Cassandra、HBase、Lucene等众多著名的软件中。
MurmurHash算法提供了两种长度的哈希值，一种是32bits，一种是128bits。为了让最终生成的短网址尽可能短，我们可以选择32bits的哈希值。对于开头那个GitHub网址，经过MurmurHash计算后，得到的哈希值就是181338494。我们再拼上短网址服务的域名，就变成了最终的短网址http://t.cn/181338494（其中，http://t.cn 是短网址服务的域名）。
1.如何让短网址更短？不过，你可能已经看出来了，通过MurmurHash算法得到的短网址还是很长啊，而且跟我们开头那个网址的格式好像也不一样。别着急，我们只需要稍微改变一个哈希值的表示方法，就可以轻松把短网址变得更短些。
我们可以将10进制的哈希值，转化成更高进制的哈希值，这样哈希值就变短了。我们知道，16进制中，我们用A～F，来表示10～15。在网址URL中，常用的合法字符有0～9、a～z、A～Z这样62个字符。为了让哈希值表示起来尽可能短，我们可以将10进制的哈希值转化成62进制。具体的计算过程，我写在这里了。最终用62进制表示的短网址就是http://t.cn/cgSqq。
2.如何解决哈希冲突问题？不过，我们前面讲过，哈希算法无法避免的一个问题，就是哈希冲突。尽管MurmurHash算法，冲突的概率非常低。但是，一旦冲突，就会导致两个原始网址被转化成同一个短网址。当用户访问短网址的时候，我们就无从判断，用户想要访问的是哪一个原始网址了。这个问题该如何解决呢？
一般情况下，我们会保存短网址跟原始网址之间的对应关系，以便后续用户在访问短网址的时候，可以根据对应关系，查找到原始网址。存储这种对应关系的方式有很多，比如我们自己设计存储系统或者利用现成的数据库。前面我们讲到的数据库有MySQL、Redis。我们就拿MySQL来举例。假设短网址与原始网址之间的对应关系，就存储在MySQL数据库中。
当有一个新的原始网址需要生成短网址的时候，我们先利用MurmurHash算法，生成短网址。然后，我们拿这个新生成的短网址，在MySQL数据库中查找。
如果没有找到相同的短网址，这也就表明，这个新生成的短网址没有冲突。于是我们就将这个短网址返回给用户（请求生成短网址的用户），然后将这个短网址与原始网址之间的对应关系，存储到MySQL数据库中。
如果我们在数据库中，找到了相同的短网址，那也并不一定说明就冲突了。我们从数据库中，将这个短网址对应的原始网址也取出来。如果数据库中的原始网址，跟我们现在正在处理的原始网址是一样的，这就说明已经有人请求过这个原始网址的短网址了。我们就可以拿这个短网址直接用。如果数据库中记录的原始网址，跟我们正在处理的原始网址不一样，那就说明哈希算法发生了冲突。不同的原始网址，经过计算，得到的短网址重复了。这个时候，我们该怎么办呢？
我们可以给原始网址拼接一串特殊字符，比如“[DUPLICATED]”，然后再重新计算哈希值，两次哈希计算都冲突的概率，显然是非常低的。假设出现非常极端的情况，又发生冲突了，我们可以再换一个拼接字符串，比如“[OHMYGOD]”，再计算哈希值。然后把计算得到的哈希值，跟原始网址拼接了特殊字符串之后的文本，一并存储在MySQL数据库中。
当用户访问短网址的时候，短网址服务先通过短网址，在数据库中查找到对应的原始网址。如果原始网址有拼接特殊字符（这个很容易通过字符串匹配算法找到），我们就先将特殊字符去掉，然后再将不包含特殊字符的原始网址返回给浏览器。
3.如何优化哈希算法生成短网址的性能？为了判断生成的短网址是否冲突，我们需要拿生成的短网址，在数据库中查找。如果数据库中存储的数据非常多，那查找起来就会非常慢，势必影响短网址服务的性能。那有没有什么优化的手段呢？
还记得我们之前讲的MySQL数据库索引吗？我们可以给短网址字段添加B+树索引。这样通过短网址查询原始网址的速度就提高了很多。实际上，在真实的软件开发中，我们还可以通过一个小技巧，来进一步提高速度。
在短网址生成的过程中，我们会跟数据库打两次交道，也就是会执行两条SQL语句。第一个SQL语句是通过短网址查询短网址与原始网址的对应关系，第二个SQL语句是将新生成的短网址和原始网址之间的对应关系存储到数据库。
我们知道，一般情况下，数据库和应用服务（只做计算不存储数据的业务逻辑部分）会部署在两个独立的服务器或者虚拟服务器上。那两条SQL语句的执行就需要两次网络通信。这种IO通信耗时以及SQL语句的执行，才是整个短网址服务的性能瓶颈所在。所以，为了提高性能，我们需要尽量减少SQL语句。那又该如何减少SQL语句呢？
我们可以给数据库中的短网址字段，添加一个唯一索引（不只是索引，还要求表中不能有重复的数据）。当有新的原始网址需要生成短网址的时候，我们并不会先拿生成的短网址，在数据库中查找判重，而是直接将生成的短网址与对应的原始网址，尝试存储到数据库中。如果数据库能够将数据正常写入，那说明并没有违反唯一索引，也就是说，这个新生成的短网址并没有冲突。
当然，如果数据库反馈违反唯一性索引异常，那我们还得重新执行刚刚讲过的“查询、写入”过程，SQL语句执行的次数不减反增。但是，在大部分情况下，我们把新生成的短网址和对应的原始网址，插入到数据库的时候，并不会出现冲突。所以，大部分情况下，我们只需要执行一条写入的SQL语句就可以了。所以，从整体上看，总的SQL语句执行次数会大大减少。
实际上，我们还有另外一个优化SQL语句次数的方法，那就是借助布隆过滤器。
我们把已经生成的短网址，构建成布隆过滤器。我们知道，布隆过滤器是比较节省内存的一种存储结构，长度是10亿的布隆过滤器，也只需要125MB左右的内存空间。
当有新的短网址生成的时候，我们先拿这个新生成的短网址，在布隆过滤器中查找。如果查找的结果是不存在，那就说明这个新生成的短网址并没有冲突。这个时候，我们只需要再执行写入短网址和对应原始网页的SQL语句就可以了。通过先查询布隆过滤器，总的SQL语句的执行次数减少了。
到此，利用哈希算法来生成短网址的思路，我就讲完了。实际上，这种解决思路已经完全满足需求了，我们已经可以直接用到真实的软件开发中。不过，我们还有另外一种短网址的生成算法，那就是利用自增的ID生成器来生成短网址。我们接下来就看一下，这种算法是如何工作的？对于哈希算法生成短网址来说，它又有什么优势和劣势？
如何通过ID生成器生成短网址？我们可以维护一个ID自增生成器。它可以生成1、2、3…这样自增的整数ID。当短网址服务接收到一个原始网址转化成短网址的请求之后，它先从ID生成器中取一个号码，然后将其转化成62进制表示法，拼接到短网址服务的域名（比如http://t.cn/）后面，就形成了最终的短网址。最后，我们还是会把生成的短网址和对应的原始网址存储到数据库中。
理论非常简单好理解。不过，这里有几个细节问题需要处理。
1.相同的原始网址可能会对应不同的短网址每次新来一个原始网址，我们就生成一个新的短网址，这种做法就会导致两个相同的原始网址生成了不同的短网址。这个该如何处理呢？实际上，我们有两种处理思路。
第一种处理思路是不做处理。听起来有点无厘头，我稍微解释下你就明白了。实际上，相同的原始网址对应不同的短网址，这个用户是可以接受的。在大部分短网址的应用场景里，用户只关心短网址能否正确地跳转到原始网址。至于短网址长什么样子，他其实根本就不关心。所以，即便是同一个原始网址，两次生成的短网址不一样，也并不会影响到用户的使用。
第二种处理思路是借助哈希算法生成短网址的处理思想，当要给一个原始网址生成短网址的时候，我们要先拿原始网址在数据库中查找，看数据库中是否已经存在相同的原始网址了。如果数据库中存在，那我们就取出对应的短网址，直接返回给用户。
不过，这种处理思路有个问题，我们需要给数据库中的短网址和原始网址这两个字段，都添加索引。短网址上加索引是为了提高用户查询短网址对应的原始网页的速度，原始网址上加索引是为了加快刚刚讲的通过原始网址查询短网址的速度。这种解决思路虽然能满足“相同原始网址对应相同短网址”这样一个需求，但是是有代价的：一方面两个索引会占用更多的存储空间，另一方面索引还会导致插入、删除等操作性能的下降。
2.如何实现高性能的ID生成器？实现ID生成器的方法有很多，比如利用数据库自增字段。当然我们也可以自己维护一个计数器，不停地加一加一。但是，一个计数器来应对频繁的短网址生成请求，显然是有点吃力的（因为计数器必须保证生成的ID不重复，笼统概念上讲，就是需要加锁）。如何提高ID生成器的性能呢？关于这个问题，实际上，有很多解决思路。我这里给出两种思路。
第一种思路是借助第54节中讲的方法。我们可以给ID生成器装多个前置发号器。我们批量地给每个前置发号器发送ID号码。当我们接受到短网址生成请求的时候，就选择一个前置发号器来取号码。这样通过多个前置发号器，明显提高了并发发号的能力。
第二种思路跟第一种差不多。不过，我们不再使用一个ID生成器和多个前置发号器这样的架构，而是，直接实现多个ID生成器同时服务。为了保证每个ID生成器生成的ID不重复。我们要求每个ID生成器按照一定的规则，来生成ID号码。比如，第一个ID生成器只能生成尾号为0的，第二个只能生成尾号为1的，以此类推。这样通过多个ID生成器同时工作，也提高了ID生成的效率。
总结引申今天，我们讲了短网址服务的两种实现方法。我现在来稍微总结一下。
第一种实现思路是通过哈希算法生成短网址。我们采用计算速度快、冲突概率小的MurmurHash算法，并将计算得到的10进制数，转化成62进制表示法，进一步缩短短网址的长度。对于哈希算法的哈希冲突问题，我们通过给原始网址添加特殊前缀字符，重新计算哈希值的方法来解决。
第二种实现思路是通过ID生成器来生成短网址。我们维护一个ID自增的ID生成器，给每个原始网址分配一个ID号码，并且同样转成62进制表示法，拼接到短网址服务的域名之后，形成最终的短网址。
课后思考 如果我们还要额外支持用户自定义短网址功能（http//t.cn/{用户自定部分}），我们又该如何改造刚刚的算法呢?
我们在讲通过ID生成器生成短网址这种实现思路的时候，讲到相同的原始网址可能会对应不同的短网址。针对这个问题，其中一个解决思路就是，不做处理。但是，如果每个请求都生成一个短网址，并且存储在数据库中，那这样会不会撑爆数据库呢？我们又该如何解决呢？
今天是农历的大年三十，我们专栏的正文到这里也就全部结束了。从明天开始，我会每天发布一篇练习题，内容针对专栏涉及的数据结构和算法。从初一到初七，帮你复习巩固所学知识，拿下数据结构和算法，打响新年进步的第一枪！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>FAQ第一期_学与不学，知识就在那里，不如就先学好了</title><link>https://artisanbox.github.io/4/57/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/57/</guid><description>你好，我是徐文浩。专栏上线三个多月，我们已经进入后半段。
首先，恭喜跟到这里的同学，很快你就可以看到胜利的曙光了。如果你已经掉队了，不要紧，现在继续依然来得及。
其次，非常感谢同学们的积极留言，看到这么多人因为我的文章受到启发、产生思考，我也感到非常开心。因此，我特意把留言区中非常棒的、值得反复阅读和思考的内容，摘录出来，供你反复阅读学习。
有些内容你可能已经非常熟悉了，但是随着工作、学习经验的不同，相信你的理解也会不一样；有些内容可能刚好也是你的疑问，但是你还没发现，这里说不定就帮你解决了。
今天第一期，我们先来聊聊，“学习”这件事。我准备了五个问题，话不多说，一起来看看吧！
Q1：“要不要学”和“学不会怎么办”系列专栏已经更新三个多月了，但是我估计很多人还是停留在前面3篇的学习上（我相信，一定有的）。
我观察了一下，其实很多人并不是真的学不会，而是“不敢学”，往往还没开始就被自己给吓到了。很多优秀的人，并非真的智商有多么高，而是他们敢于尝试，敢于突破自己的舒适区。
所以说，学习底层知识或者新知识的第一点，就是要“克服恐惧”，其实大部分东西上手了都不难，都很有意思。就像《冰与火之歌》里面，水舞者教导艾莉亚时的情况一样，“恐惧比利剑”更伤人。破除对于基础知识“难”的迷信，是迈向更高水平必经的一步。
“组成原理可以算是理解计算机运作机制的第一门入门课，这门课的交付目标就是让科班的同学们能够温故而知新，为非科班的同学们打开深入学习计算机核心课程的大门。”这是我在专栏刚上线的时候给一个同学的留言回复，现在拿过来再给你说一遍。
另外，大家在学校里学这些课程的时候，都会遇到一个问题，那就是理论和我们的编程应用实践离得比较远。在这个专栏里，我的目标是让大家能够更“实践”地去学习计算机组成原理。
所以，这门课我的目标就是尽量讲得“理论和实践相结合”，能和你的日常代码工作结合起来。让非科班的同学们也能学习到计算机组成原理的知识，所以在深入讲解知识点之外，我会尽量和你在开发过程中可能遇到的问题放到一块儿，只要跟着课程的节奏走，不会跟不上哦。（跟到这里的同学可以在留言区冒个泡，给跟不上的同学招个手，让他们放心大胆看过来。）
我自己在大学的时候也不是个“好学生”。现在回头看，我自己常常觉得大学的时候没有好好读书，浪费了很多时间。常常想，当时要是做了就好了。当时，不是就是现在么？学或者不学，知识就在那里，不如就先学好了啊。
Q2：“计算机组成原理”和“操作系统”到底有啥不一样？其实操作系统也是一个“软件”，而开发操作系统，只需要关注到“组成原理”或者“体系结构”就好了，不需要真的了解硬件。操作系统，其实是在“组成原理”所讲的“指令集”上做一层封装。
体系结构、操作系统、编译原理以及计算机网络，都可以认为是组成原理的后继课程。体系结构不是一个系统软件，它更多地是讲，如何量化地设计和研究体系结构和指令集。操作系统、编译原理和计算机网络都是基于体系结构之上的系统软件。
其实这几门基础学科，都是环环相扣，相互渗透的，每一门课都不可能独立存在。不知道你现在是否明白这几门基础学科的价值呢？
Q3：“图灵机”和“冯·诺依曼机”的区别首先，先回答一下这道题本身。有些同学已经回答的不错。我把他们的答案贴在这里。你可以看看跟你想的是不是一样。
Amanda 同学：
两者有交叉但是不同，根据了解整理如下：
图灵机是一种思想模型（计算机的基本理论基础），是一种有穷的、构造性的问题的求解思路，图灵认为凡是能用算法解决的问题也一定能用图灵机解决；
冯·诺依曼提出了“存储程序”的计算机设计思想，并“参照”图灵模型设计了历史上第一台电子计算机，即冯·诺依曼机。
图灵机其实是一个很有意思的话题。我上大学的时候，对应着图灵机也有一门课程，叫作“可计算性理论”，其实就是告诉我们什么样的问题是计算机解决得了的，什么样的问题是它解决不了的。
在我看来，图灵机就是一个抽象的“思维实验”，而冯·诺依曼机就是对应着这个“思维实验”的“物理实现”。如果我们把“图灵机”当成“灵魂”，代表计算机最抽象的本质，那么“冯诺伊曼机”就是“肉体”，代表了计算机最具体的本质。这两者之间颇有理论物理学家和实验物理学家的合作关系的意思，可谓是一个问题的两面。
冯·诺依曼体系结构距今已经几十年了，目前，我们还没有看到真正颠覆性的新的体系结构出现，更多地是针对硬件变化和应用场景变化的优化。但是过去几年随着深度学习、IoT等的发展，体系结构又有了一波新的大发展，也许未来会有新的变化呢，我们可以拭目以待。
Q4：工作多年，如何保持对知识清晰、准确的认识？我之前跟很多人聊过，发现工作很多年之后的工程师，在计算机科学的基础知识上，反而比不上很多应届的同学。我总结下来，大概有这么几个因素。
首先，很多工程师只是满足于工作的需求被满足了，没有真的深入去搞清楚一个问题的原理。从网络上搜索一段代码，复制粘贴到自己的程序里，只要能跑就认为问题解决了，并没有深入一行行看明白每行代码到底是做了什么，为什么要这么做。
比如说，我们现在要提升RPC和序列化的性能，很多人的做法是，找一个教程用一下Thrift这样的开源框架，解决眼下的问题就完事儿。至于，Thrift是怎么序列化的，每一种里面支持的RPC协议是怎么回事儿，完全不清楚。其实这些开源代码并不复杂，稍微花点时间，搞清楚里面的实现细节和原理，你对二进制存储、程序性能、网络性能，就会有一个更深刻的认识，之后遇到类似的问题你就不会再一问三不知，久而久之你的能力就会得到提升。
其次，读书的时候我们认为一个东西掌握扎实了，有时候其实未必。很多人估计都有感受，像计算机这类实践性比较强的专业，书上所学和真正实践中所用完全是两码事。背出计算机的五大组成部分，似乎和我们的实际应用没有联系，但是在实际的系统开发过程中，无论是内存地址转换使用的页表树这样的数据结构，还是各个系统组件间通过总线进行通信的模式，其实都可以和我们自己的应用系统开发里的模式和思路联系起来。
至于究竟该怎么去掌握知识，其实没有什么特别好的方法。我就说说我一般会怎么做，一方面，遇到疑难问题、复杂的系统时，必须要用更底层更本质的理解计算机运作的方式，去处理问题，自然会去回头把这些基础知识捡起来；另一方面，时不时抽点时间回头看看一些“大部头”的教科书，对我自己而言，本身就很有自我满足感，而这种自我满足感也会促使我不断去读它们，从而形成一个良性循环。
Q5：六个最实用的、督促自己学习的办法看到很多同学在留言里分享了自己学习方法，我看了也非常受益，我把这些方法筛选总结了一下，又结合我自己的学习经验，放在这里分享给你。
好奇心是一个优秀程序员必然要有的特质。多去想想“为什么是这样的”，有助于你更深入地掌握这些知识点。 先了解知识面，再寻找自己有兴趣的点深入，学习也是个反复迭代的过程。 带着问题去学习是最快的成长方式之一。彻底搞清楚实际在开发过程中遇到的困难的问题，而不是只满足于功能问题被实现和解决，是提升自己的必经之路。 “教别人”是一种非常高效的学习方式，自己有没有弄清楚，在教别人的过程中，会体会得明明白白。 每个月给自己投资100-200块在专业学习上面，这样花了钱，通过外部约束，也是一个让自己坚持下去的好办法。 坚持到底就是胜利✌️。把学习和成长变成一种习惯，这个习惯带来的惯性会让你更快地成长。 好了，到这里，我们第一期答疑就要结束了。这次我主要和你谈了谈“学习”这个话题，不知道你有什么感受呢？你还想听我和你聊什么专栏之外的话题呢？
欢迎积极留言给我。如果觉得这篇文章对你有帮助，也欢迎你收藏并分享给你的朋友。对了，看到这里的同学，记得在留言区给后面的同学招个手啊：）
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>FAQ第二期_世界上第一个编程语言是怎么来的？</title><link>https://artisanbox.github.io/4/56/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/56/</guid><description>你好，我是徐文浩，今天是第二期FAQ，我搜集了第3讲到第6讲，大家在留言区问的比较多的问题，来做一次集中解答。
有些问题，可能你已经知道了答案，不妨看看和我的理解是否一样；如果这些问题刚好你也有，那可要认真看啦！
希望今天的你，也同样有收获！
Q1：为什么user + sys运行出来会比real time多呢？我们知道，实际的计算机运行的过程中，CPU会在多个不同的进程里面切换，分配不同的时间片去执行任务。所以，运行一个程序，在现实中走过的时间，并不是实际CPU运行这个程序所花费的时间。前者在现实中走过的时间，我们叫作real time。有时候叫作wall clock time，也就是墙上挂着的钟走过的时间。
而实际CPU上所花费的时间，又可以分成在操作系统的系统调用里面花的sys time和用户态的程序所花的user time。如果我们只有一个CPU的话，那real time &amp;gt;= sys time + user time 。所以，我当时在文章里给大家看了对应的示例。
不过，有不少同学运行出来的结果不是这样的。这是因为现在大家都已经用上多核的CPU了。也就是同一时间，有两个CPU可以同时运行任务。
你在一台多核或者多CPU的机器上运行，seq和wc命令会分配到两个CPU上。虽然seq和wc这两个命令都是单线程运行的，但是这两个命令在多核CPU运行的情况下，会分别分配到两个不同的CPU。
于是，user和sys的时间是两个CPU上运行的时间之和，这就可能超过real的时间。而real只是现实时钟里走过的时间，极端情况下user+sys可以到达real的两倍。
你可以运行下面这个命令，快速验证。让这个命令多跑一会儿，并且在后台运行。
time seq 100000000 | wc -l &amp;amp; 然后，我们利用top命令，查看不同进程的CPU占用情况。你会在top的前几行里看到，seq和wc的CPU占用都接近100，实际上，它们各被分配到了一个不同的CPU执行。
我写这篇文章的时候，测试时只开了一个1u的最小的虚拟机，只有一个CPU，所以不会遇到这个问题。
Q2：时钟周期时间和指令执行耗时有直接关系吗？这个问题提的得非常好，@易儿易 同学的学习和思考都很仔细、深入。
“晶振时间与CPU执行固定指令耗时成正比”，这个说法更准确一点。我们为了理解，可以暂且认为，是晶振在触发一条一条电路变化指令。这就好比你拨算盘的节奏一样。算盘拨得快，珠算就算得快。结果就是，一条简单的指令需要的时间就和一个时钟周期一样。
当然，实际上，这个问题要比这样一句话复杂很多。你可以仔细去读一读专栏关于CPU的章节呢。
从最简单的单指令周期CPU来说，其实时钟周期应该是放下最复杂的一条指令的时间长度。但是，我们现在实际用的都没有单指令周期CPU了，而是采用了流水线技术。采用了流水线技术之后，单个时钟周期里面，能够执行的就不是一个指令了。我们会把一条机器指令，拆分成很多个小步骤。不同的指令的步骤数量可能还不一样。不同的步骤的执行时间，也不一样。所以，一个时钟周期里面，能够放下的是最耗时间的某一个指令步骤。
这样的话，单看一条指令，其实一定需要很多个时钟周期。也就是说，从响应时间的角度来看，一个时钟周期一定是不够执行一条指令的。但是呢，因为有流水线，我们同时又会去执行很多个指令的不同步骤。再加上后面讲的像超线程技术等等，从吞吐量的角度来看，我们又能够做到，平均一个时钟周期里面，完成指令数可以超过1。
想要准确理解CPU的性能问题，请你一定去仔细读一读专栏的整个CPU的部分啊。
Q3：为什么低压主频只有标压的2/3？计算向量点积的时候，怎么提高性能？低压和低主频都是为了减少能耗。比如Surface Go的电池很小，机器的尺寸也很小。如果用上高主频，性能更好了，但是耗电并没有下来。
另外，低电压对于CPU的工艺有更高的要求，因为太低的电压可能导致电路都不能导通，要高主频一样对工艺有更高的要求。所以一般低压CPU都是通过和低主频配合，用在对于移动性和续航要求比较高的机器上。
向量计算是可以通过让加法也并行来优化的，不过真实的CPU里面其实是通过SIMD指令来优化向量计算的，我在后面也会讲到SIMD指令。
Q4：世界上第一个编程语言是怎么来的？如果你去计算机历史博物馆看一下真机，就会明白，第一台通用计算机ENIAC，它的各种输入都是一些旋钮，可以认为是类似用机器码在编程，后来才有了汇编语言、C语言这样越来越高级的语言。
编程语言是自举的，指的是说，我们能用自己写出来的程序编译自己。但是自举，并不要求这门语言的第一个编译器就是用自己写的。
比如，这里说到的Go，先是有了Go语言，我们通过C++写了编译器A。然后呢，我们就可以用这个编译器A，来编译Go语言的程序。接着，我们再用Go语言写一个编译器程序B，然后用A去编译B，就得到了Go语言写好的编译器的可执行文件了。
这个之后，我们就可以一直用B来编译未来的Go语言程序，这也就实现了所谓的自举了。所以，即使是自举，也通常是先有了别的语言写好的编译器，然后再用自己来写自己语言的编译器。
更详细的关于鸡蛋问题，可以直接看Wikipedia上这个链接，里面讲了多种这个问题的解决方案。
Q5：不同指令集中，汇编语言和机器码的关系怎么对应的？不同指令集里，对应的汇编代码会对应这个指令集的机器码呀。大家不要把“汇编语言”当成是像C一样的一门统一编程语言。
“汇编语言”其实可以理解成“机器码”的一种别名或者书写方式，不同的指令集和体系结构的机器会有不同的“机器码”。
高级语言在转换成为机器码的时候，是通过编译器进行的，需要编译器指定编译成哪种汇编/机器码。
物理机自己执行的时候只有机器码，并不认识汇编代码。
编译器如果支持编译成不同的体系结构的汇编/机器码，就要维护很多不同的对应关系表，但是这个表并不会太大。以最复杂的Intel X86的指令集为例，也只有2000条不同的指令而已。
Q6：某篇文章大段大段读不懂怎么办？@胖胖胖 同学说得很好。在专栏最开始几篇，或者到后面比较深入的文章，很多非科班的或者基础不太好的同学，会觉得读不下去，甚至很多地方看不懂。这些其实都是正常现象。
即便我在写的时候，已经尽可能考虑得比较完善，照顾大家的情况，但是肯定无法面面俱到。在我平时学习过程遇到拦路虎的时候，我一般有两种方法，这里跟你分享一下。
第一种，硬读。
你可能说了，这也叫方法吗？没错，事实就是这样。如果这个知识点，我必须要攻克，就想要搞明白，那我就会尽我所能，去看每一个字眼，把每个不理解的地方，都一点一点搞明白。不吝啬花费时间和精力。
当然这种情况适合我对这个内容完全不了解，或者已经基本了解，现在需要进一步提升的情况下。因为，在完全不了解一个知识的时候，这个壁垒是很高的。如果不想办法突破的话，那可能就没办法了解这个新的领域。而在已经基本了解某个领域或者某块知识的情况下，我去攻克一些更高难度的知识，很多时候也需要同样的方法，我会建立在兴趣的基础上去硬读，但是之后会非常非常有成就感。
第二种，先抓主要矛盾，再抓细节问题。
很多时候，大家在对一个知识不了解的时候，会感觉很“恐慌”。其实完全没必要，大家学任何东西都是从不会到会这么一个过程。就像@胖胖胖 同学说的那样，先找出这篇文章的主干，先对这些东西有个大致的概念。如果有需要，在之后的过程中，你还会碰到，你可以再重读，加深印象。
有时候，学习知识可以尝试“短期多次”。也就是说，看完一遍之后，如果不明白，先放下，过一段时间再看一遍，如果还不明白，再过一段时间再看。这样循环几次，在大脑中发酵几次，说不定就明白了，要给大脑一个缓冲的时间。
好了，今天的答疑到这里就结束了。不知道能否帮你解决了一些疑惑和问题呢？
我会持续不断地回复留言，并把比较好的问题精选出来，作为答疑。欢迎你继续在留言区留言，和大家一起交流学习。</description></item><item><title>“他山之石”｜Sugar：这门课你可以试试这么学</title><link>https://artisanbox.github.io/3/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/1/</guid><description>你好，我是这门课程的助教 Sugar，曾供职于百度，现就职于某大型互联网公司，是一名软件工程师。和你一样，我也是一名编译技术的爱好者。
我们的课程更新到今天，已经过半了，不知道你学习得怎么样呢？有没有卡在哪个知识点的实现上？不要担心，如果你有任何的问题，除了在留言区留言外，还可以添加我们的微信交流群，直接 @ 我或者是宫老师，或者是其他志同道合的同学们，我们都会来帮你解决。
回归正题，今天，刚好是国庆假期，我们第一部分的起步篇也更新完了。我们就先停下来休息休息，夯实基础。在这篇分享里，我想跟你聊聊我对编译技术的看法。我也会从我个人的角度，给你总结一下已更新完的起步篇都讲了什么，以及在日常工作中，我们又可以从哪些方向把这门课学到的知识落到实践中来。最开始，我们聊聊为什么要学习编译技术这个话题。
我为什么推荐你学习编译技术？我大概是在2014年入行互联网行业的，一晃就是7年。这7年间，很多公司的口号和观点都从“从PC业务向移动端转型”，变成了“国内移动端用户饱和，期待其他新兴领域带来业务增量”。科技行业发展之快，令人欣喜也叫人唏嘘。
不得不说，作为一名计算机专业科班出身的同学，我很惭愧。我其实是在走上工作岗位后，才对作为本科必修课的编译原理有了真正的了解，也真正感受到了这个技术领域独特的魅力。所以现在，我也推荐你关注这项技术。
为什么我会推荐你学习编译技术呢？如果你一定要问我理由，我想给你分享两点。
第一点，网红热点的“花开花落”，远不如底层技术的“静水流深”有力量。
在我还是一名“产品工程师”（Product Engineer）的时候，我从事过客户端、前端、服务端，以及一些面向业务应用的算法方面的工作。
近些年，每一个技术领域都涌现过各种新概念、新趋势，这可能会让你陷入“学习了这门新技术，就能分享这种技术形态的成长红利，提升自我价值”的错觉。我也曾追赶过这些“技术时尚”：做后端时一会儿学学这个语言，一会儿瞧瞧那个中间件；做前端时，一会儿用用这个框架，一会儿看看那个三方库。
但冷静下来，你就会发现，这些“网红型”技术热点，能够沉淀下来、为业务长期赋能的寥寥无几。真正值得长期学习和不断实践的，反倒是计算机专业的那些基础课，包括但不限于数据结构与算法、计算机网络、操作系统、编译原理、计算机体系结构，还有软件工程等等。这些课程你在学校里学习的时候，可能感觉枯燥乏味、过于抽象，甚至毫无成就感，然而在工作以后你才会发现，这可能是一名 coder 应对“行业内卷”最坚实有力的后盾。
第二点，技术上卡的“脖子”也正意味着更多的机会和可能。
其实，近两年来，我们国内IT行业很多龙头企业都遭遇了“技术卡脖子”的情况。一时间，芯片、操作系统以及其他基础软件国产化的呼声，都起来了。但是，在我看来，这些“low-level-stuff”需要的不止是时间与金钱的投入，更需要技术人才的“梯队化”。
国外的许多大公司里，你都能见到50多岁，甚至年龄更大的资深程序员。他们往往不是从事一些面向用户的业务逻辑开发，而是做一些被称为 Infrastructure 的基础架构工作。我们国内这样的趋势还不够明朗，毕竟我们起步太晚，做这些基础技术的工作，又需要公司雄厚且稳定的营收去支撑。这个差距是显而易见的，甚至短期内都没法快速弥补上。
我曾经就遇到过一个Google V8引擎方面的技术难题。我当时是在一个C++的程序中集成了 V8，通过 V8 的 ObjectTemplate 和 HandleScope 等优化尽可能快速生成 jsObject，并传递到 V8-Isolate 内部的 jsContext 里。但我用尽了所有方法，依然达不到原生 JavaScript 中字面量 Object 的性能。我跟身边许多架构师同事进行了探讨，也通过e-mail和V8项目的一些参与者进行了交流。但是我发现，相比起国外的技术社区，国内工程师们能给出的一些建议确实非常有限。
当然，我不认为这是技术水平、或者是智力这些因素造成的，事实上，国内有大量非常聪明，而且比国外更加勤劳（内卷所致）的软件工程师。在我看来，造成这个现象的原因，主要是 V8 项目的历史实在太久远了，而且早期的核心开发人员又有许多来自于历史更久远的 Java 虚拟机。整个过程具有很强的“技术继承性”，国内的工程师很难有机会，真正深入地了解这些系统的技术内幕。
或许，这就是我们在很多核心技术领域被“卡脖子”现象的主要原因之一。不过，困境也就意味者突破和机会。在底层软硬件国产化的浪潮下，编译器和操作系统是两座绕不过、躲不开的大山，国产芯片也会创造出大量让编译技术大放异彩的机会。在可预见的未来5～10年内，国内的这个技术趋势都是存在的。所以，我看好编译技术、操作系统等这些技术领域的发展，也推荐你深入学习这些底层技术。
起步篇讲了什么？前面聊完了“为什么”，现在我们就来解析一下“是什么”，聊聊我们这门课已更新完的起步篇里，都讲了什么。
不过，在这里我希望你能理解一点，当前这门课程是宫老师讲编译技术的第三季课程了，所以不可避免地存在着一定的知识继承性和延续性。对于编译器的前端部分，这次的课程中讲得相对没有那么深入。如果你想深入了解前端的知识，我建议你去看看第一季《编译原理之美》和第二季《编译原理实战课》。第三季的重点是放在了编译器后端部分，和物理机、操作系统打交道。
如果你看到我梳理的概念中，有很多是你无法理解的，你也可以带着疑问，试着把整个流程串起来。有了一个整体的“大局观”之后，再回头去第一季和第二季中找寻答案，当然也可以在我们的微信交流群里提问。
编译器是一个工业级的基础软件，因此从理论体系上我们就将编译器分成了前端、中端和后端三部分（有些文献上也把中端算为后端）。
你在学习中也会发现，宫老师起步篇的安排，也是按照这个顺序：02是讲词法分析，03和04的前半部分是讲语法分析，后面的04到06的部分则是循序渐进地把编译器前端的语义分析和语法分析的功能，拆成一个个具体的 feature 一点一点放到我们的示范程序中来完成、实现。
07到11节部分呢，是有关虚拟机的话题。严格意义上，其实虚拟机相关的技术并不算是传统的编译原理范畴。在编译技术的三大圣经（《龙书》、《虎书》和《鲸书》）里有关虚拟机、垃圾回收等方面的篇幅少之又少。不过这也是因为历史的局限性，毕竟 Java 这样的语言在1995年才诞生。不过我们的课程却是与时俱进的，在读到宫老师的这部分内容时候，我眼前一亮。
接下来的11-12两节课呢，是一些基础知识的铺垫。这里涉及到编译器与操作系统、和计算机硬件之间“打交道”时的一些“责任边界”。后面的14-18节，则是对编译器后端技术的实践。由于我们课程的受众大部分是软件工程师，所以在14和15两节课，宫老师又花了不少篇幅为大家科普芯片指令集的一些基础知识。
在我看来，理解芯片和汇编语言有一个很好的方法，就是把芯片看成我们中学时期用过的“科学计算器”，甚至是更简单的“日常使用的普通计算器”。唯一的不同之处就是，芯片没有给人类手指去触摸的按钮，取而代之的是需要用程序通过一组组汇编代码去操纵这个“超级微型却功能强大的计算器”。希望我这样的描述，能减少你对芯片指令集的陌生感、缓解你对“超纲知识”的恐惧。
起步篇最后的19-21这三节课呢，是对一些难点知识的精讲。如果你的基础不牢固，我建议你优先学习前面的知识内容。除此之外，我还为你整理了一张脑图，帮你“高亮”出了一些学习这门课有必要弄懂的关键概念：
动手实践才是目的理清了我们起步篇的内容，最后我们聊聊在日常工作中，我们可以从哪些方向把这门课学到的知识落到实践中来。我想从我从事过的前端、客户端、服务端和算法这四个软件工程师岗位，给你讲讲我是如何在日常工作中实践编译技术的，希望能对你有一些参考价值。
平心而论，我非常建议你，把课程作为自己学习的一个起点而不是终点。只有你真正实践过，你才能真正明白为什么大部分语言的前端都在依靠手写递归下降+算符优先级算法的组合去实现，而鲜有教科书上那样设计精巧的LL算法实现（因为first和follow集的维护成本太高了！）。
领域一：前端你可能会问，前端领域真的有必要，学习编译原理这样的技术吗？我理解，毕竟很多前端的同学，每天的工作就是机械地进行设计稿（PS、Sketch等生成的文件）到 HTML+ CSS 代码的转换。但你可能忽略了一些我们前端每天都在使用的构建工具，比如webpack、Rollup，或者是近两年涌现出的ESbuild、swc等等，这些恰恰是我在从事前端工作期间，认为最有意思的一些infra类的工作。
而且，我们这门课也使用了 JavaScript/TypeScript 语言作为教学工具。如果你就从事前端，那我非常建议你把上面这些构建工具作为自己的研究目标，像电影《速度与激情》里的剧情一样，把自己每天开的“车子”拿过来拆开看看，动手改装“魔改”一番，这会是一件非常有乐趣和成就感的事。
另外，从 Typescript 到 Wasm 这些新工具、新技术的出现也能看出，前端是最有可能在近几年内，因为编译技术而出现新变革的技术领域。如何设计出一种在开发阶段可以使用JavaScript技术栈、而在运行时又能提供尽可能像C++一样高性能的编程语言工具链，将成为业界的一个关键课题。</description></item><item><title>“屠龙之秘”｜实现计算机语言这样的技术能用在哪里？（一）</title><link>https://artisanbox.github.io/3/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/2/</guid><description>你好，我是宫文学。
在学习这么多硬知识的间隙里，我给你准备了一些相对轻松一点的内容，想让你转换一下，让大脑休息休息。
不知道你在学习这门课的时候，有没有这样的困惑：实现计算机语言这样的“屠龙技”，到底在哪里能够发挥作用呢？毕竟，不是每个人都有机会成为像Java、JavaScript这样的通用语言的发明者的。
我的答案是：其实在任何软件领域，只要你做得足够深，其实都能用上这门课程的知识。我也一直在给你传达一个理念：任何好的软件，其实到最后都会变成一个开发平台，所以也就需要用到实现计算机语言的这些相关技术。
所以，在这里，我想分成两节课或者更多节课，给你介绍一些有意思的应用场景，希望你能从中得到启发，在自己的工作中也能更好地运用我们这节课的知识，做出一些耀眼的成绩。
今天这节课，我会跟你分享自动化控制领域（或者简称自控领域），对软件编程的需求，以及如何设计一个开发平台来满足这样的需求。在学习这节课的过程里，你可以不断地做一些印证，想想你可以用在门课学到的知识，怎么来满足这个领域的需求。
但是，我需要补充一句，我自己并不是自控领域的从业者。只不过，我恰好有一些朋友，是国内这个领域顶级的、资深的专家，所以我就有机会学习到了这个领域的一点知识。并且，我跟他们参与了一个工业领域的操作系统项目的策划工作，这也激发了我对这个领域的编程模式的兴趣。但是呢，我对这个领域了解得并不那么深，也不那么专业，我主要关注其中与编程技术有关的部分。
那么首先，我们来了解一下自控系统的作用和应用场景，方便我们理解它对编程技术的需求。
自控系统的应用场景计算机技术在工业领域有着丰富的应用，其中一个应用领域，就是自动化控制。
其实自动化控制离我们特别近，你肯定不陌生。比如，当你乘坐地铁或高铁的时候，它们总是能够准确地停在站台边，停靠的位置甚至可以精准到厘米级。这里面就有自动控制的程序在起作用。
自动化控制也有在工厂里的应用，你可能没那么熟悉，但也跟你的生活密切相关。比如在发电厂里，我们需要对发电机组机组进行精确地控制。如果转子的旋转速度出了问题，那就会酿成巨大的灾难。在核电厂里，我们也需要对核燃料进行精确地控制，以便正常发电，不要引起事故。
你知道，制造业是一个现代国家的根本。而现代工厂的生产过程，几乎全部都会用到自动控制系统，包括离散制造型企业，就是给我们制造手机、电脑、汽车的企业；还有流程型企业，比如各种化工厂，还有前面提到的电厂。
最后，其实我们身边应用得非常广泛的物联网，也属于广义上的自控领域。比如，你家里的智能电表、智能门锁，还有小区停车场的自动道闸等等。
所有这些应用场景，基本上都会采用类似下面的这种技术架构：
你看这张图，最底下是各种生产设备。在这层之上呢，是一些自动化的控制器。控制器最重要的作用就是控制设备的运行。它的原理是这样的：设备在运行过程中，会不断产生信号，信号会被实时传输给控制器。控制器接收到信息以后，在内部进行实时计算，然后根据需要输出一些控制信号，让设备的运行做出一些改变。
通常来讲，控制器对计算能力的要求不会太高，控制的逻辑通常也并不是特别复杂，因为控制程序的时延必须很低。这也很容易理解，比如你开车的时候，肯定不能容忍刹车的响应很慢，那会出事故的。也正是因为实时性的要求，自动系统一般都运行在实时操作系统上，甚至运行在裸设备上。
而且，控制器会跟一个上位机连接，并通过某种工业协议进行通信。上位机的计算能力和存储能力都会比控制器更强。我们通过上位机，就可以了解每个控制器的运行情况，可以对控制器做一些操作，比如升级控制器上的程序等等。
好了，这就是自控领域最基本的计算架构，以及系统的运行场景。那么为了满足自控系统的需求，需要什么样的开发工具呢？是不是用普通的编程语言和开发工具就能满足自动的需求呢？
自控系统的技术特征自控程序的开发和运行有特殊的要求。我们平常用到的开发语言和开发工具，很难满足这些需求，因此我们必须设计专门的工具。我来给你分析一下。
首先，自控系统运行的硬件环境是不同的。
从硬件架构来看，这些控制器的设计跟我们平常的手机和桌面电脑都不一样。。
从硬件的电气特性来看，这些硬件在环境适应上的要求要比普通民用的更加严苛。这些硬件在设计上能够更好适应比较恶劣的工业环境，比如能承受比较大的震动、电磁、灰尘等的影响。普通民用的硬件设计，在这种环境下可能会从硬件产生出错误的信号，或者存储的数据发生错误，甚至无法正常工作。
从编程者的角度看，这些控制器的CPU的架构也跟我们手机和电脑不大相同。CPU架构的概念我们前面已经学过了。不同的CPU架构，意味着不同的指令集、不同的寄存器、不同的内存寻址方式，以及不同的异常处理机制等等。
不同的CPU架构，自然会导致程序开发方式的不同。普通的编译器，有可能都不支持这些芯片，或者没有针对这些硬件进行专门的优化。这个时候，如果你掌握了一些编译技术的后端知识，就知道如何以最优的方式生成机器码，并充分发挥硬件的能力。
第二，自控系统运行的软件环境也是不同的。
我们前面提到过，对于实时性要求比较高的场景，自控程序通常运行在实时操作系统里，或者是在裸设备上运行。所以，我们的开发工具也要能够生成在实时操作系统或裸设备上运行的程序。
不知道你还记不记得，我们前面讲过，在操作系统上运行，需要遵守相应的ABI，让程序能够跟操作系统相互配合起来。所以，这就需要开发工具支持实时操作系统才可以。当然，如果运行在裸设备上，那么就需要我们的开发平台本身来提供一些运行时功能，有支持IO、任务调度等能力，那这个运行时在某种意义上也是一个简化版的实时操作系统。
第三，自控系统的软件更新机制是不同的。
自控系统跟我们其他软件一样，在开发过程上也需要进行版本迭代。不过它的版本迭代有时候要求更高。比如，像发电机组等设备一旦投入运行，是不可以随意停机的。每次停机，都会造成很大的成本和影响。还有像卫星、太空舱上面的关键设备，大概也是这样的。这就需要自控系统能够在不重启的情况下实现更新。在这种需求下，自控软件的更新可能会有下面几个特点：
第一个特点：通常这些系统的更新都不是整体更新，而是支持一个个小模块的动态更新。
如果我们拿Java语言来打个比方，这相当于程序在运行的时候每次只更新一个类。从这个角度看，我们在课程里讲过的动态编译并形成可运行的模块的技术，就会派上用场了。但是静态编译并不能满足这个场景的要求，因为静态编译后的程序，在代码区里的内容通常是固定的、不能修改的。
第二个特点：在更新的时候，要满足实时性的需求。
在自控系统上，通常会有这样的场景：在10毫秒前的一次调用，可能使用的是前一个版本，在下一次调用，就已经无缝切换成了新版本。在这种情况下，程序之间不是静态链接在一起的。当我们要执行某个函数的时候，需要动态查询这个函数的地址，然后再跳转。在采用JIT技术的系统中，通常也会采用类似的技术。
第三个特点：在模块更新的时候，要能够保留程序中原来的状态信息。
自控软件在控制设备运行的时候，需要正确地掌握设备过去的状态。拿我们熟悉的场景来举个例子，如果你要升级红绿灯的控制软件，那么程序中的状态信息就要包括每个灯的状态是什么、已经持续了多久。
如果按照原来的规则，红灯要在5秒后转绿，那么软件升级后，必须仍然是在5秒后转绿，系统整体的运行逻辑才不受影响。否则的话，可能会出现路口两个方向都是绿灯的错误状态，从而引发交通事故。这种状态的错误，如果出现在电梯控制上、发电机组控制上，都会导致致命的后果。
第四，自控系统对可靠性和安全性有着很高的要求。
通过前面的描述，你应该已经体会到了，自控系统对于可靠性和安全性，有着很高的要求。我们可以想象一下，在未来的战争中，可能我们受到的第一波攻击并不是从天而降的导弹，而是来自网络的攻击。如果网络攻击就瘫痪掉来一个国家的高铁、核电、各种基础设施和工厂，那么战争也就根本不用打了。
可靠性和安全性要通过多个方面的工作来保证，包括管理角度、物理防护角度等等。落实到IT技术上，也会有多个层面的工作。比如，在操作系统层面，我们会进行可靠性方面的增强，尽量消除由于硬件的原因而产生的数据错误，比如网络传输中的数据错误、由于存储设备的原因导致的数据错误，等等。
从软件开发和运行的角度，在可靠性和安全性方面也有很多可以提升的地方。比如，传统的软件运行方式，代码段的地址都是固定的，所以就会比较容易导致攻击，比如内存溢出攻击。如果你的运行机制，让每个函数的代码地址都是随机的，那么就可能避开这种类型的攻击。
通过我们前面这些对自控程序的运行特征的分析，你会发现它确实跟我们普通的桌面软件、服务端软件、移动APP软件都不太相同。所以，针对这个领域，我们就需要专门的开发工具。那我们就看看这个领域的开发工具都有哪些特点。
自控系统的开发平台由于自控领域自身的独特性，所以多年来这个领域也发展出了一系列独特的技术。这些技术被统称为OT，也就是Operational Technology。而你熟悉的互联网系统等，则属于IT领域。在OT领域，也形成了相应的国际组织和技术标准。在过去，这些组合和标准的话语权主要在国外一些企业的手中。
在OT领域，软件开发被叫做“组态”开发。组态是英文Configuration的意思。从字面上你可以这样理解，我们给控制器开发的软件，相当于是对控制器在做配置。
而这些控制器呢，在OT领域有个广为人知的名称，叫做PLC，也就是可编程控制器（Program Logic Controller）。那么我们可以怎么为可编程控制器编程呢？
根据相关国际标准（IEC 61131-3），如果我们要给PLC编程，可以使用5中不同的编程语言，如下图：
首先是文本化的编程语言。我们平常用的编程语言其实都是文本化的，这里面又细分为两种。一种是指令表，它相当于一种语法特别简单的计算机语言。虽然简单，但是对某些编程需求来说却足够。第二种是结构化的文本，这个跟普通的高级语言差不多，语法上有点像Pascal语言。
除了这两种文本化的编程语言以外，还有三种图形化编程语言，包括梯形图、功能块图和顺序功能流图。我相信，这几个图对大多数IT领域的同学来说，应该都不太熟悉。但对于弱电领域或OT领域的很多工程师来说，阅读这些图是他们的基本功。
通过这些图，他们就能很容易地理解程序的逻辑。其实在IT领域，一直也有图形化编程的方式，比如少儿编程领域，就可以通过拖拉图形块的方式来编程。而最近越来越为人所知的低代码开发平台，也大量采用了图形化编程的方式。
我这里给出了几张图，是对某个组态开发平台的截屏，可以帮助你更直观地理解这些编程语言。
那么，问题来了，你可以怎么来利用我们这门课学到的知识点，来实现上面这样一个开发平台呢？
首先，针对这5种语言，无论是文本化的语言，还是图形化的语言，你都可以利用词法分析、语法分析和语义分析技术，形成统一的、正确的AST。
第二，基于这个统一的AST，你再可以继续编译成机器码。我们在课程里讲过了如何针对x86-64架构的CPU生成汇编代码。不过呢，你要根据PLC所采用的芯片，来为它生成针对该芯片的汇编代码。一般芯片厂商都要提供工具链，能够把相应的汇编代码生成机器码。
第三，我们要设计一个专门的运行时。在我们的课程中，我们已经实现过虚拟机，这就是一种运行时。不过，针对OT的需求，PLC的运行时要复杂一些，比如要能够实现软件模块的动态加载和更新的管理、代码地址的转换、状态信息的维护，还有与通讯模块和底层操作系统的衔接等工作。
第四，我们要对程序进行优化。对于比较复杂的控制逻辑，我们要运用优化算法，提高程序的性能。程序的性能越高，可以满足的实时性要求就越高。像控制机器人臂这样的场景，对实时性要求就是很高的。如果我们不把代码进行充分优化，那就很难满足这些高实时性场景的要求。
所以，我们在课程的第三部分，会专门花时间学习优化技术。如果你能把优化技术也掌握透彻，那么你就有可能成为这个领域的顶级技术专家了。
通过上面的这些分析，你会发现，其实很多知识点，我们在前面的课程中都已经涉及了，还有一些知识点我们会在后面继续学习。所以，只要你认真掌握了我们这门课的内容，你基本上就可以胜任这些技术工作了！
课程小结今天的加餐，我分享了自动控制领域的一些背景信息，也讨论了如何针对这个领域的需求来研发相应的组态软件平台。我写这篇加餐其实有几个目的：
第一，是开阔你的视野。你可能并不是OT领域的技术人员，以后也不会做OT有关的事情。但是，它山之石可以攻玉。OT和IT在发展过程中，一直在互相影响。比如，OT处理高可靠性的一些思路，就有可能用于高可靠的IT应用中。
第二，我希望你能理解我们这节课的知识点，是怎么用于解决具体领域的问题的。比如，你可能发现，我在这门课里特别重视让你理解程序的运行机制，包括程序跟CPU架构的关系、跟操作系统的关系、理解ABI等等。从今天的分享中你会看到，要解决一个特定领域的问题，特别是当你需要自己研发相关工具的时候，这些知识都很重要。
最后，我也希望通过今天这节课，能让工业领域之外的人也了解一点工业软件。毕竟，工业是我们国家的立身之本。而工业领域的基础软件，还有很多工作，需要有志之士参与进来，提升我们国家在这个领域的创新能力和话语权。
我在后面的加餐里，还准备给你分析一些其他的应用领域和开发工具。我们总说“学以致用”，了解更多的应用场景，也会有助于你理解和掌握我们这门课程的知识点。
思考题如果你是来自于自控领域的，我想请你帮我补充一些信息，包括自控领域的其他应用场景、编程技术等等。
如果你来自其他领域，那么我想问问，你们的领域有没有跟自控领域类似的技术问题，以至于需要研发专门的开发平台呢？欢迎在留言区和我分享。
我是宫文学，我们下节课见。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>《数据结构与算法之美》学习指导手册</title><link>https://artisanbox.github.io/2/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/1/</guid><description>你好，我是王争。
在设计专栏内容的时候，为了兼顾不同基础的同学，我在内容上做到了难易结合，既有简单的数组、链表、栈、队列这些基础内容，也有红黑树、BM、KMP这些难度较大的算法。但是，对于初学者来说，一下子面对这么多知识，可能还是比较懵。
我觉得，对于初学者来说，先把最简单、最基础、最重要的知识点掌握好，再去研究难度较高、更加高级的知识点，这样由易到难、循序渐进的学习路径，无疑是最合理的。
基于这个路径，我对专栏内容，重新做了一次梳理，希望给你一份具体、明确、有效的学习指导。我会写清楚每个知识点的难易程度、需要你掌握到什么程度、具体如何来学习。
如果你是数据结构和算法的初学者，或者你觉得自己的基础比较薄弱，希望这份学习指导，能够让你学起来能更加有的放矢，能把精力、时间花在刀刃上，获得更好的学习效果。
下面，我先给出一个大致的学习路线。
（建议保存后查看大图）现在，针对每个知识点，我再给你逐一解释一下。我这里先说明一下，下面标记的难易程度、是否重点、掌握程度，都只是针对初学者来说的，如果你已经有一定基础，可以根据自己的情况，安排自己的学习。
1.复杂度分析尽管在专栏中，我只用了两节课的内容，来讲复杂度分析这个知识点。但是，我想说的是，它真的非常重要。你必须要牢牢掌握这两节，基本上要做到，简单代码能很快分析出时间、空间复杂度；对于复杂点的代码，比如递归代码，你也要掌握专栏中讲到的两种分析方法：递推公式和递归树。
对于初学者来说，光看入门篇的两节复杂度分析文章，可能还不足以完全掌握复杂度分析。不过，在后续讲解每种数据结构和算法的时候，我都有详细分析它们的时间、空间复杂度。所以，你可以在学习专栏中其他章节的时候，再不停地、有意识地去训练自己的复杂度分析能力。
难易程度：Medium
是否重点：10分
掌握程度：在不看我的分析的情况下，能自行分析专栏中大部分数据结构和算法的时间、空间复杂度
2.数组、栈、队列这一部分内容非常简单，初学者学起来也不会很难。但是，作为基础的数据结构，数组、栈、队列，是后续很多复杂数据结构和算法的基础，所以，这些内容你一定要掌握。
难易程度：Easy
是否重点：8分
掌握程度：能自己实现动态数组、栈、队列
3.链表链表非常重要！虽然理论内容不多，但链表上的操作却很复杂。所以，面试中经常会考察，你一定要掌握。而且，我这里说“掌握”不只是能看懂专栏中的内容，还能将专栏中提到的经典链表题目，比如链表反转、求中间结点等，轻松无bug地实现出来。
难易程度：Medium
是否重点：9分
掌握程度：能轻松写出经典链表题目代码
4.递归对于初学者来说，递归代码非常难掌握，不管是读起来，还是写起来。但是，这道坎你必须要跨过，跨不过就不能算是入门数据结构和算法。我们后面讲到的很多数据结构和算法的代码实现，都要用到递归。
递归相关的理论知识也不多，所以还是要多练。你可以先在网上找些简单的题目练手，比如斐波那契数列、求阶乘等，然后再慢慢过渡到更加有难度的，比如归并排序、快速排序、二叉树的遍历、求高度，最后是回溯八皇后、背包问题等。
难易程度：Hard
是否重点：10分
掌握程度：轻松写出二叉树遍历、八皇后、背包问题、DFS的递归代码
5.排序、二分查找这一部分并不难，你只需要能看懂我专栏里的内容即可。
难易程度：Easy
是否重点：7分
掌握程度：能自己把各种排序算法、二分查找及其变体代码写一遍就可以了
6.跳表对于初学者来说，并不需要非得掌握跳表，所以，如果没有精力，这一章节可以先跳过。
难易程度：Medium
是否重点：6分
掌握程度：初学者可以先跳过。如果感兴趣，看懂专栏内容即可，不需要掌握代码实现
7.散列表尽管散列表的内容我讲了很多，有三节课。但是，总体上来讲，这块内容理解起来并不难。但是，作为一种应用非常广泛的数据结构，你还是要掌握牢固散列表。
难易程度：Medium
是否重点：8分
掌握程度：对于初学者来说，自己能代码实现一个拉链法解决冲突的散列表即可
8.哈希算法这部分纯粹是为了开拓思路，初学者可以略过。
难易程度：Easy
是否重点：3分
掌握程度：可以暂时不看
9.二叉树这一部分非常重要！二叉树在面试中经常会被考到，所以要重点掌握。但是我这里说的二叉树，并不包含专栏中红黑树的内容。红黑树我们待会再讲。
难易程度：Medium
是否重点：9分
掌握程度：能代码实现二叉树的三种遍历算法、按层遍历、求高度等经典二叉树题目
10.红黑树对于初学者来说，这一节课完全可以不看。
难易程度：Hard
是否重点：3分
掌握程度：初学者不用把时间浪费在上面
11. B+树虽然B+树也算是比较高级的一种数据结构了，但是对初学者来说，也不是重点。有时候面试的时候还是会问的，所以这一部分内容，你能看懂专栏里的讲解就可以了。
难易程度：Medium
是否重点：5分
掌握程度：可看可不看
12.堆与堆排序这一部分内容不是很难，初学者也是要掌握的。
难易程度：Medium
是否重点：8分
掌握程度：能代码实现堆、堆排序，并且掌握堆的三种应用（优先级队列、Top k、中位数）
13.图的表示图的内容很多，但是初学者不需要掌握那么多。一般BAT等大厂面试，不怎么会面试有关图的内容，因为面试官可能也对这块不会很熟悉哈：）。但是，最基本图的概念、表示方法还是要掌握的。
难易程度：Easy
是否重点：8分
掌握程度：理解图的三种表示方法（邻接矩阵、邻接表、逆邻接表），能自己代码实现
14.深度广度优先搜索这算是图上最基础的遍历或者说是搜索算法了，所以还是要掌握一下。这两种算法的原理都不难哈，但是代码实现并不简单，一个用到了队列，另一个用到了递归。对于初学者来说，看懂这两个代码实现就是一个挑战！可以等到其他更重要的内容都掌握之后，再来挑战，也是可以的。
难易程度：Hard
是否重点：8分
掌握程度：能代码实现广度优先、深度优先搜索算法
15.拓扑排序、最短路径、A*算法这几个算法稍微高级点。如果你能轻松实现深度、广度优先搜索，那看懂这三个算法不成问题。不过，这三种算法不是重点。面试不会考的。
难易程度：Hard</description></item><item><title>不定期加餐1_远程办公，需要你我具备什么样的素质？</title><link>https://artisanbox.github.io/7/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/41/</guid><description>你好，我是宫文学。到这里，咱们课程的第一模块“预备知识篇”就已经更新完了。通过这么多讲的学习，这些编译技术的核心基础知识，你掌握得怎么样了呢？是不是感觉自己已经构建了一个编译原理的知识框架了？
不过我也知道，要理解编译技术的这些核心概念和算法，可能不是一件很容易的事儿，在跟随我一起探索编译之旅的过程中，估计也耗费了你不少的脑细胞，那咱们是时候来轻松一下了。
今天，我就想借着这个加餐的环节，跟你聊一聊一个很有意思的话题：远程办公。
之所以选择这个话题，主要有两方面的原因。
首先，由于疫情的影响，春节之后，很多公司都采取了远程办公的方式。所以，如何在远程办公的情况下做好工作，对于员工和公司来说，都是一个挑战。
第二个原因，是我个人一直对于远程办公这种工作模式很感兴趣，这些年来也一直在做这方面的思考，关注这方面的实践，所以有了一些心得体会，想跟你分享一下。
不过，要想把远程办公这个话题聊清楚，确实不容易，分歧也比较大。有一些朋友会比较悲观，觉得远程办公根本不切实际；而另一些朋友则会很乐观，觉得远程办公马上就会普及。
今天，我就来分享一下我看待远程办公的一些视角。我会从公司和员工这两个角度，来分析远程办公带来的机遇和挑战，希望能给你带来一些启发，让你以更积极和务实的姿态，迎接远程办公的浪潮，甚至在这种工作模式转换的趋势下，抓住更多的发展机遇。
首先，我来聊一聊远程办公的那些吸引人的地方。
远程办公的好处我对远程办公的了解，最早是透过开源圈的朋友了解了一些故事，后来自己也接触了一些酷酷的公司。很多做开源软件产品和技术服务的公司都是远程办公的，他们的员工可能来自世界各地。我曾经接触过一个芬兰公司的CEO，他们主要做嵌入式Linux的技术服务。一百多人的公司，平常办公室是没什么人的。据说他们公司有的员工，可以一边上班，一边去全世界旅游。
我当时认为，一百多人的公司，全部都能远程办公，并且管理良好，就已经很不错了。但后来看了一篇文章，讲到WordPress的母公司Automattic有上千名员工，分布在全球75个国家，全部都是远程办公。这就有点令人吃惊了！我才意识到，在互联网时代，原来远程办公可以适用于任何规模的企业。
这次疫情中，IT领域的很多公司都大量地采用了远程办公模式，包括谷歌、Facebook、微软等大型企业。
现在新闻上说，疫情之后，世界再也回不到过去了。其实我觉得，在很多领域，我倒是宁愿它回不去了。比如，远程教育；再比如，远程工作。
因为远程，意味着你获得了一个难得的自由：位置自由。
现代社会，我们苦“位置”久已！因为很多资源都是跟位置绑定在一起的，比如说，教育资源与学区房。
我在北京的很多朋友，他们在孩子上学期间，一直都是租房子住的，因为要住得离孩子学校近，而自己的房子会租出去。这种状态要持续到孩子上大学为止。
而对于若干都市白领来说，在大城市上班，就意味着要把整个肉体在办公室和家之间移动，所以每天可能要在路上花两三个小时，很多时候还会在路上堵个半天。
如果我们真的获得了位置自由，那么整个生活的幸福指数会提高一大截吧！
对于远程教育来说，我比较希望见到的现象，是在偏远的乡村，一样能够通过线上教育获得最优质的知识资源。至于线下的老师，更多的是关注孩子的健康成长，多带着孩子亲近大自然，扮演“育”的角色，而不是“教”的角色。
工作也是一样。现在越来越多的工作，都可以在网上进行了。互联网电商的发展，虽然让一些线下店铺的营业状况受到了影响，但只要能通网络，很多人在网上就可以卖东西了呀。另外，随着外卖的兴起，很多餐饮企业也不再需要临街的店面了。
所以，通过远程办公，我们可能就不需要北漂、深漂等各种漂了，可以选择离自己的亲人更近一些，或者可以反过来，四海为家。并且，你还可能获得更多、更好的工作机会，你可以从全世界的公司里选择你喜欢的那份工作，并且也不需要离开你喜欢居住的地方。
并且，伴随着位置自由，往往也会给我们带来时间自由。因为远程后不再需要按时上班打卡了（很多在全球都招揽人才的公司，大家的作息时间都不一样，当然不可能统一打卡），所以管理体系会更加面向绩效，而不会管你到底是在什么时间来完成这些工作的（通常也没法管理）。这就意味着，你可以在家人需要你的时候，出现在他们身边（比如接孩子），然后选择自己合适的时间段来工作。
上面说的是远程办公对员工的好处。从企业的角度来看，远程办公其实也会带来一些潜在的好处。
首先，有些员工可能会在工作上做更多的投入（这跟某些员工会偷懒恰恰相反，所以可能出现两极分化）。这些人很享受自己的工作，每天上下班反倒降低了他可能做出的贡献。如果公司有一套良好的管理体系，那就可能会因此筛选出更适合自己的员工，而避开那些混日子、划水的员工，整个团队的素质反倒会得到提高。
我曾经跟MySQL的前CEO Mårten Mickos聊天。我问他，管理远程办公的员工，需要注意些什么？
他思考了一下，说要建议员工跟家人一起住，至少要养条狗什么的。因为家人能帮助管理这些极客们的作息。不然由着这些极客们的性子，他们会昏天黑地、毫无规律地作息，不利于健康。就算养条狗，你也会因为要照料它们，而让自己的生活节奏健康一点。
他的回答其实出乎我的意料，我原本以为他会说什么公司的管理措施之类的。你体会一下，如果你是公司老板，你是不是会因为拥有这样热爱工作的员工而感到欣慰呢？
第二，因为没有了地域限制，公司也就可以充分任用全球各地的人才。这个方面在很多做开源软件的公司那里，得到了很好的体现。如果你喜欢某个开源产品，在社区里贡献自己的力量，那你很可能就会被邀请加入到该公司。
在互联网时代，企业的组织方式也正在重构。滴滴打车、美团外卖这些采用新雇佣方式的公司，不但可以更好地利用各地的人力资源，TA们也提供了一些自由工作的机会。
第三，没有了地域的限制，公司也可能更容易拓展自己的市场。这个好处也是显而易见的，就不用我多说了。
远程办公的挑战上面是我对远程的一些美好的憧憬。还是回到现实吧，因为更改现有的教育体制，可能是很难的。而让企业老板们改变公司的管理方式，难度也不低。
老板们都是理性的。真金白银投入做企业，是要见到效益的。可是，如何能保证采用远程办公模式，不会让企业变成一团散沙，纪律涣散、效率低下呢？
你可以问问，在春节后不得已实行远程办公的企业，对经营有没有产生影响。
说实话，在没做好充分的准备之前，仓促地采用远程办公，肯定会产生各种不适。
因为远程办公，对于管理体系，有着更高的要求。很多工作是难以直接度量绩效的，比如说研发工作就比销售工作更难衡量绩效。
而没有良好的管理体系，仅凭员工的自觉，是不可能产生良好的效果的。其实，硅谷有一些IT公司很早就实行过远程办公，但后来又取消了，原因就是绩效不理想，混日子的员工太多。
反过来，站在员工的角度，你真能做好自己的工作管理吗？在办公室工作的时候，迫于同事们的目光，你总得做点事情吧。可是，如果脱离了直接的监督，有多少人能够真正管好自己呢？
好，你说你的自我管理能力强，那么请问，有多少人能控制住自己每天刷手机的时间呢？据说，超过50%的成年人，都有手机上瘾症。在办公室的时候，尚且见缝插针地刷手机。如果在家办公，又会怎样呢？
有过远程工作经历的人，都会经历这么一个时期。即使是你很有责任心、很有事业心，但也要每天花费很多的精力来管理自己的行为。我认识的一个朋友，她在IT行业，主要做售前支持工作。之前跟她闲聊的时候，她说自己花了3年的时间跟自己搏斗，才养成了良好的居家工作习惯。而管理自己这件事情，也是消耗注意力的。注意力本身，又是个稀缺资源。所以在初期，你会觉得，对比在办公室里，居家办公会更累，在公司你不需要花精力来控制自己的行为，因为环境和同事帮忙做了这件事情，实际上节省了你的注意力。
我也听说，有的工程师会在网上直播自己编码的过程。这样做的一个原因，就是为了帮助管理自己的行为，因为这时候你必须更加集中注意力在自己的工作上。
还有一个是办公环境的因素。我们中国人的居住状态比较拥挤，在自己家里开辟出一个安静的、不被打搅的书房并不容易，这可能还跟中国的文化有关。而西方的文化，可能会更尊重个人的空间。
再说了，我们跟外国人的居住条件也确实不同。西方发达国家很早就开始了郊区化发展，大部分人会住在郊区和小城镇，自然环境比较好。而我们中国呢，大部分住在小区的楼房里。
不过，如果真的远程工作了，你也可以不住在大城市呀。网上有些视频经常会吸引我，某夫妇在乡村翻新出一栋漂亮的别墅，还拥有美丽的花园，等等。其实我目前就住在一个自然环境良好的山上，只不过从这个村子去办公室也很方便就是了。
远程办公还会产生心理上的挑战：白天晚上都在家里，会容易心理疲劳。而换个环境，反倒会让人兴奋起来。我就有个感觉，在家里工作久了，效率就会降低。而这时候再回公司工作的话，反倒更容易集中注意力。
而且，远程办公肯定也会降低沟通的效率。一些互联网公司，在设计办公室的时候，会故意设计一些空间，方便大家偶遇，闲聊几句。而做研发工作的同学都知道，这种看似随意的交流，有时候能激发出很多创新的思维。而如果总是自己苦思冥想，往往很快就会走入死胡同。这种线下偶遇式的沟通，往往见到了就会聊个几句。但在远程办公时，如果大家互相见不到面，还真就不聊了。
面对远程办公，我们要做好什么准备？所以，我们需要实行一些积极的操作，来更好地应对远程办公给我们带来的挑战，这样也能更好地抓住远程办公给我们带来的机遇。
从公司的角度出发那首先，我们来看看，对于企业来说，都需要做好什么准备。
第一，我觉得企业管理者要建立一个意识：远程办公是企业必须要面对的管理考验。
其实只要企业做大了，几乎都要面对远程管理的问题。比如你有了分公司，或者在各个城市设门店，甚至把生意做到国外。那么，突破地域的限制拓展业务，本来就是对企业能力的考验，是企业发展中必须踏过的门槛。
所以说，企业也一样需要获得位置自由。这些分公司、派出机构工作的人员，对于总部来说，本来就是远程工作的。有了这个意识，管理者就会开始放弃旧的思维，拥抱远程办公。
第二，从看住人，转换到管绩效。
很多比较传统的企业，他们的绩效标准都比较模糊，所以在远程办公的形势下，我们需要把绩效标准的清晰化、准确化放到第一位。像滴滴、外卖这些新职业，之所以能够迅速扩展规模，充分利用社会化人力资源，就是因为他们的工作绩效的标准是清晰而准确的。
第三，建立拥抱远程办公的文化，给员工授权和赋能。
像软件研发类的工作，它是知识密集型的，对员工的绩效评估比较难，人员更换的成本也相对较高。那么对于这类工作，我们可以多向那些开源软件公司学习，建立一个拥抱远程办公的公司文化，去吸引那些对工作充满兴趣和热爱的人参与进来。这些人，也会更加珍视公司给予的授权和自由。
第四，充分利用IT技术。
管理，一定要落实在工具上。我接触的那家芬兰公司，就花了很多年的时间，积累了一套成熟的内部管理系统。比如说，作为软件公司，你肯定要对项目进度、代码量、Bug数等基础指标有所管理才行吧？
信息技术成本的降低，也大大降低了远程管理的开销。这次疫情，促进了视频会议在全世界的普及。对于中国的中小企业来说，甚至可以0成本享受高品质的远程会议服务，这真是一个了不起的福利！
从员工的角度出发OK，说完了公司，那我们再来看看，从员工的角度出发，我们都要具备什么素质，才能更好地迎接远程办公模式。
第一，员工也要建立一个意识：无论是否远程办公，都要向绩效负责，管理好自己的工作。
即使你仍然在传统的办公模式下工作，如果你能像一个远程工作者那样对绩效负责，管理好自己的注意力，我想你很快就会获得领导的注意，从而赋予你更大的工作自由。你有没有听说过，张小龙经常睡懒觉迟到，而马化腾从来不管他？因为马化腾需要的是一个能做出微信来的张小龙，而不是一个每天按时打卡的张小龙。
第二，正视远程办公对自我管理的高要求，养成良好的工作习惯。
在办公室工作，会有环境来约束你。而当真的给了你位置自由以后，你其实要珍视这种自由，给自己定一些规矩，甚至给自己找一些监督（就像前面说的在网上直播），从而养成良好的工作习惯。
第三，建立激进的协作习惯。
由于远程工作对于协作的挑战，你必须建立激进的协作习惯，而不是仅仅停留在我们目前使用即时通讯工具和视频会议工具的习惯上。比如，你可以全时间视频在线、主动找人线上闲聊一小会儿、主动创造一些与人沟通的机会，等等。
第四，可能是最重要的：为兴趣而工作，为自己而工作。
人在没有很多督促的情况下，真正能驱动自己前行的动力，就是兴趣了。这个时候，你会把工作看作是促进自己成长的必要因素，从工作中成长，从成长中获得快乐。这个时候，你已经不是在为公司工作，而是为自己而工作。这样的人，才算获得了真正的自由。
小结今天，我们讨论了远程办公对公司和员工的好处、挑战，以及我们需要做好的准备。我讲了两个主要的观点。第一个观点是对企业的：远程办公管理能力是企业未来必须具备的能力。第二个观点是对个人的：只有能够管理好自己的人，才能抓住远程办公带来的机遇。</description></item><item><title>不定期加餐2_学习技术的过程，其实是训练心理素质的过程</title><link>https://artisanbox.github.io/7/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/42/</guid><description>你好，我是宫文学。
最近，高考刚刚结束。每年一度的高考都牵动了很多人的心，学生和家长们都把高考看作是人生的一大关键关口。可是，等上了大学以后呢？很多同学也会感到不适应，因为缺少了一个像高考那样明确的学习目标，也没有老师和家长在旁边不停地鞭策和关注。到了工作以后，就更是如此了。
对于进入计算机领域的人而言呢，很多人迫于找一份好工作的压力，会刻苦学习一段时间，包括参加各种学习班。而一旦获得了一份工作，融入职场以后，很容易就进入舒适区。反正当前的工作也能应付过去，为什么还要费力再去学呢？毕竟，工作已经够辛苦了。
在这种情况下，人生的第二次转折点就出现了。
有的人，能够管理好自己，充分利用各种时间和机会，不断地加深自己对技术的理解。虽然短时间看上去进步并不大，但成年累月地积累下来，效果就逐渐出现了，TA们开始能够胜任一些关键岗位，成了技术领头人。而另一些人，则只能掌握那些比较容易掌握的技术，时间一长就会显得平庸，等年轻人赶上来的时候，就更加没有竞争优势了。虽然这不是像高考一样，能马上分出重点大学和普通大学的差别来，但在进入职场5年、10年以后，这两类人在发展上的差别并不比高考带来的差别小。
我说这些，不是在贩卖焦虑，而是想引出我们今天要讨论的话题：从心理的角度看待学习技术的过程。特别是自己管理自己学习的过程、跟自己相处的过程。
学习没有轻松的。尤其是学习像编译原理这样的基础技术，就会显得挑战更大。想要学好它，调整和保持一个良好的心态是非常重要的。而通常，我们在心理上会面对三大问题：
第一，我为什么要学习这么难的技术？学一些比较容易的、应用层面的技术不就行了吗？这是学习的目的和动力问题。 第二，以我的能力，能学会这么难的技术吗？这是自信心和勇气的问题。 第三，如何看待学习中经常遇到的挫折？总是找不到时间怎么办？等等。这是学习过程中的心态调节和习惯养成问题。 如果对这三方面的问题，你都获得了清晰的答案，那么你应该就能保持好持续学习、终生学习的心态，从而对自己的人生有更好的掌控力。
那接下来，我就给你解读一下，我对于这三类问题的理解。
首先，我们来说说学习目的问题。
为什么要学这么难的技术？在做课程设计的时候，我和编辑同学都会尽量想着如何让这样的基础技术和你的日常工作关联起来，让你觉得它不是屠龙之术，而是能够在日常工作中发挥实际效用的。这确实是学习基础技术的收获之一。
不过，如果想长期坚持下去，我会建议你把心态调整成一种更高级的模式。用中国文化中的一句话来形容，就是“用出世的态度，做入世的事情”。如果一件事情你觉得应该去做，那就去做，不要太斤斤计较一些功利层面的东西。
那么对于学计算机而言，什么是我们应该去做的呢？那当然是要了解计算机领域的那些最基础的原理呀。如果搞了一辈子IT技术，却不明白其中的道理，那岂不是一辈子活得稀里糊涂的？
我知道，大部分人不注重基础性知识的原因，可能是觉得它们不会马上发挥作用。可是，那些最重要的知识、那些构成你知识结构的底蕴的东西，往往就是那些看上去不会马上有用的东西。
我个人非常欣赏复旦大学做教育的一种态度，就是教给学生无用之学。哲学、艺术、写作、演讲、逻辑学、历史等知识，在西方教育中被称作Liberal Arts，我们有时候翻译成通识教育，或者博雅教育。这些教育对于我们从事任何专业的工作，都是有用的。
比如说，美学素养。一个设计良好的系统架构，一定是优美的。新东方的元老之一王强，在美国学习计算机的时候，会把写完的程序拉开一定的距离看。如果看上去不够美观，那一定是程序设计得不够好。
你乍一听上去，可能会感觉是无稽之谈，但有经验的程序员一定会认同这个看法。那些写得有问题的程序，往往本身就是又臭又长、非常难读；而高质量的程序，往往是模块划分清晰、简洁易读的。做不出好的系统设计的人，肯定美学素养也不够高。像爱因斯坦等大科学家，往往驱动他们做出某个研究成果的动力，就是想去发现一条更加简洁、更具优美感的公式，因为真理往往是简洁的、优美的。
我之前公司的一名股东，他以前是一位很厉害的软件工程师，后来被一个外企挖走，担任了多年的销售副总。挖他去外企的原因，就是因为当时该外企刚开始在中国推广中间件的概念，他听了介绍以后就说，那不就跟我写的某软件的原理是一样的吗？并且一下子就说出了这类软件的关键技术点。于是，该外企下定决心要把他挖过去，并且是去负责销售。去年，他突然又写了一套科幻小说，名称是《云球》。我这里不是为他打广告，我是想说，做一个优秀的软件工程师、担任销售副总和小说家，这三个职业从表面上看相差很大，但其实背后依赖的基础素质都是一样的，都是那些乍一看上去没用的基础知识、基础素质。
所以，从这个角度，我是同意素质教育的理念的。一个缺乏美学素养、哲学素养和沟通能力等素质的软件工程师，潜力可能是有限的。
说到基础素养，我补充一个例子。有一次，我和前面说到的这位朋友在一起聊天，结果一个软件公司的老总给我们吹嘘他们公司开发的某软件平台。在说到一些特性的时候，听得我俩目瞪口呆。后来我们告诉这位老总，他声称的东西违背了基本的物理学和信息学的规律。在违背科学的底层规律的方向上做事情，那就相当于去造永动机，根本是虚妄的。这是很多项目失败的根本原因。
而另一些人，却具备抓住事情本质的能力。众所周知，马云并不懂技术。但就是不懂技术的马云，在懂技术的马化腾、李彦宏都认为云计算不是趋势，只不过是新瓶装旧酒的时候，果断拍板做云计算技术。期间，来自内部的反对声一直很强，大家都不愿意在内部使用尚不成熟的云计算技术。然而时间证明，马云的眼光更准。并且，力主开发云计算技术的王坚博士，他自己的专业也不是计算机专业。那么，为什么一拨非科班人士会比科班的技术大佬们看问题还准呢？我想可能是他们的无用之学学得更好，基础素质更全面吧。
所以，这就是我对于像编译原理、操作系统、算法等基础知识的态度。你就把它们看做是无用之学好了。我不仅鼓励你把这些基础知识学明白，并且我也希望你可以尽量再往深里挖一挖。比如，像图灵那样去思考一下，计算的本质到底是什么；编译原理用到的形式语言，也可以被继续深挖，从而跟整个西方科学体系底层的形式逻辑体系挂钩，以此去深入地理解希尔伯特猜想和哥德尔定理；了解面向对象、函数式编程这样的编程范式，跟人类的认知和思维模式的关系，跟Lamda计算、范畴论等数学工具的关系；你还可以去了解复杂科学领域的成果，并用这样的思维去看待大型复杂的信息系统。
如果你觉得编译原理这样的技术没啥用，那你一定会觉得我刚才说的那些更加没用。但我知道，一个优秀的软件工程师，其实一定是对我说的那些话题有所涉猎、有兴趣的。
总结起来，一个人的基础素质，决定了他的思维方式、思维质量和眼光，那些看上去没用的基础知识、基础原理，其实是真正做大事、承担重任所需要的素质。那，你到底要不要去学习呢？
好，如果你认可我的观点，那么我们接下来再探讨第二个话题，关于学习的信心问题。
我能学得会吗？很多人都会有一个担心，说某些基础技术、基础原理太难，自己的基础又不够好，那么能学得会吗？如果学了半天又学不会，那不是白费力气吗？
从能力角度，我必须承认，我们每个人都是有天赋的差异的。你让一个普通人去跟姚明比赛打篮球，那不是难为人吗？
学习这件事情也一样有天赋的问题。
我本人当年在高考的时候，是省里的前几名，但是等我到了北大，看到周围的同学通常也都是身手不凡；在记忆力方面，我也比不过很多同学，有的同学对普通的词汇书根本不感兴趣，会去背词典，甚至背专业领域的词典；在数学等需要逻辑思维的领域，我又比不过另一些同学，直到今天，对于一些涉及数学的问题，我都会去咨询这些同学的意见。
但从另一个角度讲，一些基础知识、基础原理，一定要有很强的天赋才能学会吗？
不是的。在人类知识的殿堂中，你要想增加一点新的理论、新的原理，那是非常难的。所以我们必须对那些大科学家们，那些计算机领域的先驱们顶礼膜拜。那些顶尖的工作，确实需要天赋，再加上努力和机缘。
不过，即使狭义相对论和广义相对论发明起来那么困难，但一般的理工科学生只要想学，都是可以弄明白的。这就证明了，发现知识和学习知识所需要的能力，是极大的不对称的。在高考季，经常会出现妈妈级、奶奶级的考生，从陪考到变成跟儿孙辈一起上大学的故事。人家奶奶都能考上大学，我们年轻大学生学不会本专业的一些基础原理，这个道理说得通吗？
同理，你常常会听到的一个理由也是不成立的，这个理由就是：我不是科班出身。这个我就不认真去反驳了。你想想看吧，费马的本职是律师，而他“业余”是个大数学家；数学家罗素却获得过诺贝尔文学奖；比尔·盖茨进的是哈佛大学商学院；我前面说的王坚博士是学心理的；罗永浩的专业也肯定跟IT没关系；刘慈欣是业余写小说的。
所以，那些所谓的困难，只是你给自己设的玻璃天花板。这不是个能力问题，而是个心理问题。儒家提倡“智、仁、勇”三种最高的道德标准，勇气是其中之一，它也是我们应该训练的一种品质呀。
好，如果你又一次认同了我的观点，那么我们再来讨论第三个问题，如何克服学习过程中的困难。
如何持之以恒？在我看来，如果理顺了前两个问题，也就是为什么要学，以及信心和勇气的问题，那么你最大的心魔其实就破除了。
但毕竟，学习贵在持之以恒的坚持。在这个过程中，我们可能会遇到很多的困难。但对于这些困难，我们也要用正确的心法来对待。所以，接下来我就针对如何面对学习中的困难、如何保证学习时间、如何找到学习的乐趣等常见问题，谈谈我的看法。
困难是必须的首先你得明白，有价值的东西，一定是要克服困难才能得到的，这是公平的。所以不要指望学知识而不需要付出努力，再好的教程和老师，也只是起到辅助作用。这里你得注意一个问题，就是不要被某些书籍和课程收了智商税，比如说，“7天学会XXX”，“学英语其实不用背单词”，等等。这种标题，就是违背学习的基本规律的。
所以，当你知道了苦难不可避免这个道理，那你剩下的就只有面对这些苦难。在学习中，你可能经常会被一个难点阻碍住，这很正常。你正确的心态应该是这样的：
没有我拿不下的山头，正面拿不下从侧面，侧面不行走背面。多换几个角度，多几次尝试，多看点参考资料，总会成功； 那么多人都能学会，我没有道理学不会，一定有更好的方法； 这个问题既然有难度，那价值一定也大，所以一定不要放弃。 有了这样的心态，其实再苦再难的事儿都好说了。
在旅途中发现乐趣我一个朋友最近正在从新疆骑行到西藏，全程3000公里，中间需要穿越无人区。这是他第三次做这样的骑行，之前已经骑过川藏线、青藏线。虽然过程很艰苦，但沿途美丽的风景，和跟自己相处的过程，就是这个旅途给他的回报。
我自己也喜欢户外。我家人有时不理解我，问我为什么要开着一辆大房车去那么远，累不累呀。我说，这就是旅行的意义呀。如果直接飞机过去，那有什么意思。
我用这两个例子作类比，是想告诉你：当我们学习那些有难度的知识的时候，其实肯定能发现出其中的乐趣来。比如，在学编译原理的时候，你去动手实现几个小例子，哪怕还不到实用的程度，但是好玩呀！当你找到了其中的乐趣，那么别人看你是在艰苦地学习，但其实你是乐在其中呢。就好像，别人看着一个人是在顶风冒雪一个人骑行，但他也是乐在其中呢！
另外呢，在互联网时代，各种不需要动脑的娱乐方式层出不穷。普通的人会在这种廉价的快乐中流连忘返。而如果你的目标是持续进步，那要培养自己另一种习惯，就是习惯于获得那些艰难的乐趣，这种乐趣是真正的充实的乐趣。
跟自己相处我前面举的朋友骑行的例子，他是自己一个人。我也喜欢自己开车出去，因为没有了其他人，也就避免了因为人际关系而导致的分神，你只需要关注大自然和你自己。你能感受到自己跟自己对话的过程，自己跟大自然对话的过程。
学习在大多数情况下也是一个人前行的过程，学到的知识也只属于你一个人。在这个时候，就只剩下了你要去攻克的知识，和你自己。你能感受到自己跟自己对话的过程，自己跟知识对话的过程。当遇到困难了，你能发现自己的苦闷和焦虑；当解决问题了，你能感受到自己的欣喜。
真正有价值的成绩，都是在这样的跟自己独处、跟自己对话的过程中做出来的。这是一种值得追求的素质。
跟志同道合者相伴独行难，众行易。除了那些内心特别强大的、从来都不屑于与普通人同行的天才，我们大部分普通人还是愿意有一些同伴一起结伴而行的，这样会大大降低驱动自己所需的努力。
我在读研时曾报过GRE的培训班。我感觉报班的最大作用，其实不是跟着老师学到多少知识，而是培训班乌泱乌泱的一大堆的同学，给我提供了一种气场，让我每天不想别的，赶紧学习就是了。
这样的群体还会有效改变自己的学习标准。在学GRE之前，我觉得一天背几十个单词已经挺辛苦的了。但到了GRE班，我很快就接受了每天背200个的新标准，因为其他人也是采用这个标准的。关键是，就算每天背200个，我也没觉得有多困难。所以你看，人的潜力有多大的弹性，而一个好的群体就是能无形中给人提供这种心理上的能量。
而且那时的同学都会有这种体会，就是每天如果不背单词就不舒服，上瘾。那段时间，随便看到一个单词，脑子里就会出现几个近义词和反义词，这种感觉很奇妙。再次印证了我前面说到的那种奋斗中的乐趣。
在软件领域，有很多技术社区，这些社区也能起到对人的心理加持作用，你可以善加利用。
最后，如果有要好的朋友和导师，能够鞭策你，那也非常难得。有管理经验的人都知道，虽然我们希望每个员工都有自我驱动的能力，但合适的外部驱动能降低员工驱动自己所需要消耗的努力。毕竟，我们大部分人其实是愿意工作在“低功耗模式”，能节省能量就节省能量。
使用运营思维在互联网时代，各种App在功能设计和运营上，充满了心理学的套路，以便培养用户的习惯。游戏公司更是会雇佣心理学专家，来设计各种套路。
那么，与其让别人套路你，不如自己套路自己，同样利用心理学的知识来培养自己的学习习惯，把自己的时间、自己的命运把握在自己手里，不是更好吗？
心理学的基础原理并不难，你自己就能从各种App的使用套路里体会到一些。比如说对取得的成绩即时给予奖励。从心理学的角度、从各种App背后的运营者的角度来看，我们每个人其实就是巴甫洛夫实验室里的动物而已。通过这样的自我训练，你可以达到一些很好的效果：
建立良好的学习流程，有明确的开始和结束时间；确认一下每天的学习目标和学习成果，或者可以建立学习过程的仪式感；给自己一个良好的环境。 没有学习的时间？那是不可能的。这是因为你没有给学习安排出专门的时间来。 以输出带动输入。很多同学有写技术博客的习惯，这个习惯非常好。因为你要写出东西来，所以会逼迫自己把思路理清楚。 激进一点的：直播自己的学习过程，给自己提供外部监督和激励机制。 小结今天这一讲，我聊了聊对于学习比较难的、比较基础的知识的心法的理解。总结起来，主要有三点：</description></item><item><title>不定期加餐3_这几年，打动我的两本好书</title><link>https://artisanbox.github.io/7/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/43/</guid><description>你好，我是宫文学。
在互联网时代，读书好像变成了一件挺奢侈的事情。因为我们现在获取信息的主要渠道，已经变成了网络。不过，在互联网统治力如此强劲的今天，我偶尔仍能发现一些好书，让我可以放下电脑和手机，对着厚厚的一大本，从头看到尾，甚至还会看很多遍。可见这些书确实是真正打动了我，让我这个理科背景的人，能以新的视角来看待世界，理解这个世界背后的运行规律。
我觉得一本书籍能达到这个阅读境界就很值得推荐了，因为这相当于是在帮助我们树立世界观、沉淀方法论。所以今天的加餐环节，我想给你分享两本打动我的好书，或者说是以其为代表的两类好书，跟你聊聊我读这些书的感受和收获，希望也能给你一些启迪。
那第一本书呢，就是《失控》。
《失控》失控这本书的作者是《连线》杂志的主编凯文 · 凯利，于1994年出版。这本书被很多人推崇，据说张小龙就曾说过，谁能看懂《失控》这本书，谁就可以到他那里工作。
这本书的神奇之处，在于它虽然成书于上个世纪90年代初，但准确预测了后来互联网时代的一系列的创新，更厉害的是它揭示了互联网时代背后蕴藏的道理。就如这本书的副标题所说的，它揭示了“全人类的最终命运和结局”。
我自己呢，是在读过这本书后，对其中的内容感觉极为惊讶。我甚至怀疑，凯文 · 凯利到底是何方神圣，为何他能够写出这样的惊世之作。
我就拿《失控》中第二章的内容，跟你一起分享一下，做一次管中窥豹。
第二章的标题是“蜂群思维”。蜜蜂是一种社会性昆虫，它们总是一大群一起生活。在研究蜂群的时候，你会发现，一群蜜蜂相当于构成了一个单独的生命体，这个生命体比单只的蜜蜂更加高级。举个例子，单只蜜蜂只有6天的记忆力，而一个蜂群却拥有三个月的记忆时间（这是个体记忆与群体记忆的区别之处）。另外这个生命体会比单只蜜蜂拥有更长的寿命，且具有繁殖能力，能分化出新的蜂群。
这样看起来，它似乎符合一个生命体的所有特征。而这种把很多个体连接起来，构成一个更高级的存在的现象，就叫做涌现（Emergence）。
另一个能很好地解释涌现的例子，就是人类的大脑。大脑中的神经元，实际上就是一个很简单的个体，它们只知道在接收到信号的时候，对其他神经元发送信号。而基于很多亿的神经元所涌现出来的大脑，却具备记忆、推理、情感等很高级的能力。试想，如果你是一个神经元，你其实是根本无法理解，以亿万个你构成的一个超级生命体，竟然会拥有情感、逻辑推理这种东西。因为在一个神经元的世界里，它只有接收信号和发送信号这两件事情。
你再往下思考，就会发现人类社会正是由亿万个你我构成的，那人类社会是不是一个超级生命体呢？这个生命体在思考什么，我们作为一个神经元级别的存在，如何能理解呢？或者说，思考仅仅是我们这个级别的个体所能理解的事情。而这个超级生命体所做的事情，可能已经根本不是人类的思考这种层面的事情了呢？早期人类的宗教，以及宗教中的神，也是高于单个的人类个体的。那么，它们是不是这个超级生命体在人类历史中早期的一种呈现方式呢？
我们再来假设一下，当前的互联网时代，连接网络的计算机、各种智能手机、智能设备越来越多，甚至已经开始接近大脑神经元的数量了。那么，它们会不会涌现出一个超级生命体？这个生命体是否会具备自己难以撼动的意志，而我们必须屈服于这种意志呢？
怎么样？这本书里的观点，是否也能同样给你带来启发，开一个大大的脑洞？是不是也引起了你去一读的兴趣呢？
这个级别的内容，在《失控》里还有很多。再举一个例子：活系统的特征是“摇摇晃晃的平衡”，而处于稳定的系统就进入了死寂。从这个角度看，如果我们的生活中问题不断，其实正是活系统的特征，因为我们要谋求持续的不均衡，这样我们才有机会去改变它，这总好过稳定的、死寂的生活。你看，这样的结论都已经带有了哲学的特征，让我们在面对生活中的挫折时，会采取更加积极的心态。
于是，还是回到我前头提到的那个疑惑：为什么凯文 · 凯利会有这么深刻的洞察力，远远超越我们这些普通人呢？
经过研究，我发现原来书中的很多观点，其实是对从上个世纪中叶以来，各学科的科研成果的总结，然后通过一个资深科技编辑的叙述普及给大众。看到这里，我才放心了：原来并不是出了一个多么逆天的天才，而是我自己对科技发展的新成果，以及其中蕴含的新思想缺少了解。这些思想或理论呢，包括了很多同学都知道的系统论、控制论和信息论三大论，以及后来的协同学、博弈论、突变论、混沌理论、分型理论、耗散结构和复杂性理论，等等。它们在过去的几十年间不断地发展，并正在形成一个宏大的、崭新的世界观和方法论体系。现在的一个新兴学科——复杂科学，似乎就是这些元素的集大成者。
我以前对复杂科学了解得不多，但我觉得其实也不能怪我。因为我们在中学、大学学的那些知识，大部分都是用来描述简单系统的。比如在大多数情况下，天体的运行就是一个简单系统，我们用相对论这样的理论就能准确地预测天体的行为。
而复杂系统，其构成部分之间的相互作用关系比较复杂，难以预测。我还是拿天体来说，三颗星体的相互作用，就变得难以预测了，这就是著名的三体现象，也是刘慈欣小说名称的来源。蝴蝶效应、混沌系统，等等，说的也是这个现象。
可以说，复杂系统破除了对还原论的迷信。也就是，即使你把一个系统分割成一个个的“零件”，并且完全理解了每个“零件”，你也可能无法理解整体系统。因为整体不等于部分的简单相加，比如说，就算你理解了一个社会的经济体中的每个企业和消费者的行为，你也无法准确掌控宏观经济。
可是，了解这些，对你我有什么意义呢？
我先讲一个小的用途。作为软件架构师，你其实应该知道，当一个软件系统复杂到一定程度的时候，你要把它看成一个动态演化的有机体。你对系统做的任何改动，都可能会引起一些你完全预料不到的结果。这就是为什么，你可以花一点儿钱甚至是免费就能搭建一套简单的电商系统，但是像淘宝这样的大型系统，则需要几千人来建设和维护它。
再举个例子。我们现在都非常熟悉的微服务架构，它的理念是，一个大型软件系统是从一个个分布式的、自治的单元中涌现出来的。流媒体巨头NetFlix，他们也是微服务架构的首批推动者之一。在NetFlix，软件工程师们会设计一些叫做Monkey的程序，随机地杀死一些服务，看看系统能否正常地自动修复。发现了吗？像微服务这样的复杂系统，它的冗余和自愈的能力已经像是一个生命体了，即使出现了一些突发的故障，比如某些服务的宕机，它也不会一下子全部瘫痪。
除了软件领域，与人类社会密切相关的系统，包括天气系统、生态系统、经济系统、社会系统，甚至包括人体本身，它们也都是复杂系统，所以现在的很多学科都在采用复杂系统的思维做研究。比如，采用演化的思维做心理学的研究，就形成了进化心理学的分支（其实更恰当的翻译方法是演化心理学，因为演化是没有方向性的）。这个学科的基本逻辑，就是现在人类具有的某种心理特质（比如为什么恋爱中男人更主动，女人更矜持），都是在进化中形成的。因为凡是没有这种心理特质的人类，都已经在进化过程中被淘汰了。
再进一步，其实你根据上面的信息可以得出一个结论：原来文科生研究的对象都是复杂系统。你一旦意识到这一点，你就可以通过复杂系统的研究成果，去研究原来属于文科生的研究范畴，比如说社会学、经济学、文学和哲学，从而拥有方法论上的优势。
给你简单举个例子，经济学中的宏观经济学部分，就是针对复杂系统的。这也是为什么大家总是说经济学家都是事后诸葛亮的原因：复杂系统是很难被简单地驾驭的。
甚至，你也可以用复杂科学的视角来重新审视哲学，特别是一些古代的哲学思维。因为基本上这些古老的哲学思想都是复杂系统的描述，是让你能够更好地适应自然系统和人类社会这两个复杂系统的一些解。比如说，儒家的思想，是理顺人际间的互动关系，从而缔造一个稳定的社会系统；而道家的思想，则是描述了包含人类社会和自然界的一个更大的系统规律。
有意思的是，凯文 · 凯利在《失控》的最后一讲，总结了复杂系统的特征，有很多地方跟道家的思想非常契合。比如说，“世界是从无中创造出来的”“从无数神经元的连接中，涌现出了大脑；而分子层面的连接，则涌现出了细胞”。
可以说，从《失控》这本书开始，就引起了我对复杂科学的兴趣，这个主题下的其他书籍，比如《复杂》，也非常值得你一读。
好，接下来，我再给你分享另一类好书，是关于文化的。而且它跟复杂科学这个主题，也是存在联系的。
文化与地方志我从大学起，就对“文化”这个主题非常感兴趣，跟东西方文化有关的东西我都乱看了一气。大学时我读过一本书，是房龙的《人类的故事》，非常喜欢，因为它不但描述了历史事实，还描述了推动历史发展背后的原因和规律。我当时想，如果历史都这么写，那么大家学历史的时候肯定不会觉得枯燥。
因为我的思维特点是非常理科生的，我很难记住那些相互之间没有逻辑关系的事实，我也很难接受强加过来的一套体系，除非我能弄清楚它背后的逻辑。而如果一本书，它能讲清楚事实背后的因果关系的脉络，就比较令人愉悦了。
而我前面所说的复杂系统的一些研究成果，就可以用来理解这些文化背后的逻辑规律。我挺喜欢的一个独立学者王东岳，他写了一本书叫做《物演通论》。王东岳很喜欢解读东西方文化背后的脉络，看他写的书就让人有一下子把厚厚的书读薄的感受，非常过瘾。
不过我想，如果我没有读过《失控》及其相关理论，我可能又会对王东岳此人惊为天人，对其著作惊为天书。但在有了前面的知识积累以后，我就不会那么惊讶了。因为王东岳先生的思考，也是建立在大半个世纪以来的科研成果的基础上的。物演的“演”字，就是演化思维的体现。当然，他能够进行提炼和再创造，构造一个完整的知识体系，也相当值得敬佩。
其实说了这么多，我的意思是，文化可以用复杂科学的思维来解构。这个方法，特别适合像我这样的、擅长逻辑思维的理科生们。每当你观察到一个文化现象，你都能解构出这背后的原因，岂不是很有意思呢？
作为一个北方人，我这几年大部分的时间都在厦门，对这里的闽南文化做了饶有兴趣的观察。去过厦门旅游的同学，应该都知道厦门的文艺气氛还挺浓厚的。那为什么厦门会有这种调调呢？还有，你在旅游的时候，应该会发现厦门的一种小吃，叫做沙茶面。那为什么沙茶面会在厦门文化中涌现出来呢？
这就需要结合闽南这个地方的地理、历史等各个要素及其互动关系来做分析。不过，我准备在课后的留言里，再分享我对这几个问题的看法。你有兴趣的话，也可以发表你的观点。
类似的文化方面的问题，还能提出很多来，比如：
为什么泉州会成为海上丝绸之路的起点？ 为什么孔圣人出在山东，而历代出状元最多的省份，却都在南方？ 中国有很多古镇，每个古镇在历史上肯定都是富甲一方的地方，那究竟是什么因素才促使它们兴盛起来的？ 如果某个地方有一个地理标识产品，是某种柿子，你能推测出那里的地质特点吗？ …… 去年的时候，我因为一个项目，翻阅了某县的县志，结果没想到我会对县志如此感兴趣，读得津津有味。我才发现，通过县志我能了解一个地方的地理、历史、经济、文化、重要人物等各种信息。通过这些信息，我基本上就能看到一个由很多要素相互作用构造出来的一个复杂系统，就能读懂当地各种文化的成因，这非常有意思。
中国的很多文化积淀很丰富。如果有机会能够一点一点地解读过去，那该多好。我估计我会一直保持阅读并解读地方志的兴趣。最近回老家，家人又给了我一本我们县在民国时代的县志。看着这些书籍，我有一种强烈的感觉：即使你是在这里生、这里长的，你也不一定真的了解本地的文化。
我为什么会推荐你去读地方志和其他讲解文化现象的书，读懂自己的本地文化呢？
第一层原因，是呼应我在加餐2“学习技术的过程，其实是训练心理素质的过程”中，提倡你多学点“无用之学”的观点。哪怕只是让你的灵魂更有趣，不是也挺好的吗？
第二层原因，是我作为一个理科生的思维方式。把自己所处的社会系统理解清楚，能够透过现象看到后面的逻辑，不是很有意思吗？
第三层原因，如果你能够运用复杂科学的思维，来理解现在的社会系统，其实是有实际意义的。举个例子，如果你要撰写一个商业计划，或者想给一个企业写一套软件，这就需要你理解其当前的商业系统、理解一个企业组织具体是如何运行的。而你之前的这些阅读积累，就会成为你的底蕴，成为你的智慧源泉呀！
小结今天这一讲，我推荐了两本书，或者说是两类书。一类书，是以《失控》为代表，讲述与复杂性相关的话题。另一类书，是以地方志为代表的文化载体。之所以给你推荐这两类书，是因为它们给我如何观察和理解这个世界开启了一扇窗户，并且给我这样一个严谨的理科生，提供了一条去打开文史哲的大门的独特的、有优势的途径，希望能对你有所启发。
思考一下 你有没有阅读过《失控》？你对复杂科学有什么了解？复杂科学在你的领域里有什么应用？ 你对自己出生地的文化了解吗？你有没有曾经发现一个文化现象背后的逻辑脉络？你觉得多研究点文化现象对于自己的职业生涯有好处吗？ 欢迎在留言区发表你的观点。如果今天的加餐让你有所启发，也欢迎把它分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>不定期加餐4_从身边的牛人身上，我学到的一些优秀品质</title><link>https://artisanbox.github.io/7/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/44/</guid><description>你好，我是宫文学。
今天的加餐环节，我想跟你分享一下让我很敬佩的那些牛人，以及从他们身上我所能学到的优秀品质。我希望你也能从这些人的故事上得到启发。这里为了叙述方便，我就不提具体的名字了。你只需要了解他的故事，从中有所感悟就好了。
我把这些牛人分为了两类，一类是搞技术的，一类是创业的。由于我自己也身兼两重身份，所以我很关注这两类人中能带给我启发的人。
首先来说说第一类人，搞技术的，也就是我们常说的极客们。
我所理解的极客我曾经在技术圈子里参加过比较多的活动，特别是开源圈子的活动，因此也接触了不少技术大牛，国内国外的都有。
早在2000年的时候，我就听过理查德·斯托曼（Richard Stallman）的讲座，听他布道自由软件。Stallman是GNU这个组织的创始人，他也发起了GPL开源协议。更重要的是，他是GCC编译器的主要作者，所以跟我们这门课也是很有渊源的。记得当时他给我们放一个幻灯片的时候，用的是Linux上的一个软件，界面没有微软的PowerPoint那么酷炫。但你能想到，Stallman是绝对不会用PowerPoint的。
后来在参加和组织开源技术活动的过程中，我也接触了不少国内国外的技术团队，他们在很多方面刷新了我的认知、拓宽了我的视野，也让我更理解极客都是一些什么样的人。
在我看来，这些人应该就是合格的极客。那么，怎样才能被称为极客？是技术水平高吗？我想不是的。技术水平高，其实只是一个结果。真正让极客显得与众不同的，其实是他们对待技术的态度，乃至是对待人生的态度。这些特质，也能给所有做技术的人一些启发。
首先，是热爱技术。
跟普通人只是把技术作为一个谋生的手段不同，极客们是真心喜欢技术，热衷于钻研和探讨各种技术细节。他们在对待工作的时候，绝不会把某项工作做到能交差就行，他们想要做到完美。
我之前公司的一位股东，他在做程序员的时候，曾经接到领导的一项任务，给了他一块语音板子，让他研究一下能否做呼叫中心的功能。两个星期以后，再问他，技术上是否可行？他说，已经做完了。不仅做完了，他还考虑了各种扩展性。比如，给他的板子只有八个语音口，但他写的程序考虑了用不同的板子，有不同的口的情况。以至于后来很多年，基于他的程序做的呼叫中心系统，底层都不用做很大的改动。
我这位朋友，我在加餐2中也提到过。他因为对于底层软件的深刻理解力，被挖到中间件公司做老总。后来又在创业什么的，最近又写了一套科幻小说。不管什么时候，我总能从他身上吸取到一些东西。
另一个例子，是我一个在苹果工作的同学提到的。这位同学负责苹果的文件系统的开发，我下面还会给你讲他的故事。这里是他讲的另一件事情。一次，一位博士分配到他们组，一时没有合适的工作给他干，就先让他做一阵测试。结果这位老兄，彻底升级了测试系统，对于大量的工作都实现了自动化测试，给整个团队带来了巨大的价值。
这个故事也让我更新了看待测试工作的视角。我现在基本上不会去招聘那些因为对自己的技术能力没有信心，而选择去做测试工作的人。我认为测试工作需要极大的技术热情才能做好。
我想，不管是从事什么岗位，能够热爱自己所做的事情，都是非常值得庆幸的。反过来，如果不喜欢自己所做的事情，为什么要去凑合呢？
换句话说，能够做自己喜欢的事情，其实是有所取舍、有所牺牲的。林纳斯·托瓦兹（Linus Torvalds）就喜欢领着一拨人折腾Linux。如果他去做某个大公司的CTO甚至是创业合伙人，也无不可。但他选择的是自己喜欢的生活方式。他没有太去想自己因此损失了多少发财的机会。
这就涉及到了第二点，就是极客们洒脱的生活态度。
极客们所展现出来的这个特质，从某种意义上来看是更具魅力的。很多极客，都是不愿意以“生活所迫”为借口，选择自己不喜欢的工作和生活方式。
我在加餐1分享远程办公话题的时候，就提到过一家芬兰公司。这家公司都是远程办公的，其中有的员工，是一边全球旅游，一边工作的。这些技术型的公司，正是以这种方式，吸引那些真正的极客加入。
还有一次我参加一个技术活动，我的朋友C指着一个老外说，这家伙在泰国买了一个小岛自己住，还弄了个度假村什么的。说实话，这样的归园田居的生活方式，对像我们这样浸染在中华文化中的人来说，是有很大的吸引力的。但我们有多少人敢于不从众，去选择自己喜欢的生活方式呢？
我还有的朋友是依托自己的技术创业的。创业这件事当然很不容易，但他们通常都会保持积极乐观的态度，并没有因为自己的项目没有及时被社会认可，就变得垂头丧气。
那第三点，就是极客们看待这个世界的方式：用代码说话。
极客这群人，是不大讲政治的。他们一般只认真实的本事。Linus就有一句名言“Talk is cheap, show me the code.”，这也代表了极客们的精神。一个人的水平如何，看看他写的代码，或者至少看看他发表的文章，其实差不多就有数了，这个是伪装不了的。
早在智能手机流行前，有一次聚会，我一个搞Linux的朋友F，就拿出了一台手机，里面装着Linux、图形界面、App什么的，看着都还行。这都是他鼓捣出来的。其实再加把劲，比如支持用Java开发应用，这就是一个Android系统了。而Android的创始人安迪·鲁宾（Andy Rubin），差不多也是这样一个极客。前一阵，我跟一个公司的老总聊天。他问我，为什么中国搞不出安卓来？我给他解释了原因。其实不是我们没有这样的技术，在极客们的眼里，最早的那个安卓版本也没什么大不了的。只不过我们没有掌握技术生态而已。
极客们一般对系统底层的技术都比较熟悉。像安卓系统这种看似很高大上的东西，不会让他们心生畏惧。这些人在互相交流的时候，也会谈论一些底层技术。几句话下来，心里已经有数。
然后呢？他们之间会缔结惺惺相惜的友谊。两个极客之间的交往可以极其简单，他们甚至不需要见过面，只需要见过对方的代码，或者读过对方的文章，就会认可彼此。如果有事情，直接打招呼就行。
某互联网大厂是如何把自己的底层技术搞扎实的呢？据我了解，就是找到一个开源圈的大牛。这位大牛进去了以后，又给技术社区的其他人打招呼，说这里有什么技术难题需要解决，过来吧。于是就聚集了一个小组的牛人，搞出了非常好的成绩。这就是极客们的特殊的社交方式：他们知道who is who，并且志同道合的人愿意聚到一起。如果光靠HR部门和猎头公司来做，要想达成这样的结果是很难的。
Github在某种意义上也是把极客们的这种倾向给充分地引导了出来。它从一个代码托管工具，几乎已经变成了程序员的社交网站。
这里我是想说明一个观点，那就是技术人并没有怀才不遇这一说。把真本事亮出来，所有的事情会变得简单很多。
好了，这就是我总结的极客们给我的三点印象：热爱技术、生活洒脱、凭本事说话。这些特质，都是我很欣赏的，也常常作为参照来调整自己。
比如说，我觉得自己也挺热爱技术的，但是在前些年，我觉得自己不够洒脱，做不好取舍，总是想各方面都兼顾，结果哪方面都顾不好。所以还不如在自己喜欢的事情上全情投入，不去计较太多得失，反倒会更加心情愉快，做事情的结果也更好。
你可能会问，那这些极客都发展得怎么样呢？
我所认识的极客，有的是在小公司工作，有的是在大公司工作，还有的是在创业。不过，不管从事什么岗位，似乎都发展得不错。我想，这是因为他们从底层上选择了一个更好的发展逻辑：首先是做好取舍，让自己能够专注技术；在拥有了比较好的技术底蕴以后，他们也有更好的施展自己才华的平台；在专注于技术价值的同时，他们的生活也变得简单和健康。
OK，讲完了搞技术的，我再讲讲搞创业的朋友的故事，以及他们给我的启迪。
创业者这个物种我周围的朋友有不少是搞创业的。这些人往往都有一些很特别的点，让我欣赏、赞叹乃至仰慕。
首先一点，是坚韧不拔的意志力。
我们都知道，创业肯定不是简单的事情。而让一个企业能够穿越惊涛骇浪，最重要的就是创始人坚韧不拔的意志品质。
我本科的同班同学中，就有一个创业者，他公司的主营业务是户外运动用品，目前已经做到上市了。他的性格就很坚韧，我给你讲两个故事。
第一个，在他成为我北大的同学之前，其实曾经考上了一所技术类的军校。但后来他发现自己并不喜欢那里，于是就想退学。可是，军校岂可以当作儿戏，想来就来，想走就走？为了能够退学，他想了很多办法，包括自己注射大剂量的抗生素，产生精神恍惚的效果，以便让校医诊断为精神疾病；此法不成，又从3楼的阳台上滚下来，想把胳膊摔断，以此理由退学……后来校领导实在看他态度坚决，也就同意了他退学。他又重新参加了高考，选择了他喜欢的学校和专业。
第二个，大约在2006年，我们一些同学因为毕业十周年又聚到了一起，去内蒙古草原上玩，其中一项活动就是骑马。我的这位同学骑术很好，在草原上策马狂奔。不过，在一个地方，突然马失前蹄，他从马背上摔了下来。这真是很惊险的一个意外，我们在场一群人看了都心惊肉跳。不过，他休息了一会以后，又要了一匹马，上马继续策马狂奔。晚上，我们问他，为什么刚摔了又骑？他说，如果今天不骑，以后就不敢骑了。
说到这，我想再多讲一个例子。这是我同一级的另一个同学的故事，他是社会学系的。如果我说他的名字，很多同学应该都会知道。他从2000年开始做一个与汽车有关的网站，结果后来互联网泡沫破裂，然后投资人撤资。他就自己筹了2000万买下了投资人的股份，坚持做了下去，直到2011年上市。想想看，那个年头的2000万，是多大的压力。但他就是咬着牙挺过来了。
我不知道有多少人能拥有像他们这样钢铁般的意志力。并且，令我沮丧的是，我怀疑这种个性可能主要是天生的？反正我是万万难以做到的。所以，创业这件事情，其实不是每个人都适合去做。而我这两个同学能做到上市，也绝不是偶然。
不过，为了不让自己的希望完全破灭，我还是倾向于相信意志和勇气这样的事情，至少在部分上是可以后天磨炼的。我在大学的时候练习过拳击，因为我觉得拳击可以锻炼人的勇气。来拳的时候不能眨眼，是拳击运动员的基本素质。那么在创业中，如果我们每次都去积极地面对挑战，那面对困难的能力也会越来越强。
我认识的其他几个创业者，虽然不像这两位那么夸张，但在意志力方面，也都属于罕见的。比如，某个技术社区，其创始人能够做到天天更新内容，十年如一日，这就是常人所不及的。最近我通过写编译原理的课程，也对内容编写这件事有了一定的体会。这样的事情，做一个星期、一个月、一个季度，是凭着兴趣和热情就可以做到的。而长年累月地去做，你要说没有意志力的因素，那是不可能的。
说完了强大的意志力，我再来说说我钦佩的这些人的第二点品质，就是有主见，不从众。
我观察，这些创业成功的人，往往判断事情都有自己的标准，这些标准往往与大众是不一致的。
还是说我同班的那个同学。在学校读书的时候，他经常就会消失不见了，过一阵再重新出现，他告诉我们，这次去陕北了，有什么感受，怎样怎样。过一段时间又会消失，回来后，说自己在新疆沙漠里独自走了几天，遇到被人追赶，差点殒命，等等等等。
等到快期末考试的时候，他拉着我在未名湖边给他补习高数，说能及格就行。几年以后，在创业的过程中，他还读了个清华的MBA班，也是连毕业证都没要。按他的意思来说，就是：学到知识就行了。证书什么的，不重要。
而我们这些俗人，天天使劲读书。等到毕业以后，又根据习惯和潮流，很多人又去出国，虽然我敢说，大部分同学那时候都想不清楚出国到底要干嘛。
所以从某种意义上来讲，比尔·盖茨、马克·扎克伯格等人敢于辍学创业，本身就意味着他们不是一般人。
而作为对比，还有一些人，都不管自己什么年纪了还在花高价去混文凭，不停地想往自己身上贴一些标签，来为自己壮胆。我觉得，这些人不要说创业了，给他一个重要的职位都是一件很冒险的事情。
前面我也提到了有些极客，会基于自己的兴趣爱好来创业。他们喜欢的技术和产品，往往在很长的时间内都不会得到社会的认可，不能变成有经济价值的商品。然而他们就是会坚持自己的方向。这些人，也是我学习的榜样。
这些技术创业者，有的发展比较顺利，但似乎也不是刻意为之。比如上海的小X，我跟他在技术活动上有几面之缘。他搞了一个用于物联网的小小的OS，搞了很多年了，前两年突然听说融了很多资，估值不错。我觉得资本投在这些人身上是投对了。
也有的朋友，会经历一些坎坷。但是他们总是按照自己的方式去折腾，保持对科技发展趋势的敏锐观察。每隔一段时间，我总能从他们那里听到一些新的思想和动态。就拿我一个做移动端底层平台朋友来说，他做这个方向已经很多年了。我相信他肯定会做成。不过先不说未来结果如何，至少我觉得他的生活状态是洒脱的、阳光的、不纠结的。
小结今天的加餐，我给你分享了周围搞技术的和做创业的一些朋友的故事。这些故事跟你有什么关联呢？
首先，你选择了编译原理这门课程，基本上已经说明你有成为一名极客的潜质，否则也不会给自己这个挑战。但是在这个过程中呢，你可能会遇到很多的困难和心理上的纠结。我希望通过我分享的故事，能够帮助你做好取舍，丢掉包袱，健康阳光地拥抱作为一个技术从业者的职业生涯。
而如果你不小心选择了创业这条路，我也希望你能够像故事中的人物一样，去磨炼自己的意志力，以及坚持自己的主见。成功不成功不敢保证，至少你的生活会是很有价值的，不会后悔的。
以上也是对我自己的勉励，希望能跟你共勉。如果你或你身边也有类似的故事，欢迎在留言区分享出来。同样，也非常欢迎你把这一讲分享出去。</description></item><item><title>不定期加餐5_借助实例，探究C++编译器的内部机制</title><link>https://artisanbox.github.io/7/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/45/</guid><description>你好，我是宫文学。欢迎来到编译原理实战课的加餐环节，今天我们来探讨一下C++的编译器。
在前面的课程中，我们已经一起解析了很多语言的编译器了，但一直没有讨论C和C++的编译器。并不是因为它们不重要，而是因为C语言家族的编译器实现起来要更复杂一些，阅读代码的难度也更高一些，会对初学者造成比较大的挑战。
不过，没有解析C和C++语言的特性及其编译器的实现，其实在我心里也多多少少有点遗憾，因为C和C++是很经典的语言。至今为止，我们仍然有一些编程任务是很难用其他语言来代替的，比如，针对响应时间和内存访问量，需要做精确控制的高性能的服务端程序，以及一些系统级的编程任务，等等。
C和C++有很多个编译器，今天我们要研究的是Clang编译器。其实它只是前端编译器，而后端用的是LLVM。之所以选择Clang，是因为它的模块划分更清晰，更便于理解，并且还可以跟课程里介绍过的LLVM后端工具串联起来学习。
另外，因为C++语言的特性比较多，编译器实现起来也比较复杂一些，下手阅读编译器的源代码会让人觉得有点挑战。所以今天这一讲，我的主要目的，就是给你展示如何借助调试工具，深入到Clang的内部，去理解它的运行机制。
我们会具体探究哪个特性呢？我选择了C++的模板技术。这个技术是很多人学习C++时感觉有困难的一个技术点。通过探究它在编译器中的实现过程，你不仅会加深了解编译器是如何支持元编程的，也能够加深对C++模板技术本身的了解。
那么下面，我们就先来认识一下Clang这个前端。
认识ClangClang是LLVM的一个子项目，它是C、C++和Objective-C的前端。在llvm.org的官方网站上，你可以下载Clang+LLVM的源代码，这次我用的是10.0.1版本。为了省事，你可以下载带有全部子项目的代码，这样就同时包含了LLVM和Clang。然后你可以参考官网的文档，用Cmake编译一下。
我使用的命令如下，你可以参考：
cd llvm-project-10.0.1 #创建用于编译的目录 mkdir build cd build
#生成用于编译的文件 cmake -DCMAKE_BUILD_TYPE=Debug -DLLVM_TARGETS_TO_BUILD=&amp;quot;X86&amp;quot; -DLLVM_BUILD_EXAMPLES=ON ../llvm
#调用底层的build工具去执行具体的build cmake &amp;ndash;build . 这里你要注意的地方，是我为Cmake提供的一些变量的值。我让Cmake只为x86架构生成代码，这样可以大大降低编译工作量，也减少了对磁盘空间的占用；并且我是编译成了debug的版本，这样的话，我就可以用LLDB或其他调试工具，来跟踪Clang编译C++代码的过程。
编译完毕以后，你要把llvm-project-10.0.1 /build/bin目录加到PATH中，以便在命令行使用Clang和LLVM的各种工具。你可以写一个简单的C++程序，比如说foo.cpp，然后就可以用“clang++ foo.cpp”来编译这个程序。
补充：如果你像我一样，是在macOS上编译C++程序，并且使用了像iostream这样的标准库，那么可能编译器会报找不到头文件的错误。这是我们经常会遇到的一个问题。
&amp;nbsp;
这个时候，你需要安装Xcode的命令行工具。甚至还要像我一样，在.zshrc文件中设置两个环境变量：
export CPLUS_INCLUDE_PATH=&amp;quot;/Library/Developer/CommandLineTools/usr/include/c++/v1:$CPLUS_INCLUDE_PATH&amp;quot; export SDKROOT=&amp;quot;/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk&amp;quot; 好，到目前为止，你就把Clang的环境配置好了。那回过头来，你可以先去看看Clang的源代码结构。
你会看到，Clang的源代码主要分为两个部分：头文件（.h文件）全部放在include目录下，而.cpp文件则都放在了lib目录下。这两个目录下的子目录结构是一致的，每个子目录代表了一个模块，模块的划分还是很清晰的。比如：
AST目录：包含了AST的数据结构，以及对AST进行遍历处理的功能。 Lex目录：词法分析功能。 Parse目录：语法分析功能。 Sema目录：语义分析功能（Sema是Sematic Analysis的缩写）。 接下来，你可以进入这些目录，去寻找一下词法分析、语法分析、语义分析等功能的实现。由于Clang的代码组织很清晰，你可以很轻松地根据源代码的名称猜到它的功能，从而找到语法分析等功能的具体实现。
现在，你可以先猜测一下，Clang的词法分析和语法分析都是如何实现的呢？
如果你已经学过了第二个模块中几个编译器的实现，可能就会猜测得非常准确，因为你已经在Java编译器、Go的编译器、V8的编译器中多次见到了这种实现思路：
词法分析：手写的词法分析器，也就是用手工的方法构造有限自动机。 语法分析：总体上，采用了手写的递归下降解析器；在表达式解析部分，采用的是运算符优先级解析器。 所以，针对词法分析和语法分析的内容，我们就不多展开了。
那么，Clang的语义分析有什么特点呢？
通过前面课程的学习，现在你已经知道，语义分析首先要做的是建立符号表，并做引用消解。C和C++在这方面的实现比较简单。简单在哪里呢？因为它要求必须声明在前，使用在后，这就让引用消解变得很简单。
而更现代一些的语言，在声明和使用的顺序上可以更加自由，比如Java类中，方法中可以引用类成员变量和其他方法，而被引用的成员变量和方法可以在该方法之后声明。这种情况，对引用消解算法的要求就要更高一些。
然后，现在你也知道，在语义分析阶段，编译器还要做类型检查、类型推导和其他很多的语义检查。这些方面Clang实现得也很清晰，你可以去看它的StaticAnalysis模块。
最后，在语义分析阶段，Clang还会做一些更加复杂的工作，比如C++的模板元编程机制。
我在探究元编程的那一讲中，介绍过C++的模板机制，它能有效地提高代码的复用能力。比如，你可以实现一个树的容器类，用来保存整型、浮点型等各种不同类型的数据，并且它不会像Java的泛型那样浪费额外的存储空间。因为C++的模板机制，会根据不同的模板类型生成不同的代码。
那么，C++具体是如何实现这一套机制的呢？接下来我就带你一起去深入了解一下，从而让你对模板元编程技术的理解也更加深入。
揭秘模板的实现机制首先，我们通过一个示例程序，来观察一下Clang是如何编译模板程序的。假设，你写了一个简单的函数min，用来比较两个参数的大小，并返回比较小的那个参数。
int min(float a, float b){ return a&amp;lt;b ? a : b; } 你可以用clang++命令带上“-ast-dump”参数来编译这个示例程序，并显示编译后产生的AST。</description></item><item><title>不定期福利第一期_数据结构与算法学习书单</title><link>https://artisanbox.github.io/2/61/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/61/</guid><description>你好，我是王争。欢迎来到不定期更新的周末福利时间。
专栏已经上线两周了，看到这么多人在留言区写下自己的疑惑或者观点，我特别开心。在留言里，很多同学让我推荐一些学习数据结构与算法的书籍。因此我特意跟编辑商量了，给你一个周末福利。所以这一期呢，我们就来聊一聊数据结构和算法学习过程中有哪些必读书籍。
有的同学还在读大学，代码还没写过几行；有的同学已经工作数十年，这之间的差别还是挺大的。而不同基础的人，适宜看的书是完全不一样的。因此，针对不同层次、不同语言的同学，我分别推荐了不同的书。希望每个同学，都能找到适合自己的学习资料，都能在现有水平上有所提高。
针对入门的趣味书入门的同学，我建议你不要过度追求上去就看经典书。像《算法导论》《算法》这些书，虽然比较经典、比较权威，但是非常厚。初学就去啃这些书肯定会很费劲。而一旦啃不下来，挫败感就会很强。所以，入门的同学，我建议你找一些比较容易看的书来看，比如《大话数据结构》和《算法图解》。不要太在意书写得深浅，重要的是能不能坚持看完。
《大话数据结构》 这本书最大的特点是，它把理论讲得很有趣，不枯燥。而且每个数据结构和算法，作者都结合生活中的例子进行了讲解，能让你有非常直观的感受。虽然这本书有400多页，但是花两天时间读完，应该是没问题的。如果你之前完全不懂数据结构和算法，可以先从这本书看起。
《算法图解》 跟《大话数据结构》走的是同样的路线，就像这本书副标题写的那样，“像小说一样有趣的算法入门书”，主打“图解”，通俗易懂。它只有不到200页，所以内容比较少。作为入门，看看这本书，能让你对数据结构和算法有个大概的认识。
这些入门书共同的问题是，缺少细节，不够系统，也不够严谨。所以，如果你想要系统地学数据结构和算法，看这两本书肯定是不够的。
针对特定编程语言的教科书讲数据结构和算法，肯定会跟代码实现挂钩。所以，很多人就很关心，某某书籍是用什么语言实现的，是不是自己熟悉的语言。市面大部分数据结构和算法书籍都是用C、C++、Java语言实现的，还有些是用伪代码。而使用Python、Go、PHP、JavaScript、Objective-C这些编程语言实现的就更少了。
我这里推荐《数据结构和算法分析》。国内外很多大学都拿这本书当作教材。这本书非常系统、全面、严谨，而且又不是特别难，适合对数据结构和算法有些了解，并且掌握了至少一门编程语言的同学。而且，这个作者也很用心。他用了三种语言，写了三个版本，分别是：《数据结构与算法分析 ：C语言描述》《数据结构与算法分析：C++描述》《数据结构与算法分析：Java语言描述》。
如果你熟悉的是Python或者JavaScript，可以参考《数据结构与算法JavaScript描述》《数据结构与算法：Python语言描述》 。至于其他语言的算法书籍，确实比较少。如果你有推荐，可以在留言区补充一下。
面试必刷的宝典算法对面试很重要，很多人也很关心。我这里推荐几本有益于面试的书籍，分别是：《剑指offer》《编程珠玑》《编程之美》。
从《剑指offer》这本书的名字就可以看出，作者的写作目的非常明确，就是为了面试。这本书几乎包含了所有常见的、经典的面试题。如果能搞懂这本书里的内容，应付一般公司的面试应该不成问题。
《编程珠玑》这本书的豆瓣评分非常高，有9分。这本书最大的特色就是讲了很多针对海量数据的处理技巧。这个可能是其他算法书籍很少涉及的。面试的时候，海量数据处理的问题也是经常会问的，特别是校招面试。不管是开拓眼界，还是应付面试，这本书都很值得一看。
《编程之美》这本书有多位作者，其中绝大部分是微软的工程师，所以书的质量很有保证。不过，这里面的算法题目稍微有点难，也不是很系统，这也是我把它归到面试这一部分的原因。如果你有一定基础，也喜欢钻研些算法问题，或者要面试Google、Facebook这样的公司，可以拿这本书里的题，先来自测一下。
经典大部头很多人一提到算法书就会搬出《算法导论》和《算法》。这两本确实非常经典，但是都太厚了，看起来比较费劲，我估计很少有人能坚持全部看下来。如果你想更加深入地学一学数据结构和算法，我还是强烈建议你看看。
我个人觉得，《算法导论》这本书的章节安排不是循序渐进的，里面充斥着各种算法的正确性、复杂度的证明、推导，数学公式比较多，一般人看起来会比较吃力。所以，作为入门书籍，并不是很推荐。
《算法》这本书也是一本经典大部头，不过它比起《算法导论》来要友好很多，更容易看懂，更适合初学者入门。但是这本书的缺点也很明显，就是内容不够全面，比如动态规划这么重要的知识点，这本书就没有讲。对于数据结构的东西，它讲的也不多，基本就是偏重讲算法。
殿堂级经典说到殿堂级经典书，如果《计算机程序设计艺术》称第二，我想没人敢称第一。这本书包括很多卷。说实话，我也只看过比较简单的几卷，比如《基本算法》《排序和查找》。
这套书的深度、广度、系统性、全面性是其他所有数据结构和算法书籍都无法相比的。但是，如果你对算法和数据结构不是特别感兴趣，没有很好的数学、算法、计算机基础，想要把这套书读完、读懂是比较难的。你可以把它当作你算法学习的终极挑战。
闲暇阅读算法无处不在。我这里再推荐几本适合闲暇时间阅读的书：《算法帝国》《数学之美》《算法之美》。
这些书共同的特点是，都列举了大量的例子，非常通俗易懂。夸张点说，像《算法帝国》，文科生都能读懂。当你看这些书的时候，你常常会深深感受到算法的力量，被算法的优美之处折服。即便不是从事IT工作的，看完这几本书也可以开拓眼界。
书籍差不多就是这些。除此之外，留言区很多人问到算法的实现语言。我这里也解释一下。因为我现在比较常用的编程语言是Java。所以，在专栏里，特别简单的、不涉及高级语法的，我会用Java或者C、C++来实现。稍微复杂的，为了让你能看懂，我会用伪代码。所以你完全不用担心语言的问题。
每节课中有需要代码实现的数据结构和算法，我都另外用Java语言实现一遍，然后放到Github上，供你参考。Github的地址我放在这里，你可以收藏一下：https://github.com/wangzheng0822/algo。
至于其他语言的同学，比如C、C++、Python、Go、PHP、JavaScript、Objective-C等，我想了一个crowd sourcing的方法。
我希望基础较好的同学，参照我的Java实现，用你熟悉的编程语言再实现一遍，并且将代码留言给我。如果你写得正确，我会将你的代码上传到Github上，分享给更多人。
还有人问，我学完这个专栏，就可以拿下数据结构和算法吗？我想说的是，每个人的基础、学习能力都不一样，掌握程度取决于你的努力程度。除了你之外，没有人能百分之百保证你能掌握什么知识。
有的同学只是把每一节课听下来、看下来，就束之高阁，也不求甚解，那效果肯定会很差。而有些同学除了听、看之外，遇到不懂的会自己去查资料、看参考书籍，还会把我讲的数据结构和算法都认真地实现一遍，这样的学习效果自然就比只听一遍、看一遍要好很多。即便我已经尽我所能把这些知识讲得深入浅出，通俗易懂，但是学习依然还是要靠你自己啊。
这种答疑的方式也会成为我们之后的固定动作，我会把留言里有价值的问题和反馈沉淀下来，希望对你的日常学习起到补充作用。如果你有什么看不懂、听不懂的地方，或者工作中有遇到算法问题、技术难题，欢迎写在留言区。（我发现留言区里卧虎藏龙啊，没事儿可以多扫扫留言区。）
这次的周末福利时间就到这啦，我们下次见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>不定期福利第三期_测一测你的算法阶段学习成果</title><link>https://artisanbox.github.io/2/59/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/59/</guid><description>专栏最重要的基础篇马上就要讲完了，不知道你掌握了多少？我从前面的文章中挑选了一些案例，稍加修改，组成了一套测试题。
你先不要着急看答案，自己先想一想怎么解决，测一测自己对之前的知识掌握的程度。如果有哪里卡壳或者不怎么清楚的，可以回过头再复习一下。
正所谓温故知新，这种通过实际问题查缺补漏的学习方法，非常利于你巩固前面讲的知识点，你可要好好珍惜这次机会哦！
实战测试题（一）假设猎聘网有10万名猎头顾问，每个猎头顾问都可以通过做任务（比如发布职位），来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这10万个猎头ID和积分信息，让它能够支持这样几个操作：
根据猎头的ID快速查找、删除、更新这个猎头的积分信息；
查找积分在某个区间的猎头ID列表；
查询积分从小到大排在第x位的猎头ID信息；
查找按照积分从小到大排名在第x位到第y位之间的猎头ID列表。
相关章节17 | 跳表：为什么Redis一定要用跳表来实现有序集合？
20 | 散列表（下）：为什么散列表和链表经常会一起使用？
25 | 红黑树：为什么工程中都用红黑树这种二叉树？
题目解析这个问题既要通过ID来查询，又要通过积分来查询，所以，对于猎头这样一个对象，我们需要将其组织成两种数据结构，才能支持这两类操作。
我们按照ID，将猎头信息组织成散列表。这样，就可以根据ID信息快速地查找、删除、更新猎头的信息。我们按照积分，将猎头信息组织成跳表这种数据结构，按照积分来查找猎头信息，就非常高效，时间复杂度是O(logn)。
我刚刚讲的是针对第一个、第二个操作的解决方案。第三个、第四个操作是类似的，按照排名来查询，这两个操作该如何实现呢？
我们可以对刚刚的跳表进行改造，每个索引结点中加入一个span字段，记录这个索引结点到下一个索引结点的包含的链表结点的个数。这样就可以利用跳表索引，快速计算出排名在某一位的猎头或者排名在某个区间的猎头列表。
实际上，这些就是Redis中有序集合这种数据类型的实现原理。在开发中，我们并不需要从零开始代码实现一个散列表和跳表，我们可以直接利用Redis的有序集合来完成。
实战测试题（二）电商交易系统中，订单数据一般都会很大，我们一般都分库分表来存储。假设我们分了10个库并存储在不同的机器上，在不引入复杂的分库分表中间件的情况下，我们希望开发一个小的功能，能够快速地查询金额最大的前K个订单（K是输入参数，可能是1、10、1000、10000，假设最大不会超过10万）。如果你是这个功能的设计开发负责人，你会如何设计一个比较详细的、可以落地执行的设计方案呢？
为了方便你设计，我先交代一些必要的背景，在设计过程中，如果有其他需要明确的背景，你可以自行假设。
数据库中，订单表的金额字段上建有索引，我们可以通过select order by limit语句来获取数据库中的数据；
我们的机器的可用内存有限，比如只有几百M剩余可用内存。希望你的设计尽量节省内存，不要发生Out of Memory Error。
相关章节12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？
28 | 堆和堆排序：为什么说堆排序没有快速排序快？
29 | 堆的应用：如何快速获取到Top 10最热门的搜索关键词？
题目解析解决这个题目的基本思路我想你应该能想到，就是借助归并排序中的合并函数，这个我们在排序（下）以及堆的应用那一节中讲过。
我们从每个数据库中，通过select order by limit语句，各取局部金额最大的订单，把取出来的10个订单放到优先级队列中，取出最大值（也就是大顶堆堆顶数据），就是全局金额最大的订单。然后再从这个全局金额最大订单对应的数据库中，取出下一条订单（按照订单金额从大到小排列的），然后放到优先级队列中。一直重复上面的过程，直到找到金额前K（K是用户输入的）大订单。
从算法的角度看起来，这个方案非常完美，但是，从实战的角度来说，这个方案并不高效，甚至很低效。因为我们忽略了，数据库读取数据的性能才是这个问题的性能瓶颈。所以，我们要尽量减少SQL请求，每次多取一些数据出来，那一次性取出多少才合适呢？这就比较灵活、比较有技巧了。一次性取太多，会导致数据量太大，SQL执行很慢，还有可能触发超时，而且，我们题目中也说了，内存有限，太多的数据加载到内存中，还有可能导致Out of Memory Error。
所以，一次性不能取太多数据，也不能取太少数据，到底是多少，还要根据实际的硬件环境做benchmark测试去找最合适的。
实战测试题（三）我们知道，CPU资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致CPU频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。
当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？
相关章节09 | 队列：队列在线程池等有限资源池中的应用</description></item><item><title>不定期福利第二期_王争：羁绊前行的，不是肆虐的狂风，而是内心的迷茫</title><link>https://artisanbox.github.io/2/58/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/58/</guid><description>你好，我是王争。
专栏更新过半，我发现有些小伙伴已经掉队，虽然有人掉队也挺正常，但是我还是想尽量拉一把。于是，周末的时间，我就在想，究竟是什么原因让有些小伙伴掉队了？是内容本身太难了吗？是我讲得不够清楚吗？还是小伙伴本身基础太差、不够努力、没有掌握学习方法？
我觉得都不是，让你掉队的原因，从根儿上讲，是你内心的迷茫。如果我们不那么确信能不能看懂、能不能学会的时候，当面对困难的时候，很容易就会否定自己，也就很容易半途而废。
这就好比你迷失在沙漠中，对你来说，肆虐的狂风并不可怕，可怕的是，你不知道该努力多久才能走出沙漠，不知道到底能不能走出沙漠。这种对结果的未知、不确定，导致了你内心的恐惧，最后就差那么一点点就可以走出沙漠的时候，你放弃了。
学习也是同样的道理。所以，我今天不打算讲学习方法，也不打算给你灌输心灵鸡汤，我就讲讲，对这个专栏的学习，或者对于任何学习来说，我觉得你应该建立的一些正确认知。有了这些认知，希望你能在后面的专栏学习中，少一点迷茫，多一份坚持。
没有捷径，没有杀手锏，更没有一招致胜的“葵花宝典”有小伙伴给我留言说：“看书五分钟，笔记两小时，急求学霸的学习方法”，还有人问，“数据结构和算法好难，到底该怎么学？是我的学习方法不对？还是我太笨？”
我想说，并没有什么杀手锏的学习方法，更没有一招致胜的“葵花宝典”。不知道这么说有没有让你失望。如果你真要“求”一个学习方法，那就再看看我在专栏开始写的“如何抓住重点，系统高效地学习数据结构与算法”那篇文章吧。
说实话，我也挺想知道学霸的学习方法的，所以，在求学路上，每当有学霸来分享学习方法，我都要去听一听。但是，听多了之后，我发现其实并没有太多用。因为那些所谓学霸的学习方法，其实都很简单，比如“认认真真听讲”“认认真真做每一道题”等等。
也不是他们说的不对，但是这种大实话，我总有一种领会不了的感觉，更别说真正指导我的学习了。而且，我觉得，很多时候，这些方法论的难点并不在于能不能听懂，而是在于能不能执行到位。比如很多人都听过“一万小时定律”，坚持一万个小时，你就能成为大牛，但有多少人能坚持一万个小时呢？
所以，这里我要纠正一个认知，那就是，学习没有“杀手锏”似的方法论。不要怀疑是不是自己的学习方法不对，不要在开始就否定自己。因为否定得越多，你就越迷茫，越不能坚持。
不要浮躁，不要丧失思考能力，不要丧失学习能力有小伙伴给我留言说：“老师，这个地方看不懂，你能不能再解释一下”，还有小伙伴留言说：“《红黑树（上）》里的图为什么跟你的定义不相符？”
对于留言的问题，我都挺重视的，但是当仔细看这些问题的时候，我发现，实际上文章里已经有答案了，他根本没有认真看、认真思考，更别说去自己搜搜资料，再研究下，就来提问了。
一般情况下，我都会回复“你自己再认真看一遍”或者“你自己先去网上搜一下，研究研究，如果还不懂再给我留言”。告诉你答案，并不会花费我太长时间，但是，这样会让你丢失最宝贵的东西，那就是，你自己的思考能力、学习能力，能自己沉下心来研究的能力。这个是很可怕的。
现在，互联网如此发达，我们每天都会面对各种各样的信息轰炸，人也变得越来越浮躁。很多人习惯看些不动脑子就能看懂的东西，看到稍微复杂的东西，就感觉脑子转不动了。
上学的时候还好，要考试，有老师督促，还能坚持学习。但是工作之后，没有人监督，很多人陷入各种手机App中不能自拔，学一会儿就想玩会儿手机，想静下心来学上半个小时都无比困难。无法自律，沉不下心来，那你就基本可以跟学习说拜拜了。
只有做好打硬仗的心理准备，遇到困难才能心态平和还有小伙伴给我留言说：“看不懂，一个4000多字的文章、10分钟的音频，反复看了、听了2个小时都没怎么看懂”。我给他的回复是：“如果之前没有基础或者基础不好的话，看2个小时还不懂，很正常，看一个礼拜试试。”
“一个礼拜”的说法，我一点都不是夸张。虽然专栏的每篇文章都只有三四千字，10分钟左右的音频，但是知识点的密度还是很高的。如果你潜意识里觉得应该一下子就能看懂，就会出现这样的情况：看了一遍不懂，又看了一遍还是不怎么懂，然后就放弃了。
数据结构和算法就是一个非常难啃的硬骨头，可以说是计算机学科中最难学的学科之一了。我当时学习也费了老大的劲，能做到讲给你听，我靠的也是十年如一的积累和坚持。如果没有基础、或者基础不好，你怎能期望看2个小时就能完全掌握呢？
面对这种硬骨头，我觉得我们要有打硬仗、打持久战的心理准备。只有这样，在学习的过程中遇到困难的时候，心态才能更加平和，才能沉下心来有条不紊地去解决一个个的疑难问题。这样，碰到问题，你可能还会“窃喜”，我又遇到了一个之前不怎么懂的知识点了，看懂它我又进步了一点。甚至你还会“坏坏地”想，又多了一个拉开我跟其他人距离的地方了。跨过这些点，我就能比别人更厉害。
一口吃不成胖子，如果你基础不好，那就从长计议吧，给自己定一个长一点的“死磕”计划，比如一年。面对不懂的知识点，沉下心来逐个突破，这样你的信心慢慢也就建立了。
“放弃”的念头像是一个心魔，它会一直围绕着你还有小伙伴给我留言说：“开始没怎么看懂，看了一下午，终于看懂了”。看到这样的留言，我其实挺为他感到庆幸的，庆幸他没有中途放弃。因为，放弃的念头就像一个心魔，在我们的学习过程中，它会一直围绕着我们，一旦被它打败一次，你就会被它打败很多次，掉队就不可避免了。
我分享一个我最近思考比较多的事情。前一段时间，我在研究多线程方面的东西，它涉及一块比较复杂的内容，“Java内存模型”。虽然看懂并不难，但是要透彻、无盲点地理解并不容易。本来以为半天就能看懂的东西，结果我从周一一直看到周五下午，断断续续花了5天的时间才把它彻底搞懂。回忆起这5天，我有不下10次都想放弃，每次心里都在想：“算了，先放一放，以后再说吧”“太难了，啃不下来，算了。”“就这样吧，反正也用不到，没必要浪费时间”等等。这种放弃的念头就像一个邪恶的魔鬼一样，一直围绕着我这5天的研究中。
现在回想起来，我很庆幸我当时没有放弃，多坚持了几天。如果当时我放弃了，那之后再遇到技术难题时，“放弃”的心魔还会再来拜访我，潜意识里我还是会认输。
之所以没有放弃，我自己总结了两点原因。
第一，我对学习这件事情认识得比较清楚，我一直觉得，没有学不会的东西，没有攻克不了的技术难题，如果有，那就说明时间花得还不够多。
第二，我之前遇到卡壳的时候，几乎从来没有放弃过，即便短暂地停歇，我也会继续拎起来再死磕，而且每次都能搞定，正是这种正向的激励，给了我信心，让我再遇到困难的时候，都能坚信自己能搞定它。
入门是一个非常漫长和煎熬的过程，谁都逃不过还有小伙伴留言说：“看到有小伙伴有很多疑问，我来帮作者说句话，文章写得很好，通俗易懂，如果有一定基础，看懂还是不成问题的。”
我觉得，有些小伙伴的觉悟还是挺高的：）。我文章写得再通俗易懂，对于之前没有任何基础的人来说，看起来还是挺费劲的。
第一，数据结构和算法这门课程本身的难度摆在那里，想要轻松看懂，本身就不太现实。第二，对于任何新知识的学习，入门都是一个非常漫长和煎熬的过程。但是这个过程都是要经历的，谁都逃不过。只要你挺过去，入了门，再学习更深的知识就简单多了。
我大学里的第一堂课是C语言，现在回想起来，当时对我来说，简直就是听天书。因为之前没有接触过计算机，更别说编程语言，对我来说，C语言就像另一个世界的东西。从完全看不懂，到慢慢有点看懂，再到完全看懂，不夸张地讲，我花了好几年的时间，但是当掌握了之后，我发现这个东西其实也不难。但是如果没有度过漫长和煎熬的入门的过程，如果没有一点韧性，没有一点点信念，那可能也没有现在的我了。
其实我一直觉得情商比智商更重要。对于很多学科的学习，智商并不是瓶颈，最终能够决定你能达到的高度的，还是情商，而情商中最重要的，我觉得就是逆商（逆境商数，Adversity Quotient），也就是，当你遇到困难时，你会如何去面对，这将会决定你的人生最终能够走多远。
好了，今天我想分享的关于学习的几个认知就讲完了。现在，你有没有对学习这件事有更加清晰的认识呢？能不能让你少一点迷茫，多一份坚持呢？
最后，我有一句送给你：吃得苦中苦，方为人上人。耐得住寂寞，才能守得住繁华。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>不定期福利第四期_刘超：我是怎么学习《数据结构与算法之美》的？</title><link>https://artisanbox.github.io/2/60/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/60/</guid><description>你好，我是刘超，是隔壁《趣谈网络协议》专栏的作者。今天来“串个门儿”，讲讲我学习《数据结构与算法之美》这个专栏的一些体会和感受。
《数据结构与算法之美》是目前“极客时间”订阅量最多的专栏，我也是其中最早购买的一员。我之所以一看就心动了，源于王争老师在开篇词里面说的那段话：
基础知识就像是一座大楼的地基，它决定了我们的技术高度。那技术人究竟都需要修炼哪些“内功”呢？我觉得，无外乎就是大学里的那些基础课程，操作系统、计算机网络、编译原理等等，当然还有数据结构和算法。
这个也是我写《趣谈网络协议》的时候，在开篇词里反复强调的观点。我为什么这么说呢？因为，我们作为面试官，在招人的时候，往往发现，使用框架速成的人很多，基础知识扎实的人少见，而基础不扎实会影响你以后学习新技术的速度和职业发展的广度。
和“极客时间”编辑聊的时候，我也多次表达，希望我们讲的东西和一般的培训机构有所区别，希望“极客时间”能做真正对程序员的技能提升和职业发展有价值的内容，希望“极客时间”能够成为真正帮助程序员成长的助手。
所以，当“极客时间”相继推出《Java核心技术36讲》《零基础学Python》《从0开始学架构》《MySQL实战45讲》这些课程的时候，我非常开心。我希望将来能够继续覆盖到编译原理、操作系统、计算机组成原理等等。在这些课程里，算法是基础的基础，也是我本人很想精进的部分。
当然，除了长远的职业发展需要，搞定算法还有一个看得见、摸得着的好处，面试。
我经常讲，越是薪资低的企业，面试的时候，它们往往越注重你会不会做网站，甚至会要求你现场做出个东西来。你要注意了，这其实是在找代码熟练工。相反，越是薪资高的企业，越是重视考察基础知识。基础好，说明可塑性强，培养起来也比较快。而最牛的公司，考的往往是算法和思路。
相信很多购买《数据结构与算法之美》专栏的同学，下单的时候，已经想象自己面试的时候，在白板上挥洒代码，面试官频频点头的场景，想着自己马上就能“进驻牛公司，迎娶白富美”了。
然而，事实却是，武功套路容易学，扎马步基本功难练，编程也是一样。框架容易学，基本功难。你没办法讨巧，你要像郭靖学习降龙十八掌那样，一掌一掌劈下去才行。
于是，咱们这个专栏就开始了，你见到的仍然是困难的复杂度计算，指针指来指去，烧脑的逻辑，小心翼翼的边界条件判断。你发现，数据结构和算法好像并不是你上下班时间顺便听一听就能攻克的问题。你需要静下心来仔细想，拿个笔画一画，甚至要写一写代码，Debug一下，才能够理解。是的，的确不轻松，那你坚持下来了吗？
我在这里分享一下我的学习思路，我将这个看起来困难的过程分成了几部分来完成。
第一部分，数据结构和算法的基础知识部分。如果在大学学过这门课，在专栏里，你会看到很多熟悉的描述。有些基础比较好的同学会质疑写这些知识的必要性。这大可不必，因为每个人的基础不一样，为了专栏内容的系统性和完整性，老师肯定要把这些基础知识重新讲述一遍的。对于这一部分内容，如果你的基础比较好，可以像学其他课程一样，在上下班或者午休的时候进行学习，主要是起到温习的作用。
第二部分，需要代码练习的部分。由于王争老师面试过很多人，所以在专栏里，他会列举一些他在面试中常常会问的题目。很多情况下，这些题目需要当场就能在白板上写出来。这些问题对于想要提升自己面试能力的同学来说，应该是很有帮助的。
我这里列举几个，你可以看看，是不是都能回答出来呢？
在链表这一节：单链表反转，链表中环的检测，两个有序的链表合并，删除链表倒数第n个结点，求链表的中间结点等。
在栈这一节，在函数调用中的应用，在表达式求值中的应用，在括号匹配中的应用。
在排序这一节，如何在O(n)的时间复杂度内查找一个无序数组中的第 K大元素？
在二分查找这一节，二分查找的四个变体。
这些问题你都应该上手写写代码，或者在面试之前拿来练练手，而且，不仅仅只是实现主要功能。大公司的面试很多情况下都会考虑边界条件。只要被面试官抓住漏洞，就会被扣分，所以你最好事先写写。
第三部分，对于海量数据的处理思路问题。现在排名靠前的大公司，大都存在海量数据的处理问题。对于这一类问题，在面试的时候，也是经常会问到的。由于这类问题复杂度比较高，很少让当场就写代码，但是基本上会让你说一个思路，或者写写伪代码。想要解决海量数据的问题，你会的就不能只是基础的数据结构和算法了，你需要综合应用。如果平时没有想过这部分问题，临时被问，肯定会懵。
在专栏里，王争老师列举了大量这类问题，你要重点思考这类问题背后的思路，然后平时自己处理问题的时候，也多想想，如果这个问题数据量大的话，应该怎么办。这样多思考，面试的时候，思路很容易就来了。
比如，我这里随便列了几个，都是很经典的问题。你要是想不起来，就赶紧去复习吧！
比如说，我们有10GB的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百MB，没办法一次性把10GB的数据都加载到内存中。这个时候该怎么办呢？
如果你所在的省有50万考生，如何通过成绩快速排序得出名次呢？
假设我们有10万个手机号码，希望将这10万个手机号码从小到大排序，你有什么比较快速的排序方法呢？
假设我们有1000万个整型数据，每个数据占8个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这1000万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过100MB，你会怎么做呢？
第四部分，工业实践部分。在每种数据结构的讲解中，老师会重点分析一些这些数据结构在工业上的实践，封装在库里面的，一般人不注意的。
我看王争老师也是个代码分析控。一般同学可能遇到问题，查一查有没有开源软件或者现成的库，可以用就完了。而王争老师会研究底层代码的实现，解析为什么这些在工业中大量使用的库，应该这样实现。这部分不但对于面试有帮助，对于实际开发也有很大的帮助。普通程序员和高手的差距，就是一个用完了就完了，一个用完了要看看为啥这样用。
例如，老师解析了Glibc中的qsort() 函数，Java中的HashMap如何实现工业级的散列表，Redis中的有序集合（Sorted Set）的实现，工程上使用的红黑树等等。
尤其是对于哈希算法，老师解析了安全加密、数据校验、唯一标识、散列函数，负载均衡、数据分片、分布式存储等应用。如果你同时订阅了架构、微服务的课程，你会发现这些算法在目前最火的架构设计中，都有使用。
师傅领进门，修行在个人。尽管老师只是解析了其中一部分，但是咱们在平时使用开源软件和库的时候，也要多问个为什么。写完了程序，看看官方文档，看看原理解析的书，看看源代码，然后映射到算法与数据结构中，你会发现，这些知识和思路到处都在使用。
最后，我还想说一句，坚持，别放弃，啃下来。基础越扎实，路走得越远，走得越宽。加油！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>划重点_7种编译器的核心概念与算法</title><link>https://artisanbox.github.io/7/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/46/</guid><description>你好，我是编辑王惠。
阶段性的总结复习和验证成果是非常重要的。所以，在8月7日到8月12日这为期一周的期中复习时间里，我们先来巩固一下“真实编译器解析篇”中的重点知识。你可以通过学习委员朱英达总结梳理的划重点内容，以及涵盖了关键知识点的7张思维导图，来回顾7种语言编译器的核心概念与算法。
另外，宫老师还精心策划了10道考试题，让你能在行至半程之时，做好自检，及时发现知识漏洞，到时候一起来挑战一下吧！
在期中复习周的最后，我还会邀请一位优秀的同学来做一次学习分享。通过他的学习故事，你也可以借此对照一下自己的编译原理学习之路。
好，下面我们就一起来复习这些核心的编译原理概念与算法知识吧。
Java编译器（javac）Java是一种广泛使用的计算机编程语言，主要应用于企业级Web应用开发、大型分布式系统以及移动应用开发（Android）。到现在，Java已经是一门非常成熟的语言了，而且它也在不断进化、与时俱进，泛型、函数式编程、模块化等特性陆续都增加了进来。与此同时，Java的编译器和虚拟机中所采用的技术，也比 20 年前发生了天翻地覆的变化。
Java的字节码编译器（javac）是用Java编写的，它实现了自举。启动Java编译器需要Java虚拟机（默认是HotSpot虚拟机，使用C++编写）作为宿主环境。
javac编译器的编译过程，主要涉及到了这样一些关键概念和核心算法：
词法分析阶段：基于有限自动机的理论实现。在处理标识符与关键字重叠的问题上，采用了先都作为标识符识别出来，然后再把其中的关键词挑出来的方式。 语法分析阶段：使用了自顶向下的递归下降算法、LL(k)方式以及多Token预读；处理左递归问题时，采用了标准的改写文法的方法；处理二元表达式时，采用了自底向上的运算符优先级解析器。 语义分析阶段：会分为多个小的阶段，且并不是顺序执行的，而是各阶段交织在一起。 语义分析阶段主要包含：ENTER（建立符号表）、PROCESS（处理注解）、ATTR（属性分析）、FLOW（数据流分析）、TRANSTYPES（处理泛型）、TRANSPATTERNS（处理模式匹配）、UNLAMBDA（处理 Lambda）和 LOWER（处理其他所有的语法糖，比如内部类、foreach 循环等）、GENERATE 阶段（生成字节码）等。在ATTR和FLOW这两个阶段，编译器完成了主要的语义检查工作。 注意：生成字节码是一个比较机械的过程，编译器只需要对 AST 进行深度优先的遍历即可。在这个过程中会用到前几个阶段形成的属性信息，特别是类型信息。 参考资料：
关于注解的官方教程，参考这个链接。 关于数据流分析的理论性内容，参考龙书（Compilers Principles, Techniques and Tools）第二版的9.2和9.3节。也可以参考《编译原理之美》 的第27、28讲，那里进行了比较直观的讲述。 关于半格这个数学工具，可以参考龙书第二版的9.3.1部分，也可以参考《编译原理之美》的第28讲。 Java语言规范第六章，参考Java虚拟机指令集。 Java JIT编译器（Graal）对于编译目标为机器码的Java后端的编译器来说，主要可以分AOT和JIT两类：如果是在运行前一次性生成，就叫做提前编译（AOT）；如果是在运行时按需生成机器码，就叫做即时编译（JIT）。Java以及基于JVM的语言，都受益于JVM的JIT编译器。
在JDK的源代码中，你能找到src/hotspot目录，这是 JVM 的运行时：HotSpot虚拟机，它是用C++编写的，其中就包括JIT编译器。
Graal是Oracle公司推出的一个完全用Java语言编写的JIT编译器。Graal编译器有两个特点：内存安全（相比C++实现的Java JIT编译器而言）；与Java配套的各种工具（比如ID）更友好、更丰富。
Java JIT编译器的编译过程，主要涉及到了这样一些关键概念和核心算法：
分层编译：C0（解释器）、C1（客户端编译器）、C2（服务端编译器）。不同阶段的代码优化激进的程度不同，且存在C2降级回C1的逆优化。 IR采用了“节点之海（Sea of Nodes）”，整合了控制流图与数据流图，符合 SSA 格式，有利于优化算法的编写和维护。 两个重要的优化算法：内联优化和逃逸分析。 几个重要的数据结构：HIR（硬件无关的IR）、LIR（硬件相关的IR）、CFG（控制流图）。 寄存器分配算法：LinearScan。 金句摘录：“编译器开发的真正的工作量，都在中后端。”
参考资料：
GraalVM项目的官方网站；Graal的Github地址；Graal项目的出版物。 基于图的IR的必读论文：程序依赖图-J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program dependence graph and its use in optimization.</description></item><item><title>加餐1_创作故事：我是如何创作“趣谈网络协议”专栏的？</title><link>https://artisanbox.github.io/5/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/42/</guid><description>我用将近半年的时间在“极客时间”写了一个专栏“趣谈网络协议”。对于我自己来讲，这真的是个非常特殊而又难忘的经历。
很多人都很好奇，这个专栏究竟是怎么一步步创作出来的，每一篇文章是怎么写出来的？自己录音频又是什么样的感受？写完整个专栏之后，我终于有时间回顾、整理一下这半年的所感所想。对我来说，这是一次难得的体验，也是一次与“极客时间”的深度沟通。
专栏是写给谁的？和极客时间的编辑谈妥主题之后，他们首先要求我基于约定的主题，写一个36节至50节的大纲，之后会以每周三篇的频率，文字加音频的方式发布。每篇文章的体量要求在3000字左右，录成音频大约就是10分钟。
我本来觉得写这么一个专栏根本就不是个事儿。毕竟咱也是在IT圈摸爬滚打了许多年的“老司机”，干货积累得也不少。只要是熟悉的领域，不用准备，聊个把小时都没啥问题。况且我原来还写过书、写过博客、写过公众号。所以，我对自己文字方面的能力很有自信。
至于语言方面，咱常年出入各大技术论坛，什么场子没趟过。一个两天的线下培训，咱都能扛过来。每篇10分钟，总共36篇，那不才是6个小时嘛，肯定没问题。
但是，写了之后我发现，自己会是一回事儿，能讲给别人是另一回事儿，而能讲给“看不见的陌生人”听，是这世上最难的事儿。
我知道，很多技术人都有这样一个“毛病”，就是觉得掌握技术本身是最重要的，其他什么产品、市场、销售，都没技术含量。这种思维导致很多技术比较牛的人会以自我为中心，仅站在自己的角度思考问题。所以，常常是自己讲得很爽，完全不管听的人是不是真的接受了。写专栏的时候，这绝对是个大忌。
除此之外，这种思维对职业发展的影响也是很大的。单打独斗，一个人搞定一个软件的时代已经过去了。学会和别人合作，才是现代社会的生存法则，而良好的合作源于沟通。
但沟通不易，高质量的沟通更难。面对的人越多，沟通的难度就越大。因为每个人的背景、知识、基础都不同，想听的内容肯定更是千差万别。况且不是每个人都能准确地表达出自己的需求，加之需求的表达、转述都会因表达方式和传递媒介而发生变形，这样一来，接收信息的一方自然很难把握真实的需求。
写专栏的时候，“极客时间”的编辑不断地告诉我，我的受众只有一个人，就是“你”。我心想，这个简单啊，因为面对的人最少嘛！可是，事实上证明，我又“错”了。
这个抽象的“你”，看起来只有一个，其实却是看不到、摸不着的许许多多的人。所以，这个其实是最难的。协议专栏上线10天，就有10000多人订阅，而订阅专栏的用户里，只有少数人会留言。所以，对于很多读者的真实情况，我都无从得知，你可能每天都听但是没有留言的习惯，也可能买了之后觉得我讲得不好，骂一句“这钱白花了”，然后再也不听。
所以，如何把控内容，写给广大未知受众，是我写这个专栏面临的最大挑战。而这里面，文章的深度、广度，音频的语调、语气，每一个细节都非常重要。
专栏文章是怎么写的？经过大纲和前几篇文稿的打磨，我对“极客时间”和专栏创作也有了更深的了解。我私下和很多人交流过一个问题，那就是，咱们平时聊一个话题的时候，有很多话可以说。但是真正去写一篇文章的时候，好像又没有什么可讲的，尤其是那些看起来很基础的内容。
我在写专栏的过程中，仔细思考过这样一个问题：很多人对某一领域或者行业研究得很深入，也有自己长期的实践，但是有多少人可以从感性认识上升到理性认知的高度呢？
现在技术变化这么快，我们每个人的精力都是有限的，不少人学习新知识的方式就是看看书，看看博客、技术文章，或者听同事讲一下，了解个大概就觉得可以直接上手去做了。我也是这样的。可是一旦到写专栏的时候，基础掌握不扎实的问题一下子全都“暴露”出来了。
落到文字上的东西一定要是严谨的。所以，在写到很多细节的时候，我查了大量的资料，找到权威的书籍、官方文档、RFC里面的具体描述，有时候我甚至要做个实验，或者打开代码再看一下，才放心下笔。
尽管我对自己写文章有很多“完美倾向”的要求，但是这其实依旧是站在我自己的角度去看的。读者究竟想要看什么内容呢？
太深入了，看不懂；太浅显了，也不行。太长了，负担太重；太短了，没有干货；同时，每篇文字还要自成一体，所有文章要是一个完整的知识体系。我发现，原来我不仅是对知识的了解没那么全面、具体，对用户阅读和倾听场景也没有过多的考虑。
除了写文字，专栏还要录音频，所以为了方便“听”，文章内不能放大量代码、实验。如果很多人在通勤路上听，而我把一张图片讲得天花乱坠，听的人却根本看不到，那肯定是不行的，所以写文章的时候，我还要把故事性、画面感都考虑进去，尽量详尽而不啰嗦。
把这些限制条件加起来之后，我发现，写专栏这件事儿，真的太不容易了。每篇文章看起来内容不多，但是都是费了很多心思的，这也是为什么很多老师说，写完专栏就像是过了火焰山。
专栏音频是怎么录的？说完写文章，我来说说录音频。我平时听播音员说话，感觉非常轻松，所以当时我毫不犹豫地就说，“我要自己录”。但是在录开篇词的时候，我就觉得这完全不是我想的那么回事啊！
专栏的文章在录音的时候一定会有个“音频稿”，我一开始很不理解，我对着发布的稿件直接讲就好了啊，为什么还要特意准备一个供录音频的稿件啊？
我在没有音频稿的情况下，自己试着“发挥”了几次，结果，我发现我的嘴会“吃”字，会反复讲一个内容而且表达不清，但是自己却经常毫无察觉，还会自己讲着讲着就收不住等等。
咱们平时说话的时候，会有很多口头语和重复的词语。面对面交流的时候，我们为什么没有注意这个问题呢？因为我们会更注重对方的表情、手势，但是一旦录成音频，这些“啰嗦”的地方就特别明显。
而有了音频稿之后，整个过程就严谨很多。如果哪句话说错了，看着稿件再说一遍就好了。而且，你会发现录音的时间大大缩短了，原来需要用十分钟，现在五分钟就可以很精炼地讲出来了。
有了稿子，那我是不是对着念就好了？这不是很容易吗？不，我又遇到了新的难题。
录音频的时候，我常常一个人关在密闭的房间里，对着显示器“读”，这和公共演讲肯定是不一样的。加上因为有写好的音频稿，我常常感觉束手束脚，找不到演讲那种有激情的感觉，很容易就变成了念课文。
为了同时满足自然和严谨，一方面我会先熟记“台词”；另一方面，每次录的时候，我都假想对面有个人，我在对着他缓缓地讲出来。讲到某些地方，我还会假想他对这个知识点是不是有疑问，这样就更加有互动感。
录音频这件事对我的改变非常大。我说话、演讲的时候变得更加严谨了。我会下意识地不去重复已经说过的话。一旦想重复，也闭嘴不发音，等想好了下一句再说。后面，我的录音也越来越顺利，一开始要录五六遍才能成功，后面基本一遍就过了。
创作专栏的过程还有许多事情，都是我很难得的记忆。我很佩服“极客时间”的编辑做专栏时的专业和认真。我也很庆幸，我没有固执地按照自己认为正确的方向和方式来做，而是尊重了他们的专业。很显然，他们没有我懂技术，但是他们比我更懂“你”。
专栏结束后，我回看这半年的准备和努力，我发现，无论对自己的领域多么熟悉，写这个专栏都让我又上升了一个新高度。
我知道很多技术人都喜欢分享，而写文章又是最容易实现的方式。写文章的时候，可以检验你对基础知识的掌握是否扎实，是不是有换位思考能力，能不能从感性认识上升到理性认知。
除此之外，我觉得最重要的一点是，在创作专栏文章的过程中，我学到了很多技术之外的东西，比如换位思考能力和细节把控的能力。
我在这里记下与“极客时间”的相识和相知。希望看到更多人在极客时间，分享自己的知识和见解。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>加餐2_“趣谈网络协议”专栏食用指南</title><link>https://artisanbox.github.io/5/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/43/</guid><description>你好，我是刘超。
“趣谈网络协议”专栏现在已经全部更新完毕。这里有一份「食用指南」，可以帮你找到学习本专栏的最佳姿势。
在这份指南中，我为你整理了专栏的所有学习资料，并告诉你如何更高效地使用这些资料，从而帮助你消化吸收，以期获得更好的学习效果。
不管你是刚刚打开这个专栏，还是进入温故的阶段，我的这份指南，都可以帮你更上一个台阶。一起加油吧！
1. 能力测试我从常用的网络协议中，精心筛选了核心知识点，编成了10道题。这里面的题目和答案都是我精心设计的。希望你一定要先拿出纸笔，认真思考，记录下自己的答案，之后再和文末的详细解析进行对照。
刚刚打开这个专栏的你，可以据此寻找自己的薄弱点，对症下药；已经学习了一段时间的你，可以检测学习成果，查漏补缺。
点击查看：网络协议能力测试题
2. 答疑解惑每篇文章后，我都会留两个思考题，其中第一个问题意在启发你的思考，是对本节内容的延伸学习；第二个问题是为引出下一节，下一节的内容其实就是答案（所以我就不单独解答啦）。
我希望你能够好好地利用这些思考题，毕竟所有的“知”，只有经过了自己的思考之后，才能成为“识”。
如果你是刚刚加入学习，你可以继续在思考题后的“留言区”写下你的答案，学习过程中遇到的问题和思考也欢迎多多分享，我依然会在这里回复你的留言，和你一起讨论。
我知道你肯定也很好奇我对这些问题的思考是怎样的，因此，我针对每一节课后的第一道思考题及留言区比较有代表性的、有深度的问题，特意写了一系列答疑文章。
我再强调一遍，对于这一系列的答疑文章，你一定要在自己进行深度思考之后，再来看文章对比答案，这样可以更有效地拓展你的知识边界。
点击查看：
第一期：第1讲至第2讲答疑解惑合辑
第二期：第3讲至第6讲答疑解惑合辑
第三期：第7讲至第13讲答疑解惑合辑
第四期：第14讲至第21讲答疑解惑合辑
第五期：第22讲至第36讲答疑解惑合辑
3. 知识串讲在学习完前面36讲的内容之后，我详细讲解了一个“下单”的过程。我把这个过程分为十个阶段，从云平台中搭建一个电商开始，到BGP路由广播，再到DNS域名解析；从客户看商品图片，到最终下单的整个过程，每个步骤我都画了详细的分解图。
你可以用这个过程，串起我们讲过的所有网络协议，还原真实的使用场景，学以致用。我相信，学完前面的详细内容之后，再来看这个串讲内容，你对网络协议一定会有一个全面、深入的把握。
点击查看：
知识串讲（上篇）
知识串讲（中篇）
知识串讲（下篇）
4. 知识图谱专栏中最精华的内容，我都整理在这张图上了。
点击查看：网络协议知识图谱
5. 实验环境纸上得来终觉浅。网络是一门实验性很强的学科，我在写专栏的过程中也深深体会到了。有时候，遇到疑问，我常常会拿一个现实的环境，上手操作一下，抓个包看看，这样心里就会有定论。
网络方面最权威的书籍《TCP/IP详解》（TCP/IP illustrated）的作者斯蒂文森（W. Richard Stevens），也是经过无数次实验，才完成这本巨著。
因此，我在这本书中的实验基础上，带你搭建一个实验环境，希望你能够上手操作一下学过的知识。毕竟，只有经过你自己动手和思考产生的内容，才是真正属于你的知识。
点击查看我搭建实验环境时候的具体操作，希望给你的思维晋升指路：《搭建一个网络实验环境：授人以鱼不如授人以渔》
6. 专栏音频我在这里想特别提一下专栏音频。我的每篇专栏文章都包含了很多图片，为了帮助你更好地理解文章内容，我在录音的时候，常常会对图片做一些补充解释和说明，所以音频和文字稿并非完全一一对应。不知道你具体的学习习惯是怎样的，我建议你除了阅读文字以外，一定要听一下音频，可以利用“倍速播放”的功能，还可以自由把控播放速度，更高效地学习。
7. 记录，高效；分享，快乐我们专栏还有不少功能，提醒你好好利用起来，成为高效的学习者。
比如，在学习的过程中，遇到自己不懂的地方，或者是有深刻感受的地方，一定要及时利用“划线笔记”的功能，记录下自己当时的想法。这样在过程中点滴积累，等学完后，还可以回过头来再过一遍。如果有可能，你可以把自己的这些思考梳理成文。相信我，这样做，你的提升速度会快到让自己意外。
再比如“请朋友读”功能。如果你觉得某篇内容对自己很有帮助，不妨把它推荐给身边有同样需求的朋友，这一个动作或许就能帮他解决一个手边的问题。最重要的是，通过这些分享，你会找到那些和你一样热爱学习的伙伴，一起学习更快乐。
最后，还有一个小小的彩蛋。我把自己这半年写专栏的经历，写成了一篇文章。我是如何写专栏中每一篇文章的？每一篇音频又是如何录出来的？创作专栏给我带来了哪些改变？带你走进“极客时间万人专栏”背后的创作故事。
点击查看：我是如何创作“趣谈网络协议”专栏的？
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>加餐_汇编代码编程与栈帧管理</title><link>https://artisanbox.github.io/6/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/40/</guid><description>在22讲中，我们侧重讲解了汇编语言的基础知识，包括构成元素、汇编指令和汇编语言中常用的寄存器。学习完基础知识之后，你要做的就是多加练习，和汇编语言“混熟”。小窍门是查看编译器所生成的汇编代码，跟着学习体会。
不过，可能你是初次使用汇编语言，对很多知识点还会存在疑问，比如：
在汇编语言里调用函数（过程）时，传参和返回值是怎么实现的呢？ 21讲中运行期机制所讲的栈帧，如何通过汇编语言实现？ 条件语句和循环语句如何实现？ …… 为此，我策划了一期加餐，针对性地讲解这样几个实际场景，希望帮你加深对汇编语言的理解。
示例1：过程调用和栈帧这个例子涉及了一个过程调用（相当于C语言的函数调用）。过程调用是汇编程序中的基础结构，它涉及到栈帧的管理、参数的传递这两个很重要的知识点。
假设我们要写一个汇编程序，实现下面C语言的功能：
/*function-call1.c */ #include &amp;lt;stdio.h&amp;gt; int fun1(int a, int b){ int c = 10; return a+b+c; } int main(int argc, char *argv[]){ printf(&amp;quot;fun1: %d\n&amp;quot;, fun1(1,2)); return 0; } fun1函数接受两个整型的参数：a和b，来看看这两个参数是怎样被传递过去的，手写的汇编代码如下：
# function-call1-craft.s 函数调用和参数传递 # 文本段,纯代码 .section __TEXT,__text,regular,pure_instructions
_fun1: # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来
movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部
subq $4, %rsp # 扩展栈 movl $10, -4(%rbp) # 变量c赋值为10，也可以写成 movl $10, (%rsp) # 做加法 movl %edi, %eax # 第一个参数放进%eax addl %esi, %eax # 把第二个参数加到%eax,%eax同时也是存放返回值的寄存器 addl -4(%rbp), %eax # 加上c的值 addq $4, %rsp # 缩小栈 # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 .</description></item><item><title>协议专栏特别福利_答疑解惑第一期</title><link>https://artisanbox.github.io/5/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/51/</guid><description>你好，我是刘超。
首先，感谢大家关注并在留言区写下近3000条留言。留言太多，没有及时回复，一是每周写三篇文章压力真的挺大的。为了保质保量地产出，晚上和周末的时间基本上都搭进去了。二是很多人的留言非常有深度，水平很高，提的问题一两句话解释不清楚。
每一节结尾我基本都会留两个思考题，其中第一个问题是启发思考的，是对本节内容的延伸学习；第二个问题是为了引出下一节，下一节的内容其实就是答案。
所以我会回答一下每一节的第一个问题，并列出第一个同我的思路最相近的同学，并对留言中比较有代表性的问题，做一个统一的回答，顺便也实现之前要送知识图谱和奖励礼券的承诺。
当然，这并不能说明我的回答就是一定是正确的或者全面的，有很多同学的留言有非常大的信息量，甚至更广的思路，也对这些同学表示感谢。还有些同学指出了我的错误，也感谢你们。
《第1讲 | 为什么要学习网络协议？》课后思考题当网络包到达一个城关的时候，可以通过路由表得到下一个城关的 IP 地址，直接通过 IP地址找就可以了，为什么还要通过本地的MAC地址呢？
徐良红同学说的比较接近。在网络包里，有源IP地址和目标IP地址、源MAC地址和目标MAC地址。从路由表中取得下一跳的IP地址后，应该把这个地址放在哪里呢？如果放在目标IP地址里面，到了城关，谁知道最终的目标在哪里呢？所以要用MAC地址。
所谓的下一跳，看起来是IP地址，其实是要通过ARP得到MAC地址，将下一跳的MAC地址放在目标MAC地址里面。
留言问题1.MAC地址可以修改吗？
我查了一下，MAC（Media Access Control，介质访问控制）地址，也叫硬件地址，长度是48比特（6字节），由16进制的数字组成，分为前24位和后24位。
前24位叫作组织唯一标志符（Organizationally Unique Identifier，OUI），是由IEEE的注册管理机构给不同厂家分配的代码，用于区分不同的厂家。后24位是厂家自己分配的，称为扩展标识符。同一个厂家生产的网卡中MAC地址后24位是不同的。
也就是说，MAC本来设计为唯一性的，但是后来设备越来越多，而且还有虚拟化的设备和网卡，有很多工具可以修改，就很难保证不冲突了。但是至少应该保持一个局域网内是唯一的。
MAC的设计，使得即便不能保证绝对唯一，但是能保证一个局域网内出现冲突的概率很小。这样，一台机器启动的时候，就能够在没有IP地址的情况下，先用MAC地址进行通信，获得IP地址。
好在MAC地址是工作在一个局域网中的，因而即便出现了冲突，网络工程师也能够在自己的范围内很快定位并解决这个问题。这就像我们生成UUID或者哈希值，大部分情况下是不会冲突的，但是如果碰巧出现冲突了，采取一定的机制解决冲突就好。
2.TCP重试有没有可能导致重复下单？
答案是不会的。这个在TCP那一节有详细的讲解。因为TCP层收到了重复包之后，TCP层自己会进行去重，发给应用层、HTTP层。还是一个唯一的下单请求，所以不会重复下单。
那什么时候会导致重复下单呢？因为网络原因或者服务端错误，导致TCP连接断了，这样会重新发送应用层的请求，也即HTTP的请求会重新发送一遍。
如果服务端设计的是无状态的，它记不住上一次已经发送了一次请求。如果处理不好，就会导致重复下单，这就需要服务端除了实现无状态，还需要根据传过来的订单号实现幂等，同一个订单只处理一次。
还会有的现象是请求被黑客拦截，发送多次，这在HTTPS层可以有很多种机制，例如通过 Timestamp和Nonce随机数联合起来，然后做一个不可逆的签名来保证。
3.TCP报平安的包是原路返回吗？
谢谢语鬼同学的指正。这里的比喻不够严谨，容易让读者产生误会，这里的原路返回的意思是原样返回，也就是返回也是这个过程，不一定是完全一样的路径。
4.IP地址和MAC地址的关系？
芒果同学的理解非常准确，讲IP和MAC的关系的时候说了这个问题。IP是有远程定位功能的，MAC是没有远程定位功能的，只能通过本地ARP的方式找到。
我个人认为，即便有了IPv6，也不会改变当前的网络分层模式，还是IP层解决远程定位问题，只不过改成IPv6了，到了本地，还是通过MAC。
5.如果最后一跳的时候，IP改变了怎么办？
对于IP层来讲，当包到达最后一跳的时候，原来的IP不存在了。比如网线拔掉了，或者服务器直接宕机了，则ARP就找不到了，所以这个包就会发送失败了。对于IP层的工作就结束了。
但是IP层之上还有TCP层，TCP会重试的，包还是会重新发送，但是如果服务器没有启动起来，超过一定的次数，最终放弃。
如果服务器重启了，IP还是原来的IP地址，这个时候TCP重新发送的一个包的时候，ARP是能够得到这个地址的，因而会发到这台机器上来，但是机器上面没有启动服务端监听那个端口，于是会发送ICMP端口不可达。
如果服务器重启了，服务端也重新启动了，也在监听那个端口了，这个时候TCP的服务端由于是新的，Sequence Number根本对不上，说明不是原来的连接，会发送RST。
那有没有可能有特殊的场景Sequence Number也能对的上呢？按照Sequence Number的生成算法，是不可能的。
但是有一个非常特殊的方式，就是虚拟机的热迁移，从一台物理机迁移到另外一台物理机，IP不变，MAC不变，内存也拷贝过去，Sequence Number在内存里面也保持住了，在迁移的过程中会丢失一两个包，但是从TCP来看，最终还是能够连接成功的。
6.TCP层报平安，怎么确认浏览器收到呢？
TCP报平安，只能保证TCP层能够收到，不保证浏览器能够收到。但是可以想象，如果浏览器是你写的一个程序，你也是通过socket编程写的，你是通过socket，建立一个TCP的连接，然后从这个连接里面读取数据，读取的数据就是TCP层确认收到的。
这个读取的动作是本地系统调用，大部分情况下不会失败的。如果读取失败呢，当然本地会报错，你的socket读取函数会返回错误，如果你是浏览器程序的实现者，你有两种选择，一个是将错误报告给用户，另一个是重新发送一次请求，获取结果显示给用户。
7.ARP协议属于哪一层？
ARP属于哪个层，一直是有争议的。比如《TCP/IP详解》把它放在了二层和三层之间，但是既然是协议，只要大家都遵守相同的格式、流程就可以了，在实际应用的时候，不会有歧义的，唯一有歧义的是参加各种考试，让你做选择题，ARP属于哪一层？平时工作中咱不用纠结这个。
《第2讲 | 网络分层的真实含义是什么？》课后思考题如果你也觉得总经理和员工的比喻不恰当，你有更恰当的比喻吗？
我觉得，寄快递和寄信这两个比喻都挺好的。关键是有了封装和解封装的过程。有的同学举了爬楼，或者公司各层之间的沟通，都无法体现封装和解封装的过程。
留言问题1.为什么要分层？
是的，仅仅用复杂性来解释分层，太过牵强了。
其实这是一个架构设计的通用问题，不仅仅是网络协议的问题。一旦涉及到复杂的逻辑，或者软件需求需要经常变动，一般都会通过分层来解决问题。
假如我们将所有的代码都写在一起，但是产品经理突然想调整一下界面，这背后的业务逻辑变不变，那要不要一起修改呢？所以会拆成两层，把UI层从业务逻辑中分离出来，调用API来进行组合。API不变，仅仅界面变，是不是就不影响后台的代码了？
为什么要把一些原子的API放在基础服务层呢？将数据库、缓存、搜索引擎等，屏蔽到基础服务层以下，基础服务层之上的组合逻辑层、API层都只能调用基础服务层的API，不能直接访问数据库。
比如我们要将Oracle切换成MySQL。MySQL有一个库，分库分表成为4个库。难道所有的代码都要修改吗？当然只要把基础服务层屏蔽，提供一致的接口就可以了。
网络协议也是这样的。有的想基于TCP，自己不操心就能够保证到达；有的想自己实现可靠通信，不基于TCP，而使用UDP。一旦分了层就好办了，定制化后要依赖于下一层的接口，只要实现自己的逻辑就可以了。如果TCP的实现将所有的逻辑耦合在了整个七层，不用TCP的可靠传输机制都没有办法。
2.层级之间真实的调用方式是什么样的？
如果文中是一个逻辑图，这个问题其实已经到实现层面上来了，需要看TCP/IP的协议栈代码了。这里首先推荐一本书《深入理解Linux网络技术内幕》。
其实下层的协议知道上层协议的，因为在每一层的包头里面，都会有上一层是哪个协议的标识，所以不是一个回调函数，每一层的处理函数都会在操作系统启动的时候，注册到内核的一个数据结构里面，但是到某一层的时候，是通过判断到底是哪一层的哪一个协议，然后去找相应的处理函数去调用。
调用的大致过程我这里再讲一下。由于TCP比较复杂，我们以UDP为例子，其实发送的包就是一个sk_buff结构。这个在Socket那一节讲过。
int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4) 接着，UDP层会调用IP层的函数。</description></item><item><title>协议专栏特别福利_答疑解惑第三期</title><link>https://artisanbox.github.io/5/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/48/</guid><description>你好，我是刘超。
第三期答疑涵盖第7讲至第13讲的内容。我依旧对课后思考题和留言中比较有代表性的问题作出回答。你可以点击文章名，回到对应的章节复习，也可以继续在留言区写下你的疑问，我会持续不断地解答。希望对你有帮助。
《第7讲 | ICMP与ping：投石问路的侦察兵》课后思考题当发送的报文出问题的时候，会发送一个ICMP的差错报文来报告错误，但是如果 ICMP 的差错报文也出问题了呢？
我总结了一下，不会导致产生ICMP差错报文的有：
ICMP差错报文（ICMP查询报文可能会产生ICMP差错报文）；
目的地址是广播地址或多播地址的IP数据报；
作为链路层广播的数据报；
不是IP分片的第一片；
源地址不是单个主机的数据报。这就是说，源地址不能为零地址、环回地址、广播地址或多播地址。
留言问题1.ping使用的是什么网络编程接口？
咱们使用的网络编程接口是Socket，对于ping来讲，使用的是ICMP，创建Socket如下：
socket(AF_INET, SOCK_RAW, IPPROTO_ICMP) SOCK_RAW就是基于IP层协议建立通信机制。
如果是TCP，则建立下面的Socket：
socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) 如果是UDP，则建立下面的Socket：
socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP) 2.ICMP差错报文是谁发送的呢？
我看留言里有很多人对这个问题有疑惑。ICMP包是由内核返回的，在内核中，有一个函数用于发送ICMP的包。
void icmp_send(struct sk_buff *skb_in, int type, int code, __be32 info); 例如，目标不可达，会调用下面的函数。
icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PROT_UNREACH, 0); 当IP大小超过MTU的时候，发送需要分片的ICMP。
if (ip_exceeds_mtu(skb, mtu)) { icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu)); goto drop; } 《第8讲 | 世界这么大，我想出网关：欧洲十国游与玄奘西行》课后思考题当在你家里要访问 163 网站的时候，你的包需要 NAT 成为公网 IP，返回的包又要 NAT 成你的私有 IP，返回包怎么知道这是你的请求呢？它怎么能这么智能地 NAT 成了你的 IP 而非别人的 IP 呢？</description></item><item><title>协议专栏特别福利_答疑解惑第二期</title><link>https://artisanbox.github.io/5/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/47/</guid><description>你好，我是刘超。
第二期答疑涵盖第3讲至第6讲的内容。我依旧对课后思考题和留言中比较有代表性的问题作出回答。你可以点击文章名，回到对应的章节复习，也可以继续在留言区写下你的疑问，我会持续不断地解答。希望对你有帮助。
《第3讲 | ifconfig：最熟悉又陌生的命令行》课后思考题你知道 net-tools 和 iproute2 的“历史”故事吗？
这个问题的答案，盖同学已经写的比较全面了。具体的对比，我这里推荐一篇文章https://linoxide.com/linux-command/use-ip-command-linux/，感兴趣的话可以看看。
留言问题1.A、B、C类地址的有效地址范围是多少？
我在写的时候，没有考虑这么严谨，平时使用地址的时候，也是看个大概的范围。所以这里再回答一下。
A类IP的地址第一个字段范围是0～127，但是由于全0和全1的地址用作特殊用途，实际可指派的范围是1～126。所以我仔细查了一下，如果较真的话，你在答考试题的时候可以说，A类地址范围和A类有效地址范围。
2.网络号、IP地址、子网掩码和广播地址的先后关系是什么？
当在一个数据中心或者一个办公室规划一个网络的时候，首先是网络管理员规划网段，一般是根据将来要容纳的机器数量来规划，一旦定了，以后就不好变了。
假如你在一个小公司里，总共就没几台机器，对于私有地址，一般选择192.168.0.0/24就可以了。
这个时候先有的是网络号。192.168.0就是网络号。有了网络号，子网掩码同时也就有了，就是前面都是网络号的是1，其他的是0，广播地址也有了，除了网络号之外都是1。
当规划完网络的时候，一般这个网络里面的第一个、第二个地址被默认网关DHCP服务器占用，你自己创建的机器，只要和其他的不冲突就可以了，当然你也可以让DHCP服务自动配置。
规划网络原来都是网络管理员的事情。有了公有云之后，一般有个概念虚拟网络（VPC），鼠标一点就能创建一个网络，网络完全软件化了，任何人都可以做网络规划。
3.组播和广播的意义和原理是什么？
C类地址的主机号8位，去掉0和255，就只有254个了。
在《TCP/IP详解》这本书里面，有两章讲了广播、多播以及IGMP。广播和组播分为两个层面，其中MAC层有广播和组播对应的地址，IP层也有自己的广播地址和组播地址。
广播相对比较简单，MAC层的广播为ff:ff:ff:ff:ff:ff，IP层指向子网的广播地址为主机号为全1且有特定子网号的地址。
组播复杂一些，MAC层中，当地址中最高字节的最低位设置为1时，表示该地址是一个组播地址，用十六进制可表示为01:00:00:00:00:00。IP层中，组播地址为D类IP地址，当IP地址为组播地址的时候，有一个算法可以计算出对应的MAC层地址。
多播进程将目的IP地址指明为多播地址，设备驱动程序将它转换为相应的以太网地址，然后把数据发送出去。这些接收进程必须通知它们的IP层，它们想接收的发给定多播地址的数据报，并且设备驱动程序必须能够接收这些多播帧。这个过程就是“加入一个多播组”。
当多播跨越路由器的时候，需要通过IGMP协议告诉多播路由器，多播数据包应该如何转发。
4.MTU 1500的具体含义是什么？
MTU（Maximum Transmission Unit，最大传输单元）是二层的一个定义。以以太网为例，MTU为1500个Byte，前面有6个Byte的目标MAC地址，6个Byte的源MAC地址，2个Byte的类型，后面有4个Byte的CRC校验，共1518个Byte。
在IP层，一个IP数据报在以太网中传输，如果它的长度大于该MTU值，就要进行分片传输。如果不允许分片DF，就会发送ICMP包，这个在ICMP那一节讲过。
在TCP层有个MSS（Maximum Segment Size，最大分段大小），它等于MTU减去IP头，再减去TCP头。即在不分片的情况下，TCP里面放的最大内容。
在HTTP层看来，它的body没有限制，而且在应用层看来，下层的TCP是一个流，可以一直发送，但其实是会被分成一个个段的。
《第4讲 | DHCP与PXE：IP是怎么来的，又是怎么没的》课后思考题PXE 协议可以用来安装操作系统，但是如果每次重启都安装操作系统，就会很麻烦。你知道如何使得第一次安装操作系统，后面就正常启动吗？
一般如果咱们手动安装一台电脑的时候，都是有启动顺序的，如果改为硬盘启动，就没有问题了。
好在服务器一般都提供IPMI接口，可以通过这个接口启动、重启、设置启动模式等等远程访问，这样就可以批量管理一大批机器。
这里提到Cobbler，这是一个批量安装操作系统的工具。在OpenStack里面，还有一个Ironic，也是用来管理裸机的。有兴趣的话可以研究一下。
留言问题1.在DHCP网络里面，手动配置IP地址会冲突吗?
在一个DHCP网络里面，如果某一台机器手动配置了一个IP地址，并且在DHCP管理的网段里的话，DHCP服务器是会将这个地址分配给其他机器的。一旦分配了，ARP的时候，就会收到两个应答，IP地址就冲突了。
当发生这种情况的时候，应该怎么办呢？DHCP的过程虽然没有明确如何处理，但是DHCP的客户端和服务器都可以添加相应的机制来检测冲突。
如果由客户端来检测冲突，一般情况是，客户端在接受分配的IP之前，先发送一个ARP，看是否有应答，有就说明冲突了，于是发送一个DHCPDECLINE，放弃这个IP地址。
如果由服务器来检测冲突，DHCP服务器会发送ping，来看某个IP是否已经被使用。如果被使用了，它就不再将这个IP分配给其他的客户端了。
2.DHCP的Offer和ACK应该是单播还是广播呢？
没心没肺 回答得很正确。
这个我们来看DHCP的RFC，我截了个图放在这儿：
这里面说了几个问题。
正常情况下，一旦有了IP地址，DHCP Server还是希望通过单播的方式发送OFFER和ACK。但是不幸的是，有的客户端协议栈的实现，如果还没有配置IP地址，就使用单播。协议栈是不接收这个包的，因为OFFER和ACK的时候，IP地址还没有配置到网卡上。
所以，一切取决于客户端的协议栈的能力，如果没配置好IP，就不能接收单播的包，那就将BROADCAST设为1，以广播的形式进行交互。
如果客户端的协议栈实现很厉害，即便是没有配置好IP，仍然能够接受单播的包，那就将BROADCAST位设置为0，就以单播的形式交互。
3.DHCP如何解决内网安全问题?
其实DHCP协议的设计是基于内网互信的基础来设计的，而且是基于UDP协议。但是这里面的确是有风险的。例如一个普通用户无意地或者恶意地安装一台DHCP服务器，发放一些错误或者冲突的配置；再如，有恶意的用户发出很多的DHCP请求，让DHCP服务器给他分配大量的IP。
对于第一种情况，DHCP服务器和二层网络都是由网管管理的，可以在交换机配置只有来自某个DHCP服务器的包才是可信的，其他全部丢弃。如果有SDN，或者在云中，非法的DHCP包根本就拦截到虚拟机或者物理机的出口。
对于第二种情况，一方面进行监控，对DHCP报文进行限速，并且异常的端口可以关闭，一方面还是SDN或者在云中，除了被SDN管控端登记过的IP和MAC地址，其他的地址是不允许出现在虚拟机和物理机出口的，也就无法模拟大量的客户端。
《第5讲 | 从物理层到MAC层：如何在宿舍里自己组网玩联机游戏？》课后思考题1.在二层中我们讲了 ARP 协议，即已知 IP 地址求 MAC；还有一种 RARP 协议，即已知 MAC 求 IP 的，你知道它可以用来干什么吗？</description></item><item><title>协议专栏特别福利_答疑解惑第五期</title><link>https://artisanbox.github.io/5/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/50/</guid><description>你好，我是刘超。
第五期答疑涵盖第22讲至第36讲的内容。我依旧对课后思考题和留言中比较有代表性的问题作出回答。你可以点击文章名，回到对应的章节复习，也可以继续在留言区写下你的疑问，我会持续不断地解答。希望对你有帮助。
《第22讲 | VPN：朝中有人好做官》 课后思考题 当前业务的高可用性和弹性伸缩很重要，所以很多机构都会在自建私有云之外，采购公有云，你知道私有云和公有云应该如何打通吗？
留言问题 DH算法会因为传输随机数被破解吗？
这位同学的笔记特别认真，让人感动。DH算法的交换材料要分公钥部分和私钥部分，公钥部分和其他非对称加密一样，都是可以传输的，所以对于安全性是没有影响的，而且传输材料远比传输原始的公钥更加安全。私钥部分是谁都不能给的，因此也是不会截获到的。
《第23讲 | 移动网络：去巴塞罗那，手机也上不了脸书》 课后思考题 咱们上网都有套餐，有交钱多的，有交钱少的，你知道移动网络是如何控制不同优先级的用户的上网流量的吗？
这个其实是PCRF协议进行控制的，它可以下发命令给PGW来控制上网的行为和特性。
《第24讲 | 云中网络：自己拿地成本高，购买公寓更灵活》 课后思考题 为了直观，这一节的内容我们以桌面虚拟化系统举例。在数据中心里面，有一款著名的开源软件OpenStack，这一节讲的网络连通方式对应OpenStack中的哪些模型呢？
OpenStack的早期网络模式有Flat、Flat DHCP、VLAN，后来才有了VPC，用VXLAN和GRE进行隔离。
《第25讲 | 软件定义网络：共享基础设施的小区物业管理办法》 课后思考题 在这一节中，提到了通过VIP可以通过流表在不同的机器之间实现负载均衡，你知道怎样才能做到吗？
可以通过ovs-ofctl下发流表规则，创建group，并把端口加入group中，所有发现某个地址的包在两个端口之间进行负载均衡。
sudo ovs-ofctl -O openflow11 add-group br-lb &amp;quot;group_id=100 type=select selection_method=dp_hash bucket=output:1 bucket=output:2&amp;quot; sudo ovs-ofctl -O openflow11 add-flow br-lb &amp;quot;table=0,ip,nw_dst=192.168.2.0/24,actions=group:100&amp;quot; 留言问题 SDN控制器是什么东西？
SDN控制器是一个独立的集群，主要是在管控面，因为要实现一定的高可用性。
主流的开源控制器有OpenContrail、OpenDaylight等。当然每个网络硬件厂商都有自己的控制器，而且可以实现自己的私有协议，进行更加细粒度的控制，所以江湖一直没有办法统一。
流表是在每一台宿主机上保存的，大小限制取决于内存，而集中存放的缺点就是下发会很慢。
《第26讲 | 云中的网络安全：虽然不是土豪，也需要基本安全和保障》 课后思考题 这一节中重点讲了iptables的filter和nat功能，iptables还可以通过QUEUE实现负载均衡，你知道怎么做吗？
我们可以在iptables里面添加下面的规则：
-A PREROUTING -p tcp -m set --match-set minuteman dst,dst -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j NFQUEUE --queue-balance 50:58 -A OUTPUT -p tcp -m set --match-set minuteman dst,dst -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j NFQUEUE --queue-balance 50:58 NFQUEUE的规则表示将把包的处理权交给用户态的一个进程。–queue-balance表示会将包发给几个queue。</description></item><item><title>协议专栏特别福利_答疑解惑第四期</title><link>https://artisanbox.github.io/5/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/49/</guid><description>你好，我是刘超。
第四期答疑涵盖第14讲至第21讲的内容。我依旧对课后思考题和留言中比较有代表性的问题作出回答。你可以点击文章名，回到对应的章节复习，也可以继续在留言区写下你的疑问，我会持续不断地解答。希望对你有帮助。
《第14讲 | HTTP协议：看个新闻原来这么麻烦》课后思考题QUIC是一个精巧的协议，所以它肯定不止今天我提到的四种机制，你知道还有哪些吗？
云学讲了一个QUIC的特性。
QUIC还有其他特性，一个是快速建立连接。这个我放在下面HTTPS的时候一起说。另一个是拥塞控制，QUIC协议当前默认使用了TCP协议的CUBIC（拥塞控制算法）。
你还记得TCP的拥塞控制算法吗？每当收到一个ACK的时候，就需要调整拥塞窗口的大小。但是这也造成了一个后果，那就是RTT比较小的，窗口增长快。
然而这并不符合当前网络的真实状况，因为当前的网络带宽比较大，但是由于遍布全球，RTT也比较长，因而基于RTT的窗口调整策略，不仅不公平，而且由于窗口增加慢，有时候带宽没满，数据就发送完了，因而巨大的带宽都浪费掉了。
CUBIC进行了不同的设计，它的窗口增长函数仅仅取决于连续两次拥塞事件的时间间隔值，窗口增长完全独立于网络的时延RTT。
CUBIC的窗口大小以及变化过程如图所示。
当出现丢包事件时，CUBIC会记录这时的拥塞窗口大小，把它作为Wmax。接着，CUBIC会通过某个因子执行拥塞窗口的乘法减小，然后，沿着立方函数进行窗口的恢复。
从图中可以看出，一开始恢复的速度是比较快的，后来便从快速恢复阶段进入拥塞避免阶段，也即当窗口接近Wmax的时候，增加速度变慢；立方函数在Wmax处达到稳定点，增长速度为零，之后，在平稳期慢慢增长，沿着立方函数的开始探索新的最大窗口。
留言问题HTTP的keepalive模式是什么样？
在没有keepalive模式下，每个HTTP请求都要建立一个TCP连接，并且使用一次之后就断开这个TCP连接。
使用keepalive之后，在一次TCP连接中可以持续发送多份数据而不会断开连接，可以减少TCP连接建立次数，减少TIME_WAIT状态连接。
然而，长时间的TCP连接容易导致系统资源无效占用，因而需要设置正确的keepalive timeout时间。当一个HTTP产生的TCP连接在传送完最后一个响应后，还需要等待keepalive timeout秒后，才能关闭这个连接。如果这个期间又有新的请求过来，可以复用TCP连接。
《第15讲 | HTTPS协议：点外卖的过程原来这么复杂》课后思考题HTTPS 协议比较复杂，沟通过程太繁复，这样会导致效率问题，那你知道有哪些手段可以解决这些问题吗？
通过HTTPS访问的确复杂，至少经历四个阶段：DNS查询、TCP连接建立、TLS连接建立，最后才是HTTP发送数据。我们可以一项一项来优化这个过程。
首先如果使用基于UDP的QUIC，可以省略掉TCP的三次握手。至于TLS的建立，如果按文章中基于TLS 1.2的，双方要交换key，经过两个来回，也即两个RTT，才能完成握手。但是咱们讲IPSec的时候，讲过通过共享密钥、DH算法进行握手的场景。
在TLS 1.3中，握手过程中移除了ServerKeyExchange和ClientKeyExchange，DH参数可以通过key_share进行传输。这样只要一个来回，就可以搞定RTT了。
对于QUIC来讲，也可以这样做。当客户端首次发起QUIC连接时，会发送一个client hello消息，服务器会回复一个消息，里面包括server config，类似于TLS1.3中的key_share交换。当客户端获取到server config以后，就可以直接计算出密钥，发送应用数据了。
留言问题1.HTTPS的双向认证流程是什么样的？
2.随机数和premaster的含义是什么？
《第16讲 | 流媒体协议：如何在直播里看到美女帅哥？》课后思考题你觉得基于 RTMP 的视频流传输的机制存在什么问题？如何进行优化？
Jason的回答很对。
Jealone的回答更加具体。
当前有基于自研UDP协议传输的，也有基于QUIC协议传输的。
留言问题RTMP建立连接的序列是什么样的？
的确，这个图我画错了，我重新画了一个。
不过文章中这部分的文字描述是没问题的。
客户端发送C0、C1、 C2，服务器发送S0、 S1、 S2。
首先，客户端发送C0表明自己的版本号，不必等对方的回复，然后发送C1表明自己的时间戳。
服务器只有在收到C0的时候，才能返回S0，表明自己的版本号。如果版本不匹配，可以断开连接。
服务器发送完S0后，也不用等什么，就直接发送自己的时间戳S1。客户端收到S1的时候，发一个知道了对方时间戳的ACK C2。同理服务器收到C1的时候，发一个知道了对方时间戳的ACK S2。
于是，握手完成。
《第17讲 | P2P协议：我下小电影，99%急死你》课后思考题除了这种去中心化分布式哈希的算法，你还能想到其他的应用场景吗？
留言问题99%卡住的原因是什么？
《第18讲 | DNS协议：网络世界的地址簿》课后思考题全局负载均衡使用过程中，常常遇到失灵的情况，你知道具体有哪些情况吗？对应应该怎么来解决呢？
留言问题如果权威DNS连不上，怎么办？
一般情况下，DNS是基于UDP协议的。在应用层设置一个超时器，如果UDP发出没有回应，则会进行重试。
DNS服务器一般也是高可用的，很少情况下会挂。即便挂了，也会很快切换，重试一般就会成功。
对于客户端来讲，为了DNS解析能够成功，也会配置多个DNS服务器，当一个不成功的时候，可以选择另一个来尝试。</description></item><item><title>参考答案_对答案，是再次学习的一个机会</title><link>https://artisanbox.github.io/9/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/48/</guid><description>你好，我是编辑宇新。
春节将至，先给你拜个早年：愿你2022年工期变长，需求变少，技术水平更加硬核。
距离我们专栏更新结束已经过去了不少时间，给坚持学习的你点个赞。学习操作系统是一个长期投资，需要持之以恒，才能见效。无论你是二刷、三刷的朋友，还是刚买课的新同学，都建议你充分利用留言区，给自己的学习加个增益buff。这种学习讨论的氛围，也会激励你持续学习。
今天这期加餐，我们整理了课程里的思考题答案，一次性发布出来，供你对照参考，查漏补缺。
建议你一定要先自己学习理解，动脑思考、动手训练，有余力还可以看看其他小伙伴的解题思路，之后再来对答案。
第1节课Q：为了实现C语言中函数的调用和返回功能，CPU实现了函数调用和返回指令，即上图汇编代码中的“call”，“ret”指令，请你思考一下：call和ret指令在逻辑上执行的操作是怎样的呢？
A：一般函数调用的情况下call和ret指令在逻辑上执行的操作如下：
1.将call指令的下一条指令的地址压入栈中；
2.将call指令数据中的地址送入IP寄存器中（指令指针寄存器），该地址就是被调用函数的地址；
3.由于IP寄存器地址设置成为被调用函数的地址，CPU自然跳转到被调用函数处开始执行指令；
4.在被调用函数的最后都有一条ret指令，当CPU执行到ret指令时，就从栈中弹出一个数据到IP寄存器，而这个数据通常是先前执行call指令的下一条指令的地址，即实现了函数返回功能。
第2节课Q：以上printf函数定义，其中有个形式参数很奇怪，请你思考下：为什么是“…”形式参数，这个形式参数有什么作用？
A：在C语言中经常使用printf(“%s :%d”,“number is :”,20);printf(“%x :%d”,0x10,20);printf(“%x,%x :%d”,0xba,0xff,20);可以看出，这些printf函数参数个数都不同，因为C语言的特性支持变参函数。而“…”表示支持0个和多个参数，C语言是通过调用者传递参数的，刚好支持这种变参函数。
第3节课Q：其实我们的内核架构不是我们首创的，它是属于微内核、宏内核之外的第三种架构，请问这是什么架构？
A：我们的内核架构是混合内核架构，是介于微、宏架构之间的一种架构，这种架构保证了宏架构的高性能又兼顾了微架构的可移植、可扩展性。
第4节课Q：Windows NT内核属于哪种架构类型？
A：Windows NT内核架构其实既不属于传统的宏内核架构，也不是新的微内核架构，说NT是微内核架构是错误的，NT这种内核架构其实是宏内核的变种——混合内核。
第5节课Q：请问实模式下能寻址多大的内存空间？
A：由于实模式下访问内存的地址是这样产生的：16位段寄存器左移4位，加一个16位通用寄存器，最后形成了20位地址，所以只能访问1MB大的内存空间。
第6节课Q：分页模式下，操作系统是如何对应用程序的地址空间进行隔离的？
A：操作系统会给每个应用程序都配置独立的一套页表数据。应用程序运行时，就让CR3寄存器指向该应用程序的页表数据。运行下一个应用程序时，则会执行同样的操作。
第7节课Q：请你思考一下，如何写出让CPU跑得更快的代码？由于Cache比内存快几个数量级，所以这个问题也可以转换成：如何写出提高Cache命中率的代码？
A：第一，定义变量时，尽量让其地址与Cache行大小对齐。
int a __attribute__((aligned (64)));&amp;nbsp; int b __attribute__((aligned (64)));&amp;nbsp; 第二，操作数据时的顺序，尽量和数据在内存中布局顺序保持一致。
int arr[M][M]; for(int i = 0; i &amp;lt; M; i++) { for(int k = 0; k &amp;lt; M; k++) { arr[i][k] = 0; } } //而非这样 for(int i = 0; i &amp;lt; M; i++) { for(int k = 0; k &amp;lt; M; k++) { arr[k][i] = 0; } } 第三，尽量少用全局变量。</description></item><item><title>学习指南_如何学习这门编译原理实战课？</title><link>https://artisanbox.github.io/7/53/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/53/</guid><description>你好，欢迎来到《编译原理实战课》，我是专栏编辑王惠，很高兴认识你。
我们都知道，“编译原理”是一门特别硬核的计算机基础专业课。你是不是也觉得编译原理知识就像是一片望不到头的大海，任自己在里面怎么扑腾、怎么挣扎都游不到学成的对岸。但是没关系，现在我们可以跟着宫老师的脚步一起探索编译的旅程了。
不过在正式开始学习这门课程之前，我想先和你聊聊这门课程的一些设计思路和特设板块，帮你找到最适合自己的学习方式，让你后面的学习能达到事半功倍的效果。
我们有“学习委员”了首先来说个好消息，咱这门课呢有学习委员陪伴我们一起学。担当学委的是我们的资深用户朱英达同学，他曾就职于百度，履任资深研发工程师，擅长Web前后端相关领域技术，对编译技术在业务场景下的应用也有自己的理解。
他的经历可能和你很相似：作为一名计算机科班出身的程序员，在大学课堂中学习过编译原理这门课，但面对教科书上庞杂的知识体系、晦涩的抽象概念、陈旧的代码用例，无奈只学了个一知半解；工作以后，作为一名一线的Coder，在大厂的环境里，看惯了层出不穷的“造轮子怪象”，最终才发现只有掌握像编译原理这样的底层技术，才是真正的精进之道。所以，他想把编译原理这门课重新捡起来，再学一次。
然而，目前市面上编译方面的技术资料却非常匮乏，被学界奉为经典的“龙书”“虎书”“鲸书”，对初学者来说又不够友好。后来，他遇到了宫老师的《编译原理之美》，跟着老师的思路重走了一遭编译之旅，发现自己之前对于编译技术的很多困惑点都迎刃而解了。比如说，宫老师在阐释虚拟机架构时，谈到了栈机和寄存器架构的优劣，这就对他理解V8引擎在虚拟机架构选型上提供了非常好的参考。
朱学委最终也发现，编译技术的学习绝对不能纸上谈兵，只有把学到的理论知识与自己从事的相关技术领域结合起来，才会真正有所感悟。
你看，编译原理或者说所有的技术，都有这么一个反复学习、反复印证的过程。所以在这门课程中，学委将会基于积累的编译原理基础，以及对这门新课程内容的学习，不定期地分享他学习编译原理的方法和思路，和你一起探讨课程要掌握的要点和难点。当然了，学委也会在留言区督促你交作业，和你一起交流讨论。
有了学委的陪伴，相信你再学习这门课，一定可以事半功倍。
如何学习预备知识模块？接下来，我来说说怎么利用好预备知识模块。
那我先来交代下为什么要特别设计这个模块。就像老师在开篇词中所说的，这门课程会带你一起阅读真实语言编译器的源码，跟踪它们的运行过程，分析编译过程的每一步是如何实现的，并会对有特点的编译技术点加以分析和点评。
但在解析编译器的过程中，一定会涉及到很多编译原理的基础概念、理论和算法，如果你从来没有接触过或者不够了解这些编译原理知识，那必然会在一定程度上影响你后面的学习效果。所以，预备知识模块就是帮你先建立起一个初步的编译原理知识体系，打好基础，为后面的学习做好准备。
如果你已经学过老师的第一季课程《编译原理之美》，预备篇的内容也建议你不要跳过。和第一季课程相比，在这个模块里宫老师会以更加高屋建瓴的方式，来重新交付编译基础知识。所以，你一定要利用这个模块来查漏补缺。
那具体怎么做呢？建议你先看每一讲的标题，然后回顾自己已经学过的、掌握了的知识要点，写下来，写好后再开始学习，学完后对比总结心得。千万不要错过这个再学一次的机会。我们都知道重复是学习的关键一环，相信通过这个模块，你一定能在编译技术的理解上更上层楼。
你可以把预备知识理解为编译基础的一个串讲，涉及到的概念会比较多。所以学习这个模块的时候，我建议你每学完一讲都要自己动手画一下这一讲的知识地图。等8篇结束后，学习委员也会总结一张编译原理的核心基础知识大地图。到时候你可以对比来看，给自己一个直接的反馈。然后一定要利用这张图，在脑子里构建起编译原理的知识框架。这样，你就做好了进入下个模块的学习准备啦。
解析7种语言编译器的过程中，你需要做什么？下面我来说说课程的重头戏，也就是解析7种语言的编译器，包括Java编译器（javac）、Java的JIT编译器（Graal）、Python编译器（CPython）、JavaScript编译器（V8）、Julia语言的编译器、Go语言的编译器（gc），以及MySQL的编译器。
这些编译器都是宫老师精选出来的，具有一定的代表性、采用了不同的编译技术，而且其中某一门语言也非常可能就是你在使用的。我们的课程就是从实战的角度切入，用你最擅长的方式（写代码、读代码）带你分析这些编译器。所以学好这门课的关键就是要动手实践，跟随老师的脚步来亲身体验不同编译器的实现机制。
我建议你最好在学习的过程中手边备着一台电脑，或者是一台能查看到源代码的其他设备，工具不重要，趁手最有效。你在自己上手修改源码的时候，就会发现对编译原理的概念理解得更加深入了。
期中复习周，停下来是为了跑得更快接着来说说期中复习周。这一周安排在“真实编译器解析篇”之后，也就是建立在你已经学习并理解了7种不同语言编译器的运行机制之后。设置复习周的目的，就是想要让你能及时、系统地了解自己前半段课程内容的掌握情况，发现学习上的漏洞，并及时弥补。
在这一周，学委首先会帮你划出复习的重点，给你总结前面解析的7种语言编译器所涉及到的核心知识。总结复习的过程，也就是你在提高编译技术能力的过程。
接下来，老师会给你出一套考试题。通过这次测试，你可以验证一下自己的学习方式是否有效，希望你能够及时调整学习心态和方法，更有效率地进行下一阶段的学习。
另外，在消化知识的同时，你还可以通过其他同学分享的心得，去看看他是如何学习、掌握编译原理知识的，毕竟通过借鉴别人来完善自己也是一种很好的学习方法嘛。
Learning by Sharing，分享了才知道自己那么优秀再接下来，我必须得说说“一课一思”这个学习环节了。
一课一思是每一讲最后的固定模块，具体内容呢，要么是给你留了一道动手实践的作业，要么就是抛出一个开放性的问题，引导你发散思考。如果你对这些问题都有自己的见解或者看法，那就不妨在留言区分享出来。这样渐渐地，你会发现自己就能解答一些同学的问题了，这是非常好的自检学习成果的方式。
另外别忘记了，极客时间还有一个社区交流的版块“部落”。在日常工作中，你一定会经常接触各种代码，也一定有自己非常熟悉的一门或多门编程语言。那么在解析了不同语言的编译器以后，你可以在部落里分享自己对于熟悉的或不熟悉的语言编译器的理解。
比如说，你原来深耕在Java领域，那么在学完了javac编译器和Graal编译器以后，你对Java是不是就有更深刻的理解了？在学完了Python的编译器以后，你是不是对这两门语言之间的共性和特性都更加清晰了？这些思考你都可以分享在部落里，通过分享自己所习得的知识，你会获得更好的成长。
如何验收学习成果？最后，在课程的收尾阶段呢，老师还会跟你一起关注一个热点话题，那就是华为的方舟编译器。相信很多同学对于国产的编译器，一直都是翘首以盼的。华为已经公开了一部分源代码，虽然资料仍然很缺乏，但是通过我们课程的学习，你是否有能力看懂华为的编译器呢？从掌握书本上的原理，到读懂流行的语言，再到理解方舟编译器的实现思路，这会是你能力一步步提升的过程。最终，你甚至可以参与到一款严肃的编译器的研发当中了。
好了，以上就是我想让你重点关注的课程设计和特设板块内容。编译原理是个难啃的硬骨头，但是我相信，只要你保有这份一定要吃透编译技术核心知识的决心，有计划、有重点，结合实践进行学习，就没有什么是看不懂、学不会的了。加油吧，祝你学有所成！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>开篇词_为什么你要学习编译原理？</title><link>https://artisanbox.github.io/6/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/43/</guid><description>你好，我是宫文学，一名技术创业者。我曾经参与过几个公司的创业过程，在开源技术社区也做过一些工作，现在是北京物演科技CEO。
我喜欢做平台性的软件，而编译技术就是产品取得优势的关键。我是国内最早一拨做BPM的，也就是流程管理平台，也是最早一拨做BI平台的，现在流行叫大数据。当时我们只有3个人，用编译技术做了一些硬核的产品原型，跟联想集团签订了战略级合作协议。之后我又做过电子表单和快速开发平台，而它们的核心就是编译技术。
我参与的第一个公司卖给了上市公司，第二个在新三板上市，这些成果在一定程度上受益于编译技术。而我呢，对编译技术一直很重视，也一直保持着兴趣。所以很高兴能在“极客时间”上分享与编译技术有关的原理和经验，希望我的分享能帮助你在编译技术这个领域获得实实在在的进步。
众所周知，编译技术是计算机科学皇冠上的明珠之一。历史上各门计算机语言的发明人，总是被当作英雄膜拜。比尔·盖茨早期最主要的成就，就是写了一个Basic的解释器。当年Brendan Eich设计的JavaScript，虽然语言略微有点儿糙，但却顽强地生存到了现在。
很多国外厂商的软件，普遍都具备二次编程能力，比如Office、CAD、GIS、Mathematica等等。德国SAP公司的企业应用软件也是用自己的业务级语言编写的。目前来看，谷歌也好，苹果也好，微软也好，这些技术巨头们的核心能力，都是拥有自己的语言和生态。可见编译技术有多么重要！
编译技术，与你的工作息息相关但也有一些程序员认为：“我不可能自己去写一门新的语言，还有必要学习编译原理吗？”
这种想法是把编译原理的用途简单化了。编译原理不是只能用于炫耀的屠龙技。 别的不说，作为程序员，在实际工作中你经常会碰到需要编译技术的场景。
Java程序员想必很熟悉Hibernate和Spring，前者用到了编译技术做HQL的解析，后者对注解的支持和字节码动态生成也属于编译技术。所以，如果你要深入理解和用好这类工具，甚至想写这种类型的工具，会需要编译技术。
而PHP程序员在写程序的时候，一般会用到模板引擎实现界面设计与代码的分离。模板引擎对模板进行编译，形成可执行的PHP代码。模板引擎可以很强大，支持条件分支、循环等语法。如果你了解编译技术，会更容易掌握这些模板引擎，甚至写出更符合领域需求的模板引擎。
我们2001年开发了一款工作流软件，里面有依据自定义公式判断流转方向的功能。像这类需要用户自定义功能的软件，比如报表软件、工资管理软件等，都需要编译技术。
如果你要参与编写一个基础设施类的软件，比如数据库软件、ETL软件、大数据平台等，很多需要采用编译技术提供软件自带的语言功能，比如SQL。这种功能无法由外部通用语言实现。
除此之外，解析用户输入，防止代码注入，为前端工程师提供像React那样的DSL，像TypeScript那样把一门语言翻译成另一门语言，像CMake和Maven那样通过配置文件来灵活工作，以及运维工程师分析日志文件等等高级别的需求，都要用到编译技术。
除了丰富的应用场景，学习编译技术对于提升程序员的竞争力也很重要。现在一些大公司在招聘程序员时，有难度的面试题都是涉及底层机制的。因为理解了底层机制，才能有更深入思考问题，以及深层次解决问题的能力，而不是只能盲目地搜索答案，从表面解决问题。而学习编译原理能让你从前端的语法维度、代码优化的维度、与硬件结合的维度几个方面，加深对计算机技术的理解，提升自己的竞争力。
所以，无论你是前端工程师、后端工程师，还是运维工程师，不论你是初级工程师还是职场老手，编译技术都能给你帮助，甚至让你提升一个级别。
编译技术并不难学但问题来了，你可能会说：“我知道编译技术很重要，我也很想把它啃下，可是我每次鼓起勇气拿起《编译原理》，啃不了多少页就放下了。编译原理已经成了我的心魔……”
在我看来，你之所以遇到困难，很大一个原因在于市面上讲述编译原理的内容往往过于抽象和理论化。学习，说到底是一个学和练，以及学以致用的过程。所以在和朋友们沟通了解之后，我想用下面的思路组织课程内容，帮你克服畏难情绪，更好地理解和学习编译原理。
我会通过具体的案例带你理解抽象的原理。比如语义分析阶段有个I属性和S属性，传统课本里只专注I属性和S属性的特点和计算过程，很抽象。那么我会分析常用语言做语义分析时，哪些属性是I属性，哪些是S属性，以及如何进一步运用这些属性，来让你更直观地了解它们。
我也会重视过程，带你一步步趟过雷区。我写了示例程序，带你逐渐迭代出一门脚本语言和一门编译型语言。当然了，我们会遇到一些挑战和问题，而在解决问题的过程中，你会切切实实体会到某个技术在哪个环节会发挥什么作用。最重要的是，你会因此逐渐战胜畏难情绪，不再担心看不懂、学不会。
我还会让你在工作中真正运用到编译技术。课程里的代码，可以给你的工作提供参考。我介绍的Antlr和LLVM工具，前者能帮你做编译器前端的工作，后者能帮你完成编译器后端的工作。在课程中，你能真正运用编译技术解决报表设计等实际问题。
为了帮你迅速了解课程的知识结构体系，我画了一张思维导图。课程从三方面展开，包括实现一门脚本语言、实现一门编译型语言和面向未来的编程语言。
课程的第一部分主要聚焦编译器前端技术，也就是通常说的词法分析、语法分析和语义分析。我会带你了解它们的原理，实现一门脚本语言。我也会教你用工具提升编译工作的效率，还会在几个应用场景中检验我们的学习成果。
第二部分主要聚焦编译器后端技术，也就是如何生成目标代码和对代码进行优化的过程。我会带你纯手工生成汇编代码，然后引入中间代码和后端工具LLVM，最后生成可执行的文件能支持即时编译，并经过了多层优化。
第三部分是对编译技术发展趋势的一些分析。这些分析会帮助你更好地把握未来技术发展的脉搏。比如人工智能与编译技术结合是否会出现人工智能编程？云计算与编译技术结合是否会催生云编程的新模式？等等。
写在后面课程虽然只有30多节，但每节课绝对是干货满满。我希望这个课程能让所有有志于提升自己技术的工程师，顺利攻下编译技术这重要的一关，能够在工作中应用它见到实效，并且对编程理解更上一层。
最后，我希望你在留言区立下Flag，写下自己的计划，在“极客时间”与志同道合的朋友互相监督，一起学习，一起进步！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>开篇词_为什么你需要学习计算机组成原理？</title><link>https://artisanbox.github.io/4/59/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/59/</guid><description>你好，我是徐文浩，一个正在创业的工程师。目前主要是通过自然语言处理技术，为走向海外的中国企业提供英语的智能客服和社交网络营销服务。
2005年从上海交通大学计算机系毕业之后，我一直以写代码为生。如果从7岁第一次在少年宫写程序开始算起，到今天，我的码龄快有30岁了。这些年里，我在Trilogy Software写过各种大型企业软件；在MediaV这样的广告科技公司，从零开始搭建过支撑每天百亿流量的广告算法系统；2015年，我又加入了拼多多，参与重写拼多多的交易系统。
这么多年一直在开发软件，我深感软件这个行业变化太快了。语言上，十年前流行Java，这两年流行Go；框架上，前两年流行TensorFlow，最近又流行PyTorch。我逐渐发现，学习应用层的各种语言、框架，好比在练拳法招式，可以短期给予你回报，而深入学习“底层知识”，就是在练扎马步、核心肌肉力量，是在提升你自己的“根骨”和“资质”。
正所谓“练拳不练功，到老一场空”。如果越早去弄清楚计算机的底层原理，在你的知识体系中“储蓄”起这些知识，也就意味着你有越长的时间来收获学习知识的“利息”。虽然一开始可能不起眼，但是随着时间带来的复利效应，你的长线投资项目，就能让你在成长的过程中越走越快。
计算机底层知识的“第一课”如果找出各大学计算机系的培养计划，你会发现，它们都有差不多十来门核心课程。其中，“计算机组成原理”是入门和底层层面的第一课。
这是为什么呢？我们直接用肉眼来看，计算机是由CPU、内存、显示器这些设备组成的硬件，但是，计算机系的学生毕业之后，大部分却都是从事各种软件开发工作。显然，在硬件和软件之间需要一座桥梁，而“计算机组成原理”就扮演了这样一个角色，它既隔离了软件和硬件，也提供了让软件无需关心硬件，就能直接操作硬件的接口。
也就是说，你只需要对硬件有原理性的理解，就可以信赖硬件的可靠性，安安心心用高级语言来写程序。无论是写操作系统和编译器这样的硬核代码，还是写Web应用和手机App这样的应用层代码，你都可以做到心里有底。
除此之外，组成原理是计算机其他核心课程的一个“导引”。学习组成原理之后，向下，你可以学习数字电路相关的课程，向上，你可以学习编译原理、操作系统这些核心课程。如果想要深入理解，甚至设计一台自己的计算机，体系结构是必不可少的一门课，而组成原理是计算机体系结构的一个入门版本。
所以说，无论你想要学习计算机的哪一门核心课程，之前你都应该先学习一下“计算机组成原理”，这样无论是对计算机的硬件原理，还是软件架构，你对计算机方方面面的知识都会有一个全局的了解。
学习这门“第一课”的过程，会为你在整个软件开发领域中打开一扇扇窗和门，让你看到更加广阔的天地。比如说，明白了高级语言是如何对应着CPU能够处理的一条条指令，能为你打开编译原理这扇门；搞清楚程序是如何加载运行的，能够让你对操作系统有更深入的理解。
因此，学好计算机组成原理，会让你对整个软件开发领域的全貌有一个系统了解，也会给你带来更多的职业发展机会。像我自己的团队里，有个小伙伴开始是做算法应用开发的，因为有扎实的计算机基础知识，后来就转去开发TVM这样的深度学习编译器了，是不是很厉害？
理论和实践相结合说了这么多计算机组成原理的重要性，但到底该怎么学呢？接下来跟你分享我的心得。
我自己对计算机硬件的发展历史一直很感兴趣，所以，我读了市面上很多组成原理相关的资料。
互联网时代，我们从来不缺少资料。无论是Coursera上北京大学的《计算机组成》开放课程，还是图灵奖作者写的《计算机组成与设计：硬件/软件接口》，都珠玉在前，是非常优秀的学习资料。不过“买书如山倒，读书如抽丝”。从业这么多年，周围想要好好学一学组成原理的工程师不少，但是真的坚持下来学完、学好的却不多。大部分买来的书，都是前面100页已经发黄了，后面500页从来没有打开过；更有不少非科班出身的程序员，直接说“这些书根本看不懂”。
对这些问题，我都深有感触。从自己学习和工作的经验看，我找到了三个主要原因。
第一，广。组成原理中的概念非常多，每个概念的信息量也非常大。比如想要理解CPU中的算术逻辑单元（也就是ALU）是怎么实现加法的，需要牵涉到如何把整数表示成二进制，还需要了解这些表示背后的电路、逻辑门、CPU时钟、触发器等知识。
第二，深。组成原理中的很多概念，阐述开来就是计算机学科的另外一门核心课程。比如，计算机的指令是怎么从你写的C、Java这样的高级语言，变成计算机可以执行的机器码的？如果我们展开并深入讲解这个问题，就会变成《编译原理》这样一门核心课程。
第三，学不能致用。学东西是要拿来用的，但因为这门课本身的属性，很多人在学习时，常常沉溺于概念和理论中，无法和自己日常的开发工作联系起来，以此来解决工作中遇到的问题，所以，学习往往没有成就感，就很难有动力坚持下去。
考虑到这些，在这个专栏构思之初，我就给自己定了一个交付目标：我要把这些知识点和日常工作、生活以及整个计算机行业的发展史联系起来，教你真正看懂、学会、记住组成原理的核心内容，教你更多地从“为什么”这个角度，去理解这些知识点，而不是只是去记忆“是什么”。
对于这个专栏，具体我是这样设计的。
第一，我把组成原理里面的知识点，和我在应用开发和架构设计中遇到的实际案例，放到一起进行印证，通过代码和案例，让你消化理解。
比如，为什么Disruptor这个高性能队列框架里，要定义很多没有用的占位变量呢？其实这是为了确保我们唯一关心的参数，能够始终保留在CPU的高速缓存里面，而高速缓存比我们的内存要快百倍以上。
第二，我会尽可能地多举一些我们日常生活里面的例子，让你理解计算机的各个组件是怎么运作的。在真实的开发中，我们会遇到什么问题，这些问题产生的根源是什么。让你从知识到应用，最终又回到知识，让学习和实践之间形成一道闭环。
计算机组成中很多组件的设计，都不是凭空发明出来，它们中的很多都来自现实生活中的想法和比喻。而底层很多硬件设计和开发的思路，其实也和你进行软件架构的开发设计和思路是一样的。
比如说，在硬件上，我们是通过最基本的与、或、非、异或门这些最基础的门电路组合形成了强大的CPU。而在面向对象和设计模式里，我们也常常是通过定义基本的Command，然后组合来完成更复杂的功能；再比如说，CPU里面的冒险和分支预测的策略，就好像在接力赛跑里面后面几棒的选手早点起跑，如果交接棒没有问题，自然占了便宜，但是如果没能交接上，就会吃个大亏。
第三，在知识点和应用之外，我会多讲一些计算机硬件发展史上的成功和失败，让你明白很多设计的历史渊源，让你更容易记住“为什么”，更容易记住这些知识点。
比如说，奔腾4的失败，就是受限于超长流水线带来的散热和功耗问题，而移动时代ARM的崛起，则是因为Intel的芯片功耗太大，不足以在小小的手机里放下足够支撑1天的电池。计算机芯片的兴盛和衰亡，往往都是因为我们的计算机遇到了“功耗墙”这个散热和能耗上的挑战。而现代的云计算数据中心的设计到选址，也是围绕功耗和散热的。理解了这些成功和失败背后的原因，你自然记住了这些背后的知识点。
最后，在这三种帮助你理解“为什么”的方法之上，我会把整个的计算机组成原理通过指令、计算、CPU、存储系统和I/O串起来。通过一个程序的执行过程进行逐层分解，让你能对整个系统有一个全貌的了解。
我希望这个专栏，不仅能够让你学好计算机组成原理的知识，更能够成为引领你进入更多底层知识的大门，让你有动力、有方法、更深入地去进一步学习体系结构、操作系统、编译原理这样的课程，成为真正的“内家高手”。
“人生如逆旅，我亦是行人”。学习总不会是一件太轻松的事情，希望在这个专栏里，你能和我多交流，坚持练完这一手内功。
下面，你可以讲一讲，你对于计算机组成原理的认识是怎样的？在之前工作中，哪些地方用到了计算机组成原理相关的知识呢？欢迎写在留言区，我们一起交流。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>开篇词_为什么要学写一个操作系统？</title><link>https://artisanbox.github.io/9/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/51/</guid><description>你好，我是彭东，网名LMOS，欢迎加入我的专栏，跟我一起开启操作系统的修炼之路。
先来介绍一下我自己。我是Intel 傲腾项目开发者之一，也是《深度探索嵌入式操作系统》这本书的作者。
我曾经为Intel做过内核层面的开发工作，也对Linux、BSD、SunOS等开源操作系统，还有Windows的NT内核很熟悉。这十几年来，我一直专注于操作系统内核研发。
LMOS（基于x86平台支持多进程、多CPU、虚拟化等技术的全64位操作系统内核）跟LMOSEM（基于ARM处理器平台的嵌入式操作系统内核）是我独立开发的两套全新的操作系统内核，其中LMOS的代码规模达到了数十万行，两个系统现在仍在更新。
当时是基于兴趣和学习的目的开始了这两套操作系统，在这个过程中，我遇到了各种各样的技术问题，解决了诸多疑难杂症，总结了大量的开发操作系统的方法和经验。非常希望能在这个专栏与你一起交流。
每个工程师都有必要学好操作系统吗？经常会有同学问我这样一些问题：我是一个做应用层开发的工程师，有必要学习操作系统吗？我的日常工作中，好像用不到什么深奥的操作系统内核知识，而且大学时已经学过了操作系统课程，还有必要再学吗？
对于这些问题，我的答案当然是“有必要”。至于理由么，请听我慢慢为你道来。
你是否也跟我一样，曾经在一个数千万行代码的大项目中茫然失措？一次次徘徊在内存为什么会泄漏、服务进程为什么会dang掉、文件为什么打不开等一系列“基础”问题的漩涡中？
你是否惊叹于Nginx的高并发性？是不是感觉Golang的垃圾回收器真的很垃圾？除了这样的感叹，你也许还好奇过这样一些问题：MySQL的I/O性能还能不能再提升？网络服务为什么会掉线？Redis中经典的Reactor设计模式靠什么技术支撑？Node.js 的 I/O 模型长什么模样……
如果你也追问过上面的这些问题，那这会儿我也差不多可以给充满求知欲的你指一条“明路”了。这些都将在后面的学习中，找到答案。
为什么说操作系统很重要？首先我们都知道，操作系统是所有软件的基础，所有上层软件都要依赖于操作系统提供的各种机制，才能运行。
而我在工作中也认识了很多技术大牛，根据我的观察，他们的基本功往往十分扎实，这对他们的架构视野、技术成长都十分有帮助。
如果你是后端工程师，在做高性能服务端编程的时候，内存、进程、线程、I/O相关的知识就会经常用到。还有，在做一些前端层面的性能调优时，操作系统相关的一些知识更是必不可少。
除了Web开发，做高性能计算超级计算机的时候，操作系统内核相关的开发能力也至关重要。其实，即使单纯的操作系统内核相关的开发能力，对于工程师来说也是绕不过的基本功。
对于运维、测试同学，你要维护和测试的任何产品，其实是基于操作系统的。比如给服务配置多大的内存、多大的缓存空间？怎样根据操作系统给出的信息，判断服务器的问题出现在哪里。随着你对操作系统的深入理解和掌握，你才能透过现象看本质，排查监控思路也会更开阔。
除了工作，操作系统离我们的生活也并不遥远，甚至可以说是息息相关。要知道，操作系统其实不仅仅局限于手机和电脑，你的智能手表、机顶盒、路由器，甚至各种家电中都运行着各种各样的操作系统。
可以说，操作系统作为计算机的灵魂，眼前的工作、日常的生活，甚至这个行业未来的“诗与远方”都离不开它。
操作系统很难，我能学得会么？但即使是大学时期就学过操作系统的同学，也可能会感觉学得云里雾里。更别说非科班的一些人，难度更甚，甚至高不可攀。那为什么我这么有信心，给你讲好操作系统这门课呢？这还要从我自己的学习经历说起。
跟许多人一样，我看的第一本C教程就是那本“老谭C”。看了之后，除了能写出那个家喻户晓的“hello world”程序，其它什么也干不了。接着我又开始折腾C++、Java，结果如出一辙，还是只能写个“hello world”程序。
还好我有互联网，它让我发现了数据结构与算法，经过一番学习，后来我总算可以写一些小功能的软件了，但或许那根本就称不上功能。既然如此，我就继续折腾，继续学习微机原理、汇编语言这些内容。
最后我终于发现，操作系统才是我最想写的软件。我像着了魔一样，一切操作系统、硬件层相关的书籍都找来看。
有了这么多的“输入”，我就想啊，既然是写操作系统，为什么不能把这些想法用代码实现出来，放在真正的计算机上验证一下呢？
LMOS的雏形至此诞生。从第一行引导代码开始，一次又一次代码重构，一次又一次地面对莫名的死机而绝望，倒逼我不断改进，最终才有了现在的LMOS。因为一个人从零开始，独立开发操作系统这种行为有点疯狂，我索性就用LMOS（liberty，madness，operating，system）来命名了我的操作系统。
经过我这几年的独立开发，现在LMOS已经发布了8个测试版本。先后从32位单CPU架构发展到64位多CPU架构，现在的LMOS已经是多进程、多线程、多CPU、支持虚拟内存的x86_64体系下的全64位操作系统内核，代码量已经有10万多行了。
后来，我又没忍住自己的好奇心，写了个嵌入式操作系统——LMOSEM。由于有了先前的功底，加上ARM体系很简单，所以我再学习和实现嵌入式操作系统时，就感觉驾轻就熟了。
经过跋山涉水，我再回头来看，很容易就发现了为什么操作系统很难学。
操作系统需要你有大量的知识储备，但是现在大多的课程、学习资料，往往都是根据目前已有的一些操作系统，做局部解读。所以，我们学的时候，前后的知识是无法串联在一起的。结果就会越看越迷惑，不去查吧，看不懂，再去搜索又加重了学习负担，最后只能遗憾放弃。
那怎样学习操作系统才是最高效的呢？理论基础是要补充的，但相对来说，实践更为重要。我认为，千里之行还得始于足下。
所以，通过这个专栏，我会带你从无到有实现一个自己的操作系统。
我会使用大量的插图代码和风趣幽默的段子，来帮助你更好地理解操作系统内核的本质。同时在介绍每个内核组件实现时，都会先给你说明白为什么，带着你基于设计理解去动手实现；然后，再给你详细描述Linux内核对应的实现，做前后对比。这样既能让你边学边练，又能帮你从“上帝视角”审视Linux内核。
我们课程怎么安排的？操作系统作为计算机王国的权力中枢，我们的课程就是讲解如何实现它。
为此，我们将从了解计算机王国的资源开始，如CPU、MMU、内存和Cache。其次要为这个权力中枢设计基本法，即各种同步机制，如信号量与自旋锁。接着进行夺权，从固件程序的手中抢过计算机并进行初始化，其中包含初始化CPU、内存、中断、显示等。
然后，开始建设中枢的各级部门，它们分别是内存管理部门、进程管理部门、I/O管理部门、文件管理部门、通信管理部门。最后将这些部门组合在一起，就形成了计算机王国的权力中枢——操作系统。
我们的课程就是按照上述逻辑，依次为你讲解这些部门的实现过程和细节。每节课都配有可以工作的代码，让你能跟着课程一步步实现。你也可以直接使用我提供的代码一步步调试，直到最终实现一个基于x86平台的64位多进程的操作系统——Cosmos。
你能获得什么？走这样一条“明路”，一步一个脚印，最终你会到达这样一个目的地：拥有一个属于自己的操作系统内核，同时收获对Linux内核更深入的理解。
学完这门课，你会明显提升操作系统架构设计能力，并且可以学会系统级别的软件编程技巧。我相信，这对你拓展技术深度和广度是大有裨益的。之后你在日常开发中遇到问题的时候，就可以尝试用更多维度的能力去解决问题了。
同时，由于操作系统内核是有核心竞争力的高技术含量软件，这能给你职业生涯的成长带来长远的帮助。如今，在任何一家中大型互联网公司都使用大量的Linux服务器。
操作系统相关的内容，已经成为你涨薪、晋升的必考项，比如 Linux 内核相关的技术，中断、I/O、网络、多线程、并发、性能、内存管理、系统稳定性、文件系统、容器和虚拟化等等，这些核心知识都来源于操作系统。
而跳出个人，从大局观出发的话，计算机作为20世纪以来人类最伟大的发明之一，已经深入人们生活的方方面面，而计算机系统作为国家级战略基础软件，却受制于人，这关系到整个国家的信息安全，也关系到互联网信息行业以及其它相关基础行业的前途和未来。
而要改变这一困局，就要从培养技术人才开始。对于我们工程师来说，树高叶茂，系于根深，只有不断升级自己的认知，才能让你的技术之路行稳致远。
下面，我给出一个简化的操作系统知识体系图，也是后面课程涉及到的所有知识点。尽管图中只是最简短的一些词汇，但随着课程的展开，你会发现图中的每一小块，都犹如一片汪洋。
现在让我们一起带着好奇，带着梦想，向星辰大海进发！
课程交流群点这里加入。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>开篇词_从今天起，跨过“数据结构与算法”这道坎</title><link>https://artisanbox.github.io/2/77/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/77/</guid><description>你好，我是王争，毕业于西安交通大学计算机专业。现在回想起来，本科毕业的时候，我的编程水平其实是很差的。直到读研究生的时候，一个师兄给了我一本《算法导论》，说你可以看看，对你的编程会很有帮助。
没想到，从此我对算法的“迷恋”便一发不可收拾。之后，我如饥似渴地把图书馆里几乎所有数据结构和算法书籍都读了一遍。
我常常边读边练。没多久，我就发现，写代码的时候，我会不由自主考虑很多性能方面的问题。我写出时间复杂度高、空间复杂度高的垃圾代码越来越少了，算法能力提升了很多，编程能力也有了质的飞跃。得益于此，研究生毕业后，我直接进入Google，从事Google翻译相关的开发工作。
这是我自己学习数据结构与算法的经历，现在，你可以想想你的情况。
是不是从学校开始，你就觉得数据结构难学，然后一直没认真学？
工作中，一遇到数据结构这个坑，你又发自本能地迅速避让，因为你觉得自己不懂，所以也不想深究，反正看起来无关大局？
当你想换工作面试，或者研究某个开源项目源码，亦或者和团队讨论某个非框架层面的高可用难题的时候，你又发现，自己的基础跟不上别人的节奏？
如果你是这种情况，其实你并不孤独，这不是你一个人遇到的问题。工作十年间，我见过许多程序员。他们有着各种各样的背景，有很多既有潜力又非常努力，但始终无法在自己现有水平上更进一步。
在技术圈里，我们经常喜欢谈论高大上的架构，比如高可用、微服务、服务治理等等。鲜有人关注代码层面的编程能力，而愿意沉下心来，花几个月时间啃一啃计算机基础知识、认认真真夯实基础的人，简直就是凤毛麟角。
我认识一位原来腾讯T4的技术大牛。在区块链大潮之前，他在腾讯工作了10多年，长期负责手机QQ后台整体建设。他经历了手机QQ从诞生到亿级用户在线的整个过程。后来他去了微众银行，有一天老板让他去做区块链。他用了不到半年时间，就把区块链的整个技术脉络摸清楚了。 现在，他是微众银行的区块链负责人，微众科技创新产品部的老总。你说厉害不？你可以花半年时间就能精通一个新的领域吗？为什么他就可以做到？
我觉得这其中最重要的就是基础足够扎实。他曾经跟我说，像区块链、人工智能这些看似很新的技术，其实一点儿都不“新”。最初学编程的时候，他就把那些基础的知识都学透了。当面临行业变动、新技术更迭的时候，他不断发现，那些所谓的新技术，核心和本质的东西其实就是当初学的那些知识。掌握了这个“规律”之后，他学任何东西都很快，任何新技术都能快速迎头赶上。这就是他快速学习并且获得成功的秘诀。
所以说，基础知识就像是一座大楼的地基，它决定了我们的技术高度。而要想快速做出点事情，前提条件一定是基础能力过硬，“内功”要到位。
那技术人究竟都需要修炼哪些“内功”呢？我觉得，无外乎就是大学里的那些基础课程，操作系统、计算机网络、编译原理等等，当然还有数据结构和算法。
可是，我们都知道，像《算法导论》这些经典书籍，虽然很全面，但是过于理论，学起来非常枯燥；而市面很多课程大多缺失真实的开发场景，费劲学完感觉好像还是用不上，过不了几天就忘了。
所以，我尝试做一个让你能真正受用的数据结构与算法课程，希望给你指明一个简洁、高效的学习路径，教你一个学习基础知识的通用方法 。那么，关于专栏内容，我是怎样设计的呢？
我根据自己研读数十本算法书籍和多年项目开发的经验，在众多的数据结构和算法中，精选了最实用的内容进行讲解。
我不只会教你怎么用，还会告诉你，我们为什么需要这种数据结构和算法，一点点帮你捋清它们背后的设计思想，培养你举一反三的能力。
对于每种数据结构和算法，我都会结合真实的软件开发案例来讲解，让你知道，数据结构和算法，究竟应该如何应用到实际的编码中。
为了由浅入深地带你学习，我把专栏分成四个递进的模块。
入门篇 时间、空间复杂度分析是数据结构和算法中非常重要的知识点，贯穿整个专栏的学习过程。但同时也是比较难掌握的，所以我用了2节课来讲这部分内容，而且还举了大量的实例，让你一边学一边练，真正能掌握复杂度分析，为后面的学习铺路。
我希望通过这一模块，你能掌握时间、空间复杂度的概念，大O表示法的由来，各种复杂度分析技巧，以及最好、最坏、平均、均摊复杂度分析方法。之后，面对任何代码的复杂度分析，你都能游刃有余、毫不畏惧！
基础篇 这部分是专栏中篇幅最大的内容，也是我们学习的重点，共有26节内容，涵盖了最基础、最常用的数据结构和算法。针对每种数据结构和算法，我都会结合具体的软件开发实例，由浅入深进行讲解，并适时总结一些实用“宝典”，保证你印象深刻、学有所用。
比如递归这一节，我会讲到，为什么递归代码比较难写？如何避免堆栈溢出？如何避免递归冗余计算？如何将递归代码转化为非递归代码？
高级篇 这部分我会讲一些不是那么常用的数据结构和算法。虽然不常用，但是这些内容你也需要知道。设置这一部分的目的，是为了让你开拓视野，强化训练算法思维、逻辑思维。如果说学完基础部分可以考80分，那掌握这一部分就能让你成为尖子生！
实战篇 我们整个专栏都是围绕数据结构和算法在具体软件实践中的应用来讲的，所以最后我会通过实战部分串讲一下前面讲到的数据结构和算法。我会拿一些开源项目、框架或者系统设计问题，剖析它们背后的数据结构和算法，让你有一个更加直观的感受。
人生路上，我们会遇到很多的坎。跨过去，你就可以成长，跨不过去就是困难和停滞。而在后面很长的一段时间里，你都需要为这个困难买单。对于我们技术人来说，更是这样。既然数据结构和算法这个坎，我们总归是要跨过去，为什么不是现在呢？
我很感激师兄当年给我的那本《算法导论》，这是我人生中为数不多的转折点之一。没有那本书，也可能就没有今天的我。我希望这个专栏也能成为你的一个人生转折点。
我希望，通过这个专栏，不仅能帮你跨过数据结构与算法这个坎，还能帮你掌握一种学习知识和技能的方法，帮你度过职场甚至人生的重要时刻！一起加油吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>开篇词_在实战中学习，是解锁MySQL技能的最佳方法</title><link>https://artisanbox.github.io/8/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/31/</guid><description>你好，我是朱晓峰。
工作二十多年来，我一直在和MySQL打交道。我曾任摩根大通银行技术部副总裁，带领团队为纽约、东京等分支银行提供数据存储和安全服务。目前，正致力于开发基于MySQL的管理信息系统，率领团队为包括国家开发银行、百度在线、北京西站等大型企业在内的客户提供了信息服务，并获得了11项软件著作权（比如商业数据管理系统、云POS系统等）。
因为具备丰富的MySQL开发经验，从2015年起，我受聘担任数据应用学院客座讲师，开始制作职业技术培训课程。我和团队开发了一个为期2周、30个课时的MySQL入门集训课。我们打破了传统的教学模式，不去讲零碎的知识点，而是借助一个实际项目去讲必备技能，帮助数百名初学者迅速掌握了MySQL的基本操作和核心技能。经过2周的集中培训，有几十位学生顺利进入谷歌、苹果和亚马逊等公司。
多年的项目开发以及培训经历，让我深刻地认识到，熟练使用MySQL，对技术人来说变得越来越重要，是我们拿到心仪Offer的敲门砖。
要知道，MySQL的入门门槛非常低，还具有免费、开放源码等优势，可以满足我们的多样化需求，是目前被广泛使用的数据库之一。
看到这里，你可能会问：“我知道学习MySQL很重要，也花了很多时间去学习，可是学来学去，还是连最简单的实际问题都无法解决，该怎么办呢？”
别着急，接下来我们就来聊聊高效的MySQL学习方法。
为什么学了很多知识，你依然不会用MySQL解决实际问题?很多人刚开始学习MySQL时，都会面临一系列问题。
市面上的MySQL资料这么多，该怎么挑选呢？ 我花了很多时间学习MysQL，但是最后真的遇到问题时，我发现我根本不知道怎么解决。 我会一些基本的操作，但还是很容易踩到坑里。比如，我曾经把字段设置成浮点数，但我不知道它是不精准的，幸亏领导发现了，不然很可能会给项目带来损失。 我储备了不少面试题，为什么一到面试就卡壳呢？ …… 其实，这些问题，本质上都是一个原因导致的：传统的资料都是在讲授一个个零碎的知识点，最多给出一些基础的小练习，让你进行一些简单的训练。所以，很多人花了很多时间去学习，好像懂得了很多东西，但是一遇到真实的项目问题，就会一头雾水，不知道如何用所学的知识去解决实际问题，更没有能力给出完整的解决方案。
我做过项目主管，也长期带团队，深知在工作中，最重要的绝对不是你的知识储备量，而是你解决实际问题的能力。但不幸的是，我见过太多面试时表现优异的人，最终却连试用期都过不了。
说到这里，我特别想和你分享一下我曾面试过的一个应届生的故事。
他是一名计算机专业的研究生，讲起MySQL数据库的相关知识，他说得非常清楚，也很有条理，所以我对他的期望值特别高。但是，等他真正上手做项目时，我才发现，他的知识都停留在理论层面。
举个小例子，一次，我们需要开发一个餐厅的点餐系统，我就请他做数据库设计。没想到他设计出来的订单表，居然没有包含客户编号，这就导致无法通过关联查询获取客户信息。这样的数据库根本不满足业务的需求，自然是不能用的。
这并不是个例，很多人在谈到某些知识时可以出口成章，但是一遇到真实的商业环境，就会毫无头绪，或者是犯这样那样的错误。
在我看来，正确的学习方法，远比你投入的时间更重要。而实战，就是最高效的方法。
为此，我特意选择了一个连锁超市的实战项目，手把手带你从0到1走完项目全流程，不仅帮你掌握核心操作，还能让你真正拥有实战能力，能够迅速上手任何一个项目。无论你是刚刚走入职场，想要迅速解锁MySQL这项技能，还是对它感兴趣，想要转岗到MySQL开发，都可以在这个课程中达到你理想的目标。
之所以选择采用连锁超市的项目，有两个原因。
你对超市这个场景足够熟悉。我们都有去超市购物的经历，会看到货架上摆着玲琅满目的商品，各种各样的促销招牌，还有忙着扫码收银的店员……借助熟悉的场景来讲解，可以最大程度减轻你的理解成本。 超市背后的业务环节非常复杂，产生的数据也多种多样，而MySQL是处理这类业务的利器。当我们完整地解决了超市项目中所遇见的复杂数据问题时，你再去做其他任何业务，就可以更游刃有余一些。 总之，我会从实际问题出发，带你学习技术点，让你能举一反三，快速应用在实战项目中。如果用一个公式去概括，就是：项目的实际需求--&amp;gt;解决问题所需的知识点--&amp;gt;用好这些知识的实战经验。
举个例子，超市的商品非常多，这些商品的名称、数量等，必须要被准确地存储、及时地更新，才能保证正常地售卖。这就是真实需求。要解决这个问题，就要用到MySQL的数据存储功能，我们就要掌握设计数据表、定义字段等知识，确保数据的存储效率最高以及数据的唯一性，同时减少错误。
不只是数据存储，我会带着你解决连锁超市所面临的一系列实际问题，从商品进货到库存查验，再到店面售卖、会员营销，等等。在这个过程中，我会给你讲解MySQL是怎样存储数据的、如何才能高效查询、如何提供经营决策的依据、如何确保数据的可靠性和安全性……
即使你没有数据库的知识基础，也完全不用担心，只要你跟着我的思路，就一定能真正地在短时间内入门MySQL，拥有解决问题的能力。
这门课是怎么设计的？说了这么多，课程的具体设计是怎样的呢？我来介绍一下。
课程总共有四个核心模块。
实践篇：我会从项目最基本的数据存储和操作开始讲起，包括创建数据库、数据表、对表中的数据进行增删改查操作、使用函数、表与表之间的关联操作等，帮你快速掌握最基本的用法。 进阶篇：随着用户管理水平的不断提升，对系统的要求也越来越多，越来越复杂，会用到MySQL的许多高级功能。我会手把手带你实现这些功能，包括把程序存储在服务器上、利用突发事件来调用程序、在不改变存储结构的前提下创建虚拟表以方便查询，等等。 优化篇：项目投入运营以后，随着数据的积累，性能优化的问题逐步凸显。在这个模块呢，我会给你讲一讲数据库的设计规范，还会带你创建数据模型，帮助你来理清设计思路。同时，我还会讲到提升性能的具体方法。 案例篇：在课程的最后，我会手把手带你从0到1设计一个连锁超市的信息系统数据库，把前面讲到的知识点都融入到项目设计中，不仅帮你巩固所学的知识，更教会你如何灵活使用。 除此之外，在课程正式开始之前，我会用图片+音频+视频的形式，带着你安装MySQL及必备的图形化管理工具Workbench。同时，我还特意设置了一个特别放送模块，给你讲解MySQL 8.0的新特性、空间定位的方法，以及大厂的高频面试题，帮你轻松拿下面试。
最后，我还想说，MySQL是一个非常优秀的数据库，里面包含了很多经典的设计思想。虽然现在你不需要掌握得这么深，但是我还是建议你多多体会这些思想，这会让你提前建立起大局观，还可以帮助你从更高的层面去看待所遇见的实际问题。
在这门课程里，我会把我这么多年的经验毫无保留地分享给你，欢迎你来学习这门课，也欢迎你把咱们的课程分享给你的朋友或同事，邀请他们和你一起学习，共同成长。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>开篇词_在真实世界的编译器中游历</title><link>https://artisanbox.github.io/7/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/48/</guid><description>你好，我是宫文学，一名技术创业者，现在是北京物演科技的CEO，很高兴在这里跟你见面。
我在IT领域里已经工作有20多年了。这其中，我个人比较感兴趣的，也是倾注时间精力最多的，是做基础平台类的软件，比如国内最早一批的BPM平台、BI平台，以及低代码/无代码开发平台（那时还没有这个名字）等。这些软件之所以会被称为平台，很重要的原因就是拥有很强的定制能力，比如流程定制、界面定制、业务逻辑定制，等等。而这些定制能力，依托的就是编译技术。
在前几年，我参与了一些与微服务有关的项目。我发现，前些年大家普遍关注那些技术问题，比如有状态的服务（Stateful Service）的横向扩展问题，在云原生、Serverless、FaaS等新技术满天飞的时代，不但没能被很好地解决，反而更恶化了。究其原因就是，状态管理还是被简单地交给数据库，而云计算的场景使得数据库的压力更大了，数据库原来在性能和扩展能力上的短板，就更加显著了。
而比较好的解决思路之一，就是大胆采用新的计算范式，发明新的计算机语言，所以我也有意想自己动手搞一下。
我从去年开始做设计，已经鼓捣了一阵了，采用了一些很前卫的理念，比如云原生的并发调度、基于Actor的数据管理等。总的目标，是要让开发云原生的、有状态的应用，像开发一个简单的单机应用一样容易。那我们就最好能把云架构和状态管理的细节给抽象掉，从而极大地降低成本、减少错误。而为编程提供更高的抽象层次，从来就是编译技术的职责。
Serverless和FaaS已经把无状态服务的架构细节透明掉了。但针对有状态的服务，目前还没有答案。对我而言，这是个有趣的课题。
在我比较熟悉的企业应用领域，ERP的鼻祖SAP、SaaS的鼻祖SalesForce，都用自己的语言开发应用，很可惜国内的企业软件厂商还没有做到这一点。而在云计算时代，设计这样一门语言绕不过去的一个问题，就是解决有状态服务的云化问题。我希望能为解决这个问题提供一个新工具。当然，这个工具必须是开源的。
正是因为给自己挖了这么大一个坑，也促使我更关心编译技术的各种前沿动态，也非常想把这些前沿的动态、理念，以及自己的一些实战经验都分享出来。
所以去年呢，我在极客时间上开了一门课程《编译原理之美》，帮你系统梳理了编译技术最核心的概念、理论和算法。不过在做第一季的过程中呢，我发现很多同学都跟我反馈：我确实理解了编译技术的相关原理、概念、算法等，但是有没有更直接的方式，能让我更加深入地把知识与实践相结合呢？
为什么要解析真实编译器？说到把编译技术的知识与实践相结合，无外乎就是解决以下问题：
我已经知道，语法分析有自顶向下的方法和自底向上的方法，但要自己动手实现的话，到底该选择哪个方法呢？是应该自己手写，还是用工具生成呢？ 我已经知道，在语义分析的过程中要做引用消解、类型检查，并且会用到符号表。那具体到自己熟悉的语言，这些工作是如何完成的呢？有什么难点和实现技巧呢？符号表又被设计成什么样子呢？ 我已经知道，编译器中会使用IR，但实际使用中的IR到底是什么样子的呢？使用什么数据结构呢？完成不同的处理任务，是否需要不同的IR呢？ 我已经知道，编译器要做很多优化工作，但针对自己熟悉的语言，这些优化是如何发生的？哪些优化最重要？又要如何写出便于编译器优化的代码呢？ 类似的问题还有很多，但总结起来其实就是：真实世界的编译器，到底是怎么写出来的？
那弄明白了这个问题，到底对我们有什么帮助呢？
第一，研究这些语言的编译机制，能直接提高我们的技术水平。
一方面，深入了解自己使用的语言的编译器，会有助于你吃透这门语言的核心特性，更好地运用它，从而让自己向着专家级别的工程师进军。举个例子，国内某互联网公司的员工，就曾经向Oracle公司提交了HotSpot的高质量补丁，因为他们在工作中发现了JVM编译器的一些不足。那么，你是不是也有可能把一门语言吃得这么透呢？
另一方面，IT技术的进化速度是很快的，作为技术人，我们需要迅速跟上技术更迭的速度。而这些现代语言的编译器，往往就是整合了最前沿的技术。比如，Java的JIT编译器和JavaScript的V8编译器，它们都不约而同地采用了“Sea of Nodes”的IR来做优化，这是为什么呢？这种IR有什么优势呢？这些问题我们都需要迅速弄清楚。
第二，阅读语言编译器的源码，是高效学习编译原理的重要路径。
传统上，我们学习编译原理，总是要先学一大堆的理论和算法，理解起来非常困难，让人望而生畏。
这个方法本身没有错，因为我们学习任何知识，都要掌握其中的原理。不过，这样可能离实现一款实用的编译器还有相当的距离。
那么根据我的经验，学习编译原理的一个有效途径，就是阅读真实世界中编译器的源代码，跟踪它的执行过程，弄懂它的运行机制。因为只要你会写程序，就能读懂代码。既然能读懂代码，那为什么不直接去阅读编译器的源代码呢？在开源的时代，源代码就是一个巨大的知识宝库。面对这个宝库，我们为什么不进去尽情搜刮呢？想带走多少就带走多少，没人拦着。
当然，你可能会犯嘀咕：编译器的代码一般都比较难吧？以我的水平，能看懂吗？
是会有这个问题。当我们面对一大堆代码的时候，很容易迷路，抓不住其中的重点和核心逻辑。不过没关系，有我呢。在本课程中，我会给你带路，并把地图准备好，带你走完这次探险之旅。而当你确实把握了编译器的脉络以后，你对自己的技术自信心会提升一大截。这些计算机语言，就被你摘掉了神秘的面纱。
俗话说“读万卷书，行万里路”。如果说了解编译原理的基础理论和算法是读书的过程，那么探索真实世界里的编译器是什么样子，就是行路的过程了。根据我的体会，当你真正了解了身边的语言的编译器是怎样编写的之后，那些抽象的理论就会变得生动和具体，你也就会在编译技术领域里往前跨出一大步了。
我们可以解析哪些语言的编译器？那你可能要问了，在本课程中，我都选择了哪些语言的编译器呢？选择这些编译器的原因又是什么呢？
这次，我要带你解析的编译器还真不少，包括了Java编译器（javac）、Java的JIT编译器（Graal）、Python编译器（CPython）、JavaScript编译器（V8）、Julia语言的编译器、Go语言的编译器（gc），以及MySQL的编译器，并且在讲并行的时候，还涉及了Erlang的编译器。
我选择剖析这些语言的编译器，有三方面的原因：
第一，它们足够有代表性，是你在平时很可能会用到的。这些语言中，除了Julia比较小众外，都比较流行。而且，虽然Julia没那么有名，但它使用的LLVM工具很重要。因为LLVM为Swift、Rust、C++、C等多种语言提供了优化和后端的支持，所以Julia也不缺乏代表性。 第二，它们采用了各种不同的编译技术。这些编译器，有的是编译静态类型的语言，有的是动态类型的语言；有的是即时编译（JIT），有的是提前编译（AOT）；有高级语言，也有DSL（SQL）；解释执行的话，有的是用栈机（Stack Machine），有的是用寄存器机，等等。不同的语言特性，就导致了编译器采用的技术会存在各种差异，从而更加有利于你开阔视野。 第三，通过研究多种编译器，你可以多次迭代对编译器的认知过程，并通过分析对比，发现这些编译器之间的异同点，探究其中的原因，激发出更多的思考，从而得到更全面的、更深入的认知。 看到这里，你可能会有所疑虑：有些语言我没用过，不怎么了解，怎么办？其实没关系。因为现代的高级语言，其实相似度很高。
一方面，对于不熟悉的语言，虽然你不能熟练地用它们来做项目，但是写一些基本的、试验性的程序，研究它的实现机制，是没有什么问题的。
另一方面，学习编译原理的人会练就一项基本功，那就是更容易掌握一门语言的本质。特别是我这一季的课程，就是要帮你成为钻到了铁扇公主肚子里的孙悟空。研究某一种语言的编译器，当然有助于你通过“捷径”去深入地理解它。
我是如何规划课程模块的？这门课程的目标，是要让你对现代语言的编译器的结构、所采用的算法以及设计上的权衡，都获得比较真切的认识。其最终结果是，如果要你使用编译技术来完成一个项目，你会心里非常有数，知道应该在什么地方使用什么技术。因为你不仅懂得原理，更有很多实际编译器的设计和实现的思路作为你的决策依据。
为了达到本课程的目标，我仔细规划了课程的内容，将其划分为预备知识篇、真实编译器解析篇和现代语言设计篇三部分。
在预备知识篇，我会简明扼要地帮你重温一下编译原理的知识体系，让你对这些关键概念的理解变得更清晰。磨刀不误砍柴工，你学完预备知识篇后，再去看各种语言编译器的源代码和相关文档时，至少不会被各种名词、术语搞晕，也能更好地建立具体实现跟原理之间的关联，能互相印证它们。
在真实编译器解析篇，我会带你研究语言编译器的源代码，跟踪它们的运行过程，分析编译过程的每一步是如何实现的，并对有特点的编译技术点加以分析和点评。这样，我们在研究了Java、Java JIT、Python、JavaScript、Julia、Go、MySQL这7个编译器以后，就相当于把编译原理印证了7遍。
在现代语言设计篇，我会带你分析和总结前面已经研究过的编译器，进一步提升你对相关编译技术的认知高度。学完这一模块以后，你对于如何设计编译器的前端、中端、后端、运行时，都会有比较全面的了解，知道如何在不同的技术路线之间做取舍。
好了，以上就是这一季课程的模块划分思路了。你会发现，这次的课程设计，除了以研究真实编译器为主要手段外，会更加致力于扩大你的知识版图、增加你的见识，达到“行万里路”的目的。
可以说，我在设计和组织这一季课程时，花了大量的时间准备。因此这一季课程的内容，不说是独一无二的，也差不多了。你在市面上很少能找到解析实际编译器的书籍和资料，这里面的很多内容，都是在我自己阅读源代码、跟踪源代码执行过程的基础上梳理出来的。
写在最后近些年，编译技术在全球范围内的进步速度很快。比如，你在学习Graal编译器的时候，你可以先去看看，市面上有多少篇围绕它的高质量论文。所以呢，作为老师，我觉得我有责任引导你去看到、理解并抓住这些技术前沿。
我也有一个感觉，在未来10年左右，中国在编译技术领域，也会逐步有拿得出手的作品出来，甚至会有我们独特的创新之处，就像我们当前在互联网、5G等领域中做到的一样。
虽然这个课程不可能涵盖编译技术领域所有的创新点，但我相信，你在其中投入的时间和精力是值得的。你通过我课程中教给你的方法，可以对你所使用的语言产生更加深入的认知，对编译器的内部结构和原理有清晰理解。最重要的是，对于如何采用编译技术来解决实际问题，你也会有能力做出正确的决策。
这样，这个课程就能起到抛砖引玉的作用，让我们能够成为大胆探索、勇于创新的群体的一份子。未来中国在编译技术的进步，就很可能有来自我们的贡献。我们一起加油！
最后，我还想正式认识一下你。你可以在留言区里做个自我介绍，和我聊聊，你目前学习编译原理的最大难点在哪？或者，你也可以聊聊你对编译原理都有哪些独特的思考和体验，欢迎在留言区和我交流讨论。
好了，让我们正式开始编译之旅吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>开篇词_想成为技术牛人？先搞定网络协议！</title><link>https://artisanbox.github.io/5/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/46/</guid><description>你好，我是刘超，网易研究院云计算技术部的首席架构师。我主要负责两部分工作，对内支撑网易核心业务上云，对外帮助客户搞定容器化与微服务化架构。
当极客时间约我做“趣谈网络协议”专栏的时候，我非常开心，因为网络协议也是我长期研究和关注的点。摸爬滚打15年，有了一些收获也溅了一身血，我才能在这里和你分享。
为什么网络协议这么重要呢？为什么“计算机组成与系统结构”“数据结构与算法”“操作系统”“计算机网络”“编译原理”，会成为大学计算机的核心课程呢？至少看起来，这些内容没有“多少天搞定MFC、Structs”这样的内容更容易帮你找到工作。我毕业的时候，也感到很困惑。
不过当时我抱着一个理想，也可能是大多数程序员的理想：我要做技术牛人，我要搞定大系统。
工作15年，我在EMC做过类似GFS的分布式存储开发，做过基于Lucene的搜索引擎，做过Hadoop的运维；在HP和华为做过OpenStack的开发、实施和解决方案；还创业倒腾过Mesos容器平台，后来在网易做Kubernetes。
随着见过的世面越来越多，我渐渐发现，无论是对于大规模系统的架构，还是对于程序员的个人职业生涯，网络和网络协议都是绕不过去的坎儿。
集群规模一大，我们首先想到的就是网络互通的问题；应用吞吐量压不上去，我们首先想到的也是网络互通的问题。不客气地讲，很多情况下，只要搞定了网络，一个大型系统也就搞定了一半。所以，要成为技术牛人，搞定大系统，一定要过网络这一关，而网络协议在网络中占有举足轻重的地位。
相信大部分人都思考过“技术变化太快，容易过时”的问题。毕竟，技术浪潮一浪接一浪，新技术层出不穷。从搜索引擎、大数据、云计算，到人工智能、区块链，简直就是“你方唱罢我登场”。这里面究竟有没有最本质的东西，使得你掌握了它，就能在新技术的滚滚浪潮中，保持快速学习的能力？
通过对大量开源技术的代码进行分析，我发现很多技术看起来轰轰烈烈，扒下外衣，本质的东西其实就是基础知识和核心概念。想要不被滚滚而来的新技术淘汰，就要掌握这些可以长久使用的知识，而网络协议就是值得你学习，而且是到40岁之后依然有价值的知识。
但是，要想真正学习和掌握网络协议，也并非易事。下面这些场景，你是不是也感同身受呢？
网络协议知识点太多，学完记不住。我们都学过计算机网络课程，学的时候感觉并不难。尤其这门课没有公式，更像是文科。学了一大堆，也背了一大堆，应付完考试之后，最终都“还给老师”了。
看上去懂了，但是经不住问。没关系，网上有很多的文章嘛。于是，你会搜索很多文章去看。看的时候，你感觉别人说的很有道理，好像理解了，但是经不住问，一问就发现，你只是了解了大概的流程，很多细节还是不知道。所以说，从能看懂到能给别人讲明白，中间还有很长一段距离。
知识学会了，实际应用依旧不会。细节都摸索得差不多了，但是当你自己去应用和调试的时候，发现还是没有思路。比如，当创建出来的虚拟机不能上网的时候，该怎么办呢？学过的东西，怎么还是不会用？
我把这样的网络协议学习过程总结为：一看觉得懂，一问就打鼓，一用就糊涂。 那网络协议究竟该怎么学？基于这个问题，我决定从以下三个角度和你分享我所理解的网络协议。
第一，我会从身边经常见到的事情出发，用故事来讲解各种网络协议，然后慢慢扩展到不熟悉的领域。
例如，每个人都会查看IP地址，那我们就从这个命令开始，展开一些概念；很多人都在大学宿舍组过简单的网络来打游戏，我就从宿舍里最简单的网络概念开始讲；然后说到办公室，说到日常上网、购物、视频下载等过程涉及的协议；最后说到最陌生的数据中心。
第二，我会用贴近场景的方式来讲解网络协议，将各个层次的关系串起来，而非孤立地讲解某个概念。
常见的计算机网络课程往往会按照网络分层，一层一层地讲，却很少讲层与层之间的关系。例如，我们学习路由协议的时候，在真实场景中，这么多的算法和二层是什么关系呢？和四层又是什么关系呢？例如，在真实的网络通信中，我们访问一个网站，做一个支付，在TCP进行三次握手的时候，IP层在干嘛？MAC层又在干嘛？这些你是不是都清楚？
第三，我会在讲解完各个层次的网络协议之后，着重剖析如何在当下热门领域使用这些协议，比如云计算、容器和微服务。
一方面你可以知道网络协议真实应用的地方，另一方面你也可以通过上手使用云计算、容器、微服务来进一步加深对于协议的理解。
千里之行，始于足下。不管何时，我相信，扎实的功底和过硬的技术，都会是你职业发展的助力器。
希望这个专栏，不仅可以帮你理清繁杂的网络协议概念，帮你构建一个精准的网络协议知识框架，帮你在热门领域应用这些底层知识，更重要的是给你一种学习知识的方法和态度：看似最枯燥、最基础的东西往往具有最长久的生命力。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>开篇词_这一次，让我们一起来搞懂MySQL</title><link>https://artisanbox.github.io/1/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/47/</guid><description>你好，我是林晓斌，网名“丁奇”，欢迎加入我的专栏，和我一起开始MySQL学习之旅。我曾先后在百度和阿里任职，从事MySQL数据库方面的工作，一步步地从一个数据库小白成为MySQL内核开发人员。回想起来，从我第一次带着疑问翻MySQL的源码查到答案至今，已经有十个年头了。在这个过程中，走了不少弯路，但同时也收获了很多的知识和思考，希望能在这个专栏里分享给你。
记得刚开始接触MySQL，是我在百度贴吧做权限系统的时候。我们遇到了一个奇怪的问题，一个正常10毫秒就能完成的SQL查询请求偶尔要执行100多毫秒才结束。当时主管问我是什么原因，我其实也搞不清楚，就上网查答案，但怎么找都找不到，又脸皮薄不想说自己不知道，只好硬着头皮翻源码。后来遇到了越来越多的问题，也是类似的情景，所以我逐步养成了通过分析源码理解原理的习惯。
当时，我自己的感觉是，即使我只是一个开发工程师，只是MySQL的用户，在了解了一个个系统模块的原理后，再来使用它，感觉是完全不一样的。当在代码里写下一行数据库命令的时候，我就能想到它在数据库端将怎么执行，它的性能是怎么样的，怎样写能让我的应用程序访问数据库的性能最高。进一步，哪些数据处理让数据库系统来做性能会更好，哪些数据处理在缓存里做性能会更好，我心里也会更清楚。在建表和建索引的时候，我也会更有意识地为将来的查询优化做综合考虑，比如确定是否使用递增主键、主键的列怎样选择，等等。
但随后我又有了一个新的困惑，我觉得自己了解的MySQL知识点是零散的，没有形成网络。于是解决完一个问题后，很容易忘记。再碰到类似的问题，我又得再翻一次代码。
所幸在阿里工作的时候，我参与了阿里云关系型数据库服务内核的开发，并且负责开发开源分支AliSQL，让我对MySQL内核和源码有了更深层次的研究和理解。在服务内部客户和公有云客户的过程中，我有机会面对和解决足够多的问题，再通过手册进行系统的学习，算是比较坎坷地将MySQL的知识网络补了起来。
所以，在回顾这个过程的时候，我的第一个感受是，如果一开始就有一些从理论到实战的系统性指导，那该多好啊，也许我可以学习得更快些。
在极客时间团队跟我联系策划这个专栏的时候，我还是持怀疑态度的。为什么呢？现在不比当年了，犹记得十余年前，你使用MySQL的过程中碰到问题的话，基本上都只能到代码里去找答案，因为那时网上的资料太少了。
而近十年来，MySQL在中国广泛普及，技术分享文章可以说是浩如烟海。所以，现在要系统地介绍一遍MySQL的话，恐怕里面提及的大多数知识点，都可以在社区文章中找到。那么我们做这个专栏的意义在哪里，而它又凭什么可以收费呢？
直到收到极客时间团队的答复，我才开始对这个专栏“想做和可以做”的事情感觉清晰起来。数据库是一个综合系统，其背后是发展了几十年的数据库理论。同时，数据库系统也是一个应用系统，可能一个业务开发人员用了两三年MySQL，还未必清楚那些自己一直在用的“最佳实践”为什么是最佳的。
于是，我希望这个专栏能够帮助这样的一些开发者：他们正在使用MySQL，知道如何写出逻辑正确的SQL语句来实现业务目标，却不确定这个语句是不是最优的；他们听说了一些使用数据库的最佳实践，但是更想了解为什么这么做；他们使用的数据库偶尔会出问题，亟需了解如何更快速、更准确地定位问题，甚至自己解决问题……
在过去的七年里，我带过十几个应届毕业生，看着他们成长，要求他们原理先行，再实践验证。几年下来，他们的成长速度都很快，其中好几个毕业没两年就成为团队的骨干力量了。我也在社招的时候面试过很多有着不错的运维实践经验和能力的候选人，但都因为对数据库原理仅有一知半解的了解，而最终遗憾地没有通过面试。
因此，我希望这个专栏能够激发开发者对数据库原理的探索欲，从而更好地理解工作中遇到的问题，更能知道背后的为什么。所以我会选那些平时使用数据库时高频出现的知识，如事务、索引、锁等内容构成专栏的主线。这些主线上是一个个的知识点。每个点就是一个概念、一个机制或者一个原理说明。在每个说明之后，我会和你讨论一个实践相关的问题。
希望能以这样的方式，让你对MySQL的几条主线有一个整体的认识，并且了解基本概念。在之后的实践篇中，我会引用到这些主线的知识背景，并着力说明它们是怎样指导实践的。这样，你可以从点到线，再到面，形成自己的MySQL知识网络。
在这里，有一份目录，你也可以先了解下整个专栏的知识结构。
如前面说的，这几条主线上的每个知识点几乎都不是最新的，有些甚至十年前就这样，并没有改过。但我希望针对这些点的说明，可以让你在使用MySQL时心里更有底，知道怎么做选择，并且明白为什么。了解了原理，才能在实践中不断创新，提升个人的价值和工作输出。
从这里开始，跟我一起搞懂MySQL!
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } .</description></item><item><title>开篇词｜让我们来写一门计算机语言吧</title><link>https://artisanbox.github.io/3/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/46/</guid><description>你好，我是宫文学，一名技术创业者。
20多年前，我从北大毕业，搞过一些基础软件，也创过业，最近也一直在研究编译器、操作系统这样的底层技术。
我其实是极客时间的老面孔了，我曾在极客时间开过两门课，《编译原理之美》和《编译原理实战》。这两门课都聚焦于编译技术，一个是“读万卷书”，带你掌握编译原理；另一个是“行万里路”，教你怎么用编译技术。
今天我又设计了这门新课，带你来实现一门计算机语言。但是，你要知道，对于实现一门计算机语言而言，编译技术只是构成要素之一。它还有另外两块大的要素：一个是计算机语言的特性，包括类型系统、面向对象、函数式编程等；另一个是运行时技术，如虚拟机技术、内存管理等。
通过这门课你更好地把握计算机语言中涉及的各类技术的全貌，体会一下实现一门计算机语言的过程。
可能你还有点懵：我为啥要经历实现一门计算机语言的过程呢？这件事能帮我们提升哪些方面的能力或者成就感呢？接下来，我慢慢告诉你。
为什么要自己折腾一门编程语言？现在，每个程序员都熟悉一门或几门计算机语言，但是，我们很少有人想过自己去动手实现一门语言。又或者，虽然豪情万丈地计划过，却又因为种种原因不曾真正付诸实践。
当然，我们也会为自己找很多理由。
最常见的想法是，计算机语言已经很多了，我们会用就行，干嘛要自己去实现呢？另一个常见的想法是，计算机语言，我学习起来都挺不容易，要想去实现它，那更是难以逾越的吧？
这些顾虑看起来都很有说服力，可是，尽管如此，仍然有不少人还是会摆脱这些顾虑，去动手亲自实现一下计算机语言。为什么要做这样看上去很不理性的事情呢？我从我自身的体会来谈一下。
第一，实现一门计算机语言所带来的能力，是真的有用。
我们有时候觉得实现一门计算机语言这样的事情，纯属闲着没事干，因为要让一门计算机语言成功的机会太渺茫了，条件太苛刻了。
不过，在实现一门计算机语言的时候，你能接触到编译技术、运行时技术、汇编语言、硬件架构和各种算法，基本上是从顶层到底层把技术做穿。有了这些硬功夫，其实你已经能够胜任大多数高层次的软件开发工作了。比如，据我了解，Python语言的异步IO模块的开源贡献者，前一阵创业就去做一种创新的异步数据库产品了。
你一样也可以。从前你只会用人家写的东西，现在你要自己来实现了，肯定能更好地理解这套东西的核心逻辑，也能获得更多技术上的高维优势。最现实的就是你能拿下一个个难啃的技术难题，获得更多的晋升机会。
第二，不仅有用，而且这个过程真的很爽。
在我的前两门课里，有同学在实现了一个简单的脚本解释器之后，留言说“激动得浑身发抖”。是的，钻研一些比较深入的技术，会给人带来极大的成就感。特别是对于天天使用计算机语言的程序员来说，如果你有机会把计算机语言实现一遍，洞悉其中的技术秘密，那带来的成就感更是难以比拟的！
第三，像计算机语言这样的领域，更是大有前景。
如果你在关注中国技术发展，那你肯定知道我们目前正奋力在补基础技术方面的课，希望有朝一日也能拥有我们中国自己的优秀基础软件，比如HarmonyOS就在做这种尝试。而且，我们会看到中国涌现出越来越多的编程平台，很多产品会具备二次编程能力，甚至我们自己的计算机语言也会出现并逐步成熟。
要实现这样的突破，需要有更多具备底层编程能力的人才加入进来，要能够深刻理解程序在计算机硬件和操作系统之上运行的基础机制，以及计算机语言编译和运行所需要的技术。而学习如何实现一门计算机语言的过程，就能够带给你这些方面的提高。
现在，我相信你已经了解了，为什么你有必要掌握实现一门计算机语言所涉及的各种技术。不管仅仅是兴趣爱好的原因，还是为了自己的发展，甚至是为了在科技创新的趋势中弄潮，我都邀请你参与进来，一起来玩一玩这些技术。
而要开始实现一门计算机语言，我们首先就需要做一个决策：去实现一门什么样的语言？
我要带你实现什么样的语言？首先，我否决了去设计一门全新的语言。
因为设计一门语言真的很有难度，也是最容易引起争议的话题。而且，这项工作不仅是一项技术工作，还是一个产品设计工作，需要兼顾艺术性和用户体验，是个见仁见智的话题。
其次，我也不想像一些教科书那样，去实现一个玩具语言，这些语言往往不具备最后真正意义上的实用性。
经过几番思考，最终我选择去实现一门已经存在的语言：TypeScript。一门计算机语言其实可以有多个具体实现，像JavaScript就有V8（用于Chrome和Node.js）、TraceMonkey（用于FireFox）、QuickJS等多个不同的实现，每个实现都有不同的适用场景。而我要带你做的是，TypeScript的一个全新的实现。
TypeScript（以及JavaScript）的程序员群体相当庞大，并且它还具备编译成原生应用的潜力，所以HarmonyOS选择了TypeScript作为主力开发语言。
我前一阵参与发起了一个开源工业操作系统的项目。为顺应HarmonyOS的趋势，我也准备用TypeScript实现工业控制软件的开发。过去这些领域都是用C++做开发，其实用TS也完全可以。我甚至也跟JavaScript（ECMAScript）标准组的一名专家热烈讨论过，我们可以把HarmonyOS的基于TypeScript的前端开发工具扩展开来，用于支持安卓、IOS、桌面应用乃至小程序的开发，变成一个跨平台的开发工具，这也完全可以。
所以说，我们这门课选择TypeScript，是看好它未来有更大的发展空间。HarmonyOS已经开了个头，我们还可以做更多的探索。
而且，这门课的大部分内容，比如编译功能等，我们也是采用TypeScript来实现的。对于前端工程师来说，他们本身就很熟悉TypeScript。对于众多的后端工程师而言，由于TypeScript是静态类型的语言，所以他们上手起来也会很快。对于移动端的开发者而言，未来肯定需要了解在HarmonyOS上如何开发应用，所以熟悉一下TypeScript也是有必要的。
那接下来，我们看看在这门课里，我会带你完成哪些工作和挑战。
我会带着你完成什么挑战？总的来说，实现一门计算机语言，我们需要实现编译器、运行时，还要实现面向对象等各种语言特性。
具体一点，首先，我会带着你实现一个纯手写的编译器前端。
编译器前端指的是词法分析、语法分析和语义分析功能。我们目前使用的大多数计算机语言，比如Java、Go、JavaScript等，其编译器前端功能都是纯手写的，而不是采用工具生成的。我会带你了解那些被这些语言所广泛采用的最佳实践，比如LL算法、运算符优先级算法等等。
这种纯手写的实现方式，能让你最大程度体会相关算法的原理。另外，也非常有利于你根据自己的需要进行修改，来满足特定领域的需求。比如，我同学的公司有一个产品，支持在浏览器里编写代码，处理遥感数据。这样的需求，完全可以用TypeScript实现，再编译成JavaScript在浏览器里运行即可。
第二，我还会带你实现纯手写的编译器后端。
编译器后端指的是生成目标代码的功能，而目标代码呢，指的是字节码或汇编代码。编译器后端不仅要生成代码，还要对代码进行优化，尽量提升它的性能。
编译器后端的工作量通常更大，所以像Rust、Julia等新兴起的语言，往往采用一个后端工具，比如LLVM，而不是自己编写后端，这样可以节省大量的工作。不过这个方式也有缺陷，比如针对移动应用或者浏览器运行环境，在资源占用、即时编译速度等方面就不够理想。所以，像JVM、Android的运行时、V8等，都会自己去实现后端。
而且，如果你对语言的运行机制有特殊的要求，并且跟C/C++这些不同，那么你最好自己实现一个后端，比如Go语言就是这样。
编译器后端通常还包含大量的优化算法（有时候，我们把这些优化功能归为中端），这些优化算法具有比较强的工程性，所以教科书里的描述往往不够具体，也不能体现业界的一些最佳实践。在这门课里呢，我们可以自己动手去体会这些最佳实践，包括基于图的IR，以及一些优化算法，从而对优化算法的理解更加具象化。
在编译器后端里，因为我们还要生成汇编代码，所以能带你掌握汇编语言的精髓。在实现一些系统级软件的时候，我们有时候必须能够想象出来，这些软件的逻辑落实到汇编代码层面是什么样子的，这样才能确定最佳的技术策略。而破除对汇编代码的陌生感，是打通技术人员奇经八脉的重要一环。
第三，我还会带你实现多个运行时机制。
要让编译后的程序运行起来，我们必须要设计程序的运行机制。当然了，让一个程序跑起来的方法很多。在这门课里，我将带你实现多种运行时机制，让你能体会它们各自的设计思想，并能进行相互间的比较。
首先，我会带你实现一个AST解释器，也就是通过遍历AST的方式来运行程序。这种方式虽然简单，但很实用，对很多应用需求来说都够用了。
接着，我会把AST编译成字节码，在虚拟机上运行。像Java、JavaScript、Python等语言，都支持这种运行方式。在这个环节，我们会讲解栈机和寄存器机的差别，设计字节码，并实现一个栈机。
并且，我还会带你实现两个不同版本的虚拟机，一个是基于TypeScript实现的，一个是基于C语言实现的。当你采用C语言时，你对于运行时的一些实现细节拥有更多的掌控能力。你会看到，只有掌控了像内存分配这些技术细节，才能让基于C语言的虚拟机在性能上胜出。
当然，最后，我还会带你把程序编译成本地可执行文件来运行。在这个过程中，我最希望你能够彻底搞清楚，当一个编译成本地代码的程序在运行的时候，到底CPU、操作系统和计算机语言本身各自都扮演了什么角色。这是打通技术上的奇经八脉来说，是非常重要的一环。
你会发现，作为计算机语言的实现者，你其实拥有比自己想象中大得多的发挥空间。所以，当你实现像协程、JIT机制等高级特性的时候，就能够更好地设计或理解相应的技术方案。
对于一些技术细节，比如通过汇编代码做栈桢的管理，我们也会上手获得细致的理解。基于这些透彻的理解，你会有能力基于栈桢的机制来实现尾递归和尾调用的优化，从而让你增强对于物理机的运行机制的掌控感。
最重要的是，每实现一个运行时机制，我们都会进行性能的测试和比拼。这些真实的测试和数据，会让你对于运行时机制产生非常具象的感受。下面这张图就是在课程的某一讲中，我们集齐了5个版本的运行时进行对比测试的结果。更重要的是，这几个版本的运行时，你都可以自己动手做出来。
第四，我会带你理解一些高级语言的特性是如何实现的。
在实现了计算机语言的一些基本特性以后，我们会去讨论一些高级一点的话题，比如类型体系的实现；在支持面向对象时，如何用最小的代价实现运行时的多态特性；在支持函数编程特性时，又是如何实现高阶函数功能、闭包功能等。
而对象、闭包等特性，又不可避免地会引出运行时的内存管理问题，因此，我们也会实现一个自己的垃圾收集器。
我会怎么带你一步步实现？看着我前面大段大段的介绍，你觉得这些东西难吗？有编译，有运行时，还有一些更高级的语言特性，看上去还挺难吧？内容也很多，你可能心里已经开始打“退堂鼓”了：这么多内容，难度又不小，我能跟下来吗？
请打住！其实你根本不用担心。我在课程内容的设计上是逐步递进的，你会自然而然地跟着走下来，不会感觉有很大的学习困难。我会从原理出发，带你走完整个语言的实现过程，一方面能避免各种繁琐的编程工作，对你理解原理带来干扰；另一方面又能保留足够多的技术细节，让我们的教学语言具备足够的实用性。
哪怕你只学了几节课，你也能够掌握编译器前端的基础技能，实现一个AST解释器。再学几节课呢，就能搞出一个基于TypeScript的虚拟机出来。然后再加两节呢，又搞出一个C语言版本的虚拟机出来。不知不觉间，你就走出很远，爬得很高了。
在第一部分起步篇中，我会主要选取少量的语言特性，带你迅速实现从前到后的技术贯穿，这样你就能对计算机语言涉及的各项技术有一个全局性的了解。
这一部分又分成了三个阶段。在第一个阶段，我会带你用AST解释器把TypeScript跑起来，并在这个过程中带你掌握业界最常用的词法分析技术、语法分析技术和语义分析技术。在第二个阶段，我会升级解释运行的机制，带你掌握字节码技术和栈机。而在第三个阶段，我们就已经能够让程序编译成本地代码运行了！
紧接着在第二部分进阶篇呢，我会把这条路拓宽，也就是增加更丰富的语言特性，比如支持更多的数据类型、支持面向对象和函数式编程特性，等等。在这一部分，你能够丰富知识面，从而有能力解决更多的基础技术问题，其中就有内存管理这个关键技术。
学完进阶篇以后，你对实现一门计算机语言中所涉及的知识点，掌握得就比较全面了。剩下的知识点，通常只有专门从事这个领域工作的人或研究人员才会去涉足，这里面就包含编译优化技术。
每一门语言都会特别重视性能，而优化技术就是提升语言性能的关键。我还看到现实中一些做开发平台的项目中，真正的硬骨头往往就是优化技术。所以在最后在第三部分优化篇里，我就主要介绍一下优化技术。我会用比较浅显和直观的方式，让你了解Java、JavaScript等语言所采用的前沿优化技术，洞悉它们最深处的奥秘，让你有能力去承担那些攻坚性的任务。
更具体的详细目录你可以看看这个：
如果把实现一门计算机语言看成是一场冒险，那么现在我已经给你规划好了目标和路线，也会在路途中给你不断充实“武器”和“弹药”。
但俗语也有说，“兵马未动，粮草先行。”贴心的我还给你储备好了“衣物”和“粮草”，我给你备好了有着上万行的实验代码的代码库。而且，我们课中采用的技术，是基于我手头正在做的一门实用级语言为素材的，而且会作为开源项目一直进行版本迭代，所以你甚至可以拿这个开源项目作为自己工作的基础。当然了，我也无比欢迎你加入其中和我一起共建，为我们的“后勤保障”添砖加瓦。
好了，现在万事俱备，只欠东风。加入我吧，我已经迫不及待和你开启这一场计算机语言的冒险了！
欢迎点击链接加入交流群 ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>总结课_在实际开发中，如何权衡选择使用哪种数据结构和算法？</title><link>https://artisanbox.github.io/2/81/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/81/</guid><description>你好，我是王争，今天是一篇总结课。我们学了这么多数据结构和算法，在实际开发中，究竟该如何权衡选择使用哪种数据结构和算法呢？今天我们就来聊一聊这个问题，希望能帮你把学习带回实践中。
我一直强调，学习数据结构和算法，不要停留在学院派的思维中，只把算法当作应付面试、考试或者竞赛的花拳绣腿。作为软件开发工程师，我们要把数据结构和算法，应用到软件开发中，解决实际的开发问题。
不过，要想在实际的开发中，灵活、恰到好处地应用数据结构和算法，需要非常深厚的实战经验积累。尽管我在课程中，一直都结合实际的开发场景来讲解，希望带你真枪实弹地演练算法如何解决实际的问题。但是，在今后的软件开发中，你要面对的问题远比我讲的场景要复杂、多变、不确定。
要想游刃有余地解决今后你要面对的问题，光是熟知每种数据结构和算法的功能、特点、时间空间复杂度，还是不够的。毕竟工程上的问题不是算法题。算法题的背景、条件、限制都非常明确，我们只需要在规定的输入、输出下，找最优解就可以了。
而工程上的问题往往都比较开放，在选择数据结构和算法的时候，我们往往需要综合各种因素，比如编码难度、维护成本、数据特征、数据规模等，最终选择一个工程的最合适解，而非理论上的最优解。
为了让你能做到活学活用，在实际的软件开发中，不生搬硬套数据结构和算法，今天，我们就聊一聊，在实际的软件开发中，如何权衡各种因素，合理地选择使用哪种数据结构和算法？关于这个问题，我总结了六条经验。
1.时间、空间复杂度不能跟性能划等号我们在学习每种数据结构和算法的时候，都详细分析了算法的时间复杂度、空间复杂度，但是，在实际的软件开发中，复杂度不能与性能简单划等号，不能表示执行时间和内存消耗的确切数据量。为什么这么说呢？原因有下面几点。
复杂度不是执行时间和内存消耗的精确值
在用大O表示法表示复杂度的时候，我们会忽略掉低阶、常数、系数，只保留高阶，并且它的度量单位是语句的执行频度。每条语句的执行时间，并非是相同、确定的。所以，复杂度给出的只能是一个非精确量值的趋势。
代码的执行时间有时不跟时间复杂度成正比
我们常说，时间复杂度是O(nlogn)的算法，比时间复杂度是O(n^2)的算法，执行效率要高。这样说的一个前提是，算法处理的是大规模数据的情况。对于小规模数据的处理，算法的执行效率并不一定跟时间复杂度成正比，有时还会跟复杂度成反比。
对于处理不同问题的不同算法，其复杂度大小没有可比性
复杂度只能用来表征不同算法，在处理同样的问题，以及同样数据类型的情况下的性能表现。但是，对于不同的问题、不同的数据类型，不同算法之间的复杂度大小并没有可比性。
2.抛开数据规模谈数据结构和算法都是“耍流氓”在平时的开发中，在数据规模很小的情况下，普通算法和高级算法之间的性能差距会非常小。如果代码执行频率不高、又不是核心代码，这个时候，我们选择数据结构和算法的主要依据是，其是否简单、容易维护、容易实现。大部分情况下，我们直接用最简单的存储结构和最暴力的算法就可以了。
比如，对于长度在一百以内的字符串匹配，我们直接使用朴素的字符串匹配算法就够了。如果用KMP、BM这些更加高效的字符串匹配算法，实际上就大材小用了。因为这对于处理时间是毫秒量级敏感的系统来说，性能的提升并不大。相反，这些高级算法会徒增编码的难度，还容易产生bug。
3.结合数据特征和访问方式来选择数据结构面对实际的软件开发场景，当我们掌握了基础数据结构和算法之后，最考验能力的并不是数据结构和算法本身，而是对问题需求的挖掘、抽象、建模。如何将一个背景复杂、开放的问题，通过细致的观察、调研、假设，理清楚要处理数据的特征与访问方式，这才是解决问题的重点。只有理清楚了这些东西，我们才能将问题转化成合理的数据结构模型，进而找到满足需求的算法。
比如我们前面讲过，Trie树这种数据结构是一种非常高效的字符串匹配算法。但是，如果你要处理的数据，并没有太多的前缀重合，并且字符集很大，显然就不适合利用Trie树了。所以，在用Trie树之前，我们需要详细地分析数据的特点，甚至还要写些分析代码、测试代码，明确要处理的数据是否适合使用Trie树这种数据结构。
再比如，图的表示方式有很多种，邻接矩阵、邻接表、逆邻接表、二元组等等。你面对的场景应该用哪种方式来表示，具体还要看你的数据特征和访问方式。如果每个数据之间联系很少，对应到图中，就是一个稀疏图，就比较适合用邻接表来存储。相反，如果是稠密图，那就比较适合采用邻接矩阵来存储。
4.区别对待IO密集、内存密集和计算密集如果你要处理的数据存储在磁盘，比如数据库中。那代码的性能瓶颈有可能在磁盘IO，而并非算法本身。这个时候，你需要合理地选择数据存储格式和存取方式，减少磁盘IO的次数。
比如我们在递归那一节讲过最终推荐人的例子。你应该注意到了，当时我给出的代码尽管正确，但其实并不高效。如果某个用户是经过层层推荐才来注册的，那我们获取他的最终推荐人的时候，就需要多次访问数据库，性能显然就不高了。
不过，这个问题解决起来不难。我们知道，某个用户的最终推荐人一旦确定，就不会变动。所以，我们可以离线计算每个用户的最终推荐人，并且保存在表中的某个字段里。当我们要查看某个用户的最终推荐人的时候，访问一次数据库就可以获取到。
刚刚我们讲了数据存储在磁盘的情况，现在我们再来看下，数据存储在内存中的情况。如果你的数据是存储在内存中，那我们还需要考虑，代码是内存密集型的还是CPU密集型的。
所谓CPU密集型，简单点理解就是，代码执行效率的瓶颈主要在CPU执行的效率。我们从内存中读取一次数据，到CPU缓存或者寄存器之后，会进行多次频繁的CPU计算（比如加减乘除），CPU计算耗时占大部分。所以，在选择数据结构和算法的时候，要尽量减少逻辑计算的复杂度。比如，用位运算代替加减乘除运算等。
所谓内存密集型，简单点理解就是，代码执行效率的瓶颈在内存数据的存取。对于内存密集型的代码，计算操作都比较简单，比如，字符串比较操作，实际上就是内存密集型的。每次从内存中读取数据之后，我们只需要进行一次简单的比较操作。所以，内存数据的读取速度，是字符串比较操作的瓶颈。因此，在选择数据结构和算法的时候，需要考虑是否能减少数据的读取量，数据是否在内存中连续存储，是否能利用CPU缓存预读。
5.善用语言提供的类，避免重复造轮子实际上，对于大部分常用的数据结构和算法，编程语言都提供了现成的类和函数实现。比如，Java中的HashMap就是散列表的实现，TreeMap就是红黑树的实现等。在实际的软件开发中，除非有特殊的要求，我们都可以直接使用编程语言中提供的这些类或函数。
这些编程语言提供的类和函数，都是经过无数验证过的，不管是正确性、鲁棒性，都要超过你自己造的轮子。而且，你要知道，重复造轮子，并没有那么简单。你需要写大量的测试用例，并且考虑各种异常情况，还要团队能看懂、能维护。这显然是一个出力不讨好的事情。这也是很多高级的数据结构和算法，比如Trie树、跳表等，在工程中，并不经常被应用的原因。
但这并不代表，学习数据结构和算法是没用的。深入理解原理，有助于你能更好地应用这些编程语言提供的类和函数。能否深入理解所用工具、类的原理，这也是普通程序员跟技术专家的区别。
6.千万不要漫无目的地过度优化掌握了数据结构和算法这把锤子，不要看哪里都是钉子。比如，一段代码执行只需要0.01秒，你非得用一个非常复杂的算法或者数据结构，将其优化成0.005秒。即便你的算法再优秀，这种微小优化的意义也并不大。相反，对应的代码维护成本可能要高很多。
不过度优化并不代表，我们在软件开发的时候，可以不加思考地随意选择数据结构和算法。我们要学会估算。估算能力实际上也是一个非常重要的能力。我们不仅要对普通情况下的数据规模和性能压力做估算，还需要对异常以及将来一段时间内，可能达到的数据规模和性能压力做估算。这样，我们才能做到未雨绸缪，写出来的代码才能经久可用。
还有，当你真的要优化代码的时候，一定要先做Benchmark基准测试。这样才能避免你想当然地换了一个更高效的算法，但真实情况下，性能反倒下降了。
总结工程上的问题，远比课本上的要复杂。所以，我今天总结了六条经验，希望你能把数据结构和算法用在刀刃上，恰当地解决实际问题。
我们在利用数据结构和算法解决问题的时候，一定要先分析清楚问题的需求、限制、隐藏的特点等。只有搞清楚了这些，才能有针对性地选择恰当的数据结构和算法。这种灵活应用的实战能力，需要长期的刻意锻炼和积累。这是一个有经验的工程师和一个学院派的工程师的区别。
好了，今天的内容就到这里了。最后，我想听你谈一谈，你在实际开发选择数据结构和算法时，有什么感受和方法呢？
欢迎在留言区写下你的想法，也欢迎你把今天的文章分享给你的朋友，帮助他在数据结构和算法的实际运用中走得更远。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>打卡召集令_60天攻克数据结构与算法</title><link>https://artisanbox.github.io/2/69/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/69/</guid><description>你好，我是王争。
今年4月，专栏更新结束之后，我在专栏发布了一篇《数据结构与算法之美》学习指导手册》，在这篇文章里，我对专栏内容重新做了一次梳理，将整个专栏拆分成四个阶段，列出了每个阶段的核心知识点、标注了每个知识点的难易程度（E-Easy，M-Medium，H-Hard），并用 1-10 分说明其重要性。
但是，我发现，很多同学还是没能坚持下来，久而久之对学习算法失去了信心。
回想起来，写专栏之初，我就立下 Flag，要做一个跟国内外经典书籍不一样、可以长期影响一些人的专栏。所以，在专栏完结 9 个月后，我想再做一些事情。
为了带你彻底拿下“数据结构与算法”这座大山，我发起了“60 天攻克数据结构与算法”打卡行动，一起登顶！
下面是我为你精心规划的学习计划表：
活动时间：2019.11.25-2020.1.19
你将获得：
1.坚持 60 天，与 2000 位优秀的工程师一起，彼此激励，相互学习；
2.整个学习周期内，我会进行2次高质量的社群分享；
3.我会精心整理 4 张知识脑图，为你梳理每个阶段的学习重点，发布在专栏里；
4.我和极客时间准备了 20 万奖学金，给坚持下来的同学。
活动规则：
在下方申请进入活动打卡群，根据课表打卡，完成学习。
打卡要求：
1.每个阶段持续 2 周，每周仅需打卡 3 次，即视为完成该阶段的学习。
2.4个阶段（8 周）的学习，打卡总数仅需 30 次，即视为完成“60 天攻克数据结构与算法行动”。
3.为了让大家养成习惯，每日只计 1 次打卡，单日内多次打卡视为 1 次。
进入打卡群后，完成学习还有如下奖励：
第一阶段（第1-2周）：¥15 奖励金 第二阶段（第3-4周）：¥25 奖励金 第三阶段（第5-6周）：¥35 奖励金 四个阶段（第7-8周）：¥50 奖励金 （注：奖励金会以无门槛优惠券形式、分阶段进行发放，发放时间为每阶段结束后的 7 个工作日内。）
当然，优惠券只是对你的小小奖励。坚持 60 天，与 2000 位优秀的工程师一起，互相学习，彼此激励，彻底拿下数据结构与算法，我奉陪到底。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>打卡召集令_第一阶段知识总结</title><link>https://artisanbox.github.io/2/73/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/73/</guid><description>ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } ._2sjJGcOH_0 ._36ChpWj4_0 { margin-left: 0.</description></item><item><title>打卡召集令_第三阶段知识总结</title><link>https://artisanbox.github.io/2/71/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/71/</guid><description>ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } ._2sjJGcOH_0 ._36ChpWj4_0 { margin-left: 0.</description></item><item><title>打卡召集令_第二阶段知识总结</title><link>https://artisanbox.github.io/2/70/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/70/</guid><description>ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } ._2sjJGcOH_0 ._36ChpWj4_0 { margin-left: 0.</description></item><item><title>打卡召集令_第四阶段知识总结</title><link>https://artisanbox.github.io/2/72/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/72/</guid><description>ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } ._2sjJGcOH_0 ._36ChpWj4_0 { margin-left: 0.</description></item><item><title>春节7天练_Day1：数组和链表</title><link>https://artisanbox.github.io/2/62/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/62/</guid><description>你好，我是王争。首先祝你新年快乐！
专栏的正文部分已经结束，相信这半年的时间，你学到了很多，究竟学习成果怎样呢？
我整理了数据结构和算法中必知必会的30个代码实现，从今天开始，分7天发布出来，供你复习巩固所用。你可以每天花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。
除此之外，@Smallfly 同学还整理了一份配套的LeetCode练习题，你也可以一起练习一下。在此，我谨代表我本人对@Smallfly 表示感谢！
另外，我还为假期坚持学习的同学准备了丰厚的春节加油礼包。
2月5日-2月14日，只要在专栏文章下的留言区写下你的答案，参与答题，并且留言被精选，即可获得极客时间10元无门槛优惠券。
7篇中的所有题目，只要回答正确3道及以上，即可获得极客时间99元专栏通用阅码。
如果7天连续参与答题，并且每天的留言均被精选，还可额外获得极客时间价值365元的每日一课年度会员。
关于数组和链表的几个必知必会的代码实现数组 实现一个支持动态扩容的数组
实现一个大小固定的有序数组，支持动态增删改操作
实现两个有序数组合并为一个有序数组
链表 实现单链表、循环链表、双向链表，支持增删操作
实现单链表反转
实现两个有序的链表合并为一个有序链表
实现求链表的中间结点
对应的LeetCode练习题（@Smallfly 整理）数组 Three Sum（求三数之和） 英文版：https://leetcode.com/problems/3sum/
中文版：https://leetcode-cn.com/problems/3sum/
Majority Element（求众数） 英文版：https://leetcode.com/problems/majority-element/
中文版：https://leetcode-cn.com/problems/majority-element/
Missing Positive（求缺失的第一个正数） 英文版：https://leetcode.com/problems/first-missing-positive/
中文版：https://leetcode-cn.com/problems/first-missing-positive/
链表 Linked List Cycle I（环形链表） 英文版：https://leetcode.com/problems/linked-list-cycle/
中文版：https://leetcode-cn.com/problems/linked-list-cycle/
Merge k Sorted Lists（合并k个排序链表） 英文版：https://leetcode.com/problems/merge-k-sorted-lists/</description></item><item><title>春节7天练_Day2：栈、队列和递归</title><link>https://artisanbox.github.io/2/63/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/63/</guid><description>你好，我是王争。初二好！
为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的30个代码实现，分7天发布出来，供你复习巩固所用。今天是第二篇。
和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。
关于栈、队列和递归的几个必知必会的代码实现栈 用数组实现一个顺序栈
用链表实现一个链式栈
编程模拟实现一个浏览器的前进、后退功能
队列 用数组实现一个顺序队列
用链表实现一个链式队列
实现一个循环队列
递归 编程实现斐波那契数列求值f(n)=f(n-1)+f(n-2)
编程实现求阶乘n!
编程实现一组数据集合的全排列
对应的LeetCode练习题（@Smallfly 整理）栈 Valid Parentheses（有效的括号） 英文版：https://leetcode.com/problems/valid-parentheses/
中文版：https://leetcode-cn.com/problems/valid-parentheses/
Longest Valid Parentheses（最长有效的括号） 英文版：https://leetcode.com/problems/longest-valid-parentheses/
中文版：https://leetcode-cn.com/problems/longest-valid-parentheses/
Evaluate Reverse Polish Notatio（逆波兰表达式求值） 英文版：https://leetcode.com/problems/evaluate-reverse-polish-notation/
中文版：https://leetcode-cn.com/problems/evaluate-reverse-polish-notation/
队列 Design Circular Deque（设计一个双端队列） 英文版：https://leetcode.com/problems/design-circular-deque/
中文版：https://leetcode-cn.com/problems/design-circular-deque/
Sliding Window Maximum（滑动窗口最大值） 英文版：https://leetcode.com/problems/sliding-window-maximum/
中文版：https://leetcode-cn.com/problems/sliding-window-maximum/
递归 Climbing Stairs（爬楼梯） 英文版：https://leetcode.com/problems/climbing-stairs/
中文版：https://leetcode-cn.com/problems/climbing-stairs/</description></item><item><title>春节7天练_Day3：排序和二分查找</title><link>https://artisanbox.github.io/2/64/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/64/</guid><description>你好，我是王争。初三好！
为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的30个代码实现，分7天发布出来，供你复习巩固所用。今天是第三篇。
和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。
前两天的内容，是关于数组和链表、排序和二分查找的。如果你错过了，点击文末的“上一篇”，即可进入测试。
关于排序和二分查找的几个必知必会的代码实现排序 实现归并排序、快速排序、插入排序、冒泡排序、选择排序
编程实现O(n)时间复杂度内找到一组数据的第K大元素
二分查找 实现一个有序数组的二分查找算法
实现模糊二分查找算法（比如大于等于给定值的第一个元素）
对应的LeetCode练习题（@Smallfly 整理） Sqrt(x) （x 的平方根） 英文版：https://leetcode.com/problems/sqrtx/
中文版：https://leetcode-cn.com/problems/sqrtx/
做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。
祝你取得好成绩！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>春节7天练_Day4：散列表和字符串</title><link>https://artisanbox.github.io/2/65/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/65/</guid><description>你好，我是王争。初四好！
为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的30个代码实现，分7天发布出来，供你复习巩固所用。今天是第四篇。
和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。
前几天的内容。如果你错过了，点击文末的“上一篇”，即可进入测试。
关于散列表和字符串的4个必知必会的代码实现散列表 实现一个基于链表法解决冲突问题的散列表
实现一个LRU缓存淘汰算法
字符串 实现一个字符集，只包含a～z这26个英文字母的Trie树
实现朴素的字符串匹配算法
对应的LeetCode练习题（@Smallfly 整理）字符串 Reverse String （反转字符串） 英文版：https://leetcode.com/problems/reverse-string/
中文版：https://leetcode-cn.com/problems/reverse-string/
Reverse Words in a String（翻转字符串里的单词） 英文版：https://leetcode.com/problems/reverse-words-in-a-string/
中文版：https://leetcode-cn.com/problems/reverse-words-in-a-string/
String to Integer (atoi)（字符串转换整数 (atoi)） 英文版：https://leetcode.com/problems/string-to-integer-atoi/
中文版：https://leetcode-cn.com/problems/string-to-integer-atoi/
做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。
祝你取得好成绩！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>春节7天练_Day5：二叉树和堆</title><link>https://artisanbox.github.io/2/66/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/66/</guid><description>你好，我是王争。春节假期进入尾声了。你现在是否已经准备返回工作岗位了呢？今天更新的是测试题的第五篇，我们继续来复习。
关于二叉树和堆的7个必知必会的代码实现二叉树 实现一个二叉查找树，并且支持插入、删除、查找操作
实现查找二叉查找树中某个节点的后继、前驱节点
实现二叉树前、中、后序以及按层遍历
堆 实现一个小顶堆、大顶堆、优先级队列
实现堆排序
利用优先级队列合并K个有序数组
求一组动态数据集合的最大Top K
对应的LeetCode练习题（@Smallfly 整理） Invert Binary Tree（翻转二叉树） 英文版：https://leetcode.com/problems/invert-binary-tree/
中文版：https://leetcode-cn.com/problems/invert-binary-tree/
Maximum Depth of Binary Tree（二叉树的最大深度） 英文版：https://leetcode.com/problems/maximum-depth-of-binary-tree/
中文版：https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/
Validate Binary Search Tree（验证二叉查找树） 英文版：https://leetcode.com/problems/validate-binary-search-tree/
中文版：https://leetcode-cn.com/problems/validate-binary-search-tree/
Path Sum（路径总和） 英文版：https://leetcode.com/problems/path-sum/
中文版：https://leetcode-cn.com/problems/path-sum/
做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友。
祝你取得好成绩！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>春节7天练_Day6：图</title><link>https://artisanbox.github.io/2/67/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/67/</guid><description>你好，我是王争。初六好！
为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的30个代码实现，分7天发布出来，供你复习巩固所用。今天是第六篇。
和之前一样，你可以花一点时间，来手写这些必知必会的代码。写完之后，你可以根据结果，回到相应章节，有针对性地进行复习。做到这些，相信你会有不一样的收获。
关于图的几个必知必会的代码实现图 实现有向图、无向图、有权图、无权图的邻接矩阵和邻接表表示方法
实现图的深度优先搜索、广度优先搜索
实现Dijkstra算法、A*算法
实现拓扑排序的Kahn算法、DFS算法
对应的LeetCode练习题（@Smallfly 整理） Number of Islands（岛屿的个数） 英文版：https://leetcode.com/problems/number-of-islands/description/
中文版：https://leetcode-cn.com/problems/number-of-islands/description/
Valid Sudoku（有效的数独） 英文版：https://leetcode.com/problems/valid-sudoku/
中文版：https://leetcode-cn.com/problems/valid-sudoku/
做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。
祝你取得好成绩！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>春节7天练_Day7：贪心、分治、回溯和动态规划</title><link>https://artisanbox.github.io/2/68/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/68/</guid><description>你好，我是王争。今天是节后的第一个工作日，也是我们“春节七天练”的最后一篇。
几种算法思想必知必会的代码实现回溯 利用回溯算法求解八皇后问题
利用回溯算法求解0-1背包问题
分治 利用分治算法求一组数据的逆序对个数 动态规划 0-1背包问题
最小路径和（详细可看@Smallfly整理的 Minimum Path Sum）
编程实现莱文斯坦最短编辑距离
编程实现查找两个字符串的最长公共子序列
编程实现一个数据序列的最长递增子序列
对应的LeetCode练习题（@Smallfly 整理） Regular Expression Matching（正则表达式匹配） 英文版：https://leetcode.com/problems/regular-expression-matching/
中文版：https://leetcode-cn.com/problems/regular-expression-matching/
Minimum Path Sum（最小路径和） 英文版：https://leetcode.com/problems/minimum-path-sum/
中文版：https://leetcode-cn.com/problems/minimum-path-sum/
Coin Change （零钱兑换） 英文版：https://leetcode.com/problems/coin-change/
中文版：https://leetcode-cn.com/problems/coin-change/
Best Time to Buy and Sell Stock（买卖股票的最佳时机） 英文版：https://leetcode.com/problems/best-time-to-buy-and-sell-stock/
中文版：https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/
Maximum Product Subarray（乘积最大子序列） 英文版：https://leetcode.com/problems/maximum-product-subarray/
中文版：https://leetcode-cn.com/problems/maximum-product-subarray/
Triangle（三角形最小路径和） 英文版：https://leetcode.com/problems/triangle/
中文版：https://leetcode-cn.com/problems/triangle/
到此为止，七天的练习就结束了。这些题目都是我精选出来的，是基础数据结构和算法中最核心的内容。建议你一定要全部手写练习。如果一遍搞不定，你可以结合前面的章节，多看几遍，反复练习，直到能够全部搞定为止。</description></item><item><title>期中测试｜快来检验你在起步篇的学习成果吧</title><link>https://artisanbox.github.io/3/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/49/</guid><description>你好，我是宫文学。
不知不觉间，我们的课程已经更新过半了。前几天，我们也更新完了第一部分，也就是起步篇的内容。到这里，我其实已经带你完整地跑完了，实现一门计算机语言需要的全部流程了。不知道你学习得怎么样呀？不如做套题来检验一下吧？
趁着国庆假期，我根据我们第一部分起步篇里讲过的知识，给你出了20道选择题，你可以检验一下自己的学习成果。如果你有什么不理解的地方，欢迎在留言区留言，也可以直接来我们的微信交流群找我。
快点击下面的按钮开始测试吧，我期待着你满分的好消息。</description></item><item><title>期中考试_来赴一场100分的约定吧！</title><link>https://artisanbox.github.io/6/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/44/</guid><description>你好，我是宫文学。
时间过得真快，从8月14日课程上线，到现在已经有一个半月的时间了。这一个半月里，有很多同学反馈说，自己学了这个课程特别有收获，而运行同学们写的编译器，我也觉得很有成就感，当然了，有的时候，我也会比较焦虑，因为一遍遍改文稿和写示例程序都需要投入大量的精力，不夸张地说，有几次晚上做梦的内容，都与咱们的课程有关……
但是，我觉得把编译原理中，看似高不可攀的一个个知识点，变成一篇篇得到你们肯定的文章是一件十分有趣的事情。动手写示例程序时，也往往让我废寝忘食，比如，报表系统的示例程序就是在飞机和火车上写出来的，一边写，一边灵感不断涌现，那时，我先写了一个版本，后来又改成了基于向量计算的版本，因为总是想给你们呈现最优质的内容，所以一直在不断地思考，优化。
在准备算法篇的示例程序时，我也有了很多新的灵感，比如对于元编程的理念，我又有了一些创新的想法。这些内容，我会在课程的第三部分与你分享。
在互联网时代，廉价的快乐随处可得，而努力拼搏才能获得的乐趣，从来都只属于少数人。这门课的目标是让尽可能多的人，有机会享受这种乐趣，当我看到你们进入编译技术的美丽花园中徜徉流连，我的内心是十分欣慰的。
我相信你们是真心喜欢计算机技术，所以想要努力搞懂这个学科的基础原理。而且，你们还能够静下心来，真正坐下来动手尝试。
有的同学会跟“这个推导过程我看过去怎么不会无限递归啊？”这样的问题较劲，而我是很感动的，因为他知道不把手弄脏（get hands dirty），是学不会手艺的。
在别人觉得没有问题的地方提问，本身就需要一定的勇气。其实，那个问题不像表面上那么简单。我在19讲里花了很大的篇幅解答了这个问题。而从这个问题，可以引出很多问题，比如，有多个产生式的时候，到底该如何选择？深度优先和广度优先有什么区别？等等。
你可以把在学习过程中发现的这些问题看做是花园的入口，而不是障碍。对于在学习编译原理时遇到了困难的同学，我要说，你至少找到了一个入口。
从这个角度来说，通过这次期中考试的20道题目，你又获得了20个新的入口。我亲自出的这20道题目，可以让你对之前学过的内容查漏补缺。你有一周的时间去回顾内容，弥补不足。在你答题的过程中，分值其实是不重要的，能引起你的思考最为重要，这可能是你又一轮的认知迭代！
接下来，我们聊一个轻松的话题，国庆将至，如果你想趁机好好休息一下，不妨找一个小众的城市，远离人群。而且还可以尝试带着电脑，在古镇上，在流水边，在星空下，在一切美景的围绕下，安安静静地写个编译器。这种尝试，难道不是一件美事吗？
当然了，也欢迎你在留言区，将你去过的城市美景分享给大家，给我们的课程增添不一样的色彩。
最后，来挑战一下，开启你的期中考试之旅吧！
编辑角：答题不限次数，但分值以第一次为准，答题前三名，有惊喜哦。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>期中考试_这些编译原理知识，你都掌握了吗？</title><link>https://artisanbox.github.io/7/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/51/</guid><description>你好，我是宫文学。
到这里，我们的课程就已经更新一半了，今天，我们来进行一场期中考试。我出了一套测试题，共有5道判断题、5道多选题，满分100，核心考点都出自前面的“预备知识”模块和“真实编译器解析”模块。
我建议你来认真地做一下这套题目，检验一下自己的学习效果。答完题之后，你也可以回顾试卷的内容，对不太理解或答错的问题，进行深入思考和学习。如果有不明白的，欢迎随时在留言区提问，我会知无不言。还等什么？一起来做下这套题吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>期末测试_一套习题，测出你的掌握程度</title><link>https://artisanbox.github.io/8/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/32/</guid><description>你好，我是朱晓峰。
咱们课程的核心内容都已经更新完了，在临近告别前，我还给你准备了一份期末测试题，这套测试题共有12道单选题和8道多选题，满分100，核心考点都出自前面讲到的所有重要知识，希望可以帮助你进行一场自测。
另外，我还为你准备了一份结课问卷，希望听一听你对这门课的反馈。只要填写，就有机会获得一个手绘护腕垫或者是价值99元的课程阅码。欢迎你花1分钟时间填写一下，期待你的畅所欲言。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>期末答疑与总结_再次审视学习编译原理的作用</title><link>https://artisanbox.github.io/7/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/49/</guid><description>你好，我是宫文学。到这里，咱们这门课程的主要内容就要结束了。有的同学在学习课程的过程中呢，提出了他感兴趣的一些话题，而我自己也会有一些想讲的话题，这个我也会在后面，以加餐等方式再做一些补充。接下来，我还会给你出一套期末测试题，帮你检测自己在整个学习过程中的所学所得。
那么，在今天这一讲，我们就来做个期末答疑与总结。在这里，我挑选了同学们提出的几个有代表性的问题，给你解答一下，帮助你更好地了解和掌握本课程的知识内容。
问题1：学习了编译原理，对于我学习算法有什么帮助？ @无缘消受人间富贵：老师，想通过编译器学算法，单独学算法总是不知道有什么意义，每次都放弃，老师有什么建议吗？但是看到评论说用到的都是简单的数据结构，编译器用不到复杂的数据结构和算法？
针对这位同学提出的问题，我想谈一谈我对算法学习的感受。
前一阵，我在跟同事聊天时，提到了一个观点。我说，大部分的程序员，其实从来都没写过一个像样的算法。他们写的程序，都是把业务逻辑简单地翻译成代码。那么，如果一个公司写出来的软件全是这样的代码，就没有什么技术壁垒了，很容易被复制。
反之，一些优秀的软件，往往都是有几个核心的算法的。比如，对于项目管理软件，那么网络优化算法就很关键；对于字处理软件，那么字体渲染算法就很关键，当年方正的激光照排系统，就是以此为基础的；对于电子表格软件，公式功能和自动计算的算法很关键；对于视频会议系统，也必须掌握与音视频有关的核心算法。这样，因为有了算法的技术壁垒，很多软件就算是摆在你的面前，你也很难克隆它。
所以说，作为一名软件工程师，你就必须要有一定的算法素养，这样才能去挑战那些有难度的软件功能。而作为一个软件公司，其实要看看自己在算法上有多少积淀，这样才能构筑自己的技术壁垒。
那么，编译原理对于提升你的算法素养，能带来什么帮助呢？我给你梳理一下。
编译原理之所以硬核，也是因为它涉及了很多的算法。
在编译器前端，主要涉及到的算法有3个：
有限自动机构造算法：这是在讲词法分析时提到的。这个算法可以根据正则文法，自动生成有限自动机。它是正则表达式工具的基础，也是像grep等强大的Linux命令能够对字符串进行模式识别的核心技术。 LL算法：这是在讲自顶向下的语法分析时涉及的。根据上下文无关文法，LL算法能够自动生成自顶向下的语法分析器，中间还涉及对First和Follow集合的计算。 LR算法：这是在讲自底向上的语法分析时涉及的。根据上下文无关文法，LR算法能自动生成自底向上的语法分析器，中间还涉及到有限自动机的构造算法。 总的来说，编译器前端的算法都是判断某个文本是否符合某个文法规则，它对于各种文本处理工作都很有效。有些同学也在留言区里分享，他在做全文检索系统时就会用到上述算法，使得搜索引擎能更容易地理解用户的搜索请求。
在编译器后端，主要涉及到的算法也有3个：
指令选择算法； 寄存器分配算法； 指令重排序（指令调度）算法。 这三个算法也有共同点，它们都是寻找较优解或最优解，而且它们都是NP Complete（NP完全）的。简单地说，就是这类问题能够很容易验证一个解对不对（多项式时间内），但求解过程的效率却可能很低。对这类问题会采用各种方法求解。在讲解指令选择算法时，我介绍了贪婪策略和动态规划这两种不同的求解思路；而寄存器选择算法的图染色算法，则采用了一种启发式算法，这些都是求解NP完全问题的具体实践。
在日常工作中，我们其实也会有很多需要求较优解或最优解的需求。比如，在文本编辑软件中，需要把一个段落的文字分成多行。而如何分行，就需要用到一个这样的算法。再比如，当做一个报表软件，并且需要分页打印的时候，如何分页也是同类型的问题。
其他类似的需求还有很多。如果你没有求较优解或最优解的算法思路，对这样的问题就会束手无策。
而在编译器的中端部分，涉及的算法数量就更多了，但是由于这些算法都是对IR的各种分析和变换，所以IR采用不同的数据结构的时候，算法的实现也会不同。它不像前端和后端算法那样，在不同的编译器里都具有很高的一致性。
不过，IR基本上就是三种数据结构：树结构、图结构和基于CFG的指令列表。所以，这些算法会训练你处理树和图的能力，比如你可以在树和图中发现一些模式，以此对树和图进行变换，等等。这在你日常的很多编程工作中也是非常重要的，因为这两种数据结构是编程中最常使用的数据结构。
那么，总结起来，认真学好编译原理，一定会给你的算法素养带来不小的提升。
问题2：现代编程语言这么多，我们真的需要一门新语言吗？ @蓝士钦：前不久看到所谓的国产编程语言“木兰”被扒皮后，发现是Python套层壳，真的是很气愤。想要掌握编译原理设计一门自己的语言，但同时又有点迷茫，现代编程语言这么多，真的需要再多一门新语言吗？从人机交互的角度来看，任何语言都是语法糖。
关于是否需要一门新语言的话题，我也想跟你聊聊我自己的看法，主要有三个方面。当然，你也可以在此过程中思考一下，看看有没有什么跟我不同的见解，欢迎与我交流讨论。
第一，编程语言其实比我们日常看到的要多，很多的细分领域都需要自己的语言。
我们平常了解的都是一些广泛流行的通用编程语言，而进入到每个细分领域，其实都需要各自领域的语言。比如SaaS的鼻祖Salesforce，就设计了自己的Apex语言，用于开发商业应用。华为的实验室在研发方舟编译器之前，也曾经研发了一门语言Cm，服务于DSP芯片的研发。
第二，中国技术生态的健康发展，都需要有自己的语言。
每当出现一个新的技术生态的时候，总是有一门语言会成为这个技术生态的“脚本”，服务于这个技术生态。比如，C语言就是Unix系统的脚本语言；JavaScript、Java、PHP等等，本质上都是Web的脚本语言；而Objective-C和Swift显然是苹果设备的脚本语言；Android虽然一开始用了Java，但最近也在转成Kotlin，这样Google更容易掌控。
那么，从这个角度看，当中国逐步发展起自己的技术生态的时候，也一定会孕育出自己的语言。以移动计算生态而言，我们有全球最大的移动互联网用户群和最丰富的应用，手机的制造量也是全球最高的。而位于应用和硬件之间的应用开发平台，我们却没有话语权，这会使中国的移动互联网技术生态受到很大的掣肘。
我在第40讲，也已经分析过了，Android系统经过了很多年的演化，但技术上仍然有明显的短板，使得Android平台的使用体验始终赶不上苹果系统。为了弥补这些短板，各个互联网公司都付出了很大的成本，比如一些头部应用的核心功能采用了C/C++开发。
并且，Android系统的编译器，在支持新的硬件上也颇为保守和封闭，让中国厂商难以参与。这也是华为之所以要做方舟编译器的另一个原因。因为华为现在自研的芯片越来越多，要想充分发挥这些芯片的能力，就必须要对编译器有更大的话语权。方舟编译器的问世，也证明了我们其实是有技术能力的，可以比国外的厂商做得更好。既然如此，我们为什么要受别人的制约？华为方舟编译器团队其实也很渴望，在方舟编译器之后推出自己的语言。至于华为内部是否已经立项，这就不太清楚了，但我觉得这是顺理成章的事情。
另外，除了在移动端的开发上会受到很多掣肘，在云端其实也一样。比如说，Java是被大量后端开发的工程师们所掌握的语言，但现在Java是被Oracle掌控的。你现在使用Java的时候，可能已经多多少少感受到了一种不愉快。先不说Java8之后的收费政策，就说我们渴望的特性（如协程、泛型中支持基础数据类型等），一直没有被满足，就会感觉不爽。
我在讲到协程的时候，就指出Java语言目前支持协程其实是很别扭的一种状态，它都是一些第三方的实现，并没有官方的支持。而如果Java的技术生态是由我们主导，可能就不是这样了。因为我国互联网的并发用户数如此之多，我们对更好的并发特性其实是更关切的。到目前为止，像微信团队解决高并发的问题，是用C++加上自己开发的协程库才实现的。而对于很多没有如此强大的技术能力的公司来说，就只能凑合了。
第三，实现一款优秀的软件，一定会用到编译技术。
每一款软件，当发展到极致的时候，都会变得像一个开发平台。这也是《黑客与画家》的作者保罗·格雷厄姆（Paul Graham）表达的思维。他原来的意思是，每个软件写到最后，都会包含一个Lisp的变种。实际上，他所要表达的意思就跟我说的一样。
我前一段时间，在北京跟某公司的老总探讨一个优秀的行业应用软件。这个软件在上世纪90年代就被开发出来了，也被我国广泛采用。一方面它是一个应用软件，另一方面它本身也是一个开发平台。所以它可以经过定制，满足不同行业的需求。
但是，我们国内的软件行业的情况是，在去客户那里实施的时候，几乎总是要修改源代码，否则就不能满足用户的个性化需求。
很多软件公司想去克隆一下我刚才说的那套软件，结果都放弃了。除了有对领域模型理解的困难以外，缺少把一个应用软件做成软件开发平台的能力，是其中很大的一个障碍。
实际上，目前在很多领域都是这样。国外的软件就是摆在那里，但中国的工程师就是做不出自己的来。而对于编译技术的掌握和运用，就是能够提升国内软件水平的重要途径。
我在开头跟同事交流的时候，也提出了软件工程师技术水平修养提升的几个境界。其中一个境界，就是要能够利用编译技术，做出在更大范围内具有通用性的软件。如果你能达到这个境界，那么也一定有更大的发展空间。
问题3：如何判断某门语言是否适合利用LLVM作为后端？ @ヾ(◍°∇°◍)ﾉﾞ：老师，很多语言都声称使用LLVM提升性能，但是在Lua领域好像一直是LuaJIT无法超越？
这个问题涉及到了如何利用后端工具的问题，比较有代表性。
LLVM是一个通用的后端工具。在它诞生之初，首先是用于支持C/C++语言的。所以一门语言，在运行机制上越接近C/C++语言，用LLVM来做后端就越合适。
比如Rust用LLVM就很成功，因为Rust语言跟C/C++一样，它们的目标都是编写系统级的程序，支持各种丰富的基础数据类型，并且也都不需要有垃圾收集机制。
那么，如果换成Python呢？你应该记得，Python不会对基础数据类型进行细粒度的控制，不需要把整型区分成8位、16位、32位和64位的，它的整型计算可以支持任意长度。这种语义就跟C/C++的相差比较远，所以采用LLVM的收益相对就会小一些。
而对于JavaScript语言来说，浏览器的应用场景要求了编译速度要尽量地快，但在这方面LLVM并没有优势。像我们讲过的隐藏类（Shapes）和内联缓存（Inline Caching）这样的对JavaScript很重要的机制，LLVM也帮不上忙。所以，如果在项目时间比较紧张的情况下，你可以暂时拿LLVM顶一顶，Safari浏览器中的JavaScript引擎之前就这么干过。但是，要想达到最好的效果，你还是编写自己的后端更好一些。
那对于Lua语言，其实你也可以用这个思路来分析一下，是采用LLVM，还是自己写后端会更好一些。不过，由于Lua语言比较简单，所以实现后端的工作量应该也相对较小。
小结这一讲，我主要回答了几个比较宏观的问题，它们都涉及到了编译原理这门课的作用。
第一个问题，我是从提升算法素养的角度来展开介绍的。编译原理知识里面涉及了大量的算法，我把它总结成了三大类，每类都有自己的特点，希望能对你宏观把握它们有所帮助。
第二个问题，其实是这门课程的一条暗线。我并没有在课程里去情绪化地鼓吹，一定要有自己的编译器、自己的语言。我的方式其实是想做一点具体的事情，所以在第二个模块中，我带着你一起探究了现有语言的编译器都是怎么实现的，破除你对编译器的神秘感、距离感；在第三个模块，我们又一起探讨了一下实现一门语言中的那些关键技术点，比如垃圾收集、并行等，它们都是如何实现的。
在课程最后呢，我又带你了解了一下具有中国血统的方舟编译器。我想说的是，其实我们不但能做出编译器和语言来，而且可能会做得更好。虽然我们对方舟编译器的分析还没有做完，但通过分析它的技术思路，你应该或多或少地感受到了它的优秀。所以，针对“我们真的需要一门新语言吗”这个问题，我的回答是确定的。并且，即使你不去参与实现一门通用的语言，在实现自己领域的语言，以及把自己的软件做得更具通用性这点上，编译原理仍然能发挥巨大的作用，对你的职业生涯也会有切实的帮助。
好，请你继续给我留言吧，我们一起交流讨论。同时我也希望你能多多地分享，做一个知识的传播者。感谢你的阅读，我们下一讲再见。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>期末考试_“编译原理实战课”100分试卷等你来挑战！</title><link>https://artisanbox.github.io/7/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/50/</guid><description>你好，我是宫文学。
咱们课程到这里就算正式更新完了，在临近告别前，我还给你准备了一份期末考试题，这套试卷共有8道单选题和12道多选题，满分100，核心考点都出自前面讲到的所有重要知识，希望可以帮助你进行一场自测。
除此之外，我也很想知道你对这门课的建议，所以我也给你准备了一份问卷。欢迎你在问卷里聊一聊你的想法，也许就有机会获得礼物或者是课程阅码哦。
好了，话不多说，请你来做一做这套期末测试题吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>期末考试｜实现编程语言这些核心知识点，你掌握得咋样了？</title><link>https://artisanbox.github.io/3/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/48/</guid><description>你好，我是宫文学。
我们的课程已接近尾声，要学的内容我们都已经全部学完了。不知道你掌握得怎么样了呢？今天，我给你准备了20道选择题，满分100分，范围囊括我们这门课的众多核心知识，一起来挑战一下吧！
如果有什么不明白的，欢迎直接在留言区提问，也可以在交流群找我，期待你满分的好消息！
另外，我还给你准备了一份调查问卷，想听一下你对我这门课的看法和建议。题目不多，两分钟就可以填完，非常希望能看到你的反馈。</description></item><item><title>热点问题答疑_如何吃透7种真实的编译器？</title><link>https://artisanbox.github.io/7/52/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/52/</guid><description>你好，我是宫文学。
到这里，咱们就已经解析完7个编译器了。在这个过程中，你可能也积累了不少问题。所以今天这一讲，我就把其中有代表性的问题，给你具体分析一下。这样，能帮助你更好地掌握本课程的学习思路。
问题1：如何真正吃透课程中讲到的7种编译器？在课程中，我们是从解析实际编译器入手的。而每一个真实的编译器里面都包含了大量的实战技术和知识点，所以你在学习的时候，很容易在某个点被卡住。那第一个问题，我想先给你解答一下，“真实编译器解析篇”这个模块的学习方法。
我们知道，学习知识最好能找到一个比较缓的坡，让自己可以慢慢爬上去，而不是一下子面对一面高墙。那么对于研究真实编译器，这个缓坡是什么呢？
我的建议是，你可以把掌握一个具体的编译器的目标，分解成四个级别的任务，逐步提高难度，直到最后吃透。
第一个级别，就是听一听文稿，看一看文稿中给出的示例程序和源代码的链接就可以了。
这个级别最重要的目标是什么？是掌握我给你梳理出来的这个编译器的技术主线，掌握一张地图，这样你就能有一个宏观且直观的把握，并且能增强你对编译原理的核心基础知识点的认知，就可以了。
小提示：关于编译器的技术主线和知识地图，你可以期待一下在期中复习周中，即将发布的“划重点：7种编译器的核心概念和算法”内容。
在这个基础上，如果你还想再进一步，那么就可以挑战第二级的任务。
第二个级别，是要动手做实验。
你可以运行一下我给出的那些使用编译器的命令，打印输出调试信息，或者使用一下课程中提到的图形化工具。
比如，在Graal和V8编译器，你可以通过修改命令行的参数，观察生成的IR是什么样子。这样你就可以了解到，什么情况下才会触发即时编译、什么时候才会触发内联优化、生成的汇编代码是什么样子的，等等。
这样，通过动手做练习，你对这些编译器的认识就会更加具体，并且会有一种自己可以驾驭的感觉，赢得信心。
第三个级别，是看源代码，并跟踪源代码的运行过程，从而进入到编译器的内部，去解析一个编译器的真相。
完成这一级的任务，对你动手能力的要求更高。你最容易遇到的问题，是搭建一个调试环境。比如，调试Graal编译器要采用远程调试的模式，跟你调试一个普通应用还是不大一样的。而采用GDB、LLDB这样的工具，对很多同学来说可能也是一个挑战。
而且，你在编译源代码和调试的过程中也会遇到很多与配置有关的问题。比如，我用GDB来调试Julia和MySQL的时候，就发现最好是使用一个Linux虚拟机，因为macOS对GDB的支持不够好。
不过，上述困难都不是说真的有多难，而是需要你的耐心。遇到问题就解决问题，最终搭建出一个你能驾驭的环境，这个过程也会大大提升你的动手实践能力。
环境搭建好了，在跟踪程序执行的过程中，一样要需要耐心。你可能要跟踪执行很多步，才能梳理出程序的执行脉络和实现思路。我在课程中建议的那些断点的位置和梳理出来程序的入口，可以给你提供一些帮助。
可以说，只要你能做好第三级的工作，终归是能吃透编译器的运行机制的。这个时候，你其实已经差不多进入了高手的行列。比如，在实际编程工作中，当遇到一个特别棘手的问题的时候，你可以跟踪到编译器、虚拟机的内部实现机制上去定位和解决问题。
而我前面说了，掌握一个具体的编译器的目标，是有四个级别的任务。那你可能要问，都能剖析源代码了，还要进一步挑战什么呢？
这第四个级别呢，就是把代码跟编译原理和算法结合起来，实现认识的升华。
在第三级，当你阅读和跟踪程序执行的时候，会遇到一个认知上的挑战。对于某些程序，你每行代码都能看懂，但为什么这么写，你其实不明白。
像编译器这样的软件，在解决每一个关键问题的时候，肯定都是有理论和算法支撑的。这跟我们平常写一些应用程序不大一样，这些应用程序很少会涉及到比较深入的原理和算法。
我举个例子，在讲Java编译器中的语法分析器的时候，我提到几点。第一，它是用递归下降算法的；第二，它在避免左递归时，采用了经典的文法改写的方法；第三，在处理二元表达式时，采用了运算符优先级算法，它是一种简单的LR算法。
我提到的这三点中的每一点，都是一个编译原理的知识点或算法。如果对这些理论没有具体的了解，那你看代码的时候就看不出门道来。类似的例子还有很多。
所以，如果你其实在编译原理的基础理论和算法上都有不错的素养的话，你会直接带着自己的假设去代码里进行印证，这样你就会发现每段程序，其实都是有一个算法去对应的，这样你就真的做到融会贯通了。
那如何才能达到第四级的境界，如何才能理论和实践兼修且互相联系呢？
第一，你要掌握“预备知识”模块中的编译原理核心基础知识和算法。 第二，你要阅读相关的论文和设计文档。有一些论文是一些经典的、奠基性的论文。比如，在讲Sea of Nodes类型的IR的时候，我介绍了三篇重要的论文，需要你去看。还有一些论文或设计文档是针对某个编译器的具体的技术点的，这些论文对于你掌握该编译器的设计思路也很有帮助。 达到第四级的境界，你其实已经可以参与编译器的开发，并能成为该领域的技术专家了。针对某个具体的技术点加以研究和钻研，你也可以写出很有见地的论文。
当然，我不会要求每个同学都成为一个编译器的专家，因为这真的要投入大量的精力和实践。你可以根据自己的技术领域和发展规划，设定自己的目标。
我的建议是：
首先，每个同学肯定要完成第一级的目标。这一级目标的要求是能理解主线，有时候要多读几遍才行。 对于第二级目标，我建议你针对2~3门你感兴趣的语言，上手做一做实验。 对于第三级目标，我希望你能够针对1门语言，去做一下深入探索，找一找跟踪调试一个编译器、甚至修改编译器的源代码的感觉。 对于第四级目标，我希望你能够针对那些常见的编译原理算法，比如前端的词法分析、语法分析，能够在编译器里找到并理解它们的实现。至于那些更加深入的算法，可以作为延伸任务。 总的来说呢，“真实编译器”这个模块的课程内容，为你的学习提供了开放式的各种可能性。
好，接下来，我就针对同学们的提问和课程的思考题，来做一下解析。
问题2：多重分派是泛型实现吗？ @d：“多重分派能够根据方法参数的类型，确定其分派到哪个实现。它的优点是容易让同一个操作，扩展到支持不同的数据类型。”宫老师，多重分派是泛型实现吗？
由于大多数同学目前使用的语言，采用的都是面向对象的编程范式，所以会比较熟悉像这样的一种函数或方法派发的方式：
Mammal mammal = new Cow(); //Cow是Mammal的一个子类 mammal.speak(); 这是调用了mammal的一个方法：speak。那这个speak方法具体指的是哪个实现呢？根据面向对象的继承规则，这个方法可以是在Cow上定义的。如果Cow本身没有定义，就去它的父类中去逐级查找。所以，speak()具体采用哪个实现，是完全由mammal对象的类型来确定的。这就是单一分派。
我们认为，mammal对象实际上是speak方法的第一个参数，虽然在语法上，它并没有出现在参数列表中。而Java的运行时机制，也确实是这么实现的。你可以通过查看编译生成的字节码或汇编代码来验证一下。你如果在方法中使用“this”对象，那么实际上访问的是方法的0号参数来获取对象的引用或地址。
在采用单一分派的情况下，对于二元（或者更多元）的运算的实现是比较别扭的，比如下面的整型和浮点型相加的方法，你需要在整型和浮点型的对象中，分别去实现整型加浮点型，以及浮点型加整型的计算：
Integer a = 2; Float b = 3.1; a.add(b); //采用整型对象的add方法。 b.add(a); //采用浮点型对象的add方法。 但如果再增加新的类型怎么办呢？那么所有原有的类都要去修改，以便支持新的加法运算吗？
多重分派的情况，就不是仅仅由第一个参数来确定函数的实现了，而是会依赖多个参数的组合。这就能很优雅地解决上述问题。在增加新的数据类型的时候，你只需要增加新的函数即可。</description></item><item><title>特别加餐_我在2019年F8大会的两日见闻录</title><link>https://artisanbox.github.io/4/61/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/61/</guid><description>你好，我是徐文浩。4月30日，我在美国圣何塞参加了F8大会，趁此机会和你分享一下，我在大会上的一些见闻。下面是我参会这两天写的见闻录，分享给你。希望可以看到更多技术人走出去，抬头看看世界，丰富自己的见识和经历。
Day 1：“The Future is Private”今年是我连续第三年来F8了。如果说第一年是带着一点好奇和忐忑，作为一个开发者来看看世界上最大的社交网络的开发者大会是怎么回事儿，到了第二年，作为一个Developer Partner，看到自己公司的logo出现在首日的Keynote里，就觉得格外兴奋；那么今年第三年就有些轻车熟路了，没有什么压力，反而很想看一看，每年一次的开发者大会，还能办出什么新花样。
作为开发者，连续两年看到自己公司的logo出现在会场里还是很高兴的从旧金山国际机场出来，一路Uber到了圣何塞住下，不禁感慨，互联网和智能手机的确改变了世界。一个中国人到美国，拿着手机也能在这里生存下来了。
为了倒时差，我硬是熬到半夜睡了一觉。早早赶到圣何塞市中心的会场，发现已经有不少人在排队入场了。
去年的F8，因为Facebook面临“剑桥门事件”，主题的Keynote颇有些疲于应对的感觉。然而在过去的一年里，Facebook推出的种种隐私保护的功能，似乎并没有解决“隐私泄露”的问题，反而给人一种此起彼伏、应接不暇的感觉。
于是，今年的F8，Facebook颇有些破釜沉舟、不破不立之感。扎克伯格的开场Keynote，就表示，Facebook要开始在整个公司的运营策略上做出重大改变，打造一个“Privacy Focused Social Platform”，接着更是亮出了“The Future is Private”的slogan。
紧接着，扎克伯格介绍了Facebook这两年力推的产品Messenger。Messenger团队重写了整个手机客户端，让整个客户端小于30MB，冷启动时间少于1.3秒，默认端到端加密，并给它起了一个代号叫LightSpeed。
这几项指标都可以直接拿来，和自家被认为简单易用的WhatsApp做对比，而且明显胜出。为了服务更多Messenger的发达国家用户，Facebook更是干脆开发了一个桌面版的客户端。要知道，在这个移动端主宰一切的年代，还会投入精力开发桌面客户端的公司可不多了。可以看出，Facebook推动Messenger产品的决心。
WhatsApp产品更新介绍的核心也还是在隐私上。他们能够通过Messenger直接和WhatsApp联系人通信，这更是可以看出，Facebook迈出了打通旗下所有产品的第一步。
然而更重磅的还在后面。在介绍完Messenger和WhatsApp的产品更新之后，会场的大屏幕上打出了白底蓝字的“FB5”的logo。
作为Facebook最核心的产品，也是自己公司名字的Facebook，迎来了多年以来的第一次大型改版：App和Web端界面完全重写，产品中心从原先的信息流转向以Group为核心。“Groups at the heart”，传统蓝底白字的“f”字logo，也变成了有背景动图的“f”字logo了。
如果说其他App上的改动，还可以认为是Facebook的尝试或者探索，作为其主要收入来源的Facebook改版，恐怕是动真格的了。从一个开放信息流式的产品，变成一个以Group为核心的、有着私密性的产品，怕是多年以来Facebook这个App的另一次重大转变了。
之后的Instagram、Portal以及Spark VR的产品更新，都没有引起太多关注。Keynote的下一个爆点自然是Oculus。
Facebook是目前市场上唯一还在大力投入VR的大型厂商。这一次让人尖叫的就是Oculus Quest。这第一个“无线”的Oculus的确引人注目。当现场宣布所有参加F8的人将人手派发一个Oculus Quest，更是引来全场的掌声。大屏幕上，看着卡马克头戴Oculus挥舞光剑，更是让老程序员们回忆起，在DOS上玩“Wolfenstein 3D”的旧时光。
早上的Keynote结束之后，就是自由活动了。参会的工程师们可以选择去不同的会议室，听各种开发和产品相关的小讲座，也可以直接在主会场的各个“摊位”前，和Facebook的工程师沟通交流。通常如果提问的话，还会拿到背包、T恤、帽子这样的小奖品。
当然，排长队去体验Oculus是每年最热门的项目。你也可以在会场里面转悠，和其他开发者认识一下。免费的零食和饮料到处都是。与其说这是一个开发者大会，其实更像是一个Facebook生态圈的嘉年华。
F8的第一天，仍然是以Facebook自己的四大产品为核心的一个主题会议，并没有介绍太多AI和VR的黑科技。按照惯例，这些黑科技会在明天的Keynote呈现，值得期待。我印象比较深的是，今天在讲解Oculus Rift S的时候，介绍了Oculus Insight Position Tracking，不知道明天又会有什么新科技出现。
Day 2：科技改变世界第二天的Keynote仍然是在圣何塞市中心的McEnery会议中心举办。虽然Keynote要到10点开始，但是我住的公寓没有早餐，我和同事们还是8点刚过就跑到会场去“蹭饭”吃。
女性在科技界始终是“少数派”，所以Facebook特地在F8的第二天，在会场旁边的万豪酒店，举办了一个Women Breakfast的活动，邀请所有参与F8的女性，一起吃早餐，相互交流。
我们的一位产品经理也早早地去了会场参与这个活动。我想起，前一天的Keynote里，介绍4个核心产品的演讲者中，有3位都是女性，这让这个充满“科技感”的活动平添了一分人文的色彩。
免费早餐之后，大家的焦点又转移到了主会场。第二天的主题Keynote，不同于第一天以Facebook的产品为核心，而是集中在“技术”这个词上。
一般来说，第一天的Keynote关注的是最近这三五年来，Facebook的产品发展方向，那么第二天的Keynote的目标则放得更加长远，关注的是Facebook未来十年会关注和投入的技术。今年也毫不例外。Connectivity、机器学习、AR和VR，把整个会场带入了一个更有科技感的主题里去。
不过，今年的Keynote和往年的还是有点不一样。过去几年里，F8第二天的Keynote都显得更有“梦想”一些，比如通过无人机为经济不发达区域提供网络接入，研究怎么通过Reinforcement Learning让机器打《星际争霸》。
过去两年里，我们常常能看到一些或许挑战很大，但是却又容易让人憧憬的项目出现。而今年第二天的Keynote主题却和前一天环环相扣，专注在了“Responsible Innovation”这样一个主题上。
如何通过机器学习找出虚假账号，如何过滤仇恨言论，乃至如何解决网络霸凌，变成了一个个的机器学习案例，反复出现在今天的Keynote里。似乎Facebook是想更坚定地传达这样一个信息，“The Future is Private”，这件事情我们是认真的。
在整个Keynote的过程里面，也让大家看到了对于有害内容的过滤，从简单的关键词匹配进化到应用计算机视觉，直到今天使用的Nearest Neighbor Manifold Expansion &amp;amp; Multi-modal Understanding这样更加复杂的机器学习技术，一步一步是如何发展的。
Keynote结束之后，第二天的其他内容都安排得更加紧凑了一些。大部分的小会场都在午餐时间同时进行，内容也更加“硬核”。各个小会场里看到的不是产品更新，而是各种机器学习问题在Facebook的实际解决方法和应用。更有不少小会场里面，Facebook的工程师直接给大家展示了代码。有心想要了解一些特定问题的工程师，可以从这里面学到不少有用的东西。
除了“学习”之外，参加F8很重要的一个方面是社交。第二天的有些小会场是以座谈会的形式，邀请外部的开发者合作伙伴，来分享他们的成功案例。开发者之间的相互交流也更多了起来。
在所有内容结束之后，Facebook新加坡办公室的Partner Manager，带着我们移师会议中心附近的餐馆，开始了一个小小的After Party。我们一群来自五湖四海的华人，就在美国一边吃着墨西哥菜一边交流。“微信”是Facebook的开发者大会上始终绕不开的话题，Facebook自己的各类消息类产品，其实也一直在从微信里面汲取养分。
晚餐过后，今年的F8就算正式结束了。认识的新朋友，重新见面的老朋友，之后就又要各奔东西了。而我自己，打算在回程之前，跑一趟心心念念的计算机历史博物馆，去看看里面收藏的从ENIAC到现代计算机的经典型号，为今年的旅程画上一个完满的句号。
推荐阅读Facebook：全球最大社交网络，向未知转型</description></item><item><title>特别加餐_我的一天怎么过？</title><link>https://artisanbox.github.io/4/60/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/60/</guid><description>你好，我是徐文浩。专栏更新到50多篇，快要结束了。在进入实战篇之前，我想先和你分享一个专栏之外的话题，那就是我的一天是怎么过的。
为什么想写这篇文章呢？主要目的是“破除神话”。周围一些朋友说，你在创业很厉害；也有朋友说，你能写专栏很厉害。其实我觉得自己和大家一样，就是一个普普通通的工程师，每一天都是普通且忙碌的。同时，我也希望通过这篇文章，能够拉近和你的距离，在专栏快要完结之际，可以在未来和你有更多的交流。
作为一个工程师出身的创业者，很多人会好奇，我是不是还常常写代码？也有朋友看我一直出差，会问我现在主要精力是不是都在产品上了？还有，我究竟要花多少时间在写这个专栏上？
事实上，作为一个创业者，我很难给自己的工作划定个小小的范围，然后说，“看，这个就是我做的事情”。在公司里，我每天在做的，其实主要就是两件事情。一件事情，我称之为“让事情按次发生”，主要是规划和推动公司里想要做的事情，推动产品结合业务往前走。另一件事情，我称之为“面对问题，解决问题”，主要是给各种突发的、意料之外的问题找解决办法。
规划和推动产品的工作，往往时间安排上主动一些，我会尽可能找完整大块的时间来做。而解决问题的事情，往往就比较碎片化，只能时时响应处理。
很多学习专栏的同学，工作时间应该都不是非常久，还有不少属于自己的业余时间。对我来说，想有属于自己的时间，基本上是奢望了。特别是最近半年多时间，每天都要抽出时间来写专栏，睡眠时间都牺牲了不少。
当然，我和大部分同学以及其他专栏作者，在时间安排上，差异最大的一点是，我会比较频繁地去海外出差了。在国内的时候，我的时间安排通常还比较有规律，比如，下面是我最近在国内的一个周一。
1.周一一早9点刚到公司，我先会看看我们用作视频会议设备是否都连上了。虽然其实公司人还不多，但是因为主要是针对海外的业务，所以有马尼拉、曼谷、杭州、深圳四个办公地点，异地沟通成了一个很大的问题。通过发消息或者视频会议的方式，沟通效率仍然很低，所以我们干脆通过Facebook Portal群组聊天的方式，8小时“直播”各个办公室的情况。需要找另外一个办公室的同事的时候，对着视频会议的屏幕吼一声就是了。
2.9:45开始，我连续参加了两个小团队的站会。站会有对应负责的同学来主持推动，我主要是多听一听，大家是否遇到什么问题，以及需要什么样的支持。这里面的问题，可能来自内部的其他团队，也可能是需要问外部的客户、Facebook、合作方的各种问题。这一天很顺利，事情团队自运转就继续正常推进了我们的产品进度。
3.因为是周一，所以10:00开始，我会和各个团队的负责人开一个非业务内容的周会。因为最近在推动公司内部做好跨团队职责的协同，所以最近的重点是在做两件事情。一个是从后端的研发团队开始推进强流程的代码审核，目标是提升代码质量和长期的迭代速度。第二个是培养整个系统里各个非功能模块的首要负责人，主要是要把从云服务器管理、CDN、网络、监测等等非功能性的需求和职责划分给到更多不同的工程师，让他们各自负责之后，再做学习分享。这样可以让大家对整个系统的全貌有个了解，而不是只是把这些问题放在一两个资深的技术同学身上。
这一天里，我发现代码审核进展很慢，主要是大家都还是觉得这样会影响进度，但是我内心深处知道不是这么回事儿，因为从开始要做这个事情已经两三周过去了。所以，我就不再是“建议”，而是“强迫”团队开始做代码审核了。各种非功能性的“负责人”的分配倒是相对比较顺利。
4.我们通常开会都很短，三个会开完，也就是10:30这样子。不过因为是周一，所以接下来的主要时间还是在清理邮件。这里面既有来自外部客户和合作伙伴的问题，也有系统自动生成的各种报告。能直接回复的都会直接回复掉，不能直接回复的我会加到Microsoft TO-DO里面，作为待办事项列表。
5.基本上把邮件清理完了，也就到了中午。我一般不叫外卖，而是和同事们一起出门觅食。因为大部分时间都是在办公室里坐着，运动也少，所以除非是暴雨天气，我一定是要出去走动走动的。和不同的同事吃饭，聊两句生活，互相之间的距离也能拉近不少。
6.吃完午饭，我自己的常备节目是去买杯瑞幸或者全家的咖啡。通常也有不少同事会一起过去，不管买还是不买，都要溜个弯儿。我自己最近有点睡得少，不靠咖啡下午就会犯困。
7.之后回到办公室，想要开始写点代码。因为团队越来越大，所以现在我已经不写任何“必须要写”的代码了，避免自己的时间安排成为发布计划的瓶颈。不过，我还是尽可能会抽一些时间来写一点效率提升的代码。这天要写的，是答应了团队，把自动化滚动部署（Auto Rolling Update）的脚本给写了。不过，还没写多少，我们的产品经理YC就来找我一起和团队过新的OMS（订单管理系统）的产品评审。虽然作为程序员被打断总是会觉得很头疼，不过该过的事情还是要过。
8.等到产品评审走完，终于又有了点儿时间，重新开始写滚动部署的脚本。脚本写起来方便，测试起来却是非常麻烦，要频繁地开关虚拟机去做检查，也没有什么太好的办法做单元测试或者自动化测试。前前后后几个小时下去，终于把整个脚本调通。不过，我又在JIRA里面记了一串新的想法，主要是想要进一步把目前手动在云平台上创建负载均衡，后端服务的手工工作都自动化掉。
9.抬头一看，已经快晚上9点了，其实已经过了饭点儿了。办公室里也空了大半，于是干脆收拾好包出门吃饭回家。
10.回家刷了一会儿抖音，重新打开电脑，开始写专栏。专栏的工作量比想象中大不少，基本上写到12点、1点，除非已经是死线了。不然即使进度比想象中慢一点，我也会先去睡了，不然第二天效率更差。毕竟，明天我们又要开始创造明天么。
这就是我上周的一天，不知道和你想象中差别大吗？下次有机会，我会再写写我在海外出差的一天是什么过的。
最后，我想听你讲讲，你的一天是怎么过的呢？欢迎在留言区和同学们一起分享。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>特别发送（一）_经典面试题讲解第一弹</title><link>https://artisanbox.github.io/8/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/33/</guid><description>你好，我是朱晓峰。
到这里，“实践篇”的内容咱们就学完了。今天，我们来学点儿不一样的——5道经典面试题。这些都是在实际面试中的原题，当然，我没有完全照搬，而是结合咱们课程的具体情况，有针对性地进行了调整。我不仅会给你提供答案，还会和你一起分析，让你能够灵活地吃透这些题目，并能举一反三。
话不多说，我们现在开始。我先带你从一道简单的关于“索引”的面试题入手，索引在面试题里经常出现，来看看这一道你能不能做对。
第一题下面关于索引的描述，正确的是：
建立索引的主要目的是减少冗余数据，使数据表占用更少的空间，并且提高查询的速度 一个表上可以建立一个或者多个索引 组合索引可以有效提高查询的速度，比单字段索引更高效，所以，我们应该创建一个由所有的字段组成的组合索引，这样就可以解决所有问题了 因为索引可以提高查询效率，所以索引建得越多越好 解析：这道题的正确答案是选项2，我们来分析一下其他选项。
选项1说对了一半，索引可以提高查询效率，但是创建索引不能减少冗余数据，而且索引还要占用额外的存储空间，所以选项1不对。 选项3不对的原因有2个。第一，组合索引不一定比单字段索引高效，因为组合索引的字段是有序的，遵循左对齐的原则。如果查询的筛选条件不包含组合索引中最左边的字段，那么组合索引就完全不能用。第二，创建索引也是有成本的，需要占用额外的存储空间。用所有的字段创建组合索引的存储成本比较高，而且利用率比较低，完全用上的可能性几乎不存在，所以很少有人会这样做。而且一旦更改任何一个字段的数据，就必须要改索引，这样操作成本也比较高。 选项4错误，因为索引有成本，很少作为筛选条件的字段，没有必要创建索引。 如果这道题你回答错了，一定要回去复习下第11讲的内容。
第二题假设我们有这样一份学生成绩单，所有同学的成绩都各不相同，请编写一个简单的SQL语句，查询分数排在第三名的同学的成绩：
解析：这道题考查的是我们对查询语句的掌握情况。针对题目中的场景，可以分两步来进行查询。
第一步，按照成绩高低进行排序：
mysql&amp;gt; SELECT * -&amp;gt; FROM demo.test1 -&amp;gt; ORDER BY score DESC; -- DESC表示降序排列 +----+------+-------+ | id | name | score | +----+------+-------+ | 2 | 李四 | 90.00 | | 4 | 赵六 | 88.00 | | 1 | 张三 | 80.00 | | 3 | 王五 | 76.00 | | 5 | 孙七 | 67.</description></item><item><title>特别放送（三）_MySQL8都有哪些新特征？</title><link>https://artisanbox.github.io/8/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/35/</guid><description>你好，我是朱晓峰。今天，我来和你聊一聊MySQL 8的新特征。
作为应用最广泛的三大关系型数据库之一，MySQL的背后有一个强大的开发团队，使MySQL能够持续迭代和创新，满足不断变化的用户需求。在MySQL 8中，就有很多新特征。
今天，我就给你介绍两个重要的新特征：窗口函数和公用表表达式（Common Table Expressions，简称CTE）。它们可以帮助我们用相对简单的查询语句，实现更加强大的查询功能。
什么是窗口函数？窗口函数的作用类似于在查询中对数据进行分组，不同的是，分组操作会把分组的结果聚合成一条记录，而窗口函数是将结果置于每一条数据记录中。一会儿我会借助一个例子来对比下，在此之前，你要先掌握窗口函数的语法结构。
窗口函数的语法结构是：
函数 OVER（[PARTITION BY 字段]） 或者是：
函数 OVER 窗口名 … WINDOW 窗口名 AS （[PARTITION BY 字段名]） 现在，我借助一个小例子来解释一下窗口函数的用法。
假设我现在有这样一个数据表，它显示了某购物网站在每个城市每个区的销售额：
mysql&amp;gt; SELECT * FROM demo.test1; +----+------+--------+------------+ | id | city | county | salesvalue | +----+------+--------+------------+ | 1 | 北京 | 海淀 | 10.00 | | 2 | 北京 | 朝阳 | 20.00 | | 3 | 上海 | 黄埔 | 30.00 | | 4 | 上海 | 长宁 | 10.</description></item><item><title>特别放送（二）_经典面试题讲解第二弹</title><link>https://artisanbox.github.io/8/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/34/</guid><description>你好，我是朱晓峰。
到这里，“进阶篇”的内容咱们就学完了。今天，我给你准备了7道面试题。这些面试题涵盖了这个模块的核心内容，我们一起借助面试题来复习一下。我不仅会给你提供正确答案，还会带你深入分析这些问题，让你真正能够做到举一反三。
好了，话不多说，我们现在开始。
第一题日志文件对数据库的故障恢复至关重要。下面这些关于日志文件的描述，正确的是：
MySQL日志记录的顺序可以与并发操作的执行顺序不一致 为了确保数据库是可恢复的，必须确保先写数据库后写日志 日志文件中检查点的主要作用是提高系统出现故障后恢复的效率 系统故障恢复必须使用日志文件以保证数据库系统重启时能正常恢复，事务故障恢复不一定需要使用日志文件 答案：选项3。
解析：
选项1是错误的。MySQL日志记录的顺序是严格按照操作执行的顺序记录的，不会出现日志记录的顺序与并发操作执行的顺序不一致的情况。
选项2也是错误的。MySQL的日志系统遵循WAL（Write-Ahead Logging）机制，也就是所谓的先写日志，后写数据库的机制。由于记录日志是顺序写入磁盘，而写入数据库的磁盘操作需要对磁头定位，因而写入日志的速度要远比写入数据快。为了提高执行的效率，MySQL执行的是先写日志，日志写入成功之后，就回复客户端操作成功，对数据库的磁盘写入则在之后的某个阶段执行。这样，即便遇到系统故障，由于有了日志记录，就可以通过日志对数据库进行恢复了。WAL机制包括3个规则：
对数据的修改操作必须在写入数据库之前写入到日志文件中； 日志必须严格按序记录，就是说，如果A操作发生在B操作之前，那么在日志中，A操作的记录必须在B操作的记录之前； 事务提交之后，必须在日志写入成功之后，才能回复事务处理成功。 选项3是正确的，检查点的作用是加快数据恢复的效率。当修改数据时，为了提高存取效率，采用的策略是先记录日志，实际的数据修改则发生在内存中，这些数据修改是没有写入数据库的，叫做脏页。MySQL把这些脏页分成小的批次，逐步写入磁盘中。因为如果把所有的脏页都一次性写入磁盘，会导致磁盘写入时间过长，影响到用户SQL操作的执行。检查点的作用就是标记哪些脏页已经写入磁盘。这样，当遇到故障时，MySQL可以从检查点的位置开始，执行日志记录的SQL操作，而不是把整个日志都检查一遍，所以，大大提高了故障恢复的效率。
选项4也是错误的。系统故障恢复必须使用日志文件以保证数据库系统重启时能正常恢复，这个表述是正确的，但后面的表述“事务故障恢复不一定需要使用日志文件”则是错误的。事务故障的恢复也必须要用到日志文件，事务故障恢复需要用到的日志文件有3个，分别是回滚日志、重做日志和二进制日志。
回滚日志：如果事务发生故障，可以借助回滚日志，恢复到故障前的状态，所以回滚日志对事务故障恢复有用。 重做日志：事务中的操作对数据更新进行提交时，会记录到重做日志，对数据的更新则只会发生在内存中，实际的数据更新写入磁盘，则会由后台的其他进程异步执行。如果这个时候事务故障，内存中的数据丢失，就必须要借助重做日志来找回。所以，重做日志对事务故障恢复有用。 二进制日志：在主从服务器架构的模式下，从服务器完全依赖二进制日志同步主服务器的操作，如果事务发生故障，从服务器只能依靠主服务器的二进制日志恢复。 第二题MySQL支持事务处理吗？
参考答案：
这个问题跟数据表用的是什么存储引擎有关。如果用的是Innodb，则支持事务处理；如果用的是MyISAM，则不支持事务处理。
MySQL 8.0 默认的存储引擎是Innodb，Innodb是支持事务处理的。在默认的情况下，MySQL启用AUTOCOMMIT模式，也就是每一个SQL操作都是一个事务操作，如果操作执行没有返回错误，MySQL会提交事务；如果操作返回错误，MySQL会执行事务回滚。
你也可以通过执行“START TRANSACTION”或者“BEGIN”来开始一个事务，这种情况下，就可以在事务处理中包含多个SQL操作语句，一直到“COMMIT”语句提交事务，或者是“ROLLBACK”语句回滚事务，来结束一个事务操作。
MyISAM存储引擎是不支持事务操作的，如果你对一个存储引擎是MyISAM的数据表执行事务操作，不管你是否执行“COMMIT”或者是“ROLLBACK”，都不会影响操作的结果。你可以通过下面的SQL语句来查看表的存储引擎：
mysql&amp;gt; SHOW CREATE TABLE demo.test; +-------+------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +-------+------------------------------------------------------------------------------------------------------------------+ | test | CREATE TABLE `test` ( `aa` int DEFAULT NULL ) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci | +-------+------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.02 sec) 第三题下面这些关于MySQL视图的描述中，错误的是：</description></item><item><title>特别放送（四）_位置信息：如何进行空间定位？</title><link>https://artisanbox.github.io/8/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/36/</guid><description>你好，我是朱晓峰。今天，我来和你聊一聊怎么进行空间定位。
我们每天都会用到空间数据，比如你在网上购买一件商品，你手机上的App就能够算出你是不是需要负担运费，负担多少运费。这其实就是因为手机获取到了你的空间位置信息，发送到网购平台，然后根据你所在的位置是否属于偏远地区，来决定你是否需要负担运费，如果需要的话，应该负担多少。
而从应用开发者的角度出发，我们需要知道怎么进行空间定位，获取用户的空间位置信息，以及如何计算发货点与客户地址的距离，这些都要借助与空间数据相关的技术才能解决。
今天，我还是借助一个真实的项目，来给你介绍下空间数据类型、空间数据处理函数，以及如何为空间数据创建索引，帮助你提升开发基于空间数据应用的能力。
在我们超市项目的实施过程中，超市经营者提出了这样一个要求：要给距离门店5公里范围内的、从线上下单的客户提供送货上门的服务。要想解决这个问题，就需要用到空间数据了。
空间数据类型与空间函数我先给你介绍下空间数据类型和空间函数。
MySQL支持的空间数据类型分为2类：
一类是包含单个值的几何类型（GEOMETRY）、点类型（POINT）、线类型（LINESTRINIG）和多边形类型（POLYGON）； 另一类是包含多个值的多点类型（MULTIPOINT）、多线类型（MULTILINESTRING）、多多边形类型（MULTIPOLYGON）和几何集类型（GEOMETRYCOLLECTION）。 我简单说明一下这几种空间数据类型的特点。
几何类型是一个通用的空间数据类型，你可以把点类型、线类型和多边形类型数据的值赋予几何类型数据。但是点类型、线类型和多边形类型数据则不具备这种通用性，你只能赋予它们各自类型数据的值。
几何集类型数据可以保存点类型数据、线类型数据和多边形类型数据值的集合。多点类型、多线类型和多多边形类型则分别只能保存点类型数据、线类型数据和多边形类型数据值的集合。
下面我们重点介绍一下点类型，因为这种类型是最简单、最基础的空间类型，也最常用。
点类型（POINT）点类型是最简单的空间数据类型，代表了坐标空间中的单个位置。在不同比例尺的坐标空间中，一个点可以有不同的含义。例如，在较大比例尺的世界地图中，一个点可能代表一座城市；而在较小比例尺的城市地图中，一个点可能只代表一个车站。
点类型数据的属性有2种：
坐标空间中的X轴的值（比如地理坐标中的经度值）； 坐标空间中的Y轴的值（比如地理坐标中的纬度值）。 点类型数据的维度是0，边界为空。
空间函数MySQL支持的空间函数有一百多种，我们没有必要全部都掌握。所以，我给你重点介绍几个比较常用的空间函数ST_Distance_Sphere()、MBRContains()、MBRWithin()和ST_GeomFromText()。
1.ST_Distance_Sphere()函数
我们先从计算空间距离的函数ST_Distance_Sphere()说起，这个函数的语法结构和功能如下所示：
ST_Distance_Sphere(g1,g2)：g1与g2为2个点，函数返回球体上2个点g1与g2之间的最小球面距离。 2.MBRContains()和MBRWithin()函数
在学习MBRContains()和MBRWithin()函数之前，我们要先了解一个概念，也就是最小边界矩形（MBR，Minimum Bounding Rectangle ）。
最小边界矩形是指以二维坐标表示的若干二维形状（例如点、直线、多边形）的最大范围，即以给定的二维形状各顶点中的最大横坐标、最小横坐标、最大纵坐标、最小纵坐标决定的边界的矩形。
知道了这个概念，你就能更好地理解这两个函数了。
MBRContains(g1,g2)：如果几何图形g1的最小边界矩形包含了几何图形g2的最小边界矩形，则返回1，否则返回0。 MBRWithin(g1,g2)：与MBRContains(g1,g2)函数正好相反，MBRWithin(g1,g2)表示，如果几何图形g1的最小边界矩形，包含在几何图形g2的最小边界矩形之内，则返回1，否则返回0。 3.ST_GeomFromText()
这个函数的作用是通过WKT形式创建几何图形。而ST_GeomFromText(WKT,SRID)就表示，返回用WKT形式和SRID指定的参照系表达的几何图形。
这里的WKT是一种文本标记语言，用来表示几何对象。SRID（Spatial Reference Identifier）是空间参照标识符，默认是0，表示平面坐标系。我们平时常用的SRID是4326，是目前世界通用的以地球质心为原点的地心坐标系。
知道了这些基础知识，我们就可以着手解决超市经营者提出的需求了。
这家超市有很多门店，该怎么计算是否应该送货上门呢？如果应该送货上门，应该从哪家门店送货呢？我带你分析下具体的思路。
第一步，把门店的位置信息录入数据表中； 第二步，根据下单客户的送货地址，获取到地理位置信息； 第三步，计算各门店位置与送货地址的距离，找出最近的门店安排送货，如果没有一家门店与客户的距离在5公里以内，则提示不能送货。 下面我们就来实际操作一下。
首先，我们创建一个门店表（demo.mybranch），包含门店编号、名称、位置等信息。
mysql&amp;gt; CREATE TABLE demo.mybranch -&amp;gt; ( -&amp;gt; branchid SMALLINT PRIMARY KEY, -&amp;gt; branchname VARCHAR(50) NOT NULL, -&amp;gt; address GEOMETRY NOT NULL SRID 4326 -&amp;gt; ); Query OK, 0 rows affected (0.</description></item><item><title>环境准备_带你安装MySQL和图形化管理工具Workbench</title><link>https://artisanbox.github.io/8/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/29/</guid><description>你好，我是朱晓峰。这节课，我来手把手带你安装和配置MySQL。
俗话说，“巧妇难为无米之炊”，我们不能凭空学习理论知识，必须要在实际的MySQL环境中进行操作，这是一切操作的基础。同时，我还会带你安装MySQL自带的图形化管理工具Workbench，我们之后要学习的表的关联、聚合、事务，以及存储过程等，都会用到它。
我会借助图文和音频给你介绍知识重点和操作要领，同时，我还录制了相应的视频，来展示具体的操作细节。你可以根据自己的习惯和需求，选择喜欢的形式来学习。
好了，话不多说，我们先来安装MySQL。
安装与配置首先，我们要下载MySQL的安装包，具体做法是，打开浏览器，输入网址：https://dev.mysql.com，进入MySQL的开发者专区进行下载。
在下载界面，你会看到需要选择操作系统。这是因为，MySQL可以在多种操作系统平台上运行，包括Windows、Linux、macOS等，因此，MySQL准备了针对不同操作系统平台的安装程序。这里我们主要介绍MySQL在Windows操作系统上的安装。因为Windows平台应用得最广泛，而且图形化操作也比较简单。
当然，如果你想了解Linux平台和macOS平台上的安装和配置，也可以通过官网https://dev.mysql.com/doc/refman/8.0/en/linux-installation.html 和https://dev.mysql.com/doc/refman/8.0/en/osx-installation.html 来进行查看。不同平台上的MySQL会略有不同，比如，同样的机器配置，Linux上的MySQL运行速度就比Windows快一些，不过它们支持的功能和SQL语法都是一样的，即使你使用的是其他系统，也不会影响到我们的学习。
好了，下载完成之后，我们就可以开始安装了。接下来我给你介绍下安装步骤。
第一步：点击运行下载的安装程序，安装MySQL数据库服务器及相关组件
我给你介绍下这些关键组件的作用。
MySQL Server：是MySQL数据库服务器，这是MySQL的核心组件。 MySQL Workbench：是一个管理MySQL的图形工具，一会儿我还会带你安装它。 MySQL Shell：是一个命令行工具。除了支持SQL语句，它还支持JavaScript和Python脚本，并且支持调用MySQL API接口。 MySQL Router：是一个轻量级的插件，可以在应用和数据库服务器之间，起到路由和负载均衡的作用。听起来有点复杂，我们来想象一个场景：假设你有多个MySQL数据库服务器，而前端的应用同时产生了很多数据库访问请求，这时，MySQL Router就可以对这些请求进行调度，把访问均衡地分配给每个数据库服务器，而不是集中在一个或几个数据库服务器上。 Connector/ODBC：是MySQL数据库的ODBC驱动程序。ODBC是微软的一套数据库连接标准，微软的产品（比如Excel）就可以通过ODBC驱动与MySQL数据库连接。 其他的组件，主要用来支持各种开发环境与MySQL的连接，还有MySQL帮助文档和示例。你一看就明白了，我就不多说了。
好了，知道这些作用，下面我们来点击“Execute”，运行安装程序，把这些组件安装到电脑上。
第二步：配置服务器
等所有组件安装完成之后，安装程序会提示配置服务器的类型（Config Type）、连接（Connectivity）以及高级选项（Advanced Configuration）等，如下图所示。这里我重点讲一下配置方法。
我们主要有2个部分需要配置，分别是服务器类别和服务器连接。
先说服务器类别配置。我们有3个选项，分别是开发计算机（Development Computer）、服务器计算机（Sever Computer）和专属计算机（Dedicated Computer）。它们的区别在于，MySQL数据库服务器会占用多大的内存。
如果选择开发计算机，MySQL数据库服务会占用所需最小的内存，以便其他应用可以正常运行。 服务器计算机是假设在这台计算机上有多个MySQL数据库服务器实例在运行，因此会占用中等程度的内存。 专属计算机则会占用计算机的全部内存资源。 这里我们选择配置成“开发计算机”，因为我们安装MySQL是为了学习它，因此，只需要MySQL占有运行所必需的最小资源就可以了。如果你要把它作为项目中的数据库服务器使用，就应该配置成服务器计算机或者专属计算机。
再来说说MySQL数据库的连接方式配置。我们也有3个选项：网络通讯协议（TCP/IP）、命名管道（Named Pipe）和共享内存（Shared Memory）。命名管道和共享内存的优势是速度很快，但是，它们都有一个局限，那就是只能从本机访问MySQL数据库服务器。所以，这里我们选择默认的网络通讯协议方式，这样的话，MySQL数据库服务就可以通过网络进行访问了。
MySQL默认的TCP/IP协议访问端口是3306，后面的X协议端口默认是33060，这里我们都不做修改。MySQL的X插件会用到X协议，主要是用来实现类似MongoDB 的文件存储服务。这方面的知识，我会在课程后面具体讲解，这里就不多说了。
高级配置（Show Advanced）和日志配置（Logging Options），在咱们的课程中用不到，这里不用勾选，系统会按照默认值进行配置。
第三步：身份验证配置
关于MySQL的身份验证的方式，我们选择系统推荐的基于SHA256的新加密算法caching_sha2_password。因为跟老版本的加密算法相比，新的加密算法具有相同的密码也不会生成相同的加密结果的特点，因此更加安全。
第四步：设置密码和用户权限
接着，我们要设置Root用户的密码。Root是MySQL的超级用户，拥有MySQL数据库访问的最高权限。这个密码很重要，我们之后会经常用到，你一定要牢记。
第五步：配置Windows服务
最后，我们要把MySQL服务器配置成Windows服务。Windows服务的好处在于，可以让MySQL数据库服务器一直在Windows环境中运行。而且，我们可以让MySQL数据库服务器随着Windows系统的启动而自动启动。
图形化管理工具Workbench安装完成之后，我再给你介绍一下MySQL自带的图形化管理工具Workbench。同时，我还会用Workbench的数据导入功能，带你导入一个Excel数据文件，创建出我们的第一个数据库和数据表。
首先，我们点击Windows左下角的“开始”按钮，如果你是Win10系统，可以直接看到所有程序，如果你是Win7系统，需要找到“所有程序”按钮，点击它就可以看到所有程序了。
接着，找到“MySQL”，点开，找到“MySQL Workbench 8.0 CE”。点击打开Workbench，如下图所示：
左下角有个本地连接，点击，录入Root的密码，登录本地MySQL数据库服务器，如下图所示：
这是一个图形化的界面，我来给你介绍下这个界面。
上方是菜单。左上方是导航栏，这里我们可以看到MySQL数据库服务器里面的数据库，包括数据表、视图、存储过程和函数；左下方是信息栏，可以显示上方选中的数据库、数据表等对象的信息。 中间上方是工作区，你可以在这里写SQL语句，点击上方菜单栏左边的第三个运行按钮，就可以执行工作区的SQL语句了。 中间下方是输出区，用来显示SQL语句的运行情况，包括什么时间开始运行的、运行的内容、运行的输出，以及所花费的时长等信息。 好了，下面我们就用Workbench实际创建一个数据库，并且导入一个Excel数据文件，来生成一个数据表。数据表是存储数据的载体，有了数据表以后，我们就能对数据进行操作了。
创建数据表第一步：录入Excel数据</description></item><item><title>用户故事_Jerry银银：这一年我的脑海里只有算法</title><link>https://artisanbox.github.io/2/79/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/79/</guid><description>比尔·盖茨曾说过：“如果你自以为是一个很好的程序员，请去读读Donald E. Knuth的《计算机程序设计艺术》吧……要是你真把它读下来了，就毫无疑问可以给我递简历了。”虽然比尔·盖茨推荐的是《计算机程序设计艺术》这本书，但是本质却折射出了算法的重要性。
大家好，我是Jerry银银，购买过算法专栏的同学应该时不时会看到我的留言！目前我是一名Android应用开发工程师，主要从事移动互联网教育软件的研发，坐标上海。
我为何要学算法？细细想来，从毕业到现在，7年多的时间，我的脑海里一直没有停止过思考这样一个问题：技术人究竟能够走多远，技术人的路究竟该如何走下去？相信很多技术人应该有同样的感受，因为技术的更新迭代实在是太快了，但是我心里明白：我得为长远做打算，否则，就算换公司、换工作，可能本质也不会有什么改变。
但是，我其实不太清楚自己到底应该往什么地方努力。于是，我翻阅了好多书籍，搜寻IT领域各种牛人的观点。多方比较之后，我终于决定，从基础开始，从计算机领域最基础、最重要的一门课开始。毫无疑问，这门课就是数据结构和算法。
我是如何遇见极客时间的？既然找到了方向，那就开始吧。可是问题来了，从哪儿开始呢？大方向虽然有了，可是具体的实现细节还是得慢慢摸索。大学没怎么学，工作这么多年也没有刻意练习，起初我还真不知道从哪儿开始，只是买了本书，慢慢地啃，也找了一些简单的题目开始做。有过自学经历的同学，应该有同感吧？刚开始连单链表翻转这样简单的题都要折腾半天，真心觉得“痛苦”。
之前我在极客时间上订阅过“Java核心技术36讲”，体会到了专栏和书本的不同。极客时间的专栏作者都是有着丰富的一线开发经验，能很好地把知识和实战结合在一起的大牛。这些课听起来非常爽。估计你应该经常跟我一样感叹：“哦！原来这些知识还可以这么使用！”当时我就在想，极客时间啥时候有一门算法课就好了。
说来真是巧，没多久，极客时间就推出了“数据结构与算法之美”。我试读了《为什么要学习数据结构和算法》和《数组：为什么很多编程语言中数组都从0开始编号？》这两篇之后，立即购买了。
到现在，专栏学完了，但是我依然记得，王争老师在《为什么要学习数据结构和算法》这篇文章里面提到的三句话，因为这每一句话都刺痛了我的小心脏！
第一句：业务开发工程师，你真的愿意做一辈子CRUD Boy吗？
第二句：基础架构研发工程师，写出达到开源水平的框架才是你的目标！
第三句：对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！
我每天是怎么学专栏的？于是，每天早上醒来，我的第一件事就是听专栏！专栏在每周的一、三、五更新，每周的这三天早上，我会听更新的文章。其它时间，我就听老的文章，当作复习。
听的过程，我一般会分这么几种情况。
第一种情况，更新的内容是我之前就已经学过的，基本已经掌握了的。这种情况下，听起来相对轻松点，基本上听一遍就够了。起床之后，再做一下老师给的思考题。这种情况在专栏的基础部分出现得比较多，像数组、链表、栈、队列、哈希表这些章节，我基本上都是这么过来的。
第二种情况，更新的内容是我学过的，但是还不太精通的。这种情况下，王争老师讲的内容都会将我的认知往前“推进”一步。顺利的话，我会在上班之前就搞懂今天更新的内容。这种情况是曾经没有接触过的内容，但是整体来说不难的理解的，比如跳表、递归等。
还有一种情况，就是听一遍不够，听完再看一遍也不行，上午上班之前也搞不定的。不过，我也不会急躁。我心里知道，我可能需要换换脑子，说不定，在上午工作期间，灵感会突然冒出来。这种情况一般出现在红黑树、字符串查找算法、动态规划这些章节。
到了中午休息时间，我会一个人在公司楼下转一圈，同样，还是听专栏、看专栏。
如果今天的文章，早上已经搞定了，我会重新看下其他同学的留言，看看其他同学是如何思考文章的课后思考题的，还有就是，我会看看其他同学学习过程中，会有哪些疑问，这些疑问自己曾经是否遇到过，现在是否已经完全解决了。
如果今天的文章，早上没有彻底搞懂，这种情况下，我会极力利用中午的时间去思考。
晚上的时间通常无法确定，我有时候会加班到很晚，回到家，再去啃算法，效率也不高。所以，我一般会在晚上“看”算法。为什么我会用双引号呢？是因为我真得只是“看”，目的就是加深印象。
以上基本是我工作日学专栏的“套路”。
等到了周末或者其它节假日，就是“打攻坚战”的时候了。估计很多上班族和我一样，只有周末才有大量集中思考的时间。这时候，我一般会通过做题来反向推动自己的算法学习。
像红黑树、Trie树、递归、动态规划这些内容，我都是在周末和节假日搞懂的。虽然到现在对其中一些知识还不能达到游刃有余的地步，但是对一般的问题，大体上我都知道该如何抽象、如何拆解了。
我在学习算法时记的笔记通过学习专栏，我有什么不一样的收获？首先，专栏学习拓宽了我的知识面。例如，很多书本不讲的跳表，王争老师用了一篇文章来讲解。犹记得当我看完跳表时，心想，这么简单、易懂、高效的数据结构，为什么很多书籍都没有呢？这个专栏真的买值了！
其次，专栏的理论和实践结合很强。书籍是通用性很强的教材，一般很少会涉及软件系统是如何使用具体的数据结构和算法的。在专栏中，老师把对应的知识和实践相互结合，听起来特别过瘾！比如堆这种数据结构，理解起来不难，但是要用好它，还得下点功夫，经过老师一讲解，搭配音频，我的理解也变得更加深入了。
最后，专栏留言这个功能真的太好了，为自学带来了诸多便利，也让我获得了很多正向反馈。很多时候，经过相当长的一段时间思考，还是不能打通任督二脉，其实后来回想，当时就差那一层窗户纸了。于是，我在文末留下了自己的疑问，结果王争老师轻描淡写一句话我就明白了。
留言功能还有个非常大的好处。如果你用心学习，用心思考，用心留言，你的留言很大概率会被同伴点赞，很多时候还能被置顶。这本身就是一种正向反馈，也会更加促进自己的学习动力。还有一种更爽的体验，突然有一天早上，我照例醒来听专栏，突然听到了自己的名字。这个专栏4万多人订阅，老师居然记得我！可见王争老师真的认真看了每一条留言。
最后，我总结下自己学这个专栏的收获。尽管很多，但是我想用三句话来概括。
第一，写代码的时候，我会很自然地从时间和空间角度去衡量代码的优劣，时间、空间意识被加强了很多。
第二，学习算法的过程，有很多的“痛苦”，也正是因为这些“痛苦”，我学到了很多知识以外的东西。
第三，过程可能比知识更重要。要从过程中体会成长和精进的乐趣，而知识是附加产品！
专栏虽然结束，但是学习并没有结束。同学们，我们开头见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>用户故事_yiyang：我的上机实验“爬坑指南”</title><link>https://artisanbox.github.io/9/52/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/52/</guid><description>你好，我是yiyang。
先简单说说我自己吧，我是一名编程爱好者，这个爱好从小学就已经播下了种子。我从求学到就业，有过很多次机会接触计算机方面的学习和相关工作，可是一直没有真正动手编程。这次能接触到LMOS老师的《操作系统实战45课》，让我眼前一亮，当时就报名了这门课。
都说编程需要能掌握一些基础的编程语言，但在这门编写操作系统的这门面前，我属于“三零基础”：Linux是零基础、汇编语言是零基础，C语言也是零基础。但这一点也没有影响我学习这门课的热情，因为我从报名那一刻，就站在了LMOS老师的肩膀上;-)
我的学习思路简单粗暴，就是先跟着老师的整个课程跑一遍，拿下整体框架。我自己也清楚，看不懂代码是我目前的一大劣势，而这不是三天两天能速成的，那我就不纠结在这方面，先跟着老师的每节课的讲解把大概意思给硬啃下来。
至于代码部分，老师在教学辅助石墨文档里已经给大家推荐了非常好的学习资料。因为明白自己当前的情况，也明确了自己的目标，所以并没有出现计划容易落地难的问题，我也一直是按计划学习的。在时间安排上，我是每天安排一课，跟着老师课程里的语音，同步看文字、插图及配套的代码。
这里有一个关键点，那就是每节课后都有编程大神们的精选留言，这些是已经学完课程的同学留下的宝贵学习经验。对于正在学习的我们，也可以用来辅助参考，这是我每节课必看的内容。
专栏里大部分内容都可以实际上机体验，LMOS老师都把配套的课程代码链接传到了Gitee上。我在学完每节课之后，一定会亲自上机运行一遍老师的代码。虽然我目前还写不出这些代码，但每次遇到有代码的课都自己跑一遍，也能更直观感受到当前这节课最终可以实现出什么样的结果。以我目前的实操能力，现在就能把这件事做到。
就从第一次上机运行的经验开始说起吧，也就是第二课“几行代码几行C，实现一个最简单的内核”。先说结果：我印象里，开始动手是夜里11点半，一直搞到了凌晨3、4点才最终完成。虽然只是运行了老师写的代码，但把自己的运行结果晒到打卡群时，内心还是感受到满满的成就感。
我在这将近4个小时的折腾里，因为这样那样的问题，不断掉坑、爬坑。那我究竟是如何解决，最终才完成了第2课的 Hello OS 呢？
兵马未动，配置先行之前在介绍里给自己的标签是“三无基础”，所以上机实践的每一步都是新的尝试和探索。运行课程代码前，自然要先搞定运行环境，主要分两个部分：安装Ubuntu和各种编译连接工具。
虽说是第一次安装Ubuntu，经过学习石墨文档里每一条和我的运行环境相关的文档链接以及在百度里大致搜索了一些安装教程，基本就能搞定这一关，安装过程中除了自己指定安装路径，其余的安装设置基本上都是选择默认的选项。
安装中途遇到第一个大坑，估计大部分初次安装Ubuntu的同学都可能遇到过：初次安装时，系统会在线安装系统的一些升级程序，具体是哪些先不深究，这个环节我至少等了45分钟还多。相比之前安装Windows操作系统区别很大，毕竟虚拟机里安装Win10，安装程序基本上是直接从iOS系统安装压缩包里把需要安装的程序文件全部解压出来，感觉整个过程十几分钟就完成了。
后来再去百度搜了一些资料才知道，安装中最好关闭本机网络。其中的关键点就是，我们电脑主机或虚拟机在安装Ubuntu系统时，有一部分程序需要从服务器下载，而默认的下载服务器传输速度缓慢是这个坑的直接产生原因。这个不光会影响Ubuntu安装，后续使用时安装一些程序还会给我们带来困扰：你会发现，下载程序速度非常慢，甚至经常中断无法完成下载。
找到了原因，对症下药就能根治问题。办法就是安装Ubuntu时，记得要关闭网络或虚拟机的网络功能，然后在安装完成后，找一个Ubuntu大陆地区镜像站点，配置到Ubuntu的设置里。
这里我推荐清华大学开源软件镜像站，超链接里有具体的使用帮助，选择跟你安装的ubuntu相同的版本，跟着帮助指导就能完成配置。重启后，再去下载和升级程序，你会发现下载速度就变得非常快了。
其实镜像站还有很多，你可以自己搜一搜，比如还有阿里云的镜像站，有兴趣的可以多找一些备用，使用和配置方法都是相似的，只要学会配置一个，其他的也都差不多。唯一不同的就是镜像站的链接网址这方面的区别。
解决了Ubuntu的下载问题，接下来就是安装课程里需要用到的代码编译等程序，把它们安装或升级到最新版本。
接下来我们就安装编译链接等工具：nasm、gcc、make。具体操作时，在Ubuntu系统里，进入终端 Terminal，在命令行中输入下面这条指令：
sudo apt install nasm gcc make 输入指令后，系统就会帮我们下载并安装这几个编译链接工具。完成安装后，第二课配套代码的程序编译环境我们就搭建好了。
上机运行的“爬坑指南”第二课的上机经验第二课的上机代码,老师已经帮我们写好了，我们只需要下载到Ubuntu里，然后进入终端Terminal ，在lesson02/HelloOS目录下，运行下面这条指令：
make -f Makefile 经过上述流程，我们就会得到 HelloOS.bin 文件。当然，这条指令的执行过程中，整个过程里生成了好几个文件，这几个文件生成的具体流程和介绍，专栏里都有详细说明，这里我们主要是讲如何运行代码，需要的就是最终生成的这个HelloOS.bin文件。
得到HelloOS.bin后，我们需要手动修改两个地方，手动选择启动项，还有把 Hello OS 添加进GRUB开机启动菜单。
手动修改第一关首先，我们要修改/etc/default/grub，把GRUB启动菜单配置改成启动时“显示”，可以让我们手动选择启动项。
这里我额外分享一个我的技巧，在对这类系统文件进行任何改动前，建议都先做一个备份，这样备份后，即使修改发生了错误后，还能用这个备份文件还原恢复。修改grub的具体操作是，在Ubuntu系统的终端 Terminal里，进入 /etc/default/ 目录，使用指令修改grub配置文件，代码如下：
sudo gedit grub 输入之上述指令后，编辑器里会显示grub配置文件，大约33行左右。
首先我们要用#号注销掉hidden行。我这个Ubuntu版本是在第7行，只需要在前面加上#号，也就是“#GRUB_TIMEOUT_STYLE=hidden”。
有的Ubuntu版本里是第7和第8行里都有hidden，那就把这两行前面都加上#号注释掉。为啥要注释掉呢？hidden的作用是启动时不显示GRUB启动菜单，而我们需要在启动时显示GRUB菜单选项，所以需要用#号注释掉。如果实验后你不需要显示GRUB启动菜单，逆向操作设置即可。
接下来，需要设置GRUB启动菜单的默认等待时间。代码如下：
GRUB_TIMEOUT=30 这里的参数30表示Ubuntu启动，进入GRUB启动菜单后，倒计时30秒，如果没有任何手动操作，就会直接进入第一个默认的启动选项系统。
接着，我们需要把GRUB_CMDLINE_LINUX_DEFAUL设置为text，也就是打开启动菜单时默认使用文本模式，代码如下：
GRUB_CMDLINE_LINUX_DEFAULT="text" 完成grub文件的这三处修改，记得按右上角的Save保存，然后关闭grub文件。
grub文件并不是修改后就完事了，还需要提示系统，我们已经更新了grub文件。操作也很简单，只需要在命令行输入如下指令：
sudo update-grub 这样，grub文件的配置修改我们就搞定了。
手动修改第二关接下来我们看看添加 Hello OS 的操作 。老师在课里“安装 Hello OS”这部分提到：</description></item><item><title>用户故事_zixuan：站在思维的高处，才有足够的视野和能力欣赏“美”</title><link>https://artisanbox.github.io/2/80/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/80/</guid><description>大家好，我是zixuan，在一家国内大型互联网公司做后端开发，坐标深圳，工作5年多了。今天和大家分享一下，我学习专栏的一些心得体会。
随着年龄的增长，我经历了不少业务、技术平台、中间件等多种环境和编程工具的迭代变更。与此同时，我越来越意识到，要做一名优秀的程序员，或者说，能够抵御年龄增长并且增值的程序员，有两样内功是必须持续积累的，那就是软件工程经验方法和算法应用能力。
通俗地讲，就是不论在什么系统或业务环境下、用什么编程工具，都能写出高质量、可维护、接口化代码的能力，以及分解并给出一个实际问题有效解决方案的能力。
我为什么会订阅这个专栏？这也是为什么我在极客时间上看到王争老师的“数据结构与算法之美”的开篇词之后，果断地加入学习行列。同时，我也抱有以下两点期望。
第一，这个专栏是从工程应用，也就是解决实际问题的角度出发来讲算法的，原理和实践相辅相成，现学现用，并且重视思考过程。从我个人经验来看，这的确是比较科学的学习方法。我相信很多人和我一样，以前在学校里都学过算法，不过一旦不碰书了，又没有了应用场景后，很快就把学过的东西丢了，重新拾起来非常困难。
第二，从专栏的标题看出，王争老师试图带我们感受算法的“美”，那必将要先引导我们站在思维的高处，这样才有足够的视野和能力去欣赏这种“美”。我很好奇他会怎么做，也好奇我能否真正地改变以前的认知，切身地感受到“美”。
我是如何学习这个专栏的？就这样，同时带着笃定和疑问，我上路了。经过几个月的认真学习，“数据结构与算法之美”成了我在极客时间打开次数最多，花费时间最多，完成度也最高的一门课。尽管如此，我觉得今后我很可能还会再二刷、多刷这门课，把它作为一个深入学习的索引入口。 接下来，我就从几个方面，跟你分享下，这半年我学习这个专栏的一些感受和收获。
1.原理和实用并重：从实践中总结，应用到实践中去学习的最终目的是为了解决实际问题，专栏里讲的很多方法甚至代码，都能够直接应用到大型项目中去，而不仅仅是简单的原理示例。
比如王争老师在讲散列表的时候，讲了实现一个工业级强度的散列表有哪些需要注意的点。基本上面面俱到，我在很多标准库里都找到了印证。再比如，老师讲的LRU Cache、Bloom Filter、范围索引上的二分查找等等，也基本和我之前阅读LevelDB源代码时，看到的实现细节如出一辙，无非是编程语言的差别。
所以，看这几部分的时候，我觉得十分惊喜，因为我经历过相关的实际应用场景。反过来，专栏这种原理和实用并重的风格，也能帮助我今后在阅读开源代码时提升效率、增进理解。
另外，我觉察到，文章的组织结构，应该也是老师试图传达给我们的“他自己的学习方法”：从开篇介绍一个经典的实际问题开始（需求），到一步步思考引导（分析），再到正式引出相关的数据结构和算法（有效解决方案），再将其应用于开篇问题的解决（实现、测试），最后提出一个课后思考题（泛化、抽象、交流、提升）。
这个形式其实和解决实际工程问题的过程非常类似。我想，大部分工程师就是在一个个这样的过程中不断积累和提升自己的，所以我觉得这个专栏，不论是内容还是形式真的都很赞。
2. 学习新知识的角度：体系、全面、严谨、精炼，可视化配图易于理解“全面”并不是指所有细节面面俱到。事实上，由于算法这门学科本身庞大的体量，这类专栏一般只能看作一个丰富的综述目录，或者深入学习的入口。尽管如此，王争老师依然用简洁精炼的语言Cover到了几乎所有最主要的数据结构和算法，以及它们背后的本质思想、原理和应用场景，知识体系结构全面完整并自成一体。
我发现只要能紧跟老师的思路，把每一节的内容理解透彻，到了语言实现部分，往往变成了一种自然的总结描述，所以代码本身并不是重点，重点是背后的思路。
例如，KMP单模式串匹配和AC自动机多模式串匹配算法是我的知识盲区。以前读过几次KMP的代码，都没完全搞懂，于是就放弃了。至于AC自动机，惭愧地说，我压根儿就没怎么听说过。
但是，在专栏里，王争老师从BruteForce方法讲起，经过系统的优化思路铺垫，通俗的举例，再结合恰到好处的配图，最后给出精简的代码。我跟随着老师一路坚持下来，当我看到第二遍时突然就豁然开朗了。而当我真正理解了AC自动机的构建和工作原理之后，在某一瞬间，我的内心的确生出了一种美的感觉（或者更多的是“妙”吧？）。
AC自动机构建的代码，让我不自觉地想到“编织”这个词。之前还觉得凌乱的、四处喷洒的指针，在这里一下子变成了一张有意义的网，编织的过程和成品都体现出了算法的巧妙。这类联想无疑加深了我对这类算法的理解，也许这也意味着，我可以把它正式加入到自己的算法工具箱里了。
另外一个例子是动态规划。以前应用DP的时候，我常常比较盲目，不知道怎么确定状态的表示，甚至需要几维的状态都不清楚，可以说是在瞎猜碰运气。经过老师从原理到实例的系统讲解后，我现在明白，原来DP本质上就是在压缩重复子问题，状态的定义可以通过最直接的回溯搜索来启发确定。明白这些之后，动态规划也被我轻松拿下了。
3. 已有知识加深的角度：促进思考，连点成线之前看目录的时候，我发现专栏里包含了不少我已经知道的知识。但真正学习了之后，我发现，以前头脑中的不少概念知识点，是相对独立存在的，基本上一个点就对应固定的那几个场景，而在专栏里，王争老师比较注重概念之间的相互关联。对于这些知识，经过王争老师的讲解，基本可以达到交叉强化理解，甚至温故知新的效果。
比如老师会问你，在链表上怎么做二分查找？哈希和链表为什么经常在一起出现？这些问题我之前很少会考虑到，但是当我看到的时候，却启发出很多新的要点和场景（比如SkipList、LRUCache）。
更重要的是，跟着专栏学习一段时间之后，我脑中原本的一些旧概念，也开始自发地建立起新的连接，连点成线，最后产生了一些我之前从未注意到的想法。
举个感触最深的例子。在跟随专栏做了大量递归状态跟进推演，以及递归树分析后，我现在深刻地认识到，递归这种编程技巧背后，其实是树和堆栈这两种看似关联不大的数据结构。为什么这么说呢？
堆栈和树在某个层面上，其实有着强烈的对应关系。我刚接触递归的时候，和大多数初学者一样，脑子很容易跟着机器执行的顺序往深里绕，就像Debug一个很深的函数调用链一样，每遇到一个函数就step into，也就是递归函数展开-&amp;gt;下一层-&amp;gt;递归函数展开-&amp;gt;下一层-&amp;gt;…，结果就是只有“递”，没有“归”，大脑连一次完整调用的一半都跑不完（或者跑完一次很辛苦），自然就会觉得无法分析。如下图，每个圈代表在某一层执行的递归函数，向下的箭头代表调用并进入下一层。
我初学递归时遇到的问题：有去无回，陷得太深随着我处理了越来越多的递归，我慢慢意识到，为什么人的思考一定要follow机器的执行呢？在递归函数体中，我完全可以不用每遇到递归调用都展开并进入下一层（step into），而是可以直接假定下一层调用能够正确返回，然后我该干嘛就继续干嘛（step over），这样的话，我只需要保证最深一层的逻辑，也就是递归的终止条件正确即可。
原因也很简单，不管在哪一层，都是在执行递归函数这同一份代码，不同的层只有一些状态数据不同而已，所以我只需要保证递归函数代码逻辑的正确性，就确保了运行时任意一层的结果正确性。像这样说服自己可以随时step over后，我的大脑终于有“递”也有“归”了，后续事务也就能够推动了。
有一定经验后我如何思考递归：有去有回，自由把握最近在学习这门课程的过程中，我进一步认识到，其实上面两个理解递归的方式，分别对应递归树的深度遍历和广度遍历。尽管机器只能按照深度优先的方式执行递归代码，但人写递归代码的时候更适合用广度的思考方式。当我在实现一个递归函数的时候，其实就是在确定这棵树的整体形状：什么时候终止，什么条件下生出子树，也就是说我实际上是在编程实现一棵树。
那递归树和堆栈又有什么关系呢？递归树中从根节点到树中任意节点的路径，都对应着某个时刻的函数调用链组成的堆栈。递归越深的节点越靠近栈顶，也越早返回。因而我们可以说，递归的背后是一棵树，递归的执行过程，就是在这棵树上做深度遍历的过程，每次进入下一层（“递”）就是压栈，每次退出当前层（“归”）就是出栈。所有的入栈、出栈形成的脉络就组成了递归树的形态。递归树是静态逻辑背景，而当前活跃堆栈是当前动态运行前景。
学完专栏后我怎么看待递归：胸有成“树”，化动为静这样理解之后，编写或阅读递归代码的时候，我真的能够站得更高，看得更全面，也更不容易掉入一些细节陷阱里去了。
说到这里，我想起之前在不同时间做过的两道题，一道是计算某个长度为n的入栈序列可以有多少种出栈序列，另一道是计算包含n个节点的二叉树有多少种形状。我惊讶地发现，这两个量竟然是相等的（其实就是卡特兰数）。当时我并不理解为什么栈和树会存在这种关联，现在通过类似递归树的思路我觉得我能够理解了，那就是每种二叉树形状的中序遍历都能够对应上一种出栈顺序。
类似这样“旧知识新理解”还有很多，尽管专栏里并没有直接提到，但是这都是我跟随专栏，坚持边学边思考，逐步感受和收获的。
总结基于以上谈的几点收获和感受，我再总结下我认为比较有用的、学习这个专栏的方法。
1.紧跟老师思路走，尽量理解每一句话、每一幅配图，亲手推演每一个例子。
王争老师语言精炼。有些文字段落虽短，但背后的信息量却很大。为了方便我们理解，老师用了大量的例子和配图来讲解。即便是非常复杂、枯燥的理论知识，我们理解起来也不会太吃力。
当然有些地方确实有点儿难，这时我们可以退而求其次，“先保接口，再求实现”。例如，红黑树保持平衡的具体策略实现，我跟不下来，就暂时跳过去了，但是我只要知道，它是一种动态数据的高效平衡树，就不妨碍我先使用这个工具，之后再慢慢理解。
2.在学的过程中回顾和刷新老知识点，并往工程实践上靠。学以致用是最高效的方法。
3.多思考，思考比结果重要；多交流，亲身感受和其他同学一起交流帮助很大。
最后，感谢王争老师和极客时间，让我在这个专栏里有了不少新收获。祝王争老师事业蒸蒸日上，继续开创新品，也希望极客时间能够联合更多的大牛老师，开发出更多严谨又实用的精品课程！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>用户故事_因为热爱，所以坚持</title><link>https://artisanbox.github.io/6/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/45/</guid><description>你好，我是宫文学。
很高兴能够看到你分享自己的学习故事。
通过你的留言和故事分享，我能深刻地感受到你对编译原理的热爱，感谢你能与我一起，坚持学习，努力进步，把编译原理这门硬骨头一点一点、一步一步地消化掉，学有所用。
你好，我是雲至，今年38岁，现在在电力公司做信息运维工作。
虽然我的工作与编译技术并不相关，学习编译原理似乎没有用武之地，但是，编译原理于我而言有着特殊的意义，它伴随了我整个大学时代，“啃”下它，攻破它，成了我多年后的目标。
大学时，我学的是信息与计算科学，那时，接触了很多计算机的数学原理，出于好奇心，我尝试去了解编译原理教材，却觉得像天书一样，整整看了50多遍就是看不懂。虽然不服输，但因为各种客观原因，只好放弃。
其实，我特别想知道计算机语言是怎么样变成能被计算机执行的语言的，步入中年后，我开始计划学习编程语言和计算机基础课，可在学习编程语言时发现如果不懂编译原理的话，自己的认识水平根本无法提高，而那时，我心里那股不服输的劲儿又燃了起来，所以当“极客时间”开设《编译原理之美》课程时，我马上就报了名。现在，学到18讲，我想把自己的感受分享给大家，可文笔不佳，还望大家不要见怪。
感受一：宫老师讲解思路特别清晰，课程设计比较巧妙。
在原理上，老师讲了很多书本上看不到的编程思想，比如清晰化，简单化和好维护。并用清晰的AST还原了程序代码从代码变成可执行的代码的过程。
在学习方法上，宫老师提供了一个比较高效的学习方法，先帮助我建立了对编译器前端技术的直观理解，在“01 | 理解代码：编译器的前端技术”里，让我真正理解了词法分析、语法分析和语义分析到底是什么意思。然后宫老师由浅入深，展开解析，帮助我理清了编译原理的知识体系。
感受二：原理和实践并行， 让我通过动手提升认知。
在学习完词法分析讲之后，我认识到有限状态机的编程思路可以大大简化编码的难度，老师通过计算器的例子，特别清楚地讲明白了这个方法。
而 “08 | 作用域和生存期：实现块作用域和函数”则解决了我很多年的困惑，让我明白了变量的作用域的概念具体是怎么一回事。与此同时，课后老师及时提供了示例代码的链接，我通过动手演练，明白了一些没有搞懂的内容，比如函数、作用域等等。
随着课程不断深入，我的困惑也多了起来，当我在留言区提出自己的疑问时，宫老师总是能不厌其烦地讲解，十分认真负责！十分感谢宫老师带来这个课程，我也会继续努力学习的。
你好，我是沁园，是一名软件工程专业的研二学生。
研一时，我曾学过编译原理的课，但课上老师只讲了一些理论，没有结合实例，学的不明所以。而我自己一直对编译原理非常好奇，好奇编程语言底层到底是怎么实现的，也一直想要探知，本想啃下“龙书”和“虎书”，却因其厚重、难懂而搁置了。
后来，宫老师在极客时间上开设了《编译原理之美》的课程，三个月讲完编译的前端与后端技术，我毫不犹豫地入了坑，并从第一讲一直坚持，学到了现在。在这个过程中，我有一些学习的心得，所以想借此分享给大家，也向宫老师表达感谢之情。
心得一：在我看来，这门课不能只利用碎片的时间，而是需要课下动手和思考的。
因为编译原理本身就比较有难度，外加篇幅所限，只看文本的话，还是会产生困惑。我记得自己在“08 | 作用域和生存期：实现块作用域和函数”时，走入了死胡同，后面的连续几讲都看不明白，几乎快要放弃。
不过，宫老师贴心地在GitHub上提供了全部的源码，而且用到了我比较擅长的Java语言。我相信Java语言的程序利用IDEA的调试器就没有什么看不明白的，于是利用一个周末，把老师提供的示例脚本全部放到解析器中跑，并把解释器用IDEA的调试功能单步执行一遍，观察解释器都是怎么处理类、对象、函数以及闭包的。
调试的过程中，我边调试，边思考，边在笔记上总结，最后才恍然大悟，真正明白了老师讲的内容。真正搞懂之后，一直以来，编译器在我心中的神秘色彩也就消失了，编译中的类型推导，引用消解等高大上的概念也不过是由判断，循环等简单逻辑组成，只不过需要考虑的东西相对多些，如果几十年前让我来创立第一门编程语言，我肯定也这么搞。（目前只学了前端，学完后端以后可能观念还会有所改变）。
心得二：除此之外，这门课非常注重实战，先帮助我们建立直观认识，再去讲细节的算法。
一开始我还很奇怪，课程怎么这么“水”？编译原理不应该先把DFA、NFA、NFA转DFA、LL、LR这些经典算法作为开场吗？这个课程怎么用前三讲就把这些东西“水”过去了，然后开始讲Antlr以及语义分析了呢？
后来我发现，这些内容放在了“算法篇”中讲解。在将老师写的解释器源代码过了一遍之后，我越发地感觉到老师用心良苦。对于实现一门编程语言，实现语义才是最重要的，之前学编译的时候完全陷进了NFA、DFA、LL、LR等算法的细节中，完全没有意识到语义才是一门编程语言的灵魂。
宫老师一开始就教我们如何复用现有的成熟的Antlr规则，然后基于这些规则实现自己的语义，在学习算法篇之前，我就已经有了“如果哪天有需要，我可以徒手写一个编程语言解释器”的自信，之后虽然算法学起来也很吃力，但是不会因为陷进去而感到慌张，因为已经对解释器有了全局的把握。
一年前，我也像很多人一样，觉得编译原理是没有用的屠龙技，后来，我越发感觉编译原理在工作和学习中无处不在，比如Java程序员都会深入学习的JVM，不懂编译很多概念就只是听听，完全不理解。当你需要深入研究某个DB的时候，SQL解析优化器也绕不过去的一个坎，只有懂编译才能搞明白。所以，我庆幸自己能够接触到宫老师的《编译原理之美》，也感谢老师的良苦用心，我会一直坚持学下去，趁年轻，趁热爱，趁一起都还来得及。
编辑角：9月30日～10月6日是期中考试周，宫老师亲自出题，为你策划了20道期中测试题，帮你回顾前端技术要点内容，9月30日来挑战一下吧，不见不散！
编译器的后端技术开篇，也就是第20讲会在10月7日00:00更新。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>用户故事_成为面向“知识库”的工程师</title><link>https://artisanbox.github.io/9/54/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/54/</guid><description>你好，我是pedro，目前是一名后端小研发。
很早的时候，就收到了小编的邀请，让我来写一写用户故事。但是因为我手上有很多事情，这事儿就被耽搁了下来，所以导致这篇小故事迟到了很久。
虽然是在操作系统这个专栏下，但是我不想受到领域的限制，我想和你们分享一下我的学习思路、学习方法和收获，真诚地和你说说话，唠唠嗑，吹吹水。
学习思路你自己知道你需要什么，这才是最重要的！
我想能来这里学习的人，大多数都是希望提升自己的小伙伴，我也和你们一样，都遇到这样的问题，那就是——好书这么多，视频这么多，专栏这么多，博文又这么多，我缺的真的不是资源，而是时间！
几年前，我想要提升自己的心态十分迫切，在B站上收藏了N多视频，在浏览器主页上收藏了 N多博文，也买了很多好书和极客专栏。然而，这一堆接着一堆的东西，让我感到焦虑和茫然，实在是太多了，我哪里学得完呀。
而且我还时不时接到各式各样的推送，告诉我：你要学习数据库，这很重要；你要学习编译原理，这很重要；你要学习这个框架，面试必考；你要学习这个技术，工作必备；你要学习如何看画，审美很重要；你要学习如何读诗，远方很重要……
可我就是一个普通人，哪能学这么多？即使是时间管理大师罗志祥也办不到。我们不妨仔细想想，这些东西真的有这么重要吗？可能很重要，但是对我们来说，我觉得辨识力最重要，知道你自己需要什么，才最重要！
什么都舍弃不了的人，什么也改变不了！
这是《巨人》里面有名的金句。我之所以放到这里和你分享，是因为我觉得把这句话放在学习上同样很有效。
聊到这里，我想说说我自己的学习思路，其实也很简单。那就是，二八定律，80%的功利主义，学对工作最有帮助的，20%的情怀主义，学自己最感兴趣的。
结合你自身的工作情况和个人爱好，选择那么几门去开始学习，不要贪多，不要把买了就当成学了，用这样的方式来缓解自己的焦虑。
以我个人为例，我自校招入职以来，主要在学习与工作相关的知识，但也没有放弃个人兴趣。这里我把我正在学习探索的方向整理成了一张导图，也分享给你做参考。
在我看来，功利主义和情怀主义二者并不冲突，相反二者是相得益彰的，可以共同帮助你成长。因为工作以后，解决工作问题是最主要的事情，所以把大部分时间花在上面是值得的，这属于功利主义。
但是，工作内容并不一定只是为了解决工作问题，在工作中也可以找到有趣的事情。比如Go 语言底层的调度实现其实是非常有意思的，也可以本着情怀主义来学习，但同时在未来这部分知识又可以帮你解决更多的工作问题。
其实我也是出于情怀来学习操作系统的。操作系统可以说是打开技术底层大门的钥匙，一方面可以开拓视野，另一方面恰好也能在工作需要的时候帮助我们解决困难。
学习方法输出是学习的最佳途径！
光有学习思路是不够的，我曾遇到过这些问题：一个Bug遇到了两次，可是每次都得去 Google上搜，下次遇到了还是忘了。或者明明看了相关的视频，可是一到用的时候，突然发现自己好像只记得几个名词。
你看，明明花费了时间，却收获极小，这会严重打击我们的学习积极性。究其根本，是因为学习方法不对，导致学不到东西。
几年前，我刚步入这行的时候，由于原来没有接触过计算机，每次都是对着黑框框终端一顿操作，遇到问题到处百度（后来才转向Google），虽然稀里糊涂地解决了问题，可是下次遇到这个问题的时候，又得再百度，知识毫无积累，水平毫无提升，成了名副其实的面向“浏览器”工程师。
后面，我发现记笔记是一个有效的学习方法，可以直接提高对知识的熟练度。
因为在记笔记的过程里，我们会思考步骤、流程的合理性，重新审视这个知识点，同时记笔记也需要我们在内心里面揉碎这个知识点，加以消化，然后重新写出来。这是极佳的思考和输出的过程，有了这个过程，你不再是走马观花，而是经过了自己大脑的“解码”和“编码”，学习自然就会变得高效起来。
我记笔记最开始使用纸来写，但是效率太低，容易丢失；再后来，我学会了Markdown，开始在Markdown上记下自己踩坑的过程，写下自己的心得体会；可是很多时候我一会儿在笔记本上，一会儿又在台式机上，也有时候我需要和别人分享，甚至邀请别人一起来协作记笔记，于是我又将记笔记的地方转向了云端，开始使用石墨文档。
石墨文档支持多人协作，而且个人就算多PC、终端也可以登录，很好地解决了我的问题。下面附上我石墨文档的桌面截图，也推荐你使用。
慢慢地，我开始有了自己的积累，因为输出是更深层次的理解过程，很多坑点，我都能记下来，下次直接解决，即使遗忘了，我也能搜索自己的笔记。渐渐地我开始有了自己的知识库。从面向浏览器工程师变成了面向知识库工程师。这样的成长蜕变绝非朝夕之功，但我相信点滴的积累，终会聚沙成塔。
当然记笔记只是输出的一种，你也可以选择其它方式，比如技术分享，和同事、同学之间进行讨论，甚至给专栏留言。这里我就不得不骄傲一把了，操作系统专栏每一个小节，我都认真阅读了，思考和回答了问题，并且做了输出——留言，所以这个专栏让我收获巨大。你也可以借鉴！
收获技术能力应该是最基础的收获，收获更多的应该是生态！
开始时，我把学习和工作的目标定为提升技术能力，一路坚持下来，我的技术确实有了进步，但是我更大的收获是生态。
这个生态可能你不太理解，我来详细解释一下，我把因为学习和工作而结交到的朋友、业务理解、商业模式和思考方式等等统称为生态。
拿这个专栏来说，我重新对操作系统进行了梳理和复盘，把很多原来一知半解的知识彻底弄懂了，这只是第一层的收获。
更上层的是，我认识了大佬东哥（作者）和他的一些朋友，可爱又有责任心的小编 Sara，人美心善的小运营洁仔，还有一堆天天在群里吹水的小伙伴，他们在群里分享了很多实用的知识，我也订阅了好几个公众号。
我们因为这个专栏而认识，我们志同道合，我们一起努力来完善这个专栏，用反馈去给专栏增值，这个因大家一起努力贡献而组建起来的生态，才是我本次最大的收获。
我希望你在学习和工作的时候，不要仅仅着眼于技术本身，而是要试着切换视角，跳脱出固有的框架，并且尝试鸟瞰全局，这样你才能收获更多。同时也建议你把专栏当作学习交友的平台，希望你能在本次专栏的学习中能够与我们成为好朋友，鼓励更多的人加入进来。
除了课程正文的干货，我总是能在课程留言区发现惊喜。其实我们才是专栏真正的主人，也是专栏增值的核心力量，专栏是我们跟作者共同的作品。
还是拿我自己来说吧，加入专栏成为助教后，我的学习激情一下子就“膨胀“了。认真学习专栏不仅仅只是兴趣，还有责任感与使命感，仿佛不追完就觉得白来了一趟。也正因如此，我才能收获如此巨大，相信你也可以。
写在最后今天的分享，我从思路、方法和收获三个方面跟你聊了聊学习这件事情，下面我来谈一谈我对操作系统的看法。
操作系统是我个人认为最应该掌握的计算机必修课！因为我们的每个程序、每个应用以及每个服务都跑在操作系统这个地基上面，可以说现代互联网完全构建在了操作系统上。
操作系统是计算机软件的集大成者，是架构的极致！无论是Windows、Linux还是macOS都有几百万行代码，在保证高效运行的同时，又能将各种能力通过开放接口提供给我们，这是优良架构才能带来的能力。
东哥将操作系统的精华浓缩，并将其实现为Cosmos，用专栏的形式提供给我们，让我们有机会去一睹操作系统的风采，去汲取最有营养的养料，让你在学习操作系统的路上少走弯路，少走弯路就是走捷径。
希望每个看到这篇用户故事的小伙伴，重新拿起这个专栏。行百里者半九十，很多人行了十里就落下了，专栏行程虽然过半，但仍然可以赶上，大家，加油！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>用户故事_技术人如何做选择，路才越走越宽？</title><link>https://artisanbox.github.io/9/55/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/55/</guid><description>你好，我是宇新。
作为《操作系统实战45讲》的编辑。从专栏上线到现在已经有3个多月的时间了，感谢你一直坚持到现在。
留意过课程评论区的同学都知道，我们有几位常驻的同学一直在主动输出。那这些“课代表”是怎样学习专栏，又有什么学习诀窍？
为了满足咱们的好奇心，我特意策划了这次特别的采访，请到了在专栏里留下很多精彩足迹的neohope同学，我会代表好奇的小伙伴向他提问，希望这次的分享能够带给你一些启发。
首先让我介绍一下neohope，他是一个技术爱好者，年龄就不说了。neohope做过很多的岗位，像是软件工程师、项目经理、项目总监、产品经理、架构师、研发总监等，现在他在医疗健康行业工作。
让我们正式开始这次采访吧！
如何搭建自己的学习体系Q1：你好，neohope。你的课程笔记帮到了不少人，看得出你学得很认真，能给同学们说说，对于学好《操作系统实战45讲》这个专栏，你有哪些建议么？
A1：你好，关于怎么学好这门课程。我有这样几个建议作为参考。
第一个建议是多动动手：前期看到有些小伙伴不会用虚拟机，也不会命令行，但其实大部分同学花上几个小时也就搞定了。有了感性的认识，后面学习就不那么抽象了；
第二个建议就是够用就好：不要一看里面有汇编语言，就去从头学汇编；也不要一看C语言，就去学C。我的建议是，能读懂就够了，不会的命令网上找一下就可以了。其实，我们不妨想一下，自己小时候是如何读书的？有些看不懂的字，其实可以跳过去，这不影响理解的；
然后，我建议你多看源码：其实我的方法很笨，就是把老师的注释先拷贝到课程源码里，再结合自己的思考理解补充一些注释，这样读起来还是很简单的；
接着，就是要多理资料：有些地方看不太懂的，就去查资料，建议你把看完的资料，用自己的方式整理出来，然后分享，这样会有很好的效果；
最后，要找到组织：不要自己孤军奋战，找几个小伙伴定期聊聊，参与一些好的技术群，多交流会让你提升很快。自己迷惑的问题，可以问一下，看看别人如何理解的，不要害羞。
Q2：感谢neohope的建议。从你的留言里可以看出你对操作系统认识很深，即使是比较复杂的调用过程，你也总能很快地理清脉络，把握全局。这是怎么做到的呢？
A2：其实操作系统也好，其他技术也好，想要理解透彻都需要一个过程，而非一蹴而就。我觉得建立自己的知识体系，是一种很好的方法。
Q3：这个知识体系你是怎么建立的呢？可否分享一下，让入行不久或即将入行的小伙伴做个参考么？
A3：在我日常工作中，经常会遇到要教新人的情况，每一次带一位新人，我都会要求他/她做这样几件事情。
1.首先，我会请他用图解的方式，画一下自己会哪些技术；
2.然后，跟他深入聊几个常见问题，比如下面这些问题：
用谷歌浏览器打开一个登录页面，输入用户名、密码，当用鼠标点击登录按钮时，究竟发生了什么？ 如何自己做一个框架，去实现Spring Boot、Flask或WCF等相关功能；自己平时用框架有没有不爽的地方，想要如何改进它？ 找一个大家都熟的业务场景，聊一聊如何在技术或非技术层面进行改进…… 3.在技术上，我还会问问他，后续的学习发展计划是怎样的，自己想学什么，优先要学什么？
4.最后，我会帮他/她去逐步建立一个技术栈，并以此为出发点，做一个为期1到3年的技术规划。
Q4：前面几步听起来有点像面试的场景。你是怎样想到用这个方法呢？
A4：我接着刚才知识体系说，因为工作以后，相比在学校系统学习，我们现在接触的信息大多都是碎片化的，对自己掌握了什么技术，我们并没有清晰的了解。而且根据我多年观察，即使是一些平时工作很认真的人，都没有去好好整理过自己的知识体系，这很可惜。
我第一次跟新入职的同学沟通时，可能最开始往往得到的是一堆的技术名词。
这个时候，我会根据小伙伴自己的技术栈，帮他/她搭一个简单的体系框架，把上面的技术名词归类放好，这里我以后端工程师为例。
然后，对于重点关注的层，还可以进一步展开。咱们是自学操作系统，那这里就把OS层展开。
之后，可以把自己整理的图和可信度高的资料进行对比。咱们这里就把上图和Cosmos、Linux进行一下对比。根据对比，摘取自己需要的内容，对自己的图进行补充。
参考：https://makelinux.github.io/kernel/map/
这样，你自己的知识体系就有了雏形。接着，对于自己要重点学的内容，进一步展开，比如说，对于锁这个知识点，我是这样拆分的。
乐观锁、悲观锁 公平锁、非公平锁 重入锁、不可重入锁 自旋锁、非自旋锁 独享锁、共享锁、读写锁 分段锁、行锁、表锁 分布式锁、共识算法 …… 之后，对于这些知识点，我们可以用不同颜色进行标记（后面我列出了我自己习惯用的标记方式）。标记好了以后，你可以把“必须，未掌握，红色”的内容，整理一个清单，排个优先级，作为未来一段时间学习计划的参考。
A、必须，已掌握，绿色
B、必须，未掌握，红色
C、非必须，已掌握，绿色
D、非必须，未掌握，黄色
其实，这个知识体系就像是一张藏宝图，上面的一个个知识点就是一个个宝藏。实际使用的时候，我们不用花很大精力去做这个图，也不用限制是何种模式，一个markdown文件足够了，对自己有帮助就好。
随着你的积累和进步，每经过一个时期，都可以重新看下这个藏宝图，常看常新。
如果你特别喜欢自己的藏宝图，但图中有不少盲点，那就先找最基础的东西看，探索一段时间，迷雾自然就少了；如果你的藏宝图虽然很大，但能挖掘的精华有限，建议先找一张对你最有用的图，精力不要过于分散。如果这张图的要点你都掌握了，就需要扩展知识面，再去开个副本吧！
Q5：你的藏宝图方法听起来很酷，看得出你对不同的技术栈都比较熟悉，可以说说你的思考么？比如，不同技术栈怎样找共同点？
A5：随着不断的学习，我发现不同的技术栈，的确有很多相似的地方，就像是同一类型的宝藏。然后去看细节，又会发现不一样的地方，就像每个宝石，纹理都不一样。
以操作系统及虚拟机为例，你有没有想过Linux、Windows、Android、iOS、Docker、VritualBox、JVM、CLR、V8，都在管理哪些事情呢？
虽然这些技术并不在一个层面，其实很多要做的事情，却是很相似的。比如，都需要CPU管理、内存管理、任务管理、处理同步问题、文件管理、I/O管理、资源隔离、提供统一而稳定的API等。
然后，从任务管理这个角度再去看，还能看到优先级、时间片、抢占式、沙盒、命名空间、配额、欺上瞒下、甩手掌柜、单脑回路等等精彩的宝石纹理。
Q6：刚才说了不少寻找共性的思路，掌握了很多技术以后，你会怎么去分析它们呢？
A6：技术千千万，但追究其本质，技术都是为了解决具体问题的，这里我举三个例子吧。
以远程调用为例（远程调用推荐你看下公开课《周志明的软件架构课》），CORBA、DCOM、EJB、Webservice、REST、Thrift、ProtocolBuffer、Dubbo等，这些技术都在解决什么问题呢？这些技术的流行和没落的原因是什么呢？
我们想要解决类似RPC的问题，都是定义了一套规范要调用方和被调用方共同遵守，而且都提供了代码的辅助生成工具。那为何至今还会有很多新的技术出来，要解决这个问题呢？咱们就又要去观察“纹理”了。
以任务调度为例，从操作系统进程调度，到线程池、Socket连接池、DB连接池、对象池，再到F5、Nginx、Dubbo的流量控制，以及到大数据的Yarn、容器的编排，它们都在解决哪些问题？
再以低代码为例，ESB、OA（流程编辑器+表单设计器）、FaaS平台、SaaS平台，都在解决什么问题，给出的答案又有什么差异？这种思考方式还有很多例子，我就不一一列举了。
随着不断的学习，你会发现，不同的技术栈，有很多重叠的地方。比如，数据结构与算法、网络、数据库、文件处理、加密解密、系统调用等。一旦一次学会，就像打通任督二脉，在另外的地方，遇到类似问题的时候，就无师自通了。
Q7：那不同的技术栈，你会怎么样做对比呢？
A7：不同的技术栈，有很多不同的思路。就拿泛型为例，每种语言各有不同。
C语言，可以通过函数指针或宏来实现，需要一定的编程技巧； C++语言，一般通过STL来实现，在编译时实现，会造成代码膨胀； Java语言，通过类型擦除实现，编译时擦除，JVM运行时并不知道处理的是什么类型； C#语言，在编译生成IL中间码时，通用类型T只是一个占位符；在实例化时，根据实际类型进行替代，并通过JIT生成本地代码，不同类型的泛型类是不一样的； Go语言，当前版本，并不支持泛型，可以通过interface强制转换，需要一些编程技巧； JavaScript语言，动态类型，天生支持泛型。 Q8：感觉这样做了对比之后，确实更容易加深理解。这个方法只能用在分析泛型么，可以不可以再举个例子？</description></item><item><title>用户故事_操作系统发烧友：看不懂？因为你没动手</title><link>https://artisanbox.github.io/9/53/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/53/</guid><description>你好，我是spring Xu。我平时的工作就是做实时嵌入式系统，坐标上海。
写操作系统这件事一直是我的兴趣，我之前写过引导器，也有移植过uboot的基础，还读了不少操作系统的书。作为一名操作系统“发烧友”，我是怎样跟操作系统、跟LMOS这门课程结缘的呢？请你听我慢慢道来。
我是怎样与操作系统结缘的？其实我并非计算机专业出身，也没有系统地学过操作系统。不过出于兴趣，我早在大学时就自学了微机原理，当时记得还在x86实模式下写了些汇编程序，但还是有很多迷惑的地方。
于是，我跑到图书馆找了本Intel的芯片手册自己随意翻看，发现x86的保护模式寻址方式好奇怪，还有调用权限的知识也弄不太懂。后来我还试着询问老师，结果当时没得到什么满意的答案，这事儿也就不了了之了。
直到我工作了，接触的是嵌入式系统ucos-II。感觉这样的系统有点简单，因为它无法动态加载外部应用程序，还是想搞个更高级点的，自己写一个操作系统的想法从此埋下种子。
于是我购买了潘爱民老师的《程序员的自我修养》阅读。潘老师的那本书是讲C语言的编译链接和运行环境，也就是C语言文件如何编译、如何链接到生成程序的过程，还有该程序如何在操作系统上加载和运行以及程序加载的知识，都可以从这本书里学到。
我还根据于渊老师的《一个操作系统的实现》这本书，试着写了一个开机引导程序。这次再看到x86的保护模式，我觉得更容易看懂了，脑子里出现了一个念头，这是用硬件提供应用程序与操作系统内核之间的隔离。其实这并不是我的阅读理解水平有了多快的“飞升”，而是因为许多知识的领悟，都要经历大量的实践才会产生。
二级引导器的编写需要包含文件系统，还要有硬盘或者软盘的读取操作。工作一忙，时间久了自己也渐渐热情冷却，只是开了个头，就这样停工了。
但我的写操作系统梦想，并没有止步。后来偶然的机会，我看到周自恒老师翻译了一位日本人川合秀实写的《30天自制操作系统》。
哈哈，真正中我下怀！我心里盘算着，只要30天就可以写个操作系统出来了，那应该挺简单的。就按一个星期七天，我只读一天的内容，一年我也能完成我的操作系统了。就这样我又开始捣鼓自己的操作系统了。
这本书面对的读者算是小白，没有啥学术名词概念，用通俗的语言把许多知识都讲解了。而且按作者的工具和思路，我确实实现了一个带图形的系统，但我按照他的步骤，没觉得自己水平有啥提升，还是觉得有点不过瘾。
于是为了补充一下理论知识，我又买了好多操作系统的书，甚至入手了一块三星的s5pv210的ARM CPU的开发板，这次是想实现个3D界面效果的操作系统，像iOS的4.3风格的那种图形，选择这块开发板，是因为iPhone4的CPU还没有这个强大。这样，我的自制操作系统之路再次启动。
工作的忙碌让我只能停停走走，只把uboot移植了，可以点亮并可以在那块s700的屏幕上输出打印字符信息，并实现了一个没有2D加速的16位5:6:5格式，24位888格式的小型点阵图形库。但很遗憾，由于那个芯片的3d图形芯片是PowerVR，根本不可能有任何开放的资料指导我写出驱动它的程序，最多用CPU模拟3D功能，就这样实验再度搁置了。
现在回想起来，当时是对操作系统的图形界面感兴趣，而并不是操作系统本身的知识。关注点不一样，导致许多知识并没有进一步学习。
真正开始学习写操作系统今年年初，我无意中看到了B站哈工大李治军老师的操作系统课程，这个课程是用Linux0.11作为实验代码，更具有实战性，于是我开始边刷课程边看代码，但进展十分缓慢。
直到五月份，在微信群中看到了极客时间有操作系统实战课程。想到写操作系统的难度大，一般不太容易自己写出操作系统，而且不光写操作系统还能教别人写，对这位作者有些钦佩。我又查了下这门课程的作者——彭东，再看开篇词里他自己学操作系统、写操作系统的故事，深深被他的这份执着精神所鼓舞。
想想自己每次都想着要写一个自己的操作系统，但真正到实施时就退缩了，直到现在我也没有实现，真是太惭愧了。于是我购买了课程，还加入了东哥的操作系统实战课程群。因为之前看书跟实操积累的基础，这门课程跟起来就更加顺畅了。所以我经常在交流群里催更，还提问过CPU多核方面的问题，也会经常在课程中留言。
如果你也跟我一样，也想自己动手写操作系统，我从一个写操作系统的爱好者角度，建议大家先学习CPU的体系结构知识，这个是在硬件上为操作系统所做的准备，比如内存的访问、中断以及异常的调用。
这个基础很有必要，目的是让你在没有模拟器的环境，又不能真机调试时，也有能力定位问题，还能锻炼如何用大脑模拟运行汇编代码。这里我顺便提一个问题，你感受下，如果写1个循环100次的代码，用累加1的操作，与用递减1的操作，哪个快？为什么？欢迎留言写出你的答案。
然后是C语言和编译链接方面的知识，这个是用来生成操作系统程序的。其他知识就是在实践中，根据自己情况主动探索、获取。这个课程的代码也可以拿来使用，也可以在这个代码上做二次开发。期待我们一起为这个Cosmos操作系统添砖加瓦！
这门课带给我的收获跟着东哥学习操作系统，我感觉收获颇多，每一节课都有许多感悟，有一些我已经在课程留言区记录了，还有一些正在整理酝酿中。这里我就从整体课程出发，简单为你描述一下课程中操作系统的知识分布。
基础铺垫阶段
前两节课是热身阶段，这是为了带我们了解，从源代码文件到可运行的程序文件的编译链接过程，以及这个程序文件如何在PC上运行。这部分包含了C语言、编译、汇编还有链接的相关知识点，这也是现在许多程序员感到神秘和陌生的部分。
课程里的Hello OS实验，相当于一个PC硬件平台上的裸机程序（这里“裸机”二字我可能用得不太严谨）。现在的开发工具IDE，都把这部分工作自动化完成了。我觉得讲这些还是很有必要的，能让我们能清晰地了解程序生成过程，也能方便我们知道无调试环境下要如何看汇编代码。毕竟太依赖IDE的话，水平不会提高。
第三、第四节课对操作系统的宏内核与微内核架构做了介绍，也是Linus与Andrew Tanenbaum争论的话题。
第五到第九节课为后面Cosmos搭建铺垫了基础知识。硬件模块是对x86硬件编程的规范说明。CPU如何寻址访问内存的，也包括硬件上如何支持操作系统内核与应用程序的分级管理、x86的中断机制、cache的运行机制。
而硬件资源有限的情况下，不同程序访问同一资源，又涉及到后面各种锁的介绍。 通常操作系统课本上，这部分是放到进程中一并描述的知识点。而这里却单独拎出来，有助于我们关注本质。
初始化、内存管理与进程模块
之后第十节课开始，我们进入到启动初始化模块。一起探索真实开发环境的搭建与初始化。在二级引导器的帮助下，可以加载Cosmos的内核系统，最为直观的体会就是显示图片跟点阵字体。整个引导完成之后进入到操作系统核心，实现了各种资源的初始化过程（包括中断框架初始化）。
接着就到了最硬核的内存管理模块，这里的确有难度，一方面代码量不少，另一方面内存设计是老师自研，我们乍一看有点陌生。我现在也处于看懂了代码，但还需要进一步分析的阶段。所以也建议你对照课程讲解慢慢揣摩。
该模块老师用四节课讲物理内存管理，也就是操作系统下，内存管理中的物理内存分配管理。 这里的物理内存是指在硬件的地址空间中可以找到该数据的内存。而区别内存的前三节课，第十九节课是讲小于4k的内存分配是如何实现的。
之后四节课，两节课讲解了进程访问虚拟内存的相关知识。所谓的虚拟就是不真实的。为什么不真实呢？因为虚拟内存的大小，是由当前CPU最大可以访问的内存大小决定的，不是当前计算机安装的物理内存大小。
比如32位的CPU，虚拟内存大小是4G，但该计算机的内存配置1G，对于进程来说，还是会认为有4G的空间可以使用。
也正是这个原因，所以访问内存时，要把虚拟内存映射到真实的物理内存上，映射过程中CPU就会产生缺页中断异常，然后需要测试中断框架的代码，处理了该中断异常后，虚拟内存就可以访问到物理内存了。
之后两节课又讲了Linux的内存管理，关于Cosmos跟Linux的内存管理做对比，这里我也在摸索，等我理清楚了再分享出来。
因为之前看过不少图书，所以老师课程里进程模块的设计让我眼前一亮。因为进程的结构与调度，在各种操作系统的书里经常是讲得最复杂难懂的部分，一般课本上是把进程相关的都讲述一遍，每个知识点一次性都提及，我们不知道来龙去脉，就容易一头雾水。这不是劝退的思路么？
但这门课安排就巧妙得多，先讲内存管理再讲进程。程序的运行第一件事就是需要内存空间。有了这个基础，你再学习进程的时候也会觉得没那么难。
所以我从这两个模块学习中，得到的最大感悟就是，做什么事，把目标定好后，别考虑那么多，要设计多么的完美，而是把任务分解开，一点点来实现。
先实现一个雏形，知道会有问题，找出问题，解决问题。通俗点，哪怕起初挖了许多个坑也可以逐步完善，慢慢把坑填了，系统也就健壮了。
驱动模型、文件系统与网络
第二十八到三十课是Cosmos操作系统的驱动模型。由于操作系统是应用程序与硬件之间的桥梁。操作系统为简化应用程序开发难度，把硬件操作做了统一规划。
做驱动的开发只要根据操作系统提供的驱动模型，实现操作硬件的代码，这样就可以让应用程序调用操作系统提供的统一接口来操控不同的硬件了。课程里用的是定时中断的例子，如果想驱动键盘和鼠标的话，你可以重点看这个部分。
第三十二到三十四节课讲的是文件系统。这部分只是在内存上建立的文件系统。虽然简易，但也构建出了一个自己的文件系统。有能力的同学还可以写个磁盘驱动进行完善。
其实在等更新的时候，我也很好奇操作系统的网络模块要怎么讲。后来真的看了内容后发现，这部分其实是讲计算机网络与操作系统的关系。首先是传统单机中，计算机的网络是什么样的。 然后扩展到集群下的、超大宽带的计算机网络与操作系统的关系。
这里我整理了一张导图，整理了课程脉络。这里先说明一下，课程里还有不少关于Linux的内核的分析，这里为了凸显Cosmos主线我没有过多涉及Linux，如果你有兴趣可以试着自己动手总结归纳。
希望我对课程的内容梳理对你有帮助。操作系统里面包含的知识可以说是博大精深，真要完全掌握，需要大量的时间和精力的投入。
所以，我想结合我个人经验跟你聊一聊，我们怎么从自身工作技术栈中的底层技术点这个维度入手，深挖出与操作系统相关的知识点，通过对底层机制的思考学习，加深自己对技术的理解。
比如做Java开发，那自然深挖技术知识点，就会挖出JVM虚拟机的内存管理原理。 内存管理在操作系统中也有，但操作系统做成了谁申请、谁释放的原则，让应用程序自己来负责。
而JVM这个应用程序，它向操作系统申请了超大的堆内存作自己的虚拟机管理的内存，并根据对象的引用计数器来判断对象的生命周期是否结束，这样就可以自动回收垃圾对象所分配的内存了。对于操作系统来说，JVM仍然是占用着那块超大的堆内存的。
我们进一步思考下，如果把这部分机制放到内核中，是不是就可以做出带垃圾回收机制的C语言了呢？
这个思路其实是可以有的，但为了兼容考虑，解决思路是放到了编译阶段了。你可以了解一下Apple系统的object-c语言的ARC机制。这里你可以想想，为什么不能在Windows或者Linux、iOS上把这个功能实现了，只能从编译阶段做成这个样子？
再比如使用Golang做开发，你会发现协程其实是在Golang的运行环境里提供了协程和协程调度，协程调度器按照调度策略，把协程调度到线程中运行。协程的调度过程由用户态程序负责，也就是golang应用程序，由M个协程对应N个内核线程，这个调度算法比较复杂，但对于开发者来说是无感知的。这样带来的好处又是什么？当然协程不光Golang提供了。
再比如做前端Web开发。微信是一个IM的应用软件，但微信可以浏览网页、公众号，甚至加载小程序，小程序的开发语言是JavaScript，它的运行环境是JavaScript虚拟机。这个不是和多年前Palm公司推出的WebOS系统很像吗？
操作系统的桌面用浏览器来代替，不需要用C++或者Java语言来开发应用，直接用JavaScript语言就可以开发所谓的桌面应用程序了。如果用浏览器的插件技术，开发的语言是C++就会导致开发人员的学习成本升高，但性能是强劲的。
通过这三个例子，你有没有发现，跟原来的技术实现相比，开发应用的难度是在降低的。而现代新出的技术有不少是操作系统里做的一些功能，换到应用程序里提供的功能，又在编程语言上提供了语法糖（在编程语言中，增加新的关键字来简化原来的写法），再通过开发工具生成应用程序。
核心思想就是屏蔽复杂的知识，降低开发难度，开发人员不用太了解底层知识，就可以快速上手开发应用程序，让更多的开发者更关心所谓的业务开发。
如果只关心业务开发，你会发现今天出了一门语言，明天又出了一个框架，你感觉好像有学不完的知识。一直在学习，但感觉学不动了，要休息了。所以，我学习新技术，或者新框架，会先试着理解这个新技术是为了解决什么问题而产生的，原来的技术是否可以这样做，新技术与原技术在开发效率与运行效率上是不是有优势？
总之，我认为无论你是不是内核开发者，都有必要了解操作系统的相关知识。如果操作系统的知识你掌握了，就相当于掌握了内功心法，学习新的语言或者新的技术，只要看看官方的文档，就可以很快开始运用该技术做项目了。
学技术不动手，就好比游泳只看理论不下水。希望你也能认识到，动手去写代码、改代码很重要，让我们跟着LMOS，在操作系统的实践中不断精进！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>用户故事_易昊：程序员不止有Bug和加班，还有诗和远方</title><link>https://artisanbox.github.io/7/54/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/54/</guid><description>你好，我是编辑王惠。在处理这门课的留言时，我注意到易昊同学一直在跟随着宫老师的脚步，学习和实践编译原理的相关知识，留言的内容十分有见地、提出的问题也能看出是经过了他深入的思考。同时，咱们这门课也具有很强的互动性，所以我邀请他来和我们分享一下他的心得体会。
Hi，我是易昊，目前在武汉做Android开发，已经工作12年了。很高兴能在这里跟你分享，关于我学习编译原理的一些心得体会。
为什么我要再学编译原理？首先，我想给你解释一下，我为什么会起“程序员不止有Bug和加班，还有诗和远方”这样一个标题呢？
这是因为，作为一名应用开发者，我经常会觉得，自己只是在和源源不断的Bug以及项目进度作斗争，日常工作好像已经无法给我带来技术上的成就感了。但我能肯定的是，我对于技术的情怀并没有消失。我也认为，我不应该只满足于完成日常的普通开发任务，而是应该去做点更有挑战性的事情，来满足自己精神上的追求。
那么，我为什么会选择学习编译技术呢？
首要的原因，是这门课的内容不像编程语言、数据结构那样清晰直观。加上在大学时期，学校安排的课时较短，只有半个学期，自己又没有对它足够重视起来，导致这门课只学了个一知半解，从而造成自己对计算机底层工作原理没有掌握透彻，留下了遗憾。
《程序员的自我修养–链接、装载与库》里，有句话让我印象深刻：“真正了不起的程序员对自己程序的每一个字节都了如指掌。”虽然说得有点夸张，但一个优秀的程序员确实应该理解自己的程序为什么能在计算机上运行起来。而不是在写完代码后，忐忑不安地看着IDE在进行编译，等终于能运行起来时，大喊一声：“哇，能编译运行了，好神奇”。所以工作了几年之后，我一直想找机会能够弥补一下大学时期的遗憾，把编译知识学扎实。
另外，编译技术的巨大挑战性，也是我想重拾学习的重要原因之一。
你可能听过一个段子：程序员的三大浪漫，是自己实现编译器、操作系统和渲染引擎。
其实一开始，我并不理解为什么编译器会在这其中，我猜大概是因为特别困难吧。
我刚接触编程的时候，觉得能学好C、Java这样的编程语言，已经很不容易了，更何况要自己去用编译器实现一门语言。
而且编译原理这门课中，还有相当多比较深奥、晦涩的理论，想要掌握起来非常困难，它完全不像学习普通技术那样，几行代码就能运行一个hello world。同时你光有理论又完全不够，编译器中包含了很多巧妙的工程化处理。比如，会用抽象语法树来表示整个代码的语法结构；在函数调用时，参数和返回值在栈帧中的内存地址顺序等。
所以，不得不说，编译器是一个非常复杂的工程，编译原理是一个非常不容易消化、掌握的基础技术，让人望而生畏。
但是一旦掌握了之后，可以说就打通了计算机技术的任督二脉，因为你已经掌握了计算机运行中相当底层部分的原理，那么再去看其他技术就不会有什么大的障碍了。
而且在工作中，即使没有什么机会需要自己去创造语言或者写编译器，编译原理对我也很有帮助。举个例子吧，Android有很多动态化的技术，像ReactNative、Weex都会使用JavaScript作为脚本语言来调用原生接口，那么为了实现原生语言（如Java、Objective-C）和JavaScript的通信，ReactNative和Weex都在框架中嵌入了JavaScriptCore这个组件，因为JavaSciptCore包含了一个JavaScript的解释器，可以在运行时执行JavaScript。
那么，如果你也想在自己的项目中使用类似的动态化技术，但又不愿意被ReactNative和Weex技术绑死，就需要自己去剖析JavaScriptCore的工作原理，甚至要去实现一个类似的脚本语言执行框架。
而真的要自己去下载JavaScriptCore的源码来看，你就会发现，如果没有一定的编译原理知识作为基础，是很难看懂的。但是如果你具备了编译原理知识基础，其实会发现，这些源码里面也不外乎就是词法分析、语法分析、IR等，这些都是编译原理中的常用概念和算法。
还有就是日常工作中会碰到的某些比较棘手的问题，如果你不理解编译技术，可能就无法找到出现问题的根源。
比如我早期的工作中，在开发C++代码的时候，经常会遇到链接时找不到符号的问题，那个时候我对“符号是怎么产生的”并不理解，所以遇到这类问题，我就会在IDE或者代码里盲目地尝试修改。
后来重新学习了编译技术之后，我就理解了编译器在编译过程中会产生符号表，每个符号包含了符号的名称、类型、地址等信息。之所以出现这类问题，可能是依赖的静态库没有包含这些符号，或者是类型不正确，再或者是这些符号的地址无法被正确解析，所以我用相应的工具去检查一下静态库文件的符号表，一般就可以解决问题了。
你看，编译技术总能帮我解决一些，我之前挠破头都想不出解决方案的问题。到现在我工作了十几年以后，就会有一个越来越强烈的感悟：最重要的还是底层知识。以前我也曾经感慨过计算机技术发展之快，各种技术层出不穷，就拿Android技术来说，各种什么插件化、组件化、动态化技术让人眼花缭乱，要不就是今天谷歌在提倡用Kotlin，明天又开始推Flutter了。
所以有一段时间我比较迷茫，“我究竟该学什么呢”，但我后来发现，虽然那些新技术层出不穷，但万变不离其宗，计算机核心的部分还是编译原理、操作系统那些底层技术。如果你对底层技术真正吃透了，再来看这些时髦的新技术，就不会感到那么神秘了，甚至是可以马上就弄明白它背后的技术原理。原理都搞懂了，那掌握起来也就非常快了。
我是怎么学习专栏的？我开始意识到自己需要重新学习编译技术是在2015年，当时为了能够在Android的原生代码里运行JavaScript脚本，我依次尝试了WebView、JavaScriptCore、Rhino等方案，觉得这种多语言混合开发的技术挺强大，也许能够改变主流的应用开发模式，于是就想继续研究这些框架是怎么工作的。但是我发现，自己对编译原理知识掌握得不牢，导致学习这些技术的时候有点无从下手。
那个时候还没有极客时间这样针对性较强的学习平台，我是自己买了些书，有权威的龙书和虎书，也有《两周自制脚本语言》这样的速成书籍，但总是不得要领。
像龙书、虎书这样的，主要花了大量的篇幅在讲理论。但面对工作和家庭的压力，又不允许我有那么大片的时间去学习理论，而且没有实践来练习的话，也很容易忘掉。
像速成书籍这样的，虽然实现了一个很简单的编译器，但它大量的篇幅是在讲代码的一些细节，原理又讲得太少，知其然而不知其所以然。后来我就没有继续深入地学下去了。
直到偶然地在极客时间上看到了《编译原理之美》这门课，我简单看了下目录之后，就立马买了下来，因为感觉非常实用，既对理论部分的重点内容有深入浅出地讲解，也有与实际问题的紧密联系。学完这门课后，我感觉有信心去尝试写一点东西了，正好临近春节，就计划着在春节期间自己写些代码来验证学习的成果。结果不成想遇到了疫情封城，因为没法复工，就索性在家自己照着龙书和课程中给出的思路，看看能否自己实现个编译器出来。
结果我花了快两个月的时间，真的写出来了一个简单的编译器，能够把类似C语言风格的代码编译成汇编执行。
回想那段时间，虽然疫情很让人焦虑，但当我全身心地投入到对编译技术的钻研时，可以暂时忘记疫情带来的困扰。通过写这个小项目，我算是对编译器的工作过程有了个切身的体会，还把多年未碰的汇编又重新拾起来投入使用，可以说是收获颇丰，疫情带给我的回忆也没有那么痛苦了，从另一个角度看，甚至还有一定的成就感。
这个简单的编译器项目完成了之后，就激发了我更大的兴趣。因为我毕竟只是实现了一个玩具型的编译器，那么工业界的编译器是如何工作的呢？
这个编译器，我是用LL算法来实现的语法分析，但龙书上还大篇幅地讲了LR、SLR、LALR，那么实际中到底是使用LL算法还是LR算法呢？
还有，我的编译器没有什么优化的功能，真实的编译器都有哪些优化措施呢？
这些问题吸引着我要去寻找答案。结果正巧，宫老师又推出了《编译原理实战课》，深入浅出地讲解各大编译器的工作原理，这可正好对我的胃口，于是又毫不犹豫地买下了。
上了几节课之后，觉得收获很大，特别是讲解javac和Graal编译器的部分。这两部分都给出了如何基于源码去剖析编译器的原理，实际操作性很强，我跟着宫老师给出的步骤，下载和一步步地调试、跟踪源码，印象十分深刻。特别是宫老师介绍的javac在处理运算符优先级时，引入了LR算法，从而避免了教科书上介绍的：当使用LL方式时，为了消除左递归，要写多级Tail函数的问题，这些都让我对编译技术的实际应用理解得更加深刻了。在学习Graal时，我也是花了不少的时间去配置Windows环境，包括下载安装Kali Linux、OpenJDK等，才终于把Graal跑起来。通过这样的实际操作，体验到了“折腾”的乐趣，对动手能力也是一种锻炼。
除此之外，课程中还有丰富的流程图、类图和思维导图，因此我可以按图索骥，去研究自己感兴趣的知识点，不用再苦苦地从海量的源码中去大海捞针，学习效率得到了很大提升。
如何更好地学习编译原理？通过这段时间的学习后，我发现，编译原理其实并没有想象中的那么困难。我觉得计算机技术的一个特点就是像在搭积木，有时候只是知识点多、层次关系复杂而已。
编译技术尤其如此，从前端到后端，从词法分析到语法分析到语义分析、IR，最后到优化。这些地方包含着各种知识点，但这些知识也不是凭空变出来的，而是环环相扣、层层叠加出来的。因此你在学习编译技术时一定要静下心来，一点一点地去吃透里面的技术细节，并且还需要不断地总结和提炼，否则对知识的理解可能就不够透彻，浮于表面。
另外，你需要多去动手实践，特别是和算法相关的部分。比如，在编译器的语法分析阶段，有个内容是计算First集合和Follow集合，其实理解起来并不困难，但它包含的细节不少，容易算错，所以需要在课后多去练习。
我就是在课下，把宫老师布置的习题和龙书上相关的题目都算了一遍，还写了计算First集合和Follow集合的程序。不过就算是做到了这些，我感觉对这部分的理解还不够透彻，但是我对编译技术的恐惧感已经消除了，对后续进一步的深入挖掘也打下了基础。所以说，静下心来学，勤动手去练，对学习编译原理这门课程来说非常重要。
我还有一个体会就是，学习编译原理没有捷径可走。因为编译技术是一个复杂而精密的工程，它的知识是环环相扣的。比如说，如果你对词法分析和语法分析的知识掌握得不够牢固，不熟悉一些常用语法规则的推导过程，那后面的IR、三地址代码、CFG等，你就会学得一头雾水。
所以，我们只能够一步一个脚印地去学习。
当然了，如果你和我一样是一个有家有口的上班族，只利用碎片时间可能不好做到连续学习，那我建议你在学习课程的时候，可以稍微花点时间去参考一下宫老师介绍的开源编译器代码，比如JavaCompiler的源码、Graal的源码。这样就可以和课程中的内容结合起来。因为看代码和调试代码会更加直观，也更容易理解。
同时，你也可以看看别人的代码设计，有哪些地方是可以做成组件的，哪些函数是可以自己去实现来练手的。然后，你可以先给自己定一个小目标，就是利用业余时间去完成这些组件或者函数的开发，因为单个组件的工作量并不是太大，因此还是可以尝试完成的。这样在巩固理论知识的同时，还能锻炼自己的动手能力，我想热爱编程的你，应该可以从中得到不少快乐。
我相信，只要最终坚持下来，你和我，都可以掌握好编译这门“屠龙”技术。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>用户故事_用好动态调试，助力课程学习</title><link>https://artisanbox.github.io/9/56/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/56/</guid><description>你好，我是leveryd。
先做个自我介绍，我在网络安全行业从事技术工作，目前在负责安全产品的研发工作，工作六年。
虽然在研发工作中，我们通常是遇到什么问题就去查，边查边学。虽然这样的学习方式能快速解决问题，但有时候这种方法也不灵，比方说学习语义分析时，就必须要把词法分析、语法分析先学了，一通搜索、查阅、汇总和学习，回头一看，需要花费的时间和精力还是不少的。
显然，只靠自己在网上搜索，学到的常常是零零散散，效率太低。尤其是和工作的关联程度很高的必修知识，我觉得不太适合边查边学，更需要系统学习。结合自己的工作需要，今年年初的时候，我给自己安排了近期学习计划，定下了相应的学习的优先级。
其中，补充操作系统的专业知识就是高优先级的一项。近期学习《操作系统实战45讲》的过程中，我也跟着课程内容开始动手实践，还在课程群里分享了自己的调试经验。接到LMOS老师的邀请，今天我就和你聊聊我是怎样学习这门课程，以及我是如何调试课程代码的。
我是怎么学习《操作系统实战45讲》的根据我的学习需求，我给自己立下了两个学习目标：
第一，理解第十三课的代码：第十三课之前的内容包括了整个机器初始化过程；
第二，理解第二十六课的代码：比第十三课内容多了“内存”和“进程”。
在这个过程中，我会遇到一些问题，我把解决这些问题的实践经验写到公众号（公众号上我记录了这门课的学习实验笔记，以及关于安全业务和技术的一些案例）上，以此加深自己的理解。
就目前我自己的学习经验来看，“内核实验”比较复杂。这主要是因为内核涉及的知识较多，比如C语言、汇编、硬件知识；而且这方面内容比较底层，某些概念我们平时接触得比较少，比如汇编层面的函数调用细节。
另外，部分算法乍一看确实有点难理解，比如第二十五课中进程的切换是利用“栈上的函数返回地址”，而“返回地址”包括初始化和后面被进程调度器更新这两种场景。我们需要弄清楚这两个场景都是怎么更新的，才能更好理解进程是如何切换运行的。
Cosmos调试思路因为刚才说的这些原因，当我们遇到疑问时，往往无法从网络上直接搜到答案。这个时候，就可以通过调试来辅助我们分析问题。
接下来，我就说一说我是怎么调试课程代码的，后面还会再分享一下我通过动态调试解决疑问的例子。
虽然我们可以在代码中打印日志，但这种方式效率不高，因为每次都需要编写代码、重新编译运行。我更喜欢用GDB和QEMU动态调试Cosmos。
结合下图中我们可以看到：使用GDB在Cosmos内核函数下了断点，并且断点生效。如果我想观察copy_pages_data的逻辑，就只需要在单步调试过程中观察内存的变化，这样就能知道copy_pages_data建立的页表数据长什么样子。
总的来说，想要动态调试，我们首先需要编译一个带调试符号的elf文件出来，然后更新hd.img镜像文件。
接着我们用QEMU启动内核，具体命令如下：
➜ myos qemu-system-x86_64 -drive format=raw,file=hd.img -m 512M -cpu kvm64,smep,smap -s // 一定要加-s参数，此参数可以打开调试服务。 最后，我们用GDB加载调试符号并调试，具体命令如下：
(gdb) symbol-file ./initldr/build/initldrkrl.elf // 加载调试符号，这样才能在显示源码、可以用函数名下断点 Reading symbols from /root/cosmos/lesson13/Cosmos/initldr/build/initldrkrl.elf...done. (gdb) target remote :1234 // 连接qemu-system-x86_64 -s选项打开的1234端口进行调试 Remote debugging using :1234 0x000000000000e82e in ?? () 我已经将编译好的带调试符号的elf文件，以及对应的hd.img镜像文件放在了GitHub上，你可以直接用这些文件和上面的命令来调试。仓库中目前我只放了对应第十三课和第二十六课的调试文件，如果你想要调试其他课的代码，不妨继续往下看。
制作“带调试符号的elf文件"的详细步骤如果你调试过Linux内核，应该比较熟悉上面的流程。不过在制作“带调试符号的elf文件”时，Cosmos和Linux内核有些不同，下面我就详细说明一下。
先说说整体思路：通过修改编译选项，即可生成“带调试符号的elf文件”。然后再生成Cosmos.eki内核文件，最后替换hd.img镜像文件中的Cosmos.eki文件。这样，我们就可以用“带调试符号的elf文件”和hd.img来调试代码了。
修复两个bug只有先修复后面这两个bug，才能成功编译，并且运行Cosmos内核代码。
第一个问题是：编译第十三课的代码时遇到一个报错，报错截图如下。
解决办法很简单：将kernel.asm文件中的“kernel.inc”修改成“/kernel.inc”，你可以对照后面的截图看一下。
第二个问题是第二十六课遇到的运行时报错，如下图所示。
因为acpi是和“电源管理”相关的模块，这里并没有用到，所以我们可以注释掉 initldr/ldrkrl/chkcpmm.c 文件中的init_acpi 函数调用。
解决掉这两个问题，就可以成功编译第十三课和第二十六课的代码了。
修改“编译选项"修复bug后，我们虽然能够成功编译运行，但是因为文件没有调试符号，所以我们在GDB调试时无法对应到c源码，也无法用函数名下断点。因此，我们需要通过修改编译选项来生成带调试符号的elf文件。
为了编译出带调试符号的执行文件，需要对编译脚本做两处修改。
第一处修改，GCC的-O2参数要修改成O0 -g参数：-O0是告诉GCC编译器，在编译时不要对代码做优化，这么做的原因是避免在GDB调试时源码和实际程序对应不上的情况；-g参数是为了告诉编译器带上调试符号。</description></item><item><title>用户故事_赵文海：怕什么真理无穷，进一寸有一寸的欢喜</title><link>https://artisanbox.github.io/4/62/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/62/</guid><description>大家好，我是赵文海，一名Android开发仔，坐标北京，目前工作刚满一年，在这里分享一下自己学习“深入浅出计算机组成原理”专栏的心得。
为什么要学计算机组成原理？一直以来我心里都有一个念想，就是好好把计算机基础知识补一补，原因有两个。
第一，我不是计算机专业的，如果连基础知识都不熟悉，那怎么与科班出身的同事交流呢？虽然我目前的工作主要是在业务层进行开发，涉及基础知识的场景其实并不多，但是，既然我要在程序员这个行业长久地走下去，我觉得自己还是有必要补一下基础知识。
第二，虽然现在各种新框架、新技术层出不穷，但它们的根基其实还是那些基础知识。我们每个人的精力有限，整天追随这些“新”的东西，在我看来并不是一个很明智的选择。相反，正所谓“磨刀不误砍柴工”，如果我把先基础知识掌握好，那学习和了解那些应用层的框架应该会更容易一点。
所以，我给自己设定了两个学习方向，一是深入学习移动端开发相关技术，比如，学习Android 系统知识、深入了解一些框架、接触 Flutter 这类跨平台技术；二是学习计算机基础知识，然后再随着工作慢慢深入去学习移动端开发技术。
正好那时候极客时间出了很多基础课程，比如王争老师的“数据结构与算法之美”、刘超老师的“趣谈网络协议”等等。我是先从数据结构与算法开始学的，后面又学了一些网络协议的知识，然后才开始学习徐文浩老师的“深入浅出计算机组成原理”。
我记得徐老师在开篇词里写过这么一段话：
正所谓“练拳不练功，到老一场空”。如果越早去弄清楚计算机的底层原理，在你的知识体系中储备这些知识，也就意味着你有越长的时间来收获学习的“利息”。虽然一开始可能不起眼，但是随着时间带来的复利效应，你的长线投资项目，就能让你在成长的过程中越走越快。
这段话和我的想法不谋而合，也给了我极大的鼓舞，我学习基础知识的决心也更加坚定了 。
我是怎么学习专栏的？刚开始看的时候，专栏已经更新了二十多讲，但我没有想要赶快跟上老师的步伐，就希望自己每天都能坚持看一讲。
我一般都是在晚上下班回家后看专栏，也有时候是早上到公司后，因为住的地方离公司比较近，还有时候一个人出去坐地铁也会拿出来看一会儿。
最开始由于我过于自信了，我想着只是把文章看了一遍就可以了。后来过了一周，我发现看完的东西，很快就没有印象了。我当时就想，这样可不行啊！不知道你是不是经常有这种感受，费很大劲搞懂的东西，结果因为只看了一遍，没有及时复习，很快就又忘记了。于是，后面每篇文章，我至少都会看两遍。第一遍认真阅读思考，第二遍、第三遍作为复习巩固。
另外，学习这个专栏时我没有做笔记，因为我觉得老师的文章不长，而且言语足够简练，没有必要自己再提炼一次。毕竟我每天都会打开极客时间，如果碰到哪里想不起来了，就直接再看一遍文章就好了，也花不了多长时间。
当然，记不记笔记是个人喜好，如果时间充裕，你也可以选择通过做笔记来加深印象。极客时间的划线笔记功能也是极好的，看到有问题或者非常好的地方，直接记录下来，方便以后查阅、学习。
就这样，保持一天一节的速度，慢慢我就赶上了老师更新的步伐，后面的文章基本就是更新当天就看完。
虽然进度跟上了，但是老师文章后面附的书籍我目前并没有去读。作为一个非科班出身的工作党，平时时间并不宽裕，掌握老师每一节的主要知识，已经挺不容易了。
这里特别说一下，徐老师每篇文章下的推荐阅读，是我个人最喜欢这个专栏的地方。徐老师每次都会在文章结尾列出相关书籍的对应章节、相关的博客或论文，这为我后面深入学习相关知识提供了很大的便利。
因此，关于这一块内容我是这么打算的。我准备学完第一遍之后，仔细去读一读老师推荐的书籍。这样有了第一遍的铺垫，读起老师推荐的书和论文，不至于那么困难和恐惧。读书的时候还可以结合书中内容再复习一遍专栏，到时候肯定会有新的收获。毕竟，基础知识的学习是一个长期积累、慢慢参悟、螺旋上升的过程，我已经做好了打持久战的准备。
学习专栏有什么收获？徐老师的文章长度适中、图文并茂、言简意赅。在解释一些名词和概念的时候，徐老师经常拿生活中我们熟悉的事物来举例。我觉得这一点非常好。
比如，他把电路组装看成“搭乐高积木”、把动态链接比喻成程序内部的“共享单车”、把总线比喻成计算机里的“高速公路”，这很容易让我这种非科班的同学，对陌生概念迅速建立起一个初步印象。当然，能把概念解释地如此清晰和“接地气”，也反映了老师的深厚功底。
通过专栏的学习，我对计算机的CPU、内存、I/O设备以及它们之间的通信有了初步的了解。
另外，像 GPU、TPU相关的章节也让我开拓了眼界，比如 GPU 那一节老师就讲到了计算机图形渲染的流程，这些知识是我之前从未接触过的。
同时，专栏还有很多很实用的章节，比如讲 CPU 的高速缓存时，讲到了 Java 中的 volatile 关键字的作用，这些都可以直接运用到实际的工作或面试中。
不过，除了计算机组成原理的知识外，我还有其他的收获，在我看来这些收获甚至比那些知识还重要。
首先，就是克服了对于基础知识的恐惧。
我之前觉得基础知识是晦涩难懂的，像计算机组成原理、网络协议、操作系统，这些课听起来就觉得很难。开始学习之前，心里总是怕自己理解不了或者坚持不下来，但是通过学习专栏，我发现它们并没有想象中那么可怕，很多技术灵感其实就是源于我们的生活实践。先去学，然后慢慢就能发现其中有趣的地方。
比如，CPU分支预测就和我们天气预测有相似之处。文章里穿插的历史知识也让我意识到，这些知识虽然看似高深，但也是无数前辈经历很长时间、很多次失败才慢慢积累下来的，学习这些知识，就是站在巨人的肩膀上，体会他们思考和实践的过程。
还有就是，我意识到了持续学习的重要性。
徐老师在第2讲时写过，“我工作之后一直在持续学习，在这个过程中，我发现最有效的方法，不是短时间冲刺，而是有节奏的坚持。”我对这句话真是深有感触，所以一直记到现在，估计你也是吧？
总结通过专栏的学习，我确实收获了很多，真的非常感谢徐文浩老师的付出，也感谢极客时间推出了这么多实用的基础课程。
其实，尽管毕业之后工作才半年左右，但是我的心里其实挺焦虑的，主要是担心自己在如此快速的技术变革中，跟不上变化，慢慢被淘汰。在这个过程中，我也思考了很多。通过学习专栏，我的焦虑情绪也化解了很多。在这里我也想说说我对于焦虑的看法。
我觉得人之所以会感到焦虑，是因为有上进心，说白了就是觉得自己不够好。这其实是一件好事，正是因为觉得自己不够好，我们才能产生变得更好的想法，进而找到变得更好的方法，所以我们要正确看待焦虑这种情绪。而缓解焦虑的方式很简单，就是行动。担心自己长胖，那就去锻炼；担心自己被淘汰，那就去学习。
我特别喜欢胡适说的一句话：“怕什么真理无穷，进一寸有一寸的欢喜”。
用这句话与各位共勉吧，希望我们都能抱着长线投资的心态，坚持下去，和时间做朋友，不要急躁。虽然学完这些基础知识，老板也看不到，短时间内也不会加薪升职，但我相信，它们会在未来的某个时间回馈你，让你知道现在的决定是正确的，现在的付出是值得的！
好了，我想要分享的内容就是这些，不知道你学习这个专栏的过程是怎样的呢？有没有什么独特的学习方法和心路历程呢？欢迎你写在留言区，我们一起分享，相互鼓励，共同进步！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>直播回顾_林晓斌：我的MySQL心路历程</title><link>https://artisanbox.github.io/1/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/48/</guid><description>在专栏上线后的11月21日，我来到极客时间做了一场直播，主题就是“我的MySQL心路历程”。今天，我特意将这个直播的回顾文章，放在了专栏下面，希望你可以从我这些年和MySQL打交道的经历中，找到对你有所帮助的点。
这里，我先和你说一下，在这个直播中，我主要分享的内容：
我和MySQL打交道的经历；
你为什么要了解数据库原理；
我建议的MySQL学习路径；
DBA的修炼之道。
我的经历以丰富的经历进入百度我是福州大学毕业的，据我了解，那时候我们学校的应届生很难直接进入百度，都要考到浙江大学读个研究生才行。没想到的是，我投递了简历后居然进了面试。
入职以后，我跑去问当时的面试官，为什么我的简历可以通过筛选？他们说：“因为你的简历厚啊”。我在读书的时候，确实做了很多项目，也实习过不少公司，所以简历里面的经历就显得很丰富了。
在面试的时候，有个让我印象很深刻的事儿。面试官问我说，你有这么多实习经历，有没有什么比较好玩儿的事？我想了想答道，跟你说个数据量很大的事儿 ，在跟移动做日志分析的时候我碰到了几千万行的数据。他听完以后就笑了。
后来，我进了百度才知道，几千万行那都是小数据。
开始尝试看源码解决问题加入百度后，我是在贴吧做后端程序，比如权限系统等等。其实很简单，就是写一个C语言程序，响应客户端请求，然后返回结果。
那个时候，我还仅仅是个MySQL的普通用户，使用了一段时间后就出现问题了：一个跑得很快的请求，偶尔会又跑得非常慢。老板问这是什么原因，而我又不好意思说不知道，于是就自己上网查资料。
但是，2008年那会儿，网上资料很少，花了挺长时间也没查出个所以然。最终，我只好去看源码。翻到源码，我当时就觉得它还蛮有意思的。而且，源码真的可以帮我解决一些问题。
于是一发不可收拾，我从那时候就入了源码的“坑”。
混社区分享经验2010年的时候，阿里正好在招数据库的开发人员。虽然那时我还只是看得懂源码，没有什么开发经验，但还是抱着试试看的态度投了简历。然后顺利通过了面试，成功进入了阿里。之后，我就跟着褚霸（霸爷）干了7年多才离开了阿里。
在百度的时候，我基本上没有参加过社区活动。因为那时候百度可能更提倡内部分享，解决问题的经验基本上都是在内网分享。所以，去了阿里以后，我才建了博客、开了微博。我在阿里的花名叫丁奇，博客、微博、社区也因此都是用的这个名字。
为什么要了解数据库原理？这里，我讲几个亲身经历的事情，和你聊聊为什么要了解数据库原理。
了解原理能帮你更好地定位问题一次同学聚会，大家谈起了技术问题。一个在政府里的同学说，他们的系统很奇怪，每天早上都得重启一下应用程序，否则就提示连接数据库失败，他们都不知道该怎么办。
我分析说，按照这个错误提示，应该就是连接时间过长了，断开了连接。数据库默认的超时时间是8小时，而你们平时六点下班，下班之后系统就没有人用了，等到第二天早上九点甚至十点才上班，这中间的时间已经超过10个小时了，数据库的连接肯定就会断开了。
我当时说，估计这个系统程序写得比较差，连接失败也不会重连，仍然用原来断掉的连接，所以就报错了。然后，我让他回去把超时时间改得长一点。后来他跟我说，按照这个方法，问题已经解决了。
由此，我也更深刻地体会到，作为开发人员，即使我们只知道每个参数的意思，可能就可以给出一些问题的正确应对方法。
了解原理能让你更巧妙地解决问题我在做贴吧系统的时候，每次访问页面都要请求一次权限。所以，这个请求权限的请求，访问概率会非常高，不可能每次都去数据库里查，怎么办呢？
我想了个简单的方案：在应用程序里面开了个很大的内存，启动的时候就把整张表全部load到内存里去。这样再有权限请求的时候，直接从内存里取就行了。
数据库重启时，我的进程也会跟着重启，接下来就会到数据表里面做全表扫描，把整个用户相关信息全部塞到内存里面去。
但是，后来我遇到了一个很郁闷的情况。有时候MySQL 崩溃了，我的程序重新加载权限到内存里，结果这个select语句要执行30分钟左右。本来MySQL正常重启一下是很快的，进程重启也很快，正常加载权限的过程只需要两分钟就跑完了。但是，为什么异常重启的时候就要30分钟呢？
我没辙了，只好去看源码。然后，我发现MySQL有个机制，当它觉得系统空闲时会尽量去刷脏页。
具体到我们的例子里，MySQL重启以后，会执行我的进程做全表扫描，但是因为这个时候权限数据还没有初始化完成，我的Server层不可能提供服务，于是MySQL里面就只有我那一个select全表扫描的请求，MySQL就认为现在很闲，开始拼命地刷脏页，结果就吃掉了大量的磁盘资源，导致我的全表扫描也跑得很慢。
知道了这个机制以后，我就写了个脚本，每隔0.5秒发一个请求，执行一个简单的SQL查询，告诉数据库其实我现在很忙，脏页刷得慢一点。
脚本一发布使用，脏页果然刷得慢了，加载权限的扫描也跑得很快了。据说我离职两年多以后，这个脚本还在用。
你看，如果我们懂得一些参数，并可以理解这些参数，就可以做正确的设置了。而如果我们进一步地懂得一些原理，就可以更巧妙地解决问题了。
看得懂源码让你有更多的方法2012年的时候，阿里双十一业务的压力比较大。当时还没有这么多的SSD，是机械硬盘的时代。
为了应对压力我们开始引入SSD，但是不敢把SSD直接当存储用，而是作为二级缓存。当时，我们用了一个叫作Flashcache的开源系统（现在已经是老古董级别了，不知道你有没有听过这个系统）。
Flashcache实现，把SSD当作物理盘的二级缓存，可以提升性能。但是，我们自己部署后发现性能提升的效果没有预想的那么好，甚至还不如纯机械盘。
于是，我跟霸爷就开始研究。霸爷负责分析Flashcache的源码，我负责分析MySQL源码。后来我们发现Flashcache是有脏页比例的，当脏页比例到了80%就会停下来强行刷盘。
一开始我们以为这个脏页比例是全部的20%，看了源码才知道，原来它分了很多个桶，比如说一个桶20M，这个桶如果用完80%，它就认为脏页满了，就开始刷脏页。这也就意味着，如果你是顺序写的话，很容易就会把一个桶写满。
知道了这个原理以后，我就把日志之类顺序写的数据全都放到了机械硬盘，把随机写的数据放到了Flashcache上。这样修改以后，效果就好了。
你看，如果能看得懂源码，你的操作行为就会不一样。
MySQL学习路径说到MySQL的学习路径，其实我上面分享的这些内容，都可以归结为学习路径。
首先你要会用，要去了解每个参数的意义，这样你的运维行为（使用行为）就会不一样。千万不要从网上拿了一些使用建议，别人怎么用，你就怎么用，而不去想为什么。再往后，就要去了解每个参数的实现原理。一旦你了解了这些原理，你的操作行为就会不一样。 再进一步，如果看得懂源码，那么你对数据库的理解也会不一样。
再来讲讲我是怎么带应届生的。实践是很好的学习方式，所以我会让新人来了以后先搭主备，然后你就会发现每个人的自学能力都不一样。比如遇到有延迟，或者我们故意构造一个主备数据不一致的场景，让新人了解怎么分析问题，解决问题。
如果一定要总结出一条学习路径的话，那首先要会用，然后可以发现问题。
在专栏里面，我在每篇文章末尾，都会提出一个常见问题，作为思考题。这些问题都不会很难，是跟专栏文章挂钩、又是会经常遇到的，但又无法直接从文章里拿到答案。
我的建议是，你可以尝试先不看答案自己去思考，或者去数据库里面翻一翻，这将会是一个不错的过程。
再下一步就是实践。之后当你觉得开始有一些“线”的概念了，再去看MySQL的官方手册。在我的专栏里，有人曾问我要不要直接去看手册？
我的建议是，一开始千万不要着急看手册，这里面有100多万个英文单词，你就算再厉害，也是看了后面忘了前面。所以，你一定要自己先有脉络，然后有一个知识网络，再看手册去查漏补缺。
我自己就是这么一路走过来的。
另外，在专栏的留言区，很多用户都希望我能推荐一本书搭配专栏学习。如果只推荐一本的话，我建议你读一下《高性能MySQL》这本书，它是MySQL这个领域的经典图书，已经出到第三版了，你可以想象一下它的流行度。
这本书的其中两位译者（彭立勋、翟卫祥）是我原团队的小伙伴，有着非常丰富的MySQL源码开发经验，他们对MySQL的深刻理解，让这本书保持了跟原作英文版同样高的质量。
极客时间的编辑说，他们已经和出版社沟通，为我们专栏的用户争取到了全网最低价，仅限3天，你可以直接点击链接购买。
DBA的修炼DBA和开发工程师有什么相同点？我带过开发团队，也带过DBA团队，所以可以分享一下这两个岗位的交集。
其实，DBA本身要有些开发底子，比如说做运维系统的开发。另外，自动化程度越高，DBA的日常运维工作量就越少，DBA得去了解开发业务逻辑，往业务架构师这个方向去做。
开发工程师也是一样，不能所有的问题都指望DBA来解决。因为，DBA在每个公司都是很少的几个人。所以，开发也需要对数据库原理有一定的了解，这样向DBA请教问题时才能更专业，更高效地解决问题。
所以说，这两个岗位应该有一定程度的融合，即：开发要了解数据库原理，DBA要了解业务和开发。
DBA有前途吗？这里我要强调的是，每个岗位都有前途，只需要根据时代变迁稍微调整一下方向。
像原来开玩笑说DBA要体力好，因为得搬服务器。后来DBA的核心技能成了会搭库、会主备切换，但是现在这些也不够用了，因为已经有了自动化系统。
所以，DBA接下来一方面是要了解业务，做业务的架构师；另一方面，是要有前瞻性，做主动诊断系统，把每个业务的问题挑出来做成月报，让业务开发去优化，有不清楚的地方，开发同学会来找你咨询。你帮助他们做好了优化之后，可以把优化的指标呈现出来。这将很好地体现出你对于公司的价值。
有哪些比较好的习惯和提高SQL效率的方法？这个方法，总结起来就是：要多写SQL，培养自己对SQL语句执行效率的感觉。以后再写或者建索引的时候，知道这个语句执行下去大概的时间复杂度，是全表扫描还是索引扫描、是不是需要回表，在心里都有一个大概的概念。
这样每次写出来的SQL都会快一点，而且不容易犯低级错误。这也正式我开设这个专栏的目标。</description></item><item><title>知识地图_一起来复习编译技术核心概念与算法</title><link>https://artisanbox.github.io/7/55/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/55/</guid><description>你好，我是学习委员朱英达。
在“预备知识篇”这个模块，宫老师系统地梳理了编译过程中各个阶段的核心要点，目的就是让我们建立一个编译原理的基础知识体系。那到今天为止，我们就学完了这部分内容，迈出了编译之旅中扎实的第一步。不知道你对这些知识掌握得怎样了？
为了复习，也为了检测我们的学习成果，我根据自己的知识积累和学习情况，整理了一张知识大地图，你可以根据这张地图中标记的七大编译阶段，随时速查常用的编译原理概念和关键算法。
如果你也总结了知识地图，那你可以对照着我这个，给自己一个反馈，看看它们之间有哪些异同点，我们可以在留言区中一起交流和讨论。
不过知识地图的形式，虽然便于你保存、携带、速查，但考虑到图中涉及的概念等内容较多，不方便查看和检索。所以，我还把地图上的知识点，用文字的形式帮你梳理出来了。你可以对照着它，来复习和回顾编译技术的核心概念和算法的知识点，构建自己的知识框架。
你在学习这些预备知识的过程中，可能会发现，宫老师并没有非常深入地讲解编译原理的具体概念、理论和算法。所以，如果你想继续深入学习这些基础知识，可以根据宫老师在每讲最后给出的参考资料，去学习龙书、虎书、鲸书等经典编译原理书籍。当然，你也可以去看看宫老师的第一季专栏课《编译原理之美》。
在我看来，相较于编译方面的教科书而言，《编译原理之美》这门课的优势在于，更加通俗易懂、与时俱进，既可以作为新手的起步指导，也能够帮助已经熟悉编译技术的工程师扩展视野，我很推荐你去学习这门课。所以，我邀请编辑添加了相应的知识点到《编译原理之美》的文章链接，如果你有深入学习的需要，你会很方便地找到它。
好了，一起开始复习吧！
一、词法分析：根据词法规则，把字符串转换为Token核心概念：正则文法 正则文法：词法分析工作的主要文法，它涉及的重要概念是正则表达式。 正则表达式：正则文法的一种直观描述，它是按照指定模式匹配字符串的一种字符组合。 正则表达式工具：字符串的模式匹配工具。大多数程序语言都内置了正则表达式的匹配方法，也可以借助一些编译工具，自动化根据正则表达式生成字符串匹配程序，例如C++的Lex/Yacc以及Java的ANTLR。 具体实现：手工构造词法分析器、自动生成词法分析器手工构造词法分析器
构造词法分析器使用的计算模型：有限自动机（FSA）。它是用于识别正则文法的一种程序实现方案。 其组成的词法单元是Token，也就是指程序中标记出来的单词和标点符号，它可以分成关键字、标识符、字面量、操作符号等多个种类。 在实际的编译器中，词法分析器一般都是手写的。 自动生成词法分析器
具体实现思路：把一个正则表达式翻译成NFA，然后把NFA转换成DFA。 DFA：确定的有限自动机。它的特点是：该状态机在任何一个状态，基于输入的字符，都能做一个确定的状态转换。 NFA：不确定的有限自动机。它的特点是：该状态机中存在某些状态，针对某些输入，不能做一个确定的转换。这里可以细分成两种情况：一种是对于一个输入，它有两个状态可以转换；另一种是存在ε转换的情况，也就是没有任何字符输入的情况下，NFA也可以从一个状态迁移到另一个状态。 技术难点首先，你需要注意，NFA和DFA都有各自的优缺点，以及不同的适用场景。
NFA：优点是在设计上更简单直观，缺点是它无法避免回溯问题，在某些极端的情况下可能会造成编译器运行的性能低下。主要适用于状态较为简单，且不存在回溯的场景。 DFA：优点是它可以避免回溯问题，运行性能较高，缺点是DFA通常不容易直接设计出来，需要通过一系列方案，基于NFA的转换而得到，并且需要占用额外的空间。主要适用于状态较为复杂，或者对时间复杂度要求较为严苛的工业级词法分析器。 其次，你需要了解基于正则表达式构造NFA，再去进行模式匹配的算法思路。
从正则表达式到NFA：这是自动生成词法分析器的一种算法思路。它的翻译方法是，匹配一个字符i —&amp;gt;匹配“或”模式s|t —&amp;gt; 匹配“与”模式st —&amp;gt; 重复模式，如“?”“*”和“+”等符号，它们的意思是可以重复0次、0到多次、1到多次，注意在转换时要增加额外的状态和边。 从NFA到DFA：NFA的运行可能导致大量的回溯，所以我们可以把NFA转换成DFA，让字符串的匹配过程更简单。从NFA转换成DFA的算法是子集构造法，具体的算法思路你可以参考第16讲。 二、语法分析：依据语法规则，编写语法分析程序，把 Token 串转化成 AST核心概念：上下文无关文法 上下文无关的意思：在任何情况下，文法的推导规则都是一样的。 语法规则由4个部分组成：一个有穷的非终结符（或变元）的集合、一个有穷的终结符的集合、一个有穷的产生式集合、一个起始非终结符（变元）。符合这四个特点的文法规则就是上下文无关文法。 两种描述形式：一种是巴科斯范式（BNF），另一种是巴科斯范式的一种扩展形式（EBNF），它更利于自动化生成语法分析器。其中，产生式、终结符、非终结符、开始符号是巴科斯范式的基本组成要素。 上下文无关文法与正则文法的区别：上下文无关文法允许递归调用，而正则文法不允许。上下文无关文法比正则文法的表达能力更强，正则文法是上下文无关文法的一个子集。 具体实现：自顶向下、自底向上一种是自顶向下的算法思路，它是指从根节点逐层往下分解，形成最后的AST。
递归下降算法：它的算法思路是按照语法规则去匹配Token串。优点：程序结构基本上是跟文法规则同构的。缺点：会造成左递归和回溯问题。注意，递归下降是深度优先（DFS）的，只有最左边的子树都生成完了，才会往右生成它的兄弟节点。 LL算法：对于一些比较复杂的语法规则来说，这个算法可以自动计算出选择不同产生式的依据。方法：从左到右地消化掉 Token。要点：计算 First 和 Follow 集合。 另一种是自底向上的算法思路，它是指从底下先拼凑出AST的一些局部拼图，并逐步组装成一棵完整的AST。
自底向上的语法分析思路：移进，把token加入工作区；规约，在工作区内组装AST的片段。 LR算法和 LL 算法一样，也是从左到右地消化掉 Token。 技术难点首先，你需要掌握LL算法的要点，也就是计算First和Follow集合。
其次，你要了解LL算法与LR算法的异同点。
LL算法：优点是较为直观、容易实现，缺点是在一些情况下不得不处理左递归消除和提取左因子问题。 LR算法：优点是不怕左递归，缺点是缺少完整的上下文信息，编译错误显示不友好。 三、语义分析：检查程序是否符合语义规则，并为后续的编译工作收集语义信息核心概念：上下文相关文法 属性文法：上下文相关文法对EBNF进行了扩充，在上下文无关的推导过程中，辅助性解决一些上下文相关的问题。 注意：上下文相关文法没有像状态图、BNF那样直观的分析范式。 应用场景：控制流检查、闭包分析、引用消解等。 场景案例1.控制流检查</description></item><item><title>第10讲_UDP协议：因性善而简单，难免碰到“城会玩”</title><link>https://artisanbox.github.io/5/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/1/</guid><description>讲完了IP层以后，接下来我们开始讲传输层。传输层里比较重要的两个协议，一个是TCP，一个是UDP。对于不从事底层开发的人员来讲，或者对于开发应用的人来讲，最常用的就是这两个协议。由于面试的时候，这两个协议经常会被放在一起问，因而我在讲的时候，也会结合着来讲。
TCP和UDP有哪些区别？ 一般面试的时候我问这两个协议的区别，大部分人会回答，TCP是面向连接的，UDP是面向无连接的。
什么叫面向连接，什么叫无连接呢？在互通之前，面向连接的协议会先建立连接。例如，TCP会三次握手，而UDP不会。为什么要建立连接呢？你TCP三次握手，我UDP也可以发三个包玩玩，有什么区别吗？
所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。
例如，TCP提供可靠交付。通过TCP连接传输的数据，无差错、不丢失、不重复、并且按序到达。我们都知道IP包是没有任何可靠性保证的，一旦发出去，就像西天取经，走丢了、被妖怪吃了，都只能随它去。但是TCP号称能做到那个连接维护的程序做的事情，这个下两节我会详细描述。而UDP继承了IP包的特性，不保证不丢失，不保证按顺序到达。
再如，TCP是面向字节流的。发送的时候发的是一个流，没头没尾。IP包可不是一个流，而是一个个的IP包。之所以变成了流，这也是TCP自己的状态维护做的事情。而UDP继承了IP的特性，基于数据报的，一个一个地发，一个一个地收。
还有TCP是可以有拥塞控制的。它意识到包丢弃了或者网络的环境不好了，就会根据情况调整自己的行为，看看是不是发快了，要不要发慢点。UDP就不会，应用让我发，我就发，管它洪水滔天。
因而TCP其实是一个有状态服务，通俗地讲就是有脑子的，里面精确地记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点儿都不行。而UDP则是无状态服务。通俗地说是没脑子的，天真无邪的，发出去就发出去了。
我们可以这样比喻，如果MAC层定义了本地局域网的传输行为，IP层定义了整个网络端到端的传输行为，这两层基本定义了这样的基因：网络传输是以包为单位的，二层叫帧，网络层叫包，传输层叫段。我们笼统地称为包。包单独传输，自行选路，在不同的设备封装解封装，不保证到达。基于这个基因，生下来的孩子UDP完全继承了这些特性，几乎没有自己的思想。
UDP包头是什么样的？ 我们来看一下UDP包头。
前面章节我已经讲过包的传输过程，这里不再赘述。当我发送的UDP包到达目标机器后，发现MAC地址匹配，于是就取下来，将剩下的包传给处理IP层的代码。把IP头取下来，发现目标IP匹配，接下来呢？这里面的数据包是给谁呢？
发送的时候，我知道我发的是一个UDP的包，收到的那台机器咋知道的呢？所以在IP头里面有个8位协议，这里会存放，数据里面到底是TCP还是UDP，当然这里是UDP。于是，如果我们知道UDP头的格式，就能从数据里面，将它解析出来。解析出来以后呢？数据给谁处理呢？
处理完传输层的事情，内核的事情基本就干完了，里面的数据应该交给应用程序自己去处理，可是一台机器上跑着这么多的应用程序，应该给谁呢？
无论应用程序写的使用TCP传数据，还是UDP传数据，都要监听一个端口。正是这个端口，用来区分应用程序，要不说端口不能冲突呢。两个应用监听一个端口，到时候包给谁呀？所以，按理说，无论是TCP还是UDP包头里面应该有端口号，根据端口号，将数据交给相应的应用程序。
当我们看到UDP包头的时候，发现的确有端口号，有源端口号和目标端口号。因为是两端通信嘛，这很好理解。但是你还会发现，UDP除了端口号，再没有其他的了。和下两节要讲的TCP头比起来，这个简直简单得一塌糊涂啊！
UDP的三大特点 UDP就像小孩子一样，有以下这些特点：
第一，沟通简单，不需要一肚子花花肠子（大量的数据结构、处理逻辑、包头字段）。前提是它相信网络世界是美好的，秉承性善论，相信网络通路默认就是很容易送达的，不容易被丢弃的。
第二，轻信他人。它不会建立连接，虽然有端口号，但是监听在这个地方，谁都可以传给他数据，他也可以传给任何人数据，甚至可以同时传给多个人数据。
第三，愣头青，做事不懂权变。不知道什么时候该坚持，什么时候该退让。它不会根据网络的情况进行发包的拥塞控制，无论网络丢包丢成啥样了，它该怎么发还怎么发。
UDP的三大使用场景 基于UDP这种“小孩子”的特点，我们可以考虑在以下的场景中使用。
第一，需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用。这很好理解，就像如果你是领导，你会让你们组刚毕业的小朋友去做一些没有那么难的项目，打一些没有那么难的客户，或者做一些失败了也能忍受的实验性项目。
我们在第四节讲的DHCP就是基于UDP协议的。一般的获取IP地址都是内网请求，而且一次获取不到IP又没事，过一会儿还有机会。我们讲过PXE可以在启动的时候自动安装操作系统，操作系统镜像的下载使用的TFTP，这个也是基于UDP协议的。在还没有操作系统的时候，客户端拥有的资源很少，不适合维护一个复杂的状态机，而且因为是内网，一般也没啥问题。
第二，不需要一对一沟通，建立连接，而是可以广播的应用。咱们小时候人都很简单，大家在班级里面，谁成绩好，谁写作好，应该表扬谁惩罚谁，谁得几个小红花都是当着全班的面讲的，公平公正公开。长大了人心复杂了，薪水、奖金要背靠背，和员工一对一沟通。
UDP的不面向连接的功能，可以使得可以承载广播或者多播的协议。DHCP就是一种广播的形式，就是基于UDP协议的，而广播包的格式前面说过了。
对于多播，我们在讲IP地址的时候，讲过一个D类地址，也即组播地址，使用这个地址，可以将包组播给一批机器。当一台机器上的某个进程想监听某个组播地址的时候，需要发送IGMP包，所在网络的路由器就能收到这个包，知道有个机器上有个进程在监听这个组播地址。当路由器收到这个组播地址的时候，会将包转发给这台机器，这样就实现了跨路由器的组播。
在后面云中网络部分，有一个协议VXLAN，也是需要用到组播，也是基于UDP协议的。
第三，需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候。记得曾国藩建立湘军的时候，专门招出生牛犊不怕虎的新兵，而不用那些“老油条”的八旗兵，就是因为八旗兵经历的事情多，遇到敌军不敢舍死忘生。
同理，UDP简单、处理速度快，不像TCP那样，操这么多的心，各种重传啊，保证顺序啊，前面的不收到，后面的没法处理啊。不然等这些事情做完了，时延早就上去了。而TCP在网络不好出现丢包的时候，拥塞控制策略会主动的退缩，降低发送速度，这就相当于本来环境就差，还自断臂膀，用户本来就卡，这下更卡了。
当前很多应用都是要求低时延的，它们可不想用TCP如此复杂的机制，而是想根据自己的场景，实现自己的可靠和连接保证。例如，如果应用自己觉得，有的包丢了就丢了，没必要重传了，就可以算了，有的比较重要，则应用自己重传，而不依赖于TCP。有的前面的包没到，后面的包到了，那就先给客户展示后面的嘛，干嘛非得等到齐了呢？如果网络不好，丢了包，那不能退缩啊，要尽快传啊，速度不能降下来啊，要挤占带宽，抢在客户失去耐心之前到达。
由于UDP十分简单，基本啥都没做，也就给了应用“城会玩”的机会。就像在和平年代，每个人应该有独立的思考和行为，应该可靠并且礼让；但是如果在战争年代，往往不太需要过于独立的思考，而需要士兵简单服从命令就可以了。
曾国藩说哪支部队需要诱敌牺牲，也就牺牲了，相当于包丢了就丢了。两军狭路相逢的时候，曾国藩说上，没有带宽也要上，这才给了曾国藩运筹帷幄，城会玩的机会。同理如果你实现的应用需要有自己的连接策略，可靠保证，时延要求，使用UDP，然后再应用层实现这些是再好不过了。
基于UDP的“城会玩”的五个例子 我列举几种“城会玩”的例子。
“城会玩”一：网页或者APP的访问 原来访问网页和手机APP都是基于HTTP协议的。HTTP协议是基于TCP的，建立连接都需要多次交互，对于时延比较大的目前主流的移动互联网来讲，建立一次连接需要的时间会比较长，然而既然是移动中，TCP可能还会断了重连，也是很耗时的。而且目前的HTTP协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是TCP的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。
而QUIC（全称Quick UDP Internet Connections，快速UDP互联网连接）是Google提出的一种基于UDP改进的通信协议，其目的是降低网络通信的延迟，提供更好的用户互动体验。
QUIC在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制，是应用层“城会玩”的代表。这一节主要是讲UDP，QUIC我们放到应用层去讲。
“城会玩”二：流媒体的协议 现在直播比较火，直播协议多使用RTMP，这个协议我们后面的章节也会讲，而这个RTMP协议也是基于TCP的。TCP的严格顺序传输要保证前一个收到了，下一个才能确认，如果前一个收不到，下一个就算包已经收到了，在缓存里面，也需要等着。对于直播来讲，这显然是不合适的，因为老的视频帧丢了其实也就丢了，就算再传过来用户也不在意了，他们要看新的了，如果老是没来就等着，卡顿了，新的也看不了，那就会丢失客户，所以直播，实时性比较比较重要，宁可丢包，也不要卡顿的。
另外，对于丢包，其实对于视频播放来讲，有的包可以丢，有的包不能丢，因为视频的连续帧里面，有的帧重要，有的不重要，如果必须要丢包，隔几个帧丢一个，其实看视频的人不会感知，但是如果连续丢帧，就会感知了，因而在网络不好的情况下，应用希望选择性的丢帧。
还有就是当网络不好的时候，TCP协议会主动降低发送速度，这对本来当时就卡的看视频来讲是要命的，应该应用层马上重传，而不是主动让步。因而，很多直播应用，都基于UDP实现了自己的视频传输协议。
“城会玩”三：实时游戏 游戏有一个特点，就是实时性比较高。快一秒你干掉别人，慢一秒你被别人爆头，所以很多职业玩家会买非常专业的鼠标和键盘，争分夺秒。
因而，实时游戏中客户端和服务端要建立长连接，来保证实时传输。但是游戏玩家很多，服务器却不多。由于维护TCP连接需要在内核维护一些数据结构，因而一台机器能够支撑的TCP连接数目是有限的，然后UDP由于是没有连接的，在异步IO机制引入之前，常常是应对海量客户端连接的策略。
另外还是TCP的强顺序问题，对战的游戏，对网络的要求很简单，玩家通过客户端发送给服务器鼠标和键盘行走的位置，服务器会处理每个用户发送过来的所有场景，处理完再返回给客户端，客户端解析响应，渲染最新的场景展示给玩家。
如果出现一个数据包丢失，所有事情都需要停下来等待这个数据包重发。客户端会出现等待接收数据，然而玩家并不关心过期的数据，激战中卡1秒，等能动了都已经死了。
游戏对实时要求较为严格的情况下，采用自定义的可靠UDP协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。
“城会玩”四：IoT物联网 一方面，物联网领域终端资源少，很可能只是个内存非常小的嵌入式系统，而维护TCP协议代价太大；另一方面，物联网对实时性要求也很高，而TCP还是因为上面的那些原因导致时延大。Google旗下的Nest建立Thread Group，推出了物联网通信协议Thread，就是基于UDP协议的。
“城会玩”五：移动通信领域 在4G网络里，移动流量上网的数据面对的协议GTP-U是基于UDP的。因为移动网络协议比较复杂，而GTP协议本身就包含复杂的手机上线下线的通信协议。如果基于TCP，TCP的机制就显得非常多余，这部分协议我会在后面的章节单独讲解。
小结 好了，这节就到这里了，我们来总结一下：
如果将TCP比作成熟的社会人，UDP则是头脑简单的小朋友。TCP复杂，UDP简单；TCP维护连接，UDP谁都相信；TCP会坚持知进退；UDP愣头青一个，勇往直前；
UDP虽然简单，但它有简单的用法。它可以用在环境简单、需要多播、应用层自己控制传输的地方。例如DHCP、VXLAN、QUIC等。
最后，给你留两个思考题吧。</description></item><item><title>第11讲_TCP协议（上）：因性恶而复杂，先恶后善反轻松</title><link>https://artisanbox.github.io/5/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/2/</guid><description>上一节，我们讲的UDP，基本上包括了传输层所必须的端口字段。它就像我们小时候一样简单，相信“网之初，性本善，不丢包，不乱序”。
后来呢，我们都慢慢长大，了解了社会的残酷，变得复杂而成熟，就像TCP协议一样。它之所以这么复杂，那是因为它秉承的是“性恶论”。它天然认为网络环境是恶劣的，丢包、乱序、重传，拥塞都是常有的事情，一言不合就可能送达不了，因而要从算法层面来保证可靠性。
TCP包头格式我们先来看TCP头的格式。从这个图上可以看出，它比UDP复杂得多。
首先，源端口号和目标端口号是不可少的，这一点和UDP是一样的。如果没有这两个端口号。数据就不知道应该发给哪个应用。
接下来是包的序号。为什么要给包编号呢？当然是为了解决乱序的问题。不编好号怎么确认哪个应该先来，哪个应该后到呢。编号是为了解决乱序问题。既然是社会老司机，做事当然要稳重，一件件来，面临再复杂的情况，也临危不乱。
还应该有的就是确认序号。发出去的包应该有确认，要不然我怎么知道对方有没有收到呢？如果没有收到就应该重新发送，直到送达。这个可以解决不丢包的问题。作为老司机，做事当然要靠谱，答应了就要做到，暂时做不到也要有个回复。
TCP是靠谱的协议，但是这不能说明它面临的网络环境好。从IP层面来讲，如果网络状况的确那么差，是没有任何可靠性保证的，而作为IP的上一层TCP也无能为力，唯一能做的就是更加努力，不断重传，通过各种算法保证。也就是说，对于TCP来讲，IP层你丢不丢包，我管不着，但是我在我的层面上，会努力保证可靠性。
这有点像如果你在北京，和客户约十点见面，那么你应该清楚堵车是常态，你干预不了，也控制不了，你唯一能做的就是早走。打车不行就改乘地铁，尽力不失约。
接下来有一些状态位。例如SYN是发起一个连接，ACK是回复，RST是重新连接，FIN是结束连接等。TCP是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。
不像小时候，随便一个不认识的小朋友都能玩在一起，人大了，就变得礼貌，优雅而警觉，人与人遇到会互相热情的寒暄，离开会不舍地道别，但是人与人之间的信任会经过多次交互才能建立。
还有一个重要的就是窗口大小。TCP要做流量控制，通信双方各声明一个窗口，标识自己当前能够的处理能力，别发送的太快，撑死我，也别发的太慢，饿死我。
作为老司机，做事情要有分寸，待人要把握尺度，既能适当提出自己的要求，又不强人所难。除了做流量控制以外，TCP还会做拥塞控制，对于真正的通路堵车不堵车，它无能为力，唯一能做的就是控制自己，也即控制发送的速度。不能改变世界，就改变自己嘛。
作为老司机，要会自我控制，知进退，知道什么时候应该坚持，什么时候应该让步。
通过对TCP头的解析，我们知道要掌握TCP协议，重点应该关注以下几个问题：
顺序问题 ，稳重不乱；
丢包问题，承诺靠谱；
连接维护，有始有终；
流量控制，把握分寸；
拥塞控制，知进知退。
TCP的三次握手所有的问题，首先都要先建立一个连接，所以我们先来看连接维护问题。
TCP的连接建立，我们常常称为三次握手。
A：您好，我是A。
B：您好A，我是B。
A：您好B。
我们也常称为“请求-&amp;gt;应答-&amp;gt;应答之应答”的三个回合。这个看起来简单，其实里面还是有很多的学问，很多的细节。
首先，为什么要三次，而不是两次？按说两个人打招呼，一来一回就可以了啊？为了可靠，为什么不是四次？
我们还是假设这个通路是非常不可靠的，A要发起一个连接，当发了第一个请求杳无音信的时候，会有很多的可能性，比如第一个请求包丢了，再如没有丢，但是绕了弯路，超时了，还有B没有响应，不想和我连接。
A不能确认结果，于是再发，再发。终于，有一个请求包到了B，但是请求包到了B的这个事情，目前A还是不知道的，A还有可能再发。
B收到了请求包，就知道了A的存在，并且知道A要和它建立连接。如果B不乐意建立连接，则A会重试一阵后放弃，连接建立失败，没有问题；如果B是乐意建立连接的，则会发送应答包给A。
当然对于B来说，这个应答包也是一入网络深似海，不知道能不能到达A。这个时候B自然不能认为连接是建立好了，因为应答包仍然会丢，会绕弯路，或者A已经挂了都有可能。
而且这个时候B还能碰到一个诡异的现象就是，A和B原来建立了连接，做了简单通信后，结束了连接。还记得吗？A建立连接的时候，请求包重复发了几次，有的请求包绕了一大圈又回来了，B会认为这也是一个正常的的请求的话，因此建立了连接，可以想象，这个连接不会进行下去，也没有个终结的时候，纯属单相思了。因而两次握手肯定不行。
B发送的应答可能会发送多次，但是只要一次到达A，A就认为连接已经建立了，因为对于A来讲，他的消息有去有回。A会给B发送应答之应答，而B也在等这个消息，才能确认连接的建立，只有等到了这个消息，对于B来讲，才算它的消息有去有回。
当然A发给B的应答之应答也会丢，也会绕路，甚至B挂了。按理来说，还应该有个应答之应答之应答，这样下去就没底了。所以四次握手是可以的，四十次都可以，关键四百次也不能保证就真的可靠了。只要双方的消息都有去有回，就基本可以了。
好在大部分情况下，A和B建立了连接之后，A会马上发送数据的，一旦A发送数据，则很多问题都得到了解决。例如A发给B的应答丢了，当A后续发送的数据到达的时候，B可以认为这个连接已经建立，或者B压根就挂了，A发送的数据，会报错，说B不可达，A就知道B出事情了。
当然你可以说A比较坏，就是不发数据，建立连接后空着。我们在程序设计的时候，可以要求开启keepalive机制，即使没有真实的数据包，也有探活包。
另外，你作为服务端B的程序设计者，对于A这种长时间不发包的客户端，可以主动关闭，从而空出资源来给其他客户端使用。
三次握手除了双方建立连接外，主要还是为了沟通一件事情，就是TCP包的序号的问题。
A要告诉B，我这面发起的包的序号起始是从哪个号开始的，B同样也要告诉A，B发起的包的序号起始是从哪个号开始的。为什么序号不能都从1开始呢？因为这样往往会出现冲突。
例如，A连上B之后，发送了1、2、3三个包，但是发送3的时候，中间丢了，或者绕路了，于是重新发送，后来A掉线了，重新连上B后，序号又从1开始，然后发送2，但是压根没想发送3，但是上次绕路的那个3又回来了，发给了B，B自然认为，这就是下一个包，于是发生了错误。
因而，每个连接都要有不同的序号。这个序号的起始序号是随着时间变化的，可以看成一个32位的计数器，每4微秒加一，如果计算一下，如果到重复，需要4个多小时，那个绕路的包早就死翘翘了，因为我们都知道IP包头里面有个TTL，也即生存时间。
好了，双方终于建立了信任，建立了连接。前面也说过，为了维护这个连接，双方都要维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。
一开始，客户端和服务端都处于CLOSED状态。先是服务端主动监听某个端口，处于LISTEN状态。然后客户端主动发起连接SYN，之后处于SYN-SENT状态。服务端收到发起的连接，返回SYN，并且ACK客户端的SYN，之后处于SYN-RCVD状态。客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态，因为它一发一收成功了。服务端收到ACK的ACK之后，处于ESTABLISHED状态，因为它也一发一收了。
TCP四次挥手好了，说完了连接，接下来说一说“拜拜”，好说好散。这常被称为四次挥手。
A：B啊，我不想玩了。
B：哦，你不想玩了啊，我知道了。
这个时候，还只是A不想玩了，也即A不会再发送数据，但是B能不能在ACK的时候，直接关闭呢？当然不可以了，很有可能A是发完了最后的数据就准备不玩了，但是B还没做完自己的事情，还是可以发送数据的，所以称为半关闭的状态。
这个时候A可以选择不再接收数据了，也可以选择最后再接收一段数据，等待B也主动关闭。
B：A啊，好吧，我也不玩了，拜拜。
A：好的，拜拜。
这样整个连接就关闭了。但是这个过程有没有异常情况呢？当然有，上面是和平分手的场面。
A开始说“不玩了”，B说“知道了”，这个回合，是没什么问题的，因为在此之前，双方还处于合作的状态，如果A说“不玩了”，没有收到回复，则A会重新发送“不玩了”。但是这个回合结束之后，就有可能出现异常情况了，因为已经有一方率先撕破脸。
一种情况是，A说完“不玩了”之后，直接跑路，是会有问题的，因为B还没有发起结束，而如果A跑路，B就算发起结束，也得不到回答，B就不知道该怎么办了。另一种情况是，A说完“不玩了”，B直接跑路，也是有问题的，因为A不知道B是还有事情要处理，还是过一会儿会发送结束。
那怎么解决这些问题呢？TCP协议专门设计了几个状态来处理这些问题。我们来看断开连接的时候的状态时序图。
断开的时候，我们可以看到，当A说“不玩了”，就进入FIN_WAIT_1的状态，B收到“A不玩”的消息后，发送知道了，就进入CLOSE_WAIT的状态。
A收到“B说知道了”，就进入FIN_WAIT_2的状态，如果这个时候B直接跑路，则A将永远在这个状态。TCP协议里面并没有对这个状态的处理，但是Linux有，可以调整tcp_fin_timeout这个参数，设置一个超时时间。
如果B没有跑路，发送了“B也不玩了”的请求到达A时，A发送“知道B也不玩了”的ACK后，从FIN_WAIT_2状态结束，按说A可以跑路了，但是最后的这个ACK万一B收不到呢？则B会重新发一个“B不玩了”，这个时候A已经跑路了的话，B就再也收不到ACK了，因而TCP协议要求A最后等待一段时间TIME_WAIT，这个时间要足够长，长到如果B没收到ACK的话，“B说不玩了”会重发的，A会重新发一个ACK并且足够时间到达B。
A直接跑路还有一个问题是，A的端口就直接空出来了，但是B不知道，B原来发过的很多包很可能还在路上，如果A的端口被一个新的应用占用了，这个新的应用会收到上个连接中B发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而也需要等足够长的时间，等到原来B发送的所有的包都死翘翘，再空出端口来。</description></item><item><title>第12讲_TCP协议（下）：西行必定多妖孽，恒心智慧消磨难</title><link>https://artisanbox.github.io/5/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/3/</guid><description>我们前面说到玄奘西行，要出网关。既然出了网关，那就是在公网上传输数据，公网往往是不可靠的，因而需要很多的机制去保证传输的可靠性，这里面需要恒心，也即各种重传的策略，还需要有智慧，也就是说，这里面包含着大量的算法。
如何做个靠谱的人？TCP想成为一个成熟稳重的人，成为一个靠谱的人。那一个人怎么样才算靠谱呢？咱们工作中经常就有这样的场景，比如你交代给下属一个事情以后，下属到底能不能做到，做到什么程度，什么时候能够交付，往往就会有应答，有回复。这样，处理事情的过程中，一旦有异常，你也可以尽快知道，而不是交代完之后就石沉大海，过了一个月再问，他说，啊我不记得了。
对应到网络协议上，就是客户端每发送的一个包，服务器端都应该有个回复，如果服务器端超过一定的时间没有回复，客户端就会重新发送这个包，直到有回复。
这个发送应答的过程是什么样呢？可以是上一个收到了应答，再发送下一个。这种模式有点像两个人直接打电话，你一句，我一句。但是这种方式的缺点是效率比较低。如果一方在电话那头处理的时间比较长，这一头就要干等着，双方都没办法干其他事情。咱们在日常工作中也不是这样的，不能你交代你的下属办一件事情，就一直打着电话看着他做，而是应该他按照你的安排，先将事情记录下来，办完一件回复一件。在他办事情的过程中，你还可以同时交代新的事情，这样双方就并行了。
如果使⽤这种模式，其实需要你和你的下属就不能靠脑⼦了，⽽是要都准备⼀个本⼦，你每交代下属⼀个事情，双方的本子都要记录⼀下。
当你的下属做完⼀件事情，就回复你，做完了，你就在你的本⼦上将这个事情划去。同时你的本⼦上每件事情都有时限，如果超过了时限下属还没有回复，你就要主动重新交代⼀下：上次那件事情，你还没回复我，咋样啦？
既然多件事情可以一起处理，那就需要给每个事情编个号，防止弄错了。例如，程序员平时看任务的时候，都会看JIRA的ID，而不是每次都要描述一下具体的事情。在大部分情况下，对于事情的处理是按照顺序来的，先来的先处理，这就给应答和汇报工作带来了方便。等开周会的时候，每个程序员都可以将JIRA ID的列表拉出来，说以上的都做完了，⽽不⽤⼀个个说。
如何实现一个靠谱的协议？TCP协议使用的也是同样的模式。为了保证顺序性，每一个包都有一个ID。在建立连接的时候，会商定起始的ID是什么，然后按照ID一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的ID，表示都收到了，这种模式称为累计确认或者累计应答（cumulative acknowledgment）。
为了记录所有发送的包和接收的包，TCP也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的ID一个个排列，根据处理的情况分成四个部分。
第一部分：发送了并且已经确认的。这部分就是你交代下属的，并且也做完了的，应该划掉的。
第二部分：发送了并且尚未确认的。这部分是你交代下属的，但是还没做完的，需要等待做完的回复之后，才能划掉。
第三部分：没有发送，但是已经等待发送的。这部分是你还没有交代给下属，但是马上就要交代的。
第四部分：没有发送，并且暂时还不会发送的。这部分是你还没有交代给下属，而且暂时还不会交代给下属的。
这里面为什么要区分第三部分和第四部分呢？没交代的，一下子全交代了不就完了吗？
这就是我们上一节提到的十个词口诀里的“流量控制，把握分寸”。作为项目管理人员，你应该根据以往的工作情况和这个员工反馈的能力、抗压力等，先在心中估测一下，这个人一天能做多少工作。如果工作布置少了，就会不饱和；如果工作布置多了，他就会做不完；如果你使劲逼迫，人家可能就要辞职了。
到底一个员工能够同时处理多少事情呢？在TCP里，接收端会给发送端报一个窗口的大小，叫Advertised window。这个窗口的大小应该等于上面的第二部分加上第三部分，就是已经交代了没做完的加上马上要交代的。超过这个窗口的，接收端做不过来，就不能发送了。
于是，发送端需要保持下面的数据结构。
LastByteAcked：第一部分和第二部分的分界线
LastByteSent：第二部分和第三部分的分界线
LastByteAcked + AdvertisedWindow：第三部分和第四部分的分界线
对于接收端来讲，它的缓存里记录的内容要简单一些。
第一部分：接受并且确认过的。也就是我领导交代给我，并且我做完的。
第二部分：还没接收，但是马上就能接收的。也即是我自己的能够接受的最大工作量。
第三部分：还没接收，也没法接收的。也即超过工作量的部分，实在做不完。
对应的数据结构就像这样。
﻿﻿
MaxRcvBuffer：最大缓存的量；
LastByteRead之后是已经接收了，但是还没被应用层读取的；
NextByteExpected是第一部分和第二部分的分界线。
第二部分的窗口有多大呢？
NextByteExpected和LastByteRead的差其实是还没被应用层读取的部分占用掉的MaxRcvBuffer的量，我们定义为A。
AdvertisedWindow其实是MaxRcvBuffer减去A。
也就是：AdvertisedWindow=MaxRcvBuffer-((NextByteExpected-1)-LastByteRead)。
那第二部分和第三部分的分界线在哪里呢？NextByteExpected加AdvertisedWindow就是第二部分和第三部分的分界线，其实也就是LastByteRead加上MaxRcvBuffer。
其中第二部分里面，由于受到的包可能不是顺序的，会出现空档，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。
顺序问题与丢包问题接下来我们结合一个例子来看。
还是刚才的图，在发送端来看，1、2、3已经发送并确认；4、5、6、7、8、9都是发送了还没确认；10、11、12是还没发出的；13、14、15是接收方没有空间，不准备发的。
在接收端来看，1、2、3、4、5是已经完成ACK，但是没读取的；6、7是等待接收的；8、9是已经接收，但是没有ACK的。
发送端和接收端当前的状态如下：
1、2、3没有问题，双方达成了一致。
4、5接收方说ACK了，但是发送方还没收到，有可能丢了，有可能在路上。
6、7、8、9肯定都发了，但是8、9已经到了，但是6、7没到，出现了乱序，缓存着但是没办法ACK。
根据这个例子，我们可以知道，顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。</description></item><item><title>第13讲_套接字Socket：Talkischeap,showmethecode</title><link>https://artisanbox.github.io/5/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/4/</guid><description>前面讲完了TCP和UDP协议，还没有上手过，这一节咱们讲讲基于TCP和UDP协议的Socket编程。
在讲TCP和UDP协议的时候，我们分客户端和服务端，在写程序的时候，我们也同样这样分。
Socket这个名字很有意思，可以作插口或者插槽讲。虽然我们是写软件程序，但是你可以想象为弄一根网线，一头插在客户端，一头插在服务端，然后进行通信。所以在通信之前，双方都要建立一个Socket。
在建立Socket的时候，应该设置什么参数呢？Socket编程进行的是端到端的通信，往往意识不到中间经过多少局域网，多少路由器，因而能够设置的参数，也只能是端到端协议之上网络层和传输层的。
在网络层，Socket函数需要指定到底是IPv4还是IPv6，分别对应设置为AF_INET和AF_INET6。另外，还要指定到底是TCP还是UDP。还记得咱们前面讲过的，TCP协议是基于数据流的，所以设置为SOCK_STREAM，而UDP是基于数据报的，因而设置为SOCK_DGRAM。
基于TCP协议的Socket程序函数调用过程两端创建了Socket之后，接下来的过程中，TCP和UDP稍有不同，我们先来看TCP。
TCP的服务端要先监听一个端口，一般是先调用bind函数，给这个Socket赋予一个IP地址和端口。为什么需要端口呢？要知道，你写的是一个应用程序，当一个网络包来的时候，内核要通过TCP头里面的这个端口，来找到你这个应用程序，把包给你。为什么要IP地址呢？有时候，一台机器会有多个网卡，也就会有多个IP地址，你可以选择监听所有的网卡，也可以选择监听一个网卡，这样，只有发给这个网卡的包，才会给你。
当服务端有了IP和端口号，就可以调用listen函数进行监听。在TCP的状态图里面，有一个listen状态，当调用这个函数之后，服务端就进入了这个状态，这个时候客户端就可以发起连接了。
在内核中，为每个Socket维护两个队列。一个是已经建立了连接的队列，这时候连接三次握手已经完毕，处于established状态；一个是还没有完全建立连接的队列，这个时候三次握手还没完成，处于syn_rcvd的状态。
接下来，服务端调用accept函数，拿出一个已经完成的连接进行处理。如果还没有完成，就要等着。
在服务端等待的时候，客户端可以通过connect函数发起连接。先在参数中指明要连接的IP地址和端口号，然后开始发起三次握手。内核会给客户端分配一个临时的端口。一旦握手成功，服务端的accept就会返回另一个Socket。
这是一个经常考的知识点，就是监听的Socket和真正用来传数据的Socket是两个，一个叫作监听Socket，一个叫作已连接Socket。
连接建立成功之后，双方开始通过read和write函数来读写数据，就像往一个文件流里面写东西一样。
这个图就是基于TCP协议的Socket程序函数调用过程。
说TCP的Socket就是一个文件流，是非常准确的。因为，Socket在Linux中就是以文件的形式存在的。除此之外，还存在文件描述符。写入和读出，也是通过文件描述符。
在内核中，Socket是一个文件，那对应就有文件描述符。每一个进程都有一个数据结构task_struct，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是一个整数，是这个数组的下标。
这个数组中的内容是一个指针，指向内核中所有打开的文件的列表。既然是一个文件，就会有一个inode，只不过Socket对应的inode不像真正的文件系统一样，保存在硬盘上的，而是在内存中的。在这个inode中，指向了Socket在内核中的Socket结构。
在这个结构里面，主要的是两个队列，一个是发送队列，一个是接收队列。在这两个队列里面保存的是一个缓存sk_buff。这个缓存里面能够看到完整的包的结构。看到这个，是不是能和前面讲过的收发包的场景联系起来了？
整个数据结构我也画了一张图。
基于UDP协议的Socket程序函数调用过程对于UDP来讲，过程有些不一样。UDP是没有连接的，所以不需要三次握手，也就不需要调用listen和connect，但是，UDP的交互仍然需要IP和端口号，因而也需要bind。UDP是没有维护连接状态的，因而不需要每对连接建立一组Socket，而是只要有一个Socket，就能够和多个客户端通信。也正是因为没有连接状态，每次通信的时候，都调用sendto和recvfrom，都可以传入IP地址和端口。
这个图的内容就是基于UDP协议的Socket程序函数调用过程。
服务器如何接更多的项目？会了这几个基本的Socket函数之后，你就可以轻松地写一个网络交互的程序了。就像上面的过程一样，在建立连接后，进行一个while循环。客户端发了收，服务端收了发。
当然这只是万里长征的第一步，因为如果使用这种方法，基本上只能一对一沟通。如果你是一个服务器，同时只能服务一个客户，肯定是不行的。这就相当于老板成立一个公司，只有自己一个人，自己亲自上来服务客户，只能干完了一家再干下一家，这样赚不来多少钱。
那作为老板你就要想了，我最多能接多少项目呢？当然是越多越好。
我们先来算一下理论值，也就是最大连接数，系统会用一个四元组来标识一个TCP连接。
{本机IP, 本机端口, 对端IP, 对端端口} 服务器通常固定在某个本地端口上监听，等待客户端的连接请求。因此，服务端端TCP连接四元组中只有对端IP, 也就是客户端的IP和对端的端口，也即客户端的端口是可变的，因此，最大TCP连接数=客户端IP数×客户端端口数。对IPv4，客户端的IP数最多为2的32次方，客户端的端口数最多为2的16次方，也就是服务端单机最大TCP连接数，约为2的48次方。
当然，服务端最大并发TCP连接数远不能达到理论上限。首先主要是文件描述符限制，按照上面的原理，Socket都是文件，所以首先要通过ulimit配置文件描述符的数目；另一个限制是内存，按上面的数据结构，每个TCP连接都要占用一定内存，操作系统是有限的。
所以，作为老板，在资源有限的情况下，要想接更多的项目，就需要降低每个项目消耗的资源数目。
方式一：将项目外包给其他公司（多进程方式）这就相当于你是一个代理，在那里监听来的请求。一旦建立了一个连接，就会有一个已连接Socket，这时候你可以创建一个子进程，然后将基于已连接Socket的交互交给这个新的子进程来做。就像来了一个新的项目，但是项目不一定是你自己做，可以再注册一家子公司，招点人，然后把项目转包给这家子公司做，以后对接就交给这家子公司了，你又可以去接新的项目了。
这里有一个问题是，如何创建子公司，并如何将项目移交给子公司呢？
在Linux下，创建子进程使用fork函数。通过名字可以看出，这是在父进程的基础上完全拷贝一个子进程。在Linux内核中，会复制文件描述符的列表，也会复制内存空间，还会复制一条记录当前执行到了哪一行程序的进程。显然，复制的时候在调用fork，复制完毕之后，父进程和子进程都会记录当前刚刚执行完fork。这两个进程刚复制完的时候，几乎一模一样，只是根据fork的返回值来区分到底是父进程，还是子进程。如果返回值是0，则是子进程；如果返回值是其他的整数，就是父进程。
进程复制过程我画在这里。
因为复制了文件描述符列表，而文件描述符都是指向整个内核统一的打开文件列表的，因而父进程刚才因为accept创建的已连接Socket也是一个文件描述符，同样也会被子进程获得。
接下来，子进程就可以通过这个已连接Socket和客户端进行互通了，当通信完毕之后，就可以退出进程，那父进程如何知道子进程干完了项目，要退出呢？还记得fork返回的时候，如果是整数就是父进程吗？这个整数就是子进程的ID，父进程可以通过这个ID查看子进程是否完成项目，是否需要退出。
方式二：将项目转包给独立的项目组（多线程方式）上面这种方式你应该也能发现问题，如果每次接一个项目，都申请一个新公司，然后干完了，就注销掉这个公司，实在是太麻烦了。毕竟一个新公司要有新公司的资产，有新的办公家具，每次都买了再卖，不划算。
于是你应该想到了，我们可以使用线程。相比于进程来讲，这样要轻量级的多。如果创建进程相当于成立新公司，购买新办公家具，而创建线程，就相当于在同一个公司成立项目组。一个项目做完了，那这个项目组就可以解散，组成另外的项目组，办公家具可以共用。
在Linux下，通过pthread_create创建一个线程，也是调用do_fork。不同的是，虽然新的线程在task列表会新创建一项，但是很多资源，例如文件描述符列表、进程空间，还是共享的，只不过多了一个引用而已。
新的线程也可以通过已连接Socket处理请求，从而达到并发处理的目的。
上面基于进程或者线程模型的，其实还是有问题的。新到来一个TCP连接，就需要分配一个进程或者线程。一台机器无法创建很多进程或者线程。有个C10K，它的意思是一台机器要维护1万个连接，就要创建1万个进程或者线程，那么操作系统是无法承受的。如果维持1亿用户在线需要10万台服务器，成本也太高了。
其实C10K问题就是，你接项目接的太多了，如果每个项目都成立单独的项目组，就要招聘10万人，你肯定养不起，那怎么办呢？
方式三：一个项目组支撑多个项目（IO多路复用，一个线程维护多个Socket）当然，一个项目组可以看多个项目了。这个时候，每个项目组都应该有个项目进度墙，将自己组看的项目列在那里，然后每天通过项目墙看每个项目的进度，一旦某个项目有了进展，就派人去盯一下。
由于Socket是文件描述符，因而某个线程盯的所有的Socket，都放在一个文件描述符集合fd_set中，这就是项目进度墙，然后调用select函数来监听文件描述符集合是否有变化。一旦有变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在fd_set对应的位都设为1，表示Socket可读或者可写，从而可以进行读写操作，然后再调用select，接着盯着下一轮的变化。
方式四：一个项目组支撑多个项目（IO多路复用，从“派人盯着”到“有事通知”）上面select函数还是有问题的，因为每次Socket所在的文件描述符集合中有Socket发生变化的时候，都需要通过轮询的方式，也就是需要将全部项目都过一遍的方式来查看进度，这大大影响了一个项目组能够支撑的最大的项目数量。因而使用select，能够同时盯的项目数量由FD_SETSIZE限制。
如果改成事件通知的方式，情况就会好很多，项目组不需要通过轮询挨个盯着这些项目，而是当项目进度发生变化的时候，主动通知项目组，然后项目组再根据项目进展情况做相应的操作。
能完成这件事情的函数叫epoll，它在内核中的实现不是通过轮询的方式，而是通过注册callback函数的方式，当某个文件描述符发送变化的时候，就会主动通知。
如图所示，假设进程打开了Socket m, n, x等多个文件描述符，现在需要通过epoll来监听是否这些Socket都有事件发生。其中epoll_create创建一个epoll对象，也是一个文件，也对应一个文件描述符，同样也对应着打开文件列表中的一项。在这项里面有一个红黑树，在红黑树里，要保存这个epoll要监听的所有Socket。
当epoll_ctl添加一个Socket的时候，其实是加入这个红黑树，同时红黑树里面的节点指向一个结构，将这个结构挂在被监听的Socket的事件列表中。当一个Socket来了一个事件的时候，可以从这个列表中得到epoll对象，并调用call back通知它。
这种通知方式使得监听的Socket数据增加的时候，效率不会大幅度降低，能够同时监听的Socket的数目也非常的多了。上限就为系统定义的、进程打开的最大文件描述符个数。因而，epoll被称为解决C10K问题的利器。
小结好了，这一节就到这里了，我们来总结一下：
你需要记住TCP和UDP的Socket的编程中，客户端和服务端都需要调用哪些函数；
写一个能够支撑大量连接的高并发的服务端不容易，需要多进程、多线程，而epoll机制能解决C10K问题。
最后，给你留两个思考题：
epoll是Linux上的函数，那你知道Windows上对应的机制是什么吗？如果想实现一个跨平台的程序，你知道应该怎么办吗？</description></item><item><title>第14讲_HTTP协议：看个新闻原来这么麻烦</title><link>https://artisanbox.github.io/5/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/5/</guid><description>前面讲述完传输层，接下来开始讲应用层的协议。从哪里开始讲呢，就从咱们最常用的HTTP协议开始。
HTTP协议，几乎是每个人上网用的第一个协议，同时也是很容易被人忽略的协议。
既然说看新闻，咱们就先登录 http://www.163.com 。
http://www.163.com 是个URL，叫作统一资源定位符。之所以叫统一，是因为它是有格式的。HTTP称为协议，www.163.com是一个域名，表示互联网上的一个位置。有的URL会有更详细的位置标识，例如 http://www.163.com/index.html 。正是因为这个东西是统一的，所以当你把这样一个字符串输入到浏览器的框里的时候，浏览器才知道如何进行统一处理。
HTTP请求的准备浏览器会将www.163.com这个域名发送给DNS服务器，让它解析为IP地址。有关DNS的过程，其实非常复杂，这个在后面专门介绍DNS的时候，我会详细描述，这里我们先不管，反正它会被解析成为IP地址。那接下来是发送HTTP请求吗？
不是的，HTTP是基于TCP协议的，当然是要先建立TCP连接了，怎么建立呢？还记得第11节讲过的三次握手吗？
目前使用的HTTP协议大部分都是1.1。在1.1的协议里面，默认是开启了Keep-Alive的，这样建立的TCP连接，就可以在多次请求中复用。
学习了TCP之后，你应该知道，TCP的三次握手和四次挥手，还是挺费劲的。如果好不容易建立了连接，然后就做了一点儿事情就结束了，有点儿浪费人力和物力。
HTTP请求的构建建立了连接以后，浏览器就要发送HTTP的请求。
请求的格式就像这样。
HTTP的报文大概分为三大部分。第一部分是请求行，第二部分是请求的首部，第三部分才是请求的正文实体。
第一部分：请求行在请求行中，URL就是 http://www.163.com ，版本为HTTP 1.1。这里要说一下的，就是方法。方法有几种类型。
对于访问网页来讲，最常用的类型就是GET。顾名思义，GET就是去服务器获取一些资源。对于访问网页来讲，要获取的资源往往是一个页面。其实也有很多其他的格式，比如说返回一个JSON字符串，到底要返回什么，是由服务器端的实现决定的。
例如，在云计算中，如果我们的服务器端要提供一个基于HTTP协议的API，获取所有云主机的列表，这就会使用GET方法得到，返回的可能是一个JSON字符串。字符串里面是一个列表，列表里面是一项的云主机的信息。
另外一种类型叫做POST。它需要主动告诉服务端一些信息，而非获取。要告诉服务端什么呢？一般会放在正文里面。正文可以有各种各样的格式。常见的格式也是JSON。
例如，我们下一节要讲的支付场景，客户端就需要把“我是谁？我要支付多少？我要买啥？”告诉服务器，这就需要通过POST方法。
再如，在云计算里，如果我们的服务器端，要提供一个基于HTTP协议的创建云主机的API，也会用到POST方法。这个时候往往需要将“我要创建多大的云主机？多少CPU多少内存？多大硬盘？”这些信息放在JSON字符串里面，通过POST的方法告诉服务器端。
还有一种类型叫PUT，就是向指定资源位置上传最新内容。但是，HTTP的服务器往往是不允许上传文件的，所以PUT和POST就都变成了要传给服务器东西的方法。
在实际使用过程中，这两者还会有稍许的区别。POST往往是用来创建一个资源的，而PUT往往是用来修改一个资源的。
例如，云主机已经创建好了，我想对这个云主机打一个标签，说明这个云主机是生产环境的，另外一个云主机是测试环境的。那怎么修改这个标签呢？往往就是用PUT方法。
再有一种常见的就是DELETE。这个顾名思义就是用来删除资源的。例如，我们要删除一个云主机，就会调用DELETE方法。
第二部分：首部字段请求行下面就是我们的首部字段。首部是key value，通过冒号分隔。这里面，往往保存了一些非常重要的字段。
例如，Accept-Charset，表示客户端可以接受的字符集。防止传过来的是另外的字符集，从而导致出现乱码。
再如，Content-Type是指正文的格式。例如，我们进行POST的请求，如果正文是JSON，那么我们就应该将这个值设置为JSON。
这里需要重点说一下的就是缓存。为啥要使用缓存呢？那是因为一个非常大的页面有很多东西。
例如，我浏览一个商品的详情，里面有这个商品的价格、库存、展示图片、使用手册等等。商品的展示图片会保持较长时间不变，而库存会根据用户购买的情况经常改变。如果图片非常大，而库存数非常小，如果我们每次要更新数据的时候都要刷新整个页面，对于服务器的压力就会很大。
对于这种高并发场景下的系统，在真正的业务逻辑之前，都需要有个接入层，将这些静态资源的请求拦在最外面。
这个架构的图就像这样。
其中DNS、CDN我在后面的章节会讲。和这一节关系比较大的就是Nginx这一层，它如何处理HTTP协议呢？对于静态资源，有Vanish缓存层。当缓存过期的时候，才会访问真正的Tomcat应用集群。
在HTTP头里面，Cache-control是用来控制缓存的。当客户端发送的请求中包含max-age指令时，如果判定缓存层中，资源的缓存时间数值比指定时间的数值小，那么客户端可以接受缓存的资源；当指定max-age值为0，那么缓存层通常需要将请求转发给应用集群。
另外，If-Modified-Since也是一个关于缓存的。也就是说，如果服务器的资源在某个时间之后更新了，那么客户端就应该下载最新的资源；如果没有更新，服务端会返回“304 Not Modified”的响应，那客户端就不用下载了，也会节省带宽。
到此为止，我们仅仅是拼凑起了HTTP请求的报文格式，接下来，浏览器会把它交给下一层传输层。怎么交给传输层呢？其实也无非是用Socket这些东西，只不过用的浏览器里，这些程序不需要你自己写，有人已经帮你写好了。
HTTP请求的发送HTTP协议是基于TCP协议的，所以它使用面向连接的方式发送请求，通过stream二进制流的方式传给对方。当然，到了TCP层，它会把二进制流变成一个个报文段发送给服务器。
在发送给每个报文段的时候，都需要对方有一个回应ACK，来保证报文可靠地到达了对方。如果没有回应，那么TCP这一层会进行重新传输，直到可以到达。同一个包有可能被传了好多次，但是HTTP这一层不需要知道这一点，因为是TCP这一层在埋头苦干。
TCP层发送每一个报文的时候，都需要加上自己的地址（即源地址）和它想要去的地方（即目标地址），将这两个信息放到IP头里面，交给IP层进行传输。
IP层需要查看目标地址和自己是否是在同一个局域网。如果是，就发送ARP协议来请求这个目标地址对应的MAC地址，然后将源MAC和目标MAC放入MAC头，发送出去即可；如果不在同一个局域网，就需要发送到网关，还要需要发送ARP协议，来获取网关的MAC地址，然后将源MAC和网关MAC放入MAC头，发送出去。
网关收到包发现MAC符合，取出目标IP地址，根据路由协议找到下一跳的路由器，获取下一跳路由器的MAC地址，将包发给下一跳路由器。
这样路由器一跳一跳终于到达目标的局域网。这个时候，最后一跳的路由器能够发现，目标地址就在自己的某一个出口的局域网上。于是，在这个局域网上发送ARP，获得这个目标地址的MAC地址，将包发出去。
目标的机器发现MAC地址符合，就将包收起来；发现IP地址符合，根据IP头中协议项，知道自己上一层是TCP协议，于是解析TCP的头，里面有序列号，需要看一看这个序列包是不是我要的，如果是就放入缓存中然后返回一个ACK，如果不是就丢弃。
TCP头里面还有端口号，HTTP的服务器正在监听这个端口号。于是，目标机器自然知道是HTTP服务器这个进程想要这个包，于是将包发给HTTP服务器。HTTP服务器的进程看到，原来这个请求是要访问一个网页，于是就把这个网页发给客户端。
HTTP返回的构建HTTP的返回报文也是有一定格式的。这也是基于HTTP 1.1的。
状态码会反映HTTP请求的结果。“200”意味着大吉大利；而我们最不想见的，就是“404”，也就是“服务端无法响应这个请求”。然后，短语会大概说一下原因。
接下来是返回首部的key value。
这里面，Retry-After表示，告诉客户端应该在多长时间以后再次尝试一下。“503错误”是说“服务暂时不再和这个值配合使用”。
在返回的头部里面也会有Content-Type，表示返回的是HTML，还是JSON。
构造好了返回的HTTP报文，接下来就是把这个报文发送出去。还是交给Socket去发送，还是交给TCP层，让TCP层将返回的HTML，也分成一个个小的段，并且保证每个段都可靠到达。
这些段加上TCP头后会交给IP层，然后把刚才的发送过程反向走一遍。虽然两次不一定走相同的路径，但是逻辑过程是一样的，一直到达客户端。
客户端发现MAC地址符合、IP地址符合，于是就会交给TCP层。根据序列号看是不是自己要的报文段，如果是，则会根据TCP头中的端口号，发给相应的进程。这个进程就是浏览器，浏览器作为客户端也在监听某个端口。
当浏览器拿到了HTTP的报文。发现返回“200”，一切正常，于是就从正文中将HTML拿出来。HTML是一个标准的网页格式。浏览器只要根据这个格式，展示出一个绚丽多彩的网页。
这就是一个正常的HTTP请求和返回的完整过程。
HTTP 2.0当然HTTP协议也在不断的进化过程中，在HTTP1.1基础上便有了HTTP 2.0。
HTTP 1.1在应用层以纯文本的形式进行通信。每次通信都要带完整的HTTP的头，而且不考虑pipeline模式的话，每次的过程总是像上面描述的那样一去一回。这样在实时性、并发性上都存在问题。
为了解决这些问题，HTTP 2.</description></item><item><title>第15讲_HTTPS协议：点外卖的过程原来这么复杂</title><link>https://artisanbox.github.io/5/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/6/</guid><description>用HTTP协议，看个新闻还没有问题，但是换到更加严肃的场景中，就存在很多的安全风险。例如，你要下单做一次支付，如果还是使用普通的HTTP协议，那你很可能会被黑客盯上。
你发送一个请求，说我要点个外卖，但是这个网络包被截获了，于是在服务器回复你之前，黑客先假装自己就是外卖网站，然后给你回复一个假的消息说：“好啊好啊，来来来，银行卡号、密码拿来。”如果这时候你真把银行卡密码发给它，那你就真的上套了。
那怎么解决这个问题呢？当然一般的思路就是加密。加密分为两种方式一种是对称加密，一种是非对称加密。
在对称加密算法中，加密和解密使用的密钥是相同的。也就是说，加密和解密使用的是同一个密钥。因此，对称加密算法要保证安全性的话，密钥要做好保密。只能让使用的人知道，不能对外公开。
在非对称加密算法中，加密使用的密钥和解密使用的密钥是不相同的。一把是作为公开的公钥，另一把是作为谁都不能给的私钥。公钥加密的信息，只有私钥才能解密。私钥加密的信息，只有公钥才能解密。
因为对称加密算法相比非对称加密算法来说，效率要高得多，性能也好，所以交互的场景下多用对称加密。
对称加密假设你和外卖网站约定了一个密钥，你发送请求的时候用这个密钥进行加密，外卖网站用同样的密钥进行解密。这样就算中间的黑客截获了你的请求，但是它没有密钥，还是破解不了。
这看起来很完美，但是中间有个问题，你们两个怎么来约定这个密钥呢？如果这个密钥在互联网上传输，也是很有可能让黑客截获的。黑客一旦截获这个秘钥，它可以佯作不知，静静地等着你们两个交互。这时候你们之间互通的任何消息，它都能截获并且查看，就等你把银行卡账号和密码发出来。
我们在谍战剧里面经常看到这样的场景，就是特工破译的密码会有个密码本，截获无线电台，通过密码本就能将原文破解出来。怎么把密码本给对方呢？只能通过线下传输。
比如，你和外卖网站偷偷约定时间地点，它给你一个纸条，上面写着你们两个的密钥，然后说以后就用这个密钥在互联网上定外卖了。当然你们接头的时候，也会先约定一个口号，什么“天王盖地虎”之类的，口号对上了，才能把纸条给它。但是，“天王盖地虎”同样也是对称加密密钥，同样存在如何把“天王盖地虎”约定成口号的问题。而且在谍战剧中一对一接头可能还可以，在互联网应用中，客户太多，这样是不行的。
非对称加密所以，只要是对称加密，就会永远在这个死循环里出不来，这个时候，就需要非对称加密介入进来。
非对称加密的私钥放在外卖网站这里，不会在互联网上传输，这样就能保证这个密钥的私密性。但是，对应私钥的公钥，是可以在互联网上随意传播的，只要外卖网站把这个公钥给你，你们就可以愉快地互通了。
比如说你用公钥加密，说“我要定外卖”，黑客在中间就算截获了这个报文，因为它没有私钥也是解不开的，所以这个报文可以顺利到达外卖网站，外卖网站用私钥把这个报文解出来，然后回复，“那给我银行卡和支付密码吧”。
先别太乐观，这里还是有问题的。回复的这句话，是外卖网站拿私钥加密的，互联网上人人都可以把它打开，当然包括黑客。那外卖网站可以拿公钥加密吗？当然不能，因为它自己的私钥只有它自己知道，谁也解不开。
另外，这个过程还有一个问题，黑客也可以模拟发送“我要定外卖”这个过程的，因为它也有外卖网站的公钥。
为了解决这个问题，看来一对公钥私钥是不够的，客户端也需要有自己的公钥和私钥，并且客户端要把自己的公钥，给外卖网站。
这样，客户端给外卖网站发送的时候，用外卖网站的公钥加密。而外卖网站给客户端发送消息的时候，使用客户端的公钥。这样就算有黑客企图模拟客户端获取一些信息，或者半路截获回复信息，但是由于它没有私钥，这些信息它还是打不开。
数字证书不对称加密也会有同样的问题，如何将不对称加密的公钥给对方呢？一种是放在一个公网的地址上，让对方下载；另一种就是在建立连接的时候，传给对方。
这两种方法有相同的问题，那就是，作为一个普通网民，你怎么鉴别别人给你的公钥是对的。会不会有人冒充外卖网站，发给你一个它的公钥。接下来，你和它所有的互通，看起来都是没有任何问题的。毕竟每个人都可以创建自己的公钥和私钥。
例如，我自己搭建了一个网站cliu8site，可以通过这个命令先创建私钥。
openssl genrsa -out cliu8siteprivate.key 1024 然后，再根据这个私钥，创建对应的公钥。
openssl rsa -in cliu8siteprivate.key -pubout -outcliu8sitepublic.pem 这个时候就需要权威部门的介入了，就像每个人都可以打印自己的简历，说自己是谁，但是有公安局盖章的，就只有户口本，这个才能证明你是你。这个由权威部门颁发的称为证书（Certificate）。
证书里面有什么呢？当然应该有公钥，这是最重要的；还有证书的所有者，就像户口本上有你的姓名和身份证号，说明这个户口本是你的；另外还有证书的发布机构和证书的有效期，这个有点像身份证上的机构是哪个区公安局，有效期到多少年。
这个证书是怎么生成的呢？会不会有人假冒权威机构颁发证书呢？就像有假身份证、假户口本一样。生成证书需要发起一个证书请求，然后将这个请求发给一个权威机构去认证，这个权威机构我们称为CA（ Certificate Authority）。
证书请求可以通过这个命令生成。
openssl req -key cliu8siteprivate.key -new -out cliu8sitecertificate.req 将这个请求发给权威机构，权威机构会给这个证书卡一个章，我们称为签名算法。问题又来了，那怎么签名才能保证是真的权威机构签名的呢？当然只有用只掌握在权威机构手里的东西签名了才行，这就是CA的私钥。
签名算法大概是这样工作的：一般是对信息做一个Hash计算，得到一个Hash值，这个过程是不可逆的，也就是说无法通过Hash值得出原来的信息内容。在把信息发送出去时，把这个Hash值加密后，作为一个签名和信息一起发出去。
权威机构给证书签名的命令是这样的。
openssl x509 -req -in cliu8sitecertificate.req -CA cacertificate.pem -CAkey caprivate.key -out cliu8sitecertificate.pem 这个命令会返回Signature ok，而cliu8sitecertificate.pem就是签过名的证书。CA用自己的私钥给外卖网站的公钥签名，就相当于给外卖网站背书，形成了外卖网站的证书。
我们来查看这个证书的内容。
openssl x509 -in cliu8sitecertificate.pem -noout -text 这里面有个Issuer，也即证书是谁颁发的；Subject，就是证书颁发给谁；Validity是证书期限；Public-key是公钥内容；Signature Algorithm是签名算法。</description></item><item><title>第16讲_流媒体协议：如何在直播里看到美女帅哥？</title><link>https://artisanbox.github.io/5/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/7/</guid><description>最近直播比较火，很多人都喜欢看直播，那一个直播系统里面都有哪些组成部分，都使用了什么协议呢？
无论是直播还是点播，其实都是对于视频数据的传输。一提到视频，大家都爱看，但是一提到视频技术，大家都头疼，因为名词实在是太多了。
三个名词系列我这里列三个名词系列，你先大致有个印象。
名词系列一：AVI、MPEG、RMVB、MP4、MOV、FLV、WebM、WMV、ASF、MKV。例如RMVB和MP4，看着是不是很熟悉？
名词系列二：H.261、 H.262、H.263、H.264、H.265。这个是不是就没怎么听过了？别着急，你先记住，要重点关注H.264。
名词系列三：MPEG-1、MPEG-2、MPEG-4、MPEG-7。MPEG好像听说过，但是后面的数字是怎么回事？是不是又熟悉又陌生？
这里，我想问你个问题，视频是什么？我说，其实就是快速播放一连串连续的图片。
每一张图片，我们称为一帧。只要每秒钟帧的数据足够多，也即播放得足够快。比如每秒30帧，以人的眼睛的敏感程度，是看不出这是一张张独立的图片的，这就是我们常说的帧率（FPS）。
每一张图片，都是由像素组成的，假设为1024*768（这个像素数不算多）。每个像素由RGB组成，每个8位，共24位。
我们来算一下，每秒钟的视频有多大？
30帧 × 1024 × 768 × 24 = 566,231,040Bits = 70,778,880Bytes
如果一分钟呢？4,246,732,800Bytes，已经是4个G了。
是不是不算不知道，一算吓一跳？这个数据量实在是太大，根本没办法存储和传输。如果这样存储，你的硬盘很快就满了；如果这样传输，那多少带宽也不够用啊！
怎么办呢？人们想到了编码，就是看如何用尽量少的Bit数保存视频，使播放的时候画面看起来仍然很精美。编码是一个压缩的过程。
视频和图片的压缩过程有什么特点？之所以能够对视频流中的图片进行压缩，因为视频和图片有这样一些特点。
空间冗余：图像的相邻像素之间有较强的相关性，一张图片相邻像素往往是渐变的，不是突变的，没必要每个像素都完整地保存，可以隔几个保存一个，中间的用算法计算出来。
时间冗余：视频序列的相邻图像之间内容相似。一个视频中连续出现的图片也不是突变的，可以根据已有的图片进行预测和推断。
视觉冗余：人的视觉系统对某些细节不敏感，因此不会每一个细节都注意到，可以允许丢失一些数据。
编码冗余：不同像素值出现的概率不同，概率高的用的字节少，概率低的用的字节多，类似霍夫曼编码（Huffman Coding）的思路。
总之，用于编码的算法非常复杂，而且多种多样，但是编码过程其实都是类似的。
视频编码的两大流派能不能形成一定的标准呢？要不然开发视频播放的人得累死了。当然能，我这里就给你介绍，视频编码的两大流派。
流派一：ITU（International Telecommunications Union）的VCEG（Video Coding Experts Group），这个称为国际电联下的VCEG。既然是电信，可想而知，他们最初做视频编码，主要侧重传输。名词系列二，就是这个组织制定的标准。
流派二：ISO（International Standards Organization）的MPEG（Moving Picture Experts Group），这个是ISO旗下的MPEG，本来是做视频存储的。例如，编码后保存在VCD和DVD中。当然后来也慢慢侧重视频传输了。名词系列三，就是这个组织制定的标准。
后来，ITU-T（国际电信联盟电信标准化部门，ITU Telecommunication Standardization Sector）与MPEG联合制定了H.</description></item><item><title>第17讲_P2P协议：我下小电影，99%急死你</title><link>https://artisanbox.github.io/5/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/8/</guid><description>如果你想下载一个电影，一般会通过什么方式呢？
当然，最简单的方式就是通过HTTP进行下载。但是相信你有过这样的体验，通过浏览器下载的时候，只要文件稍微大点，下载的速度就奇慢无比。
还有种下载文件的方式，就是通过FTP，也即文件传输协议。FTP采用两个TCP连接来传输一个文件。
控制连接：服务器以被动的方式，打开众所周知用于FTP的端口21，客户端则主动发起连接。该连接将命令从客户端传给服务器，并传回服务器的应答。常用的命令有：list——获取文件目录；reter——取一个文件；store——存一个文件。
数据连接：每当一个文件在客户端与服务器之间传输时，就创建一个数据连接。
FTP的两种工作模式每传输一个文件，都要建立一个全新的数据连接。FTP有两种工作模式，分别是主动模式（PORT）和被动模式（PASV），这些都是站在FTP服务器的角度来说的。
主动模式下，客户端随机打开一个大于1024的端口N，向服务器的命令端口21发起连接，同时开放N+1端口监听，并向服务器发出 “port N+1” 命令，由服务器从自己的数据端口20，主动连接到客户端指定的数据端口N+1。
被动模式下，当开启一个FTP连接时，客户端打开两个任意的本地端口N（大于1024）和N+1。第一个端口连接服务器的21端口，提交PASV命令。然后，服务器会开启一个任意的端口P（大于1024），返回“227 entering passive mode”消息，里面有FTP服务器开放的用来进行数据传输的端口。客户端收到消息取得端口号之后，会通过N+1号端口连接服务器的端口P，然后在两个端口之间进行数据传输。
P2P是什么？但是无论是HTTP的方式，还是FTP的方式，都有一个比较大的缺点，就是难以解决单一服务器的带宽压力， 因为它们使用的都是传统的客户端服务器的方式。
后来，一种创新的、称为P2P的方式流行起来。P2P就是peer-to-peer。资源开始并不集中地存储在某些设备上，而是分散地存储在多台设备上。这些设备我们姑且称为peer。
想要下载一个文件的时候，你只要得到那些已经存在了文件的peer，并和这些peer之间，建立点对点的连接，而不需要到中心服务器上，就可以就近下载文件。一旦下载了文件，你也就成为peer中的一员，你旁边的那些机器，也可能会选择从你这里下载文件，所以当你使用P2P软件的时候，例如BitTorrent，往往能够看到，既有下载流量，也有上传的流量，也即你自己也加入了这个P2P的网络，自己从别人那里下载，同时也提供给其他人下载。可以想象，这种方式，参与的人越多，下载速度越快，一切完美。
种子（.torrent）文件但是有一个问题，当你想下载一个文件的时候，怎么知道哪些peer有这个文件呢？
这就用到种子啦，也即咱们比较熟悉的.torrent文件。.torrent文件由两部分组成，分别是：announce（tracker URL）和文件信息。
文件信息里面有这些内容。
info区：这里指定的是该种子有几个文件、文件有多长、目录结构，以及目录和文件的名字。
Name字段：指定顶层目录名字。
每个段的大小：BitTorrent（简称BT）协议把一个文件分成很多个小段，然后分段下载。
段哈希值：将整个种子中，每个段的SHA-1哈希值拼在一起。
下载时，BT客户端首先解析.torrent文件，得到tracker地址，然后连接tracker服务器。tracker服务器回应下载者的请求，将其他下载者（包括发布者）的IP提供给下载者。下载者再连接其他下载者，根据.torrent文件，两者分别对方告知自己已经有的块，然后交换对方没有的数据。此时不需要其他服务器参与，并分散了单个线路上的数据流量，因此减轻了服务器的负担。
下载者每得到一个块，需要算出下载块的Hash验证码，并与.torrent文件中的对比。如果一样，则说明块正确，不一样则需要重新下载这个块。这种规定是为了解决下载内容的准确性问题。
从这个过程也可以看出，这种方式特别依赖tracker。tracker需要收集下载者信息的服务器，并将此信息提供给其他下载者，使下载者们相互连接起来，传输数据。虽然下载的过程是非中心化的，但是加入这个P2P网络的时候，都需要借助tracker中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源。
所以，这种工作方式有一个弊端，一旦tracker服务器出现故障或者线路遭到屏蔽，BT工具就无法正常工作了。
去中心化网络（DHT）那能不能彻底非中心化呢？
于是，后来就有了一种叫作DHT（Distributed Hash Table）的去中心化网络。每个加入这个DHT网络的人，都要负责存储这个网络里的资源信息和其他成员的联系信息，相当于所有人一起构成了一个庞大的分布式存储数据库。
有一种著名的DHT协议，叫Kademlia协议。这个和区块链的概念一样，很抽象，我来详细讲一下这个协议。
任何一个BitTorrent启动之后，它都有两个角色。一个是peer，监听一个TCP端口，用来上传和下载文件，这个角色表明，我这里有某个文件。另一个角色DHT node，监听一个UDP的端口，通过这个角色，这个节点加入了一个DHT的网络。
在DHT网络里面，每一个DHT node都有一个ID。这个ID是一个很长的串。每个DHT node都有责任掌握一些知识，也就是文件索引，也即它应该知道某些文件是保存在哪些节点上。它只需要有这些知识就可以了，而它自己本身不一定就是保存这个文件的节点。
哈希值当然，每个DHT node不会有全局的知识，也即不知道所有的文件保存在哪里，它只需要知道一部分。那应该知道哪一部分呢？这就需要用哈希算法计算出来。
每个文件可以计算出一个哈希值，而DHT node的ID是和哈希值相同长度的串。
DHT算法是这样规定的：如果一个文件计算出一个哈希值，则和这个哈希值一样的那个DHT node，就有责任知道从哪里下载这个文件，即便它自己没保存这个文件。
当然不一定这么巧，总能找到和哈希值一模一样的，有可能一模一样的DHT node也下线了，所以DHT算法还规定：除了一模一样的那个DHT node应该知道，ID和这个哈希值非常接近的N个DHT node也应该知道。
什么叫和哈希值接近呢？例如只修改了最后一位，就很接近；修改了倒数2位，也不远；修改了倒数3位，也可以接受。总之，凑齐了规定的N这个数就行。
刚才那个图里，文件1通过哈希运算，得到匹配ID的DHT node为node C，当然还会有其他的，我这里没有画出来。所以，node C有责任知道文件1的存放地址，虽然node C本身没有存放文件1。</description></item><item><title>第18讲_DNS协议：网络世界的地址簿</title><link>https://artisanbox.github.io/5/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/9/</guid><description>前面我们讲了平时常见的看新闻、支付、直播、下载等场景，现在网站的数目非常多，常用的网站就有二三十个，如果全部用IP地址进行访问，恐怕很难记住。于是，就需要一个地址簿，根据名称，就可以查看具体的地址。
例如，我要去西湖边的“外婆家”，这就是名称，然后通过地址簿，查看到底是哪条路多少号。
DNS服务器在网络世界，也是这样的。你肯定记得住网站的名称，但是很难记住网站的IP地址，因而也需要一个地址簿，就是DNS服务器。
由此可见，DNS在日常生活中多么重要。每个人上网，都需要访问它，但是同时，这对它来讲也是非常大的挑战。一旦它出了故障，整个互联网都将瘫痪。另外，上网的人分布在全世界各地，如果大家都去同一个地方访问某一台服务器，时延将会非常大。因而，DNS服务器，一定要设置成高可用、高并发和分布式的。
于是，就有了这样树状的层次结构。
- 根DNS服务器 ：返回顶级域DNS服务器的IP地址
顶级域DNS服务器：返回权威DNS服务器的IP地址
权威DNS服务器 ：返回相应主机的IP地址
DNS解析流程为了提高DNS的解析性能，很多网络都会就近部署DNS缓存服务器。于是，就有了以下的DNS解析流程。
电脑客户端会发出一个DNS请求，问www.163.com的IP是啥啊，并发给本地域名服务器 (本地DNS)。那本地域名服务器 (本地DNS) 是什么呢？如果是通过DHCP配置，本地DNS由你的网络服务商（ISP），如电信、移动等自动分配，它通常就在你网络服务商的某个机房。
本地DNS收到来自客户端的请求。你可以想象这台服务器上缓存了一张域名与之对应IP地址的大表格。如果能找到 www.163.com，它就直接返回IP地址。如果没有，本地DNS会去问它的根域名服务器：“老大，能告诉我www.163.com的IP地址吗？”根域名服务器是最高层次的，全球共有13套。它不直接用于域名解析，但能指明一条道路。
根DNS收到来自本地DNS的请求，发现后缀是 .com，说：“哦，www.163.com啊，这个域名是由.com区域管理，我给你它的顶级域名服务器的地址，你去问问它吧。”
本地DNS转向问顶级域名服务器：“老二，你能告诉我www.163.com的IP地址吗？”顶级域名服务器就是大名鼎鼎的比如 .com、.net、 .org这些一级域名，它负责管理二级域名，比如 163.com，所以它能提供一条更清晰的方向。
顶级域名服务器说：“我给你负责 www.163.com 区域的权威DNS服务器的地址，你去问它应该能问到。”
本地DNS转向问权威DNS服务器：“您好，www.163.com 对应的IP是啥呀？”163.com的权威DNS服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。
权威DNS服务器查询后将对应的IP地址X.X.X.X告诉本地DNS。
本地DNS再将IP地址返回客户端，客户端和目标建立连接。
至此，我们完成了DNS的解析过程。现在总结一下，整个过程我画成了一个图。
负载均衡站在客户端角度，这是一次DNS递归查询过程。因为本地DNS全权为它效劳，它只要坐等结果即可。在这个过程中，DNS除了可以通过名称映射为IP地址，它还可以做另外一件事，就是负载均衡。
还是以访问“外婆家”为例，还是我们开头的“外婆家”，但是，它可能有很多地址，因为它在杭州可以有很多家。所以，如果一个人想去吃“外婆家”，他可以就近找一家店，而不用大家都去同一家，这就是负载均衡。
DNS首先可以做内部负载均衡。
例如，一个应用要访问数据库，在这个应用里面应该配置这个数据库的IP地址，还是应该配置这个数据库的域名呢？显然应该配置域名，因为一旦这个数据库，因为某种原因，换到了另外一台机器上，而如果有多个应用都配置了这台数据库的话，一换IP地址，就需要将这些应用全部修改一遍。但是如果配置了域名，则只要在DNS服务器里，将域名映射为新的IP地址，这个工作就完成了，大大简化了运维。
在这个基础上，我们可以再进一步。例如，某个应用要访问另外一个应用，如果配置另外一个应用的IP地址，那么这个访问就是一对一的。但是当被访问的应用撑不住的时候，我们其实可以部署多个。但是，访问它的应用，如何在多个之间进行负载均衡？只要配置成为域名就可以了。在域名解析的时候，我们只要配置策略，这次返回第一个IP，下次返回第二个IP，就可以实现负载均衡了。
另外一个更加重要的是，DNS还可以做全局负载均衡。
为了保证我们的应用高可用，往往会部署在多个机房，每个地方都会有自己的IP地址。当用户访问某个域名的时候，这个IP地址可以轮询访问多个数据中心。如果一个数据中心因为某种原因挂了，只要在DNS服务器里面，将这个数据中心对应的IP地址删除，就可以实现一定的高可用。
另外，我们肯定希望北京的用户访问北京的数据中心，上海的用户访问上海的数据中心，这样，客户体验就会非常好，访问速度就会超快。这就是全局负载均衡的概念。
示例：DNS访问数据中心中对象存储上的静态资源我们通过DNS访问数据中心中对象存储上的静态资源为例，看一看整个过程。
假设全国有多个数据中心，托管在多个运营商，每个数据中心三个可用区（Available Zone）。对象存储通过跨可用区部署，实现高可用性。在每个数据中心中，都至少部署两个内部负载均衡器，内部负载均衡器后面对接多个对象存储的前置服务器（Proxy-server）。
当一个客户端要访问object.yourcompany.com的时候，需要将域名转换为IP地址进行访问，所以它要请求本地DNS解析器。
本地DNS解析器先查看看本地的缓存是否有这个记录。如果有则直接使用，因为上面的过程太复杂了，如果每次都要递归解析，就太麻烦了。</description></item><item><title>第19讲_HttpDNS：网络世界的地址簿也会指错路</title><link>https://artisanbox.github.io/5/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/10/</guid><description>上一节我们知道了DNS的两项功能，第一是根据名称查到具体的地址，另外一个是可以针对多个地址做负载均衡，而且可以在多个地址中选择一个距离你近的地方访问。
然而有时候这个地址簿也经常给你指错路，明明距离你500米就有个吃饭的地方，非要把你推荐到5公里外。为什么会出现这样的情况呢？
还记得吗？当我们发出请求解析DNS的时候，首先，会先连接到运营商本地的DNS服务器，由这个服务器帮我们去整棵DNS树上进行解析，然后将解析的结果返回给客户端。但是本地的DNS服务器，作为一个本地导游，往往有自己的“小心思”。
传统DNS存在哪些问题？1.域名缓存问题它可以在本地做一个缓存，也就是说，不是每一个请求，它都会去访问权威DNS服务器，而是访问过一次就把结果缓存到自己本地，当其他人来问的时候，直接就返回这个缓存数据。
这就相当于导游去过一个饭店，自己脑子记住了地址，当有一个游客问的时候，他就凭记忆回答了，不用再去查地址簿。这样经常存在的一个问题是，人家那个饭店明明都已经搬了，结果作为导游，他并没有刷新这个缓存，结果你辛辛苦苦到了这个地点，发现饭店已经变成了服装店，你是不是会非常失望？
另外，有的运营商会把一些静态页面，缓存到本运营商的服务器内，这样用户请求的时候，就不用跨运营商进行访问，这样既加快了速度，也减少了运营商之间流量计算的成本。在域名解析的时候，不会将用户导向真正的网站，而是指向这个缓存的服务器。
很多情况下是看不出问题的，但是当页面更新，用户会访问到老的页面，问题就出来了。例如，你听说一个餐馆推出了一个新菜，你想去尝一下。结果导游告诉你，在这里吃也是一样的。有的游客会觉得没问题，但是对于想尝试新菜的人来说，如果导游说带你去，但其实并没有吃到新菜，你是不是也会非常失望呢？
再就是本地的缓存，往往使得全局负载均衡失败，因为上次进行缓存的时候，缓存中的地址不一定是这次访问离客户最近的地方，如果把这个地址返回给客户，那肯定就会绕远路。
就像上一次客户要吃西湖醋鱼的事，导游知道西湖边有一家，因为当时游客就在西湖边，可是，下一次客户在灵隐寺，想吃西湖醋鱼的时候，导游还指向西湖边的那一家，那这就绕得太远了。
2.域名转发问题缓存问题还是说本地域名解析服务，还是会去权威DNS服务器中查找，只不过不是每次都要查找。可以说这还是大导游、大中介。还有一些小导游、小中介，有了请求之后，直接转发给其他运营商去做解析，自己只是外包了出去。
这样的问题是，如果是A运营商的客户，访问自己运营商的DNS服务器，如果A运营商去权威DNS服务器查询的话，权威DNS服务器知道你是A运营商的，就返回给一个部署在A运营商的网站地址，这样针对相同运营商的访问，速度就会快很多。
但是A运营商偷懒，将解析的请求转发给B运营商，B运营商去权威DNS服务器查询的话，权威服务器会误认为，你是B运营商的，那就返回给你一个在B运营商的网站地址吧，结果客户的每次访问都要跨运营商，速度就会很慢。
3.出口NAT问题前面讲述网关的时候，我们知道，出口的时候，很多机房都会配置NAT，也即网络地址转换，使得从这个网关出去的包，都换成新的IP地址，当然请求返回的时候，在这个网关，再将IP地址转换回去，所以对于访问来说是没有任何问题。
但是一旦做了网络地址的转换，权威的DNS服务器，就没办法通过这个地址，来判断客户到底是来自哪个运营商，而且极有可能因为转换过后的地址，误判运营商，导致跨运营商的访问。
4.域名更新问题本地DNS服务器是由不同地区、不同运营商独立部署的。对域名解析缓存的处理上，实现策略也有区别，有的会偷懒，忽略域名解析结果的TTL时间限制，在权威DNS服务器解析变更的时候，解析结果在全网生效的周期非常漫长。但是有的时候，在DNS的切换中，场景对生效时间要求比较高。
例如双机房部署的时候，跨机房的负载均衡和容灾多使用DNS来做。当一个机房出问题之后，需要修改权威DNS，将域名指向新的IP地址，但是如果更新太慢，那很多用户都会出现访问异常。
这就像，有的导游比较勤快、敬业，时时刻刻关注酒店、餐馆、交通的变化，问他的时候，往往会得到最新情况。有的导游懒一些，8年前背的导游词就没换过，问他的时候，指的路往往就是错的。
5.解析延迟问题从上一节的DNS查询过程来看，DNS的查询过程需要递归遍历多个DNS服务器，才能获得最终的解析结果，这会带来一定的时延，甚至会解析超时。
HttpDNS的工作模式既然DNS解析中有这么多问题，那怎么办呢？难不成退回到直接用IP地址？这样显然不合适，所以就有了 HttpDNS。
HttpDNS其实就是，不走传统的DNS解析，而是自己搭建基于HTTP协议的DNS服务器集群，分布在多个地点和多个运营商。当客户端需要DNS解析的时候，直接通过HTTP协议进行请求这个服务器集群，得到就近的地址。
这就相当于每家基于HTTP协议，自己实现自己的域名解析，自己做一个自己的地址簿，而不使用统一的地址簿。但是默认的域名解析都是走DNS的，因而使用HttpDNS需要绕过默认的DNS路径，就不能使用默认的客户端。使用HttpDNS的，往往是手机应用，需要在手机端嵌入支持HttpDNS的客户端SDK。
通过自己的HttpDNS服务器和自己的SDK，实现了从依赖本地导游，到自己上网查询做旅游攻略，进行自由行，爱怎么玩怎么玩。这样就能够避免依赖导游，而导游又不专业，你还不能把他怎么样的尴尬。
下面我来解析一下 HttpDNS的工作模式。
在客户端的SDK里动态请求服务端，获取HttpDNS服务器的IP列表，缓存到本地。随着不断地解析域名，SDK也会在本地缓存DNS域名解析的结果。
当手机应用要访问一个地址的时候，首先看是否有本地的缓存，如果有就直接返回。这个缓存和本地DNS的缓存不一样的是，这个是手机应用自己做的，而非整个运营商统一做的。如何更新、何时更新，手机应用的客户端可以和服务器协调来做这件事情。
如果本地没有，就需要请求HttpDNS的服务器，在本地HttpDNS服务器的IP列表中，选择一个发出HTTP的请求，会返回一个要访问的网站的IP列表。
请求的方式是这样的。
curl http://106.2.xxx.xxx/d?dn=c.m.163.com {&amp;quot;dns&amp;quot;:[{&amp;quot;host&amp;quot;:&amp;quot;c.m.163.com&amp;quot;,&amp;quot;ips&amp;quot;:[&amp;quot;223.252.199.12&amp;quot;],&amp;quot;ttl&amp;quot;:300,&amp;quot;http2&amp;quot;:0}],&amp;quot;client&amp;quot;:{&amp;quot;ip&amp;quot;:&amp;quot;106.2.81.50&amp;quot;,&amp;quot;line&amp;quot;:269692944}} 手机客户端自然知道手机在哪个运营商、哪个地址。由于是直接的HTTP通信，HttpDNS服务器能够准确知道这些信息，因而可以做精准的全局负载均衡。
当然，当所有这些都不工作的时候，可以切换到传统的LocalDNS来解析，慢也比访问不到好。那HttpDNS是如何解决上面的问题的呢？
其实归结起来就是两大问题。一是解析速度和更新速度的平衡问题，二是智能调度的问题，对应的解决方案是HttpDNS的缓存设计和调度设计。
HttpDNS的缓存设计解析DNS过程复杂，通信次数多，对解析速度造成很大影响。为了加快解析，因而有了缓存，但是这又会产生缓存更新速度不及时的问题。最要命的是，这两个方面都掌握在别人手中，也即本地DNS服务器手中，它不会为你定制，你作为客户端干着急没办法。
而HttpDNS就是将解析速度和更新速度全部掌控在自己手中。一方面，解析的过程，不需要本地DNS服务递归的调用一大圈，一个HTTP的请求直接搞定，要实时更新的时候，马上就能起作用；另一方面为了提高解析速度，本地也有缓存，缓存是在客户端SDK维护的，过期时间、更新时间，都可以自己控制。
HttpDNS的缓存设计策略也是咱们做应用架构中常用的缓存设计模式，也即分为客户端、缓存、数据源三层。
对于应用架构来讲，就是应用、缓存、数据库。常见的是Tomcat、Redis、MySQL。
对于HttpDNS来讲，就是手机客户端、DNS缓存、HttpDNS服务器。
只要是缓存模式，就存在缓存的过期、更新、不一致的问题，解决思路也是很像的。
例如DNS缓存在内存中，也可以持久化到存储上，从而APP重启之后，能够尽快从存储中加载上次累积的经常访问的网站的解析结果，就不需要每次都全部解析一遍，再变成缓存。这有点像Redis是基于内存的缓存，但是同样提供持久化的能力，使得重启或者主备切换的时候，数据不会完全丢失。
SDK中的缓存会严格按照缓存过期时间，如果缓存没有命中，或者已经过期，而且客户端不允许使用过期的记录，则会发起一次解析，保障记录是更新的。
解析可以同步进行，也就是直接调用HttpDNS的接口，返回最新的记录，更新缓存；也可以异步进行，添加一个解析任务到后台，由后台任务调用HttpDNS的接口。
同步更新的优点是实时性好，缺点是如果有多个请求都发现过期的时候，同时会请求HttpDNS多次，其实是一种浪费。
同步更新的方式对应到应用架构中缓存的Cache-Aside机制，也即先读缓存，不命中读数据库，同时将结果写入缓存。
异步更新的优点是，可以将多个请求都发现过期的情况，合并为一个对于HttpDNS的请求任务，只执行一次，减少HttpDNS的压力。同时可以在即将过期的时候，就创建一个任务进行预加载，防止过期之后再刷新，称为预加载。
它的缺点是当前请求拿到过期数据的时候，如果客户端允许使用过期数据，需要冒一次风险。如果过期的数据还能请求，就没问题；如果不能请求，则失败一次，等下次缓存更新后，再请求方能成功。
异步更新的机制对应到应用架构中缓存的Refresh-Ahead机制，即业务仅仅访问缓存，当过期的时候定期刷新。在著名的应用缓存Guava Cache中，有个RefreshAfterWrite机制，对于并发情况下，多个缓存访问不命中从而引发并发回源的情况，可以采取只有一个请求回源的模式。在应用架构的缓存中，也常常用数据预热或者预加载的机制。
HttpDNS的调度设计由于客户端嵌入了SDK，因而就不会因为本地DNS的各种缓存、转发、NAT，让权威DNS服务器误会客户端所在的位置和运营商，而可以拿到第一手资料。
在客户端，可以知道手机是哪个国家、哪个运营商、哪个省，甚至哪个市，HttpDNS服务端可以根据这些信息，选择最佳的服务节点访问。
如果有多个节点，还会考虑错误率、请求时间、服务器压力、网络状况等，进行综合选择，而非仅仅考虑地理位置。当有一个节点宕机或者性能下降的时候，可以尽快进行切换。
要做到这一点，需要客户端使用HttpDNS返回的IP访问业务应用。客户端的SDK会收集网络请求数据，如错误率、请求时间等网络请求质量数据，并发送到统计后台，进行分析、聚合，以此查看不同的IP的服务质量。
在服务端，应用可以通过调用HttpDNS的管理接口，配置不同服务质量的优先级、权重。HttpDNS会根据这些策略综合地理位置和线路状况算出一个排序，优先访问当前那些优质的、时延低的IP地址。
HttpDNS通过智能调度之后返回的结果，也会缓存在客户端。为了不让缓存使得调度失真，客户端可以根据不同的移动网络运营商WIFI的SSID来分维度缓存。不同的运营商或者WIFI解析出来的结果会不同。
小结好了，这节就到这里了，我们来总结一下，你需要记住这两个重点：
传统的DNS有很多问题，例如解析慢、更新不及时。因为缓存、转发、NAT问题导致客户端误会自己所在的位置和运营商，从而影响流量的调度。
HttpDNS通过客户端SDK和服务端，通过HTTP直接调用解析DNS的方式，绕过了传统DNS的这些缺点，实现了智能的调度。
最后，给你留两个思考题。</description></item><item><title>第1讲_为什么要学习网络协议？</title><link>https://artisanbox.github.io/5/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/11/</guid><description>《圣经》中有一个通天塔的故事，大致是说，上帝为了阻止人类联合起来，就让人类说不同的语言。人类没法儿沟通，达不成“协议”，通天塔的计划就失败了。
但是千年以后，有一种叫“程序猿”的物种，敲着一种这个群体通用的语言，连接着全世界所有的人，打造这互联网世界的通天塔。如今的世界，正是因为互联网，才连接在一起。
当"Hello World!"从显示器打印出来的时候，还记得你激动的心情吗？
public class HelloWorld { public static void main(String[] args){ System.out.println(&amp;quot;Hello World!&amp;quot;); } } 如果你是程序员，一定看得懂上面这一段文字。这是每一个程序员向计算机世界说“你好，世界”的方式。但是，你不一定知道，这段文字也是一种协议，是人类和计算机沟通的协议，只有通过这种协议，计算机才知道我们想让它做什么。
协议三要素当然，这种协议还是更接近人类语言，机器不能直接读懂，需要进行翻译，翻译的工作教给编译器，也就是程序员常说的compile。这个过程比较复杂，其中的编译原理非常复杂，我在这里不进行详述。
但是可以看得出，计算机语言作为程序员控制一台计算机工作的协议，具备了协议的三要素。
语法，就是这一段内容要符合一定的规则和格式。例如，括号要成对，结束要使用分号等。
语义，就是这一段内容要代表某种意义。例如数字减去数字是有意义的，数字减去文本一般来说就没有意义。
顺序，就是先干啥，后干啥。例如，可以先加上某个数值，然后再减去某个数值。
会了计算机语言，你就能够教给一台计算机完成你的工作了。恭喜你，入门了！
但是，要想打造互联网世界的通天塔，只教给一台机器做什么是不够的，你需要学会教给一大片机器做什么。这就需要网络协议。只有通过网络协议，才能使一大片机器互相协作、共同完成一件事。
这个时候，你可能会问，网络协议长啥样，这么神奇，能干成啥事？我先拿一个简单的例子，让你尝尝鲜，然后再讲一个大事。
当你想要买一个商品，常规的做法就是打开浏览器，输入购物网站的地址。浏览器就会给你显示一个缤纷多彩的页面。
那你有没有深入思考过，浏览器是如何做到这件事情的？它之所以能够显示缤纷多彩的页面，是因为它收到了一段来自HTTP协议的“东西”。我拿网易考拉来举例，格式就像下面这样：
HTTP/1.1 200 OK Date: Tue, 27 Mar 2018 16:50:26 GMT Content-Type: text/html;charset=UTF-8 Content-Language: zh-CN &amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;base href=&amp;quot;https://pages.kaola.com/&amp;quot; /&amp;gt; &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;/&amp;gt; &amp;lt;title&amp;gt;网易考拉3周年主会场&amp;lt;/title&amp;gt; 这符合协议的三要素吗？我带你来看一下。
首先，符合语法，也就是说，只有按照上面那个格式来，浏览器才认。例如，上来是状态，然后是首部，然后是内容。
第二，符合语义，就是要按照约定的意思来。例如，状态200，表述的意思是网页成功返回。如果不成功，就是我们常见的“404”。
第三，符合顺序，你一点浏览器，就是发送出一个HTTP请求，然后才有上面那一串HTTP返回的东西。
浏览器显然按照协议商定好的做了，最后一个五彩缤纷的页面就出现在你面前了。
我们常用的网络协议有哪些？接下来揭秘我要说的大事情，“双十一”。这和我们要讲的网络协议有什么关系呢？
在经济学领域，有个伦纳德·里德（Leonard E. Read）创作的《铅笔的故事》。这个故事通过一个铅笔的诞生过程，来讲述复杂的经济学理论。这里，我也用一个下单的过程，看看互联网世界的运行过程中，都使用了哪些网络协议。
你先在浏览器里面输入 https://www.</description></item><item><title>第20讲_CDN：你去小卖部取过快递么？</title><link>https://artisanbox.github.io/5/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/12/</guid><description>上一节，我们看到了网站的一般访问模式。
当一个用户想访问一个网站的时候，指定这个网站的域名，DNS就会将这个域名解析为地址，然后用户请求这个地址，返回一个网页。就像你要买个东西，首先要查找商店的位置，然后去商店里面找到自己想要的东西，最后拿着东西回家。
那这里面还有没有可以优化的地方呢？
例如你去电商网站下单买个东西，这个东西一定要从电商总部的中心仓库送过来吗？原来基本是这样的，每一单都是单独配送，所以你可能要很久才能收到你的宝贝。但是后来电商网站的物流系统学聪明了，他们在全国各地建立了很多仓库，而不是只有总部的中心仓库才可以发货。
电商网站根据统计大概知道，北京、上海、广州、深圳、杭州等地，每天能够卖出去多少书籍、卫生纸、包、电器等存放期比较长的物品。这些物品用不着从中心仓库发出，所以平时就可以将它们分布在各地仓库里，客户一下单，就近的仓库发出，第二天就可以收到了。
这样，用户体验大大提高。当然，这里面也有个难点就是，生鲜这类东西保质期太短，如果提前都备好货，但是没有人下单，那肯定就坏了。这个问题，我后文再说。
我们先说，我们的网站访问可以借鉴“就近配送”这个思路。
全球有这么多的数据中心，无论在哪里上网，临近不远的地方基本上都有数据中心。是不是可以在这些数据中心里部署几台机器，形成一个缓存的集群来缓存部分数据，那么用户访问数据的时候，就可以就近访问了呢？
当然是可以的。这些分布在各个地方的各个数据中心的节点，就称为边缘节点。
由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上。有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。
这就是CDN分发系统的架构。CDN系统的缓存，也是一层一层的，能不访问后端真正的源，就不打扰它。这也是电商网站物流系统的思路，北京局找不到，找华北局，华北局找不到，再找北方局。
有了这个分发系统之后，接下来，客户端如何找到相应的边缘节点进行访问呢？
还记得我们讲过的基于DNS的全局负载均衡吗？这个负载均衡主要用来选择一个就近的同样运营商的服务器进行访问。你会发现，CDN分发网络也是一个分布在多个区域、多个运营商的分布式系统，也可以用相同的思路选择最合适的边缘节点。
在没有CDN的情况下，用户向浏览器输入www.web.com这个域名，客户端访问本地DNS服务器的时候，如果本地DNS服务器有缓存，则返回网站的地址；如果没有，递归查询到网站的权威DNS服务器，这个权威DNS服务器是负责web.com的，它会返回网站的IP地址。本地DNS服务器缓存下IP地址，将IP地址返回，然后客户端直接访问这个IP地址，就访问到了这个网站。
然而有了CDN之后，情况发生了变化。在web.com这个权威DNS服务器上，会设置一个CNAME别名，指向另外一个域名 www.web.cdn.com，返回给本地DNS服务器。
当本地DNS服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是web.com的权威DNS服务器了，而是web.cdn.com的权威DNS服务器，这是CDN自己的权威DNS服务器。在这个服务器上，还是会设置一个CNAME，指向另外一个域名，也即CDN网络的全局负载均衡器。
接下来，本地DNS服务器去请求CDN的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括：
根据用户IP地址，判断哪一台服务器距用户最近；
用户所处的运营商；
根据用户所请求的URL中携带的内容名称，判断哪一台服务器上有用户所需的内容；
查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。
基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的IP地址。
本地DNS服务器缓存这个IP地址，然后将IP返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。
CDN可以进行缓存的内容有很多种。
保质期长的日用品比较容易缓存，因为不容易过期，对应到就像电商仓库系统里，就是静态页面、图片等，因为这些东西也不怎么变，所以适合缓存。
还记得这个接入层缓存的架构吗？在进入数据中心的时候，我们希望通过最外层接入层的缓存，将大部分静态资源的访问拦在边缘。而CDN则更进一步，将这些静态资源缓存到离用户更近的数据中心。越接近客户，访问性能越好，时延越低。
但是静态内容中，有一种特殊的内容，也大量使用了CDN，这个就是前面讲过的流媒体。
CDN支持流媒体协议，例如前面讲过的RTMP协议。在很多情况下，这相当于一个代理，从上一级缓存读取内容，转发给用户。由于流媒体往往是连续的，因而可以进行预先缓存的策略，也可以预先推送到用户的客户端。
对于静态页面来讲，内容的分发往往采取拉取的方式，也即当发现未命中的时候，再去上一级进行拉取。但是，流媒体数据量大，如果出现回源，压力会比较大，所以往往采取主动推送的模式，将热点数据主动推送到边缘节点。
对于流媒体来讲，很多CDN还提供预处理服务，也即文件在分发之前，经过一定的处理。例如将视频转换为不同的码流，以适应不同的网络带宽的用户需求；再如对视频进行分片，降低存储压力，也使得客户端可以选择使用不同的码率加载不同的分片。这就是我们常见的，“我要看超清、标清、流畅等”。
对于流媒体CDN来讲，有个关键的问题是防盗链问题。因为视频是要花大价钱买版权的，为了挣点钱，收点广告费，如果流媒体被其他的网站盗走，在人家的网站播放，那损失可就大了。
最常用也最简单的方法就是HTTP头的referer字段， 当浏览器发送请求的时候，一般会带上referer，告诉服务器是从哪个页面链接过来的，服务器基于此可以获得一些信息用于处理。如果refer信息不是来自本站，就阻止访问或者跳到其它链接。
referer的机制相对比较容易破解，所以还需要配合其他的机制。
一种常用的机制是时间戳防盗链。使用CDN的管理员可以在配置界面上，和CDN厂商约定一个加密字符串。
客户端取出当前的时间戳，要访问的资源及其路径，连同加密字符串进行签名算法得到一个字符串，然后生成一个下载链接，带上这个签名字符串和截止时间戳去访问CDN。
在CDN服务端，根据取出过期时间，和当前 CDN 节点时间进行比较，确认请求是否过期。然后CDN服务端有了资源及路径，时间戳，以及约定的加密字符串，根据相同的签名算法计算签名，如果匹配则一致，访问合法，才会将资源返回给客户。
然而比如在电商仓库中，我在前面提过，有关生鲜的缓存就是非常麻烦的事情，这对应着就是动态的数据，比较难以缓存。怎么办呢？现在也有动态CDN，主要有两种模式。
一种为生鲜超市模式，也即边缘计算的模式。既然数据是动态生成的，所以数据的逻辑计算和存储，也相应的放在边缘的节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。就像对生鲜的烹饪是动态的，没办法事先做好缓存，因而将生鲜超市放在你家旁边，既能够送货上门，也能够现场烹饪，也是边缘计算的一种体现。
另一种是冷链运输模式，也即路径优化的模式。数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过CDN的网络，对路径进行优化。因为CDN节点较多，能够找到离源站很近的边缘节点，也能找到离用户很近的边缘节点。中间的链路完全由CDN来规划，选择一个更加可靠的路径，使用类似专线的方式进行访问。
对于常用的TCP连接，在公网上传输的时候经常会丢数据，导致TCP的窗口始终很小，发送速度上不去。根据前面的TCP流量控制和拥塞控制的原理，在CDN加速网络中可以调整TCP的参数，使得TCP可以更加激进地传输数据。
可以通过多个请求复用一个连接，保证每次动态请求到达时。连接都已经建立了，不必临时三次握手或者建立过多的连接，增加服务器的压力。另外，可以通过对传输数据进行压缩，增加传输效率。
所有这些手段就像冷链运输，整个物流优化了，全程冷冻高速运输。不管生鲜是从你旁边的超市送到你家的，还是从产地送的，保证到你家是新鲜的。
小结好了，这节就到这里了。咱们来总结一下，你记住这两个重点就好。
CDN和电商系统的分布式仓储系统一样，分为中心节点、区域节点、边缘节点，而数据缓存在离用户最近的位置。
CDN最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链。它也支持动态数据的缓存，一种是边缘计算的生鲜超市模式，另一种是链路优化的冷链运输模式。
最后，给你留两个思考题：</description></item><item><title>第21讲_数据中心：我是开发商，自己拿地盖别墅</title><link>https://artisanbox.github.io/5/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/13/</guid><description>无论你是看新闻、下订单、看视频、下载文件，最终访问的目的地都在数据中心里面。我们前面学了这么多的网络协议和网络相关的知识，你是不是很好奇，数据中心究竟长啥样呢？
数据中心是一个大杂烩，几乎要用到前面学过的所有知识。
前面讲办公室网络的时候，我们知道办公室里面有很多台电脑。如果要访问外网，需要经过一个叫网关的东西，而网关往往是一个路由器。
数据中心里面也有一大堆的电脑，但是它和咱们办公室里面的笔记本或者台式机不一样。数据中心里面是服务器。服务器被放在一个个叫作机架（Rack）的架子上面。
数据中心的入口和出口也是路由器，由于在数据中心的边界，就像在一个国家的边境，称为边界路由器（Border Router）。为了高可用，边界路由器会有多个。
一般家里只会连接一个运营商的网络，而为了高可用，为了当一个运营商出问题的时候，还可以通过另外一个运营商来提供服务，所以数据中心的边界路由器会连接多个运营商网络。
既然是路由器，就需要跑路由协议，数据中心往往就是路由协议中的自治区域（AS）。数据中心里面的机器要想访问外面的网站，数据中心里面也是有对外提供服务的机器，都可以通过BGP协议，获取内外互通的路由信息。这就是我们常听到的多线BGP的概念。
如果数据中心非常简单，没几台机器，那就像家里或者宿舍一样，所有的服务器都直接连到路由器上就可以了。但是数据中心里面往往有非常多的机器，当塞满一机架的时候，需要有交换机将这些服务器连接起来，可以互相通信。
这些交换机往往是放在机架顶端的，所以经常称为TOR（Top Of Rack）交换机。这一层的交换机常常称为接入层（Access Layer）。注意这个接入层和原来讲过的应用的接入层不是一个概念。
当一个机架放不下的时候，就需要多个机架，还需要有交换机将多个机架连接在一起。这些交换机对性能的要求更高，带宽也更大。这些交换机称为汇聚层交换机（Aggregation Layer）。
数据中心里面的每一个连接都是需要考虑高可用的。这里首先要考虑的是，如果一台机器只有一个网卡，上面连着一个网线，接入到TOR交换机上。如果网卡坏了，或者不小心网线掉了，机器就上不去了。所以，需要至少两个网卡、两个网线插到TOR交换机上，但是两个网卡要工作得像一张网卡一样，这就是常说的网卡绑定（bond）。
这就需要服务器和交换机都支持一种协议LACP（Link Aggregation Control Protocol）。它们互相通信，将多个网卡聚合称为一个网卡，多个网线聚合成一个网线，在网线之间可以进行负载均衡，也可以为了高可用作准备。
网卡有了高可用保证，但交换机还有问题。如果一个机架只有一个交换机，它挂了，那整个机架都不能上网了。因而TOR交换机也需要高可用，同理接入层和汇聚层的连接也需要高可用性，也不能单线连着。
最传统的方法是，部署两个接入交换机、两个汇聚交换机。服务器和两个接入交换机都连接，接入交换机和两个汇聚都连接，当然这样会形成环，所以需要启用STP协议，去除环，但是这样两个汇聚就只能一主一备了。STP协议里我们学过，只有一条路会起作用。
交换机有一种技术叫作堆叠，所以另一种方法是，将多个交换机形成一个逻辑的交换机，服务器通过多根线分配连到多个接入层交换机上，而接入层交换机多根线分别连接到多个交换机上，并且通过堆叠的私有协议，形成双活的连接方式。
由于对带宽要求更大，而且挂了影响也更大，所以两个堆叠可能就不够了，可以就会有更多的，比如四个堆叠为一个逻辑的交换机。
汇聚层将大量的计算节点相互连接在一起，形成一个集群。在这个集群里面，服务器之间通过二层互通，这个区域常称为一个POD（Point Of Delivery），有时候也称为一个可用区（Available Zone）。
当节点数目再多的时候，一个可用区放不下，需要将多个可用区连在一起，连接多个可用区的交换机称为核心交换机。
核心交换机吞吐量更大，高可用要求更高，肯定需要堆叠，但是往往仅仅堆叠，不足以满足吞吐量，因而还是需要部署多组核心交换机。核心和汇聚交换机之间为了高可用，也是全互连模式的。
这个时候还存在一个问题，出现环路怎么办？
一种方式是，不同的可用区在不同的二层网络，需要分配不同的网段。汇聚和核心之间通过三层网络互通的，二层都不在一个广播域里面，不会存在二层环路的问题。三层有环是没有问题的，只要通过路由协议选择最佳的路径就可以了。那为啥二层不能有环路，而三层可以呢？你可以回忆一下二层环路的情况。
如图，核心层和汇聚层之间通过内部的路由协议OSPF，找到最佳的路径进行访问，而且还可以通过ECMP等价路由，在多个路径之间进行负载均衡和高可用。
但是随着数据中心里面的机器越来越多，尤其是有了云计算、大数据，集群规模非常大，而且都要求在一个二层网络里面。这就需要二层互连从汇聚层上升为核心层，也即在核心以下，全部是二层互连，全部在一个广播域里面，这就是常说的大二层。
如果大二层横向流量不大，核心交换机数目不多，可以做堆叠，但是如果横向流量很大，仅仅堆叠满足不了，就需要部署多组核心交换机，而且要和汇聚层进行全互连。由于堆叠只解决一个核心交换机组内的无环问题，而组之间全互连，还需要其他机制进行解决。
如果是STP，那部署多组核心无法扩大横向流量的能力，因为还是只有一组起作用。
于是大二层就引入了TRILL（Transparent Interconnection of Lots of Link），即多链接透明互联协议。它的基本思想是，二层环有问题，三层环没有问题，那就把三层的路由能力模拟在二层实现。
运行TRILL协议的交换机称为RBridge，是具有路由转发特性的网桥设备，只不过这个路由是根据MAC地址来的，不是根据IP来的。
Rbridage之间通过链路状态协议运作。记得这个路由协议吗？通过它可以学习整个大二层的拓扑，知道访问哪个MAC应该从哪个网桥走；还可以计算最短的路径，也可以通过等价的路由进行负载均衡和高可用性。
TRILL协议在原来的MAC头外面加上自己的头，以及外层的MAC头。TRILL头里面的Ingress RBridge，有点像IP头里面的源IP地址，Egress RBridge是目标IP地址，这两个地址是端到端的，在中间路由的时候，不会发生改变。而外层的MAC，可以有下一跳的Bridge，就像路由的下一跳，也是通过MAC地址来呈现的一样。
如图中所示的过程，有一个包要从主机A发送到主机B，中间要经过RBridge 1、RBridge 2、RBridge X等等，直到RBridge 3。在RBridge 2收到的包里面，分内外两层，内层就是传统的主机A和主机B的MAC地址以及内层的VLAN。
在外层首先加上一个TRILL头，里面描述这个包从RBridge 1进来的，要从RBridge 3出去，并且像三层的IP地址一样有跳数。然后再外面，目的MAC是RBridge 2，源MAC是RBridge 1，以及外层的VLAN。
当RBridge 2收到这个包之后，首先看MAC是否是自己的MAC，如果是，要看自己是不是Egress RBridge，也即是不是最后一跳；如果不是，查看跳数是不是大于0，然后通过类似路由查找的方式找到下一跳RBridge X，然后将包发出去。
RBridge 2发出去的包，内层的信息是不变的，外层的TRILL头里面。同样，描述这个包从RBridge 1进来的，要从RBridge 3出去，但是跳数要减1。外层的目标MAC变成RBridge X，源MAC变成RBridge 2。
如此一直转发，直到RBridge 3，将外层解出来，发送内层的包给主机B。</description></item><item><title>第22讲_VPN：朝中有人好做官</title><link>https://artisanbox.github.io/5/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/14/</guid><description>前面我们讲到了数据中心，里面很复杂，但是有的公司有多个数据中心，需要将多个数据中心连接起来，或者需要办公室和数据中心连接起来。这该怎么办呢？
第一种方式是走公网，但是公网太不安全，你的隐私可能会被别人偷窥。
第二种方式是租用专线的方式把它们连起来，这是土豪的做法，需要花很多钱。
第三种方式是用VPN来连接，这种方法比较折中，安全又不贵。
VPN，全名Virtual Private Network，虚拟专用网，就是利用开放的公众网络，建立专用数据传输通道，将远程的分支机构、移动办公人员等连接起来。
VPN是如何工作的？VPN通过隧道技术在公众网络上仿真一条点到点的专线，是通过利用一种协议来传输另外一种协议的技术，这里面涉及三种协议：乘客协议、隧道协议和承载协议。
我们以IPsec协议为例来说明。
你知道如何通过自驾进行海南游吗？这其中，你的车怎么通过琼州海峡呢？这里用到轮渡，其实这就用到隧道协议。
在广州这边开车是有“协议”的，例如靠右行驶、红灯停、绿灯行，这个就相当于“被封装”的乘客协议。当然在海南那面，开车也是同样的协议。这就相当于需要连接在一起的一个公司的两个分部。
但是在海上坐船航行，也有它的协议，例如要看灯塔、要按航道航行等。这就是外层的承载协议。
那我的车如何从广州到海南呢？这就需要你遵循开车的协议，将车开上轮渡，所有通过轮渡的车都关在船舱里面，按照既定的规则排列好，这就是隧道协议。
在大海上，你的车是关在船舱里面的，就像在隧道里面一样，这个时候内部的乘客协议，也即驾驶协议没啥用处，只需要船遵从外层的承载协议，到达海南就可以了。
到达之后，外部承载协议的任务就结束了，打开船舱，将车开出来，就相当于取下承载协议和隧道协议的头。接下来，在海南该怎么开车，就怎么开车，还是内部的乘客协议起作用。
在最前面的时候说了，直接使用公网太不安全，所以接下来我们来看一种十分安全的VPN，IPsec VPN。这是基于IP协议的安全隧道协议，为了保证在公网上面信息的安全，因而采取了一定的机制保证安全性。
机制一：私密性，防止信息泄露给未经授权的个人，通过加密把数据从明文变成无法读懂的密文，从而确保数据的私密性。
前面讲HTTPS的时候，说过加密可以分为对称加密和非对称加密。对称加密速度快一些。而VPN一旦建立，需要传输大量数据，因而我们采取对称加密。但是同样，对称加密还是存在加密密钥如何传输的问题，这里需要用到因特网密钥交换（IKE，Internet Key Exchange）协议。
机制二：完整性，数据没有被非法篡改，通过对数据进行hash运算，产生类似于指纹的数据摘要，以保证数据的完整性。
机制三：真实性，数据确实是由特定的对端发出，通过身份认证可以保证数据的真实性。
那如何保证对方就是真正的那个人呢？
第一种方法就是预共享密钥，也就是双方事先商量好一个暗号，比如“天王盖地虎，宝塔镇河妖”，对上了，就说明是对的。
另外一种方法就是用数字签名来验证。咋签名呢？当然是使用私钥进行签名，私钥只有我自己有，所以如果对方能用我的数字证书里面的公钥解开，就说明我是我。
基于以上三个特性，组成了IPsec VPN的协议簇。这个协议簇内容比较丰富。
在这个协议簇里面，有两种协议，这两种协议的区别在于封装网络包的格式不一样。
一种协议称为AH（Authentication Header），只能进行数据摘要 ，不能实现数据加密。
还有一种ESP（Encapsulating Security Payload），能够进行数据加密和数据摘要。
在这个协议簇里面，还有两类算法，分别是加密算法和摘要算法。
这个协议簇还包含两大组件，一个用于VPN的双方要进行对称密钥的交换的IKE组件，另一个是VPN的双方要对连接进行维护的SA（Security Association）组件。
IPsec VPN的建立过程下面来看IPsec VPN的建立过程，这个过程分两个阶段。
第一个阶段，建立IKE自己的SA。这个SA用来维护一个通过身份认证和安全保护的通道，为第二个阶段提供服务。在这个阶段，通过DH（Diffie-Hellman）算法计算出一个对称密钥K。
DH算法是一个比较巧妙的算法。客户端和服务端约定两个公开的质数p和q，然后客户端随机产生一个数a作为自己的私钥，服务端随机产生一个b作为自己的私钥，客户端可以根据p、q和a计算出公钥A，服务端根据p、q和b计算出公钥B，然后双方交换公钥A和B。
到此客户端和服务端可以根据已有的信息，各自独立算出相同的结果K，就是对称密钥。但是这个过程，对称密钥从来没有在通道上传输过，只传输了生成密钥的材料，通过这些材料，截获的人是无法算出的。
有了这个对称密钥K，接下来是第二个阶段，建立IPsec SA。在这个SA里面，双方会生成一个随机的对称密钥M，由K加密传给对方，然后使用M进行双方接下来通信的数据。对称密钥M是有过期时间的，会过一段时间，重新生成一次，从而防止被破解。</description></item><item><title>第23讲_移动网络：去巴塞罗那，手机也上不了脸书</title><link>https://artisanbox.github.io/5/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/15/</guid><description>前面讲的都是电脑上网的场景，那使用手机上网有什么不同呢？
移动网络的发展历程你一定知道手机上网有2G、3G、4G的说法，究竟这都是什么意思呢？有一个通俗的说法就是：用2G看txt，用3G看jpg，用4G看avi。
2G网络手机本来是用来打电话的，不是用来上网的，所以原来在2G时代，上网使用的不是IP网络，而是电话网络，走模拟信号，专业名称为公共交换电话网（PSTN，Public Switched Telephone Network）。
那手机不连网线，也不连电话线，它是怎么上网的呢？
手机是通过收发无线信号来通信的，专业名称是Mobile Station，简称MS，需要嵌入SIM。手机是客户端，而无线信号的服务端，就是基站子系统（BSS，Base Station SubsystemBSS）。至于什么是基站，你可以回想一下，你在爬山的时候，是不是看到过信号塔？我们平时城市里面的基站比较隐蔽，不容易看到，所以只有在山里才会注意到。正是这个信号塔，通过无线信号，让你的手机可以进行通信。
但是你要知道一点，无论无线通信如何无线，最终还是要连接到有线的网络里。前面讲数据中心的时候我也讲过，电商的应用是放在数据中心的，数据中心的电脑都是插着网线的。
因而，基站子系统分两部分，一部分对外提供无线通信，叫作基站收发信台（BTS，Base Transceiver Station），另一部分对内连接有线网络，叫作基站控制器（BSC，Base Station Controller）。基站收发信台通过无线收到数据后，转发给基站控制器。
这部分属于无线的部分，统称为无线接入网（RAN，Radio Access Network）。
基站控制器通过有线网络，连接到提供手机业务的运营商的数据中心，这部分称为核心网（CN，Core Network）。核心网还没有真的进入互联网，这部分还是主要提供手机业务，是手机业务的有线部分。
首先接待基站来的数据的是移动业务交换中心（MSC，Mobile Service Switching Center），它是进入核心网的入口，但是它不会让你直接连接到互联网上。
因为在让你的手机真正进入互联网之前，提供手机业务的运营商，需要认证是不是合法的手机接入。你别自己造了一张手机卡，就连接上来。鉴权中心（AUC，Authentication Center）和设备识别寄存器（EIR，Equipment Identity Register）主要是负责安全性的。
另外，需要看你是本地的号，还是外地的号，这个牵扯到计费的问题，异地收费还是很贵的。访问位置寄存器（VLR，Visit Location Register）是看你目前在的地方，归属位置寄存器（HLR，Home Location Register）是看你的号码归属地。
当你的手机卡既合法又有钱的时候，才允许你上网，这个时候需要一个网关，连接核心网和真正的互联网。网关移动交换中心（GMSC ，Gateway Mobile Switching Center）就是干这个的，然后是真正的互连网。在2G时代，还是电话网络PSTN。
数据中心里面的这些模块统称为网络子系统（NSS，Network and Switching Subsystem）。
因而2G时代的上网如图所示，我们总结一下，有这几个核心点：
手机通过无线信号连接基站；
基站一面朝前接无线，一面朝后接核心网；
核心网一面朝前接到基站请求，一是判断你是否合法，二是判断你是不是本地号，还有没有钱，一面通过网关连接电话网络。
2.5G网络后来从2G到了2.5G，也即在原来电路交换的基础上，加入了分组交换业务，支持Packet的转发，从而支持IP网络。
在上述网络的基础上，基站一面朝前接无线，一面朝后接核心网。在朝后的组件中，多了一个分组控制单元（PCU，Packet Control Unit），用以提供分组交换通道。
在核心网里面，有个朝前的接待员（SGSN，Service GPRS Supported Node）和朝后连接IP网络的网关型GPRS支持节点（GGSN，Gateway GPRS Supported Node）。
3G网络到了3G时代，主要是无线通信技术有了改进，大大增加了无线的带宽。
以W-CDMA为例，理论最高2M的下行速度，因而基站改变了，一面朝外的是Node B，一面朝内连接核心网的是无线网络控制器（RNC，Radio Network Controller）。核心网以及连接的IP网络没有什么变化。</description></item><item><title>第24讲_云中网络：自己拿地成本高，购买公寓更灵活</title><link>https://artisanbox.github.io/5/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/16/</guid><description>前面我们讲了，数据中心里面堆着一大片一大片的机器，用网络连接起来，机器数目一旦非常多，人们就发现，维护这么一大片机器还挺麻烦的，有好多不灵活的地方。
采购不灵活：如果客户需要一台电脑，那就需要自己采购、上架、插网线、安装操作系统，周期非常长。一旦采购了，一用就N年，不能退货，哪怕业务不做了，机器还在数据中心里留着。
运维不灵活：一旦需要扩容CPU、内存、硬盘，都需要去机房手动弄，非常麻烦。
规格不灵活：采购的机器往往动不动几百G的内存，而每个应用往往可能只需要4核8G，所以很多应用混合部署在上面，端口各种冲突，容易相互影响。
复用不灵活：一台机器，一旦一个用户不用了，给另外一个用户，那就需要重装操作系统。因为原来的操作系统可能遗留很多数据，非常麻烦。
从物理机到虚拟机为了解决这些问题，人们发明了一种叫虚拟机的东西，并基于它产生了云计算技术。
其实在你的个人电脑上，就可以使用虚拟机。如果你对虚拟机没有什么概念，你可以下载一个桌面虚拟化的软件，自己动手尝试一下。它可以让你灵活地指定CPU的数目、内存的大小、硬盘的大小，可以有多个网卡，然后在一台笔记本电脑里面创建一台或者多台虚拟电脑。不用的时候，一点删除就没有了。
在数据中心里面，也有一种类似的开源技术qemu-kvm，能让你在一台巨大的物理机里面，掏出一台台小的机器。这套软件就能解决上面的问题：一点就能创建，一点就能销毁。你想要多大就有多大，每次创建的系统还都是新的。
我们常把物理机比喻为自己拿地盖房子，而虚拟机则相当于购买公寓，更加灵活方面，随时可买可卖。 那这个软件为什么能做到这些事儿呢？
它用的是软件模拟硬件的方式。刚才说了，数据中心里面用的qemu-kvm。从名字上来讲，emu就是Emulator（模拟器）的意思，主要会模拟CPU、内存、网络、硬盘，使得虚拟机感觉自己在使用独立的设备，但是真正使用的时候，当然还是使用物理的设备。
例如，多个虚拟机轮流使用物理CPU，内存也是使用虚拟内存映射的方式，最终映射到物理内存上。硬盘在一块大的文件系统上创建一个N个G的文件，作为虚拟机的硬盘。
简单比喻，虚拟化软件就像一个“骗子”，向上“骗”虚拟机里面的应用，让它们感觉独享资源，其实自己啥都没有，全部向下从物理机里面弄。
虚拟网卡的原理那网络是如何“骗”应用的呢？如何将虚拟机的网络和物理机的网络连接起来？
首先，虚拟机要有一张网卡。对于qemu-kvm来说，这是通过Linux上的一种TUN/TAP技术来实现的。
虚拟机是物理机上跑着的一个软件。这个软件可以像其他应用打开文件一样，打开一个称为TUN/TAP的Char Dev（字符设备文件）。打开了这个字符设备文件之后，在物理机上就能看到一张虚拟TAP网卡。
虚拟化软件作为“骗子”，会将打开的这个文件，在虚拟机里面虚拟出一张网卡，让虚拟机里面的应用觉得它们真有一张网卡。于是，所有的网络包都往这里发。
当然，网络包会到虚拟化软件这里。它会将网络包转换成为文件流，写入字符设备，就像写一个文件一样。内核中TUN/TAP字符设备驱动会收到这个写入的文件流，交给TUN/TAP的虚拟网卡驱动。这个驱动将文件流再次转成网络包，交给TCP/IP协议栈，最终从虚拟TAP网卡发出来，成为标准的网络包。
就这样，几经转手，数据终于从虚拟机里面，发到了虚拟机外面。
虚拟网卡连接到云中我们就这样有了虚拟TAP网卡。接下来就要看，这个卡怎么接入庞大的数据中心网络中。
在接入之前，我们先来看，云计算中的网络都需要注意哪些点。
共享：尽管每个虚拟机都会有一个或者多个虚拟网卡，但是物理机上可能只有有限的网卡。那这么多虚拟网卡如何共享同一个出口？
隔离：分两个方面，一个是安全隔离，两个虚拟机可能属于两个用户，那怎么保证一个用户的数据不被另一个用户窃听？一个是流量隔离，两个虚拟机，如果有一个疯狂下片，会不会导致另外一个上不了网？
互通：分两个方面，一个是如果同一台机器上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？另一个是如果不同物理机上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？
灵活：虚拟机和物理不同，会经常创建、删除，从一个机器漂移到另一台机器，有的互通、有的不通等等，灵活性比物理网络要好得多，需要能够灵活配置。
共享与互通问题这些问题，我们一个个来解决。
首先，一台物理机上有多个虚拟机，有多个虚拟网卡，这些虚拟网卡如何连在一起，进行相互访问，并且可以访问外网呢？
还记得我们在大学宿舍里做的事情吗？你可以想象你的物理机就是你们宿舍，虚拟机就是你的个人电脑，这些电脑应该怎么连接起来呢？当然应该买一个交换机。
在物理机上，应该有一个虚拟的交换机，在Linux上有一个命令叫作brctl，可以创建虚拟的网桥brctl addbr br0。创建出来以后，将两个虚拟机的虚拟网卡，都连接到虚拟网桥brctl addif br0 tap0上，这样将两个虚拟机配置相同的子网网段，两台虚拟机就能够相互通信了。
那这些虚拟机如何连接外网呢？在桌面虚拟化软件上面，我们能看到以下选项。
这里面，host-only的网络对应的，其实就是上面两个虚拟机连到一个br0虚拟网桥上，而且不考虑访问外部的场景，只要虚拟机之间能够相互访问就可以了。
如果要访问外部，往往有两种方式。
一种方式称为桥接。如果在桌面虚拟化软件上选择桥接网络，则在你的笔记本电脑上，就会形成下面的结构。
每个虚拟机都会有虚拟网卡，在你的笔记本电脑上，会发现多了几个网卡，其实是虚拟交换机。这个虚拟交换机将虚拟机连接在一起。在桥接模式下，物理网卡也连接到这个虚拟交换机上，物理网卡在桌面虚拟化软件上，在“界面名称”那里选定。
如果使用桥接网络，当你登录虚拟机里看IP地址的时候会发现，你的虚拟机的地址和你的笔记本电脑的，以及你旁边的同事的电脑的网段是一个网段。这是为什么呢？这其实相当于将物理机和虚拟机放在同一个网桥上，相当于这个网桥上有三台机器，是一个网段的，全部打平了。我将图画成下面的样子你就好理解了。
在数据中心里面，采取的也是类似的技术，只不过都是Linux，在每台机器上都创建网桥br0，虚拟机的网卡都连到br0上，物理网卡也连到br0上，所有的br0都通过物理网卡出来连接到物理交换机上。
同样我们换一个角度看待这个拓扑图。同样是将网络打平，虚拟机会和你的物理网络具有相同的网段。
在这种方式下，不但解决了同一台机器的互通问题，也解决了跨物理机的互通问题，因为都在一个二层网络里面，彼此用相同的网段访问就可以了。但是当规模很大的时候，会存在问题。
你还记得吗？在一个二层网络里面，最大的问题是广播。一个数据中心的物理机已经很多了，广播已经非常严重，需要通过VLAN进行划分。如果使用了虚拟机，假设一台物理机里面创建10台虚拟机，全部在一个二层网络里面，那广播就会很严重，所以除非是你的桌面虚拟机或者数据中心规模非常小，才可以使用这种相对简单的方式。
另外一种方式称为NAT。如果在桌面虚拟化软件中使用NAT模式，在你的笔记本电脑上会出现如下的网络结构。
在这种方式下，你登录到虚拟机里面查看IP地址，会发现虚拟机的网络是虚拟机的，物理机的网络是物理机的，两个不相同。虚拟机要想访问物理机的时候，需要将地址NAT成为物理机的地址。
除此之外，它还会在你的笔记本电脑里内置一个DHCP服务器，为笔记本电脑上的虚拟机动态分配IP地址。因为虚拟机的网络自成体系，需要进行IP管理。为什么桥接方式不需要呢？因为桥接将网络打平了，虚拟机的IP地址应该由物理网络的DHCP服务器分配。
在数据中心里面，也是使用类似的方式。这种方式更像是真的将你宿舍里面的情况，搬到一台物理机上来。
虚拟机是你的电脑，路由器和DHCP Server相当于家用路由器或者寝室长的电脑，物理网卡相当于你们宿舍的外网网口，用于访问互联网。所有电脑都通过内网网口连接到一个网桥br0上，虚拟机要想访问互联网，需要通过br0连到路由器上，然后通过路由器将请求NAT成为物理网络的地址，转发到物理网络。</description></item><item><title>第25讲_软件定义网络：共享基础设施的小区物业管理办法</title><link>https://artisanbox.github.io/5/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/17/</guid><description>上一节我们说到，使用原生的VLAN和Linux网桥的方式来进行云平台的管理，但是这样在灵活性、隔离性方面都显得不足，而且整个网络缺少统一的视图、统一的管理。
可以这样比喻，云计算就像大家一起住公寓，要共享小区里面的基础设施，其中网络就相当于小区里面的电梯、楼道、路、大门等，大家都走，往往会常出现问题，尤其在上班高峰期，出门的人太多，对小区的物业管理就带来了挑战。
物业可以派自己的物业管理人员，到每个单元的楼梯那里，将电梯的上下行速度调快一点，可以派人将隔离健身区、景色区的栅栏门暂时打开，让大家可以横穿小区，直接上地铁，还可以派人将多个小区出入口，改成出口多、入口少等等。等过了十点半，上班高峰过去，再派人都改回来。
软件定义网络（SDN）这种模式就像传统的网络设备和普通的Linux网桥的模式，配置整个云平台的网络通路，你需要登录到这台机器上配置这个，再登录到另外一个设备配置那个，才能成功。
如果物业管理人员有一套智能的控制系统，在物业监控室里就能看到小区里每个单元、每个电梯的人流情况，然后在监控室里面，只要通过远程控制的方式，拨弄一个手柄，电梯的速度就调整了，栅栏门就打开了，某个入口就改出口了。
这就是软件定义网络（SDN）。它主要有以下三个特点。
控制与转发分离：转发平面就是一个个虚拟或者物理的网络设备，就像小区里面的一条条路。控制平面就是统一的控制中心，就像小区物业的监控室。它们原来是一起的，物业管理员要从监控室出来，到路上去管理设备，现在是分离的，路就是走人的，控制都在监控室。
控制平面与转发平面之间的开放接口：控制器向上提供接口，被应用层调用，就像总控室提供按钮，让物业管理员使用。控制器向下调用接口，来控制网络设备，就像总控室会远程控制电梯的速度。这里经常使用两个名词，前面这个接口称为北向接口，后面这个接口称为南向接口，上北下南嘛。
逻辑上的集中控制：逻辑上集中的控制平面可以控制多个转发面设备，也就是控制整个物理网络，因而可以获得全局的网络状态视图，并根据该全局网络状态视图实现对网络的优化控制，就像物业管理员在监控室能够看到整个小区的情况，并根据情况优化出入方案。
OpenFlow和OpenvSwitchSDN有很多种实现方式，我们来看一种开源的实现方式。
OpenFlow是SDN控制器和网络设备之间互通的南向接口协议，OpenvSwitch用于创建软件的虚拟交换机。OpenvSwitch是支持OpenFlow协议的，当然也有一些硬件交换机也支持OpenFlow协议。它们都可以被统一的SDN控制器管理，从而实现物理机和虚拟机的网络连通。
SDN控制器是如何通过OpenFlow协议控制网络的呢？
在OpenvSwitch里面，有一个流表规则，任何通过这个交换机的包，都会经过这些规则进行处理，从而接收、转发、放弃。
那流表长啥样呢？其实就是一个个表格，每个表格好多行，每行都是一条规则。每条规则都有优先级，先看高优先级的规则，再看低优先级的规则。
对于每一条规则，要看是否满足匹配条件。这些条件包括，从哪个端口进来的，网络包头里面有什么等等。满足了条件的网络包，就要执行一个动作，对这个网络包进行处理。可以修改包头里的内容，可以跳到任何一个表格，可以转发到某个网口出去，也可以丢弃。
通过这些表格，可以对收到的网络包随意处理。
具体都能做什么处理呢？通过上面的表格可以看出，简直是想怎么处理怎么处理，可以覆盖TCP/IP协议栈的四层。
对于物理层：
匹配规则包括从哪个口进来；
执行动作包括从哪个口出去。
对于MAC层：
匹配规则包括：源MAC地址是多少？（dl_src），目标MAC是多少？（dl_dst），所属vlan是多少？（dl_vlan）；
执行动作包括：修改源MAC（mod_dl_src），修改目标MAC（mod_dl_dst），修改VLAN（mod_vlan_vid），删除VLAN（strip_vlan），MAC地址学习（learn）。
对于网络层：
匹配规则包括：源IP地址是多少？(nw_src)，目标IP是多少？（nw_dst）。
执行动作包括：修改源IP地址（mod_nw_src），修改目标IP地址（mod_nw_dst）。
对于传输层：
匹配规则包括：源端口是多少？（tp_src），目标端口是多少？（tp_dst）。
执行动作包括：修改源端口（mod_tp_src），修改目标端口（mod_tp_dst）。
总而言之，对于OpenvSwitch来讲，网络包到了我手里，就是一个Buffer，我想怎么改怎么改，想发到哪个端口就发送到哪个端口。
OpenvSwitch有本地的命令行可以进行配置，能够实验咱们前面讲过的一些功能。我们可以通过OpenvSwitch的命令创建一个虚拟交换机。然后可以将多个虚拟端口port添加到这个虚拟交换机上。比如说下面这个add-br命令，就是创建虚拟交换机的。
ovs-vsctl add-br br0 实验一：用OpenvSwitch实现VLAN的功能下面我们实验一下通过OpenvSwitch实现VLAN的功能，在OpenvSwitch中端口port分两种，分别叫做access port和trunk port。
第一类是access port：
这个端口可以配置一个tag，其实就是一个VLAN ID，从这个端口进来的包都会被打上这个tag； 如果网络包本身带有某个VLAN ID并且等于这个tag，则这个包就会从这个port发出去； 从access port发出的包就会把VLAN ID去掉。 第二类是trunk port：</description></item><item><title>第26讲_云中的网络安全：虽然不是土豪，也需要基本安全和保障</title><link>https://artisanbox.github.io/5/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/18/</guid><description>在今天的内容开始之前，我先卖个关子。文章结尾，我会放一个超级彩蛋，所以，今天的内容你一定要看到最后哦！
上一节我们看到，做一个小区物业维护一个大家共享的环境，还是挺不容易的。如果都是自觉遵守规则的住户那还好，如果遇上不自觉的住户就会很麻烦。
就像公有云的环境，其实没有你想的那么纯净，各怀鬼胎的黑客到处都是。扫描你的端口呀，探测一下你启动的什么应用啊，看一看是否有各种漏洞啊。这就像小偷潜入小区后，这儿看看，那儿瞧瞧，窗户有没有关严了啊，窗帘有没有拉上啊，主人睡了没，是不是时机潜入室内啊，等等。
假如你创建了一台虚拟机，里面明明跑了一个电商应用，这是你非常重要的一个应用，你会把它进行安全加固。这台虚拟机的操作系统里，不小心安装了另外一个后台应用，监听着一个端口，而你的警觉性没有这么高。
虚拟机的这个端口是对着公网开放的，碰巧这个后台应用本身是有漏洞的，黑客就可以扫描到这个端口，然后通过这个后台应用的端口侵入你的机器，将你加固好的电商网站黑掉。这就像你买了一个五星级的防盗门，卡车都撞不开，但是厕所窗户的门把手是坏的，小偷从厕所里面就进来了。
所以对于公有云上的虚拟机，我的建议是仅仅开放需要的端口，而将其他的端口一概关闭。这个时候，你只要通过安全措施守护好这个唯一的入口就可以了。采用的方式常常是用ACL（Access Control List，访问控制列表）来控制IP和端口。
设置好了这些规则，只有指定的IP段能够访问指定的开放接口，就算有个有漏洞的后台进程在那里，也会被屏蔽，黑客进不来。在云平台上，这些规则的集合常称为安全组。那安全组怎么实现呢？
我们来复习一下，当一个网络包进入一台机器的时候，都会做什么事情。
首先拿下MAC头看看，是不是我的。如果是，则拿下IP头来。得到目标IP之后呢，就开始进行路由判断。在路由判断之前，这个节点我们称为PREROUTING。如果发现IP是我的，包就应该是我的，就发给上面的传输层，这个节点叫作INPUT。如果发现IP不是我的，就需要转发出去，这个节点称为FORWARD。如果是我的，上层处理完毕后，一般会返回一个处理结果，这个处理结果会发出去，这个节点称为OUTPUT，无论是FORWARD还是OUTPUT，都是路由判断之后发生的，最后一个节点是POSTROUTING。
整个过程如图所示。
整个包的处理过程还是原来的过程，只不过为什么要格外关注这五个节点呢？
是因为在Linux内核中，有一个框架叫Netfilter。它可以在这些节点插入hook函数。这些函数可以截获数据包，对数据包进行干预。例如做一定的修改，然后决策是否接着交给TCP/IP协议栈处理；或者可以交回给协议栈，那就是ACCEPT；或者过滤掉，不再传输，就是DROP；还有就是QUEUE，发送给某个用户态进程处理。
这个比较难理解，经常用在内部负载均衡，就是过来的数据一会儿传给目标地址1，一会儿传给目标地址2，而且目标地址的个数和权重都可能变。协议栈往往处理不了这么复杂的逻辑，需要写一个函数接管这个数据，实现自己的逻辑。
有了这个Netfilter框架就太好了，你可以在IP转发的过程中，随时干预这个过程，只要你能实现这些hook函数。
一个著名的实现，就是内核模块ip_tables。它在这五个节点上埋下函数，从而可以根据规则进行包的处理。按功能可分为四大类：连接跟踪（conntrack）、数据包的过滤（filter）、网络地址转换（nat）和数据包的修改（mangle）。其中连接跟踪是基础功能，被其他功能所依赖。其他三个可以实现包的过滤、修改和网络地址转换。
在用户态，还有一个你肯定知道的客户端程序iptables，用命令行来干预内核的规则。内核的功能对应iptables的命令行来讲，就是表和链的概念。
iptables的表分为四种：raw--&amp;gt;mangle--&amp;gt;nat--&amp;gt;filter。这四个优先级依次降低，raw不常用，所以主要功能都在其他三种表里实现。每个表可以设置多个链。
filter表处理过滤功能，主要包含三个链：
INPUT链：过滤所有目标地址是本机的数据包；
FORWARD链：过滤所有路过本机的数据包；
OUTPUT链：过滤所有由本机产生的数据包。
nat表主要是处理网络地址转换，可以进行Snat（改变数据包的源地址）、Dnat（改变数据包的目标地址），包含三个链：
PREROUTING链：可以在数据包到达防火墙时改变目标地址；
OUTPUT链：可以改变本地产生的数据包的目标地址；
POSTROUTING链：在数据包离开防火墙时改变数据包的源地址。
mangle表主要是修改数据包，包含：
PREROUTING链；
INPUT链；
FORWARD链；
OUTPUT链；
POSTROUTING链。
将iptables的表和链加入到上面的过程图中，就形成了下面的图和过程。
数据包进入的时候，先进mangle表的PREROUTING链。在这里可以根据需要，改变数据包头内容之后，进入nat表的PREROUTING链，在这里可以根据需要做Dnat，也就是目标地址转换。
进入路由判断，要判断是进入本地的还是转发的。
如果是进入本地的，就进入INPUT链，之后按条件过滤限制进入。</description></item><item><title>第27讲_云中的网络QoS：邻居疯狂下电影，我该怎么办？</title><link>https://artisanbox.github.io/5/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/19/</guid><description>在小区里面，是不是经常有住户不自觉就霸占公共通道，如果你找他理论，他的话就像一个相声《楼道曲》说的一样：“公用公用，你用我用，大家都用，我为什么不能用？”。
除此之外，你租房子的时候，有没有碰到这样的情况：本来合租共享WiFi，一个人狂下小电影，从而你网都上不去，是不是很懊恼？
在云平台上，也有这种现象，好在有一种流量控制的技术，可以实现QoS（Quality of Service），从而保障大多数用户的服务质量。
对于控制一台机器的网络的QoS，分两个方向，一个是入方向，一个是出方向。
其实我们能控制的只有出方向，通过Shaping，将出的流量控制成自己想要的模样。而进入的方向是无法控制的，只能通过Policy将包丢弃。
控制网络的QoS有哪些方式？在Linux下，可以通过TC控制网络的QoS，主要就是通过队列的方式。
无类别排队规则第一大类称为无类别排队规则（Classless Queuing Disciplines）。还记得我们讲ip addr的时候讲过的pfifo_fast，这是一种不把网络包分类的技术。
pfifo_fast分为三个先入先出的队列，称为三个Band。根据网络包里面TOS，看这个包到底应该进入哪个队列。TOS总共四位，每一位表示的意思不同，总共十六种类型。
通过命令行tc qdisc show dev eth0，可以输出结果priomap，也是十六个数字。在0到2之间，和TOS的十六种类型对应起来，表示不同的TOS对应的不同的队列。其中Band 0优先级最高，发送完毕后才轮到Band 1发送，最后才是Band 2。
另外一种无类别队列规则叫作随机公平队列（Stochastic Fair Queuing）。
会建立很多的FIFO的队列，TCP Session会计算hash值，通过hash值分配到某个队列。在队列的另一端，网络包会通过轮询策略从各个队列中取出发送。这样不会有一个Session占据所有的流量。
当然如果两个Session的hash是一样的，会共享一个队列，也有可能互相影响。hash函数会经常改变，从而session不会总是相互影响。
还有一种无类别队列规则称为令牌桶规则（TBF，Token Bucket Filte）。
所有的网络包排成队列进行发送，但不是到了队头就能发送，而是需要拿到令牌才能发送。
令牌根据设定的速度生成，所以即便队列很长，也是按照一定的速度进行发送的。
当没有包在队列中的时候，令牌还是以既定的速度生成，但是不是无限累积的，而是放满了桶为止。设置桶的大小为了避免下面的情况：当长时间没有网络包发送的时候，积累了大量的令牌，突然来了大量的网络包，每个都能得到令牌，造成瞬间流量大增。
基于类别的队列规则另外一大类是基于类别的队列规则（Classful Queuing Disciplines），其中典型的为分层令牌桶规则（HTB， Hierarchical Token Bucket）。
HTB往往是一棵树，接下来我举个具体的例子，通过TC如何构建一棵HTB树来带你理解。
使用TC可以为某个网卡eth0创建一个HTB的队列规则，需要付给它一个句柄为（1:）。
这是整棵树的根节点，接下来会有分支。例如图中有三个分支，句柄分别为（:10）、（:11）、（:12）。最后的参数default 12，表示默认发送给1:12，也即发送给第三个分支。
tc qdisc add dev eth0 root handle 1: htb default 12 对于这个网卡，需要规定发送的速度。一般有两个速度可以配置，一个是rate，表示一般情况下的速度；一个是ceil，表示最高情况下的速度。对于根节点来讲，这两个速度是一样的，于是创建一个root class，速度为（rate=100kbps，ceil=100kbps）。
tc class add dev eth0 parent 1: classid 1:1 htb rate 100kbps ceil 100kbps 接下来要创建分支，也即创建几个子class。每个子class统一有两个速度。三个分支分别为（rate=30kbps，ceil=100kbps）、（rate=10kbps，ceil=100kbps）、（rate=60kbps，ceil=100kbps）。</description></item><item><title>第28讲_云中网络的隔离GRE、VXLAN：虽然住一个小区，也要保护隐私</title><link>https://artisanbox.github.io/5/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/20/</guid><description>对于云平台中的隔离问题，前面咱们用的策略一直都是VLAN，但是我们也说过这种策略的问题，VLAN只有12位，共4096个。当时设计的时候，看起来是够了，但是现在绝对不够用，怎么办呢？
一种方式是修改这个协议。这种方法往往不可行，因为当这个协议形成一定标准后，千千万万设备上跑的程序都要按这个规则来。现在说改就放，谁去挨个儿告诉这些程序呢？很显然，这是一项不可能的工程。
另一种方式就是扩展，在原来包的格式的基础上扩展出一个头，里面包含足够用于区分租户的ID，外层的包的格式尽量和传统的一样，依然兼容原来的格式。一旦遇到需要区分用户的地方，我们就用这个特殊的程序，来处理这个特殊的包的格式。
这个概念很像咱们第22讲讲过的隧道理论，还记得自驾游通过摆渡轮到海南岛的那个故事吗？在那一节，我们说过，扩展的包头主要是用于加密的，而我们现在需要的包头是要能够区分用户的。
底层的物理网络设备组成的网络我们称为Underlay网络，而用于虚拟机和云中的这些技术组成的网络称为Overlay网络，这是一种基于物理网络的虚拟化网络实现。这一节我们重点讲两个Overlay的网络技术。
GRE第一个技术是GRE，全称Generic Routing Encapsulation，它是一种IP-over-IP的隧道技术。它将IP包封装在GRE包里，外面加上IP头，在隧道的一端封装数据包，并在通路上进行传输，到另外一端的时候解封装。你可以认为Tunnel是一个虚拟的、点对点的连接。
从这个图中可以看到，在GRE头中，前32位是一定会有的，后面的都是可选的。在前4位标识位里面，有标识后面到底有没有可选项？这里面有个很重要的key字段，是一个32位的字段，里面存放的往往就是用于区分用户的Tunnel ID。32位，够任何云平台喝一壶的了！
下面的格式类型专门用于网络虚拟化的GRE包头格式，称为NVGRE，也给网络ID号24位，也完全够用了。
除此之外，GRE还需要有一个地方来封装和解封装GRE的包，这个地方往往是路由器或者有路由功能的Linux机器。
使用GRE隧道，传输的过程就像下面这张图。这里面有两个网段、两个路由器，中间要通过GRE隧道进行通信。当隧道建立之后，会多出两个Tunnel端口，用于封包、解封包。
主机A在左边的网络，IP地址为192.168.1.102，它想要访问主机B，主机B在右边的网络，IP地址为192.168.2.115。于是发送一个包，源地址为192.168.1.102，目标地址为192.168.2.115。因为要跨网段访问，于是根据默认的default路由表规则，要发给默认的网关192.168.1.1，也即左边的路由器。
根据路由表，从左边的路由器，去192.168.2.0/24这个网段，应该走一条GRE的隧道，从隧道一端的网卡Tunnel0进入隧道。
在Tunnel隧道的端点进行包的封装，在内部的IP头之外加上GRE头。对于NVGRE来讲，是在MAC头之外加上GRE头，然后加上外部的IP地址，也即路由器的外网IP地址。源IP地址为172.17.10.10，目标IP地址为172.16.11.10，然后从E1的物理网卡发送到公共网络里。
在公共网络里面，沿着路由器一跳一跳地走，全部都按照外部的公网IP地址进行。
当网络包到达对端路由器的时候，也要到达对端的Tunnel0，然后开始解封装，将外层的IP头取下来，然后根据里面的网络包，根据路由表，从E3口转发出去到达服务器B。
从GRE的原理可以看出，GRE通过隧道的方式，很好地解决了VLAN ID不足的问题。但是，GRE技术本身还是存在一些不足之处。
首先是Tunnel的数量问题。GRE是一种点对点隧道，如果有三个网络，就需要在每两个网络之间建立一个隧道。如果网络数目增多，这样隧道的数目会呈指数性增长。
其次，GRE不支持组播，因此一个网络中的一个虚机发出一个广播帧后，GRE会将其广播到所有与该节点有隧道连接的节点。
另外一个问题是目前还是有很多防火墙和三层网络设备无法解析GRE，因此它们无法对GRE封装包做合适地过滤和负载均衡。
VXLAN第二种Overlay的技术称为VXLAN。和三层外面再套三层的GRE不同，VXLAN则是从二层外面就套了一个VXLAN的头，这里面包含的VXLAN ID为24位，也够用了。在VXLAN头外面还封装了UDP、IP，以及外层的MAC头。
VXLAN作为扩展性协议，也需要一个地方对VXLAN的包进行封装和解封装，实现这个功能的点称为VTEP（VXLAN Tunnel Endpoint）。
VTEP相当于虚拟机网络的管家。每台物理机上都可以有一个VTEP。每个虚拟机启动的时候，都需要向这个VTEP管家注册，每个VTEP都知道自己上面注册了多少个虚拟机。当虚拟机要跨VTEP进行通信的时候，需要通过VTEP代理进行，由VTEP进行包的封装和解封装。
和GRE端到端的隧道不同，VXLAN不是点对点的，而是支持通过组播的来定位目标机器的，而非一定是这一端发出，另一端接收。
当一个VTEP启动的时候，它们都需要通过IGMP协议。加入一个组播组，就像加入一个邮件列表，或者加入一个微信群一样，所有发到这个邮件列表里面的邮件，或者发送到微信群里面的消息，大家都能收到。而当每个物理机上的虚拟机启动之后，VTEP就知道，有一个新的VM上线了，它归我管。
如图，虚拟机1、2、3属于云中同一个用户的虚拟机，因而需要分配相同的VXLAN ID=101。在云的界面上，就可以知道它们的IP地址，于是可以在虚拟机1上ping虚拟机2。
虚拟机1发现，它不知道虚拟机2的MAC地址，因而包没办法发出去，于是要发送ARP广播。
ARP请求到达VTEP1的时候，VTEP1知道，我这里有一台虚拟机，要访问一台不归我管的虚拟机，需要知道MAC地址，可是我不知道啊，这该咋办呢？
VTEP1想，我不是加入了一个微信群么？可以在里面@all 一下，问问虚拟机2归谁管。于是VTEP1将ARP请求封装在VXLAN里面，组播出去。
当然在群里面，VTEP2和VTEP3都收到了消息，因而都会解开VXLAN包看，里面是一个ARP。
VTEP3在本地广播了半天，没人回，都说虚拟机2不归自己管。
VTEP2在本地广播，虚拟机2回了，说虚拟机2归我管，MAC地址是这个。通过这次通信，VTEP2也学到了，虚拟机1归VTEP1管，以后要找虚拟机1，去找VTEP1就可以了。
VTEP2将ARP的回复封装在VXLAN里面，这次不用组播了，直接发回给VTEP1。
VTEP1解开VXLAN的包，发现是ARP的回复，于是发给虚拟机1。通过这次通信，VTEP1也学到了，虚拟机2归VTEP2管，以后找虚拟机2，去找VTEP2就可以了。
虚拟机1的ARP得到了回复，知道了虚拟机2的MAC地址，于是就可以发送包了。
虚拟机1发给虚拟机2的包到达VTEP1，它当然记得刚才学的东西，要找虚拟机2，就去VTEP2，于是将包封装在VXLAN里面，外层加上VTEP1和VTEP2的IP地址，发送出去。
网络包到达VTEP2之后，VTEP2解开VXLAN封装，将包转发给虚拟机2。
虚拟机2回复的包，到达VTEP2的时候，它当然也记得刚才学的东西，要找虚拟机1，就去VTEP1，于是将包封装在VXLAN里面，外层加上VTEP1和VTEP2的IP地址，也发送出去。
网络包到达VTEP1之后，VTEP1解开VXLAN封装，将包转发给虚拟机1。
有了GRE和VXLAN技术，我们就可以解决云计算中VLAN的限制了。那如何将这个技术融入云平台呢？
还记得将你宿舍里面的情况，所有东西都搬到一台物理机上那个故事吗？
虚拟机是你的电脑，路由器和DHCP Server相当于家用路由器或者寝室长的电脑，外网网口访问互联网，所有的电脑都通过内网网口连接到一个交换机br0上，虚拟机要想访问互联网，需要通过br0连到路由器上，然后通过路由器将请求NAT后转发到公网。
接下来的事情就惨了，你们宿舍闹矛盾了，你们要分成三个宿舍住，对应上面的图，你们寝室长，也即路由器单独在一台物理机上，其他的室友也即VM分别在两台物理机上。这下把一个完整的br0一刀三断，每个宿舍都是单独的一段。
可是只有你的寝室长有公网口可以上网，于是你偷偷在三个宿舍中间打了一个隧道，用网线通过隧道将三个宿舍的两个br0连接起来，让其他室友的电脑和你寝室长的电脑，看起来还是连到同一个br0上，其实中间是通过你隧道中的网线做了转发。
为什么要多一个br1这个虚拟交换机呢？主要通过br1这一层将虚拟机之间的互联和物理机机之间的互联分成两层来设计，中间隧道可以有各种挖法，GRE、VXLAN都可以。
使用了OpenvSwitch之后，br0可以使用OpenvSwitch的Tunnel功能和Flow功能。
OpenvSwitch支持三类隧道：GRE、VXLAN、IPsec_GRE。在使用OpenvSwitch的时候，虚拟交换机就相当于GRE和VXLAN封装的端点。
我们模拟创建一个如下的网络拓扑结构，来看隧道应该如何工作。
三台物理机，每台上都有两台虚拟机，分别属于两个不同的用户，因而VLAN tag都得打地不一样，这样才不能相互通信。但是不同物理机上的相同用户，是可以通过隧道相互通信的，因而通过GRE隧道可以连接到一起。</description></item><item><title>第29讲_容器网络：来去自由的日子，不买公寓去合租</title><link>https://artisanbox.github.io/5/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/21/</guid><description>如果说虚拟机是买公寓，容器则相当于合租，有一定的隔离，但是隔离性没有那么好。云计算解决了基础资源层的弹性伸缩，却没有解决PaaS层应用随基础资源层弹性伸缩而带来的批量、快速部署问题。于是，容器应运而生。
容器就是Container，而Container的另一个意思是集装箱。其实容器的思想就是要变成软件交付的集装箱。集装箱的特点，一是打包，二是标准。
在没有集装箱的时代，假设要将货物从A运到B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，弄得乱七八糟，然后还要再搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能干完活。
有了尺寸全部都一样的集装箱以后，可以把所有的货物都打包在一起，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不用耗费很长时间了。这是集装箱的“打包”“标准”两大特点在生活中的应用。
那么容器如何对应用打包呢？
学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。
封闭的环境主要使用了两种技术，一种是看起来是隔离的技术，称为namespace，也即每个 namespace中的应用看到的是不同的 IP地址、用户空间、程号等。另一种是用起来是隔离的技术，称为cgroup，也即明明整台机器有很多的 CPU、内存，而一个应用只能用其中的一部分。
有了这两项技术，就相当于我们焊好了集装箱。接下来的问题就是如何“将这个集装箱标准化”，并在哪艘船上都能运输。这里的标准首先就是镜像。
所谓镜像，就是将你焊好集装箱的那一刻，将集装箱的状态保存下来，就像孙悟空说：“定！”，集装箱里的状态就被定在了那一刻，然后将这一刻的状态保存成一系列文件。无论从哪里运行这个镜像，都能完整地还原当时的情况。
接下来我们就具体来看看，这两种网络方面的打包技术。
命名空间（namespace）我们首先来看网络namespace。
namespace翻译过来就是命名空间。其实很多面向对象的程序设计语言里面，都有命名空间这个东西。大家一起写代码，难免会起相同的名词，编译就会冲突。而每个功能都有自己的命名空间，在不同的空间里面，类名相同，不会冲突。
在Linux下也是这样的，很多的资源都是全局的。比如进程有全局的进程ID，网络也有全局的路由表。但是，当一台Linux上跑多个进程的时候，如果我们觉得使用不同的路由策略，这些进程可能会冲突，那就需要将这个进程放在一个独立的namespace里面，这样就可以独立配置网络了。
网络的namespace由ip netns命令操作。它可以创建、删除、查询namespace。
我们再来看将你们宿舍放进一台物理机的那个图。你们宿舍长的电脑是一台路由器，你现在应该知道怎么实现这个路由器吧？可以创建一个Router虚拟机来做这件事情，但是还有一个更加简单的办法，就是我在图里画的这条虚线，这个就是通过namespace实现的。
我们创建一个routerns，于是一个独立的网络空间就产生了。你可以在里面尽情设置自己的规则。
ip netns add routerns 既然是路由器，肯定要能转发嘛，因而forward开关要打开。
ip netns exec routerns sysctl -w net.ipv4.ip_forward=1 exec的意思就是进入这个网络空间做点事情。初始化一下iptables，因为这里面要配置NAT规则。
ip netns exec routerns iptables-save -c ip netns exec routerns iptables-restore -c 路由器需要有一张网卡连到br0上，因而要创建一个网卡。
ovs-vsctl -- add-port br0 taprouter -- set Interface taprouter type=internal -- set Interface taprouter external-ids:iface-status=active -- set Interface taprouter external-ids:attached-mac=fa:16:3e:84:6e:cc 这个网络创建完了，但是是在namespace外面的，如何进去呢？可以通过这个命令：
ip link set taprouter netns routerns 要给这个网卡配置一个IP地址，当然应该是虚拟机网络的网关地址。例如虚拟机私网网段为192.</description></item><item><title>第2季回归_这一次，我们一起拿下设计模式！</title><link>https://artisanbox.github.io/2/74/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/74/</guid><description>你好，我是王争。“数据结构与算法之美”在今年2月底全部更新完毕。时隔8个月，我又为你带来了一个新的专栏“设计模式之美”。如果说“数据结构与算法之美”是教你如何写出高效的代码，那“设计模式之美”就是教你如何写出高质量的代码。
在设计“设计模式之美”专栏的时候，我仍然延续“数据结构与算法之美”的讲述方式。在专栏的整体设计上，我希望尽量还原一对一、手把手code review的场景，通过100篇正文和10篇不定期加餐，200多个真实的项目实战代码案例剖析，100多个有深度的课堂讨论、头脑风暴，来为你交付这个“设计模式之美”专栏。
我希望通过这个专栏，一次性把跟编写高质量代码相关的所有知识，都系统、全面地讲清楚，一次性给你讲透彻。让你看完这个专栏，就能搞清楚所有跟写高质量代码相关的知识点。
专栏共100期正文和10期不定期加餐，分为5个模块。下面是专栏的目录：
为了感谢老同学，我为你准备了一个专属福利：
11月4日，专栏上新时，我会送你一张30元专属优惠券，可与限时优惠同享，有效期48小时，建议尽早使用。点击下方图片，立即免费试读新专栏。
一段新的征程，期待与你一起见证成长！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>第2季回归_这次我们来“趣谈Linux操作系统”</title><link>https://artisanbox.github.io/5/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/22/</guid><description>你好，我是你的老朋友刘超。在“趣谈网络协议”结课半年之后，我又给你带来了一个新的基础课程，“趣谈Linux操作系统”。
在咱们“趣谈网络协议”的留言里，我和同学们进行了很多互动，同时，我也和其他做基础知识专栏的作者有了不少交流，我发现，无论是从个人的职业发展角度，还是从公司招聘候选人的角度来看，扎实的基础知识是很多人的诉求。这让我更加坚信，我应该在“趣谈基础知识”这条道路上走下去。
在设计“趣谈Linux操作系统”专栏的时候，我仍然秉承“趣谈”和“故事化”的方式，将枯燥的基础知识结合某个场景，给你生动、具象地讲述出来，帮你加深理解、巩固记忆、夯实基础。
在我看来，操作系统在计算机中承担着“大管家”的角色，这个“大管家”就好比一家公司的老板，我们的目标就是把这家公司做上市，具体的过程，我用了一张图来表示：
Linux操作系统中的概念非常多，数据结构也很多，流程也复杂，一般人在学习的过程中很容易迷路。我希望能够将这些复杂的概念、数据结构、流程通俗地讲解出来，争取每篇文章都用一张图串起这篇的知识点。
最终，整个专栏下来，你如果能把这些图都掌握了，你的知识就会形成体系和连接。在此基础上再进行深入学习，就会如鱼得水、易如反掌。
一段新的征途即将开始，期待与你继续同行。为了感谢老同学，我为你送上一张10元专属优惠券，可以与限时优惠同享，优惠券有效期仅5天，建议你抓紧使用。点击下方图片，可试读专栏最新文章。
我们新专栏见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>第2讲_网络分层的真实含义是什么？</title><link>https://artisanbox.github.io/5/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/23/</guid><description>长时间从事计算机网络相关的工作，我发现，计算机网络有一个显著的特点，就是这是一个不仅需要背诵，而且特别需要将原理烂熟于胸的学科。很多问题看起来懂了，但是就怕往细里问，一问就发现你懂得没有那么透彻。
我们上一节列了之后要讲的网络协议。这些协议本来没什么稀奇，每一本教科书都会讲，并且都要求你背下来。因为考试会考，面试会问。可以这么说，毕业了去找工作还答不出这类题目的，那你的笔试基本上也就挂了。
当你听到什么二层设备、三层设备、四层LB和七层LB中层的时候，是否有点一头雾水，不知道这些所谓的层，对应的各种协议具体要做什么“工作”？
这四个问题你真的懂了吗？ 因为教科书或者老师往往会打一个十分不恰当的比喻：为什么网络要分层呀？因为不同的层次之间有不同的沟通方式，这个叫作协议。例如，一家公司也是分“层次”的，分总经理、经理、组长、员工。总经理之间有他们的沟通方式，经理和经理之间也有沟通方式，同理组长和员工。有没有听过类似的比喻？
那么第一个问题来了。请问经理在握手的时候，员工在干什么？很多人听过TCP建立连接的三次握手协议，也会把它当知识点背诵。同理问你，TCP在进行三次握手的时候，IP层和MAC层对应都有什么操作呢？
除了上面这个不恰当的比喻，教科书还会列出每个层次所包含的协议，然后开始逐层地去讲这些协议。但是这些协议之间的关系呢？却很少有教科书会讲。
学习第三层的时候会提到，IP协议里面包含目标地址和源地址。第三层里往往还会学习路由协议。路由就像中转站，我们从原始地址A到目标地址D，中间经过两个中转站A-&amp;gt;B-&amp;gt;C-&amp;gt;D，是通过路由转发的。
那么第二个问题来了。A知道自己的下一个中转站是B，那从A发出来的包，应该把B的IP地址放在哪里呢？B知道自己的下一个中转站是C，从B发出来的包，应该把C的IP地址放在哪里呢？如果放在IP协议中的目标地址，那包到了中转站，怎么知道最终的目的地址是D呢？
教科书不会通过场景化的例子，将网络包的生命周期讲出来，所以你就会很困惑，不知道这些协议实际的应用场景是什么。
我再问你一个问题。你一定经常听说二层设备、三层设备。二层设备处理的通常是MAC层的东西。那我发送一个HTTP的包，是在第七层工作的，那是不是不需要经过二层设备？或者即便经过了，二层设备也不处理呢？或者换一种问法，二层设备处理的包里，有没有HTTP层的内容呢？
最终，我想问你一个综合的问题。从你的电脑，通过SSH登录到公有云主机里面，都需要经历哪些过程？或者说你打开一个电商网站，都需要经历哪些过程？说得越详细越好。
实际情况可能是，很多人回答不上来。尽管对每一层都很熟悉，但是知识点却串不起来。
上面的这些问题，有的在这一节就会有一个解释，有的则会贯穿我们整个课程。好在后面一节中我会举一个贯穿的例子，将很多层的细节讲过后，你很容易就能把这些知识点串起来。
网络为什么要分层？ 这里我们先探讨第一个问题，网络为什么要分层？因为，是个复杂的程序都要分层。
理解计算机网络中的概念，一个很好的角度是，想象网络包就是一段Buffer，或者一块内存，是有格式的。同时，想象自己是一个处理网络包的程序，而且这个程序可以跑在电脑上，可以跑在服务器上，可以跑在交换机上，也可以跑在路由器上。你想象自己有很多的网口，从某个口拿进一个网络包来，用自己的程序处理一下，再从另一个网口发送出去。
当然网络包的格式很复杂，这个程序也很复杂。复杂的程序都要分层，这是程序设计的要求。比如，复杂的电商还会分数据库层、缓存层、Compose层、Controller层和接入层，每一层专注做本层的事情。
程序是如何工作的？ 我们可以简单地想象“你”这个程序的工作过程。
当一个网络包从一个网口经过的时候，你看到了，首先先看看要不要请进来，处理一把。有的网口配置了混杂模式，凡是经过的，全部拿进来。
拿进来以后，就要交给一段程序来处理。于是，你调用process_layer2(buffer)。当然，这是一个假的函数。但是你明白其中的意思，知道肯定是有这么个函数的。那这个函数是干什么的呢？从Buffer中，摘掉二层的头，看一看，应该根据头里面的内容做什么操作。
假设你发现这个包的MAC地址和你的相符，那说明就是发给你的，于是需要调用process_layer3(buffer)。这个时候，Buffer里面往往就没有二层的头了，因为已经在上一个函数的处理过程中拿掉了，或者将开始的偏移量移动了一下。在这个函数里面，摘掉三层的头，看看到底是发送给自己的，还是希望自己转发出去的。
如何判断呢？如果IP地址不是自己的，那就应该转发出去；如果IP地址是自己的，那就是发给自己的。根据IP头里面的标示，拿掉三层的头，进行下一层的处理，到底是调用process_tcp(buffer)呢，还是调用process_udp(buffer)呢？
假设这个地址是TCP的，则会调用process_tcp(buffer)。这时候，Buffer里面没有三层的头，就需要查看四层的头，看这是一个发起，还是一个应答，又或者是一个正常的数据包，然后分别由不同的逻辑进行处理。如果是发起或者应答，接下来可能要发送一个回复包；如果是一个正常的数据包，就需要交给上层了。交给谁呢？是不是有process_http(buffer)函数呢？
没有的，如果你是一个网络包处理程序，你不需要有process_http(buffer)，而是应该交给应用去处理。交给哪个应用呢？在四层的头里面有端口号，不同的应用监听不同的端口号。如果发现浏览器应用在监听这个端口，那你发给浏览器就行了。至于浏览器怎么处理，和你没有关系。
浏览器自然是解析HTML，显示出页面来。电脑的主人看到页面很开心，就点了鼠标。点击鼠标的动作被浏览器捕获。浏览器知道，又要发起另一个HTTP请求了，于是使用端口号，将请求发给了你。
你应该调用send_tcp(buffer)。不用说，Buffer里面就是HTTP请求的内容。这个函数里面加一个TCP的头，记录下源端口号。浏览器会给你目的端口号，一般为80端口。
然后调用send_layer3(buffer)。Buffer里面已经有了HTTP的头和内容，以及TCP的头。在这个函数里面加一个IP的头，记录下源IP的地址和目标IP的地址。
然后调用send_layer2(buffer)。Buffer里面已经有了HTTP的头和内容、TCP的头，以及IP的头。这个函数里面要加一下MAC的头，记录下源MAC地址，得到的就是本机器的MAC地址和目标的MAC地址。不过，这个还要看当前知道不知道，知道就直接加上；不知道的话，就要通过一定的协议处理过程，找到MAC地址。反正要填一个，不能空着。
万事俱备，只要Buffer里面的内容完整，就可以从网口发出去了，你作为一个程序的任务就算告一段落了。
揭秘层与层之间的关系 知道了这个过程之后，我们再来看一下原来困惑的问题。
首先是分层的比喻。所有不能表示出层层封装含义的比喻，都是不恰当的。总经理握手，不需要员工在吧，总经理之间谈什么，不需要员工参与吧，但是网络世界不是这样的。正确的应该是，总经理之间沟通的时候，经理将总经理放在自己兜里，然后组长把经理放自己兜里，员工把组长放自己兜里，像套娃娃一样。那员工直接沟通，不带上总经理，就不恰当了。
现实生活中，往往是员工说一句，组长补充两句，然后经理补充两句，最后总经理再补充两句。但是在网络世界，应该是总经理说话，经理补充两句，组长补充两句，员工再补充两句。
那TCP在三次握手的时候，IP层和MAC层在做什么呢？当然是TCP发送每一个消息，都会带着IP层和MAC层了。因为，TCP每发送一个消息，IP层和MAC层的所有机制都要运行一遍。而你只看到TCP三次握手了，其实，IP层和MAC层为此也忙活好久了。
这里要记住一点：只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。
所以，对TCP协议来说，三次握手也好，重试也好，只要想发出去包，就要有IP层和MAC层，不然是发不出去的。
经常有人会问这样一个问题，我都知道那台机器的IP地址了，直接发给他消息呗，要MAC地址干啥？这里的关键就是，没有MAC地址消息是发不出去的。
所以如果一个HTTP协议的包跑在网络上，它一定是完整的。无论这个包经过哪些设备，它都是完整的。
所谓的二层设备、三层设备，都是这些设备上跑的程序不同而已。一个HTTP协议的包经过一个二层设备，二层设备收进去的是整个网络包。这里面HTTP、TCP、 IP、 MAC都有。什么叫二层设备呀，就是只把MAC头摘下来，看看到底是丢弃、转发，还是自己留着。那什么叫三层设备呢？就是把MAC头摘下来之后，再把IP头摘下来，看看到底是丢弃、转发，还是自己留着。
小结 总结一下今天的内容，理解网络协议的工作模式，有两个小窍门：
始终想象自己是一个处理网络包的程序：如何拿到网络包，如何根据规则进行处理，如何发出去； 始终牢记一个原则：只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。 最后，给你留两个思考题吧。
如果你也觉得总经理和员工的比喻不恰当，你有更恰当的比喻吗？ 要想学习网络协议，IP这个概念是最最基本的，那你知道如何查看IP地址吗？ 欢迎你留言和我讨论。趣谈网络协议，我们下期见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>第30讲_容器网络之Flannel：每人一亩三分地</title><link>https://artisanbox.github.io/5/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/24/</guid><description>上一节我们讲了容器网络的模型，以及如何通过NAT的方式与物理网络进行互通。
每一台物理机上面安装好了Docker以后，都会默认分配一个172.17.0.0/16的网段。一台机器上新创建的第一个容器，一般都会给172.17.0.2这个地址，当然一台机器这样玩玩倒也没啥问题。但是容器里面是要部署应用的，就像上一节讲过的一样，它既然是集装箱，里面就需要装载货物。
如果这个应用是比较传统的单体应用，自己就一个进程，所有的代码逻辑都在这个进程里面，上面的模式没有任何问题，只要通过NAT就能访问进来。
但是因为无法解决快速迭代和高并发的问题，单体应用越来越跟不上时代发展的需要了。
你可以回想一下，无论是各种网络直播平台，还是共享单车，是不是都是很短时间内就要积累大量用户，否则就会错过风口。所以应用需要在很短的时间内快速迭代，不断调整，满足用户体验；还要在很短的时间内，具有支撑高并发请求的能力。
单体应用作为个人英雄主义的时代已经过去了。如果所有的代码都在一个工程里面，开发的时候必然存在大量冲突，上线的时候，需要开大会进行协调，一个月上线一次就很不错了。而且所有的流量都让一个进程扛，怎么也扛不住啊！
没办法，一个字：拆！拆开了，每个子模块独自变化，减少相互影响。拆开了，原来一个进程扛流量，现在多个进程一起扛。所以，微服务就是从个人英雄主义，变成集团军作战。
容器作为集装箱，可以保证应用在不同的环境中快速迁移，提高迭代的效率。但是如果要形成容器集团军，还需要一个集团军作战的调度平台，这就是Kubernetes。它可以灵活地将一个容器调度到任何一台机器上，并且当某个应用扛不住的时候，只要在Kubernetes上修改容器的副本数，一个应用马上就能变八个，而且都能提供服务。
然而集团军作战有个重要的问题，就是通信。这里面包含两个问题，第一个是集团军的A部队如何实时地知道B部队的位置变化，第二个是两个部队之间如何相互通信。
第一个问题位置变化，往往是通过一个称为注册中心的地方统一管理的，这个是应用自己做的。当一个应用启动的时候，将自己所在环境的IP地址和端口，注册到注册中心指挥部，这样其他的应用请求它的时候，到指挥部问一下它在哪里就好了。当某个应用发生了变化，例如一台机器挂了，容器要迁移到另一台机器，这个时候IP改变了，应用会重新注册，则其他的应用请求它的时候，还是能够从指挥部得到最新的位置。
接下来是如何相互通信的问题。NAT这种模式，在多个主机的场景下，是存在很大问题的。在物理机A上的应用A看到的IP地址是容器A的，是172.17.0.2，在物理机B上的应用B看到的IP地址是容器B的，不巧也是172.17.0.2，当它们都注册到注册中心的时候，注册中心就是这个图里这样子。
这个时候，应用A要访问应用B，当应用A从注册中心将应用B的IP地址读出来的时候，就彻底困惑了，这不是自己访问自己吗？
怎么解决这个问题呢？一种办法是不去注册容器内的IP地址，而是注册所在物理机的IP地址，端口也要是物理机上映射的端口。
这样存在的问题是，应用是在容器里面的，它怎么知道物理机上的IP地址和端口呢？这明明是运维人员配置的，除非应用配合，读取容器平台的接口获得这个IP和端口。一方面，大部分分布式框架都是容器诞生之前就有了，它们不会适配这种场景；另一方面，让容器内的应用意识到容器外的环境，本来就是非常不好的设计。
说好的集装箱，说好的随意迁移呢？难道要让集装箱内的货物意识到自己传的信息？而且本来Tomcat都是监听8080端口的，结果到了物理机上，就不能大家都用这个端口了，否则端口就冲突了，因而就需要随机分配端口，于是在注册中心就出现了各种各样奇怪的端口。无论是注册中心，还是调用方都会觉得很奇怪，而且不是默认的端口，很多情况下也容易出错。
Kubernetes作为集团军作战管理平台，提出指导意见，说网络模型要变平，但是没说怎么实现。于是业界就涌现了大量的方案，Flannel就是其中之一。
对于IP冲突的问题，如果每一个物理机都是网段172.17.0.0/16，肯定会冲突啊，但是这个网段实在太大了，一台物理机上根本启动不了这么多的容器，所以能不能每台物理机在这个大网段里面，抠出一个小的网段，每个物理机网段都不同，自己看好自己的一亩三分地，谁也不和谁冲突。
例如物理机A是网段172.17.8.0/24，物理机B是网段172.17.9.0/24，这样两台机器上启动的容器IP肯定不一样，而且就看IP地址，我们就一下子识别出，这个容器是本机的，还是远程的，如果是远程的，也能从网段一下子就识别出它归哪台物理机管，太方便了。
接下来的问题，就是物理机A上的容器如何访问到物理机B上的容器呢？
你是不是想到了熟悉的场景？虚拟机也需要跨物理机互通，往往通过Overlay的方式，容器是不是也可以这样做呢？
这里我要说Flannel使用UDP实现Overlay网络的方案。
在物理机A上的容器A里面，能看到的容器的IP地址是172.17.8.2/24，里面设置了默认的路由规则default via 172.17.8.1 dev eth0。
如果容器A要访问172.17.9.2，就会发往这个默认的网关172.17.8.1。172.17.8.1就是物理机上面docker0网桥的IP地址，这台物理机上的所有容器都是连接到这个网桥的。
在物理机上面，查看路由策略，会有这样一条172.17.0.0/24 via 172.17.0.0 dev flannel.1，也就是说发往172.17.9.2的网络包会被转发到flannel.1这个网卡。
这个网卡是怎么出来的呢？在每台物理机上，都会跑一个flanneld进程，这个进程打开一个/dev/net/tun字符设备的时候，就出现了这个网卡。
你有没有想起qemu-kvm，打开这个字符设备的时候，物理机上也会出现一个网卡，所有发到这个网卡上的网络包会被qemu-kvm接收进来，变成二进制串。只不过接下来qemu-kvm会模拟一个虚拟机里面的网卡，将二进制的串变成网络包，发给虚拟机里面的网卡。但是flanneld不用这样做，所有发到flannel.1这个网卡的包都会被flanneld进程读进去，接下来flanneld要对网络包进行处理。
物理机A上的flanneld会将网络包封装在UDP包里面，然后外层加上物理机A和物理机B的IP地址，发送给物理机B上的flanneld。
为什么是UDP呢？因为不想在flanneld之间建立两两连接，而UDP没有连接的概念，任何一台机器都能发给另一台。
物理机B上的flanneld收到包之后，解开UDP的包，将里面的网络包拿出来，从物理机B的flannel.1网卡发出去。
在物理机B上，有路由规则172.17.9.0/24 dev docker0 proto kernel scope link src 172.17.9.1。
将包发给docker0，docker0将包转给容器B。通信成功。
上面的过程连通性没有问题，但是由于全部在用户态，所以性能差了一些。
跨物理机的连通性问题，在虚拟机那里有成熟的方案，就是VXLAN，那能不能Flannel也用VXLAN呢？
当然可以了。如果使用VXLAN，就不需要打开一个TUN设备了，而是要建立一个VXLAN的VTEP。如何建立呢？可以通过netlink通知内核建立一个VTEP的网卡flannel.1。在我们讲OpenvSwitch的时候提过，netlink是一种用户态和内核态通信的机制。
当网络包从物理机A上的容器A发送给物理机B上的容器B，在容器A里面通过默认路由到达物理机A上的docker0网卡，然后根据路由规则，在物理机A上，将包转发给flannel.1。这个时候flannel.1就是一个VXLAN的VTEP了，它将网络包进行封装。
内部的MAC地址这样写：源为物理机A的flannel.1的MAC地址，目标为物理机B的flannel.1的MAC地址，在外面加上VXLAN的头。
外层的IP地址这样写：源为物理机A的IP地址，目标为物理机B的IP地址，外面加上物理机的MAC地址。
这样就能通过VXLAN将包转发到另一台机器，从物理机B的flannel.1上解包，变成内部的网络包，通过物理机B上的路由转发到docker0，然后转发到容器B里面。通信成功。
小结好了，今天的内容就到这里，我来总结一下。
基于NAT的容器网络模型在微服务架构下有两个问题，一个是IP重叠，一个是端口冲突，需要通过Overlay网络的机制保持跨节点的连通性。
Flannel是跨节点容器网络方案之一，它提供的Overlay方案主要有两种方式，一种是UDP在用户态封装，一种是VXLAN在内核态封装，而VXLAN的性能更好一些。
最后，给你留两个问题：
通过Flannel的网络模型可以实现容器与容器直接跨主机的互相访问，那你知道如果容器内部访问外部的服务应该怎么融合到这个网络模型中吗？
基于Overlay的网络毕竟做了一次网络虚拟化，有没有更加高性能的方案呢？</description></item><item><title>第31讲_容器网络之Calico：为高效说出善意的谎言</title><link>https://artisanbox.github.io/5/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/25/</guid><description>上一节我们讲了Flannel如何解决容器跨主机互通的问题，这个解决方式其实和虚拟机的网络互通模式是差不多的，都是通过隧道。但是Flannel有一个非常好的模式，就是给不同的物理机设置不同网段，这一点和虚拟机的Overlay的模式完全不一样。
在虚拟机的场景下，整个网段在所有的物理机之间都是可以“飘来飘去”的。网段不同，就给了我们做路由策略的可能。
Calico网络模型的设计思路我们看图中的两台物理机。它们的物理网卡是同一个二层网络里面的。由于两台物理机的容器网段不同，我们完全可以将两台物理机配置成为路由器，并按照容器的网段配置路由表。
例如，在物理机A中，我们可以这样配置：要想访问网段172.17.9.0/24，下一跳是192.168.100.101，也即到物理机B上去。
这样在容器A中访问容器B，当包到达物理机A的时候，就能够匹配到这条路由规则，并将包发给下一跳的路由器，也即发给物理机B。在物理机B上也有路由规则，要访问172.17.9.0/24，从docker0的网卡进去即可。
当容器B返回结果的时候，在物理机B上，可以做类似的配置：要想访问网段172.17.8.0/24，下一跳是192.168.100.100，也即到物理机A上去。
当包到达物理机B的时候，能够匹配到这条路由规则，将包发给下一跳的路由器，也即发给物理机A。在物理机A上也有路由规则，要访问172.17.8.0/24，从docker0的网卡进去即可。
这就是Calico网络的大概思路，即不走Overlay网络，不引入另外的网络性能损耗，而是将转发全部用三层网络的路由转发来实现，只不过具体的实现和上面的过程稍有区别。
首先，如果全部走三层的路由规则，没必要每台机器都用一个docker0，从而浪费了一个IP地址，而是可以直接用路由转发到veth pair在物理机这一端的网卡。同样，在容器内，路由规则也可以这样设定：把容器外面的veth pair网卡算作默认网关，下一跳就是外面的物理机。
于是，整个拓扑结构就变成了这个图中的样子。
Calico网络的转发细节我们来看其中的一些细节。
容器A1的IP地址为172.17.8.2/32，这里注意，不是/24，而是/32，将容器A1作为一个单点的局域网了。
容器A1里面的默认路由，Calico配置得比较有技巧。
default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link 这个IP地址169.254.1.1是默认的网关，但是整个拓扑图中没有一张网卡是这个地址。那如何到达这个地址呢？
前面我们讲网关的原理的时候说过，当一台机器要访问网关的时候，首先会通过ARP获得网关的MAC地址，然后将目标MAC变为网关的MAC，而网关的IP地址不会在任何网络包头里面出现，也就是说，没有人在乎这个地址具体是什么，只要能找到对应的MAC，响应ARP就可以了。
ARP本地有缓存，通过ip neigh命令可以查看。
169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee STALE 这个MAC地址是Calico硬塞进去的，但是没有关系，它能响应ARP，于是发出的包的目标MAC就是这个MAC地址。
在物理机A上查看所有网卡的MAC地址的时候，我们会发现veth1就是这个MAC地址。所以容器A1里发出的网络包，第一跳就是这个veth1这个网卡，也就到达了物理机A这个路由器。
在物理机A上有三条路由规则，分别是去两个本机的容器的路由，以及去172.17.9.0/24，下一跳为物理机B。
172.17.8.2 dev veth1 scope link 172.17.8.3 dev veth2 scope link 172.17.9.0/24 via 192.168.100.101 dev eth0 proto bird onlink 同理，物理机B上也有三条路由规则，分别是去两个本机的容器的路由，以及去172.17.8.0/24，下一跳为物理机A。
172.17.9.2 dev veth1 scope link 172.17.9.3 dev veth2 scope link 172.17.8.0/24 via 192.</description></item><item><title>第32讲_RPC协议综述：远在天边，近在眼前</title><link>https://artisanbox.github.io/5/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/26/</guid><description>前面我们讲了容器网络如何实现跨主机互通，以及微服务之间的相互调用。
网络是打通了，那服务之间的互相调用，该怎么实现呢？你可能说，咱不是学过Socket吗。服务之间分调用方和被调用方，我们就建立一个TCP或者UDP的连接，不就可以通信了？
你仔细想一下，这事儿没这么简单。我们就拿最简单的场景，客户端调用一个加法函数，将两个整数加起来，返回它们的和。
如果放在本地调用，那是简单的不能再简单了，只要稍微学过一种编程语言，三下五除二就搞定了。但是一旦变成了远程调用，门槛一下子就上去了。
首先你要会Socket编程，至少先要把咱们这门网络协议课学一下，然后再看N本砖头厚的Socket程序设计的书，学会咱们学过的几种Socket程序设计的模型。这就使得本来大学毕业就能干的一项工作，变成了一件五年工作经验都不一定干好的工作，而且，搞定了Socket程序设计，才是万里长征的第一步。后面还有很多问题呢！
如何解决这五个问题？问题一：如何规定远程调用的语法？客户端如何告诉服务端，我是一个加法，而另一个是乘法。我是用字符串“add”传给你，还是传给你一个整数，比如1表示加法，2表示乘法？服务端该如何告诉客户端，我的这个加法，目前只能加整数，不能加小数，不能加字符串；而另一个加法“add1”，它能实现小数和整数的混合加法。那返回值是什么？正确的时候返回什么，错误的时候又返回什么？
问题二：如果传递参数？我是先传两个整数，后传一个操作符“add”，还是先传操作符，再传两个整数？是不是像咱们数据结构里一样，如果都是UDP，想要实现一个逆波兰表达式，放在一个报文里面还好，如果是TCP，是一个流，在这个流里面，如何将两次调用进行分界？什么时候是头，什么时候是尾？把这次的参数和上次的参数混了起来，TCP一端发送出去的数据，另外一端不一定能一下子全部读取出来。所以，怎么才算读完呢？
问题三：如何表示数据？在这个简单的例子中，传递的就是一个固定长度的int值，这种情况还好，如果是变长的类型，是一个结构体，甚至是一个类，应该怎么办呢？如果是int，不同的平台上长度也不同，该怎么办呢？
在网络上传输超过一个Byte的类型，还有大端Big Endian和小端Little Endian的问题。
假设我们要在32位四个Byte的一个空间存放整数1，很显然只要一个Byte放1，其他三个Byte放0就可以了。那问题是，最后一个Byte放1呢，还是第一个Byte放1呢？或者说1作为最低位，应该是放在32位的最后一个位置呢，还是放在第一个位置呢？
最低位放在最后一个位置，叫作Little Endian，最低位放在第一个位置，叫作Big Endian。TCP/IP协议栈是按照Big Endian来设计的，而X86机器多按照Little Endian来设计的，因而发出去的时候需要做一个转换。
问题四：如何知道一个服务端都实现了哪些远程调用？从哪个端口可以访问这个远程调用？假设服务端实现了多个远程调用，每个可能实现在不同的进程中，监听的端口也不一样，而且由于服务端都是自己实现的，不可能使用一个大家都公认的端口，而且有可能多个进程部署在一台机器上，大家需要抢占端口，为了防止冲突，往往使用随机端口，那客户端如何找到这些监听的端口呢？
问题五：发生了错误、重传、丢包、性能等问题怎么办？本地调用没有这个问题，但是一旦到网络上，这些问题都需要处理，因为网络是不可靠的，虽然在同一个连接中，我们还可通过TCP协议保证丢包、重传的问题，但是如果服务器崩溃了又重启，当前连接断开了，TCP就保证不了了，需要应用自己进行重新调用，重新传输会不会同样的操作做两遍，远程调用性能会不会受影响呢？
协议约定问题看到这么多问题，你是不是想起了我第一节讲过的这张图。
本地调用函数里有很多问题，比如词法分析、语法分析、语义分析等等，这些编译器本来都能帮你做了。但是在远程调用中，这些问题你都需要重新操心。
很多公司的解决方法是，弄一个核心通信组，里面都是Socket编程的大牛，实现一个统一的库，让其他业务组的人来调用，业务的人不需要知道中间传输的细节。通信双方的语法、语义、格式、端口、错误处理等，都需要调用方和被调用方开会协商，双方达成一致。一旦有一方改变，要及时通知对方，否则通信就会有问题。
可是不是每一个公司都有这种大牛团队，往往只有大公司才配得起，那有没有已经实现好的框架可以使用呢？
当然有。一个大牛Bruce Jay Nelson写了一篇论文Implementing Remote Procedure Calls，定义了RPC的调用标准。后面所有RPC框架，都是按照这个标准模式来的。
当客户端的应用想发起一个远程调用时，它实际是通过本地调用本地调用方的Stub。它负责将调用的接口、方法和参数，通过约定的协议规范进行编码，并通过本地的RPCRuntime进行传输，将调用网络包发送到服务器。
服务器端的RPCRuntime收到请求后，交给提供方Stub进行解码，然后调用服务端的方法，服务端执行方法，返回结果，提供方Stub将返回结果编码后，发送给客户端，客户端的RPCRuntime收到结果，发给调用方Stub解码得到结果，返回给客户端。
这里面分了三个层次，对于用户层和服务端，都像是本地调用一样，专注于业务逻辑的处理就可以了。对于Stub层，处理双方约定好的语法、语义、封装、解封装。对于RPCRuntime，主要处理高性能的传输，以及网络的错误和异常。
最早的RPC的一种实现方式称为Sun RPC或ONC RPC。Sun公司是第一个提供商业化RPC库和 RPC编译器的公司。这个RPC框架是在NFS协议中使用的。
NFS（Network File System）就是网络文件系统。要使NFS成功运行，要启动两个服务端，一个是mountd，用来挂载文件路径；一个是nfsd，用来读写文件。NFS可以在本地mount一个远程的目录到本地的一个目录，从而本地的用户在这个目录里面写入、读出任何文件的时候，其实操作的是远程另一台机器上的文件。
操作远程和远程调用的思路是一样的，就像操作本地一样。所以NFS协议就是基于RPC实现的。当然无论是什么RPC，底层都是Socket编程。
XDR（External Data Representation，外部数据表示法）是一个标准的数据压缩格式，可以表示基本的数据类型，也可以表示结构体。
这里是几种基本的数据类型。
在RPC的调用过程中，所有的数据类型都要封装成类似的格式。而且RPC的调用和结果返回，也有严格的格式。
XID唯一标识一对请求和回复。请求为0，回复为1。
RPC有版本号，两端要匹配RPC协议的版本号。如果不匹配，就会返回Deny，原因就是RPC_MISMATCH。
程序有编号。如果服务端找不到这个程序，就会返回PROG_UNAVAIL。
程序有版本号。如果程序的版本号不匹配，就会返回PROG_MISMATCH。
一个程序可以有多个方法，方法也有编号，如果找不到方法，就会返回PROC_UNAVAIL。
调用需要认证鉴权，如果不通过，则Deny。
最后是参数列表，如果参数无法解析，则返回GABAGE_ARGS。
为了可以成功调用RPC，在客户端和服务端实现RPC的时候，首先要定义一个双方都认可的程序、版本、方法、参数等。
如果还是上面的加法，则双方约定为一个协议定义文件，同理如果是NFS、mount和读写，也会有类似的定义。</description></item><item><title>第33讲_基于XML的SOAP协议：不要说NBA，请说美国职业篮球联赛</title><link>https://artisanbox.github.io/5/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/27/</guid><description>上一节我们讲了RPC的经典模型和设计要点，并用最早期的ONC RPC为例子，详述了具体的实现。
ONC RPC存在哪些问题？ ONC RPC将客户端要发送的参数，以及服务端要发送的回复，都压缩为一个二进制串，这样固然能够解决双方的协议约定问题，但是存在一定的不方便。
首先，需要双方的压缩格式完全一致，一点都不能差。一旦有少许的差错，多一位，少一位或者错一位，都可能造成无法解压缩。当然，我们可以用传输层的可靠性以及加入校验值等方式，来减少传输过程中的差错。
其次，协议修改不灵活。如果不是传输过程中造成的差错，而是客户端因为业务逻辑的改变，添加或者删除了字段，或者服务端添加或者删除了字段，而双方没有及时通知，或者线上系统没有及时升级，就会造成解压缩不成功。
因而，当业务发生改变，需要多传输一些参数或者少传输一些参数的时候，都需要及时通知对方，并且根据约定好的协议文件重新生成双方的Stub程序。自然，这样灵活性比较差。
如果仅仅是沟通的问题也还好解决，其实更难弄的还有版本的问题。比如在服务端提供一个服务，参数的格式是版本一的，已经有50个客户端在线上调用了。现在有一个客户端有个需求，要加一个字段，怎么办呢？这可是一个大工程，所有的客户端都要适配这个，需要重新写程序，加上这个字段，但是传输值是0，不需要这个字段的客户端很“冤”，本来没我啥事儿，为啥让我也忙活？
最后，ONC RPC的设计明显是面向函数的，而非面向对象。而当前面向对象的业务逻辑设计与实现方式已经成为主流。
这一切的根源就在于压缩。这就像平时我们爱用缩略语。如果是篮球爱好者，你直接说NBA，他马上就知道什么意思，但是如果你给一个大妈说NBA，她可能就不知所云。
所以，这种RPC框架只能用于客户端和服务端全由一拨人开发的场景，或者至少客户端和服务端的开发人员要密切沟通，相互合作，有大量的共同语言，才能按照既定的协议顺畅地进行工作。
XML与SOAP 但是，一般情况下，我们做一个服务，都是要提供给陌生人用的，你和客户不会经常沟通，也没有什么共同语言。就像你给别人介绍NBA，你要说美国职业篮球赛，这样不管他是干啥的，都能听得懂。
放到我们的场景中，对应的就是用文本类的方式进行传输。无论哪个客户端获得这个文本，都能够知道它的意义。
一种常见的文本类格式是XML。我们这里举个例子来看。
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt; &amp;lt;geek:purchaseOrder xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; xmlns:geek=&amp;quot;http://www.example.com/geek&amp;quot;&amp;gt; &amp;lt;order&amp;gt; &amp;lt;date&amp;gt;2018-07-01&amp;lt;/date&amp;gt; &amp;lt;className&amp;gt;趣谈网络协议&amp;lt;/className&amp;gt; &amp;lt;Author&amp;gt;刘超&amp;lt;/Author&amp;gt; &amp;lt;price&amp;gt;68&amp;lt;/price&amp;gt; &amp;lt;/order&amp;gt; &amp;lt;/geek:purchaseOrder&amp;gt; 我这里不准备详细讲述XML的语法规则，但是你相信我，看完下面的内容，即便你没有学过XML，也能一看就懂，这段XML描述的是什么，不像全面的二进制，你看到的都是010101，不知所云。
有了这个，刚才我们说的那几个问题就都不是问题了。
首先，格式没必要完全一致。比如如果我们把price和author换个位置，并不影响客户端和服务端解析这个文本，也根本不会误会，说这个作者的名字叫68。
如果有的客户端想增加一个字段，例如添加一个推荐人字段，只需要在上面的文件中加一行：
&amp;lt;recommended&amp;gt; Gary &amp;lt;/recommended&amp;gt; 对于不需要这个字段的客户端，只要不解析这一行就是了。只要用简单的处理，就不会出现错误。
另外，这种表述方式显然是描述一个订单对象的，是一种面向对象的、更加接近用户场景的表示方式。
既然XML这么好，接下来我们来看看怎么把它用在RPC中。
传输协议问题 我们先解决第一个，传输协议的问题。
基于XML的最著名的通信协议就是SOAP了，全称简单对象访问协议（Simple Object Access Protocol）。它使用XML编写简单的请求和回复消息，并用HTTP协议进行传输。
SOAP将请求和回复放在一个信封里面，就像传递一个邮件一样。信封里面的信分抬头和正文。
POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/soap+xml; charset=utf-8 Content-Length: nnn &amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt; &amp;lt;soap:Envelope xmlns:soap=&amp;quot;http://www.w3.org/2001/12/soap-envelope&amp;quot; soap:encodingStyle=&amp;quot;http://www.w3.org/2001/12/soap-encoding&amp;quot;&amp;gt; &amp;lt;soap:Header&amp;gt; &amp;lt;m:Trans xmlns:m=&amp;quot;http://www.w3schools.com/transaction/&amp;quot; soap:mustUnderstand=&amp;quot;1&amp;quot;&amp;gt;1234 &amp;lt;/m:Trans&amp;gt; &amp;lt;/soap:Header&amp;gt; &amp;lt;soap:Body xmlns:m=&amp;quot;http://www.</description></item><item><title>第34讲_基于JSON的RESTful接口协议：我不关心过程，请给我结果</title><link>https://artisanbox.github.io/5/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/28/</guid><description>上一节我们讲了基于XML的SOAP协议，SOAP的S是啥意思来着？是Simple，但是好像一点儿都不简单啊！
你会发现，对于SOAP来讲，无论XML中调用的是什么函数，多是通过HTTP的POST方法发送的。但是咱们原来学HTTP的时候，我们知道HTTP除了POST，还有PUT、DELETE、GET等方法，这些也可以代表一个个动作，而且基本满足增、删、查、改的需求，比如增是POST，删是DELETE，查是GET，改是PUT。
传输协议问题 对于SOAP来讲，比如我创建一个订单，用POST，在XML里面写明动作是CreateOrder；删除一个订单，还是用POST，在XML里面写明了动作是DeleteOrder。其实创建订单完全可以使用POST动作，然后在XML里面放一个订单的信息就可以了，而删除用DELETE动作，然后在XML里面放一个订单的ID就可以了。
于是上面的那个SOAP就变成下面这个简单的模样。
POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/xml; charset=utf-8 Content-Length: nnn &amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt; &amp;lt;order&amp;gt; &amp;lt;date&amp;gt;2018-07-01&amp;lt;/date&amp;gt; &amp;lt;className&amp;gt;趣谈网络协议&amp;lt;/className&amp;gt; &amp;lt;Author&amp;gt;刘超&amp;lt;/Author&amp;gt; &amp;lt;price&amp;gt;68&amp;lt;/price&amp;gt; &amp;lt;/order&amp;gt;
而且XML的格式也可以改成另外一种简单的文本化的对象表示格式JSON。
POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/json; charset=utf-8 Content-Length: nnn { &amp;quot;order&amp;quot;: { &amp;quot;date&amp;quot;: &amp;quot;2018-07-01&amp;quot;, &amp;quot;className&amp;quot;: &amp;quot;趣谈网络协议&amp;quot;, &amp;quot;Author&amp;quot;: &amp;quot;刘超&amp;quot;, &amp;quot;price&amp;quot;: &amp;quot;68&amp;quot; } }
经常写Web应用的应该已经发现，这就是RESTful格式的API的样子。
协议约定问题 然而RESTful可不仅仅是指API，而是一种架构风格，全称Representational State Transfer，表述性状态转移，来自一篇重要的论文《架构风格与基于网络的软件架构设计》（Architectural Styles and the Design of Network-based Software Architectures）。
这篇文章从深层次，更加抽象地论证了一个互联网应用应该有的设计要点，而这些设计要点，成为后来我们能看到的所有高并发应用设计都必须要考虑的问题，再加上REST API比较简单直接，所以后来几乎成为互联网应用的标准接口。
因此，和SOAP不一样，REST不是一种严格规定的标准，它其实是一种设计风格。如果按这种风格进行设计，RESTful接口和SOAP接口都能做到，只不过后面的架构是REST倡导的，而SOAP相对比较关注前面的接口。
而且由于能够通过WSDL生成客户端的Stub，因而SOAP常常被用于类似传统的RPC方式，也即调用远端和调用本地是一样的。
然而本地调用和远程跨网络调用毕竟不一样，这里的不一样还不仅仅是因为有网络而导致的客户端和服务端的分离，从而带来的网络性能问题。更重要的问题是，客户端和服务端谁来维护状态。所谓的状态就是对某个数据当前处理到什么程度了。
这里举几个例子，例如，我浏览到哪个目录了，我看到第几页了，我要买个东西，需要扣减一下库存，这些都是状态。本地调用其实没有人纠结这个问题，因为数据都在本地，谁处理都一样，而且一边处理了，另一边马上就能看到。
当有了RPC之后，我们本来期望对上层透明，就像上一节说的“远在天边，尽在眼前”。于是使用RPC的时候，对于状态的问题也没有太多的考虑。</description></item><item><title>第35讲_二进制类RPC协议：还是叫NBA吧，总说全称多费劲</title><link>https://artisanbox.github.io/5/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/29/</guid><description>前面我们讲了两个常用文本类的RPC协议，对于陌生人之间的沟通，用NBA、CBA这样的缩略语，会使得协议约定非常不方便。
在讲CDN和DNS的时候，我们讲过接入层的设计，对于静态资源或者动态资源静态化的部分都可以做缓存。但是对于下单、支付等交易场景，还是需要调用API。
对于微服务的架构，API需要一个API网关统一的管理。API网关有多种实现方式，用Nginx或者OpenResty结合Lua脚本是常用的方式。在上一节讲过的Spring Cloud体系中，有个组件Zuul也是干这个的。
数据中心内部是如何相互调用的？API网关用来管理API，但是API的实现一般在一个叫作Controller层的地方。这一层对外提供API。由于是让陌生人访问的，我们能看到目前业界主流的，基本都是RESTful的API，是面向大规模互联网应用的。
在Controller之内，就是咱们互联网应用的业务逻辑实现。上节讲RESTful的时候，说过业务逻辑的实现最好是无状态的，从而可以横向扩展，但是资源的状态还需要服务端去维护。资源的状态不应该维护在业务逻辑层，而是在最底层的持久化层，一般会使用分布式数据库和ElasticSearch。
这些服务端的状态，例如订单、库存、商品等，都是重中之重，都需要持久化到硬盘上，数据不能丢，但是由于硬盘读写性能差，因而持久化层往往吞吐量不能达到互联网应用要求的吞吐量，因而前面要有一层缓存层，使用Redis或者memcached将请求拦截一道，不能让所有的请求都进入数据库“中军大营”。
缓存和持久化层之上一般是基础服务层，这里面提供一些原子化的接口。例如，对于用户、商品、订单、库存的增删查改，将缓存和数据库对再上层的业务逻辑屏蔽一道。有了这一层，上层业务逻辑看到的都是接口，而不会调用数据库和缓存。因而对于缓存层的扩容，数据库的分库分表，所有的改变，都截止到这一层，这样有利于将来对于缓存和数据库的运维。
再往上就是组合层。因为基础服务层只是提供简单的接口，实现简单的业务逻辑，而复杂的业务逻辑，比如下单，要扣优惠券，扣减库存等，就要在组合服务层实现。
这样，Controller层、组合服务层、基础服务层就会相互调用，这个调用是在数据中心内部的，量也会比较大，还是使用RPC的机制实现的。
由于服务比较多，需要一个单独的注册中心来做服务发现。服务提供方会将自己提供哪些服务注册到注册中心中去，同时服务消费方订阅这个服务，从而可以对这个服务进行调用。
调用的时候有一个问题，这里的RPC调用，应该用二进制还是文本类？其实文本的最大问题是，占用字节数目比较多。比如数字123，其实本来二进制8位就够了，但是如果变成文本，就成了字符串123。如果是UTF-8编码的话，就是三个字节；如果是UTF-16，就是六个字节。同样的信息，要多费好多的空间，传输起来也更加占带宽，时延也高。
因而对于数据中心内部的相互调用，很多公司选型的时候，还是希望采用更加省空间和带宽的二进制的方案。
这里一个著名的例子就是Dubbo服务化框架二进制的RPC方式。
Dubbo会在客户端的本地启动一个Proxy，其实就是客户端的Stub，对于远程的调用都通过这个Stub进行封装。
接下来，Dubbo会从注册中心获取服务端的列表，根据路由规则和负载均衡规则，在多个服务端中选择一个最合适的服务端进行调用。
调用服务端的时候，首先要进行编码和序列化，形成Dubbo头和序列化的方法和参数。将编码好的数据，交给网络客户端进行发送，网络服务端收到消息后，进行解码。然后将任务分发给某个线程进行处理，在线程中会调用服务端的代码逻辑，然后返回结果。
这个过程和经典的RPC模式何其相似啊！
如何解决协议约定问题？接下来我们还是来看RPC的三大问题，其中注册发现问题已经通过注册中心解决了。下面我们就来看协议约定问题。
Dubbo中默认的RPC协议是Hessian2。为了保证传输的效率，Hessian2将远程调用序列化为二进制进行传输，并且可以进行一定的压缩。这个时候你可能会疑惑，同为二进制的序列化协议，Hessian2和前面的二进制的RPC有什么区别呢？这不绕了一圈又回来了吗？
Hessian2是解决了一些问题的。例如，原来要定义一个协议文件，然后通过这个文件生成客户端和服务端的Stub，才能进行相互调用，这样使得修改就会不方便。Hessian2不需要定义这个协议文件，而是自描述的。什么是自描述呢？
所谓自描述就是，关于调用哪个函数，参数是什么，另一方不需要拿到某个协议文件、拿到二进制，靠它本身根据Hessian2的规则，就能解析出来。
原来有协议文件的场景，有点儿像两个人事先约定好，0表示方法add，然后后面会传两个数。服务端把两个数加起来，这样一方发送012，另一方知道是将1和2加起来，但是不知道协议文件的，当它收到012的时候，完全不知道代表什么意思。
而自描述的场景，就像两个人说的每句话都带前因后果。例如，传递的是“函数：add，第一个参数1，第二个参数2”。这样无论谁拿到这个表述，都知道是什么意思。但是只不过都是以二进制的形式编码的。这其实相当于综合了XML和二进制共同优势的一个协议。
Hessian2是如何做到这一点的呢？这就需要去看Hessian2的序列化的语法描述文件。
看起来很复杂，编译原理里面是有这样的语法规则的。
我们从Top看起，下一层是value，直到形成一棵树。这里面的有个思想，为了防止歧义，每一个类型的起始数字都设置成为独一无二的。这样，解析的时候，看到这个数字，就知道后面跟的是什么了。
这里还是以加法为例子，“add(2,3)”被序列化之后是什么样的呢？
H x02 x00 # Hessian 2.0 C # RPC call x03 add # method &amp;quot;add&amp;quot; x92 # two arguments x92 # 2 - argument 1 x93 # 3 - argument 2 H开头，表示使用的协议是Hession，H的二进制是0x48。
C开头，表示这是一个RPC调用。
0x03，表示方法名是三个字符。
0x92，表示有两个参数。其实这里存的应该是2，之所以加上0x90，就是为了防止歧义，表示这里一定是一个int。</description></item><item><title>第36讲_跨语言类RPC协议：交流之前，双方先来个专业术语表</title><link>https://artisanbox.github.io/5/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/30/</guid><description>到目前为止，咱们讲了四种RPC，分别是ONC RPC、基于XML的SOAP、基于JSON的RESTful和Hessian2。
通过学习，我们知道，二进制的传输性能好，文本类的传输性能差一些；二进制的难以跨语言，文本类的可以跨语言；要写协议文件的严谨一些，不写协议文件的灵活一些。虽然都有服务发现机制，有的可以进行服务治理，有的则没有。
我们也看到了RPC从最初的客户端服务器模式，最终演进到微服务。对于RPC框架的要求越来越多了，具体有哪些要求呢？
首先，传输性能很重要。因为服务之间的调用如此频繁了，还是二进制的越快越好。
其次，跨语言很重要。因为服务多了，什么语言写成的都有，而且不同的场景适宜用不同的语言，不能一个语言走到底。
最好既严谨又灵活，添加个字段不用重新编译和发布程序。
最好既有服务发现，也有服务治理，就像Dubbo和Spring Cloud一样。
Protocol Buffers这是要多快好省地建设社会主义啊。理想还是要有的嘛，这里我就来介绍一个向“理想”迈进的GRPC。
GRPC首先满足二进制和跨语言这两条，二进制说明压缩效率高，跨语言说明更灵活。但是又是二进制，又是跨语言，这就相当于两个人沟通，你不但说方言，还说缩略语，人家怎么听懂呢？所以，最好双方弄一个协议约定文件，里面规定好双方沟通的专业术语，这样沟通就顺畅多了。
对于GRPC来讲，二进制序列化协议是Protocol Buffers。首先，需要定义一个协议文件.proto。
我们还看买极客时间专栏的这个例子。
syntax = “proto3”; package com.geektime.grpc option java_package = “com.geektime.grpc”; message Order { required string date = 1; required string classname = 2; required string author = 3; required int price = 4; } message OrderResponse { required string message = 1; }
service PurchaseOrder { rpc Purchase (Order) returns (OrderResponse) {} } 在这个协议文件中，我们首先指定使用proto3的语法，然后我们使用Protocol Buffers的语法，定义两个消息的类型，一个是发出去的参数，一个是返回的结果。里面的每一个字段，例如date、classname、author、price都有唯一的一个数字标识，这样在压缩的时候，就不用传输字段名称了，只传输这个数字标识就行了，能节省很多空间。</description></item><item><title>第37讲_知识串讲：用双十一的故事串起碎片的网络协议（上）</title><link>https://artisanbox.github.io/5/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/31/</guid><description>基本的网络知识我们都讲完了，还记得最初举的那个“双十一”下单的例子吗？这一节开始，我们详细地讲解这个过程，用这个过程串起我们讲过的网络协议。
我把这个过程分为十个阶段，从云平台中搭建一个电商开始，到BGP路由广播，再到DNS域名解析，从客户看商品图片，到最终下单的整个过程，每一步我都会详细讲解。这节我们先来看前三个阶段。
1.部署一个高可用高并发的电商平台 首先，咱们要有个电商平台。假设我们已经有了一个特别大的电商平台，这个平台应该部署在哪里呢？假设我们用公有云，一般公有云会有多个位置，比如在华东、华北、华南都有。毕竟咱们的电商是要服务全国的，当然到处都要部署了。我们把主站点放在华东。
为了每个点都能“雨露均沾”，也为了高可用性，往往需要有多个机房，形成多个可用区（Available Zone）。由于咱们的应用是分布在两个可用区的，所以假如任何一个可用区挂了，都不会受影响。
我们来回想数据中心那一节，每个可用区里有一片一片的机柜，每个机柜上有一排一排的服务器，每个机柜都有一个接入交换机，有一个汇聚交换机将多个机柜连在一起。
这些服务器里面部署的都是计算节点，每台上面都有Open vSwitch创建的虚拟交换机，将来在这台机器上创建的虚拟机，都会连到Open vSwitch上。
接下来，你在云计算的界面上创建一个VPC（Virtual Private Cloud，虚拟私有网络），指定一个IP段，这样以后你部署的所有应用都会在这个虚拟网络里，使用你分配的这个IP段。为了不同的VPC相互隔离，每个VPC都会被分配一个VXLAN的ID。尽管不同用户的虚拟机有可能在同一个物理机上，但是不同的VPC二层压根儿是不通的。
由于有两个可用区，在这个VPC里面，要为每一个可用区分配一个Subnet，也就是在大的网段里分配两个小的网段。当两个可用区里面网段不同的时候，就可以配置路由策略，访问另外一个可用区，走某一条路由了。
接下来，应该创建数据库持久化层。大部分云平台都会提供PaaS服务，也就是说，不需要你自己搭建数据库，而是采用直接提供数据库的服务，并且单机房的主备切换都是默认做好的，数据库也是部署在虚拟机里面的，只不过从界面上，你看不到数据库所在的虚拟机而已。
云平台会给每个Subnet的数据库实例分配一个域名。创建数据库实例的时候，需要你指定可用区和Subnet，这样创建出来的数据库实例可以通过这个Subnet的私网IP进行访问。
为了分库分表实现高并发的读写，在创建的多个数据库实例之上，会创建一个分布式数据库的实例，也需要指定可用区和Subnet，还会为分布式数据库分配一个私网IP和域名。
对于数据库这种高可用性比较高的，需要进行跨机房高可用，因而两个可用区都要部署一套，但是只有一个是主，另外一个是备，云平台往往会提供数据库同步工具，将应用写入主的数据同步给备数据库集群。
接下来是创建缓存集群。云平台也会提供PaaS服务，也需要每个可用区和Subnet创建一套，缓存的数据在内存中，由于读写性能要求高，一般不要求跨可用区读写。
再往上层就是部署咱们自己写的程序了。基础服务层、组合服务层、Controller层，以及Nginx层、API网关等等，这些都是部署在虚拟机里面的。它们之间通过RPC相互调用，需要到注册中心进行注册。
它们之间的网络通信是虚拟机和虚拟机之间的。如果是同一台物理机，则那台物理机上的OVS就能转发过去；如果是不同的物理机，这台物理机的OVS和另一台物理机的OVS中间有一个VXLAN的隧道，将请求转发过去。
再往外就是负载均衡了，负载均衡也是云平台提供的PaaS服务，也是属于某个VPC的，部署在虚拟机里面的，但是负载均衡有个外网的IP，这个外网的IP地址就是在网关节点的外网网口上的。在网关节点上，会有NAT规则，将外网IP地址转换为VPC里面的私网IP地址，通过这些私网IP地址访问到虚拟机上的负载均衡节点，然后通过负载均衡节点转发到API网关的节点。
网关节点的外网网口是带公网IP地址的，里面有一个虚拟网关转发模块，还会有一个OVS，将私网IP地址放到VXLAN隧道里面，转发到虚拟机上，从而实现外网和虚拟机网络之间的互通。
不同的可用区之间，通过核心交换机连在一起，核心交换机之外是边界路由器。
在华北、华东、华南同样也部署了一整套，每个地区都创建了VPC，这就需要有一种机制将VPC连接到一起。云平台一般会提供硬件的VPC互连的方式，当然也可以使用软件互连的方式，也就是使用VPN网关，通过IPsec VPN将不同地区的不同VPC通过VPN连接起来。
对于不同地区和不同运营商的用户，我们希望他能够就近访问到网站，而且当一个点出了故障之后，我们希望能够在不同的地区之间切换，这就需要有智能DNS，这个也是云平台提供的。
对于一些静态资源，可以保持在对象存储里面，通过CDN下发到边缘节点，这样客户端就能尽快加载出来。
2.大声告诉全世界，可以到我这里买东西 当电商应用搭建完毕之后，接下来需要将如何访问到这个电商网站广播给全网。
刚才那张图画的是一个可用区的情况，对于多个可用区的情况，我们可以隐去计算节点的情况，将外网访问区域放大。
外网IP是放在虚拟网关的外网网口上的，这个IP如何让全世界知道呢？当然是通过BGP路由协议了。
每个可用区都有自己的汇聚交换机，如果机器数目比较多，可以直接用核心交换机，每个Region也有自己的核心交换区域。
在核心交换外面是安全设备，然后就是边界路由器。边界路由器会和多个运营商连接，从而每个运营商都能够访问到这个网站。边界路由器可以通过BGP协议，将自己数据中心里面的外网IP向外广播，也就是告诉全世界，如果要访问这些外网IP，都来我这里。
每个运营商也有很多的路由器、很多的点，于是就可以将如何到达这些IP地址的路由信息，广播到全国乃至全世界。
3.打开手机来上网，域名解析得地址 这个时候，不但你的这个网站的IP地址全世界都知道了，你打的广告可能大家也都看到了，于是有客户下载App来买东西了。
客户的手机开机以后，在附近寻找基站eNodeB，发送请求，申请上网。基站将请求发给MME，MME对手机进行认证和鉴权，还会请求HSS看有没有钱，看看是在哪里上网。
当MME通过了手机的认证之后，开始建立隧道，建设的数据通路分两段路，其实是两个隧道。一段是从eNodeB到SGW，第二段是从SGW到PGW，在PGW之外，就是互联网。
PGW会为手机分配一个IP地址，手机上网都是带着这个IP地址的。
当在手机上面打开一个App的时候，首先要做的事情就是解析这个网站的域名。
在手机运营商所在的互联网区域里，有一个本地的DNS，手机会向这个DNS请求解析DNS。当这个DNS本地有缓存，则直接返回；如果没有缓存，本地DNS才需要递归地从根DNS服务器，查到.com的顶级域名服务器，最终查到权威DNS服务器。
如果你使用云平台的时候，配置了智能DNS和全局负载均衡，在权威DNS服务中，一般是通过配置CNAME的方式，我们可以起一个别名，例如 vip.yourcomany.com ，然后告诉本地DNS服务器，让它请求GSLB解析这个域名，GSLB就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。
GSLB通过查看请求它的本地DNS服务器所在的运营商和地址，就知道用户所在的运营商和地址，然后将距离用户位置比较近的Region里面，三个负载均衡SLB的公网IP地址，返回给本地DNS服务器。本地DNS解析器将结果缓存后，返回给客户端。
对于手机App来说，可以绕过刚才的传统DNS解析机制，直接只要HTTPDNS服务，通过直接调用HTTPDNS服务器，得到这三个SLB的公网IP地址。
看，经过了如此复杂的过程，咱们的万里长征还没迈出第一步，刚刚得到IP地址，包还没发呢？话说手机App拿到了公网IP地址，接下来应该做什么呢？
欢迎你留言和我讨论。趣谈网络协议，我们下期见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>第38讲_知识串讲：用双十一的故事串起碎片的网络协议（中）</title><link>https://artisanbox.github.io/5/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/32/</guid><description>上一节我们讲到，手机App经过了一个复杂的过程，终于拿到了电商网站的SLB的IP地址，是不是该下单了？
别忙，俗话说的好，买东西要货比三家。大部分客户在购物之前要看很多商品图片，比来比去，最后好不容易才下决心，点了下单按钮。下单按钮一按，就要开始建立连接。建立连接这个过程也挺复杂的，最终还要经过层层封装，才构建出一个完整的网络包。今天我们就来看这个过程。
4.购物之前看图片，静态资源CDN客户想要在购物网站买一件东西的时候，一般是先去详情页看看图片，是不是想买的那一款。
我们部署电商应用的时候，一般会把静态资源保存在两个地方，一个是接入层nginx后面的varnish缓存里面，一般是静态页面；对于比较大的、不经常更新的静态图片，会保存在对象存储里面。这两个地方的静态资源都会配置CDN，将资源下发到边缘节点。
配置了CDN之后，权威DNS服务器上，会为静态资源设置一个CNAME别名，指向另外一个域名 cdn.com ，返回给本地DNS服务器。
当本地DNS服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的时候就不是原来的权威DNS服务器了，而是 cdn.com 的权威DNS服务器。这是CDN自己的权威DNS服务器。
在这个服务器上，还是会设置一个CNAME，指向另外一个域名，也即CDN网络的全局负载均衡器。
本地DNS服务器去请求CDN的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，将IP返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。
如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器，将内容拉到本地。
5.看上宝贝点下单，双方开始建连接当你浏览了很多图片，发现实在喜欢某个商品，于是决定下单购买。
电商网站会对下单的情况提供RESTful的下单接口，而对于下单这种需要保密的操作，需要通过HTTPS协议进行请求。
在所有这些操作之前，首先要做的事情是建立连接。
HTTPS协议是基于TCP协议的，因而要先建立TCP的连接。在这个例子中，TCP的连接是在手机上的App和负载均衡器SLB之间的。
尽管中间要经过很多的路由器和交换机，但是TCP的连接是端到端的。TCP这一层和更上层的HTTPS无法看到中间的包的过程。尽管建立连接的时候，所有的包都逃不过在这些路由器和交换机之间的转发，转发的细节我们放到那个下单请求的发送过程中详细解读，这里只看端到端的行为。
对于TCP连接来讲，需要通过三次握手建立连接，为了维护这个连接，双方都需要在TCP层维护一个连接的状态机。
一开始，客户端和服务端都处于CLOSED状态。服务端先是主动监听某个端口，处于LISTEN状态。然后客户端主动发起连接SYN，之后处于SYN-SENT状态。服务端收到发起的连接，返回SYN，并且ACK客户端的SYN，之后处于SYN-RCVD状态。
客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态。这是因为，它一发一收成功了。服务端收到ACK的ACK之后，也会处于ESTABLISHED状态，因为它的一发一收也成功了。
当TCP层的连接建立完毕之后，接下来轮到HTTPS层建立连接了，在HTTPS的交换过程中，TCP层始终处于ESTABLISHED。
对于HTTPS，客户端会发送Client Hello消息到服务器，用明文传输TLS版本信息、加密套件候选列表、压缩算法候选列表等信息。另外，还会有一个随机数，在协商对称密钥的时候使用。
然后，服务器会返回Server Hello消息，告诉客户端，服务器选择使用的协议版本、加密套件、压缩算法等。这也有一个随机数，用于后续的密钥协商。
然后，服务器会给你一个服务器端的证书，然后说：“Server Hello Done，我这里就这些信息了。”
客户端当然不相信这个证书，于是从自己信任的CA仓库中，拿CA的证书里面的公钥去解密电商网站的证书。如果能够成功，则说明电商网站是可信的。这个过程中，你可能会不断往上追溯CA、CA的CA、CA的CA的CA，反正直到一个授信的CA，就可以了。
证书验证完毕之后，觉得这个服务端是可信的，于是客户端计算产生随机数字Pre-master，发送Client Key Exchange，用证书中的公钥加密，再发送给服务器，服务器可以通过私钥解密出来。
接下来，无论是客户端还是服务器，都有了三个随机数，分别是：自己的、对端的，以及刚生成的Pre-Master随机数。通过这三个随机数，可以在客户端和服务器产生相同的对称密钥。
有了对称密钥，客户端就可以说：“Change Cipher Spec，咱们以后都采用协商的通信密钥和加密算法进行加密通信了。”
然后客户端发送一个Encrypted Handshake Message，将已经商定好的参数等，采用协商密钥进行加密，发送给服务器用于数据与握手验证。
同样，服务器也可以发送Change Cipher Spec，说：“没问题，咱们以后都采用协商的通信密钥和加密算法进行加密通信了”，并且也发送Encrypted Handshake Message的消息试试。
当双方握手结束之后，就可以通过对称密钥进行加密传输了。
真正的下单请求封装成网络包的发送过程，我们先放一放，我们来接着讲这个网络包的故事。
6.发送下单请求网络包，西行需要出网关当客户端和服务端之间建立了连接后，接下来就要发送下单请求的网络包了。
在用户层发送的是HTTP的网络包，因为服务端提供的是RESTful API，因而HTTP层发送的就是一个请求。
POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/json; charset=utf-8 Content-Length: nnn { &amp;quot;order&amp;quot;: { &amp;quot;date&amp;quot;: &amp;quot;2018-07-01&amp;quot;, &amp;quot;className&amp;quot;: &amp;quot;趣谈网络协议&amp;quot;, &amp;quot;Author&amp;quot;: &amp;quot;刘超&amp;quot;, &amp;quot;price&amp;quot;: &amp;quot;68&amp;quot; } } HTTP的报文大概分为三大部分。第一部分是请求行，第二部分是请求的首部，第三部分才是请求的正文实体。</description></item><item><title>第39讲_知识串讲：用双十一的故事串起碎片的网络协议（下）</title><link>https://artisanbox.github.io/5/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/33/</guid><description>上一节，我们封装了一个长长的网络包，“大炮”准备完毕，开始发送。
发送的时候可以说是重重关隘，从手机到移动网络、互联网，还要经过多个运营商才能到达数据中心，到了数据中心就进入第二个复杂的过程，从网关到VXLAN隧道，到负载均衡，到Controller层、组合服务层、基础服务层，最终才下单入库。今天，我们就来看这最后一段过程。
7.一座座城池一道道关，流控拥塞与重传网络包已经组合完毕，接下来我们来看，如何经过一道道城关，到达目标公网IP。
对于手机来讲，默认的网关在PGW上。在移动网络里面，从手机到SGW，到PGW是有一条隧道的。在这条隧道里面，会将上面的这个包作为隧道的乘客协议放在里面，外面SGW和PGW在核心网机房的IP地址。网络包直到PGW（PGW是隧道的另一端）才将里面的包解出来，转发到外部网络。
所以，从手机发送出来的时候，网络包的结构为：
源MAC：手机也即UE的MAC；
目标MAC：网关PGW上面的隧道端点的MAC；
源IP：UE的IP地址；
目标IP：SLB的公网IP地址。
进入隧道之后，要封装外层的网络地址，因而网络包的格式为：
外层源MAC：E-NodeB的MAC；
外层目标MAC：SGW的MAC；
外层源IP：E-NodeB的IP；
外层目标IP：SGW的IP；
内层源MAC：手机也即UE的MAC；
内层目标MAC：网关PGW上面的隧道端点的MAC；
内层源IP：UE的IP地址；
内层目标IP：SLB的公网IP地址。
当隧道在SGW的时候，切换了一个隧道，会从SGW到PGW的隧道，因而网络包的格式为：
外层源MAC：SGW的MAC；
外层目标MAC：PGW的MAC；
外层源IP：SGW的IP；
外层目标IP：PGW的IP；
内层源MAC：手机也即UE的MAC；
内层目标MAC：网关PGW上面的隧道端点的MAC；
内层源IP：UE的IP地址；
内层目标IP：SLB的公网IP地址。</description></item><item><title>第3讲_ifconfig：最熟悉又陌生的命令行</title><link>https://artisanbox.github.io/5/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/34/</guid><description>上一节结尾给你留的一个思考题是，你知道怎么查看IP地址吗？
当面试听到这个问题的时候，面试者常常会觉得走错了房间。我面试的是技术岗位啊，怎么问这么简单的问题？
的确，即便没有专业学过计算机的人，只要倒腾过电脑，重装过系统，大多也会知道这个问题的答案：在Windows上是ipconfig，在Linux上是ifconfig。
那你知道在Linux上还有什么其他命令可以查看IP地址吗？答案是ip addr。如果回答不上来这个问题，那你可能没怎么用过Linux。
那你知道ifconfig和ip addr的区别吗？这是一个有关net-tools和iproute2的“历史”故事，你刚来到第三节，暂时不用了解这么细，但这也是一个常考的知识点。
想象一下，你登录进入一个被裁剪过的非常小的Linux系统中，发现既没有ifconfig命令，也没有ip addr命令，你是不是感觉这个系统压根儿没法用？这个时候，你可以自行安装net-tools和iproute2这两个工具。当然，大多数时候这两个命令是系统自带的。
安装好后，我们来运行一下ip addr。不出意外，应该会输出下面的内容。
root@test:~# ip addr 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::f816:3eff:fec7:7975/64 scope link valid_lft forever preferred_lft forever 这个命令显示了这台机器上所有的网卡。大部分的网卡都会有一个IP地址，当然，这不是必须的。在后面的分享中，我们会遇到没有IP地址的情况。</description></item><item><title>第40讲_搭建一个网络实验环境：授人以鱼不如授人以渔</title><link>https://artisanbox.github.io/5/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/35/</guid><description>因为这门课是基础课程，而且配合音频的形式发布，所以我多以理论为主来进行讲解。在专栏更新的过程中，不断有同学让我推荐一些网络方面的书籍，还有同学说能不能配合一些实验来说明理论。
的确，网络是一门实验性很强的学科，就像我在开篇词里面说的一样：一看觉得懂，一问就打鼓，一用就糊涂。 在写专栏的过程中，我自己也深深体会到了。这个时候，我常常会拿一个现实的环境，上手操作一下，抓个包看看，这样心里就会有定论。
《TCP/IP详解》实验环境搭建 对于网络方面的书籍，我当然首推Rechard Stevens的《TCP/IP illustrated》（《TCP/IP详解》）。这本书把理论讲得深入浅出，还配有大量的上手实践和抓包，看到这些抓包，原来不理解的很多理论，一下子就能懂了。
这本书里有个拓扑图，书上的很多实验都是基于这个图的，但是这个拓扑图还是挺复杂的。我这里先不说，一会儿详细讲。
Rechard Stevens，因为工作中有这么一个环境，很方便做实验，最终才写出了这样一本书，而我们一般人学习网络，没有这个环境应该怎么办呢？
时代不同了，咱们现在有更加强大的工具了。例如，这里这么多的机器，我们可以用Docker来实现，多个网络可以用Open vSwitch来实现。你甚至不需要一台物理机，只要一台1核2G的虚拟机，就能将这个环境搭建起来。
搭建这个环境的时候，需要一些脚本。我把脚本都放在了Github里面，你可以自己取用。
1.创建一个Ubuntu虚拟机 在你的笔记本电脑上，用VirtualBox创建就行。1核2G，随便一台电脑都能搭建起来。
首先，我们先下载一个Ubuntu的镜像。我是从Ubuntu官方网站下载的。
然后，在VirtualBox里面安装Ubuntu。安装过程网上一大堆教程，你可以自己去看，我这里就不详细说了。
这里我需要说明的是网络的配置。
对于这个虚拟机，我们创建两个网卡，一个是Host-only，只有你的笔记本电脑上能够登录进去。这个网卡上的IP地址也只有在你的笔记本电脑上管用。这个网卡的配置比较稳定，用于在SSH上做操作。这样你的笔记本电脑就可以搬来搬去，在公司里安装一半，回家接着安装另一半都没问题。
这里有一个虚拟的网桥，这个网络可以在管理&amp;gt;主机网络管理里面进行配置。
在这里可以虚拟网桥的的IP地址，同时启用一个DHCP服务器，为新创建的虚拟机配置IP地址。
另一个网卡配置为NAT网络，用于访问互联网。配置了NAT网络之后，只要你的笔记本电脑能上网，虚拟机就能上网。由于咱们在Ubuntu里面要安装一些东西，因而需要联网。
你可能会问了，这个配置复杂吗？一点儿都不复杂。咱们讲虚拟机网络的时候，讲过这个。
安装完了Ubuntu之后，需要对Ubuntu里面的网卡进行配置。对于Ubuntu来讲，网卡的配置在/etc/network/interfaces这个文件里面。在我的环境里，NAT的网卡名称为enp0s3，Host-only的网卡的名称为enp0s8，都可以配置为自动配置。
auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet dhcp
auto enp0s8 iface enp0s8 inet dhcp
这样，重启之后，IP就配置好了。
2.安装Docker和Open vSwitch 接下来，在Ubuntu里面，以root用户，安装Docker和Open vSwitch。
你可以按照Docker的官方安装文档来做。我这里也贴一下我的安装过程。
apt-get remove docker docker-engine docker.io apt-get -y update apt-get -y install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg &amp;gt; gpg apt-key add gpg apt-key fingerprint 0EBFCD88 add-apt-repository &amp;quot;deb [arch=amd64] https://download.</description></item><item><title>第4讲_DHCP与PXE：IP是怎么来的，又是怎么没的？</title><link>https://artisanbox.github.io/5/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/36/</guid><description>上一节，我们讲了IP的一些基本概念。如果需要和其他机器通讯，我们就需要一个通讯地址，我们需要给网卡配置这么一个地址。
如何配置IP地址？那如何配置呢？如果有相关的知识和积累，你可以用命令行自己配置一个地址。可以使用ifconfig，也可以使用ip addr。设置好了以后，用这两个命令，将网卡up一下，就可以开始工作了。
使用net-tools：
$ sudo ifconfig eth1 10.0.0.1/24 $ sudo ifconfig eth1 up 使用iproute2：
$ sudo ip addr add 10.0.0.1/24 dev eth1 $ sudo ip link set up eth1 你可能会问了，自己配置这个自由度太大了吧，我是不是配置什么都可以？如果配置一个和谁都不搭边的地址呢？例如，旁边的机器都是192.168.1.x，我非得配置一个16.158.23.6，会出现什么现象呢？
不会出现任何现象，就是包发不出去呗。为什么发不出去呢？我来举例说明。
192.168.1.6就在你这台机器的旁边，甚至是在同一个交换机上，而你把机器的地址设为了16.158.23.6。在这台机器上，你企图去ping192.168.1.6，你觉得只要将包发出去，同一个交换机的另一台机器马上就能收到，对不对？
可是Linux系统不是这样的，它没你想的那么智能。你用肉眼看到那台机器就在旁边，它则需要根据自己的逻辑进行处理。
还记得我们在第二节说过的原则吗？只要是在网络上跑的包，都是完整的，可以有下层没上层，绝对不可能有上层没下层。
所以，你看着它有自己的源IP地址16.158.23.6，也有目标IP地址192.168.1.6，但是包发不出去，这是因为MAC层还没填。
自己的MAC地址自己知道，这个容易。但是目标MAC填什么呢？是不是填192.168.1.6这台机器的MAC地址呢？
当然不是。Linux首先会判断，要去的这个地址和我是一个网段的吗，或者和我的一个网卡是同一网段的吗？只有是一个网段的，它才会发送ARP请求，获取MAC地址。如果发现不是呢？
Linux默认的逻辑是，如果这是一个跨网段的调用，它便不会直接将包发送到网络上，而是企图将包发送到网关。
如果你配置了网关的话，Linux会获取网关的MAC地址，然后将包发出去。对于192.168.1.6这台机器来讲，虽然路过它家门的这个包，目标IP是它，但是无奈MAC地址不是它的，所以它的网卡是不会把包收进去的。
如果没有配置网关呢？那包压根就发不出去。
如果将网关配置为192.168.1.6呢？不可能，Linux不会让你配置成功的，因为网关要和当前的网络至少一个网卡是同一个网段的，怎么可能16.158.23.6的网关是192.168.1.6呢？
所以，当你需要手动配置一台机器的网络IP时，一定要好好问问你的网络管理员。如果在机房里面，要去网络管理员那里申请，让他给你分配一段正确的IP地址。当然，真正配置的时候，一定不是直接用命令配置的，而是放在一个配置文件里面。不同系统的配置文件格式不同，但是无非就是CIDR、子网掩码、广播地址和网关地址。
动态主机配置协议（DHCP）原来配置IP有这么多门道儿啊。你可能会问了，配置了IP之后一般不能变的，配置一个服务端的机器还可以，但是如果是客户端的机器呢？我抱着一台笔记本电脑在公司里走来走去，或者白天来晚上走，每次使用都要配置IP地址，那可怎么办？还有人事、行政等非技术人员，如果公司所有的电脑都需要IT人员配置，肯定忙不过来啊。
因此，我们需要有一个自动配置的协议，也就是动态主机配置协议（Dynamic Host Configuration Protocol），简称DHCP。
有了这个协议，网络管理员就轻松多了。他只需要配置一段共享的IP地址。每一台新接入的机器都通过DHCP协议，来这个共享的IP地址里申请，然后自动配置好就可以了。等人走了，或者用完了，还回去，这样其他的机器也能用。
所以说，如果是数据中心里面的服务器，IP一旦配置好，基本不会变，这就相当于买房自己装修。DHCP的方式就相当于租房。你不用装修，都是帮你配置好的。你暂时用一下，用完退租就可以了。
解析DHCP的工作方式当一台机器新加入一个网络的时候，肯定一脸懵，啥情况都不知道，只知道自己的MAC地址。怎么办？先吼一句，我来啦，有人吗？这时候的沟通基本靠“吼”。这一步，我们称为DHCP Discover。
新来的机器使用IP地址0.0.0.0发送了一个广播包，目的IP地址为255.255.255.255。广播包封装了UDP，UDP封装了BOOTP。其实DHCP是BOOTP的增强版，但是如果你去抓包的话，很可能看到的名称还是BOOTP协议。
在这个广播包里面，新人大声喊：我是新来的（Boot request），我的MAC地址是这个，我还没有IP，谁能给租给我个IP地址！
格式就像这样：
如果一个网络管理员在网络里面配置了DHCP Server的话，他就相当于这些IP的管理员。他立刻能知道来了一个“新人”。这个时候，我们可以体会MAC地址唯一的重要性了。当一台机器带着自己的MAC地址加入一个网络的时候，MAC是它唯一的身份，如果连这个都重复了，就没办法配置了。
只有MAC唯一，IP管理员才能知道这是一个新人，需要租给它一个IP地址，这个过程我们称为DHCP Offer。同时，DHCP Server为此客户保留为它提供的IP地址，从而不会为其他DHCP客户分配此IP地址。
DHCP Offer的格式就像这样，里面有给新人分配的地址。
DHCP Server仍然使用广播地址作为目的地址，因为，此时请求分配IP的新人还没有自己的IP。DHCP Server回复说，我分配了一个可用的IP给你，你看如何？除此之外，服务器还发送了子网掩码、网关和IP地址租用期等信息。
新来的机器很开心，它的“吼”得到了回复，并且有人愿意租给它一个IP地址了，这意味着它可以在网络上立足了。当然更令人开心的是，如果有多个DHCP Server，这台新机器会收到多个IP地址，简直受宠若惊。
它会选择其中一个DHCP Offer，一般是最先到达的那个，并且会向网络发送一个DHCP Request广播数据包，包中包含客户端的MAC地址、接受的租约中的IP地址、提供此租约的DHCP服务器地址等，并告诉所有DHCP Server它将接受哪一台服务器提供的IP地址，告诉其他DHCP服务器，谢谢你们的接纳，并请求撤销它们提供的IP地址，以便提供给下一个IP租用请求者。</description></item><item><title>第5讲_从物理层到MAC层：如何在宿舍里自己组网玩联机游戏？</title><link>https://artisanbox.github.io/5/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/37/</guid><description>上一节，我们见证了IP地址的诞生，或者说是整个操作系统的诞生。一旦机器有了IP，就可以在网络的环境里和其他的机器展开沟通了。
故事就从我的大学宿舍开始讲起吧。作为一个八零后，我要暴露年龄了。
我们宿舍四个人，大一的时候学校不让上网，不给开通网络。但是，宿舍有一个人比较有钱，率先买了一台电脑。那买了电脑干什么呢？
首先，有单机游戏可以打，比如说《拳皇》。两个人用一个键盘，照样打得火热。后来有第二个人买了电脑，那两台电脑能不能连接起来呢？你会说，当然能啊，买个路由器不就行了。
现在一台家用路由器非常便宜，一百多块的事情。那时候路由器绝对是奢侈品。一直到大四，我们宿舍都没有买路由器。可能是因为那时候技术没有现在这么发达，导致我对网络技术的认知是逐渐深入的，而且每一层都是实实在在接触到的。
第一层（物理层）使用路由器，是在第三层上。我们先从第一层物理层开始说。
物理层能折腾啥？现在的同学可能想不到，我们当时去学校配电脑的地方买网线，卖网线的师傅都会问，你的网线是要电脑连电脑啊，还是电脑连网口啊？
我们要的是电脑连电脑。这种方式就是一根网线，有两个头。一头插在一台电脑的网卡上，另一头插在另一台电脑的网卡上。但是在当时，普通的网线这样是通不了的，所以水晶头要做交叉线，用的就是所谓的1－3、2－6交叉接法。
水晶头的第1、2和第3、6脚，它们分别起着收、发信号的作用。将一端的1号和3号线、2号和6号线互换一下位置，就能够在物理层实现一端发送的信号，另一端能收到。
当然电脑连电脑，除了网线要交叉，还需要配置这两台电脑的IP地址、子网掩码和默认网关。这三个概念上一节详细描述过了。要想两台电脑能够通信，这三项必须配置成为一个网络，可以一个是192.168.0.1/24，另一个是192.168.0.2/24，否则是不通的。
这里我想问你一个问题，两台电脑之间的网络包，包含MAC层吗？当然包含，要完整。IP层要封装了MAC层才能将包放入物理层。
到此为止，两台电脑已经构成了一个最小的局域网，也即LAN。可以玩联机局域网游戏啦！
等到第三个哥们也买了一台电脑，怎么把三台电脑连在一起呢？
先别说交换机，当时交换机也贵。有一个叫做Hub的东西，也就是集线器。这种设备有多个口，可以将宿舍里的多台电脑连接起来。但是，和交换机不同，集线器没有大脑，它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。
第二层（数据链路层）你可能已经发现问题了。Hub采取的是广播的模式，如果每一台电脑发出的包，宿舍的每个电脑都能收到，那就麻烦了。这就需要解决几个问题：
这个包是发给谁的？谁应该接收？ 大家都在发，会不会产生混乱？有没有谁先发、谁后发的规则？ 如果发送的时候出现了错误，怎么办？ 这几个问题，都是第二层，数据链路层，也即MAC层要解决的问题。MAC的全称是Medium Access Control，即媒体访问控制。控制什么呢？其实就是控制在往媒体上发数据的时候，谁先发、谁后发的问题。防止发生混乱。这解决的是第二个问题。这个问题中的规则，学名叫多路访问。有很多算法可以解决这个问题。就像车管所管束马路上跑的车，能想的办法都想过了。
比如接下来这三种方式：
方式一：分多个车道。每个车一个车道，你走你的，我走我的。这在计算机网络里叫作信道划分；
方式二：今天单号出行，明天双号出行，轮着来。这在计算机网络里叫作轮流协议；
方式三：不管三七二十一，有事儿先出门，发现特堵，就回去。错过高峰再出。我们叫作随机接入协议。著名的以太网，用的就是这个方式。
解决了第二个问题，就是解决了媒体接入控制的问题，MAC的问题也就解决好了。这和MAC地址没什么关系。
接下来要解决第一个问题：发给谁，谁接收？这里用到一个物理地址，叫作链路层地址。但是因为第二层主要解决媒体接入控制的问题，所以它常被称为MAC地址。
解决第一个问题就牵扯到第二层的网络包格式。对于以太网，第二层的最开始，就是目标的MAC地址和源的MAC地址。
接下来是类型，大部分的类型是IP数据包，然后IP里面包含TCP、UDP，以及HTTP等，这都是里层封装的事情。
有了这个目标MAC地址，数据包在链路上广播，MAC的网卡才能发现，这个包是给它的。MAC的网卡把包收进来，然后打开IP包，发现IP地址也是自己的，再打开TCP包，发现端口是自己，也就是80，而nginx就是监听80。
于是将请求提交给nginx，nginx返回一个网页。然后将网页需要发回请求的机器。然后层层封装，最后到MAC层。因为来的时候有源MAC地址，返回的时候，源MAC就变成了目标MAC，再返给请求的机器。
对于以太网，第二层的最后面是CRC，也就是循环冗余检测。通过XOR异或的算法，来计算整个包是否在发送的过程中出现了错误，主要解决第三个问题。
这里还有一个没有解决的问题，当源机器知道目标机器的时候，可以将目标地址放入包里面，如果不知道呢？一个广播的网络里面接入了N台机器，我怎么知道每个MAC地址是谁呢？这就是ARP协议，也就是已知IP地址，求MAC地址的协议。
在一个局域网里面，当知道了IP地址，不知道MAC怎么办呢？靠“吼”。
广而告之，发送一个广播包，谁是这个IP谁来回答。具体询问和回答的报文就像下面这样：
为了避免每次都用ARP请求，机器本地也会进行ARP缓存。当然机器会不断地上线下线，IP也可能会变，所以ARP的MAC地址缓存过一段时间就会过期。
局域网好了，至此我们宿舍四个电脑就组成了一个局域网。用Hub连接起来，就可以玩局域网版的《魔兽争霸》了。
打开游戏，进入“局域网选项”，选择一张地图，点击“创建游戏”，就可以进入这张地图的房间中。等同一个局域网里的其他小伙伴加入后，游戏就可以开始了。
这种组网的方法，对一个宿舍来说没有问题，但是一旦机器数目增多，问题就出现了。因为Hub是广播的，不管某个接口是否需要，所有的Bit都会被发送出去，然后让主机来判断是不是需要。这种方式路上的车少就没问题，车一多，产生冲突的概率就提高了。而且把不需要的包转发过去，纯属浪费。看来Hub这种不管三七二十一都转发的设备是不行了，需要点儿智能的。因为每个口都只连接一台电脑，这台电脑又不怎么换IP和MAC地址，只要记住这台电脑的MAC地址，如果目标MAC地址不是这台电脑的，这个口就不用转发了。
谁能知道目标MAC地址是否就是连接某个口的电脑的MAC地址呢？这就需要一个能把MAC头拿下来，检查一下目标MAC地址，然后根据策略转发的设备，按第二节课中讲过的，这个设备显然是个二层设备，我们称为交换机。
交换机怎么知道每个口的电脑的MAC地址呢？这需要交换机会学习。
一台MAC1电脑将一个包发送给另一台MAC2电脑，当这个包到达交换机的时候，一开始交换机也不知道MAC2的电脑在哪个口，所以没办法，它只能将包转发给除了来的那个口之外的其他所有的口。但是，这个时候，交换机会干一件非常聪明的事情，就是交换机会记住，MAC1是来自一个明确的口。以后有包的目的地址是MAC1的，直接发送到这个口就可以了。
当交换机作为一个关卡一样，过了一段时间之后，就有了整个网络的一个结构了，这个时候，基本上不用广播了，全部可以准确转发。当然，每个机器的IP地址会变，所在的口也会变，因而交换机上的学习的结果，我们称为转发表，是有一个过期时间的。
有了交换机，一般来说，你接个几十台、上百台机器打游戏，应该没啥问题。你可以组个战队了。能上网了，就可以玩网游了。
小结好了，今天的内容差不多了，我们来总结一下，有三个重点需要你记住：
第一，MAC层是用来解决多路访问的堵车问题的；
第二，ARP是通过吼的方式来寻找目标MAC地址的，吼完之后记住一段时间，这个叫作缓存；
第三，交换机是有MAC地址学习能力的，学完了它就知道谁在哪儿了，不用广播了。
最后，给你留两个思考题吧。
在二层中我们讲了ARP协议，即已知IP地址求MAC；还有一种RARP协议，即已知MAC求IP的，你知道它可以用来干什么吗？ 如果一个局域网里面有多个交换机，ARP广播的模式会出现什么问题呢？ 欢迎你留言和我讨论。趣谈网络协议，我们下期见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>第6讲_交换机与VLAN：办公室太复杂，我要回学校</title><link>https://artisanbox.github.io/5/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/38/</guid><description>上一次，我们在宿舍里组建了一个本地的局域网LAN，可以愉快地玩游戏了。这是一个非常简单的场景，因为只有一台交换机，电脑数目很少。今天，让我们切换到一个稍微复杂一点的场景，办公室。
拓扑结构是怎么形成的？我们常见到的办公室大多是一排排的桌子，每个桌子都有网口，一排十几个座位就有十几个网口，一个楼层就会有几十个甚至上百个网口。如果算上所有楼层，这个场景自然比你宿舍里的复杂多了。具体哪里复杂呢？我来给你具体讲解。
首先，这个时候，一个交换机肯定不够用，需要多台交换机，交换机之间连接起来，就形成一个稍微复杂的拓扑结构。
我们先来看两台交换机的情形。两台交换机连接着三个局域网，每个局域网上都有多台机器。如果机器1只知道机器4的IP地址，当它想要访问机器4，把包发出去的时候，它必须要知道机器4的MAC地址。
于是机器1发起广播，机器2收到这个广播，但是这不是找它的，所以没它什么事。交换机A一开始是不知道任何拓扑信息的，在它收到这个广播后，采取的策略是，除了广播包来的方向外，它还要转发给其他所有的网口。于是机器3也收到广播信息了，但是这和它也没什么关系。
当然，交换机B也是能够收到广播信息的，但是这时候它也是不知道任何拓扑信息的，因而也是进行广播的策略，将包转发到局域网三。这个时候，机器4和机器5都收到了广播信息。机器4主动响应说，这是找我的，这是我的MAC地址。于是一个ARP请求就成功完成了。
在上面的过程中，交换机A和交换机B都是能够学习到这样的信息：机器1是在左边这个网口的。当了解到这些拓扑信息之后，情况就好转起来。当机器2要访问机器1的时候，机器2并不知道机器1的MAC地址，所以机器2会发起一个ARP请求。这个广播消息会到达机器1，也同时会到达交换机A。这个时候交换机A已经知道机器1是不可能在右边的网口的，所以这个广播信息就不会广播到局域网二和局域网三。
当机器3要访问机器1的时候，也需要发起一个广播的ARP请求。这个时候交换机A和交换机B都能够收到这个广播请求。交换机A当然知道主机A是在左边这个网口的，所以会把广播消息转发到局域网一。同时，交换机B收到这个广播消息之后，由于它知道机器1是不在右边这个网口的，所以不会将消息广播到局域网三。
如何解决常见的环路问题？这样看起来，两台交换机工作得非常好。随着办公室越来越大，交换机数目肯定越来越多。当整个拓扑结构复杂了，这么多网线，绕过来绕过去，不可避免地会出现一些意料不到的情况。其中常见的问题就是环路问题。
例如这个图，当两个交换机将两个局域网同时连接起来的时候。你可能会觉得，这样反而有了高可用性。但是却不幸地出现了环路。出现了环路会有什么结果呢？
我们来想象一下机器1访问机器2的过程。一开始，机器1并不知道机器2的MAC地址，所以它需要发起一个ARP的广播。广播到达机器2，机器2会把MAC地址返回来，看起来没有这两个交换机什么事情。
但是问题来了，这两个交换机还是都能够收到广播包的。交换机A一开始是不知道机器2在哪个局域网的，所以它会把广播消息放到局域网二，在局域网二广播的时候，交换机B右边这个网口也是能够收到广播消息的。交换机B会将这个广播信息发送到局域网一。局域网一的这个广播消息，又会到达交换机A左边的这个接口。交换机A这个时候还是不知道机器2在哪个局域网，于是将广播包又转发到局域网二。左转左转左转，好像是个圈哦。
可能有人会说，当两台交换机都能够逐渐学习到拓扑结构之后，是不是就可以了？
别想了，压根儿学不会的。机器1的广播包到达交换机A和交换机B的时候，本来两个交换机都学会了机器1是在局域网一的，但是当交换机A将包广播到局域网二之后，交换机B右边的网口收到了来自交换机A的广播包。根据学习机制，这彻底损坏了交换机B的三观，刚才机器1还在左边的网口呢，怎么又出现在右边的网口呢？哦，那肯定是机器1换位置了，于是就误会了，交换机B就学会了，机器1是从右边这个网口来的，把刚才学习的那一条清理掉。同理，交换机A右边的网口，也能收到交换机B转发过来的广播包，同样也误会了，于是也学会了，机器1从右边的网口来，不是从左边的网口来。
然而当广播包从左边的局域网一广播的时候，两个交换机再次刷新三观，原来机器1是在左边的，过一会儿，又发现不对，是在右边的，过一会，又发现不对，是在左边的。
这还是一个包转来转去，每台机器都会发广播包，交换机转发也会复制广播包，当广播包越来越多的时候，按照上一节讲过一个共享道路的算法，也就是路会越来越堵，最后谁也别想走。所以，必须有一个方法解决环路的问题，怎么破除环路呢？
STP协议中那些难以理解的概念在数据结构中，有一个方法叫做最小生成树。有环的我们常称为图。将图中的环破了，就生成了树。在计算机网络中，生成树的算法叫作STP，全称Spanning Tree Protocol。
STP协议比较复杂，一开始很难看懂，但是其实这是一场血雨腥风的武林比武或者华山论剑，最终决出五岳盟主的方式。
在STP协议里面有很多概念，译名就非常拗口，但是我一作比喻，你很容易就明白了。
Root Bridge，也就是根交换机。这个比较容易理解，可以比喻为“掌门”交换机，是某棵树的老大，是掌门，最大的大哥。
Designated Bridges，有的翻译为指定交换机。这个比较难理解，可以想像成一个“小弟”，对于树来说，就是一棵树的树枝。所谓“指定”的意思是，我拜谁做大哥，其他交换机通过这个交换机到达根交换机，也就相当于拜他做了大哥。这里注意是树枝，不是叶子，因为叶子往往是主机。
Bridge Protocol Data Units （BPDU） ，网桥协议数据单元。可以比喻为“相互比较实力”的协议。行走江湖，比的就是武功，拼的就是实力。当两个交换机碰见的时候，也就是相连的时候，就需要互相比一比内力了。BPDU只有掌门能发，已经隶属于某个掌门的交换机只能传达掌门的指示。
Priority Vector，优先级向量。可以比喻为实力 （值越小越牛）。实力是啥？就是一组ID数目，[Root Bridge ID, Root Path Cost, Bridge ID, and Port ID]。为什么这样设计呢？这是因为要看怎么来比实力。先看Root Bridge ID。拿出老大的ID看看，发现掌门一样，那就是师兄弟；再比Root Path Cost，也即我距离我的老大的距离，也就是拿和掌门关系比，看同一个门派内谁和老大关系铁；最后比Bridge ID，比我自己的ID，拿自己的本事比。
STP的工作过程是怎样的？接下来，我们来看STP的工作过程。
一开始，江湖纷争，异常混乱。大家都觉得自己是掌门，谁也不服谁。于是，所有的交换机都认为自己是掌门，每个网桥都被分配了一个ID。这个ID里有管理员分配的优先级，当然网络管理员知道哪些交换机贵，哪些交换机好，就会给它们分配高的优先级。这种交换机生下来武功就很高，起步就是乔峰。
既然都是掌门，互相都连着网线，就互相发送BPDU来比功夫呗。这一比就发现，有人是岳不群，有人是封不平，赢的接着当掌门，输的就只好做小弟了。当掌门的还会继续发BPDU，而输的人就没有机会了。它们只有在收到掌门发的BPDU的时候，转发一下，表示服从命令。
数字表示优先级。就像这个图，5和6碰见了，6的优先级低，所以乖乖做小弟。于是一个小门派形成，5是掌门，6是小弟。其他诸如1-7、2-8、3-4这样的小门派，也诞生了。于是江湖出现了很多小的门派，小的门派，接着合并。
合并的过程会出现以下四种情形，我分别来介绍。
情形一：掌门遇到掌门当5碰到了1，掌门碰见掌门，1觉得自己是掌门，5也刚刚跟别人PK完成为掌门。这俩掌门比较功夫，最终1胜出。于是输掉的掌门5就会率领所有的小弟归顺。结果就是1成为大掌门。
情形二：同门相遇同门相遇可以是掌门与自己的小弟相遇，这说明存在“环”了。这个小弟已经通过其他门路拜在你门下，结果你还不认识，就PK了一把。结果掌门发现这个小弟功夫不错，不应该级别这么低，就把它招到门下亲自带，那这个小弟就相当于升职了。
我们再来看，假如1和6相遇。6原来就拜在1的门下，只不过6的上司是5，5的上司是1。1发现，6距离我才只有2，比从5这里过来的5（=4+1）近多了，那6就直接汇报给我吧。于是，5和6分别汇报给1。
同门相遇还可以是小弟相遇。这个时候就要比较谁和掌门的关系近，当然近的当大哥。刚才5和6同时汇报给1了，后来5和6在比较功夫的时候发现，5你直接汇报给1距离是4，如果5汇报给6再汇报给1，距离只有2+1=3，所以5干脆拜6为上司。
情形三：掌门与其他帮派小弟相遇小弟拿本帮掌门和这个掌门比较，赢了，这个掌门拜入门来。输了，会拜入新掌门，并且逐渐拉拢和自己连接的兄弟，一起弃暗投明。
例如，2和7相遇，虽然7是小弟，2是掌门。就个人武功而言，2比7强，但是7的掌门是1，比2牛，所以没办法，2要拜入7的门派，并且连同自己的小弟都一起拜入。</description></item><item><title>第7讲_ICMP与ping：投石问路的侦察兵</title><link>https://artisanbox.github.io/5/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/39/</guid><description>无论是在宿舍，还是在办公室，或者运维一个数据中心，我们常常会遇到网络不通的问题。那台机器明明就在那里，你甚至都可以通过机器的终端连上去看。它看着好好的，可是就是连不上去，究竟是哪里出了问题呢？
ICMP协议的格式一般情况下，你会想到ping一下。那你知道ping是如何工作的吗？
ping是基于ICMP协议工作的。ICMP全称Internet Control Message Protocol，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢？
网络包在异常复杂的网络环境中传输时，常常会遇到各种各样的问题。当遇到问题的时候，总不能“死个不明不白”，要传出消息来，报告情况，这样才可以调整传输策略。这就相当于我们经常看到的电视剧里，古代行军的时候，为将为帅者需要通过侦察兵、哨探或传令兵等人肉的方式来掌握情况，控制整个战局。
ICMP报文是封装在IP包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。因为作为侦查兵，要轻装上阵，不能携带大量的包袱。
ICMP报文有很多的类型，不同的类型有不同的代码。最常用的类型是主动请求为8，主动请求的应答为0。
查询报文类型我们经常在电视剧里听到这样的话：主帅说，来人哪！前方战事如何，快去派人打探，一有情况，立即通报！
这种是主帅发起的，主动查看敌情，对应ICMP的查询报文类型。例如，常用的ping就是查询报文，是一种主动请求，并且获得主动应答的ICMP协议。所以，ping发的包也是符合ICMP协议格式的，只不过它在后面增加了自己的格式。
对ping的主动请求，进行网络抓包，称为ICMP ECHO REQUEST。同理主动请求的回复，称为ICMP ECHO REPLY。比起原生的ICMP，这里面多了两个字段，一个是标识符。这个很好理解，你派出去两队侦查兵，一队是侦查战况的，一队是去查找水源的，要有个标识才能区分。另一个是序号，你派出去的侦查兵，都要编个号。如果派出去10个，回来10个，就说明前方战况不错；如果派出去10个，回来2个，说明情况可能不妙。
在选项数据中，ping还会存放发送请求的时间值，来计算往返时间，说明路程的长短。
差错报文类型当然也有另外一种方式，就是差错报文。
主帅骑马走着走着，突然来了一匹快马，上面的小兵气喘吁吁的：报告主公，不好啦！张将军遭遇埋伏，全军覆没啦！这种是异常情况发起的，来报告发生了不好的事情，对应ICMP的差错报文类型。
我举几个ICMP差错报文的例子：终点不可达为3，源抑制为4，超时为11，重定向为5。这些都是什么意思呢？我给你具体解释一下。
第一种是终点不可达。小兵：报告主公，您让把粮草送到张将军那里，结果没有送到。
如果你是主公，你肯定会问，为啥送不到？具体的原因在代码中表示就是，网络不可达代码为0，主机不可达代码为1，协议不可达代码为2，端口不可达代码为3，需要进行分片但设置了不分片位代码为4。
具体的场景就像这样：
网络不可达：主公，找不到地方呀？ 主机不可达：主公，找到地方没这个人呀？ 协议不可达：主公，找到地方，找到人，口号没对上，人家天王盖地虎，我说12345！ 端口不可达：主公，找到地方，找到人，对了口号，事儿没对上，我去送粮草，人家说他们在等救兵。 需要进行分片但设置了不分片位：主公，走到一半，山路狭窄，想换小车，但是您的将令，严禁换小车，就没办法送到了。 第二种是源站抑制，也就是让源站放慢发送速度。小兵：报告主公，您粮草送的太多了吃不完。
第三种是时间超时，也就是超过网络包的生存时间还是没到。小兵：报告主公，送粮草的人，自己把粮草吃完了，还没找到地方，已经饿死啦。
第四种是路由重定向，也就是让下次发给另一个路由器。小兵：报告主公，上次送粮草的人本来只要走一站地铁，非得从五环绕，下次别这样了啊。
差错报文的结构相对复杂一些。除了前面还是IP，ICMP的前8字节不变，后面则跟上出错的那个IP包的IP头和IP正文的前8个字节。
而且这类侦查兵特别恪尽职守，不但自己返回来报信，还把一部分遗物也带回来。
侦察兵：报告主公，张将军已经战死沙场，这是张将军的印信和佩剑。 主公：神马？张将军是怎么死的（可以查看ICMP的前8字节）？没错，这是张将军的剑，是他的剑（IP数据包的头及正文前8字节）。 ping：查询报文类型的使用接下来，我们重点来看ping的发送和接收过程。
假定主机A的IP地址是192.168.1.1，主机B的IP地址是192.168.1.2，它们都在同一个子网。那当你在主机A上运行“ping 192.168.1.2”后，会发生什么呢?
ping命令执行的时候，源主机首先会构建一个ICMP请求数据包，ICMP数据包内包含多个字段。最重要的是两个，第一个是类型字段，对于请求数据包而言该字段为 8；另外一个是顺序号，主要用于区分连续ping的时候发出的多个数据包。每发出一个请求数据包，顺序号会自动加1。为了能够计算往返时间RTT，它会在报文的数据部分插入发送时间。
然后，由ICMP协议将这个数据包连同地址192.168.1.2一起交给IP层。IP层将以192.168.1.2作为目的地址，本机IP地址作为源地址，加上一些其他控制信息，构建一个IP数据包。
接下来，需要加入MAC头。如果在本节ARP映射表中查找出IP地址192.168.1.2所对应的MAC地址，则可以直接使用；如果没有，则需要发送ARP协议查询MAC地址，获得MAC地址后，由数据链路层构建一个数据帧，目的地址是IP层传过来的MAC地址，源地址则是本机的MAC地址；还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。
主机B收到这个数据帧后，先检查它的目的MAC地址，并和本机的MAC地址对比，如符合，则接收，否则就丢弃。接收后检查该数据帧，将IP数据包从帧中提取出来，交给本机的IP层。同样，IP层检查后，将有用的信息提取后交给ICMP协议。
主机B会构建一个 ICMP 应答包，应答数据包的类型字段为 0，顺序号为接收到的请求数据包中的顺序号，然后再发送出去给主机A。
在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明目标主机不可达；如果接收到了 ICMP 应答包，则说明目标主机可达。此时，源主机会检查，用当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。
当然这只是最简单的，同一个局域网里面的情况。如果跨网段的话，还会涉及网关的转发、路由器的转发等等。但是对于ICMP的头来讲，是没什么影响的。会影响的是根据目标IP地址，选择路由的下一跳，还有每经过一个路由器到达一个新的局域网，需要换MAC头里面的MAC地址。这个过程后面几节会详细描述，这里暂时不多说。
如果在自己的可控范围之内，当遇到网络不通的问题的时候，除了直接ping目标的IP地址之外，还应该有一个清晰的网络拓扑图。并且从理论上来讲，应该要清楚地知道一个网络包从源地址到目标地址都需要经过哪些设备，然后逐个ping中间的这些设备或者机器。如果可能的话，在这些关键点，通过tcpdump -i eth0 icmp，查看包有没有到达某个点，回复的包到达了哪个点，可以更加容易推断出错的位置。
经常会遇到一个问题，如果不在我们的控制范围内，很多中间设备都是禁止ping的，但是ping不通不代表网络不通。这个时候就要使用telnet，通过其他协议来测试网络是否通，这个就不在本篇的讲述范围了。
说了这么多，你应该可以看出ping这个程序是使用了ICMP里面的ECHO REQUEST和ECHO REPLY类型的。
Traceroute：差错报文类型的使用那其他的类型呢？是不是只有真正遇到错误的时候，才能收到呢？那也不是，有一个程序Traceroute，是个“大骗子”。它会使用ICMP的规则，故意制造一些能够产生错误的场景。
所以，Traceroute的第一个作用就是故意设置特殊的TTL，来追踪去往目的地时沿途经过的路由器。Traceroute的参数指向某个目的IP地址，它会发送一个UDP的数据包。将TTL设置成1，也就是说一旦遇到一个路由器或者一个关卡，就表示它“牺牲”了。
如果中间的路由器不止一个，当然碰到第一个就“牺牲”。于是，返回一个ICMP包，也就是网络差错包，类型是时间超时。那大军前行就带一顿饭，试一试走多远会被饿死，然后找个哨探回来报告，那我就知道大军只带一顿饭能走多远了。
接下来，将TTL设置为2。第一关过了，第二关就“牺牲”了，那我就知道第二关有多远。如此反复，直到到达目的主机。这样，Traceroute就拿到了所有的路由器IP。当然，有的路由器压根不会回这个ICMP。这也是Traceroute一个公网的地址，看不到中间路由的原因。</description></item><item><title>第8讲_世界这么大，我想出网关：欧洲十国游与玄奘西行</title><link>https://artisanbox.github.io/5/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/40/</guid><description>前几节，我主要跟你讲了宿舍里和办公室里用到的网络协议。你已经有了一些基础，是时候去外网逛逛了！
怎么在宿舍上网？还记得咱们在宿舍的时候买了台交换机，几台机器组了一个局域网打游戏吗？可惜啊，只能打局域网的游戏，不能上网啊！盼啊盼啊，终于盼到大二，允许宿舍开通网络了。学校给每个宿舍的网口分配了一个IP地址。这个IP是校园网的IP，完全由网管部门控制。宿舍网的IP地址多为192.168.1.x。校园网的IP地址，假设是10.10.x.x。
这个时候，你要在宿舍上网，有两个办法：
第一个办法，让你们宿舍长再买一个网卡。这个时候，你们宿舍长的电脑里就有两张网卡。一张网卡的线插到你们宿舍的交换机上，另一张网卡的线插到校园网的网口。而且，这张新的网卡的IP地址要按照学校网管部门分配的配置，不然上不了网。这种情况下，如果你们宿舍的人要上网，就需要一直开着宿舍长的电脑。
第二个办法，你们共同出钱买个家庭路由器（反正当时我们买不起）。家庭路由器会有内网网口和外网网口。把外网网口的线插到校园网的网口上，将这个外网网口配置成和网管部的一样。内网网口连上你们宿舍的所有的电脑。这种情况下，如果你们宿舍的人要上网，就需要一直开着路由器。
这两种方法其实是一样的。只不过第一种方式，让你的宿舍长的电脑，变成一个有多个口的路由器而已。而你买的家庭路由器，里面也跑着程序，和你宿舍长电脑里的功能一样，只不过是一个嵌入式的系统。
当你的宿舍长能够上网之后，接下来，就是其他人的电脑怎么上网的问题。这就需要配置你们的网卡。当然DHCP是可以默认配置的。在进行网卡配置的时候，除了IP地址，还需要配置一个Gateway的东西，这个就是网关。
你了解MAC头和IP头的细节吗？一旦配置了IP地址和网关，往往就能够指定目标地址进行访问了。由于在跨网关访问的时候，牵扯到MAC地址和IP地址的变化，这里有必要详细描述一下MAC头和IP头的细节。
在MAC头里面，先是目标MAC地址，然后是源MAC地址，然后有一个协议类型，用来说明里面是IP协议。IP头里面的版本号，目前主流的还是IPv4，服务类型TOS在第三节讲ip addr命令的时候讲过，TTL在第7节讲ICMP协议的时候讲过。另外，还有8位标识协议。这里到了下一层的协议，也就是，是TCP还是UDP。最重要的就是源IP和目标IP。先是源IP地址，然后是目标IP地址。
在任何一台机器上，当要访问另一个IP地址的时候，都会先判断，这个目标IP地址，和当前机器的IP地址，是否在同一个网段。怎么判断同一个网段呢？需要CIDR和子网掩码，这个在第三节的时候也讲过了。
如果是同一个网段，例如，你访问你旁边的兄弟的电脑，那就没网关什么事情，直接将源地址和目标地址放入IP头中，然后通过ARP获得MAC地址，将源MAC和目的MAC放入MAC头中，发出去就可以了。
如果不是同一网段，例如，你要访问你们校园网里面的BBS，该怎么办？这就需要发往默认网关Gateway。Gateway的地址一定是和源IP地址是一个网段的。往往不是第一个，就是第二个。例如192.168.1.0/24这个网段，Gateway往往会是192.168.1.1/24或者192.168.1.2/24。
如何发往默认网关呢？网关不是和源IP地址是一个网段的么？这个过程就和发往同一个网段的其他机器是一样的：将源地址和目标IP地址放入IP头中，通过ARP获得网关的MAC地址，将源MAC和网关的MAC放入MAC头中，发送出去。网关所在的端口，例如192.168.1.1/24将网络包收进来，然后接下来怎么做，就完全看网关的了。
网关往往是一个路由器，是一个三层转发的设备。啥叫三层设备？前面也说过了，就是把MAC头和IP头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。
在你的宿舍里面，网关就是你宿舍长的电脑。一个路由器往往有多个网口，如果是一台服务器做这个事情，则就有多个网卡，其中一个网卡是和源IP同网段的。
很多情况下，人们把网关就叫做路由器。其实不完全准确，而另一种比喻更加恰当：路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的IP地址都和局域网的IP地址相同的网段，每只手都是它握住的那个局域网的网关。
任何一个想发往其他局域网的包，都会到达其中一只手，被拿进来，拿下MAC头和IP头，看看，根据自己的路由算法，选择另一只手，加上IP头和MAC头，然后扔出去。
静态路由是什么？这个时候，问题来了，该选择哪一只手？IP头和MAC头加什么内容，哪些变、哪些不变呢？这个问题比较复杂，大致可以分为两类，一个是静态路由，一个是动态路由。动态路由下一节我们详细地讲。这一节我们先说静态路由。
静态路由，其实就是在路由器上，配置一条一条规则。这些规则包括：想访问BBS站（它肯定有个网段），从2号口出去，下一跳是IP2；想访问教学视频站（它也有个自己的网段），从3号口出去，下一跳是IP3，然后保存在路由器里。
每当要选择从哪只手抛出去的时候，就一条一条的匹配规则，找到符合的规则，就按规则中设置的那样，从某个口抛出去，找下一跳IPX。
IP头和MAC头哪些变、哪些不变？对于IP头和MAC头哪些变、哪些不变的问题，可以分两种类型。我把它们称为“欧洲十国游”型和“玄奘西行”型。
之前我说过，MAC地址是一个局域网内才有效的地址。因而，MAC地址只要过网关，就必定会改变，因为已经换了局域网。两者主要的区别在于IP地址是否改变。不改变IP地址的网关，我们称为转发网关；改变IP地址的网关，我们称为NAT网关。
“欧洲十国游”型结合这个图，我们先来看“欧洲十国游”型。
服务器A要访问服务器B。首先，服务器A会思考，192.168.4.101和我不是一个网段的，因而需要先发给网关。那网关是谁呢？已经静态配置好了，网关是192.168.1.1。网关的MAC地址是多少呢？发送ARP获取网关的MAC地址，然后发送包。包的内容是这样的：
源MAC：服务器A的MAC
目标MAC：192.168.1.1这个网口的MAC
源IP：192.168.1.101
目标IP：192.168.4.101
包到达192.168.1.1这个网口，发现MAC一致，将包收进来，开始思考往哪里转发。
在路由器A中配置了静态路由之后，要想访问192.168.4.0/24，要从192.168.56.1这个口出去，下一跳为192.168.56.2。
于是，路由器A思考的时候，匹配上了这条路由，要从192.168.56.1这个口发出去，发给192.168.56.2，那192.168.56.2的MAC地址是多少呢？路由器A发送ARP获取192.168.56.2的MAC地址，然后发送包。包的内容是这样的：
源MAC：192.168.56.1的MAC地址
目标MAC：192.168.56.2的MAC地址
源IP：192.168.1.101
目标IP：192.168.4.101
包到达192.168.56.2这个网口，发现MAC一致，将包收进来，开始思考往哪里转发。
在路由器B中配置了静态路由，要想访问192.168.4.0/24，要从192.168.4.1这个口出去，没有下一跳了。因为我右手这个网卡，就是这个网段的，我是最后一跳了。
于是，路由器B思考的时候，匹配上了这条路由，要从192.168.4.1这个口发出去，发给192.168.4.101。那192.168.4.101的MAC地址是多少呢？路由器B发送ARP获取192.168.4.101的MAC地址，然后发送包。包的内容是这样的：
源MAC：192.168.4.1的MAC地址
目标MAC：192.168.4.101的MAC地址
源IP：192.168.1.101
目标IP：192.168.4.101</description></item><item><title>第9讲_路由协议：西出网关无故人，敢问路在何方</title><link>https://artisanbox.github.io/5/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/41/</guid><description>俗话说得好，在家千日好，出门一日难。网络包一旦出了网关，就像玄奘西行一样踏上了江湖漂泊的路。
上一节我们描述的是一个相对简单的情形。出了网关之后，只有一条路可以走。但是，网络世界复杂得多，一旦出了网关，会面临着很多路由器，有很多条道路可以选。如何选择一个更快速的道路求取真经呢？这里面还有很多门道可以讲。
如何配置路由？通过上一节的内容，你应该已经知道，路由器就是一台网络设备，它有多张网卡。当一个入口的网络包送到路由器时，它会根据一个本地的转发信息库，来决定如何正确地转发流量。这个转发信息库通常被称为路由表。
一张路由表中会有多条路由规则。每一条规则至少包含这三项信息。
目的网络：这个包想去哪儿？
出口设备：将包从哪个口扔出去？
下一跳网关：下一个路由器的地址。
通过route命令和ip route命令都可以进行查询或者配置。
例如，我们设置ip route add 10.176.48.0/20 via 10.173.32.1 dev eth0，就说明要去10.176.48.0/20这个目标网络，要从eth0端口出去，经过10.173.32.1。
上一节的例子中，网关上的路由策略就是按照这三项配置信息进行配置的。这种配置方式的一个核心思想是：根据目的IP地址来配置路由。
如何配置策略路由？当然，在真实的复杂的网络环境中，除了可以根据目的ip地址配置路由外，还可以根据多个参数来配置路由，这就称为策略路由。
可以配置多个路由表，可以根据源IP地址、入口设备、TOS等选择路由表，然后在路由表中查找路由。这样可以使得来自不同来源的包走不同的路由。
例如，我们设置：
ip rule add from 192.168.1.0/24 table 10 ip rule add from 192.168.2.0/24 table 20 表示从192.168.1.10/24这个网段来的，使用table 10中的路由表，而从192.168.2.0/24网段来的，使用table20的路由表。
在一条路由规则中，也可以走多条路径。例如，在下面的路由规则中：
ip route add default scope global nexthop via 100.100.100.1 weight 1 nexthop via 200.200.200.1 weight 2 下一跳有两个地方，分别是100.100.100.1和200.200.200.1，权重分别为1比2。
在什么情况下会用到如此复杂的配置呢？我来举一个现实中的例子。
我是房东，家里从运营商那儿拉了两根网线。这两根网线分别属于两个运行商。一个带宽大一些，一个带宽小一些。这个时候，我就不能买普通的家用路由器了，得买个高级点的，可以接两个外网的。
家里的网络呢，就是普通的家用网段192.168.1.x/24。家里有两个租户，分别把线连到路由器上。IP地址为192.168.1.101/24和192.168.1.102/24，网关都是192.168.1.1/24，网关在路由器上。
就像上一节说的一样，家里的网段是私有网段，出去的包需要NAT成公网的IP地址，因而路由器是一个NAT路由器。
两个运营商都要为这个网关配置一个公网的IP地址。如果你去查看你们家路由器里的网段，基本就是我图中画的样子。
运行商里面也有一个IP地址，在运营商网络里面的网关。不同的运营商方法不一样，有的是/32的，也即一个一对一连接。
例如，运营商1给路由器分配的地址是183.134.189.34/32，而运营商网络里面的网关是183.134.188.1/32。有的是/30的，也就是分了一个特别小的网段。运营商2给路由器分配的地址是60.190.27.190/30，运营商网络里面的网关是60.190.27.189/30。</description></item><item><title>第二季回归_这次，我们一起实战解析真实世界的编译器</title><link>https://artisanbox.github.io/6/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/39/</guid><description>你好，我是宫文学，这次我带着一门全新的课程《编译原理实战课》回来了。
我在《编译原理之美》的开篇词中就说过，编译原理与你的工作息息相关，无论你是前端工程师、后端工程师，还是运维工程师，不论你是初级工程师还是职场老手，编译技术都能给你帮助，甚至让你提升一个级别。
在第一季，我带你一起梳理了编译技术最核心的概念、理论和算法，帮你构建出了一条相对平坦的学习曲线，让你能够理解大多数技术人都很畏惧的编译原理核心知识。在课程更新的过程中，我发现有很多同学都会有这样一个疑问，那就是：“我确实理解了编译技术的相关原理、概念、算法等，但是有没有更直接的方式，能让我更加深入地把知识与实践相结合呢？”
所以，在第二季，我会以实战的方式带你挑战编译原理这个领域，也就是带你一起解析真实世界中的编译器。在课程中，我会带你研究不同语言的编译器的源代码，一起跟踪它们的运行过程，分析编译过程的每一步是如何实现的，并会对有特点的编译技术点加以分析和点评。在这个过程中，你会获得对编译器的第一手的理解。
另外，我还会带你分析和总结前面已经研究过的编译器，让你对现代语言的编译器的结构、所采用的算法以及设计上的权衡，都获得比较真切的认识。
下面是专栏的目录：
为了感谢老同学， 我还准备了一个「专属福利」：
6月1日课程上线，我会送你一张15元专属优惠券，可与限时优惠同享，有效期48小时，建议尽早使用。
点击下方图片，立即免费试读新专栏。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>结束语_不盲从于群体思维，走一条适合自己的路</title><link>https://artisanbox.github.io/8/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/8/30/</guid><description>你好，我是朱晓峰。
到这里，咱们的课程就全部结束了。不过，课程结束更新，并不意味着MySQL的学习结束了。相反，也许对于你来说，这是一个新的开始。
我一直强调，在真实项目中进行学习，是掌握MySQL的最好方法。所以，在课程结束以后，你一定要把学到的内容真正应用到实际工作中，反复实践，这样才能真正成为优秀的数据库开发人员。
那在今天这篇结束语里呢，我想暂且抛开MySQL，和你分享一个我的人生准则：不盲从于群体思维，按自己的意愿和个人的特质去做选择，走一条适合自己的路。
为什么在这告别之际，我最想给你分享这一点呢？
因为我们每个人都生活在群体之中，每天都在和不同的人打交道，尤其是现在信息极度丰富，每天都有人在表达着自己的观点，试图成为“意见领袖”，一不小心，别人的想法就会左右我们的行为。
但每个个体都是与众不同的，有的人擅长把细节做到极致，有的人擅长把控全局；有的人对数字特别敏感，有的人长于艺术想象……无论是学习技术，还是选择职业路径，只有适合自己的，才是我们成为人生赢家的捷径。
说到这儿，我想和你分享一个我的真实经历。
大学毕业以后，我进入一家外企工作，从普通程序员做起，一直做到构架师。工作很稳定，待遇也好，家人觉得很不错，希望我买套房子，赶紧结婚生子。
可是，我对这样的生活一点也不满意。因为我发现，在熟悉了工作内容之后，日常的一切对我已经没有什么挑战了，每天不过是重复熟悉的流程，感觉一年跟一天没有什么差别。这样的人生，不是我想要的。
我决定辞职，去美国留学，这件事情花光了我多年的积蓄。当时，很多人都在质疑我，觉得我是放弃了光明的前途，走向了未知的恐怖。但是，未知并不意味着恐怖，甚至可能是出乎预料的精彩。
事实证明，那段经历彻底改变了我。我学到了崭新的知识和不同的思维方式，也正因此，我又有机会进入到世界顶级的企业工作，成为了摩根大通银行东京分行副总裁，保证了一系列重大项目的落地和实施。
试想一下，假使我当时听从别人的劝导，选择过安安稳稳的日子，现在可能还在公司里面打工，计算着还有多少年可以退休。而现在的我，在开拓职业新赛道，同时也有机会写下这门课。
其实，人类的一个特点就是很容易接受暗示，尤其是在你犹豫不决的时候，往往一个简单暗示，就会影响你的决定。所以，你一定要慎重对待群体建议，在做每一个重要的决定前，都要先想想这个决定是为自己而做，还是为迎合群体而做。
你可能会说，我知道不要盲从群体思维，但是怎么按照自己的意愿去生活呢？接下来，我就结合我的人生经历，给你分享3条建议。
首先，你要制定长期规划，形成自己的一个稳定的内在架构。
有句话叫做：“谋定而后动，知止而有得。”如果做什么都没有规划，凡事跟着感觉走，是很难有作为的。而所谓的长期规划，我认为可以容纳进一套架构里。实际上，不只企业有架构，我们每个人都要形成自己的一个架构。
什么意思呢？我给你举个简单的小例子，你一听就明白了。
记得在银行工作的时候，有一次跟我们部门的一位传奇的交易员聊天，他每年为公司创造的利润超过你的想象。我问他怎么知道一笔交易能够盈利呢，他的回答出乎我的意料，他说：“我不知道这笔交易能不能赢，但是我的计算模型告诉我，我赢的可能性大于输的可能性。我一直这样做，只要交易足够多，我就不可能输。”
你看，我们眼中的一笔笔独立的交易，都是他长线规划中的一环。他实际上是预先做了计算的，一切都是按照计划进行的。虽然他不知道下一笔交易能不能赢，但按照计划行事，赢的可能性就比较大，只要时间足够长，交易量足够多，那么在最终的结果里，他肯定是赢家。
所以，要想走在自己期待的道路上，就要把视角放在更广阔的空间维度和更长久的时间维度上，建立起自己的一套架构，并不断用经验和经历去修正它，让它尽可能稳固，这样你才能在做决定时，有个指导，至少不会让你跑偏。
其次，对待任何结果，不过多计较一时的得失。
正确的决定，并不一定会带来好的结果；错误的决定，也可能会侥幸成功，重要的是客观地分析造成这个结果的原因，而不是简单地以结果的好坏，来判断决定的正确性。
我还是想分享一个我经历过的一件事。
记得云服务器刚开始流行起来的时候，因为我们觉得云服务器比自己搭建服务器，在可靠性、资源利用率和成本等方面都很有优势，于是，我们就给客户推荐了大厂的云服务器。可是，没想到大厂也有失手的时候，整个地区全部宕机，所有的客户都没法访问云服务器了。
这个时候，如果是你，你会怎么做呢？放弃云服务器，重新回归本地服务器，肯定会防止类似的事件再次发生，但问题是，这样的选择真的是正确的吗？
当时，云服务器确实比较新，大厂也可能会失手，但是，一次失败并不意味着云服务器就不能用了。从长远来说，云服务器比本地服务器的优势多太多了，大厂的技术也在不断成熟，给客户推荐云服务器肯定仍然是正确的选择。
实际上，任何事情都有成功和失败的概率。正确的选择是成功概率大的那个方案，不要因为一次偶然的失败，就轻易改变我们的选择。
总之，一定要学会不简单地以结果来评判对错，不妨问问自己，当初决定这样做的理由是什么？为什么会导致这个结果？是必然的呢？还是偶然因素造成的？哪个方案成功的概率最大？等等，在回答这些问题的过程中理清思路，努力做出全面客观的判断。
最后，正确的要坚持，但如果发现错了，果断放弃非常重要。
几乎所有人都在强调，做事要坚持不懈，这当然是非常重要的，毕竟不能忽视时间的力量。但是，如果你已经意识到方向错了，就一定要果断放弃。
我曾经非常迷信某大厂的软件开发生态，觉得它们能够提供全套的开发环境，稳定性好，特别人性化。自以为站在巨人的肩膀上，就可以借到力了。可是有一次，我们给客户定制的软件在客户的机器上无法启动，由于大厂的开发环境都是封闭的，我们无法了解内部到底发生了什么。这样一来，问题就一直得不到解决。最后，我们不得不回到几个月前的版本，重新编写相应的模块。
这件事使我意识到，要开发出真正满足客户需求的软件，必须要了解底层，开源的开发环境，才是最佳的选择。因为开源软件可以通过阅读源代码，了解所有的技术细节。
这个时候，我意识到，如果明知现在的方向不对，还要一直走下去，虽然比较省力，但是很多项目就不能做了，那样就会把路越走越窄。
最后，我们果断放弃了某大厂的软件，逐步切换到开源体系。刚开始的时候，因为要熟悉新的操作系统、新的编程语言、新的数据库软件，要学的东西太多，每一步都十分痛苦，很多人离开了。
但是，随着对开源体系越来越熟悉，发现原来的项目可以做，甚至很多原来不能做的项目也可以做了，这样一来，路就越走越宽了。
放弃正在走的错误的道路就意味着改变，而人十分容易产生惰性，一旦适应了现在的环境，就不愿意改变，这样就很容易忽视正确的东西。微小积累会引发持续改变这件事，不仅会在正确的地方得到验证，更会在错误的道路上体现得淋漓尽致。因此，在错误处调头，虽难，但意义重大。
好了，这些就是我在这几十年中，踩过无数坑、走了无数弯路后提炼出来的生活准则，很高兴有机会分享给你，但愿对你有所帮助。希望你不仅成为一个具有很强技术能力的人，同时还能真正走出一条适合自己的路，成为人生赢家。
课程的最后，我准备了一份调研问卷，希望你能花1分钟填写下，聊聊你对这门课的想法。同时，我也给你准备了礼物，只要填写问卷，就有机会获得一个手绘护腕垫或者是价值99元的课程阅码。期待你的畅所欲言。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>结束语_实战是唯一标准！</title><link>https://artisanbox.github.io/7/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/47/</guid><description>你好，我是宫文学。
转眼之间，“编译原理实战课”计划中的内容已经发布完毕了。在这季课程中，你的感受如何？有了哪些收获？遇到了哪些困难？
很多同学可能会觉得这一季的课程比上一季的“编译原理之美”要难一些。不过为什么一定要推出这么一门课，来研究实际编译器的实现呢？这是因为我相信，实战是检验你是否掌握了编译原理的唯一标准，也是学习编译原理的真正目标。
计算机领域的工程性很强。这决定了我们学习编译原理，不仅仅是掌握理论，而是要把它付诸实践。在我们学习编译原理的过程中，如果遇到内心有疑惑的地方，那不妨把实战作为决策的标准。
这些疑惑可能包括很多，比如：
词法分析和语法分析工具，应该手写，还是用工具生成？ 应该用LL算法，还是LR算法？ 后端应该用工具，还是自己手写？ 我是否应该学习后端？ IR应该用什么数据结构？ 寄存器分配采用什么算法比较好？ …… 上述问题，如果想在教科书里找到答案，哪怕是“读万卷书”，也是比较难的。而换一个思路，“行万里路”，那就很容易了。你会发现每种语言，因为其适用的领域和设计的目标不同，对于上述问题会采用不同的技术策略，而每种技术策略都有它的道理。从中，你不仅仅可以为上述问题找到答案，更重要的是学会权衡不同技术方案的考虑因素，从而对知识点活学活用起来。
我们说实战是标准。那你可能会反问，难道掌握基础理论和原理就不重要了吗？这是在很多领域都在争论的一个话题：理论重要，还是实践重要。
理论重要，还是实践重要？理论和原理当然重要，在编译原理中也是如此。形式语言有关的理论，以及前端、中端和后端的各个经典算法，构筑了编译原理这门课坚实的理论基础。
但是，在出现编译原理这门课之前，在出现龙书虎书之前，工程师们已经在写编译器了。
你在工作中，有时候就会遇到理论派和实践派之争。举例来说，有时候从理论角度，某一个方案会“看上去很美”。那到底是否采用该方案呢？这个时候，就需要拿实践来说话了。
我拿Linux内核的发展举个例子。当年Linus推出Linux内核的时候，并没有采用学术界推崇的微内核架构，为此Linus还跟Minix的作者有一场著名的辩论。而实践证明，Linux内核发展得很成功，而GNU的另一个采用微架构的内核Hurd发展了20多年还没落地。
客观地说，Linux内核后来也吸收了很多微内核的设计理念。而声称采用微内核架构的Windows系统和macOS系统，其实在很多地方也已经违背了微内核的原则，而具备Linux那样的单内核的特征。之所以有上述的融合，其实都是一个原因，就是为了得到更好的实用效果。所以，实践会为很多历史上的争论划上句号。
在编译技术和计算机语言设计领域，也存在着很多的理论与实践之争。比如，理论上，似乎函数式编程更简洁、更强大，学术界也很偏爱它，但是纯函数的编程语言，至今没有成为主流，这是为什么呢？
再比如，是否一定要把龙书虎书都读明白，才算学会了编译原理呢？
再进一步，如果你使用编译技术的时候，遇到一个实际的问题，是跟着龙书、虎书还有各种课本走，还是拿出一个能解决问题的方案就行？
在课程里，我鼓励你抛弃一些传统上学习编译原理的困扰。如果龙书、虎书看不明白，那也不用过于纠结，这并不会挡住你学习的道路。多看实际的编译器，多自己动手实践，在这个过程中，你自然会明白课本里原来不知所云的知识点。
那么如何以实践为指导，从而具备更好的技术方案鉴别力呢？在本课程里，我们有三个重点。包括研究常用语言的编译器、从计算机语言设计的高度来理解编译原理，以及从运行时的实现来理解编译原理。
对于你所使用的语言，应该把它的编译器研究透这门课程的主张是，你最好把自己所使用语言的编译器研究透。这个建议有几个理由。
第一，因为这门语言是你所熟悉的，所以你研究起来速度会更快。比如，可以更快地写出测试用的程序。并且，由于很多语言的编译器都已经实现了自举，比如说Go语言和Java语言的编译器，所以你可以更快地理解源代码，以及对编译器本身做调试。
第二，这门语言的编译器所采用的实现技术，一定是体现了该语言的特性的。比如V8会强调解析速度快，Java编译器要支持注解特性等，值得你去仔细体会。
第三，研究透编译器，会加深你对这门语言的理解。比如说，你了解清楚了Java的编译器是如何处理泛型的，那你就会彻底理解Java泛型机制的优缺点。而C++的模板机制，对于学习C++的同学是有一定挑战的。但一旦你把它在编译期的实现机制弄明白，就会彻底掌握模板机制。我也计划在后续用一篇加餐，把C++的模板机制给你拆解一下。
那么，既然编译器是为具体语言服务的，所以，我们也在课程里介绍了计算机语言设计所考虑的那些关键因素，以及它们对编译技术的影响。
从计算机语言设计的高度，去理解编译技术在课程里你已经体会到了，语言设计的不同，必然导致采用编译技术不同。
其实，从计算机语言设计的高度上看，编译器只是实现计算机语言的一块底层基石。计算机语言设计本身有很多的研究课题，比如类型系统、所采用的编程范式、泛型特性、元编程特性等等，我们在课程里有所涉猎，但并没有在理论层面深挖。有些学校会从这个方向上来培养博士生，他们会在理论层面做更深入的研究。
什么样的计算机语言是一个好的设计？这是一个充满争议的话题，我们这门课程尽量不参与这个话题的讨论。我们的任务，是要知道当采用不同的语言设计时，应该如何运用编译技术来落地。特别是，要了解自己所使用的语言的实现机制。
如果说计算机语言设计，是一种偏理论性的视角，那么程序具体的运行机制，则是更加注重落地的一种视角。
从程序运行机制的角度，去理解编译技术学习编译原理的一个挑战，就在于你必须真正理解程序是如何运行的，以及程序都可以有哪几种运行方式。这样，你才能理解如何生成服务于这种运行机制的目标代码。
最最基础的，你需要了解像C语言这样的编译成机器码直接运行的语言，它的运行机制是怎样的。代码放在哪里，又是如何一步步被执行的。在执行过程中，栈是怎么变化的。函数调用的过程中，都发生了些什么事情。什么数据是放在栈里的，什么数据是放在堆里的，等等。
在此基础上，如果从C语言换成C++呢？C++多了个对象机制，那对象在内存里是一个什么结构？多重继承的时候是一个什么结构？在存在多态的时候，如何实现方法的正确绑定？这些C++比C语言多出来的语义，你也要能够在运行时机制中把它弄清楚。
再进一步，到了Go语言，仍然是编译成机器码运行的，但跟C和C++又有本质区别。因为Go语言的运行时里包含了垃圾收集机制和并发调度机制，这两个机制要跟你的程序编译成的代码互相配合，所以编译器生成的目标代码里要体现内存管理和并发这两大机制。像Go语言这种特殊的运行机制，还导致了跨语言调用的难度。用Go语言调用C语言的库，要有一定的转换和开销。
然后呢，语言运行时的抽象度进一步增加。到了Java语言，就用到一个虚拟机。字节码也正式登台亮相。你需要知道栈机和寄存器机这两种不同的运行字节码的解释器，也要知道它们对应的字节码的差别。而为了提升运行速度，JIT、分层编译和逆优化机制又登场，栈上替换（OSR）技术也产生。这个时候，你需要理解解释执行和运行JIT生成的本地代码，是如何无缝衔接的。这个时候的栈桢，又有何不同。
然后是JavaScript的运行时机制，就更加复杂了。V8不仅具备JVM的那些能力，在编译时还要去推断变量的类型，并且通过隐藏类的方式安排对象的内存布局，以及通过内联缓存的技术去加快对象属性的访问速度。
这样从最简单的运行时，到最复杂的虚拟机，你都能理解其运行机制的话，你其实不仅知道在不同场景下如何使用编译技术，甚至可以参与虚拟机等底层软件的研发了。
不再是谈论，来参与实战吧！今天，我们学习编译原理，目标不能放在考试考多少分上。中国的技术生态，使得我们已经能够孕育自己的编译器、自己的语言、自己的虚拟机。方舟编译器已经带了个头。我想，中国不会只有方舟编译器孤军奋战的！
就算是开发普通的应用软件，我们也要运用编译技术，让它们平台化，让中国制造的软件，具有更高的技术含量，颠覆世界对于“中国软件”的品牌认知。这样的颠覆，在手机、家电等制造业已经发生了，也应该轮到软件业了。
而经验告诉我们，一旦中国的厂商和工程师开始动起来，那么速度会是非常快的。编译技术并没有多么难。我相信，只要短短几年时间，中国软件界就会在这个领域崭露头角！
这就是我们这门课程的目的。不是为了学习而学习，而是为了实战而学习。
当然，课程虽然看似结束了，但也代表着你学习的重新开始。后面我计划再写几篇加餐，会针对C++、Rust等编译器再做一些解析，拓展你的学习地图。并且，针对方舟编译器，我还会进一步跟你分享我的一些研究成果，希望我们可以形成一个持续不断地对编译器进行研究的社群，让学习和研究不断深入下去，不断走向实用。
另外，我还给你准备了一份毕业问卷，题目不多，希望你能在问卷里聊一聊你对这门课的看法。欢迎你点击下面的图片，用1～2分钟时间填写一下，期待你畅所欲言。当然了，如果你对课程内容还有什么问题，也欢迎你在留言区继续提问，我会持续回复你的留言。
我们江湖再见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>结束语_放弃完美主义，执行力就是限时限量认真完成</title><link>https://artisanbox.github.io/5/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/45/</guid><description>你好，我是刘超。
从筹备、上线到今天专栏完结，过去了将近半年的时间。200多天，弹指一挥间。
我原本计划写36篇，最后愣是写到了45篇。原本编辑让我一篇写两三千字，结果几乎每篇都是四五千字。这里面涉及图片张数我没具体数过，但是据说多到让编辑上传到吐。编辑一篇我的稿件的工作量相当于别的专栏的两倍。
人常说，有多少付出，就有多少回报。但是，写这个“趣谈网络协议”专栏，我收获的东西远超过我的想象。我希望你的收获也是如此。为什么这么说呢？我们把时间放回到这个专栏最开始的时候，我慢慢跟你讲。
我不是最懂的人，但我想尝试成为这样的人今年年初，极客时间来找我，希望我讲一些偏重基础的知识，比如网络协议。
他们一提到这个主题，我就很兴奋，因为这也触动了我心中长期以来的想法，因为网络这个东西学起来实在是太痛苦。
但是，说实话，接下这个重任，我心里其实是有点“怕”的。我怕自己不够专业，毕竟业内有这么多网络工程师和研究网络理论的教授。我讲这个课会不会贻笑大方啊？
我知道，很多技术人员不敢写博客、写公众号，其实都有这种“怕”的心理：我又不牛，没啥要分享的，要是误导了别人怎么办？
如果你想做出一些成绩，这个心理一定要克服。其实每个人都有自己的相对优势。对于某个东西，你研究的时间不一定是最长的，但是你可能有特殊的角度、表达方式和应用场景。坚定了这个想法之后，我就开始投入热火朝天的专栏写作了。
一旦开始写，我发现，这个事情远没有看上去那么简单。它会花费你非常多的个人时间。写专栏这几个月，晚上两点之后睡，周末全在写专栏，基本成为我的生活常态。但是我想挑战一下自己，我觉得，只要咬牙挺过去，自己的技术就会上升一个层次。
放弃完美主义，执行力就是限时限量认真完成技术人都有完美主义倾向，觉得什么事情都要钻研个底儿朝天，才拿出来见人。我也一样。
我曾经答应某出版社写一本搜索引擎的书。这本书分为原理篇和实践篇。我总觉得我还没把原理篇写完，就不能写实践篇。但是，仅原理篇我就写了一年。搜索引擎就火了一两年，最后时间窗口过了，书稿没有完成，这件事儿也就这么搁浅了。
所以，完美主义虽然是个很好听的词，但是它往往是和拖延症如影随形的，它常常会给拖延症披上一个华丽的外衣，说，我是因为追求完美嘛。但是，最终的结果往往是，理论研究半天还没动手，执行力很差。时间点过了，就心安理得地说，反正现在也不需要了，那就算了吧。久而久之，你就会发现，自己好像陷入了瓶颈。
我慢慢明白过来，我们不是为了做技术而做技术，做技术是为了满足人类需求的。完美主义是好事儿，但是，坚持完美主义的同时要限时限量地完成，才能形成执行力。
写这个专栏之后，我更加深刻地体会到这一点。每周都要写三篇文章，压力很大，根本容不得任何拖延。如果我还是坚持以前完美主义的做法，读完十本书，用三年时间把网络协议都研究透再来写，那现在就没有这个专栏了。
如果我们要强调执行力，时间点这个因素就至关重要。在固定的时间点上，就要把控范围，不能顾虑太多，要勇于放弃。就像给产品做排期，先做最小闭环的功能集合，其他的放在以后再补充。在这个前提下，以自己最大的限度往完美的方向上努力。比如，我觉得每天2点睡是我的身体极限，努力到这个程度，我也就无愧于心了。
所以说，我们做事情的目的并不是完美，而是在固定的时间点，以固定的数量和质量，尽可能认真地满足当时的客户需求，这才是最重要的。
这样做肯定会有不满意的地方，比如很多同学在留言区指出我的错误，甚至有的同学提的问题，我原来都没思考过。但是，我觉得这些都不是事儿。我可以再查资料，再补充、再完善。所以，后来时间宽裕了，我还增加了5期答疑，回答了一下之前没来得及回答地问题。高手在民间，咱们一起来讨论和进步。这个过程已经让我受益良多。
保持饥渴，不怕被“鄙视”，勇于脱离舒适区有人可能会问了，你看你既不是最专业的，还不追求完美，真的不怕被人“鄙视”吗？
被“鄙视”，谁都怕，这也是为什么越大的会议，参加人数越多的演讲，越是没有人提出具体的问题。大家都怕丢人，看上去好像大家都听懂了，就我啥都不懂，我要是问，被大家笑话怎么办？我想很多人都有这样的经历吧？我也来给你讲讲我的亲身经历。
我从Windows开发去做Linux的存储系统开发时，连Linux man都不会看；我在惠普从事OpenStack实施工作的时候，对于网络的了解一塌糊涂，一直被甲方骂；我在华为做云计算，支撑运营商项目的时候，面对一大堆核心网词汇，一脸懵；我在网易云对内支撑考拉的时候，在微服务架构方面也是小白……被“鄙视”了这么多次之后，我不怕了。因为这每一次“鄙视”都可以让我发现自己的短板，然后啃下这些东西，这不就是最大的收获吗？
我就是这样一直被“鄙视”着成长起来的人，我就是常常在别人分享的时候坐第一排问很傻的那种问题的人，我就是常常一知半解还愿意和别人讨论的人……
怕被“鄙视”，说明你还不够饥渴，还没有勇气脱离你的舒适区。 在你熟悉的领域里面，你是最最权威的，但是，天下之大，你真的只满足于眼前这一亩三分地吗？
很多人因为怕被“鄙视”，不敢问、不敢做，因而与很多美好的东西都擦肩而过了。直到有一天你用到了，你才后悔，当时自己怎么没去多问一句。
所以，当你看到一个特别好的、突破自己的学习机会，别犹豫，搭上这辆车。等过了十年，你会发现，当年那些嘲笑、轻视，甚至谩骂，都算不了什么，进步本身才是最最重要的。
今天，咱们没有谈具体的知识，我只表达了一下我的观点。我就是那个你在直播里看到的，那个邋遢、搞笑、不装，同时做事认真，愿意和你一起进步的技术大叔。
脱离舒适区吧，希望我们可以一起成长！
最后，我在这里放了一个毕业调查问卷。如果你对这个专栏或者我本人有什么建议，可以通过这个问卷进行反馈，我一定会认真查看每一封的内容。期待你的反馈！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>结束语_点线网面，一起构建MySQL知识网络</title><link>https://artisanbox.github.io/1/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/46/</guid><description>时光流逝，这是专栏的最后一篇文章。回顾整个过程，如果用一个词来描述，就是“没料到”：
我没料到文章这么难写，似乎每一篇文章都要用尽所学；
我没料到评论这么精彩，以致于我花在评论区的时间并不比正文少；
我没料到收获这么大，每一次被评论区的提问问到盲点，都会带着久违的兴奋去分析代码。
如果让我自己评价这个专栏：
我最满意的部分，是每一篇文章都带上了实践案例，也尽量讲清楚了原理；
我最得意的段落，是在讲事务隔离级别的时候，把文章重写到第三遍，终于能够写上“到这里，我们把一致性读、当前读和行锁就串起来了”；
我最开心的时候，是看到评论区有同学在回答课后思考题时，准确地用上了之前文章介绍的知识点。因为我理解的构建知识网络，就是这么从点到线，从线到网，从网到面的过程，很欣喜能跟大家一起走过这个过程。
当然，我更看重的还是你的评价。所以，当我看到你们在评论区和知乎说“好”的时候，就只会更细致地设计文章内容和课后思考题。
同时，我知道专栏的订阅用户中，有刚刚接触MySQL的新人，也有使用MySQL多年的同学。所以，我始终都在告诫自己，要尽量让大家都能有所收获。
在我的理解里，介绍数据库的文章需要有操作性，每一个操作有相应的原理，每一个原理背后又有它的原理，这是一个链条。能够讲清楚链条中的一个环节，就可能是一篇好文章。但是，每一层都有不同的受众。所以，我给这45篇文章定的目标就是：讲清楚操作和第一层的原理，并适当触及第二层原理。希望这样的设计不会让你觉得太浅。
有同学在问MySQL的学习路径，我在这里就和你谈谈我的理解。
1. 路径千万条，实践第一条如果你问一个DBA“理解得最深刻的知识点”，他很可能告诉你是他踩得最深的那个坑。由此，“实践”的重要性可见一斑。
以前我带新人的时候，第一步就是要求他们手动搭建一套主备复制结构。并且，平时碰到问题的时候，我要求要动手复现。
从专栏评论区的留言可以看出来，有不少同学在跟着专栏中的案例做实验，我觉得这是个非常好的习惯，希望你能继续坚持下去。在阅读其他技术文章、图书的时候，也是同样的道理。如果你觉得自己理解了一个知识点，也一定要尝试设计一个例子来验证它。
同时，在设计案例的时候，我建议你也设计一个对照的反例，从而达到知识融汇贯通的目的。就像我在写这个专栏的过程中，就感觉自己也涨了不少知识，主要就得益于给文章设计案例的过程。
2. 原理说不清，双手白费劲不论是先实践再搞清楚原理去解释，还是先明白原理再通过实践去验证，都不失为一种好的学习方法，因人而异。但是，怎么证明自己是不是真的把原理弄清楚了呢？答案是说出来、写出来。
如果有人请教你某个知识点，那真是太好了，一定要跟他讲明白。不要觉得这是在浪费时间。因为这样做，一来可以帮你验证自己确实搞懂了这个知识点；二来可以提升自己的技术表达能力，毕竟你终究要面临和这样的三类人讲清楚原理的情况，即：老板、晋升答辩的评委、新工作的面试官。
我在带新人的时候，如果这一届的新人不止一个，就会让他们组成学习小组，并定期给他们出一个已经有确定答案的问题。大家分头去研究，之后在小组内进行讨论。如果你能碰到愿意跟你结成学习小组的同学，一定要好好珍惜。
而“写出来”又是一个更高的境界。因为，你在写的过程中，就会发现这个“明白”很可能只是一个假象。所以，在专栏下面写下自己对本章知识点的理解，也是一个不错的夯实学习成果的方法。
3. 知识没体系，转身就忘记把知识点“写下来”，还有一个好处，就是你会发现这个知识点的关联知识点。深究下去，点就连成线，然后再跟别的线找交叉。
比如，我们专栏里面讲到对临时表的操作不记录日志，然后你就可以给自己一个问题，这会不会导致备库同步出错？再比如，了解了临时表在不同的binlog格式下的行为，再追问一句，如果创建表的时候是statement格式，之后再修改为row格式（或者反之），会怎么样呢？
把这些都搞明白以后，你就能够把临时表、日志格式、同步机制，甚至于事务机制都连起来了。
相信你和我一样，在学习过程中最喜欢的就是这种交叉的瞬间。交叉多了，就形成了网络。而有了网络以后，吸收新知识的速度就很快了。
比如，如果你对事务隔离级别弄得很清楚了，在看到第45篇文章讲的max_trx_id超限会导致持续脏读的时候，相信你理解起来就很容易了。
4. 手册补全面，案例扫盲点有同学还问我，要不要一开始就看手册？我的建议是不要。看手册的时机，应该是你的知识网络构建得差不多的时候。
那你可能会问，什么时候算是差不多呢？其实，这没有一个固定的标准。但是，有一些基本实践可以帮你去做一个检验。
能否解释清楚错误日志（error log）、慢查询日志（slow log）中每一行的意思？ 能否快速评估出一个表结构或者一条SQL语句，设计得是否合理？ 能否通过explain的结果，来“脑补”整个执行过程（我们已经在专栏中练习几次了）？ 到网络上找MySQL的实践建议，对于每一条做一次分析： 如果觉得不合理，能否给出自己的意见？ 如果觉得合理，能否给出自己的解释？ 那，怎么判断自己的意见或者解释对不对呢？最快速、有效的途径，就是找有经验的人讨论。比如说，留言到我们专栏的相关文章的评论区，就是一个可行的方法。
这些实践做完后，你就应该对自己比较有信心了。这时候，你可以再去看手册，把知识网络中的盲点补全，进而形成面。而补全的方法就是前两点了，理论加实践。
我希望这45篇文章，可以在你构建MySQL知识体系的过程中，起到一个加速器的作用。
我特意安排在最后一篇文章，和你介绍MySQL里各种自增id达到定义的上限以后的不同行为。“45”就是我们这个专栏的id上限，而这一篇结束语，便是超过上限后的第一个值。这是一个未定义的值，由你来定义：
有的同学可能会像表定义的自增id一样，就让它定格在这里； 有的同学可能会像row_id一样，二刷，然后用新的、更全面的理解去替代之前的理解； 也许最多的情况是会像thread_id一样，将已经彻底掌握的文章标记起来，专门刷那些之前看过、但是已经印象模糊的文章。 不论是哪一种策略，只要这45篇文章中，有那么几个知识点，像Xid或者InnoDB trx_id一样，持久化到了你的知识网络里，你和我在这里花费的时间，就是“极客”的时间，就值了。
这是专栏的最后一篇文章的最后一句话，江湖再见。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>结束语_生活可以一地鸡毛，但操作系统却是心中的光</title><link>https://artisanbox.github.io/9/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/50/</guid><description>你好，我是LMOS。
感谢你的一路相伴，我们的《操作系统实战45讲》专栏写到此处，你亦能学至此处，多半是出于兴趣，出于一种对操作系统的热爱，出于一种对事物本质发自内心的苛求……
如果是这样，请你永远保持这份心性，它会给你带来更多意想不到的结果。走到这里，也让我们先停住前进的脚步，回忆一下这一路走来都做了些什么事情，收获了什么，有什么让我们印象深刻的体会？
我作为Cosmos和《操作系统实战45讲》专栏这两大作品的作者先来开个头，跟你说说我自己的感受和体会，可以用两个“出乎意料”来表示。
第一个“出乎意料”，是课程出乎意料的难写。我之前写过书，也写过多个操作系统内核，更是做过业界重量级的傲腾项目，但是专栏之难，超过我之前做过所有的项目之难。
一开始，我也不明白为什么写专栏比写代码难？但经历了整个专栏的筹备、备稿、修改，一直到更新和答疑的各个环节，我才深刻地体会到这点。
我最初设计整个专栏的时候，就想兼顾宏观思路和细节实现，既带你领略操作系统的壮观风景，也能作为指导手册让你跟着我动手实现。但是写起来才发现，为了完成这两点，我实际花的时间跟精力，远远超过了预估。
写专栏，难就难在要用通俗的大白话，把复杂的操作系统“讲”出来，而不只是写出来；难就难在细节与重点的把握和梳理。如果只有细节，就难以体现出重点。可是如果只有重点思路，我又担心内容会让你觉得过于抽象；难就难在，我要交付的对象，不再是编译器，而是各个不同思想层次、不同思维方式的人。
第二个“出我意料”，是出我意料的“热”。我搞了很多年的操作系统，感觉操作系统在整个行业之中非常冷，操作系统之冷，是那种高处不胜寒的“冷”，是学校老师都只愿意从理论上一笔带过的“冷” ，是互联网时代的创新企业无法触及，也不敢触及的“冷”。
但是出我意料的是，专栏刚刚上线不久就引起了业界广泛关注，其热度超出了我的想像。我以为在业务为王的今天，很少有人会关注这么底层的操作系统。不得不说，这些关注从侧面说明了操作系统在各从业人员心中的重要性，同时也说明了我们对亲手实现一个操作系统这件事充满好奇。
前面这些是对专栏的体会和感受，下面我想谈一谈写Cosmos的感受。相信看过专栏的同学，对操作系统工程之浩大，代码之精微，都有了深切的体会和认知。说开发成熟操作系统之难，难于上青天，这绝不是夸张和开玩笑。
在互联网时代，我可能比围观的同学更清楚，不能基于功利的目的去开发Cosmos，在今天它无法直接给我们产生价值，我开发Cosmos是基于兴趣，是对技术的探索和追求。我就是那种人——生活可以一地鸡毛，但操作系统却是心中的光。
Cosmos断断续续开发很多年，几次推倒重来，正是在这种一次次重构之下，摸索、总结，才设计出了今天Cosmos的架构。很多代码要反复测试验证，对于没有达到预期的代码，我需要对其算法进行分析，找出原因。
Cosmos的调试是最难的，往往需要查找其文件的反汇编代码，然后一条一条对比，在脑中模拟指令的执行过程和结果，并发现隐藏其中的Bug，这些都是极其烦琐的事情。不瞒你说，我也会一个bug卡好几天，感觉写内核仿佛是一场“法事”，一个人念咒、画符，请神跳舞……可以说，若没有“爱”的加持，真的很难坚持下来。
其实，我们写专栏的顺序正是我开发Cosmos过程的顺序，只有这样，才能把我的经验原样分享给你们。
因为我就是从Hello World应用程序开始，探索计算机是如何运行一个应用程序的，进而一步步了解了操作系统内核中的所有组件，在心中建立了一个现代操作系统内核的模型。
因为操作系统内核必须要运行在具体的计算平台上，所以我研读了大量的芯片手册，并且着重了解了其中CPU和内存的细节。接着，我又学习了编译工具集。有了这些基础，我开始写引导器和初始化代码，逐步实现了内存管理、进程调度、设备I/O、文件系统、网络和若干设备驱动程序，最后实现了系统调用和应用程序库。
虽然这些组件比成熟的操作系统内核中的组件简单得多，但实现的都是最关键、最核心、最必要的功能机制，简小而全面一直是我的思想，而这也正是我们这些操作系统初学者想要的。
我们的专栏虽然结束了，但是我们的Cosmos才刚刚开始。不知道你是否也在思考，我们亲自建造的Cosmos，为什么没有强大的文件系统和网络组件，为什么没有精美且高性能的图形界面，为什么没有工业级的安全性？
如果你真的在思考、在好奇，如果你真的有兴趣，还想继续探索，我真诚地希望你能再次阅读更多的书籍，或者借助万能的互联网，去搜寻资料，去寻找答案。
相信以这份好奇和兴趣为动力，必定会从一无所知，到知道一点点，再到知道一部分，慢慢地积累，也许有一天你会惊奇地跳起来，用尽全身力气喊出来：“原来我也能了”，“我真的能了”！
到了那一天，想必我们也已经有了全新的开始，那一定将是真正具备创造性的开始。
如果你愿意，也可以加入我们的Cosmos开源社区，让我们再续前缘，一起开发Cosmos操作系统，让我们一起开始创造性的工作。
Cosmos开源社区以Cosmos的“Ψ(Psi)”架构为基础进行展开，Ψ(Psi)内核架构有别于微内核、宏内核、混合内核，它吸收了其它内核架构的优势，完全摒弃了其它内核架构的劣势，这就导致了现有的硬件架构体系，不适应运行这样的Cosmos。
为此，我们将以RISCV处理器为基础进行扩展，形成“Ψ(Psi)”架构的RISCV处理器，这个处理器将成为运行Cosmos特有的处理器，同时这个“Ψ(Psi)”架构的RISCV处理器也会开源，形成硬件、软件双开源的方式，欢迎各方勇士加入，一起迎接挑战，一起开创IT新纪元。
也许有一天，人们会用着我们建造的操作系统，在我们自己设计的计算机上，听着杜比级别的音乐、看着4K画质的高清电影、玩着如梦如幻的3D游戏、和远方的恋人进行视频通话、进行超大规模的科学计算、处理着海量级的网络数据……
但是别忘了，这仅仅是因为我们最初那一点点求知欲和兴趣……
虽然暂时需要告别，但我期待后会有期。感谢3个多月的同行，真心希望我的专栏对你有所帮助。
我知道，很多同学总是默默潜水，默默学习。所以在专栏即将结束的今天，我希望听听你学习这个专栏的感受。这里我为你准备了一份毕业问卷，题目不多，希望你可以花两分钟填一下。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>结束语_用程序语言，推动这个世界的演化</title><link>https://artisanbox.github.io/6/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/42/</guid><description>据说，第二次世界大战期间，图灵和同事破译的情报，在盟军诺曼底登陆等重大军事行动中发挥了重要作用。历史学家认为，他让二战提早了2年结束，至少拯救了2000万人的生命。也据说，苹果公司的Logo就是用来纪念图灵的。
图灵的故事我不再赘述，你上网随便搜个关键词都能找到。不过，通过这个故事，我们能得到两点启示：
对信息的处理能力至关重要，从此信息技术成为了科技进步的主角，一直到现在。 科技永远关乎人性，科技是客观的，而推动科技发展的人，是有温度、有故事的。 所以，在《编译原理之美》这个课程结束的今天，除了想跟你好好地说声再见之外，我更多地是想分享作为一个程序员，我们的挣扎、骄傲，以及跟这个社会的关系，跟时代洪流的关系。我有一些感受分享一下。
学习技术的过程，是跟大师对话的过程，是融入科技发展这条历史河流的过程，是一个有温度的心路历程。
有同学在留言区说，这门课，串联了计算机领域的很多基础课程。的确如他所说，当然，我也认为编译原理这门课，串联着整个计算机发展的历史，以及做出重要贡献的一代代大师。
什么是大师？这么说吧。比如你针对某方面的问题琢磨了很多年，有所心得。刚想进一步梳理头绪，就发现有人在多年前，已经针对这方面的问题发表了一个理论，并且论述得很完整，很严密。这个人，就可以叫做大师。
我的一个朋友，某上市公司的副总，原来是在大学教物理的，闲暇时间还会琢磨物理学的理论。有时候，他在琢磨一个点的时候，觉得很有心得，刚想整理出来，再一查文献，发现某个人已经在这个方向发表了成果。他形象地比喻说，刚想写《红楼梦》呢，发现一个叫曹雪芹的已经写了。
计算机领域也有很多大师。我们在学编译原理的时候，其实一直在跟各位大师邂逅。
比如，当讨论有限自动机的时候，你知道那是一个最简单的图灵机（Turling Machine）。你再去阅读这方面的资料，会发现图灵那时在思考什么是计算，这种根本性的问题。
当我们探讨到程序运行环境、汇编语言、机器语言的时候，你会感觉似乎跟冯·诺依曼（John Von Neumann）走近了。你会感受到第一代程序员，用机器码写程序是什么感觉。
第一代程序员的人数只有个位数，他们甚至当时都没有考虑到，还可以用别的方式写程序。所以，当冯·诺依曼的一个学生发明汇编的写法时，这位老师甚至觉得那不叫写程序。
而只有你自己动手写了汇编代码，你才能体会到，第二代程序员是怎样写程序的，其中包括比尔·盖茨（Bill Gates）。显然，比尔·盖茨认为普通程序员应该用更简单的语言，于是他写了一个Basic语言的解释器。其他熟练使用汇编语言的程序员，还包括为阿波罗登月计划，编写程序的传奇女程序员，玛格丽特·希菲尔德·汉密尔顿（Margaret Heafield Hamilton）。以及中国的雷军等等。题外话，我看过一个图表，早期程序员中，女性的比例很高，希望未来更多的女性回归这个行业。
接下来，你会遇到C语言的发明人丹尼斯·里奇（Dennis Ritchie）， 他的工作是基于肯.汤普森（Ken Thompson）的B语言。这俩人还是Unix操作系统的发明者。目前，肯.汤普森仍在Go语言项目组中工作。
我们使用的Java、JavaScript、Go语言等的语法风格，都是一路受到C语言的影响。我们做编译器的时候，要考虑调用约定、二进制接口，也能从这里找到源头。
在前端部分，我们讨论过面向对象的语义特征，和类型系统。而面向对象的编程思想，在60年代就被提出了，经由80年代的C++和90年代的Java才开始盛行。
我们同样简单实现过一等公民的函数和高阶函数，它们是函数式编程的特征。最近几年函数式编程的思想开始热起来，但它的起源更早，可以追溯到30年代阿隆佐·邱奇（Alonzo Church）提出的Lambda演算理论中。
邱奇用一种与图灵不同的方式，探讨了什么叫做计算，这个根本问题。他的思想于50年代体现在Lisp语言上。Lisp的发明人是人工智能的先驱约翰·麦卡锡（John McCarthy），这门语言成了计算机语言一些重要基因的来源，JavaScript、Ruby、Clojure、Scala、Julia等语言都从中汲取营养。我最近在研究云计算环境下的分布式数据库问题，发现可能还是要借鉴函数式编程的思想。
再有，编译原理中的属性语法和很多算法，不能不提高德纳（Donald Ervin Knuth）的贡献。他的著作应该成为你的必读。
当我们讨论Java的一些特征时，你可以试着体会Java语言之父詹姆斯·高斯林（James Gosling）当初设计字节码时在想什么。你还可以体会一下 布兰登·艾奇（Brendan Eich） 用很短的时间发明JavaScript时，是汲取了前人的哪些思想，以及是如何做出那些重要的决定的，这些决定使得JavaScript在元编程能力、函数式编程等方面，直到现在都焕发出勃勃生机。
当你学会编译原理的一个个知识点的时候，就是一步步走近大师们的过程。他们的名字不再是教科书上抽象的符号，你已经能够逐渐欣赏他们的思想，感受到他们的感受，和他们隔着时空交流。而当你凭着自己的经验，探索到了跟他们相同的方向上，你会更觉得有成就感，会觉得自己真正融入了科技演化的洪流中，算是开了窍了，算是其中的一份子了。
我想，真正在科技领域做出重大成绩的人，都会有这样一种，摸到了科技发展脉搏的感觉。据说，张小龙曾经说过，读懂了《失控》这本书的人，可以直接去他的团队上班。我猜，他对复杂系统科学情有独钟，产生了很多的心得。而任正非先生则对热力学中熵的理论感触很深，并把它深刻地融入到了华为的价值观和管理体系中。
除此之外，我们还要感谢Antlr工具的作者特恩斯·帕尔（Terence Parr）以及LLVM的核心作者 克里斯·拉特纳（Chris Lattner） 。通过阅读他们的文章和代码，以及其他研究者的论文，你会感受到这个领域最前沿的脉搏。
而通过编译原理中的一些应用课程，我们还可以更好地理解Spring等工具的设计者的思维。并且思考，是否自己也有能力驾驭这样的项目，从而成为技术进步洪流中的博浪者。
我相信，如果你不想学习编译原理，可以轻松找到一百个理由。比如：
这个课程太难，我恐怕学不会； 这个课程跟我现在的工作关系不大； 我没有时间； 连谁谁谁都没有学，我就不凑这个热闹了； … 但如果你想下定决心学会它的话，只要有一个理由就行了，那就是，你也可以成为技术进步洪流中的博浪者，而不是岸边的旁观者。这时，你的自我意识会觉醒：我来了，我要参与。在信息技术成为社会进步关键推动力的今天，这是作为一名程序员的傲骨。
更为重要的是，越来越多的中国程序员已经登上了舞台。越来越多高质量的开源项目，背后是一个个中国名字。我查阅自动化编程这个最前沿领域的文献时，发现文献上也不乏中国名字！
整个世界的目光也开始投向中国，因为他们越来越相信中国的创新能力。我们也确实有能力，因为我们已经有了云计算、人工智能和5G技术的积淀，我们正在芯片领域奋起直追，完全自主的操作系统已经开始萌芽。而在这些领域，编译技术都能大展身手。最重要的是，中国作为全球最大的市场之一，拥有最丰富的应用场景，也拥有越来越相信中国创新能力的消费者。
我相信，学习这门课的学员中，不管是大学生，还是已经很有工作经验的大侠，会有相当一批人，在下一个10年，使用编译技术做出一番成绩。
对我来说，我很高兴有机会专心致志地梳理编译原理相关的知识体系。而在梳理到每个知识点的时候，我都会迸发出很多灵感。这些灵感将会融入到我正在开发的一个软件和后续的工作中。
在这个过程中，我还有一个额外的收获，就是感觉自己的写作水平和普通话水平都提高了。原因很简单：因为每篇文稿都要改好几遍，录音有时也要录几遍。而把陡峭的学习曲线，变成一个让你缓缓爬坡的过程，也促使我必须竭尽全力！
我也觉得用仅仅40讲左右的课程，涵盖整个编译原理的知识体系，恐怕会显得不足。虽然涵盖了主要的知识点和脉络，但我在进入每个技术点的时候，发现要把这个点完全展开，可能都需要好几讲才行。不过没关系，我和极客时间还有进一步的计划，你可以等待好消息！
总的来说，信息技术的进步史，也是一代代大师的人文故事史。而编译技术让我们有机会走近这些大师，与他们对话，并加入他们。中国的程序员面临着历史的机遇，而抓住机遇的关键，是自我意识的觉醒，是敢于成为科技进步历史洪流中的博浪者的决心。
希望与你共勉，一起进步！
最后，我为你准备了一份结课问卷，题目不多，两三分钟就可以完成。希望你能畅所欲言，把自己真实的学习感受和意见表达出来，我一定会认真看，期待你的反馈。当然，如果你对专栏内容还有什么问题，也欢迎你在留言区继续提问，我会持续回复你的留言，我们江湖再见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>结束语_知也无涯，愿你也享受发现的乐趣</title><link>https://artisanbox.github.io/4/58/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/58/</guid><description>你好，我是徐文浩。伴随着无数个不眠之夜，“深入浅出计算机组成原理”专栏终于来到了结束语。
去年11月份，极客时间找到我，我开始构思这个专栏。本以为今年4、5月份就能把专栏写完。结果，一方面因为创业过程中时间总是不够用，另一方面，写出有价值内容的并不是一件容易的事情，直到9月10号的凌晨，我才写完这最后一篇结束语。原本计划的45讲，也在这个过程中变成了近60讲。现在回过去看，写这个“深入浅出计算机组成原理”专栏，是一个远比想象中要困难的挑战，但同时也是一个有趣的发现之旅。
完成比完美更好Facebook的文化里面喜欢用各种小标语，其中有一条我很喜欢：“Done is better than perfect”。翻译成中文就是，“完成比完美更好”。写这个专栏的时候，我对这一点的体会特别深刻。在学习更多深入知识的时候，我希望你也可以抱有这样的态度。
在初期构思专栏的时候，我期望写成一个完美的专栏。不过随着时间的推移，我发现其实并没有什么完美可言。
一方面，组成原理的知识点很多，如果每一个都写下来，没有个一两百讲怕是讲不完。更何况有那么多大师的教科书珠玉在前，只是做解读知识点、覆盖已有的知识点，我觉得价值不大。思来想去，我希望尽可能找到最重要、最核心的知识点，以及能和大多数工程师日常工作有结合的知识点，希望能够从应用中多给你一些启发。
另一方面，写专栏和我们写程序一样，都是有deadline的。无论是在系统发版之后的午夜里，还是去美国出差的飞机上，乃至偶尔忘带了录音笔的时候，总是要打起精神想尽方法，写出一篇让自己满意的文章来。同时，也有不少同学给我挑出了错漏或者不准确的部分，一起把这个专栏打磨地更“完美”。
不知道正在读结束语的你，有没有在过去5个月里坚持学习这个专栏呢？有没有认真阅读我每一节后的推荐阅读呢？有没有尝试去做一做每一讲后面的思考题呢？
如果你能够坚持下来，那首先要恭喜你，我相信能够学完的同学并不太多。如果你还没有学完，也不要紧，先跟着整个课程走一遍，有个大致印象。与其半途而费，不如先囫囵吞枣，硬着头皮看完再说。新的知识第一遍没有百分百看懂，而随着时间的推移，慢慢领悟成长了，这才是人生的常态。而我所见到的优秀的工程师大都会经历这样的成长过程。
我们这个行业，经常喜欢把软件开发和建筑放在一起类比，所以才会有经典的《设计模式》这样的书。甚至有不少人干脆从《建筑的永恒之道》里面去寻找灵感。然而，建筑能够在历史上留下长久的刻印，但是软件却完全不同。无论多么完美的代码都会不断迭代，就好像新陈代谢一样。几年过去之后，最初那些代码的踪影早已经没有了。软件工程师放弃了追求永恒，而是投身在创作的快乐之中。
希望在日后的学习过程中，你也能抱着“日拱一卒、不期速成”的心态坚持下去，不断地学习、反思、练习、再学习，这样的迭代才是最快的成长之路。
知也无涯，愿你享受发现的乐趣说实话，从构思到写作这个专栏，这整个过程对我来说，还是有些忐忑的。组成原理是一门离大部分工程师的日常工作比较远的话题，却又是一个很多经典教材会讲的主题。“到底从什么角度去切入讲解”，我在构思文章的时候常常问自己。
组成原理其实是一门类似于“计算机科学101”的课程，固然我可以在里面讲VHDL这样的硬件编程语言，不过说实话，这样的知识对于大部分的人意义并不大。我期望，能够通过这个专栏，让你体会到计算机科学知识是真的有用的，能够让你把学专栏的过程变成一个发现之旅。
比如，在学习HDD硬盘原理的时候，你能知道为什么用它来记录日志很好，但是拿来作为KV数据库就很糟糕；在学习CPU Cache的时候，你实际用代码体会一下它有多快，为什么Disruptor里面的缓存行填充这样的小技巧，能够把性能发挥到极致。
除此之外，撰写整个专栏的过程，也是我对自己的一个发现之旅。
虽然在过去开发大型系统的时候，已经体会到掌握各种计算机科学基础知识的重要性，但是，这个专栏还是给了我一个系统性地、对基础知识回顾和整理的机会，在忙碌的日常工作之外，在离开学校那么多年后，重新把基础的理论知识和实际的系统开发做了一一印证。
在这个过程中，对我自己是一个温故而知新的过程，我自己新学到不少过去不了解的知识点，也因此重新找到了很多新的技术兴奋点。乃至在专栏写了一半的时候，我特地在出差的空隙跑了一趟计算机历史博物馆，去感受创造新事物的那种激动人心的感觉。
不过，在这整个过程中，我也深深体会到了内容创作的难。
过去这10个月里，持续地写稿、画图、写实验程序，在编辑的反馈下再改稿和录音，对我也是一个全新的体验。没有思路、时间不够、工作和写稿压力太大的时候，抓狂、发脾气、骂人都发生过。如果没有编辑在背后一直督促着，只靠自律，我想我无论如何也不可能写完这样一个规模的专栏。
但是，我相信只有不断地逼迫自己走出习惯的舒适区，去尝试、体验新的挑战，才会进一步的成长。而很多未来的机会，也孕育在其间。就像史蒂夫·乔布斯说的，我们未来生活的可能性就是靠这些点点滴滴串联起来的。
也许你今天只是在学校写简单的课程管理系统，可能会觉得有些无聊。抽一些时间出来，去了解计算机科学的底层知识，可能会让你找到求知的乐趣，无形中，这也为你去解决更有挑战的问题做好了铺垫。就像我自己在过去研究底层的数据系统、写技术博客的时候，也没有想到会有机会写上这样一个20万字以上的专栏。
就像罗素说的那样，“对爱的渴望，对知识的追求，对人类苦难不可遏制的同情，是支配我一生的单纯而强烈的三种情感”。
我希望，在学习成长的过程中，你能够摆脱一些功利性，不用去回避遇到的痛苦和挫败感，多从这个过程中找到获得知识的快乐。
希望这个专栏能够给你带来发现的乐趣，也能够为你在未来的生活里铺垫上那小小的一步。相信这个专栏不是你学习的终点，也也不是我探索和发现新主题的终点。说不定，在不久的未来我们还会有缘再见。
对了，我在文章末尾放了一个毕业调查问卷。在这5个月的学习过程中，如果你对这个专栏或者我本人有什么建议，可以通过这个问卷给我反馈，我一定会认真查看每一封的内容。期待你的反馈！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>结束语_送君千里，终须一别</title><link>https://artisanbox.github.io/2/76/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/76/</guid><description>专栏到今天真的要结束了。在写这篇结束语的时候，我的心情还是蛮复杂的，既有点如释重负，又有点不舍。如释重负，是因为我自己对专栏的整体质量非常满意；不舍，是因为我还想分享更多“压箱底”的东西给你。
专栏是在2018年9月发布的。在发布后的两三天时间里，就有2万多人订阅，同时也引来了很多争议。有人说，我就是随便拿个目录就来“割韭菜”。也有人说，数据结构和算法的书籍那么多，国外还有那么多动画、视频教程，为什么要来学我的专栏？
这些质疑我都非常理解，毕竟大部分基础学科的教材，的确是国外的更全面。实际上，在专栏构思初期，我就意识到了这一点。不夸张地讲，我几乎读过市面上所有有关数据结构和算法的书籍，所以，我也深知市面上的数据结构和算法书籍存在的问题。
尽管有很多书籍讲得通俗易懂，也有很多书籍全面、经典，但是大部分都偏理论，书中的例子也大多脱离真实的软件开发。这些书籍毫无疑问是有用的，但是看完书之后，很多人只是死记硬背了一些知识点而已。这样填鸭式的学习，对于锻炼思维、开拓眼界并没有太多作用。而且，从基础理论到应用实践，有一个非常大的鸿沟要跨越，这是大学教育的普遍不足之处，这也是为什么我们常常觉得大学里学过的很多知识都没用。
我本人是一个追求完美、极致的人，凡事都想做到最好，都想争第一。所以，就我个人而言，我也不允许自己写一个“太普通”“烂大街”的专栏。那时我就给自己立了一个flag：我一定要写一个跟所有国内、国外经典书籍都不一样的专栏，写出一个可以长期影响一些人的专栏。
所以，在这个专栏写作过程中，我力争并非只是单纯地把某个知识点讲清楚，而是结合自己的理解、实践和经验来讲解。我写每篇文章的时候，几乎都是从由来讲起，做到让你知其然、知其所以然，并且列举大量的实际软件开发中的场景，给你展示如何利用数据结构和算法解决真实的问题。
除此之外，课后思考题我也不拿一些现成的LeetCode的题目来应付。这些题目都是我精心设计的、贴合具体实践、非常考验逻辑思维的问题。毫不夸张地讲，只把这些课后思考题做个解答，就可以写成一个有价值、有干货的专栏！
专栏到今天就要结束了。尽管有些内容稍有瑕疵，但我觉得我实现了最初给自己立下的flag。那你又学得怎么样呢？
如果这是你第一次接触数据结构和算法，只是跟着学一遍，你可能不会完全理解所有的内容。关于这个专栏，我从来也不想标榜，我的专栏是易懂到地铁里听听就可以的。因为你要知道，没有难度的学习，也就没有收获。所以，作为初学者，你要想真的拿下数据结构和算法，时间允许的话，建议你再二刷、三刷。
如果你是有一定基础的小伙伴，希望你能够真的做到学以致用。在开发项目、阅读开源代码、理解中间件架构设计方面，多结合数据结构和算法，从本质上理解原理，掌握创新的源头。
如果你是数据结构和算法高手，那我的专栏应该也没有让你失望吧？我个人觉得，专栏里还是有很多可以给你惊喜的地方。对于你来说，哪怕只学到了一个之前没有接触的知识点，我觉得其实已经值得了。
送君千里终须一别。数据结构和算法的学习，我暂时只能陪你到这里了。感谢你订阅我的专栏，感谢这5个月的同行，真心希望我的专栏能对你有所帮助。
我知道，很多小伙伴都是“潜水党”，喜欢默默地学习，在专栏要结束的今天，我希望能听到你的声音，希望听听你学习这个专栏的感受和收获。最后，再次感谢！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 .</description></item><item><title>结束语｜等待你大展身手的那些领域</title><link>https://artisanbox.github.io/3/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/45/</guid><description>你好，我是宫文学。
到今天为止，我们这门课的主要内容就都更新完了。不过，还有一些补充性的内容，我会通过加餐和开源项目的方式，继续和你保持沟通。
今天的结束语，我想跟你探讨一下，学习实现一门语言的相关技术，到底会有什么用途。
我会分成领域编程语言、平台级的软件和通用编程语言这三个话题，分析一下 编程语言技术能帮助你抓住哪些机会，让你有机会从普通的程序员进阶成大神级的程序员，并创造出一些卓越的产品。
首先，我们来谈谈领域编程语言这个话题。
领域编程语言（DSL）对于我们大部分同学来说，其实很难有机会，或者也没有这个意愿，去参与实现一门通用性编程语言。不过，其实在大部分情况下，我们也没有必要追求那么大的目标。有时候，针对我们所在的领域，实现一门领域编程语言，就是很有意义、很有成就感的事情。
我举几个我遇到的DSL的例子，看看能否抛转引玉，让你找到更多可以设计和使用DSL的场景。
MiniZinc：最优化领域的开发工具在2020年的12月，我曾经研究了一下最优化算法相关的技术和工具，看看它能否用于我们的一个产品。
很多同学在大学都学过最优化相关的理论，像线性规划、非线性规划这些，都属于这个领域。你也可能听说过运筹学，它们的意思差不多。最优化理论在实践中有很多用途。比如，我要解决一个应用问题，就是在某个领域，有很多员工，也有很多任务要完成。每个员工的技能是不同的，我需要通过算法来安排这些员工的工作，取得整体最优的效果。
为了实现最优化求解，有人开发了各种求解器，有商业的，也有开源的。但对于我一个新手来说，我一开始并不知道要用哪个工具，有点茫然。
通过某些途径，我了解到了MiniZinc这个工具。这个工具提供了一种DSL，能够描述各种最优化问题，然后调用各种不同的求解器来求解。比如在下图，你能看到菜单栏有一个下拉菜单，里面有多个求解器。
MiniZinc这个工具一下子解决了我的两个需求。首先，这个DSL很友好、很直观。你完全可以按照最优化的理论，描述一个问题的变量、参数、约束条件，然后就可以求解了，非常方便。第二，我暂时也不用关心不同的求解器的差别，可以随便选一个先用着，或者换着用不同的求解器，看看它们在性能和求解结果有哪些差异。
所以我很快就用MiniZinc编写了几个小程序来验证我的想法，并在较短的时间内取得了一些成果。
在使用这个工具的时候，我就在想，上过我这门课的同学，有没有能力做这么一个工具呢？我们来分析一下。
其实，要实现MiniZinc，主要的工作就是实现一个编译器的前端，做词法分析、语法分析和语义分析工作。这个DSL的语法和语义都不是很复杂，所以工作量并不大。
做完前端的工作以后，程序就可以基于AST来解释执行了。解释执行的过程，其实就是调用各个求解器的API，并把结果显示到界面上。
这么一个小工具，会给那些最优化领域的工作者和科研人员带来很大的便利。因为他们通常专注于研究算法，对于通用的计算机编程并不是很熟练。
那在你的领域中，是不是也有这样的情况呢？你是IT专业的人员，而你同事可能是其他专业的专家，比如是工程专家、投资专家、财务专家等等。你能否针对他们的领域，设计出一些DSL，并提供一个开发工具来解决他们的一些痛点问题呢？
我们再看看第二个例子，这个例子是遥感领域的一个二次开发平台。
遥感领域的二次开发平台我硕士的专业是在遥感和GIS领域，我有一个硕士同学在这个领域做出了一个上市公司。他们有一个产品，是一个遥感云平台，也就是把全国各地的很多遥感资料都放到云上管理。这个云平台里有一个开发工具，能够让用户在浏览器里编写程序，调用云平台的API，实现遥感数据分析等功能。
据说这个平台是跟Google Earth对标的，国外很多科学家都会在Google Earth平台上写程序处理遥感数据，并构建自己领域的应用。
如果让你去实现这个开发工具，你会怎么做呢？
首先，你肯定需要一个基于Web的代码编辑器，这方面有好几个开源工具，所以这点并不是难题。
接下来，你仍然要实现编译器前端的工作。这一次，你需要编译的是JavaScript语言，它的语法特性和语义特性都比较多，所以实现的工作量要大一些。你可以把我们现在的词法分析器和语法分析器改一改，来实现JavaScript的解析。如果你想偷懒，还可以直接用antlr和现成的语法规则生成一下解析器。不过，无论如何，语义分析的工作是省不了的。你需要建立符号表、进行符号的消解，但不需要像我们这门课这样做那么多的类型处理。
完成编译器前端以后，还要做些什么呢？我们还要做一些中端的优化工作。因为这个开发工具要调用与遥感有关的API。而如何调用这些API才会让效率最高呢？所以这一点上，我们实际上是要做一些优化的。
做完中端优化后，后续的编译和运行过程，有可能只要交给一个成熟的JavaScript引擎就可以了。
怎么样？使用我们这门课上学过的技术，你可以把很多科研工作者、各个行业的应用开发人员，都聚集到一个平台上，充分释放海量遥感数据的价值，这是不是一件挺酷的事情？
接下来，我再举一个CAD领域的例子，这个产品是OpenCAD。
OpenCAD几个月前，我一时兴起买了一台3D打印机，想自己打印点好玩的东西。但是在打印之前，还需要建立3D模型，所以我就搜了搜建模工具，发现了一个叫做OpenCAD的软件。
这个软件提供了一个编程界面。你可以在这个界面中，通过编程来创建长方体、圆柱体这样的三维对象，也可以通过编程来控制它们的位置、旋转的角度。
另外，你还可以创建模块，把多个基础的对象拼成复杂的对象，比如把一些长方体、圆柱体拼成一辆车。之后，这辆车就可以作为一个整体，用来构架更复杂的场景。
并且，模块还可以带参数，就像一个函数或者一个类那样。你通过调整模块的参数，就可以调整生成的3D对象。比如同样是一辆车，你可以通过调整参数生成一辆很大的车，也可以生成一辆玩具车。
我并不熟悉CAD领域，所以很难把这个软件跟其他CAD软件做客观地对比。不过，我能够看出，OpenCAD通过编程来建模的形式，有几个特别的优势：
第一个优势是精准。在建立某些机械模型的时候，零件的大小、位置等信息必须是精准的。而使用编程语言，你可以用数学公式做出各种精准地计算。比如，上面的例子就使用了三角函数来精确的绘制曲线。
第二个优势是可重用性。可重用性是编程语言的基本特征，模块、函数、类，都是可重用的元素。并且，这种可重用的元素都是参数化的。在不同的场景中，可以通过调整参数来获得想要的功能。
所以，要开发一款优秀的CAD软件，我们需要充分吸收编程语言的技术。类似的领域还有建筑建模软件（BIM）、城市建模软件（CIM）等，现在很多公司的产品，都声称提供了低代码的数字城市平台，那这些产品也应该充分使用计算机语言的技术，并形成特定领域的DSL。
你看，我们现在分析了三个案例，看到了三个领域对计算机语言的要求。其实，这样的领域还有很多，并且它们都可以受益于领域编程语言。我希望你能受到这些例子的启发，看看在自己的领域内还有没有这样的需求，说不定你会也能做出一些开创性的事情。因为这样级别的工作，必须在掌握了我们这门课的知识体系以后，你才有能力驾驭。
上面这些采用了DSL的软件，基本都属于一些平台级的软件。那我们再围绕如何实现平台级的软件这个话题，展开讨论一下编程语言技术的作用，和你当前面对的机会。
实现平台级的软件Lisp语言的重要推广者、《黑客与画家》的作者、创业孵化器Y Combinator的创始人保罗·格雷厄姆曾经说过一段话，大概就是说，每个软件演化到最后，都会内置一个Lisp语言的实现。
他的这段话，其实是说，任何软件，如果想覆盖尽量多的应用场景，都需要提供一定的编程能力。我举几个例子来说明一下这个观点：
为了更高效的管理很多服务器上的操作系统，搞运维的技术人员都会用Python来写脚本。这个时候，我们是在用脚本语言来扩展操作系统的功能； 数据库系统之所以能够满足各种应用的需求，是因为它是通过SQL语言来访问数据库的功能； 微软的各种应用产品，几乎都提供了二次开发的能力，这让你可以基于微软的产品形成各种不同领域的解决方案。比如，如果你能够熟练使用Excel里面的宏和编程技术，你可以完成很多的数据分析需求； 三维游戏引擎提供编程功能，让你能够创建各种三维游戏场景； 各种报表或BI工具，都提供了定义数据源、定义报表公式等功能，方便你设计各种报表； 工作流或BPM系统，需要提供流程设计、公式定义和自定义逻辑的功能，来满足各种不同的流程场景了； 对于一个API网关来说，需要提供一定的编程逻辑，来定义在什么情况下，把API访问路由到哪个微服务，或者进行熔断。 类似的例子还有很多。
如果你的软件只是为某个用户个性化定制的，只需要满足这一个客户的需求，那么你只要弄清楚需求，然后实现出来就行了。但如果你想让更多的用户使用你的软件，那该怎么办呢？
在少量情况下，你可以设计一套标准的软件，并让所有的用户都满意。比如，几乎所有的字处理软件的功能都差不多，你只需要购买一个License就行了，很少会提出个性化的需求。
但这样的通用软件是很少的。更多的情况下，特别是在企业应用领域，我们都需要对软件的标准功能做一定的调整，让它符合某个客户特定的需求。比如，你可能需要调整某些业务规则、某些流程、某些数据项，等等。
而国内大部分软件公司，目前都是通过修改源代码来满足这些个性化的需求。这就导致软件的实施成本很高，版本难以维护，这是很多应用软件开发商所处的困境。你在的公司，也可能存在这样的困境。
这个时候，如果你能把应用软件上升为平台，也就是在不修改原来的源代码的基础上，提供二次开发的能力，二次开发的部分由各个客户自行维护，才可以从根本上打破这种困境。从这个角度看，我们每个同学所在的领域，都存在着大量的潜在机会，采用这门课学习到的编程语言技术，你可以把你所在领域的软件，提升成一个平台级的软件。
还有最后一种情况，就是你本来就要开发平台级的软件，比如数据库系统、表单系统、报表系统、游戏引擎等，那么编程能力就更加是缺省的要求。
不管怎样，只要你想让你的软件变成平台级的软件，具备适应各种不同应用场景的能力，具备扩展功能的能力，你就需要采用这门课教给你的编程语言技术，对这些软件进行改造。在我看来，国内有太多软件产品需要进行这种提升了，这都是你可以施展身手的机会。
好了，聊完了领域编程语言和平台级软件以后，你还有没有其他机会来大显身手呢？有的，这就是实现通用编程语言这个终极大Boss。
通用编程语言像Java、C、Go这些语言，能够用于很多领域，所以它们被叫做通用编程语言。到目前为止，我们国家还没有正式发布的、被广泛接受的通用编程语言。
不过信息灵通的同学可能也知道，我们多个大厂，其实都在内部酝酿和研发这样的语言。我本人也在参与某门语言的内测和评价工作。鉴于保密协议的约束，在这门语言没有正式发布前，我是不能谈论它的名称和技术细节的。
所以，如果你对实现通用编程语言很感兴趣的话，其实现在就有机会进入这些团队，贡献自己的一份力量。
如果你参与了这样的项目团队，那么你就可以选定一个具体的领域，深入研究我们这门课涉及的那些知识点。有人可能变成语法分析的专家，有人可能成为优化技术的专家，有人可能成为后端技术的专家，还有人可能会成为运行时方面的专家。
而我相信，正是你们这些未来的专家，将来必然会让中国的通用编程语言领域大放异彩！
写在最后这门课程的主体内容，到这里就正式结束了，不过我还会跟同学们保持联系。保持联系的方式有几个，一是开源项目PlayScript，一是计划不定期发布的几篇加餐，还有就是这门课的微信群。
我会继续在编程语言的领域探索和实践，希望能够跟你多多交流。也希望你在前进的道路上，能够找到更多的志同道合的朋友，一起砥砺前行，创造出优秀的作品！
另外，我还给你准备了一份毕业问卷，题目不多，希望你能在问卷里聊一聊你对这门课的看法。欢迎你点击下面的图片，用1～2分钟的时间填写一下。当然了，如果你对课程内容还有什么问题，也欢迎你在留言区或交流群继续提问，我会持续回复你的留言。</description></item><item><title>结课测试_编译原理的这些知识，你都掌握了吗？</title><link>https://artisanbox.github.io/6/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/6/41/</guid><description>你好，我是宫文学。
不知道你学完这些编译原理的相关知识以后，掌握得怎么样呢？
为了帮助你检测自己的学习成果，我特别准备了一套结课测试题。这套测试题共有3道单选题，17道多选题，满分100分。
题目中涉及到的知识点，我在这个系列课程中都讲过。
希望你能认真完成这次测试，如果你发现有些知识还没有掌握，可以回顾相应的内容，再去加深一下理解。也欢迎你在留言区与我互动交流。
好，点击下面的图片，开始测试吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>结课测试_这些网络协议你都掌握了吗？</title><link>https://artisanbox.github.io/5/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/44/</guid><description>经过三个多月的学习，相信你对网络协议的基础概念和使用场景有了更深入的了解。我从专栏中精心筛选了核心知识点，编成了这20道测试题。希望可以帮助你学习自检，消化吸收，以期获得更好的学习效果。
如果你刚刚打开这个专栏，可以用这20道题，找到自己的薄弱点，对症下药；如果你已经学习了一段时间，可以用这20道题，检测一下学习成果，查漏补缺。
还等什么，点击下面按钮开始测试吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>结课测试｜这些MySQL知识你都掌握了吗？</title><link>https://artisanbox.github.io/1/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/49/</guid><description>你好，我是林晓斌。
《MySQL实战45讲》这门课程已经全部结束了。我给你准备了一个结课小测试，来帮助你检验自己的学习效果。
这套测试题共有 20 道题目，包括3道单选题和17道多选题，满分 100 分，系统自动评分。
还等什么，点击下面按钮开始测试吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>结课测试｜这些操作系统的问题，你都掌握了么？</title><link>https://artisanbox.github.io/9/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/49/</guid><description>你好，我是LMOS。
《操作系统实战45讲》已经完结了。在这段时间里，我依然收到了很多用户的留言，很感谢你一直以来的认真学习和支持！
为了帮助你检验自己的学习效果，我特别给你准备了一套结课测试题。这套测试题共有 20道题目，包括 10 道单选题，10 道多选题，满分 100 分，系统会自动评分。点击下面按钮，马上开始测试吧！
最后，我很希望听听你学习这个专栏的感受。这里我为你准备了一份毕业问卷，题目不多，希望你可以花两分钟填一下。</description></item><item><title>结课测试｜这些数据结构与算法，你真的掌握了吗？</title><link>https://artisanbox.github.io/2/75/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/75/</guid><description>你好，我是王争。
《数据结构与算法之美》已经完结一段时间了。在这段时间里，我依然收到了很多用户的留言，很感谢你一直以来的认真学习和支持！
为了帮助你检验自己的学习效果，我特别给你准备了一套结课测试题。这套测试题共有 20 道题目，都是单选题，满分 100 分。
点击下面按钮，马上开始测试吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>编辑手记_升级认知，迭代自己的操作系统</title><link>https://artisanbox.github.io/9/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/47/</guid><description>你好，我是宇新，《操作系统实战45讲》的专栏编辑。
除了负责更新课程里的内容，我也一直关注着小伙伴们的留言。这次，终于有机会自己也留一回言了，很开心能用编辑手记的方式，和你聊一聊我的想法。
这门课的独特之处细心的小伙伴可能发现了，我们的开篇词标题是“为什么要学写一个操作系统？”注意，不只是学操作系统，而是学着去“写”一个操作系统。
你可能还会想，平时我们接触不到的“黑盒子”，现在却要我们自己写代码实现，听起来很有挑战啊？为什么会这样设计呢，且听我慢慢道来。
操作系统博大精深，甚至每个子模块单拿出来讲，都有无数的知识点，太容易只见树木不见森林。但用一个实战项目连起来的话，就能很好地帮助我们聚焦关键问题。
看似“写”操作系统，这是把难度升级了，其实是为了控制我们的作战范围。写操作系统的时候，涉及哪些关键要点，我们就相应地学习研究这部分内容。
现在成熟的操作系统，像是Linux系统，它的源码量级已是今非昔比，我们去看源代码总会晕头转向。但老师的课程像是一条线，把实战需要的东西都展示出来，想要深入研究的同学建议对照查漏补缺，然后继续跟着课程走，这样才能实现“螺旋式”进步。
如果你也喜欢玩游戏的话，估计有这样的体验，把游戏调成了无敌模式，很容易就会索然无味。没错，有挑战的游戏才好玩。有时候卡在某一处确实很痛苦，但是突破以后也会爽。你不妨把自己当作玩家，去攻克一个个操作系统的关卡。当然了，你也不是孤军奋战，遇到疑问，还可以通过学习、交流和讨论去解决。
课程的思路我就说到这里，如果你感兴趣，还可以看看我们的课程设计文档。
更多课程设计的缘起，也可以看看LMOS老师好友Yason Lee的解读：《大咖助场｜以无法为有法，以无限为有限》。
怎样学习这门课课程上线以后啊，LMOS老师跟我都在关注大家的留言反馈。
学习这门课的同学身份各异，从学生党到已经退休的朋友都有，但共同特点就是对操作系统充满热情，因为这样一个专栏而结缘。无论是在课程交流群，还是课程留言区里，这两个疑问算是高频出现的。
学习这门课，我需要什么前置知识？ 某个问题/知识点好难啊，我该怎么办？ 这里我就从编辑的视角说说我的看法吧。
先说第一个问题，需要什么基础。我一直在琢磨这个问题背后的含义。同学们的水平参差不齐，有畏难心理这很正常，你学习课程的时候，其实是明确了自己哪里“不会”，换个角度想，这样学习的时候不就能有的放矢了么？
不少同学担心自己不是科班出身，其实LMOS也不是科班出身的，这些历史问题还是翻篇更好，你过去怎么样，并不代表你之后不可以学习、研究操作系统。而且，就算是计算机相关专业的同学，可能学生时代上的操作系统课程也没留下特别深刻的印象，考完试就还给老师的，也大有人在。
现在还没有看完的同学也不要着急，因为更新的速度肯定要比你们的学习速度快上不少。你需要做的是按照课程顺序持续学习，慢慢来，遇到不懂的，就多看几篇，多看几遍。
课代表陈诚同学说过一句话，我记得特别深，他是这么说的：
“其实，我觉得我们想学写操作系统，有时候是为了一碟醋包了一顿饺子，但是最终饺子是自己的了。”
我注意到有不少小伙伴为了打牢基础，为了跟上课程，去补充了汇编、C语言，以及计算机组成原理方面的知识，我要给这些人点赞。
但是，就算你没有把那些图书从头看到尾，其实也同样可以跟着课程，循序渐进地学习。建议你边学边练，动手跑起来。哪怕最初你只能复制老师给的配套代码，但是只要肯用心，也会对操作系统有更深的理解。与其苦恼于自己基础不行，不如踏踏实实去学习精进。
为了让你明确每个模块的内容重点和难易程度，我为你整理了一张表格，你可以做个参考。
如果你还是想把操作系统的相关资料也都一并啃下来，那可以看看LMOS提供的参考书单，在学有余力的情况下拓展阅读。
1.关于编译工具：LD手册、GAS手册、GCC手册、nasm手册、make手册；
2.关于GRUB：GRUB手册；
3.关于CPU：Intel手册；
4.关于汇编：《汇编程序设计》；
5.关于C语言：《C语言程序设计现代方法》；
6.关于操作系统：《操作系统设计与实现》。
如果你想参考优秀课代表的学习经验与方法，可以参考后面这些用户故事。
1.零基础yiyang同学的课程实战经验；
2.优秀课代表pedro的技术学习方法；
3.技术发烧友spring Xu的课程学习思考；
4.安全产品研发leveryd的动态调试学习法；
5.课程优质笔记分享达人neohope的访谈加餐：技术学习与职业成长方法论。
下面我再说说第二个问题，当你具体学习的时候，觉得某个知识点很难，应该怎么办？对于这个问题我想给你分享三个小建议。
第一个建议就是做好心理建设。
就拿不少同学都觉得头疼的内存管理来说吧。其实当时我在看这部分稿件的时候，也觉得压力山大。记得当时LMOS老师还鼓励我说，挺过去就好了。现在你看到的内存章节，其中16～18讲原先是一整块的内容，我们经过讨论优化，考虑让大伙儿更容易跟上，才拆分细化成了三节课。
内存是内核的内核，肯定很难。不过就像英语单词不能永远背到“abandon”一样，想要深入地探索操作系统，这关必须迎难而上。
以第19课如何管理内存对象为例，不知道你看没看到置顶评论中“neohope”同学的学习笔记，建议重点关注一下他抓“关键”内容的能力。
古语说，不积跬步无以至千里。你可能会怀疑自己，但不必过度焦虑。如果咱们因为差距过大，而陷入弃疗状态，那就太可惜了。哪怕是“大佬”，也曾有萌新时期，基础不好就慢慢跟进。
第二个建议就是明确自己的需求，按需学习。
虽然没有什么“跳关”秘籍，但还是有些技巧让你快速掌握一节课内容的。没错，就像数据结构一样，每节课也有“内容结构”，想要快速消化，可以着重理一理后面这几点：
这个模块/这节课要解决什么问题（What） 思路是什么/ 为什么要这么解决（Why） 具体如何用代码实现（How） 你还可以自己动手用流程图画一下（pedro同学推荐了此方法，你可以试试绘图工具 Graph-Easy），这样不容易迷路。
当然，如果你已经有不少的学习积累，或者目的不在于“全景浏览”和“扫盲”，而是想要更加深入，那你必然要花费更多苦工。操作系统是星辰大海，建议以你困惑的问题为导向，进行专项突破。
比如，第23节课 Slab 内存对象，来自课程交流 1 群的zzyj同学就分享了Slab作者写的参考文献，你不妨搭配使用。
我的第三个建议是，积极交流，在反馈和记录中激励自己。
虽然学习方法重要，但我们也不要沉迷于把时间消耗在“找方法”上。很可能“优质方法”给你节省的时间，还赶不上你在找方法这件事上花掉的时间。
一人计短，众人计长，我们课程开设留言区，在部落开话题（推荐你在话题下分享自己的学习收获，晒一晒实战截图），搭建用户交流群，就是为了让你的学习之旅不再孤单，让我们在分享交流中一同进步。
除了多交流，我也强烈推荐你学习留痕，把你的阶段性学习成果、经验记录下来，这些都能激励自己坚持学习。都说闻道有先后，术业有专攻。百科全书式的人毕竟是少数，但爱学习的小伙伴总会遇到志同道合的朋友。
今天的提问者，也许明天就有能力给别人解答问题了，这就是教学相长。我们的助教 Jason提到：
“教别人是个沟通的过程，各种感官都会调度起来。调度越多，大脑参与理解记忆的部分就越多，以后回忆起来，搜索路径就越多。光看的话，只是眼睛。这跟实践出真知，道理类似。”</description></item><item><title>课前热身｜开始学习之前我们要准备什么？</title><link>https://artisanbox.github.io/3/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/47/</guid><description>你好！我是宫文学，欢迎来到《手把手带你写一门编程语言》的课程。
其实，你从课程题目就可以看出，我们这个课强调动手实践。所以在这一节课，我要给你介绍一下我们这个课程示例代码所采用的计算机语言，以及相关编程环境的搭建。这样，会方便你阅读、运行和修改课程的示例代码。
对于课程里用到的汇编语言、编译原理知识，如果你之前没有相关的经验，也不要担心。我会介绍一下我们这方面的设计思路，保证你通过这个课程会更快、更扎实地掌握它们。
通过这篇导读，你会对课程里用到的语言、工具、技术心里有数，以便更好地开启你的学习之旅。
好，我们先从使用的计算机语言和环境说起。
怎么快速上手TypeScript语言我们这个课程的目标呢，是要实现TypeScript的编译器和各种运行时。既然如此，那么我就尽可能地用TypeScript来实现这个目标。
虽然我们这个课程主体的代码都是用TypeScript写的，但我正式使用TypeScript其实是从2021年5月份开始，也就是我开始准备这个课的时间。
我知道你肯定会问：用几个月的时间，既要了解TypeScript，又要用TypeScript写自己的编译器，是不是太不靠谱了？当然，你可能也是因为要学习这门课程，第一次使用TypeScript，所以我就分享一下自己的一些经验。
第一，使用它！
我一直觉得，真正的语言学习开始于你使用它的那一刻。否则，你就是一直看这门语言的资料，也只能留下个大概印象，而且很快就会忘掉，只有动手使用，才会形成肌肉记忆。比如，现在我一写for循环，手指不自觉地打出“for (let i = …)”或"for (let x of …)"开头，这就是形成肌肉记忆了。
第二，看资料！
说实在的，现在学习计算机语言实在是太方便了，各种资料应有尽有，又有很多热心同学在网上的分享，遇到什么需求一查就有，用过才会记住。
如果你让我推荐一本学习资料，我比较推荐流浪小猫写的开源电子书《TypeScript入门教程》。这个作者可能比较懂我想看什么，提供的内容会到点上。比如，我们做面向对象编程的时候，都关心该语言是否具备运行时的类型判断能力，因为这个功能几乎百分之百会被用到，而这个教程里就专门有类型断言的章节。
第三，靠经验直觉！
其实，很多同学都学过多门语言，那么学一门新语言的速度就会很快。有经验的程序员会建立一些直觉，能够猜到一门语言可能具备什么特性。
在准备课程代码的时候，我有一次要编写一个类，代表汇编指令中的寄存器操作数。而在x86的汇编指令中，不同位数的寄存器的名称是不一样的，所以，我需要用一个成员变量bits，来表示这个寄存器的位数。这个时候，我想当然地写出了下面的代码：
class Register extends Oprand{ bits:32|64 = 32; //寄存器的位数 ... } 也就是位数只能取两个值，要么是32，要么是64，赋其他值给它都是错误的。
在使用这个写法的时候，我其实不是很确定是否可以用两个值的集合来描述变量的类型，因为在教程里提到联合类型（Union Types）的时候，只是说了可以用两个类型的联合。我直觉上觉得也应该支持两个值的联合，因为如果我是TypeScript的作者，我可能不会忽视这种使用场景。我根据自己的猜测试了一下，然后就成功了。
如果上升到类型理论的高度，那么我们可以说，类型本来就是可以取的值的集合。但如果我们不上升到理论高度，仅凭直觉，其实也能去正确地使用类型。
说到老程序员的直觉，其实这门课程的一个重要目标，就是想帮你建立起更多的直觉，建立仅仅通过高级语言的语法表象就能看透其内部实现机制的能力，让计算机语言在你面前成为一个白盒子，从而让你能够更加自如地去支配不同的语言来为自己服务。
好了，了解了怎么上手以后，我们再来看看在这门课程中的TypeScript的环境配置问题。
TypeScript的环境配置我们这个课程关于TypeScript的环境配置主要包括这些：
1.编译和运行环境：Node.js。
首先，我使用Node.js来编译和运行TypeScript，所以你要先在自己的电脑上安装Node.js，配置好相应的环境。这方面的资料很多，我就不提供链接了。
2.安装和配置TypeScript。
使用下面的命令，可以安装TypeScript：
npm install typescript -g 之后，你可以用git命令下载示例代码：
git clone https://gitee.com/richard-gong/craft-a-language.git 在用git下载了示例代码以后，需要你在示例代码的目录中运行下面这个命令，安装示例程序依赖的node.js中的一些包。安装完毕以后，会在craft-a-language目录中建立一个node_modules子目录：
npm i --save-dev @types/node 3.IDE：Visual Studio Code（简称VS Code）。
我在课程里使用了VS Code作为IDE，VS Code缺省就支持TypeScript语言，毕竟这个IDE本身就是用TypeScript编写的。
而且，我们每一节课的代码，都会被放在我们代码库里的一个单独的目录下，比如01、02……在每个目录下，都会有几个json文件，是TypeScript的工程配置文件。你只要在目录下输入tsc，就会编译该工程的所有.ts文件。
打开tscongfig.json，你会看到我提供的一些配置项：
target项是编译目标，这里我用的是es6，因为es6具备了很多高级特性。 module项是模块管理工具，我们选用的是CommonJS。 另外，有“exclude”选项，是排除了一些.</description></item><item><title>课程迭代｜全新交付71讲音频</title><link>https://artisanbox.github.io/2/78/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/78/</guid><description>你好，我是极客时间专栏主编李佳。今天是2021年1月8日，距离《数据结构与算法之美》课程结课快2年的时间，我代表极客时间教研团队，在此向你汇报一下这门课程的情况和迭代规划。
2020年12月7日，咱们这门课程的订阅超过了十万，这也就意味着我们成为了极客时间上最大的一个班集体。十万多位同学一起死磕数据结构和算法，这件事想想都振奋人心。
在我们的课程里，我们的文章有45,443次收藏，有438,273处划线，36,684条笔记，22,076条留言。这些学习数据都是我们这个班集体一起学习、一起进步的见证。
特别地，在课程留言里，你不仅分享了自己的学习收获、心得与经验，提出了自己的疑惑和问题，还指出了音频里的错误之处，真心感谢你的每一次批评指正。正是基于大家的反馈，我们决定重新交付课程音频，修改之前音频里的错误内容，同时，也在音频迭代的过程中，重新检查一遍文稿内容。这样，后面新加入的同学可以获得更好的学习体验，已经学完的同学也可以在复习的时候有不一样的感受。
这次音频迭代涉及课程里的71讲内容，我们会分2次全部替换完。音频替换计划如下：
1月8日，替换开篇词、01讲～30讲； 1月29日，替换31讲～56讲，以及加餐和结束语。 如果你还没有学完，或者是刚刚加入，那就跟着这个节奏，重新来学一遍吧。
希望这次全新迭代的音频，能带给你不一样的学习体验。也欢迎你继续提出问题、分享经验，我们一起学习、一起进步。
2021年刚刚开始，世界依然充满了种种不确定性，但是日日学习、点滴精进，这些都是确定的，而能给自己这种确定感的也只有我们自己。还等什么，就今天，就此刻，就从搞定一个算法、一个数据结构开始吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item></channel></rss>