<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>操作系统实战45讲 on Gen 的学习笔记</title><link>https://artisanbox.github.io/9/</link><description>Recent content in 操作系统实战45讲 on Gen 的学习笔记</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 08 Mar 2022 18:37:53 +0800</lastBuildDate><atom:link href="https://artisanbox.github.io/9/index.xml" rel="self" type="application/rss+xml"/><item><title>01_程序的运行过程：从代码到机器运行</title><link>https://artisanbox.github.io/9/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/1/</guid><description>你好，我是LMOS。
欢迎来到操作系统第一课。在真正打造操作系统前，有一条必经之路：你知道程序是如何运行的吗？
一个熟练的编程老手只需肉眼看着代码，就能对其运行的过程了如指掌。但对于初学者来说，这常常是很困难的事，这需要好几年的程序开发经验，和在长期的程序开发过程中对编程基本功的积累。
我记得自己最初学习操作系统的时候，面对逻辑稍微复杂的一些程序，在编写、调试代码时，就会陷入代码的迷宫，找不到东南西北。
不知道你现在处在什么阶段，是否曾有同样的感受？我常常说，扎实的基本功就像手里的指南针，你可以一步步强大到不依赖它，但是不能没有。
因此今天，我将带领你从“Hello World”起，扎实基本功，探索程序如何运行的所有细节和原理。这节课的配套代码，你可以从这里下载。
一切要从牛人做的牛逼事说起第一位牛人，是世界级计算机大佬的传奇——Unix之父Ken Thompson。
在上世纪60年代的一个夏天，Ken Thompson的妻子要回娘家一个月。呆在贝尔实验室的他，竟然利用这极为孤独的一个月，开发出了UNiplexed Information and Computing System（UNICS）——即UNIX的雏形，一个全新的操作系统。
要知道，在当时C语言并没有诞生，从严格意义上说，他是用B语言和汇编语言在PDP-7的机器上完成的。
牛人的朋友也是牛人，他的朋友Dennis Ritchie也随之加入其中，共同创造了大名鼎鼎的C语言，并用C语言写出了UNIX和后来的类UNIX体系的几十种操作系统，也写出了对后世影响深远的第一版“Hello World”：
#include &amp;quot;stdio.h&amp;quot; int main(int argc, char const *argv[]) { printf(&amp;quot;Hello World!\n&amp;quot;); return 0; } 计算机硬件是无法直接运行这个C语言文本程序代码的，需要C语言编译器，把这个代码编译成具体硬件平台的二进制代码。再由具体操作系统建立进程，把这个二进制文件装进其进程的内存空间中，才能运行。
听起来很复杂？别急，接着往下看。
程序编译过程我们暂且不急着摸清操作系统所做的工作，先来研究一下编译过程和硬件执行程序的过程，约定使用GCC相关的工具链。
那么使用命令：gcc HelloWorld.c -o HelloWorld 或者 gcc ./HelloWorld.c -o ./HelloWorld ，就可以编译这段代码。其实，GCC只是完成编译工作的驱动程序，它会根据编译流程分别调用预处理程序、编译程序、汇编程序、链接程序来完成具体工作。
下图就是编译这段代码的过程：
其实，我们也可以手动控制以上这个编译流程，从而留下中间文件方便研究：
gcc HelloWorld.c -E -o HelloWorld.i预处理：加入头文件，替换宏。 gcc HelloWorld.c -S -c -o HelloWorld.s编译：包含预处理，将C程序转换成汇编程序。 gcc HelloWorld.c -c -o HelloWorld.o汇编：包含预处理和编译，将汇编程序转换成可链接的二进制程序。 gcc HelloWorld.c -o HelloWorld链接：包含以上所有操作，将可链接的二进制程序和其它别的库链接在一起，形成可执行的程序文件。 程序装载执行对运行内容有了了解后，我们开始程序的装载执行。</description></item><item><title>02_几行汇编几行C：实现一个最简单的内核</title><link>https://artisanbox.github.io/9/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/2/</guid><description>你好，我是LMOS。
我们知道，在学习许多编程语言一开始的时候，都有一段用其语言编写的经典程序——Hello World。这不过是某一操作系统平台之上的应用程序，却心高气傲地问候世界。
而我们学习操作系统的时候，那么也不妨撇开其它现有的操作系统，基于硬件，写一个最小的操作系统——Hello OS，先练练手、热热身，直观感受一下。
本节课的配套代码，你可以从这里下载。
请注意，这节课主要是演示思路，不要求你马上动手实现。详细的环境安装、配置我们到第十节课再详细展开。有兴趣上手的同学，可以参考留言区置顶的实验笔记探索。
PC机的引导流程看标题就知道，写操作系统要用汇编和C语言，尽管这个Hello OS很小，但也要用到两种编程语言。其实，现有的商业操作系统都是用这两种语言开发出来的。
先不用害怕，Hello OS的代码量很少。
其实，我们也不打算从PC的引导程序开始写起，原因是目前我们的知识储备还不够，所以先借用一下GRUB引导程序，只要我们的PC机上安装了Ubuntu Linux操作系统，GRUB就已经存在了。这会大大降低我们开始的难度，也不至于打消你的热情。
那在写Hello OS之前，我们先要搞清楚Hello OS的引导流程，如下图所示：
简单解释一下，PC机BIOS固件是固化在PC机主板上的ROM芯片中的，掉电也能保存，PC机上电后的第一条指令就是BIOS固件中的，它负责检测和初始化CPU、内存及主板平台，然后加载引导设备（大概率是硬盘）中的第一个扇区数据，到0x7c00地址开始的内存空间，再接着跳转到0x7c00处执行指令，在我们这里的情况下就是GRUB引导程序。
当然，更先进的UEFI BIOS则不同，这里就不深入其中了，你可以通过链接自行了解。
Hello OS引导汇编代码明白了PC机的启动流程，下面只剩下我们的Hello OS了，我们马上就去写好它。
我们先来写一段汇编代码。这里我要特别说明一个问题：为什么不能直接用C？
C作为通用的高级语言，不能直接操作特定的硬件，而且C语言的函数调用、函数传参，都需要用栈。
栈简单来说就是一块内存空间，其中数据满足后进先出的特性，它由CPU特定的栈寄存器指向，所以我们要先用汇编代码处理好这些C语言的工作环境。
;彭东 @ 2021.01.09 MBT_HDR_FLAGS EQU 0x00010003 MBT_HDR_MAGIC EQU 0x1BADB002 ;多引导协议头魔数 MBT_HDR2_MAGIC EQU 0xe85250d6 ;第二版多引导协议头魔数 global _start ;导出_start符号 extern main ;导入外部的main函数符号 [section .start.text] ;定义.start.text代码节 [bits 32] ;汇编成32位代码 _start: jmp _entry ALIGN 8 mbt_hdr: dd MBT_HDR_MAGIC dd MBT_HDR_FLAGS dd -(MBT_HDR_MAGIC+MBT_HDR_FLAGS) dd mbt_hdr dd _start dd 0 dd 0 dd _entry ;以上是GRUB所需要的头 ALIGN 8 mbt2_hdr: DD MBT_HDR2_MAGIC DD 0 DD mbt2_hdr_end - mbt2_hdr DD -(MBT_HDR2_MAGIC + 0 + (mbt2_hdr_end - mbt2_hdr)) DW 2, 0 DD 24 DD mbt2_hdr DD _start DD 0 DD 0 DW 3, 0 DD 12 DD _entry DD 0 DW 0, 0 DD 8 mbt2_hdr_end: ;以上是GRUB2所需要的头 ;包含两个头是为了同时兼容GRUB、GRUB2 ALIGN 8 _entry: ;关中断 cli ;关不可屏蔽中断 in al, 0x70 or al, 0x80 out 0x70,al ;重新加载GDT lgdt [GDT_PTR] jmp dword 0x8 :_32bits_mode _32bits_mode: ;下面初始化C语言可能会用到的寄存器 mov ax, 0x10 mov ds, ax mov ss, ax mov es, ax mov fs, ax mov gs, ax xor eax,eax xor ebx,ebx xor ecx,ecx xor edx,edx xor edi,edi xor esi,esi xor ebp,ebp xor esp,esp ;初始化栈，C语言需要栈才能工作 mov esp,0x9000 ;调用C语言函数main call main ;让CPU停止执行指令 halt_step: halt jmp halt_step GDT_START: knull_dsc: dq 0 kcode_dsc: dq 0x00cf9e000000ffff kdata_dsc: dq 0x00cf92000000ffff k16cd_dsc: dq 0x00009e000000ffff k16da_dsc: dq 0x000092000000ffff GDT_END: GDT_PTR: GDTLEN dw GDT_END-GDT_START-1 GDTBASE dd GDT_START 以上的汇编代码（/lesson02/HelloOS/entry.</description></item><item><title>03_黑盒之中有什么：内核结构与设计</title><link>https://artisanbox.github.io/9/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/3/</guid><description>你好，我是LMOS。
在上节课中，我们写了一个极简的操作系统——Hello OS，并成功运行，直观地感受了一下自己控制计算机的乐趣，或许你正沉浸在这种乐趣之中，但我不得不提醒你赶快从这种快乐中走出来。
因为我们的Hello OS虽然能使计算机运行起来，但其实没有任何实际的功能。
什么？没有实际功能，我们往里增加功能不就好了吗？
你可能会这样想，但是这样想就草率了，开发操作系统内核（以下简称内核）就像建房子一样，房子要建得好，就先要设计。比如用什么结构，什么材料，房间怎么布局，电路、水路等，最后画出设计图纸，依据图纸按部就班地进行建造。
而一个内核的复杂程度要比房子的复杂程度高出几个数量级，所以在开发内核之前先要对其进行设计。
下面我们就先搞清楚内核之中有些什么东西，然后探讨一下怎么组织它们、用什么架构来组织、并对比成熟的架构，最后设计出我们想要的内核架构。
黑盒之中有什么从用户和应用程序的角度来看，内核之中有什么并不重要，能提供什么服务才是重要的，所以内核在用户和上层应用眼里，就像一个大黑盒，至于黑盒里面有什么，怎么实现的，就不用管了。
不过，作为内核这个黑盒的开发者，我们要实现它，就必先设计它，而要设计它，就必先搞清楚内核中有什么。
从抽象角度来看，内核就是计算机资源的管理者，当然管理资源是为了让应用使用资源。既然内核是资源的管理者，我们先来看看计算机中有哪些资源，然后通过资源的归纳，就能推导出内核这个大黑盒中应该有什么。
计算机中资源大致可以分为两类资源，一种是硬件资源，一种是软件资源。先来看看硬件资源有哪些，如下：
1.总线，负责连接各种其它设备，是其它设备工作的基础。
2.CPU，即中央处理器，负责执行程序和处理数据运算。
3.内存，负责储存运行时的代码和数据。
4.硬盘，负责长久储存用户文件数据。
5.网卡，负责计算机与计算机之间的通信。
6.显卡，负责显示工作。
7.各种I/O设备，如显示器，打印机，键盘，鼠标等。
下面给出一幅经典的计算机内部结构图，如下：
而计算机中的软件资源，则可表示为计算机中的各种形式的数据。如各种文件、软件程序等。
内核作为硬件资源和软件资源的管理者，其内部组成在逻辑上大致如下：
1.管理CPU，由于CPU是执行程序的，而内核把运行时的程序抽象成进程，所以又称为进程管理。
2.管理内存，由于程序和数据都要占用内存，内存是非常宝贵的资源，所以内核要非常小心地分配、释放内存。
3.管理硬盘，而硬盘主要存放用户数据，而内核把用户数据抽象成文件，即管理文件，文件需要合理地组织，方便用户查找和读写，所以形成了文件系统。
4.管理显卡，负责显示信息，而现在操作系统都是支持GUI（图形用户接口）的，管理显卡自然而然地就成了内核中的图形系统。
5.管理网卡，网卡主要完成网络通信，网络通信需要各种通信协议，最后在内核中就形成了网络协议栈，又称网络组件。
6.管理各种I/O设备，我们经常把键盘、鼠标、打印机、显示器等统称为I/O（输入输出）设备，在内核中抽象成I/O管理器。
内核除了这些必要组件之外，根据功能不同还有安全组件等，最值得一提的是，各种计算机硬件的性能不同，硬件型号不同，硬件种类不同，硬件厂商不同，内核要想管理和控制这些硬件就要编写对应的代码，通常这样的代码我们称之为驱动程序。
硬件厂商就可以根据自己不同的硬件编写不同的驱动，加入到内核之中。
以上我们已经大致知道了内核之中有哪些组件，但是另一个问题又出现了，即如何组织这些组件，让系统更加稳定和高效，这就需要我们从现有的一些经典内核结构里找灵感了。
宏内核结构其实看这名字，就已经能猜到了，宏即大也，这种最简单适用，也是最早的一种内核结构。
宏内核就是把以上诸如管理进程的代码、管理内存的代码、管理各种I/O设备的代码、文件系统的代码、图形系统代码以及其它功能模块的代码，把这些所有的代码经过编译，最后链接在一起，形成一个大的可执行程序。
这个大程序里有实现支持这些功能的所有代码，向用户应用软件提供一些接口，这些接口就是常说的系统API函数。而这个大程序会在处理器的特权模式下运行，这个模式通常被称为宏内核模式。结构如下图所示。
尽管图中一层一层的，这并不是它们有层次关系，仅仅表示它们链接在一起。
为了理解宏内核的工作原理，我们来看一个例子，宏内核提供内存分配功能的服务过程，具体如下：
1.应用程序调用内存分配的API（应用程序接口）函数。
2.处理器切换到特权模式，开始运行内核代码。
3.内核里的内存管理代码按照特定的算法，分配一块内存。
4.把分配的内存块的首地址，返回给内存分配的API函数。
5.内存分配的API函数返回，处理器开始运行用户模式下的应用程序，应用程序就得到了一块内存的首地址，并且可以使用这块内存了。
上面这个过程和一个实际的操作系统中的运行过程，可能有差异，但大同小异。当然，系统API和应用程序之间可能还有库函数，也可能只是分配了一个虚拟地址空间，但是我们关注的只是这个过程。
上图的宏内核结构有明显的缺点，因为它没有模块化，没有扩展性、没有移植性，高度耦合在一起，一旦其中一个组件有漏洞，内核中所有的组件可能都会出问题。
开发一个新的功能也得重新编译、链接、安装内核。其实现在这种原始的宏内核结构已经没有人用了。这种宏内核唯一的优点是性能很好，因为在内核中，这些组件可以互相调用，性能极高。
为了方便我们了解不同内核架构间的优缺点，下面我们看一个和宏内核结构对应的反例。
微内核结构微内核架构正好与宏内核架构相反，它提倡内核功能尽可能少：仅仅只有进程调度、处理中断、内存空间映射、进程间通信等功能（目前不懂没事，这是属于管理进程和管理内存的功能模块，后面课程里还会专门探讨的）。
这样的内核是不能完成什么实际功能的，开发者们把实际的进程管理、内存管理、设备管理、文件管理等服务功能，做成一个个服务进程。和用户应用进程一样，只是它们很特殊，宏内核提供的功能，在微内核架构里由这些服务进程专门负责完成。
微内核定义了一种良好的进程间通信的机制——消息。应用程序要请求相关服务，就向微内核发送一条与此服务对应的消息，微内核再把这条消息转发给相关的服务进程，接着服务进程会完成相关的服务。服务进程的编程模型就是循环处理来自其它进程的消息，完成相关的服务功能。其结构如下所示：
为了理解微内核的工程原理，我们来看看微内核提供内存分配功能的服务过程，具体如下：
1.应用程序发送内存分配的消息，这个发送消息的函数是微内核提供的，相当于系统API，微内核的API（应用程序接口）相当少，极端情况下仅需要两个，一个接收消息的API和一个发送消息的API。
2.处理器切换到特权模式，开始运行内核代码。
3.微内核代码让当前进程停止运行，并根据消息包中的数据，确定消息发送给谁，分配内存的消息当然是发送给内存管理服务进程。
4.内存管理服务进程收到消息，分配一块内存。
5.内存管理服务进程，也会通过消息的形式返回分配内存块的地址给内核，然后继续等待下一条消息。
6.微内核把包含内存块地址的消息返回给发送内存分配消息的应用程序。
7.处理器开始运行用户模式下的应用程序，应用程序就得到了一块内存的首地址，并且可以使用这块内存了。
微内核的架构实现虽然不同，但是大致过程和上面一样。同样是分配内存，在微内核下拐了几个弯，一来一去的消息带来了非常大的开销，当然各个服务进程的切换开销也不小。这样系统性能就大打折扣。
但是微内核有很多优点，首先，系统结构相当清晰利于协作开发。其次，系统有良好的移植性，微内核代码量非常少，就算重写整个内核也不是难事。最后，微内核有相当好的伸缩性、扩展性，因为那些系统功能只是一个进程，可以随时拿掉一个服务进程以减少系统功能，或者增加几个服务进程以增强系统功能。
微内核的代表作有MACH、MINIX、L4系统，这些系统都是微内核，但是它们不是商业级的系统，商业级的系统不采用微内核主要还是因为性能差。
好了，粗略了解了宏内核和微内核两大系统内核架构的优、缺点，以后设计我们自己的系统内核时，心里也就有了底了，到时就可以扬长避短了，下面我们先学习一点其它的东西，即分离硬件相关性，为设计出我们自己的内核架构打下基础。
分离硬件的相关性我们会经常听说，Windows内核有什么HAL层、Linux内核有什么arch层。这些xx层就是Windows和Linux内核设计者，给他们的系统内核分的第一个层。
今天如此庞杂的计算机，其实也是一层一层地构建起来的，从硬件层到操作系统层再到应用软件层这样构建。分层的主要目的和好处在于屏蔽底层细节，使上层开发更加简单。
计算机领域的一个基本方法是增加一个抽象层，从而使得抽象层的上下两层独立地发展，所以在内核内部再分若干层也不足为怪。
分离硬件的相关性，就是要把操作硬件和处理硬件功能差异的代码抽离出来，形成一个独立的软件抽象层，对外提供相应的接口，方便上层开发。
为了让你更好理解，我们举进程管理中的一个模块实现细节的例子：进程调度模块。通过这个例子，来看看分层对系统内核的设计与开发有什么影响。
一般操作系统理论课程都会花大量篇幅去讲进程相关的概念，其实说到底，进程是操作系统开发者为了实现多任务而提出的，并让每个进程在CPU上运行一小段时间，这样就能实现多任务同时运行的假象。
当然，这种假象十分奏效。要实现这种假象，就要实现下面这两种机制：
1.进程调度，它的目的是要从众多进程中选择一个将要运行的进程，当然有各种选择的算法，例如，轮转算法、优先级算法等。</description></item><item><title>04_震撼的Linux全景图：业界成熟的内核架构长什么样？</title><link>https://artisanbox.github.io/9/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/4/</guid><description>你好，我是LMOS。
什么？你想成为计算机黑客？
梦想坐在计算机前敲敲键盘，银行账号里的数字就会自己往上涨。拜托，估计明天你就该被警察逮捕了。真正的黑客是对计算机技术有近乎极致的追求，而不是干坏事。
下面我就带你认识这样一个计算机黑客，看看他是怎样创造出影响世界的Linux，然后进一步了解一下Linux的内部结构。
同时，我也会带你看看Windows NT和Darwin的内部结构，三者形成对比，你能更好地了解它们之间的差异和共同点，这对我们后面写操作系统会很有帮助。
关于LinusLinus Benedict Torvalds，这个名字很长，下面简称Linus，他1969年12月28日出生在芬兰的赫尔辛基市，并不是美国人。Linus在赫尔辛基大学学的就是计算机，妻子还是空手道高手，一个“码林高手”和一个“武林高手”真的是绝配啊。
Linus在小时候就对各种事情充满好奇，这点非常具有黑客精神，后来有了自己的计算机更是痴迷其中，开始自己控制计算机做一些事情，并深挖其背后的原理。就是这种黑客精神促使他后来写出了颠覆世界的软件——Linux，也因此登上了美国《时代》周刊。
你是否对很多垃圾软件感到愤慨，但自己又无法改变。Linus就不一样，他为了方便访问大学服务器中的资源 ，而在自己的机器上写了一个文件系统和硬盘驱动，这样就可以把自己需要的资源下载到自己的机器中。
再后来，这成为了Linux的第一个版本。看看，牛人之所以为牛人就是敢于对现有的规则说不，并勇于改变。
如果仅仅如此，那么也不会有后来的Linux内核。Linus随后做了一个重要决定，他把这款操作系统雏形开源，并加入到自由软件运动，以GPL协议授权，允许用户自由复制或者改动程序代码，但用户必须公开自己的修改并传播。
无疑，正是Linus的这一重要决定使得Linux和他自己名声大振。短短几年时间，就已经聚集了成千上万的狂热分子，大家不计得失的为Linux添砖加瓦，很多程序员更是对Linus像神明一样顶礼膜拜。
Linux内核好了回到正题，回到Linux。Linus也不是什么神明，现有的Linux，99.9%的代码都不是Linus所写，而且他的代码，也不一定比你我的代码写得更好。
Linux，全称GNU/Linux，是一套免费使用和自由传播的操作系统，支持类UNIX、POSIX标准接口，也支持多用户、多进程、多线程，可以在多CPU的机器上运行。由于互联网的发展，Linux吸引了来自全世界各地软件爱好者、科技公司的支持，它已经从大型机到服务器蔓延至个人电脑、嵌入式系统等领域。
Linux系统性能稳定且开源。在很多公司企业网络中被当作服务器来使用，这是Linux的一大亮点，也是它得以壮大的关键。
Linux的基本思想是一切都是文件：每个文件都有确定的用途，包括用户数据、命令、配置参数、硬件设备等对于操作系统内核而言，都被视为各种类型的文件。Linux支持多用户，各个用户对于自己的文件有自己特殊的权利，保证了各用户之间互不影响。多任务则是现代操作系统最重要的一个特点，Linux可以使多个程序同时并独立地运行。
Linux发展到今天，不是哪一个人能做到的，更不是一群计算机黑客能做到的，而是由很多世界级的顶尖科技公司联合开发，如IBM、甲骨文、红帽、英特尔、微软，它们开发Linux并向Linux社区提供补丁，使Linux工作在它们的服务器上，向客户出售业务服务。
Linux发展到今天其代码量近2000万行，可以用浩如烟海来形容，没人能在短时间内弄清楚。但是你也不用害怕，我们可以先看看Linux内部的全景图，从全局了解一下Linux的内部结构，如下图。
啊哈！是不是感觉壮观之后一阵头晕目眩，头晕目眩就对了，因为Linux太大了，别怕，下面我们来分解一下。但这里我要先解释一下，上图仍然不足于描述Linux的全部，只是展示了重要且显而易见的部分。
上图中大致分为五大重要组件，每个组件又分成许多模块从上到下贯穿各个层次，每个模块中有重要的函数和数据结构。具体每个模块的主要功能，我都给你列在了文稿里，你可以详细看看后面这张图。
不要着急，不要心慌，因为现在我们不需要搞清楚这些Linux模块的全部实现细节，只要在心里默念Linux的模块真多啊，大概有五大组件，有好几十个模块，每个模块主要完成什么功能就行了。
是不是松了口气，先定定神，然后我们就能发现Linux这么多模块挤在一起，之间的通信主要是函数调用，而且函数间的调用没有一定的层次关系，更加没有左右边界的限定。函数的调用路径是纵横交错的，从图中的线条可以得到印证。
继续深入思考你就会发现，这些纵横交错的路径上有一个函数出现了问题，就麻烦大了，它会波及到全部组件，导致整个系统崩溃。当然调试解决这个问题，也是相当困难的。同样，模块之间没有隔离，安全隐患也是巨大的。
当然，这种结构不是一无是处，它的性能极高，而性能是衡量操作系统的一个重要指标。这种结构就是传统的内核结构，也称为宏内核架构。
想要评判一个产品好不好，最直接的方法就是用相似的产品对比。你说Linux很好，但是什么为好呢？我说Linux很差，它又差在什么地方呢？
下面我们就拿出Windows和macOS进行对比，注意我们只是对比它们的内核架构。
Darwin-XNU内核我们先来看看Darwin，Darwin是由苹果公司在2000年开发的一个开放源代码的操作系统。
一个经久不衰的公司，必然有自己的核心竞争力，也许是商业策略，也许是技术产品，又或是这两者的结合。而作为苹果公司各种产品和强大的应用生态系统的支撑者——Darwin，更是公司核心竞争力中的核心。
苹果公司有台式计算机、笔记本、平板、手机，台式计算机、笔记本使用了macOS操作系统，平板和手机则使用了iOS操作系统。Darwin作为macOS与iOS操作系统的核心，从技术实现角度说，它必然要支持PowerPC、x86、ARM架构的处理器。
Darwin 使用了一种微内核（Mach）和相应的固件来支持不同的处理器平台，并提供操作系统原始的基础服务，上层的功能性系统服务和工具则是整合了BSD系统所提供的。苹果公司还为其开发了大量的库、框架和服务，不过它们都工作在用户态且闭源。
下面我们先从整体看一下Darwin的架构。
什么？两套内核？惊不惊喜？由于我们是研究Darwin内核，所以上图中我们只需要关注内核-用户转换层以下的部分即可。显然它有两个内核层——Mach层与BSD层。
Mach内核是卡耐基梅隆大学开发的经典微内核，意在提供最基本的操作系统服务，从而达到高性能、安全、可扩展的目的，而BSD则是伯克利大学开发的类UNIX操作系统，提供一整套操作系统服务。
那为什么两套内核会同时存在呢？
MAC OS X（2011年之前的称呼）的发展经过了不同时期，随着时代的进步，产品功能需求增加，单纯的Mach之上实现出现了性能瓶颈，但是为了兼容之前为Mach开发的应用和设备驱动，就保留了Mach内核，同时加入了BSD内核。
Mach内核仍然提供十分简单的进程、线程、IPC通信、虚拟内存设备驱动相关的功能服务，BSD则提供强大的安全特性，完善的网络服务，各种文件系统的支持，同时对Mach的进程、线程、IPC、虚拟内核组件进行细化、扩展延伸。
那么应用如何使用Darwin系统的服务呢？应用会通过用户层的框架和库来请求Darwin系统的服务，即调用Darwin系统API。
在调用Darwin系统API时，会传入一个API号码，用这个号码去索引Mach陷入中断服务表中的函数。此时，API号码如果小于0，则表明请求的是Mach内核的服务，API号码如果大于0，则表明请求的是BSD内核的服务，它提供一整套标准的POSIX接口。
就这样，Mach和BSD就同时存在了。
Mach中还有一个重要的组件Libkern，它是一个库，提供了很多底层的操作函数，同时支持C++运行环境。
依赖这个库的还有IOKit，IOKit管理所有的设备驱动和内核功能扩展模块。驱动程序开发人员则可以使用C++面向对象的方式开发驱动，这个方式很优雅，你完全可以找一个成熟的驱动程序作为父类继承它，要特别实现某个功能就重载其中的函数，也可以同时继承其它驱动程序，这大大节省了内存，也大大降低了出现BUG的可能。
如果你要详细了解Darwin内核的话，可以自行阅读相应的代码。而在这里，你只要从全局认识一下它的结构就行了。
Windows NT内核接下来我们再看下 NT 内核。现代Windows的内核就是NT，我们不妨先看看NT的历史。
如果你是90后，大概没有接触过MS-DOS，它的交互方式是你在键盘上输入相应的功能命令，它完成相应的功能后给用户返回相应的操作信息，没有图形界面。
在MS-DOS内核的实现上，也没有应用现代硬件的保护机制，这导致后来微软基于它开发的图形界面的操作系统，如Windows 3.1、Windows95/98/ME，极其不稳定，且容易死机。
加上类UNIX操作系统在互联网领域大行其道，所以微软急需一款全新的操作系统来与之竞争。所以，Windows NT诞生了。
Windows NT是微软于1993年推出的面向工作站、网络服务器和大型计算机的网络操作系统，也可做PC操作系统。它是一款全新从零开始开发的新操作系统，并应用了现代硬件的所有特性，“NT”所指的便是“新技术”（New Technology）。
而普通用户第一次接触基于NT内核的Windows是Windows 2000，一开始用户其实是不愿意接受的，因为Windows 2000对用户的硬件和应用存在兼容性问题。
随着硬件厂商和应用厂商对程序的升级，这个兼容性问题被缓解了，加之Windows 2000的高性能、高稳定性、高安全性，用户很快便接受了这个操作系统。这可以从Windows 2000的迭代者Windows XP的巨大成功，得到验证。
现在，NT内核在设计上层次非常清晰明了，各组件之间界限耦合程度很低。下面我们就来看看NT内核架构图，了解一下NT内核是如何“庄严宏伟”。如下图：
这样看NT内核架构，是不是就清晰了很多？但这并不是我画图画得清晰，事实上的NT确实如此。</description></item><item><title>05_CPU工作模式：执行程序的三种模式</title><link>https://artisanbox.github.io/9/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/5/</guid><description>你好，我是LMOS。
我们在前面已经设计了我们的OS架构，你也许正在考虑怎么写代码实现它。恕我直言，现在我们还有很多东西没搞清楚。
由于OS内核直接运行在硬件之上，所以我们要对运行我们代码的硬件平台有一定的了解。接下来，我会通过三节课，带你搞懂硬件平台的关键内容。
今天我们先来学习CPU的工作模式，硬件中最重要的就是CPU，它就是执行程序的核心部件。而我们常用的电脑就是x86平台，所以我们要对x86 CPU有一些基本的了解。
按照CPU功能升级迭代的顺序，CPU的工作模式有实模式、保护模式、长模式，这几种工作模式下CPU执行程序的方式截然不同，下面我们一起来探讨这几种工作模式。
从一段死循环的代码说起请思考一下，如果下面这段应用程序代码能够成功运行，会有什么后果？
int main() { int* addr = (int*)0; cli(); //关中断 while(1) { *addr = 0; addr++; } return 0; } 上述代码首先关掉了CPU中断，让CPU停止响应中断信号，然后进入死循环，最后从内存0地址开始写入0。你马上就会想到，这段代码只做了两件事：一是锁住了CPU，二是清空了内存，你也许会觉得如果这样的代码能正常运行，那简直太可怕了。
不过如果是在实模式下，这样的代码确实是能正常运行。因为在很久以前，计算机资源太少，内存太小，都是单道程序执行，程序大多是由专业人员编写调试好了，才能预约到一个时间去上机运行，没有现代操作系统的概念。
后来有DOS操作系统，也是单道程序系统，不具备执行多道程序的能力，所以CPU这种模式也能很好地工作。
下面我们就从最简单，也是最原始的实模式开始讲起。
实模式实模式又称实地址模式，实，即真实，这个真实分为两个方面，一个方面是运行真实的指令，对指令的动作不作区分，直接执行指令的真实功能，另一方面是发往内存的地址是真实的，对任何地址不加限制地发往内存。
实模式寄存器由于CPU是根据指令完成相应的功能，举个例子：ADD AX,CX；这条指令完成加法操作，AX、CX为ADD指令的操作数，可以理解为ADD函数的两个参数，其功能就是把AX、CX中的数据相加。
指令的操作数，可以是寄存器、内存地址、常数，其实通常情况下是寄存器，AX、CX就是x86 CPU中的寄存器。
下面我们就去看看x86 CPU在实模式下的寄存器。表中每个寄存器都是16位的。
实模式下访问内存虽然有了寄存器，但是数据和指令都是存放在内存中的。通常情况下，需要把数据装载进寄存器中才能操作，还要有获取指令的动作，这些都要访问内存才行，而我们知道访问内存靠的是地址值。
那问题来了，这个值是如何计算的呢？计算过程如下图。
结合上图可以发现，所有的内存地址都是由段寄存器左移4位，再加上一个通用寄存器中的值或者常数形成地址，然后由这个地址去访问内存。这就是大名鼎鼎的分段内存管理模型。
只不过这里要特别注意的是，代码段是由CS和IP确定的，而栈段是由SS和SP段确定的。
下面我们写一个DOS下的Hello World应用程序，这是一个工作在实模式下的汇编代码程序，一共16位，具体代码如下：
data SEGMENT ;定义一个数据段存放Hello World! hello DB 'Hello World!$' ;注意要以$结束 data ENDS code SEGMENT ;定义一个代码段存放程序指令 ASSUME CS:CODE,DS:DATA ;告诉汇编程序，DS指向数据段，CS指向代码段 start: MOV AX,data ;将data段首地址赋值给AX MOV DS,AX ;将AX赋值给DS，使DS指向data段 LEA DX,hello ;使DX指向hello首地址 MOV AH,09h ;给AH设置参数09H，AH是AX高8位，AL是AX低8位，其它类似 INT 21h ;执行DOS中断输出DS指向的DX指向的字符串hello MOV AX,4C00h ;给AX设置参数4C00h INT 21h ;调用4C00h号功能，结束程序 code ENDS END start 上述代码中的结构模型，也是符合CPU实模式下分段内存管理模式的，它们被汇编器转换成二进制数据后，也是以段的形式存在的。</description></item><item><title>06_虚幻与真实：程序中的地址如何转换？</title><link>https://artisanbox.github.io/9/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/6/</guid><description>你好，我是LMOS。
从前面的课程我们得知，CPU执行程序、处理数据都要和内存打交道，这个打交道的方式就是内存地址。
读取指令、读写数据都需要首先告诉内存芯片：hi，内存老哥请你把0x10000地址处的数据交给我……hi，内存老哥，我已经计算完成，请让我把结果写回0x200000地址的空间。这些地址存在于代码指令字段后的常数，或者存在于某个寄存器中。
今天，我们就来专门研究一下程序中的地址。说起程序中的地址，不知道你是否好奇过，为啥系统设计者要引入虚拟地址呢？
我会先带你从一个多程序并发的场景热身，一起思考这会导致哪些问题，为什么能用虚拟地址解决这些问题。
搞懂原理之后，我还会带你一起探索虚拟地址和物理地址的关系和转换机制。在后面的课里，你会发现，我们最宝贵的内存资源正是通过这些机制来管理的。
从一个多程序并发的场景说起设想一下，如果一台计算机的内存中只运行一个程序A，这种方式正好用前面CPU的实模式来运行，因为程序A的地址在链接时就可以确定，例如从内存地址0x8000开始，每次运行程序A都装入内存0x8000地址处开始运行，没有其它程序干扰。
现在改变一下，内存中又放一道程序B，程序A和程序B各自运行一秒钟，如此循环，直到其中之一结束。这个新场景下就会产生一些问题，当然这里我们只关心内存相关的这几个核心问题。
1.谁来保证程序A跟程序B 没有内存地址的冲突？换句话说，就是程序A、B各自放在什么内存地址，这个问题是由A、B程序协商，还是由操作系统决定。
2.怎样保证程序A跟程序B 不会互相读写各自的内存空间？这个问题相对简单，用保护模式就能解决。
3.如何解决内存容量问题？程序A和程序B，在不断开发迭代中程序代码占用的空间会越来越大，导致内存装不下。
4.还要考虑一个扩展后的复杂情况，如果不只程序A、B，还可能有程序C、D、E、F、G……它们分别由不同的公司开发，而每台计算机的内存容量不同。这时候，又对我们的内存方案有怎样的影响呢？
要想完美地解决以上最核心的4个问题，一个较好的方案是：让所有的程序都各自享有一个从0开始到最大地址的空间，这个地址空间是独立的，是该程序私有的，其它程序既看不到，也不能访问该地址空间，这个地址空间和其它程序无关，和具体的计算机也无关。
事实上，计算机科学家们早就这么做了，这个方案就是虚拟地址，下面我们就来看看它。
虚拟地址正如其名，这个地址是虚拟的，自然而然地和具体环境进行了解耦，这个环境包括系统软件环境和硬件环境。
虚拟地址是逻辑上存在的一个数据值，比如0~100就有101个整数值，这个0~100的区间就可以说是一个虚拟地址空间，该虚拟地址空间有101个地址。
我们再来看看最开始Hello World的例子，我们用objdump工具反汇编一下Hello World二进制文件，就会得到如下的代码片段：
00000000000004e8 &amp;lt;_init&amp;gt;: 4e8: 48 83 ec 08 sub $0x8,%rsp 4ec: 48 8b 05 f5 0a 20 00 mov 0x200af5(%rip),%rax # 200fe8 &amp;lt;__gmon_start__&amp;gt; 4f3: 48 85 c0 test %rax,%rax 4f6: 74 02 je 4fa &amp;lt;_init+0x12&amp;gt; 4f8: ff d0 callq *%rax 4fa: 48 83 c4 08 add $0x8,%rsp 4fe: c3 retq 上述代码中，左边第一列数据就是虚拟地址，第三列中是程序指令，如：“mov 0x200af5(%rip),%rax，je 4fa，callq *%rax”指令中的数据都是虚拟地址。</description></item><item><title>07_Cache与内存：程序放在哪儿？</title><link>https://artisanbox.github.io/9/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/7/</guid><description>你好，我是LMOS。
在前面的课程里，我们已经知道了CPU是如何执行程序的，也研究了程序的地址空间，这里我们终于到了程序的存放地点——内存。
你知道什么是Cache吗？在你心中，真实的内存又是什么样子呢？今天我们就来重新认识一下Cache和内存，这对我们利用Cache写出高性能的程序代码和实现操作系统管理内存，有着巨大的帮助。
通过这节课的内容，我们一起来看看内存到底是啥，它有什么特性。有了这个认识，你就能更加深入地理解我们看似熟悉的局部性原理，从而搞清楚，为啥Cache是解决内存瓶颈的神来之笔。最后，我还会带你分析x86平台上的Cache，规避Cache引发的一致性问题，并让你掌握获取内存视图的方法。
那话不多说，带着刚才的问题，我们正式进入今天的学习吧！
从一段“经典”代码看局部性原理不知道，你还记不记得C语言打印九九乘法表的代码，想不起来也没关系，下面我把它贴出来，代码很短，也很简单，就算你自己写一个也用不了一分钟，如下所示。
#include &amp;lt;stdio.h&amp;gt; int main(){ int i,j; for(i=1;i&amp;lt;=9;i++){ for(j=1;j&amp;lt;=i;j++){ printf(&amp;quot;%d*%d=%2d &amp;quot;,i,j,i*j); } printf(&amp;quot;\n&amp;quot;); } return 0; } 我们当然不是为了研究代码本身，这个代码非常简单，这里我们主要是观察这个结构，代码的结构主要是顺序、分支、循环，这三种结构可以写出现存所有算法的程序。
我们常规情况下写的代码是顺序和循环结构居多。上面的代码中有两重循环，内层循环的次数受到外层循环变量的影响。就是这么简单，但是越简单的东西越容易看到本质。
可以看到，这个代码大数时间在执行一个乘法计算和调用一个printf函数，而程序一旦编译装载进内存中，它的地址就确定了。也就是说，CPU大多数时间在访问相同或者与此相邻的地址，换句话说就是：CPU大多数时间在执行相同的指令或者与此相邻的指令。这就是大名鼎鼎的程序局部性原理。
内存明白了程序的局部性原理之后，我们再来看看内存。你或许感觉这跨越有点大，但是只有明白了内存的结构和特性，你才能明白程序局部性原理的应用场景和它的重要性。
内存也可称为主存，不管硬盘多大、里面存放了多少程序和数据，只要程序运行或者数据要进行计算处理，就必须先将它们装入内存。我们先来看看内存长什么样（你也可以上网自行搜索），如下图所示。
从上图可以看到在PCB板上有内存颗粒芯片，主要是用来存放数据的。SPD芯片用于存放内存自身的容量、频率、厂商等信息。还有最显眼的金手指，用于连接数据总线和地址总线，电源等。
其实从专业角度讲，内存应该叫DRAM，即动态随机存储器。内存储存颗粒芯片中的存储单元是由电容和相关元件做成的，电容存储电荷的多、少代表数字信号0和1。
而随着时间的流逝，电容存在漏电现象，这导致电荷不足，就会让存储单元的数据出错，所以DRAM需要周期性刷新，以保持电荷状态。DRAM结构较简单且集成度很高，通常用于制造内存条中的储存颗粒芯片。
虽然内存技术标准不断更新，但是储存颗粒的内部结构没有本质改变，还是电容存放电荷，标准看似更多，实际上只是提升了位宽、工作频率，以及传输时预取的数据位数。
比如DDR SDRAM，即双倍速率同步动态随机存储器，它使用2.5V的工作电压，数据位宽为64位，核心频率最高为166MHz。下面简称DDR内存，它表示每一个时钟脉冲传输两次数据，分别在时钟脉冲的上升沿和下降沿各传输一次数据，因此称为双倍速率的SDRAM。
后来的DDR2、DDR3、DDR4也都在核心频率和预取位数上做了提升。最新的DDR4采用1.2V工作电压，数据位宽为64位，预取16位数据。DDR4取消了双通道机制，一条内存即为一条通道，工作频率最高可达4266MHz，单根DDR4内存的数据传输带宽最高为34GB/s。
其实我们无需过多关注内存硬件层面的技术规格标准，重点需要关注的是，内存的速度还有逻辑上内存和系统的连接方式和结构，这样你就能意识到内存有多慢，还有是什么原因导致内存慢的。
我们还是画幅图说明吧，如下图所示。
结合图片我们看到，控制内存刷新和内存读写的是内存控制器，而内存控制器集成在北桥芯片中。传统方式下，北桥芯片存在于系统主板上，而现在由于芯片制造工艺的升级，芯片集成度越来越高，所以北桥芯片被就集成到CPU芯片中了，同时这也大大提升了CPU访问内存的性能。
而作为软件开发人员，从逻辑上我们只需要把内存看成一个巨大的字节数组就可以，而内存地址就是这个数组的下标。
CPU到内存的性能瓶颈尽管CPU和内存是同时代发展的，但CPU所使用技术工艺的材料和内存是不同的，侧重点也不同，价格也不同。如果内存使用CPU的工艺和材料制造，那内存条的昂贵程度会超乎想象，没有多少人能买得起。
由于这些不同，导致了CPU和内存条的数据吞吐量天差地别。尽管最新的DDR4内存条带宽高达34GB/s，然而这相比CPU的数据吞吐量要慢上几个数量级。再加上多核心CPU同时访问内存，会导致总线争用问题，数据吞吐量会进一步下降。
CPU要数据，内存一时给不了怎么办？CPU就得等，通常CPU会让总线插入等待时钟周期，直到内存准备好，到这里你就会发现，无论CPU的性能多高都没用，而内存才是决定系统整体性能的关键。显然依靠目前的理论直接提升内存性能，达到CPU的同等水平，这是不可行的，得想其它的办法。
Cache让我们重新回到前面的场景中，回到程序的局部性原理，它告诉我们：CPU大多数时间在访问相同或者与此相邻的地址。那么，我们立马就可以想到用一块小而快的储存器，放在CPU和内存之间，就可以利用程序的局部性原理来缓解CPU和内存之间的性能瓶颈。这块小而快的储存器就是Cache，即高速缓存。
Cache中存放了内存中的一部分数据，CPU在访问内存时要先访问Cache，若Cache中有需要的数据就直接从Cache中取出，若没有则需要从内存中读取数据，并同时把这块数据放入Cache中。但是由于程序的局部性原理，在一段时间内，CPU总是能从Cache中读取到自己想要的数据。
Cache可以集成在CPU内部，也可以做成独立的芯片放在总线上，现在x86 CPU和ARM CPU都是集成在CPU内部的。其逻辑结构如下图所示。
Cache主要由高速的静态储存器、地址转换模块和Cache行替换模块组成。
Cache会把自己的高速静态储存器和内存分成大小相同的行，一行大小通常为32字节或者64字节。Cache和内存交换数据的最小单位是一行，为方便管理，在Cache内部的高速储存器中，多个行又会形成一组。
除了正常的数据空间外，Cache行中还有一些标志位，如脏位、回写位，访问位等，这些位会被Cache的替换模块所使用。
Cache大致的逻辑工作流程如下。
1.CPU发出的地址由Cache的地址转换模块分成3段：组号，行号，行内偏移。
2.Cache会根据组号、行号查找高速静态储存器中对应的行。如果找到即命中，用行内偏移读取并返回数据给CPU，否则就分配一个新行并访问内存，把内存中对应的数据加载到Cache行并返回给CPU。写入操作则比较直接，分为回写和直通写，回写是写入对应的Cache行就结束了，直通写则是在写入Cache行的同时写入内存。
3.如果没有新行了，就要进入行替换逻辑，即找出一个Cache行写回内存，腾出空间，替换行有相关的算法，替换算法是为了让替换的代价最小化。例如，找出一个没有修改的Cache行，这样就不用把它其中的数据回写到内存中了，还有找出存在时间最久远的那个Cache行，因为它大概率不会再访问了。
以上这些逻辑都由Cache硬件独立实现，软件不用做任何工作，对软件是透明的。
Cache带来的问题Cache虽然带来性能方面的提升，但同时也给和硬件和软件开发带来了问题，那就是数据一致性问题。
为了搞清楚这个问题，我们必须先搞清楚Cache在硬件层面的结构，下面我画了x86 CPU的Cache结构图：
这是一颗最简单的双核心CPU，它有三级Cache，第一级Cache是指令和数据分开的，第二级Cache是独立于CPU核心的，第三级Cache是所有CPU核心共享的。
下面来看看Cache的一致性问题，主要包括这三个方面.
1.一个CPU核心中的指令Cache和数据Cache的一致性问题。
2.多个CPU核心各自的2级Cache的一致性问题。
3.CPU的3级Cache与设备内存，如DMA、网卡帧储存，显存之间的一致性问题。这里我们不需要关注这个问题。
我们先来看看CPU核心中的指令Cache和数据Cache的一致性问题，对于程序代码运行而言，指令都是经过指令Cache，而指令中涉及到的数据则会经过数据Cache。
所以，对自修改的代码（即修改运行中代码指令数据，变成新的程序）而言，比如我们修改了内存地址A这个位置的代码（典型的情况是Java运行时编译器），这个时候我们是通过储存的方式去写的地址A，所以新的指令会进入数据Cache。
但是我们接下来去执行地址A处的指令的时候，指令Cache里面可能命中的是修改之前的指令。所以，这个时候软件需要把数据Cache中的数据写入到内存中，然后让指令Cache无效，重新加载内存中的数据。
再来看看多个CPU核心各自的2级Cache的一致性问题。从上图中可以发现，两个CPU核心共享了一个3级Cache。比如第一个CPU核心读取了一个A地址处的变量，第二个CPU也读取A地址处的变量，那么第二个CPU核心是不是需要从内存里面经过第3、2、1级Cache再读一遍，这个显然是没有必要的。
在硬件上Cache相关的控制单元，可以把第一个CPU核心的A地址处Cache内容直接复制到第二个CPU的第2、1级Cache，这样两个CPU核心都得到了A地址的数据。不过如果这时第一个CPU核心改写了A地址处的数据，而第二个CPU核心的2级Cache里面还是原来的值，数据显然就不一致了。
为了解决这些问题，硬件工程师们开发了多种协议，典型的多核心Cache数据同步协议有MESI和MOESI。MOESI和MESI大同小异，下面我们就去研究一下MESI协议。
Cache的MESI协议MESI协议定义了4种基本状态：M、E、S、I，即修改（Modified）、独占（Exclusive）、共享（Shared）和无效（Invalid）。下面我结合示意图，给你解释一下这四种状态。</description></item><item><title>08_锁：并发操作中，解决数据同步的四种方法</title><link>https://artisanbox.github.io/9/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/8/</guid><description>你好，我是LMOS。
我们在前面的课程中探索了，开发操作系统要了解的最核心的硬件——CPU、MMU、Cache、内存，知道了它们的工作原理。在程序运行中，它们起到了至关重要的作用。
在开发我们自己的操作系统以前，还不能一开始就把机器跑起来，而是先要弄清楚数据同步的问题。如果不解决掉数据同步的问题，后面机器跑起来，就会出现很多不可预知的结果。
通过这节课，我会给你讲清楚为什么在并发操作里，很可能得不到预期的访问数据，还会带你分析这个问题的原因以及解决方法。有了这样一个研究、解决问题的过程，对最重要的几种锁（原子变量，关中断，信号量，自旋锁），你就能做到心中有数了。
非预期结果的全局变量来看看下面的代码，描述的是一个线程中的函数和中断处理函数，它们分别对一个全局变量执行加1操作，代码如下。
int a = 0; void interrupt_handle() { a++; } void thread_func() { a++; } 首先我们梳理一下编译器的翻译过程，通常编译器会把a++语句翻译成这3条指令。
1.把a加载某个寄存器中。
2.这个寄存器加1。
3.把这个寄存器写回内存。
那么不难推断，可能导致结果不确定的情况是这样的：thread_func函数还没运行完第2条指令时，中断就来了。
因此，CPU转而处理中断，也就是开始运行interrupt_handle函数，这个函数运行完a=1，CPU还会回去继续运行第3条指令，此时a依然是1，这显然是错的。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;下面来看一下表格，你就明白了。
显然在t2时刻发生了中断，导致了t2到t4运行了interrupt_handle函数，t5时刻thread_func又恢复运行，导致interrupt_handle函数中a的操作丢失，因此出错。
方法一：原子操作 拿下单体变量要解决上述场景中的问题，有这样两种思路。一种是把a++变成原子操作，这里的原子是不可分隔的，也就是说要a++这个操作不可分隔，即a++要么不执行，要么一口气执行完；另一种就是控制中断，比如在执行a++之前关掉中断，执行完了之后打开中断。
我们先来看看原子操作，显然靠编译器自动生成原子操作不太可能。第一，编译器没有这么智能，能检测哪个变量需要原子操作；第二，编译器必须要考虑代码的移植性，例如有些硬件平台支持原子操作的机器指令，有的硬件平台不支持原子操作。
既然实现原子操作无法依赖于具体编译器，那就需要我们自己动手，x86平台支持很多原子指令，我们只需要直接应用这些指令，比如原子加、原子减，原子读写等，用汇编代码写出对应的原子操作函数就行了。
好在现代C语言已经支持嵌入汇编代码，可以在C函数中按照特定的方式嵌入汇编代码了，实现原子操作就更方便了，代码如下。
//定义一个原子类型 typedef struct s_ATOMIC{ volatile s32_t a_count; //在变量前加上volatile，是为了禁止编译器优化，使其每次都从内存中加载变量 }atomic_t; //原子读 static inline s32_t atomic_read(const atomic_t v) { //x86平台取地址处是原子 return ((volatile u32_t*)&amp;amp;(v)-&amp;gt;a_count); } //原子写 static inline void atomic_write(atomic_t *v, int i) { //x86平台把一个值写入一个地址处也是原子的 v-&amp;gt;a_count = i; } //原子加上一个整数 static inline void atomic_add(int i, atomic_t *v) { asm volatile(&amp;quot;lock;&amp;quot; &amp;quot;addl %1,%0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;a_count) : &amp;quot;ir&amp;quot; (i)); } //原子减去一个整数 static inline void atomic_sub(int i, atomic_t *v) { asm volatile(&amp;quot;lock;&amp;quot; &amp;quot;subl %1,%0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;a_count) : &amp;quot;ir&amp;quot; (i)); } //原子加1 static inline void atomic_inc(atomic_t *v) { asm volatile(&amp;quot;lock;&amp;quot; &amp;quot;incl %0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;a_count)); } //原子减1 static inline void atomic_dec(atomic_t *v) { asm volatile(&amp;quot;lock;&amp;quot; &amp;quot;decl %0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;a_count)); }</description></item><item><title>09_瞧一瞧Linux：Linux的自旋锁和信号量如何实现？</title><link>https://artisanbox.github.io/9/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/9/</guid><description>你好，我是LMOS。
上节课，我们学习了解决数据同步问题的思路与方法。Linux作为成熟的操作系统内核，当然也有很多数据同步的机制，它也有原子变量、开启和关闭中断、自旋锁、信号量。
那今天我们就来探讨一下这些机制在Linux中的实现。看看Linux的实现和前面我们自己的实现有什么区别，以及Linux为什么要这么实现，这么实现背后的机理是什么。
Linux的原子变量首先，我们一起来看看Linux下的原子变量的实现，在Linux中，有许多共享的资源可能只是一个简单的整型数值。
例如在文件描述符中，需要包含一个简单的计数器。这个计数器表示有多少个应用程序打开了文件。在文件系统的open函数中，将这个计数器变量加1；在close函数中，将这个计数器变量减1。
如果单个进程执行打开和关闭操作，那么这个计数器变量不会出现问题，但是Linux是支持多进程的系统，如果有多个进程同时打开或者关闭文件，那么就可能导致这个计数器变量多加或者少加，出现错误。
为了避免这个问题，Linux提供了一个原子类型变量atomic_t。该变量的定义如下。
typedef struct { int counter; } atomic_t;//常用的32位的原子变量类型 #ifdef CONFIG_64BIT typedef struct { s64 counter; } atomic64_t;//64位的原子变量类型 #endif 上述代码自然不能用普通的代码去读写加减，而是要用Linux专门提供的接口函数去操作，否则就不能保证原子性了，代码如下。
//原子读取变量中的值 static __always_inline int arch_atomic_read(const atomic_t *v) { return __READ_ONCE((v)-&amp;gt;counter); } //原子写入一个具体的值 static __always_inline void arch_atomic_set(atomic_t *v, int i) { __WRITE_ONCE(v-&amp;gt;counter, i); } //原子加上一个具体的值 static __always_inline void arch_atomic_add(int i, atomic_t *v) { asm volatile(LOCK_PREFIX &amp;quot;addl %1,%0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;counter) : &amp;quot;ir&amp;quot; (i) : &amp;quot;memory&amp;quot;); } //原子减去一个具体的值 static __always_inline void arch_atomic_sub(int i, atomic_t *v) { asm volatile(LOCK_PREFIX &amp;quot;subl %1,%0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;counter) : &amp;quot;ir&amp;quot; (i) : &amp;quot;memory&amp;quot;); } //原子加1 static __always_inline void arch_atomic_inc(atomic_t *v) { asm volatile(LOCK_PREFIX &amp;quot;incl %0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;counter) :: &amp;quot;memory&amp;quot;); } //原子减1 static __always_inline void arch_atomic_dec(atomic_t *v) { asm volatile(LOCK_PREFIX &amp;quot;decl %0&amp;quot; : &amp;quot;+m&amp;quot; (v-&amp;gt;counter) :: &amp;quot;memory&amp;quot;); } Linux原子类型变量的操作函数有很多，这里我只是介绍了最基础的几个函数，其它的原子类型变量操作也依赖于上述几个基础的函数。</description></item><item><title>10_设置工作模式与环境（上）：建立计算机</title><link>https://artisanbox.github.io/9/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/10/</guid><description>你好，我是LMOS。
经过前面那么多课程的准备，现在我们距离把我们自己操作系统跑起来，已经是一步之遥了。现在，你是不是很兴奋，很激动？有这些情绪说明你是喜欢这门课程的。
接下来的三节课，我们会一起完成一个壮举，从GRUB老大哥手中接过权柄，让计算机回归到我们的革命路线上来，为我们之后的开发自己的操作系统做好准备。
具体我是这样来安排的，今天这节课，我们先来搭好操作系统的测试环境。第二节课，我们一起实现一个初始化环境的组件——二级引导器，让它真正继承GRUB权力。第三节课，我们正式攻下初始化的第一个山头，对硬件抽象层进行初始化。
好，让我们正式开始今天的学习。首先我们来解决内核文件封装的问题，然后动手一步步建好虚拟机和生产虚拟硬盘。课程配套代码你可以在这里下载。
从内核映像格式说起我们都知道，一个内核工程肯定有多个文件组成，为了不让GRUB老哥加载多个文件，因疲劳过度而产生问题，我们决定让GRUB只加载一个文件。
但是要把多个文件变成一个文件就需要封装，即把多个文件组装在一起形成一个文件。这个文件我们称为内核映像文件，其中包含二级引导器的模块，内核模块，图片和字库文件。为了这映像文件能被GRUB加载，并让它自身能够解析其中的内容，我们就要定义好具体的格式。如下图所示。
上图中的GRUB头有4KB大小，GRUB正是通过这一小段代码，来识别映像文件的。另外，根据映像文件头描述符和文件头描述符里的信息，这一小段代码还可以解析映像文件中的其它文件。
映像文件头描述符和文件描述符是两个C语言结构体，如下所示。
//映像文件头描述符 typedef struct s_mlosrddsc { u64_t mdc_mgic; //映像文件标识 u64_t mdc_sfsum;//未使用 u64_t mdc_sfsoff;//未使用 u64_t mdc_sfeoff;//未使用 u64_t mdc_sfrlsz;//未使用 u64_t mdc_ldrbk_s;//映像文件中二级引导器的开始偏移 u64_t mdc_ldrbk_e;//映像文件中二级引导器的结束偏移 u64_t mdc_ldrbk_rsz;//映像文件中二级引导器的实际大小 u64_t mdc_ldrbk_sum;//映像文件中二级引导器的校验和 u64_t mdc_fhdbk_s;//映像文件中文件头描述的开始偏移 u64_t mdc_fhdbk_e;//映像文件中文件头描述的结束偏移 u64_t mdc_fhdbk_rsz;//映像文件中文件头描述的实际大小 u64_t mdc_fhdbk_sum;//映像文件中文件头描述的校验和 u64_t mdc_filbk_s;//映像文件中文件数据的开始偏移 u64_t mdc_filbk_e;//映像文件中文件数据的结束偏移 u64_t mdc_filbk_rsz;//映像文件中文件数据的实际大小 u64_t mdc_filbk_sum;//映像文件中文件数据的校验和 u64_t mdc_ldrcodenr;//映像文件中二级引导器的文件头描述符的索引号 u64_t mdc_fhdnr;//映像文件中文件头描述符有多少个 u64_t mdc_filnr;//映像文件中文件头有多少个 u64_t mdc_endgic;//映像文件结束标识 u64_t mdc_rv;//映像文件版本 }mlosrddsc_t; #define FHDSC_NMAX 192 //文件名长度 //文件头描述符 typedef struct s_fhdsc { u64_t fhd_type;//文件类型 u64_t fhd_subtype;//文件子类型 u64_t fhd_stuts;//文件状态 u64_t fhd_id;//文件id u64_t fhd_intsfsoff;//文件在映像文件位置开始偏移 u64_t fhd_intsfend;//文件在映像文件的结束偏移 u64_t fhd_frealsz;//文件实际大小 u64_t fhd_fsum;//文件校验和 char fhd_name[FHDSC_NMAX];//文件名 }fhdsc_t; 有了映像文件格式，我们还要有个打包映像的工具，我给你提供了一个Linux命令行下的工具，你只要明白使用方法就可以，如下所示。</description></item><item><title>11_设置工作模式与环境（中）：建造二级引导器</title><link>https://artisanbox.github.io/9/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/11/</guid><description>你好，我是LMOS。
上节课，我们建造了属于我们的“计算机”，并且在上面安装好了GRUB。这节课我会带你一起实现二级引导器这个关键组件。
看到这儿你可能会问，GRUB不是已经把我们的操作系统加载到内存中了吗？我们有了GRUB，我们为什么还要实现二级引导器呢？
这里我要给你说说我的观点，二级引导器作为操作系统的先驱，它需要收集机器信息，确定这个计算机能不能运行我们的操作系统，对CPU、内存、显卡进行一些初级的配置，放置好内核相关的文件。
因为我们二级引导器不是执行具体的加载任务的，而是解析内核文件、收集机器环境信息，它具体收集哪些信息，我会在下节课详细展开。
设计机器信息结构二级引导器收集的信息，需要地点存放，我们需要设计一个数据结构。信息放在这个数据结构中，这个结构放在内存1MB的地方，方便以后传给我们的操作系统。
为了让你抓住重点，我选取了这个数据结构的关键代码，这里并没有列出该结构的所有字段（Cosmos/initldr/include/ldrtype.h），这个结构如下所示。
typedef struct s_MACHBSTART { u64_t mb_krlinitstack;//内核栈地址 u64_t mb_krlitstacksz;//内核栈大小 u64_t mb_imgpadr;//操作系统映像 u64_t mb_imgsz;//操作系统映像大小 u64_t mb_bfontpadr;//操作系统字体地址 u64_t mb_bfontsz;//操作系统字体大小 u64_t mb_fvrmphyadr;//机器显存地址 u64_t mb_fvrmsz;//机器显存大小 u64_t mb_cpumode;//机器CPU工作模式 u64_t mb_memsz;//机器内存大小 u64_t mb_e820padr;//机器e820数组地址 u64_t mb_e820nr;//机器e820数组元素个数 u64_t mb_e820sz;//机器e820数组大小 //…… u64_t mb_pml4padr;//机器页表数据地址 u64_t mb_subpageslen;//机器页表个数 u64_t mb_kpmapphymemsz;//操作系统映射空间大小 //…… graph_t mb_ghparm;//图形信息 }__attribute__((packed)) machbstart_t; 规划二级引导器在开始写代码之前，我们先来从整体划分一下二级引导器的功能模块，从全局了解下功能应该怎么划分，这里我特意为你梳理了一个表格。
前面表格里的这些文件，我都放在了课程配套源码中了，你可以从这里下载。
上述这些文件都在lesson10～11/Cosmos/initldr/ldrkrl目录中，它们在编译之后会形成三个文件，编译脚本我已经写好了，下面我们用一幅图来展示这个编译过程。
这最后三个文件用我们前面说的映像工具打包成映像文件，其指令如下。
lmoskrlimg -m k -lhf initldrimh.bin -o Cosmos.eki -f initldrkrl.bin initldrsve.bin 实现GRUB头我们的GRUB头有两个文件组成，一个imginithead.asm汇编文件，它有两个功能，既能让GRUB识别，又能设置C语言运行环境，用于调用C函数；第二就是inithead.c文件，它的主要功能是查找二级引导器的核心文件——initldrkrl.bin，然后把它放置到特定的内存地址上。
我们先来实现imginithead.asm，它主要工作是初始化CPU的寄存器，加载GDT，切换到CPU的保护模式，我们一步一步来实现。
首先是GRUB1和GRUB2需要的两个头结构，代码如下。
MBT_HDR_FLAGS EQU 0x00010003 MBT_HDR_MAGIC EQU 0x1BADB002 MBT2_MAGIC EQU 0xe85250d6 global _start extern inithead_entry [section .</description></item><item><title>12_设置工作模式与环境（下）：探查和收集信息</title><link>https://artisanbox.github.io/9/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/12/</guid><description>你好，我是LMOS。
上节课我们动手实现了自己的二级引导器。今天这节课我们将进入二级引导器，完成具体工作的环节。
在二级引导器中，我们要检查CPU是否支持64位的工作模式、收集内存布局信息，看看是不是合乎我们操作系统的最低运行要求，还要设置操作系统需要的MMU页表、设置显卡模式、释放中文字体文件。
今天课程的配套代码，你可以点击这里，自行下载。
检查与收集机器信息如果ldrkrl_entry()函数是总裁，那么init_bstartparm()函数则是经理，它负责管理检查CPU模式、收集内存信息，设置内核栈，设置内核字体、建立内核MMU页表数据。
为了使代码更加清晰，我们并不直接在ldrkrl_entry()函数中搞事情，而是准备在另一个bstartparm.c文件中实现一个init_bstartparm()。
下面我们就来动手实现它，如下所示。
//初始化machbstart_t结构体，清0,并设置一个标志 void machbstart_t_init(machbstart_t* initp) { memset(initp,0,sizeof(machbstart_t)); initp-&amp;gt;mb_migc=MBS_MIGC; return; } void init_bstartparm() { machbstart_t* mbsp = MBSPADR;//1MB的内存地址 machbstart_t_init(mbsp); return; } 目前我们的经理init_bstartparm()函数只是调用了一个machbstart_t_init()函数，在1MB内存地址处初始化了一个机器信息结构machbstart_t，后面随着干活越来越多，还会调用更多的函数的。
检查CPU首先要检查我们的CPU，因为它是执行程序的关键。我们要搞清楚它能执行什么形式的代码，支持64位长模式吗？
这个工作我们交给init_chkcpu()函数来干，由于我们要CPUID指令来检查CPU是否支持64位长模式，所以这个函数中需要找两个帮工：chk_cpuid、chk_cpu_longmode来干两件事，一个是检查CPU否支持CPUID指令，然后另一个用CPUID指令检查CPU支持64位长模式。
下面我们去写好它们，如下所示。
//通过改写Eflags寄存器的第21位，观察其位的变化判断是否支持CPUID int chk_cpuid() { int rets = 0; __asm__ __volatile__( &amp;quot;pushfl \n\t&amp;quot; &amp;quot;popl %%eax \n\t&amp;quot; &amp;quot;movl %%eax,%%ebx \n\t&amp;quot; &amp;quot;xorl $0x0200000,%%eax \n\t&amp;quot; &amp;quot;pushl %%eax \n\t&amp;quot; &amp;quot;popfl \n\t&amp;quot; &amp;quot;pushfl \n\t&amp;quot; &amp;quot;popl %%eax \n\t&amp;quot; &amp;quot;xorl %%ebx,%%eax \n\t&amp;quot; &amp;quot;jz 1f \n\t&amp;quot; &amp;quot;movl $1,%0 \n\t&amp;quot; &amp;quot;jmp 2f \n\t&amp;quot; &amp;quot;1: movl $0,%0 \n\t&amp;quot; &amp;quot;2: \n\t&amp;quot; : &amp;quot;=c&amp;quot;(rets) : :); return rets; } //检查CPU是否支持长模式 int chk_cpu_longmode() { int rets = 0; __asm__ __volatile__( &amp;quot;movl $0x80000000,%%eax \n\t&amp;quot; &amp;quot;cpuid \n\t&amp;quot; //把eax中放入0x80000000调用CPUID指令 &amp;quot;cmpl $0x80000001,%%eax \n\t&amp;quot;//看eax中返回结果 &amp;quot;setnb %%al \n\t&amp;quot; //不为0x80000001,则不支持0x80000001号功能 &amp;quot;jb 1f \n\t&amp;quot; &amp;quot;movl $0x80000001,%%eax \n\t&amp;quot; &amp;quot;cpuid \n\t&amp;quot;//把eax中放入0x800000001调用CPUID指令，检查edx中的返回数据 &amp;quot;bt $29,%%edx \n\t&amp;quot; //长模式 支持位 是否为1 &amp;quot;setcb %%al \n\t&amp;quot; &amp;quot;1: \n\t&amp;quot; &amp;quot;movzx %%al,%%eax \n\t&amp;quot; : &amp;quot;=a&amp;quot;(rets) : :); return rets; } //检查CPU主函数 void init_chkcpu(machbstart_t *mbsp) { if (!</description></item><item><title>13_第一个C函数：如何实现板级初始化？</title><link>https://artisanbox.github.io/9/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/13/</guid><description>你好，我是LMOS。
前面三节课，我们为调用Cosmos的第一个C函数hal_start做了大量工作。这节课我们要让操作系统Cosmos里的第一个C函数真正跑起来啦，也就是说，我们会真正进入到我们的内核中。
今天我们会继续在这个hal_start函数里，首先执行板级初始化，其实就是hal层（硬件抽象层，下同）初始化，其中执行了平台初始化，hal层的内存初始化，中断初始化，最后进入到内核层的初始化。
这节课的配套代码，你可以从这里下载。
第一个C函数任何软件工程，第一个函数总是简单的，因为它是总调用者，像是一个管理者，坐在那里发号施令，自己却是啥活也不干。
由于这是第一个C函数，也是初始化函数，我们还是要为它单独建立一个文件，以显示对它的尊重，依然在Cosmos/hal/x86/下建立一个hal_start.c文件。写上这样一个函数。
void hal_start() { //第一步：初始化hal层 //第二步：初始化内核层 for(;;); return; } 根据前面的设计，Cosmos是有hal层和内核层之分，所以在上述代码中，要分两步走。第一步是初始化hal层；第二步，初始化内核层。只是这两步的函数我们还没有写。
然而最后的死循环却有点奇怪，其实它的目的很简单，就是避免这个函数返回，因为这个返回了就无处可去，避免走回头路。
hal层初始化为了分离硬件的特性，我们设计了hal层，把硬件相关的操作集中在这个层，并向上提供接口，目的是让内核上层不用关注硬件相关的细节，也能方便以后移植和扩展。(关于hal层的设计，可以回顾第3节课)
也许今天我们是在x86平台上写Cosmos，明天就要在ARM平台上开发Cosmos，那时我们就可以写个ARM平台的hal层，来替换Cosmos中的x86平台的hal层。
下面我们在Cosmos/hal/x86/下建立一个halinit.c文件，写出hal层的初始化函数。
void init_hal() { //初始化平台 //初始化内存 //初始化中断 return; } 这个函数也是一个调用者，没怎么干活。不过根据代码的注释能看出，它调用的函数多一点，但主要是完成初始化平台、初始化内存、初始化中断的功能函数。
初始化平台我们先来写好平台初始化函数，因为它需要最先被调用。
这个函数主要负责完成两个任务，一是把二级引导器建立的机器信息结构复制到hal层中的一个全局变量中，方便内核中的其它代码使用里面的信息，之后二级引导器建立的数据所占用的内存都会被释放。二是要初始化图形显示驱动，内核在运行过程要在屏幕上输出信息。
下面我们在Cosmos/hal/x86/下建立一个halplatform.c文件，写上如下代码。
void machbstart_t_init(machbstart_t *initp) { //清零 memset(initp, 0, sizeof(machbstart_t)); return; } void init_machbstart() { machbstart_t *kmbsp = &amp;amp;kmachbsp; machbstart_t *smbsp = MBSPADR;//物理地址1MB处 machbstart_t_init(kmbsp); //复制，要把地址转换成虚拟地址 memcopy((void *)phyadr_to_viradr((adr_t)smbsp), (void *)kmbsp, sizeof(machbstart_t)); return; } //平台初始化函数 void init_halplaltform() { //复制机器信息结构 init_machbstart(); //初始化图形显示驱动 init_bdvideo(); return; } 这个代码中别的地方很好理解，就是kmachbsp你可能会有点奇怪，它是个结构体变量，结构体类型是machbstart_t，这个结构和二级引导器所使用的一模一样。</description></item><item><title>14_Linux初始化（上）：GRUB与vmlinuz的结构</title><link>https://artisanbox.github.io/9/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/14/</guid><description>你好，我是LMOS。
在前面的课程中，我们建好了二级引导器，启动了我们的Cosmos，并进行了我们Cosmos的Hal层初始化。
我会用两节课带你领会Linux怎样做初始化。虽然我们自己具体实现过了初始化，不过我们也不妨看看Linux的初始化流程，借鉴一下Linux开发者的玩法。
这节课，我会先为你梳理启动的整体流程，重点为你解读Linux上GRUB是怎样启动，以及内核里的“实权人物”——vmlinuz内核文件是如何产生和运转的。下节课，我们从setup.bin文件的_start函数入手，研究Linux初始化流程。
好，接下来我们从全局流程讲起，正式进入今天的学习。
全局流程x86平台的启动流程，是非常复杂的。为了帮助你理解，我们先从全局粗略地看一看整体流程，然后一步步细化。
在机器加电后，BIOS会进行自检，然后由BIOS加载引导设备中引导扇区。在安装有Linux操作系统的情况下，在引导扇区里，通常是安装的GRUB的一小段程序（安装windows的情况则不同）。最后，GRUB会加载Linux的内核映像vmlinuz，如下图所示。
上图中的引导设备通常是机器中的硬盘，但也可以是U盘或者光盘甚至是软盘。BIOS会自动读取保存在CMOS中的引导设备信息。
从BIOS到GRUB从前面的课程我们已经知道，CPU被设计成只能运行内存中的程序，没有办法直接运行储存在硬盘或者U盘中的操作系统程序。
如果想要运行硬盘或者U盘中的程序，就必须要先加载到内存（RAM）中才能运行。这是因为硬盘、U盘（外部储存器）并不和CPU直接相连，它们的访问机制和寻址方式与内存截然不同。
内存在断电后就没法保存数据了，那BIOS又是如何启动的呢？硬件工程师设计CPU时，硬性地规定在加电的瞬间，强制将CS寄存器的值设置为0XF000，IP寄存器的值设置为0XFFF0。
这样一来，CS:IP就指向了0XFFFF0这个物理地址。在这个物理地址上连接了主板上的一块小的ROM芯片。这种芯片的访问机制和寻址方式和内存一样，只是它在断电时不会丢失数据，在常规下也不能往这里写入数据，它是一种只读内存，BIOS程序就被固化在该ROM芯片里。
现在，CS:IP指向了0XFFFF0这个位置，正是BIOS程序的入口地址。这意味着BIOS正式开始启动。
BIOS一开始会初始化CPU，接着检查并初始化内存，然后将自己的一部分复制到内存，最后跳转到内存中运行。BIOS的下一步就是枚举本地设备进行初始化，并进行相关的检查，检查硬件是否损坏，这期间BIOS会调用其它设备上的固件程序，如显卡、网卡等设备上的固件程序。
当设备初始化和检查步骤完成之后，BIOS会在内存中建立中断表和中断服务程序，这是启动Linux至关重要的工作，因为Linux会用到它们。
具体是怎么操作的呢？BIOS会从内存地址（0x00000）开始用1KB的内存空间（0x00000~0x003FF）构建中断表，在紧接着中断表的位置，用256KB的内存空间构建BIOS数据区（0x00400~0x004FF），并在0x0e05b的地址加载了8KB大小的与中断表对应的中断服务程序。
中断表中有256个条目，每个条目占用4个字节，其中两个字节是CS寄存器的值，两个字节是IP寄存器的值。每个条目都指向一个具体的中断服务程序。
为了启动外部储存器中的程序，BIOS会搜索可引导的设备，搜索的顺序是由CMOS中的设置信息决定的（这也是我们平时讲的，所谓的在BIOS中设置的启动设备顺序）。一个是软驱，一个是光驱，一个是硬盘上，还可以是网络上的设备甚至是一个usb 接口的U盘，都可以作为一个启动设备。
当然，Linux通常是从硬盘中启动的。硬盘上的第1个扇区（每个扇区512字节空间），被称为MBR（主启动记录），其中包含有基本的GRUB启动程序和分区表，安装GRUB时会自动写入到这个扇区，当MBR被BIOS装载到0x7c00地址开始的内存空间中后，BIOS就会将控制权转交给了MBR。在当前的情况下，其实是交给了GRUB。
到这里，BIOS到GRUB的过程结束。
GRUB是如何启动的根据前面内容可以发现，BIOS只会加载硬盘上的第1个扇区。不过这个扇区仅有512字节，这512字节中还有64字节的分区表加2字节的启动标志，很显然，剩下446字节的空间，是装不下GRUB这种大型通用引导器的。
于是，GRUB的加载分成了多个步骤，同时GRUB也分成了多个文件，其中有两个重要的文件boot.img和core.img，如下所示：
其中，boot.img被GRUB的安装程序写入到硬盘的MBR中，同时在boot.img文件中的一个位置写入core.img文件占用的第一个扇区的扇区号。
而core.img文件是由GRUB安装程序根据安装时环境信息，用其它GRUB的模块文件动态生成。如下图所示：
如果是从硬盘启动的话，core.img中的第一个扇区的内容就是diskboot.img文件。diskboot.img文件的作用是，读取core.img中剩余的部分到内存中。
由于这时diskboot.img文件还不识别文件系统，所以我们将core.img文件的全部位置，都用文件块列表的方式保存到diskboot.img文件中。这样就能确保diskboot.img文件找到core.img文件的剩余内容，最后将控制权交给kernel.img文件。
因为这时core.img文件中嵌入了足够多的功能模块，所以可以保证GRUB识别出硬盘分区上文件系统，能够访问/boot/grub目录，并且可以加载相关的配置文件和功能模块，来实现相关的功能，例如加载启动菜单、加载目标操作系统等。
正因为GRUB2大量使用了动态加载功能模块，这使得core.img文件的体积变得足够小。而GRUB的core.img文件一旦开始工作，就可以加载Linux系统的vmlinuz内核文件了。
详解vmlinuz文件结构我们在/boot目录下会发现vmlinuz文件，这个文件是怎么来的呢？
其实它是由Linux编译生成的bzImage文件复制而来的，你自己可以下载最新的Linux代码.
我们一致把Linux源码解压到一个linux目录中，也就是说我们后面查找Linux源代码文件总是从linux目录开始的，切换到代码目录执行make ARCH=x86_64，再执行make install，就会产生vmlinuz文件，你可以参考后面的makefile代码。
#linux/arch/x86/boot/Makefile install: sh $(srctree)/$(src)/install.sh $(KERNELRELEASE) $(obj)/bzImage \ System.map &amp;quot;$(INSTALL_PATH)&amp;quot; install.sh脚本文件只是完成复制的功能，所以我们只要搞懂了bzImage文件结构，就等同于理解了vmlinuz文件结构。
那么bzImage文件又是怎么来的呢？我们只要研究bzImage文件在Makefile中的生成规则，就会恍然大悟，代码如下 ：
#linux/arch/x86/boot/Makefile $(obj)/bzImage: $(obj)/setup.bin $(obj)/vmlinux.bin $(obj)/tools/build FORCE $(call if_changed,image) @$(kecho) 'Kernel: $@ is ready' ' (#'`cat .version`')' 从前面的代码可以知道，生成bzImage文件需要三个依赖文件：setup.bin、vmlinux.bin，linux/arch/x86/boot/tools目录下的build。让我们挨个来分析一下。
其实，build只是一个HOSTOS（正在使用的Linux）下的应用程序，它的作用就是将setup.bin、vmlinux.bin两个文件拼接成一个bzImage文件，如下图所示：
剩下的就是搞清楚setup.bin、vmlinux.bin这两个文件的的结构，先来看看setup.bin文件，setup.bin文件是由objcopy命令根据setup.elf生成的。
setup.elf文件又怎么生成的呢？我们结合后面的代码来看看。
#这些目标文件正是由/arch/x86/boot/目录下对应的程序源代码文件编译产生 setup-y += a20.</description></item><item><title>15_Linux初始化（下）：从_start到第一个进程</title><link>https://artisanbox.github.io/9/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/15/</guid><description>你好，我是LMOS。
今天我们继续来研究Linux的初始化流程，为你讲解如何解压内核，然后讲解Linux内核第一个C函数。最后，我们会用Linux的第一个用户进程的建立来收尾。
如果用你上手去玩一款新游戏做类比的话，那么上节课只是新手教程，而这节课就是更深入的实战了。后面你会看到很多熟悉的“面孔”，像是我们前面讲过的CPU工作模式、MMU页表等等基础知识，这节课都会得到运用。
解压后内核初始化下面，我们先从setup.bin文件的入口_start开始，了解启动信息结构，接着由16位main函数切换CPU到保护模式，然后跳入vmlinux.bin文件中的startup_32函数重新加载段描述符。
如果是64位的系统，就要进入startup_64函数，切换到CPU到长模式，最后调用extract_kernel函数解压Linux内核，并进入内核的startup_64函数，由此Linux内核开始运行。
为何要从_start开始通过上节课对vmlinuz文件结构的研究，我们已经搞清楚了其中的vmlinux.bin是如何产生的，它是由linux/arch/x86/boot/compressed目录下的一些目标文件，以及piggy.S包含的一个vmlinux.bin.gz的压缩文件一起生成的。
vmlinux.bin.gz文件则是由编译的Linux内核所生成的elf格式的vmlinux文件，去掉了文件的符号信息和重定位信息后，压缩得到的。
CPU是无法识别压缩文件中的指令直接运行的，必须先进行解压后，然后解析elf格式的文件，把其中的指令段和数据段加载到指定的内存空间中，才能由CPU执行。
这就需要用到前面的setup.bin文件了，_start正是setup.bin文件的入口，在head.S文件中定义，代码如下。
#linux/arch/x86/boot/head.S .code16 .section &amp;quot;.bstext&amp;quot;, &amp;quot;ax&amp;quot; .global bootsect_start bootsect_start: ljmp $BOOTSEG, $start2 start2: #…… #这里的512字段bootsector对于硬盘启动是用不到的 #…… .globl _start _start: .byte 0xeb # short (2-byte) jump .byte start_of_setup-1f #这指令是用.byte定义出来的，跳转start_of_setup-1f #…… #这里是一个庞大的数据结构，没展示出来，与linux/arch/x86/include/uapi/asm/bootparam.h文件中的struct setup_header一一对应。这个数据结构定义了启动时所需的默认参数 #…… start_of_setup: movw %ds, %ax movw %ax, %es #ds = es cld #主要指定si、di寄存器的自增方向，即si++ di++ movw %ss, %dx cmpw %ax, %dx # ds 是否等于 ss movw %sp, %dx je 2f # 如果ss为空则建立新栈 movw $_end, %dx testb $CAN_USE_HEAP, loadflags jz 1f movw heap_end_ptr, %dx 1: addw $STACK_SIZE, %dx jnc 2f xorw %dx, %dx 2: andw $~3, %dx jnz 3f movw $0xfffc, %dx 3: movw %ax, %ss movzwl %dx, %esp sti # 栈已经初始化好，开中断 pushw %ds pushw $6f lretw # cs=ds ip=6：跳转到标号6处 6: cmpl $0x5a5aaa55, setup_sig #检查setup标记 jne setup_bad movw $__bss_start, %di movw $_end+3, %cx xorl %eax, %eax subw %di, %cx shrw $2, %cx rep; stosl #清空setup程序的bss段 calll main #调用C语言main函数 setup_header结构下面我们重点研究一下setup_header结构，这对我们后面的流程很关键。它定义在linux/arch/x86/include/uapi/asm/bootparam.</description></item><item><title>16_划分土地（上）：如何划分与组织内存？</title><link>https://artisanbox.github.io/9/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/16/</guid><description>你好，我是LMOS。
内存跟操作系统的关系，就像土地和政府的关系一样。政府必须合理规划这个国家的土地，才能让人民安居乐业。为了发展，政府还要进而建立工厂、学校，发展工业和教育，规划城镇，国家才能繁荣富强。
而作为计算机的实际掌权者，操作系统必须科学合理地管理好内存，应用程序才能高效稳定地运行。
内存管理是一项复杂的工作，我会用三节课带你搞定它。
具体我是这么安排的：这节课，我们先解决内存的划分方式和内存页的表示、组织问题，设计好数据结构。下一节课，我会带你在内存中建立数据结构对应的实例变量，搞定内存页的初始化问题。最后一节课，我们会依赖前面建好的数据结构，实现内存页面管理算法。
好，今天我们先从内存的划分单位讲起，一步步为内存管理工作做好准备。
今天课程的配套代码，你可以点击这里，自行下载。
分段还是分页要划分内存，我们就要先确定划分的单位是按段还是按页，就像你划分土地要选择按亩还是按平方分割一样。
其实分段与分页的优缺点，前面MMU相关的课程已经介绍过了。这里我们从内存管理角度，理一理分段与分页的问题。
第一点，从表示方式和状态确定角度考虑。段的长度大小不一，用什么数据结构表示一个段，如何确定一个段已经分配还是空闲呢？而页的大小固定，我们只需用位图就能表示页的分配与释放。
比方说，位图中第1位为1，表示第一个页已经分配；位图中第2位为0，表示第二个页是空闲，每个页的开始地址和大小都是固定的。
第二点，从内存碎片的利用看，由于段的长度大小不一，更容易产生内存碎片，例如内存中有A段（内存地址：0～5000）、 B段（内存地址：5001～8000）、C段（内存地址：8001～9000），这时释放了B段，然后需要给D段分配内存空间，且D段长度为5000。
你立马就会发现A段和C段之间的空间（B段）不能满足，只能从C段之后的内存空间开始分配，随着程序运行，这些情况会越来越多。段与段之间存在着不大不小的空闲空间，内存总的空闲空间很多，但是放不下一个新段。
而页的大小固定，分配最小单位是页，页也会产生碎片，比如我需要请求分配4个页，但在内存中从第1～3个页是空闲的，第4个页是分配出去了，第5个页是空闲的。这种情况下，我们通过修改页表的方式，就能让连续的虚拟页面映射到非连续的物理页面。
第三点，从内存和硬盘的数据交换效率考虑，当内存不足时，操作系统希望把内存中的一部分数据写回硬盘，来释放内存。这就涉及到内存和硬盘交换数据，交换单位是段还是页？
如果是段的话，其大小不一，A段有50MB，B段有1KB，A、B段写回硬盘的时间也不同，有的段需要时间长，有的段需要时间短，硬盘的空间分配也会有上面第二点同样的问题，这样会导致系统性能抖动。如果每次交换一个页，则没有这些问题。
还有最后一点，段最大的问题是使得虚拟内存地址空间，难于实施。（后面的课还会展开讲）
综上，我们自然选择分页模式来管理内存，其实现在所有的商用操作系统都使用了分页模式管理内存。我们用4KB作为页大小，这也正好对应x86 CPU长模式下MMU 4KB的分页方式。
如何表示一个页我们使用分页模型来管理内存。首先是把物理内存空间分成4KB大小页，这页表示从地址x开始到x+0xFFF这一段的物理内存空间，x必须是0x1000对齐的。这一段x+0xFFF的内存空间，称为内存页。
在逻辑上的结构图如下所示：
上图这是一个接近真实机器的情况，不过一定不要忘记前面的内存布局示图，真实的物理内存地址空间不是连续的，这中间可能有空洞，可能是显存，也可能是外设的寄存器。
真正的物理内存空间布局信息来源于e820map_t结构数组，之前的初始化中，我们已经将其转换成phymmarge_t结构数组了，由 kmachbsp-&amp;gt;mb_e820expadr指向。
那问题来了，现在我们已经搞清楚了什么是页，但如何表示一个页呢？
你可能会想到位图或者整型变量数组，用其中一个位代表一个页，位值为0时表示页空闲，位值为1时表示页已分配；或者用整型数组中一个元素表示一个页，用具体数组元素的数值代表页的状态。
如果这样的话，分配、释放内存页的算法就确定了，就是扫描位图或者扫描数组。这样确实可以做出最简单的内存页管理器，但这也是最低效的。
上面的方案之所以低效，是因为我们仅仅只是保存了内存页的空闲和已分配的信息，这是不够的。我们的Cosmos当然不能这么做，我们需要页的状态、页的地址、页的分配记数、页的类型、页的链表，你自然就会想到，这些信息可以用一个C语言结构体封装起来。
让我们马上就来实现这个结构体，在cosmos/include/halinc/下建立一个msadsc_t.h文件，在其中实现这个结构体，代码如下所示。
//内存空间地址描述符标志 typedef struct s_MSADFLGS { u32_t mf_olkty:2; //挂入链表的类型 u32_t mf_lstty:1; //是否挂入链表 u32_t mf_mocty:2; //分配类型，被谁占用了，内核还是应用或者空闲 u32_t mf_marty:3; //属于哪个区 u32_t mf_uindx:24; //分配计数 }__attribute__((packed)) msadflgs_t; //物理地址和标志 typedef struct s_PHYADRFLGS { u64_t paf_alloc:1; //分配位 u64_t paf_shared:1; //共享位 u64_t paf_swap:1; //交换位 u64_t paf_cache:1; //缓存位 u64_t paf_kmap:1; //映射位 u64_t paf_lock:1; //锁定位 u64_t paf_dirty:1; //脏位 u64_t paf_busy:1; //忙位 u64_t paf_rv2:4; //保留位 u64_t paf_padrs:52; //页物理地址位 }__attribute__((packed)) phyadrflgs_t; //内存空间地址描述符 typedef struct s_MSADSC { list_h_t md_list; //链表 spinlock_t md_lock; //保护自身的自旋锁 msadflgs_t md_indxflgs; //内存空间地址描述符标志 phyadrflgs_t md_phyadrs; //物理地址和标志 void* md_odlink; //相邻且相同大小msadsc的指针 }__attribute__((packed)) msadsc_t; msadsc_t结构看似很大，实则很小，也必须要小，因为它表示一个页面，物理内存页有多少就需要有多少个msadsc_t结构。正是因为页面地址总是按4KB对齐，所以phyadrflgs_t结构的低12位才可以另作它用。</description></item><item><title>17_划分土地（中）：如何实现内存页面初始化？</title><link>https://artisanbox.github.io/9/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/17/</guid><description>你好，我是LMOS。
上节课，我们确定了用分页方式管理内存，并且一起动手设计了表示内存页、内存区相关的内存管理数据结构。不过，虽然内存管理相关的数据结构已经定义好了，但是我们还没有在内存中建立对应的实例变量。
我们都知道，在代码中实际操作的数据结构必须在内存中有相应的变量，这节课我们就去建立对应的实例变量，并初始化它们。
初始化前面的课里，我们在hal层初始化中，初始化了从二级引导器中获取的内存布局信息，也就是那个e820map_t数组，并把这个数组转换成了phymmarge_t结构数组，还对它做了排序。
但是，我们Cosmos物理内存管理器剩下的部分还没有完成初始化，下面我们就去实现它。
Cosmos的物理内存管理器，我们依然要放在Cosmos的hal层。
因为物理内存还和硬件平台相关，所以我们要在cosmos/hal/x86/目录下建立一个memmgrinit.c文件，在这个文件中写入一个Cosmos物理内存管理器初始化的大总管——init_memmgr函数，并在init_halmm函数中调用它，代码如下所示。
//cosmos/hal/x86/halmm.c中 //hal层的内存初始化函数 void init_halmm() { init_phymmarge(); init_memmgr(); return; } //Cosmos物理内存管理器初始化 void init_memmgr() { //初始化内存页结构msadsc_t //初始化内存区结构memarea_t return; } 根据前面我们对内存管理相关数据结构的设计，你应该不难想到，在init_memmgr函数中应该要完成内存页结构msadsc_t和内存区结构memarea_t的初始化，下面就分别搞定这两件事。
内存页结构初始化内存页结构的初始化，其实就是初始化msadsc_t结构对应的变量。因为一个msadsc_t结构体变量代表一个物理内存页，而物理内存由多个页组成，所以最终会形成一个msadsc_t结构体数组。
这会让我们的工作变得简单，我们只需要找一个内存地址，作为msadsc_t结构体数组的开始地址，当然这个内存地址必须是可用的，而且之后内存空间足以存放msadsc_t结构体数组。
然后，我们要扫描phymmarge_t结构体数组中的信息，只要它的类型是可用内存，就建立一个msadsc_t结构体，并把其中的开始地址作为第一个页面地址。
接着，要给这个开始地址加上0x1000，如此循环，直到其结束地址。
当这个phymmarge_t结构体的地址区间，它对应的所有msadsc_t结构体都建立完成之后，就开始下一个phymmarge_t结构体。依次类推，最后，我们就能建好所有可用物理内存页面对应的msadsc_t结构体。
下面，我们去cosmos/hal/x86/目录下建立一个msadsc.c文件。在这里写下完成这些功能的代码，如下所示。
void write_one_msadsc(msadsc_t *msap, u64_t phyadr) { //对msadsc_t结构做基本的初始化，比如链表、锁、标志位 msadsc_t_init(msap); //这是把一个64位的变量地址转换成phyadrflgs_t*类型方便取得其中的地址位段 phyadrflgs_t *tmp = (phyadrflgs_t *)(&amp;amp;phyadr); //把页的物理地址写入到msadsc_t结构中 msap-&amp;gt;md_phyadrs.paf_padrs = tmp-&amp;gt;paf_padrs; return; } u64_t init_msadsc_core(machbstart_t *mbsp, msadsc_t *msavstart, u64_t msanr) { //获取phymmarge_t结构数组开始地址 phymmarge_t *pmagep = (phymmarge_t *)phyadr_to_viradr((adr_t)mbsp-&amp;gt;mb_e820expadr); u64_t mdindx = 0; //扫描phymmarge_t结构数组 for (u64_t i = 0; i &amp;lt; mbsp-&amp;gt;mb_e820exnr; i++) { //判断phymmarge_t结构的类型是不是可用内存 if (PMR_T_OSAPUSERRAM == pmagep[i].</description></item><item><title>18_划分土地（下）：如何实现内存页的分配与释放？</title><link>https://artisanbox.github.io/9/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/18/</guid><description>你好，我是LMOS。
通过前面两节课的学习，我们已经组织好了内存页，也初始化了内存页和内存区。我们前面做了这么多准备工作，就是为了实现分配和释放内存页面，达到内存管理的目的。
那有了前面的基础，我想你自己也能大概实现这个分配和释放的代码。但是，根据前面我们设计的数据结构和对其初始化的工作，估计你也可以隐约感觉到，我们的内存管理的算法还是有一点点难度的。
今天这节课，就让我们一起来实现这项富有挑战性的任务吧！这节课的配套代码，你可以通过这里下载。
内存页的分配如果让你实现一次只分配一个页面，我相信这个问题很好解决，因为你只需要写一段循环代码，在其中遍历出一个空闲的msadsc_t结构，就可以返回了，这个算法就可以结束了。
但现实却不容许我们这么简单地处理问题，我们内存管理器要为内核、驱动，还有应用提供服务，它们对请求内存页面的多少、内存页面是不是连续，内存页面所处的物理地址都有要求。
这样一来，问题就复杂了。不过你也不必担心，我们可以从内存分配的接口函数下手。
下面我们根据上述要求来设计实现内存分配接口函数。我们还是先来建立一个新的C语言代码文件，在cosmos/hal/x86目录中建立一个memdivmer.c文件，在其中写一个内存分配接口函数，代码如下所示。
//内存分配页面框架函数 msadsc_t *mm_divpages_fmwk(memmgrob_t *mmobjp, uint_t pages, uint_t *retrelpnr, uint_t mrtype, uint_t flgs) { //返回mrtype对应的内存区结构的指针 memarea_t *marea = onmrtype_retn_marea(mmobjp, mrtype); if (NULL == marea) { *retrelpnr = 0; return NULL; } uint_t retpnr = 0; //内存分配的核心函数 msadsc_t *retmsa = mm_divpages_core(marea, pages, &amp;amp;retpnr, flgs); if (NULL == retmsa) { *retrelpnr = 0; return NULL; } *retrelpnr = retpnr; return retmsa; } //内存分配页面接口
//mmobjp-&amp;gt;内存管理数据结构指针 //pages-&amp;gt;请求分配的内存页面数 //retrealpnr-&amp;gt;存放实际分配内存页面数的指针 //mrtype-&amp;gt;请求的分配内存页面的内存区类型 //flgs-&amp;gt;请求分配的内存页面的标志位 msadsc_t *mm_division_pages(memmgrob_t *mmobjp, uint_t pages, uint_t *retrealpnr, uint_t mrtype, uint_t flgs) { if (NULL == mmobjp || NULL == retrealpnr || 0 == mrtype) { return NULL; }</description></item><item><title>19_土地不能浪费：如何管理内存对象？</title><link>https://artisanbox.github.io/9/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/19/</guid><description>你好，我是LMOS。
在前面的课程中，我们建立了物理内存页面管理器，它既可以分配单个页面，也可以分配多个连续的页面，还能指定在特殊内存地址区域中分配页面。
但你发现没有，物理内存页面管理器一次分配至少是一个页面，而我们对内存分页是一个页面4KB，即4096字节。对于小于一个页面的内存分配请求，它无能为力。如果要实现小于一个页面的内存分配请求，又该怎么做呢？
这节课我们就一起来解决这个问题。课程配套代码，你可以从这里获得。
malloc给我们的启发首先，我想和你说说，为什么小于一个页面的内存我们也要格外珍惜？
如果你在大学学过C程序设计语言的话，相信你对C库中的malloc函数也不会陌生，它负责完成分配一块内存空间的功能。
下面的代码。我相信你也写过，或者写过类似的，不用多介绍你也可以明白。
#include &amp;lt;stdio.h&amp;gt; #include &amp;lt;string.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; int main() { char *str; //内存分配 存放15个char字符类型 str = (char *) malloc(15); if (str == NULL) { printf(&amp;quot;mem alloc err\n&amp;quot;); return -1; } //把hello world字符串复制到str开始的内存地址空间中 strcpy(str, &amp;quot;hello world&amp;quot;); //打印hello world字符串和它的地址 printf(&amp;quot;String = %s, Address = %u\n&amp;quot;, str, str); //释放分配的内存 free(str); return(0); } 这个代码流程很简单，就是分配一块15字节大小的内存空间，然后把字符串复制到分配的内存空间中，最后用字符串的形式打印了那个块内存，最后释放该内存空间。
但我们并不是要了解malloc、free函数的工作原理，而是要清楚，像这样分配几个字节内存空间的操作，这在内核中比比皆是。
页还能细分吗是的，单从内存角度来看，页最小是以字节为单位的。但是从MMU角度看，内存是以页为单位的，所以我们的Cosmos的物理内存分配器也以页为单位。现在的问题是，内核中有大量远小于一个页面的内存分配请求，如果对此还是分配一个页面，就会浪费内存。
要想解决这个问题，就要细分“页”这个单位。虽然从MMU角度来看，页不能细分，但是从软件逻辑层面页可以细分，但是如何分，则十分讲究。
结合历史经验和硬件特性（Cache行大小）来看，我们可以把一个页面或者连续的多个页面，分成32字节、64字节、128字节、256字节、512字节、1024字节、2048字节、4096字节（一个页）。这些都是Cache行大小的倍数。我们给这些小块内存取个名字，叫内存对象。
我们可以这样设计：把一个或者多个内存页面分配出来，作为一个内存对象的容器，在这个容器中容纳相同的内存对象，即同等大小的内存块。你可以把这个容器，想像成一个内存对象数组。为了让你更好理解，我还给你画了张图解释。
如何表示一个内存对象前面只是进行了理论上的设计和构想，下面我们就通过代码来实现这些构想，真正把想法变成现实。
我们从内存对象开始入手。如何表示一个内存对象呢？当然是要设计一个表示内存对象的数据结构，代码如下所示：
typedef struct s_FREOBJH { list_h_t oh_list; //链表 uint_t oh_stus; //对象状态 void* oh_stat; //对象的开始地址 }freobjh_t; 我们在后面的代码中就用freobjh_t结构表示一个对象，其中的链表是为了找到这个对象。是不是很简单？没错，表示一个内存对象就是如此简单。</description></item><item><title>20_土地需求扩大与保障：如何表示虚拟内存？</title><link>https://artisanbox.github.io/9/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/20/</guid><description>你好，我是LMOS。
在现实中，有的人需要向政府申请一大块区域，在这块区域中建楼办厂，但是土地有限且已经被占用。所以可能的方案是，只给你分配一个总的面积区域，今年湖北有空地就在湖北建立一部分厂房，明年广东有空地就在广东再建另一部分厂房，但是总面积不变。
其实在计算机系统中也有类似的情况，一个应用往往拥有很大的连续地址空间，并且每个应用都是一样的，只有在运行时才能分配到真正的物理内存，在操作系统中这称为虚拟内存。
那问题来了，操作系统要怎样实现虚拟内存呢？由于内容比较多，我会用两节课的时间带你解决这个问题。今天这节课，我们先进行虚拟地址空间的划分，搞定虚拟内存数据结构的设计。下节课再动手实现虚拟内存的核心功能。
好，让我们进入正题，先从虚拟地址空间的划分入手，配套代码你可以从这里获得。
虚拟地址空间的划分虚拟地址就是逻辑上的一个数值，而虚拟地址空间就是一堆数值的集合。通常情况下，32位的处理器有0～0xFFFFFFFF的虚拟地址空间，而64位的虚拟地址空间则更大，有0～0xFFFFFFFFFFFFFFFF的虚拟地址空间。
对于如此巨大的地址空间，我们自然需要一定的安排和设计，比如什么虚拟地址段放应用，什么虚拟地址段放内核等。下面我们首先看看处理器硬件层面的划分，再来看看在此基础上我们系统软件层面是如何划分的。
x86 CPU如何划分虚拟地址空间我们Cosmos工作在x86 CPU上，所以我们先来看看x86 CPU是如何划分虚拟地址空间的。
由于x86 CPU支持虚拟地址空间时，要么开启保护模式，要么开启长模式，保护模式下是32位的，有0～0xFFFFFFFF个地址，可以使用完整的4GB虚拟地址空间。
在保护模式下，对这4GB的虚拟地址空间没有进行任何划分，而长模式下是64位的虚拟地址空间有0～0xFFFFFFFFFFFFFFFF个地址，这个地址空间非常巨大，硬件工程师根据需求设计，把它分成了3段，如下图所示。
长模式下，CPU目前只实现了48位地址空间，但寄存器却是64位的，CPU自己用地址数据的第47位的值扩展到最高16位，所以64位地址数据的最高16位，要么是全0，要么全1，这就是我们在上图看到的情形。
Cosmos如何划分虚拟地址空间现在我们来规划一下，Cosmos对x86 CPU长模式下虚拟地址空间的使用。由前面的图形可以看出，在长模式下，整个虚拟地址空间只有两段是可以用的，很自然一段给内核，另一段就给应用。
我们把0xFFFF800000000000～0xFFFFFFFFFFFFFFFF虚拟地址空间分给内核，把0～0x00007FFFFFFFFFFF虚拟地址空间分给应用，内核占用的称为内核空间，应用占用的就叫应用空间。
在内核空间和应用空间中，我们又继续做了细分。后面的图并不是严格按比例画的，应用程序在链接时，会将各个模块的指令和数据分别放在一起，应用程序的栈是在最顶端，向下增长，应用程序的堆是在应用程序数据区的后面，向上增长。
内核空间中有个线性映射区0xFFFF800000000000～0xFFFF800400000000，这是我们在二级引导器中建立的MMU页表映射。
如何设计数据结构根据前面经验，我们要实现一个功能模块，首先要设计出相应的数据结构，虚拟内存模块也一样。
这里涉及到虚拟地址区间，管理虚拟地址区间以及它所对应的物理页面，最后让进程和虚拟地址空间相结合。这些数据结构小而多，下面我们一个个来设计。
虚拟地址区间我们先来设计虚拟地址区间数据结构，由于虚拟地址空间非常巨大，我们绝不能像管理物理内存页面那样，一个页面对应一个结构体。那样的话，我们整个物理内存空间或许都放不下所有的虚拟地址区间数据结构的实例变量。
由于虚拟地址空间往往是以区为单位的，比如栈区、堆区，指令区、数据区，这些区内部往往是连续的，区与区之间却间隔了很大空间，而且每个区的空间扩大时我们不会建立新的虚拟地址区间数据结构，而是改变其中的指针，这就节约了内存空间。
下面我们来设计这个数据结构，代码如下所示。
typedef struct KMVARSDSC { spinlock_t kva_lock; //保护自身自旋锁 u32_t kva_maptype; //映射类型 list_h_t kva_list; //链表 u64_t kva_flgs; //相关标志 u64_t kva_limits; void* kva_mcstruct; //指向它的上层结构 adr_t kva_start; //虚拟地址的开始 adr_t kva_end; //虚拟地址的结束 kvmemcbox_t* kva_kvmbox; //管理这个结构映射的物理页面 void* kva_kvmcobj; }kmvarsdsc_t; 如你所见，除了自旋锁、链表、类型等字段外，最重要的就是虚拟地址的开始与结束字段，它精确描述了一段虚拟地址空间。
整个虚拟地址空间如何描述有了虚拟地址区间的数据结构，怎么描述整个虚拟地址空间呢？我们整个的虚拟地址空间，正是由多个虚拟地址区间连接起来组成，也就是说，只要把许多个虚拟地址区间数据结构按顺序连接起来，就可以表示整个虚拟地址空间了。
这个数据结构我们这样来设计。
typedef struct s_VIRMEMADRS { spinlock_t vs_lock; //保护自身的自旋锁 u32_t vs_resalin; list_h_t vs_list; //链表，链接虚拟地址区间 uint_t vs_flgs; //标志 uint_t vs_kmvdscnr; //多少个虚拟地址区间 mmadrsdsc_t* vs_mm; //指向它的上层的数据结构 kmvarsdsc_t* vs_startkmvdsc; //开始的虚拟地址区间 kmvarsdsc_t* vs_endkmvdsc; //结束的虚拟地址区间 kmvarsdsc_t* vs_currkmvdsc; //当前的虚拟地址区间 adr_t vs_isalcstart; //能分配的开始虚拟地址 adr_t vs_isalcend; //能分配的结束虚拟地址 void* vs_privte; //私有数据指针 void* vs_ext; //扩展数据指针 }virmemadrs_t; 从上述代码可以看出，virmemadrs_t结构管理了整个虚拟地址空间的kmvarsdsc_t结构，kmvarsdsc_t结构表示一个虚拟地址区间。这样我们就能知道，虚拟地址空间中哪些地址区间没有分配，哪些地址区间已经分配了。</description></item><item><title>21_土地需求扩大与保障：如何分配和释放虚拟内存？</title><link>https://artisanbox.github.io/9/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/21/</guid><description>你好，我是LMOS。
今天，我们继续研究操作系统如何实现虚拟内存。在上节课，我们已经建立了虚拟内存的初始流程，这节课我们来实现虚拟内存的核心功能：写出分配、释放虚拟地址空间的代码，最后实现虚拟地址空间到物理地址空间的映射。
这节课的配套代码，你可以点击这里下载。
虚拟地址的空间的分配与释放通过上节课的学习，我们知道整个虚拟地址空间就是由一个个虚拟地址区间组成的。那么不难猜到，分配一个虚拟地址空间就是在整个虚拟地址空间分割出一个区域，而释放一块虚拟地址空间，就是把这个区域合并到整个虚拟地址空间中去。
虚拟地址空间分配接口我们先来研究地址的分配，依然从虚拟地址空间的分配接口开始实现，一步步带着你完成虚拟 空间的分配。
在我们的想像中，分配虚拟地址空间应该有大小、有类型、有相关标志，还有从哪里开始分配等信息。根据这些信息，我们在krlvadrsmem.c文件中设计好分配虚拟地址空间的接口，如下所示。
adr_t vma_new_vadrs_core(mmadrsdsc_t *mm, adr_t start, size_t vassize, u64_t vaslimits, u32_t vastype) { adr_t retadrs = NULL; kmvarsdsc_t *newkmvd = NULL, *currkmvd = NULL; virmemadrs_t *vma = &amp;amp;mm-&amp;gt;msd_virmemadrs; knl_spinlock(&amp;amp;vma-&amp;gt;vs_lock); //查找虚拟地址区间 currkmvd = vma_find_kmvarsdsc(vma, start, vassize); if (NULL == currkmvd) { retadrs = NULL; goto out; } //进行虚拟地址区间进行检查看能否复用这个数据结构 if (((NULL == start) || (start == currkmvd-&amp;gt;kva_end)) &amp;amp;&amp;amp; (vaslimits == currkmvd-&amp;gt;kva_limits) &amp;amp;&amp;amp; (vastype == currkmvd-&amp;gt;kva_maptype)) {//能复用的话，当前虚拟地址区间的结束地址返回 retadrs = currkmvd-&amp;gt;kva_end; //扩展当前虚拟地址区间的结束地址为分配虚拟地址区间的大小 currkmvd-&amp;gt;kva_end += vassize; vma-&amp;gt;vs_currkmvdsc = currkmvd; goto out; } //建立一个新的kmvarsdsc_t虚拟地址区间结构 newkmvd = new_kmvarsdsc(); if (NULL == newkmvd) { retadrs = NULL; goto out; } //如果分配的开始地址为NULL就由系统动态决定 if (NULL == start) {//当然是接着当前虚拟地址区间之后开始 newkmvd-&amp;gt;kva_start = currkmvd-&amp;gt;kva_end; } else {//否则这个新的虚拟地址区间的开始就是请求分配的开始地址 newkmvd-&amp;gt;kva_start = start; } //设置新的虚拟地址区间的结束地址 newkmvd-&amp;gt;kva_end = newkmvd-&amp;gt;kva_start + vassize; newkmvd-&amp;gt;kva_limits = vaslimits; newkmvd-&amp;gt;kva_maptype = vastype; newkmvd-&amp;gt;kva_mcstruct = vma; vma-&amp;gt;vs_currkmvdsc = newkmvd; //将新的虚拟地址区间加入到virmemadrs_t结构中 list_add(&amp;amp;newkmvd-&amp;gt;kva_list, &amp;amp;currkmvd-&amp;gt;kva_list); //看看新的虚拟地址区间是否是最后一个 if (list_is_last(&amp;amp;newkmvd-&amp;gt;kva_list, &amp;amp;vma-&amp;gt;vs_list) == TRUE) { vma-&amp;gt;vs_endkmvdsc = newkmvd; } //返回新的虚拟地址区间的开始地址 retadrs = newkmvd-&amp;gt;kva_start; out: knl_spinunlock(&amp;amp;vma-&amp;gt;vs_lock); return retadrs; } //分配虚拟地址空间的接口 adr_t vma_new_vadrs(mmadrsdsc_t *mm, adr_t start, size_t vassize, u64_t vaslimits, u32_t vastype) { if (NULL == mm || 1 &amp;gt; vassize) { return NULL; } if (NULL !</description></item><item><title>22_瞧一瞧Linux：伙伴系统如何分配内存？</title><link>https://artisanbox.github.io/9/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/22/</guid><description>你好，我是LMOS。
前面我们实现了Cosmos的内存管理组件，相信你对计算机内存管理已经有了相当深刻的认识和见解。那么，像Linux这样的成熟操作系统，又是怎样实现内存管理的呢？
这就要说到Linux系统中，用来管理物理内存页面的伙伴系统，以及负责分配比页更小的内存对象的SLAB分配器了。
我会通过两节课给你理清这两种内存管理技术，这节课我们先来说说伙伴系统，下节课再讲SLAB。只要你紧跟我的思路，再加上前面的学习，真正理解这两种技术也并不难。
伙伴系统伙伴系统源于Sun公司的Solaris操作系统，是Solaris操作系统上极为优秀的物理内存页面管理算法。
但是，好东西总是容易被别人窃取或者效仿，伙伴系统也成了Linux的物理内存管理算法。由于Linux的开放和非赢利，这自然无可厚非，这不得不让我们想起了鲁迅《孔乙己》中的：“窃书不算偷”。
那Linux上伙伴系统算法是怎样实现的呢？我们不妨从一些重要的数据结构开始入手。
怎样表示一个页Linux也是使用分页机制管理物理内存的，即Linux把物理内存分成4KB大小的页面进行管理。那Linux用了一个什么样的数据结构，表示一个页呢？
早期Linux使用了位图，后来使用了字节数组，但是现在Linux定义了一个page结构体来表示一个页，代码如下所示。
struct page { //page结构体的标志，它决定页面是什么状态 unsigned long flags; union { struct { //挂载上级结构的链表 struct list_head lru; //用于文件系统，address_space结构描述上文件占用了哪些内存页面 struct address_space *mapping; pgoff_t index; unsigned long private; }; //DMA设备的地址 struct { dma_addr_t dma_addr; }; //当页面用于内存对象时指向相关的数据结构 struct { union { struct list_head slab_list; struct { struct page *next; #ifdef CONFIG_64BIT int pages; int pobjects; #else short int pages; short int pobjects; #endif }; }; //指向管理SLAB的结构kmem_cache struct kmem_cache *slab_cache; //指向SLAB的第一个对象 void *freelist; union { void *s_mem; unsigned long counters; struct { unsigned inuse:16; unsigned objects:15; unsigned frozen:1; }; }; }; //用于页表映射相关的字段 struct { unsigned long _pt_pad_1; pgtable_t pmd_huge_pte; unsigned long _pt_pad_2; union { struct mm_struct *pt_mm; atomic_t pt_frag_refcount; }; //自旋锁 #if ALLOC_SPLIT_PTLOCKS spinlock_t *ptl; #else spinlock_t ptl; #endif }; //用于设备映射 struct { struct dev_pagemap *pgmap; void *zone_device_data; }; struct rcu_head rcu_head; }; //页面引用计数 atomic_t _refcount; #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS int _last_cpupid; #endif } _struct_page_alignment; 这个page结构看上去非常巨大，信息量很多，但其实它占用的内存很少，根据Linux内核配置选项不同，占用20～40个字节空间。page结构大量使用了C语言union联合体定义结构字段，这个联合体的大小，要根据它里面占用内存最大的变量来决定。</description></item><item><title>23_瞧一瞧Linux：SLAB如何分配内存？</title><link>https://artisanbox.github.io/9/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/23/</guid><description>你好，我是LMOS。
上节课我们学习了伙伴系统，了解了它是怎样管理物理内存页面的。那么你自然会想到这个问题：Linux系统中，比页更小的内存对象要怎样分配呢？
带着这个问题，我们来一起看看SLAB分配器的原理和实现。在学习过程中，你也可以对照一下我们Cosmos的内存管理组件，看看两者的内存管理有哪些异同。
SLAB与Cosmos物理内存页面管理器一样，Linux中的伙伴系统是以页面为最小单位分配的，现实更多要以内核对象为单位分配内存，其实更具体一点说，就是根据内核对象的实例变量大小来申请和释放内存空间，这些数据结构实例变量的大小通常从几十字节到几百字节不等，远远小于一个页面的大小。
如果一个几十字节大小的数据结构实例变量，就要为此分配一个页面，这无疑是对宝贵物理内存的一种巨大浪费，因此一个更好的技术方案应运而生，就是Slab分配器（由Sun公司的雇员Jeff Bonwick在Solaris 2.4中设计并实现）。
由于作者公开了实现方法，后来被Linux所借鉴，用于实现内核中更小粒度的内存分配。看看吧，你以为Linux很强大，真的强大吗？不过是站在巨人的肩膀上飞翔的。
走进SLAB对象何为SLAB对象？在SLAB分配器中，它把一个内存页面或者一组连续的内存页面，划分成大小相同的块，其中这一个小的内存块就是SLAB对象，但是这一组连续的内存页面中不只是SLAB对象，还有SLAB管理头和着色区。
我画个图你就明白了，如下所示。
上图中有一个内存页面和两个内存页面的SLAB，你可能对着色区有点陌生，我来给你讲解一下。
这个着色区也是一块动态的内存块，建立SLAB时才会设置它的大小，目的是为了错开不同SLAB中的对象地址，降低硬件Cache行中的地址争用，以免导致Cache抖动效应，整个系统性能下降。
SLAB头其实是一个数据结构，但是它不一定放在保存对象内存页面的开始。通常会有一个保存SLAB管理头的SLAB，在Linux中，SLAB管理头用kmem_cache结构来表示，代码如下。
struct array_cache { unsigned int avail; unsigned int limit; void *entry[]; }; struct kmem_cache { //是每个CPU一个array_cache类型的变量，cpu_cache是用于管理空闲对象的 struct array_cache __percpu *cpu_cache; unsigned int size; //cache大小 slab_flags_t flags;//slab标志 unsigned int num;//对象个数 unsigned int gfporder;//分配内存页面的order gfp_t allocflags; size_t colour;//着色区大小 unsigned int colour_off;//着色区的开始偏移 const char *name;//本SLAB的名字 struct list_head list;//所有的SLAB都要链接起来 int refcount;//引用计数 int object_size;//对象大小 int align;//对齐大小 struct kmem_cache_node *node[MAX_NUMNODES];//指向管理kmemcache的上层结构 }; 上述代码中，有多少个CPU，就会有多少个array_cache类型的变量。这种为每个CPU构造一个变量副本的同步机制，就是每CPU变量（per-cpu-variable）。array_cache结构中"entry[]"表示了一个遵循LIFO顺序的数组，"avail"和"limit"分别指定了当前可用对象的数目和允许容纳对象的最大数目。</description></item><item><title>24_活动的描述：到底什么是进程？</title><link>https://artisanbox.github.io/9/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/24/</guid><description>你好，我是LMOS。
在前面的课程里，我们已经实现了数据同步、hal层的初始化，中断框架、物理内存、内存对象、虚拟内存管理，这些都是操作系统中最核心的东西。
今天，我再给你讲讲操作系统里一个层次非常高的组件——进程，而它又非常依赖于内存管理、中断、硬件体系结构。好在前面课程中，这些基础知识我们已经搞得清清楚楚，安排得明明白白了，所以我们今天理解进程就变得顺理成章。
感受一下在你看来，什么是进程呢？日常我们跟计算机打交道的时候，最常接触的就是一些应用程序，比如Word、浏览器，你可以直观感受到它们的存在。而我们却很难直观感受到什么是进程，自然也就不容易描述它的模样与形态了。
其实，在我们启用Word这些应用时，操作系统在背后就会建立至少一个进程。虽然我们难以观察它的形态，但我们绝对可以通过一些状态数据来发现进程的存在。
在Linux的终端下输入ps命令， 我们就可以看到系统中有多少个进程了。如下图所示。
这是进程吗？是的，不过这只是一些具体进程的数据，如创建进程和用户、进程ID、使用CPU的百分比，进程运行状态，进程的建立时间、进程的运行时间、进程名等，这些数据综合起来就代表了一个进程。
也许看到这，你会呵呵一笑，觉得原来抽象的进程背后，不过是一堆数据而已，关于进程这就是我们能直观感受到的东西，这就完了吗？当然没有，我们接着往下看。
什么是进程如果你要组织一个活动怎么办？你首先会想到，这个活动的流程是什么，需要配备哪些人员和物资，中途要不要休息，活动当前进行到哪里了……如果你是个精明的人，你大概会用表格把这些信息记录下来。
同理，你运行一个应用程序时，操作系统也要记录这个应用程序使用多少内存，打开了什么文件，当有些资源不可用的时候要不要睡眠，当前进程运行到哪里了。操作系统把这些信息综合统计，存放在内存中，抽象为进程。
现在你就可以回答什么是进程了：进程是一个应用程序运行时刻的实例（从进程的结构看）；进程是应用程序运行时所需资源的容器（从进程的功能看）；甚至进程是一堆数据结构（从操作系统对进程实现的角度来说）。
这也太简单了吧？对，进程的抽象概念就是这么简单。我知道这一定不能让你真正明白什么是进程，抽象的概念就是如此，你不在实践中设计并实现它，是很难真正明白的。下面我们先来细化设计。
进程的结构首先，进程是一个应用程序运行时刻的实例，它的目的就是操作系统用于管理和运行多个应用程序的；其次，从前面我们实现的内存管理组件角度看，操作系统是给应用程序提供服务的。
所以，从这两个角度看，进程必须要有一个地址空间，这个地址空间至少包括两部分内容：一部分是内核，一部分是用户的应用程序。
最后，结合x86硬件平台对虚拟地址空间的制约，我给你画了一幅图，如下所示。
上图中有8个进程，每个进程拥有x86 CPU的整个虚拟地址空间，这个虚拟地址空间被分成了两个部分，上半部分是所有进程都共享的内核部分 ，里面放着一份内核代码和数据，下半部分是应用程序，分别独立，互不干扰。
还记得我们讲过的x86 CPU的特权级吗？
当CPU在R0特权级运行时，就运行在上半部分内核的地址空间中，当CPU在R3特权级时，就运行在下半部分的应用程序地址空间中。各进程的虚拟地址空间是相同的，它们之间物理地址不同，是由MMU页表进行隔离的，所以每个进程的应用程序的代码绝对不能随意访问内核的代码和数据。
以上是整体结构，下面我们来细化一下进程需要实现哪些功能？
我们先从应用程序和内核的关系看。应用程序需要内核提供资源，而内核需要控制应用程序的运行。那么内核必须能够命令应用程序，让它随时中断（进入内核地址空间）或恢复执行，这就需要保存应用程序的机器上下文和它运行时刻的栈。
接着，我们深入内核提供服务的机制。众所周知，内核是这样提供服务的：通过停止应用程序代码运行，进入内核地址空间运行内核代码，然后返回结果。就像活动组织者会用表格备案一样，内核还需要记录一个应用程序都访问了哪些资源，比如打开了某个文件，或是访问了某个设备。而这样的“记录表”，我们就用“资源描述符”来表示。
而我们前面已经说了，进程是一个应用程序运行时刻的实例。那这样一来，一个细化的进程结构，就可以像下图这样设计。
上图中表示了一个进程详细且必要的结构，其中带*号是每个进程都有独立一份，有了这样的设计结构，多个进程就能并发运行了。前面这些内容还是纸上谈兵，你重点搞明白进程的概念和结构就行了。
实现进程前面我们简单介绍了进程的概念和结构，之所以简单，是为了不在理论层面就把问题复杂化，这对我们实现Cosmos的进程组件没有任何好处。
但只懂理论还是空中阁楼，我们可以一步步在设计实现中，由浅到深地理解什么是进程。我们这就把前面的概念和设计，一步步落实到代码，设计出对应的数据结构。
如何表示一个进程根据前面课程的经验，如果要在软件代码中表示一个什么东西时，就要设计出对应的数据结构。
那么对于一个进程，它有状态，id，运行时间，优先级，应用程序栈，内核栈，机器上下文，资源描述符，地址空间，我们将这些信息组织在一起，就形成了一个进程的数据结构。
下面我带你把它变成代码，在cosmos/include/knlinc/目录下建立一个krlthread_t.h文件，在其中写上代码，如下所示。
typedef struct s_THREAD { spinlock_t td_lock; //进程的自旋锁 list_h_t td_list; //进程链表 uint_t td_flgs; //进程的标志 uint_t td_stus; //进程的状态 uint_t td_cpuid; //进程所在的CPU的id uint_t td_id; //进程的id uint_t td_tick; //进程运行了多少tick uint_t td_privilege; //进程的权限 uint_t td_priority; //进程的优先级 uint_t td_runmode; //进程的运行模式 adr_t td_krlstktop; //应用程序内核栈顶地址 adr_t td_krlstkstart; //应用程序内核栈开始地址 adr_t td_usrstktop; //应用程序栈顶地址 adr_t td_usrstkstart; //应用程序栈开始地址 mmadrsdsc_t* td_mmdsc; //地址空间结构 context_t td_context; //机器上下文件结构 objnode_t* td_handtbl[TD_HAND_MAX];//打开的对象数组 }thread_t; 在Cosmos中，我们就使用thread_t结构的一个实例变量代表一个进程。进程的内核栈和进程的应用程序栈是两块内存空间，进程的权限表示一个进程是用户进程还是系统进程。进程的权限不同，它们能完成功能也不同。</description></item><item><title>25_多个活动要安排（上）：多进程如何调度？</title><link>https://artisanbox.github.io/9/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/25/</guid><description>你好，我是LMOS。
上节课，我们了解了什么是进程，还一起写好了建立进程的代码。不知道你想过没有，如果在系统中只有一个进程，那我们提出进程相关的概念和实现与进程有关的功能，是不是就失去了意义呢？
显然，提出进程的目的之一，就是为了实现多个进程，使系统能运行多个应用程序。今天我们就在单进程的基础上扩展多进程，并在进程与进程之间进行调度。
“你存在，我深深的脑海里，我的梦里，我的心里，我的代码里”，我经常一边哼着歌，一边写着代码，这就是我们大脑中最典型“多进程”场景。
再来举一个例子：你在Windows上，边听音乐，边浏览网页，还能回复微信消息。Windows之所以能同时运行多个应用程序，就是因为Windows内核支持多进程机制，这就是最典型的多进程场景了。
这节课配套代码，你可以点击这里下载。
为什么需要多进程调度我们先来搞清楚多进程调度的原因是什么，我来归纳一下。
第一，CPU同一时刻只能运行一个进程，而CPU个数总是比进程个数少，这就需要让多进程共用一个CPU，每个进程在这个CPU上运行一段时间。
第二点原因，当一个进程不能获取某种资源，导致它不能继续运行时，就应该让出CPU。当然你也可以把第一点中的CPU时间，也归纳为一种资源，这样就合并为一点：进程拿不到资源就要让出CPU。我来为你画幅图就明白了，如下所示。
上图中，有五个进程，其中浏览器进程和微信进程依赖于网络和键盘的数据资源，如果不能满足它们，就应该通过进程调度让出CPU。
而两个科学计算进程，则更多的依赖于CPU，但是如果它们中的一个用完了自己的CPU时间，也得借助进程调度让出CPU，不然它就会长期霸占CPU，导致其它进程无法运行。需要注意的是，每个进程都会依赖一种资源，那就是CPU时间，你可以把CPU时间理解为它就是CPU，一个进程必须要有CPU才能运行。
这里我们只需要明白，多个进程为什么要进行调度，就可以了。
管理进程下面我们一起来看看怎么管理进程，我们的Cosmos操作系统也支持多个进程，有了多个进程就要把它们管理起来。说白了，就是弄清楚这些进程有哪些状态，是如何组织起来的，又要从哪找到它们。
进程的生命周期人有生老病死，对于一个进程来说也是一样。一个进程从建立开始，接着运行，然后因为资源问题不得不暂停运行，最后退出系统。这一过程，我们称为进程的生命周期。在系统实现中，通常用进程的状态表示进程的生命周期。进程的状态我们用几个宏来定义，如下所示。
#define TDSTUS_RUN 0 //进程运行状态 #define TDSTUS_SLEEP 3 //进程睡眠状态 #define TDSTUS_WAIT 4 //进程等待状态 #define TDSTUS_NEW 5 //进程新建状态 #define TDSTUS_ZOMB 6 //进程僵死状态 可以发现，我们的进程有5个状态。其中进程僵死状态，表示进程将要退出系统不再进行调度。那么进程状态之间是如何转换的，别急，我来给画一幅图解释，如下所示。
上图中已经为你展示了，从建立进程到进程退出系统各状态之间的转换关系和需要满足的条件。
如何组织进程首先我们来研究如何组织进程。由于系统中会有许多个进程，在上节课中我们用thread_t结构表示一个进程，因此会有多个thread_t结构。而根据刚才我们对进程生命周期的解读，我们又知道了进程是随时可能建立或者退出的，所以系统中会随时分配或者删除thread_t结构。
要应对这样的情况，最简单的办法就是使用链表数据结构，而且我们的进程有优先级，所以我们可以设计成每个优先级对应一个链表头。
下面我们来把设计落地成数据结构，由于这是调度器模块，所以我们要建立几个文件krlsched.h、krlsched.c，在其中写上代码，如下所示。
typedef struct s_THRDLST { list_h_t tdl_lsth; //挂载进程的链表头 thread_t* tdl_curruntd; //该链表上正在运行的进程 uint_t tdl_nr; //该链表上进程个数 }thrdlst_t; typedef struct s_SCHDATA { spinlock_t sda_lock; //自旋锁 uint_t sda_cpuid; //当前CPU id uint_t sda_schdflgs; //标志 uint_t sda_premptidx; //进程抢占计数 uint_t sda_threadnr; //进程数 uint_t sda_prityidx; //当前优先级 thread_t* sda_cpuidle; //当前CPU的空转进程 thread_t* sda_currtd; //当前正在运行的进程 thrdlst_t sda_thdlst[PRITY_MAX]; //进程链表数组 }schdata_t; typedef struct s_SCHEDCALSS { spinlock_t scls_lock; //自旋锁 uint_t scls_cpunr; //CPU个数 uint_t scls_threadnr; //系统中所有的进程数 uint_t scls_threadid_inc; //分配进程id所用 schdata_t scls_schda[CPUCORE_MAX]; //每个CPU调度数据结构 }schedclass_t; 从上述代码中，我们发现schedclass_t是个全局数据结构，这个结构里包含一个schdata_t结构数组，数组大小根据CPU的数量决定。在每个schdata_t结构中，又包含一个进程优先级大小的thrdlst_t结构数组。我画幅图，你就明白了。这幅图能让你彻底理清以上数据结构之间的关系。</description></item><item><title>26_多个活动要安排（下）：如何实现进程的等待与唤醒机制？</title><link>https://artisanbox.github.io/9/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/26/</guid><description>你好，我是LMOS。
上节课，我带你一起设计了我们Cosmos的进程调度器，但有了进程调度器还不够，因为调度器它始终只是让一个进程让出CPU，切换到它选择的下一个进程上去运行。
结合前面我们对进程生命周期的讲解，估计你已经反应过来了。没错，多进程调度方面，我们还要实现进程的等待与唤醒机制，今天我们就来搞定它。
这节课的配套代码，你可以从这里下载。
进程的等待与唤醒我们已经知道，进程得不到所需的某个资源时就会进入等待状态，直到这种资源可用时，才会被唤醒。那么进程的等待与唤醒机制到底应该这样设计呢，请听我慢慢为你梳理。
进程等待结构很显然，在实现进程的等待与唤醒的机制之前，我们需要设计一种数据结构，用于挂载等待的进程，在唤醒的时候才可以找到那些等待的进程 ，这段代码如下所示。
typedef struct s_KWLST { spinlock_t wl_lock; //自旋锁 uint_t wl_tdnr; //等待进程的个数 list_h_t wl_list; //挂载等待进程的链表头 }kwlst_t; 其实，这个结构在前面讲信号量的时候，我们已经见过了。这是因为它经常被包含在信号量等上层数据结构中，而信号量结构，通常用于保护访问受限的共享资源。这个结构非常简单，我们不用多说。
进程等待现在我们来实现让进程进入等待状态的机制，它也是一个函数。这个函数会设置进程状态为等待状态，让进程从调度系统数据结构中脱离，最后让进程加入到kwlst_t等待结构中，代码如下所示。
void krlsched_wait(kwlst_t *wlst) { cpuflg_t cufg, tcufg; uint_t cpuid = hal_retn_cpuid(); schdata_t *schdap = &amp;amp;osschedcls.scls_schda[cpuid]; //获取当前正在运行的进程 thread_t *tdp = krlsched_retn_currthread(); uint_t pity = tdp-&amp;gt;td_priority; krlspinlock_cli(&amp;amp;schdap-&amp;gt;sda_lock, &amp;amp;cufg); krlspinlock_cli(&amp;amp;tdp-&amp;gt;td_lock, &amp;amp;tcufg); tdp-&amp;gt;td_stus = TDSTUS_WAIT;//设置进程状态为等待状态 list_del(&amp;amp;tdp-&amp;gt;td_list);//脱链 krlspinunlock_sti(&amp;amp;tdp-&amp;gt;td_lock, &amp;amp;tcufg); if (schdap-&amp;gt;sda_thdlst[pity].tdl_curruntd == tdp) { schdap-&amp;gt;sda_thdlst[pity].tdl_curruntd = NULL; } schdap-&amp;gt;sda_thdlst[pity].tdl_nr--; krlspinunlock_sti(&amp;amp;schdap-&amp;gt;sda_lock, &amp;amp;cufg); krlwlst_add_thread(wlst, tdp);//将进程加入等待结构中 return; } 上述代码也不难，你结合注释就能理解。有一点需要注意，这个函数使进程进入等待状态，而这个进程是当前正在运行的进程，而当前正在运行的进程正是调用这个函数的进程，所以一个进程想要进入等待状态，只要调用这个函数就好了。</description></item><item><title>27_瞧一瞧Linux：Linux如何实现进程与进程调度</title><link>https://artisanbox.github.io/9/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/27/</guid><description>你好，我是LMOS。
在前面的课程中，我们已经写好了Cosmos的进程管理组件，实现了多进程调度运行，今天我们一起探索Linux如何表示进程以及如何进行多进程调度。
好了，话不多说，我们开始吧。
Linux如何表示进程在Cosmos中，我们设计了一个thread_t数据结构来代表一个进程，Linux也同样是用一个数据结构表示一个进程。
下面我们先来研究Linux的进程数据结构，然后看看Linux进程的地址空间数据结构，最后再来理解Linux的文件表结构。
Linux进程的数据结构Linux系统下，把运行中的应用程序抽象成一个数据结构task_struct，一个应用程序所需要的各种资源，如内存、文件等都包含在task_struct结构中。
因此，task_struct结构是非常巨大的一个数据结构，代码如下。
struct task_struct { struct thread_info thread_info;//处理器特有数据 volatile long state; //进程状态 void *stack; //进程内核栈地址 refcount_t usage; //进程使用计数 int on_rq; //进程是否在运行队列上 int prio; //动态优先级 int static_prio; //静态优先级 int normal_prio; //取决于静态优先级和调度策略 unsigned int rt_priority; //实时优先级 const struct sched_class *sched_class;//指向其所在的调度类 struct sched_entity se;//普通进程的调度实体 struct sched_rt_entity rt;//实时进程的调度实体 struct sched_dl_entity dl;//采用EDF算法调度实时进程的调度实体 struct sched_info sched_info;//用于调度器统计进程的运行信息 struct list_head tasks;//所有进程的链表 struct mm_struct *mm; //指向进程内存结构 struct mm_struct *active_mm; pid_t pid; //进程id struct task_struct __rcu *parent;//指向其父进程 struct list_head children; //链表中的所有元素都是它的子进程 struct list_head sibling; //用于把当前进程插入到兄弟链表中 struct task_struct *group_leader;//指向其所在进程组的领头进程 u64 utime; //用于记录进程在用户态下所经过的节拍数 u64 stime; //用于记录进程在内核态下所经过的节拍数 u64 gtime; //用于记录作为虚拟机进程所经过的节拍数 unsigned long min_flt;//缺页统计 unsigned long maj_flt; struct fs_struct *fs; //进程相关的文件系统信息 struct files_struct *files;//进程打开的所有文件 struct vm_struct *stack_vm_area;//内核栈的内存区 }; 为了帮你掌握核心思路，关于task_struct结构体，我省略了进程的权能、性能跟踪、信号、numa、cgroup等相关的近500行内容，你若有兴趣可以自行阅读，这里你只需要明白，在内存中，一个task_struct结构体的实例变量代表一个Linux进程就行了。</description></item><item><title>28_部门分类：如何表示设备类型与设备驱动？</title><link>https://artisanbox.github.io/9/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/28/</guid><description>你好，我是LMOS。
小到公司，大到国家，都有各种下属部门，比如我们国家现在有教育部、科学技术部、外交部，财政部等，这些部门各自负责完成不同的职能工作，如教育部负责教育事业和语言文字工作，科学技术部负责推动解决经济社会发展的重大科技问题。
既然大道相通，那我们的Cosmos中是否也是类似这样的结构呢？
答案是肯定的，在前面的课中，我们搞定了内存管理和进程管理，它们是内核不可分隔的，但是计算机中还有各种类型的设备需要管理。
我们的Cosmos也会“成立各类部门”，用于管理众多设备，一个部门负责一类设备。具体要怎么管理设备呢？你不妨带着这个问题，正式开始今天的学习！
这节课的代码，你可以从这里下载。
计算机的结构不知道你是否和我一样，经常把计算机的机箱打开，看看 CPU，看看内存条，看看显卡，看看主板上的各种芯片。
其实，这些芯片并非独立存在，而是以总线为基础连接在一起的，各自完成自己的工作，又能互相打配合，共同实现用户要求的功能。
为了帮你理清它们的连接关系，我为你画了一幅图，如下所示。
上图是一个典型的桌面系统，你先不用管是物理上怎么样连接的，逻辑上就是这样的。实际可能比图中有更多或者更少的总线。但是总线有层级关系，各种设备通过总线相连。这里我们只需要记住，计算机中有很多种类的设备，脑中有刚才这幅图就行了。
如何管理设备在前面的课程中，我们实现了管理内存和进程，其实进程从正面看它是管理应用程序的，反过来看它也是管理CPU的，它能使CPU的使用率达到最高。
管理内存和管理CPU是操作系统最核心的部分，但是这还不够，因为计算机不止有CPU，还有各种设备。
如果把计算机内部所有的设备和数据都描述成资源，操作系统内核无疑是这些资源的管理者。既然设备也是一种资源，如何高效管理它们，以便提供给应用进程使用和操作，就是操作系统内核的重要任务。
分权而治一个国家之所以有那么多部门，就是要把管理工作分开，专权专职专责，对于操作系统也是一样。
现代计算机早已不限于只处理计算任务，它还可以呈现图像、音频，和远程计算机通信，储存大量数据，以及和用户交互。所以，计算机内部需要处理图像、音频、网络、储存、交互的设备。这从上面的图中也可以看得出来。
操作系统内核要控制这些设备，就要包含每个设备的控制代码。如果操作系统内核被设计为通用可移植的内核，那是相当可怕的。试想一下，这个世界上有如此多的设备，操作系统内核代码得多庞大，越庞大就越危险，因为其中一行代码有问题，整个操作系统就崩溃了。
可是仅仅只有这些问题吗？当然不是，我们还要考虑到后面这几点。
1.操作系统内核开发人员，不可能罗列世界上所有的设备，并为其写一套控制代码。
2.为了商业目的，有很多设备厂商并不愿意公开设备的编程细节。就算内核开发人员想为其写控制代码，实际也不可行。
3.如果设备更新换代，就要重写设备的控制代码，然后重新编译操作系统内核，这样的话操作很麻烦，操作系统内核开发人员和用户都可能受不了。
以上三点，足于证明这种方案根本不可取。
既然操作系统内核无法包含所有的设备控制代码，那就索性不包含，或者只包含最基本、最通用的设备控制代码。这样操作系统内核就可以非常通用，非常精巧。
但是要控制设备就必须要有设备的相关控制代码才行，所以我们要把设备控制代码独立出来，与操作系统内核分开、独立开发，设备控制代码可由设备厂商人员开发。
每个设备对应一个设备控制代码模块，操作系统内核要控制哪个设备，就加载相应的设备代码模块，以后不使用这个设备了，就可以删除对应的设备控制代码模块。
这种方式，给操作系统内核带来了巨大的灵活性。设备厂商在发布新设备时，只要随之发布一个与此相关的设备控制代码模块就行了。
设备分类要想管理设备，先要对其分门别类，在开始分类之前，你不妨先思考一个问题：操作系统内核所感知的设备，一定要与物理设备一一对应吗？
举个例子，储存设备，其实不管它是机械硬盘，还是TF卡，或者是一个设备控制代码模块，它向操作系统内核表明它是储存设备，但它完全有可能分配一块内存空间来储存数据，不必访问真正的储存设备。所以，操作系统内核所感知的设备，并不需要和物理设备对应，这取决于设备控制代码自身的行为。
操作系统内核所定义的设备，可称为内核设备或者逻辑设备，其实这只是对物理计算平台中几种类型设备的一种抽象。下面，我们在cosmos/include/knlinc/krldevice_t.h文件中对设备进行分类定义，代码如下。
#define NOT_DEVICE 0 //不表示任何设备 #define BRIDGE_DEVICE 4 //总线桥接器设备 #define CPUCORE_DEVICE 5 //CPU设备，CPU也是设备 #define RAMCONTER_DEVICE 6 //内存控制器设备 #define RAM_DEVICE 7 //内存设备 #define USBHOSTCONTER_DEVICE 8 //USB主控制设备 #define INTUPTCONTER_DEVICE 9 //中断控制器设备 #define DMA_DEVICE 10 //DMA设备 #define CLOCKPOWER_DEVICE 11 //时钟电源设备 #define LCDCONTER_DEVICE 12 //LCD控制器设备 #define NANDFLASH_DEVICE 13 //nandflash设备 #define CAMERA_DEVICE 14 //摄像头设备 #define UART_DEVICE 15 //串口设备 #define TIMER_DEVICE 16 //定时器设备 #define USB_DEVICE 17 //USB设备 #define WATCHDOG_DEVICE 18 //看门狗设备 #define RTC_DEVICE 22 //实时时钟设备 #define SD_DEVICE 25 //SD卡设备 #define AUDIO_DEVICE 26 //音频设备 #define TOUCH_DEVICE 27 //触控设备 #define NETWORK_DEVICE 28 //网络设备 #define VIR_DEVICE 29 //虚拟设备 #define FILESYS_DEVICE 30 //文件系统设备 #define SYSTICK_DEVICE 31 //系统TICK设备 #define UNKNOWN_DEVICE 32 //未知设备，也是设备 #define HD_DEVICE 33 //硬盘设备 上面定义的这些类型的设备，都是Cosmos内核抽象出来的逻辑设备，例如NETWORK_DEVICE网络设备，不管它是有线网卡还是无线网卡，或者是设备控制代码虚拟出来的虚拟网卡。Cosmos内核都将认为它是一个网络设备，这就是设备的抽象，这样有利于我们灵活、简便管理设备。</description></item><item><title>29_部门建立：如何在内核中注册设备？</title><link>https://artisanbox.github.io/9/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/29/</guid><description>你好，我是LMOS。
在上节课里，我们对设备进行了分类，建立了设备与驱动的数据结构，同时也规定了一个驱动程序应该提供哪些标准操作方法，供操作系统内核调用。这相当于设计了行政部门的规章制度，一个部门叫什么，应该干什么，这些就确定好了。
今天我们来继续探索部门的建立，也就是设备在内核中是如何注册的。我们先从全局了解一下设备的注册流程，然后了解怎么加载驱动，最后探索怎么让驱动建立一个设备，并在内核中注册。让我们正式开始今天的学习吧！
这节课配套代码，你可以从这里下载。
设备的注册流程你是否想象过，你在电脑上插入一个USB鼠标时，操作系统会作出怎样的反应呢？
我来简单作个描述，这个过程可以分成这样五步。
1.操作系统会收到一个中断。
2.USB总线驱动的中断处理程序会执行。
3.调用操作系统内核相关的服务，查找USB鼠标对应的驱动程序。
4.操作系统加载驱动程序。
5.驱动程序开始执行，向操作系统内核注册一个鼠标设备。这就是一般操作系统加载驱动的粗略过程。对于安装在主板上的设备，操作系统会枚举设备信息，然后加载驱动程序，让驱动程序创建并注册相应的设备。当然，你还可以手动加载驱动程序。
为了简单起见，我们的Cosmos不会这样复杂，暂时也不支持设备热拨插功能。我们让Cosmos自动加载驱动，在驱动中向Cosmos注册相应的设备，这样就可以大大降低问题的复杂度，我们先从简单的做起嘛，相信你明白了原理之后，还可以自行迭代。
为了让你更清楚地了解这个过程，我为你画了一幅图，如下所示。
上图中，完整展示了Cosmos自动加载驱动的整个流程，Cosmos在初始化驱动时会扫描整个驱动表，然后加载表中每个驱动，分别调用各个驱动的入口函数，最后在驱动中建立设备并向内核注册。接下来，我们分别讨论这些流程的实现。
驱动程序表为了简化问题，便于你理解，我们把驱动程序和内核链接到一起，省略了加载驱动程序的过程，因为加载程序不仅仅是把驱动程序放在内存中就可以了，还要进行程序链接相关的操作，这个操作极其复杂，我们先不在这里研究，感兴趣的话你可以自行拓展。
既然我们把内核和驱动程序链接在了一起，就需要有个机制让内核知道驱动程序的存在。这个机制就是驱动程序表，它可以这样设计。
//cosmos/kernel/krlglobal.c KRL_DEFGLOB_VARIABLE(drventyexit_t,osdrvetytabl)[]={NULL}; drventyexit_t类型，在上一课中，我们已经了解过了。它就是一个函数指针类型，这里就是定义了一个函数指针数组，而这个函数指针数组中放的就是驱动程序的入口函数，而内核只需要扫描这个函数指针数组，就可以调用到每个驱动程序了。
有了这个函数指针数组，接着我们还需要写好这个驱动程序的初始化函数，代码如下。
void init_krldriver() { //遍历驱动程序表中的每个驱动程序入口函数 for (uint_t ei = 0; osdrvetytabl[ei] != NULL; ei++) { //运行一个驱动程序入口 if (krlrun_driverentry(osdrvetytabl[ei]) == DFCERRSTUS) { hal_sysdie(&amp;quot;init driver err&amp;quot;); } } return; } void init_krl() { init_krlmm(); init_krldevice(); init_krldriver(); //…… return; } 像上面代码这样，我们的初始化驱动的代码就写好了。init_krldriver函数主要的工作就是遍历驱动程序表中的每个驱动程序入口，并把它作为参数传给krlrun_driverentry函数。
有了init_krldriver函数，还要在init_krl函数中调用它，主要调用上述代码中的调用顺序，请注意，一定要先初始化设备表，然后才能初始化驱动程序，否则在驱动程序中建立的设备和驱动就无处安放了。
运行驱动程序我们使用驱动程序表，虽然省略了加载驱动程序的步骤，但是驱动程序必须要运行，才能工作。接下来我们就详细看看运行驱动程序的全过程。
调用驱动程序入口函数我们首先来解决怎么调用驱动程序入口函数。你要知道，我们直接调用驱动程序入口函数是不行的，要先给它准备一个重要的参数，也就是驱动描述符指针。
为了帮你进一步理解，我们来写一个函数描述内核加载驱动的过程，后面代码中drvp就是一个驱动描述符指针。
drvstus_t krlrun_driverentry(drventyexit_t drventry) { driver_t *drvp = new_driver_dsc();//建立driver_t实例变量 if (drvp == NULL) { return DFCERRSTUS; } if (drventry(drvp, 0, NULL) == DFCERRSTUS)//运行驱动程序入口函数 { return DFCERRSTUS; } if (krldriver_add_system(drvp) == DFCERRSTUS)//把驱动程序加入系统 { return DFCERRSTUS; } return DFCOKSTUS; } 上述代码中，我们先调用了 一个new_driver_dsc函数，用来建立一个driver_t结构实例变量，这个函数我已经帮你写好了。</description></item><item><title>30_部门响应：设备如何处理内核IO包？</title><link>https://artisanbox.github.io/9/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/30/</guid><description>你好，我是LMOS。
在上一课中，我们实现了建立设备的接口，这相当于制定了部门的相关法规，只要遵守这些法规就能建立一个部门。当然，建立了一个部门，是为了干活的，吃空饷可不行。
其实一个部门的职责不难确定，它应该能对上级下发的任务作出响应，并完成相关工作，而这对应到设备，就是如何处理内核的I/O包，这节课我们就来解决这个问题。
首先，我们需要搞清楚什么是I/O包，然后实现内核向设备发送I/O包的工作。最后，我还会带你一起来完成一个驱动实例，用于处理I/O包，这样你就能真正理解这里的来龙去脉了。
好，让我们开始今天的学习吧！代码你可以从这里下载。
什么是I/O包就像你要给部门下达任务时，需要准备材料报表之类的东西。同样，内核要求设备做什么事情，完成什么功能，必须要告诉设备的驱动程序。
内核要求设备完成任务，无非是调用设备的驱动程序函数，把完成任务的细节用参数的形式传递给设备的驱动程序。
由于参数很多，而且各种操作所需的参数又不相同，所以我们就想到了更高效的管理方法，也就是把各种操作所需的各种参数封装在一个数据结构中，称为I/O包，这样就可以统一驱动程序功能函数的形式了。
思路理清以后，现在我们来设计这个数据结构，如下所示。
typedef struct s_OBJNODE { spinlock_t on_lock; //自旋锁 list_h_t on_list; //链表 sem_t on_complesem; //完成信号量 uint_t on_flgs; //标志 uint_t on_stus; //状态 sint_t on_opercode; //操作码 uint_t on_objtype; //对象类型 void* on_objadr; //对象地址 uint_t on_acsflgs; //访问设备、文件标志 uint_t on_acsstus; //访问设备、文件状态 uint_t on_currops; //对应于读写数据的当前位置 uint_t on_len; //对应于读写数据的长度 uint_t on_ioctrd; //IO控制码 buf_t on_buf; //对应于读写数据的缓冲区 uint_t on_bufcurops; //对应于读写数据的缓冲区的当前位置 size_t on_bufsz; //对应于读写数据的缓冲区的大小 uint_t on_count; //对应于对象节点的计数 void* on_safedsc; //对应于对象节点的安全描述符 void* on_fname; //对应于访问数据文件的名称 void* on_finode; //对应于访问数据文件的结点 void* on_extp; //用于扩展 }objnode_t; 现在你可能还无法从objnode_t这个名字看出它跟I/O包的关系。但你从刚才的代码里可以看出，objnode_t的数据结构中包括了各个驱动程序功能函数的所有参数。</description></item><item><title>31_瞧一瞧Linux：如何获取所有设备信息？</title><link>https://artisanbox.github.io/9/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/31/</guid><description>你好，我是LMOS。
前面我们已经完成了Cosmos的驱动设备的建立，还写好了一个真实的设备驱动。
今天，我们就来看看Linux是如何管理设备的。我们将从Linux如何组织设备开始，然后研究设备驱动相关的数据结构，最后我们还是要一起写一个Linux设备驱动实例，这样才能真正理解它。
感受一下Linux下的设备信息Linux的设计哲学就是一切皆文件，各种设备在Linux系统下自然也是一个个文件。不过这个文件并不对应磁盘上的数据文件，而是对应着存在内存当中的设备文件。实际上，我们对设备文件进行操作，就等同于操作具体的设备。
既然我们了解万事万物，都是从最直观的感受开始的，想要理解Linux对设备的管理，自然也是同样的道理。那么Linux设备文件在哪个目录下呢？其实现在我们在/sys/bus目录下，就可以查看所有的设备了。
Linux用BUS（总线）组织设备和驱动，我们在/sys/bus目录下输入tree命令，就可以看到所有总线下的所有设备了，如下图所示。
上图中，显示了部分Linux设备文件，有些设备文件是链接到其它目录下文件，这不是重点，重点是你要在心中有这个目录层次结构，即总线目录下有设备目录，设备目录下是设备文件。
数据结构我们接着刚才的图往下说，我们能感觉到Linux的驱动模型至少有三个核心数据结构，分别是总线、设备和驱动，但是要像上图那样有层次化地组织它们，只有总线、设备、驱动这三个数据结构是不够的，还得有两个数据结构来组织它们，那就是kobject和kset，下面我们就去研究它们。
kobject与ksetkobject和kset是构成/sys目录下的目录节点和文件节点的核心，也是层次化组织总线、设备、驱动的核心数据结构，kobject、kset数据结构都能表示一个目录或者文件节点。下面我们先来研究一下kobject数据结构，代码如下所示。
struct kobject { const char *name; //名称，反映在sysfs中 struct list_head entry; //挂入kset结构的链表 struct kobject *parent; //指向父结构 struct kset *kset; //指向所属的kset struct kobj_type *ktype; struct kernfs_node *sd; //指向sysfs文件系统目录项 struct kref kref; //引用计数器结构 unsigned int state_initialized:1;//初始化状态 unsigned int state_in_sysfs:1; //是否在sysfs中 unsigned int state_add_uevent_sent:1; unsigned int state_remove_uevent_sent:1; unsigned int uevent_suppress:1; }; 每一个 kobject，都对应着 /sys目录下（其实是sysfs文件系统挂载在/sys目录下） 的一个目录或者文件，目录或者文件的名字就是kobject结构中的name。
我们从kobject结构中可以看出，它挂载在kset下，并且指向了kset，那kset是什么呢？我们来分析分析，它是kobject结构的容器吗？
其实是也不是，因为kset结构中本身又包含一个kobject结构，所以它既是kobject的容器，同时本身还是一个kobject。kset结构代码如下所示。
struct kset { struct list_head list; //挂载kobject结构的链表 spinlock_t list_lock; //自旋锁 struct kobject kobj;//自身包含一个kobject结构 const struct kset_uevent_ops *uevent_ops;//暂时不关注 } __randomize_layout; 看到这里你应该知道了，kset不仅仅自己是个kobject，还能挂载多个kobject，这说明kset是kobject的集合容器。在Linux内核中，至少有两个顶层kset，代码如下所示。</description></item><item><title>32_仓库结构：如何组织文件</title><link>https://artisanbox.github.io/9/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/32/</guid><description>你好，我是LMOS。
你有没有想过，蜜蜂把劳动成果变成蜜糖存放在蜂巢中，人类把劳动成果量化成财富存放在银行，但一个进程的劳动成果放在哪里呢？
看到这里，你可能有疑问，进程有劳动成果吗？当然有，进程加工处理的数据就是进程的劳动成果，可是这个“劳动成果”，如何表示、如何组织，又放在哪里呢？这些问题都会在我们讲解文件系统的过程中一一得到解答。
那今天我们先来搞清楚什么是文件系统，然后解决文件系统如何组织文件，最后对我们文件系统进行设计并抽象成数据结构。好了，下面我们正式开始今天的学习吧。
这节课的配套代码，你可以从这里获取。
什么是文件系统我们经常在计算机上听APE音乐、看4K视频、阅读各种文档、浏览各种精美的网页，这些东西都是一些特定格式的数据，我们习惯把它们叫做文件，这些文件可能储存在HD机械硬盘、SSD固态硬盘、TF卡，甚至远程计算机上。
所以你可以这样理解，文件系统解决的就是如何把许多文件储存在某一种储存设备上，方便进程对各种文件执行打开、关闭、读写、增加和删除等操作。因为这些操作实际上非常复杂，所以操作系统中分出一个子系统专门处理这些问题，这个系统就叫文件系统。
文件系统的核心现在我们还没法直观地感受到，但是它在上层为用户或者进程提供了一个逻辑视图，也就是目录结构。
下图中就是典型的文件系统逻辑视图，从/（根）目录开始，就能找到每个文件、每个目录和每个目录下的所有文件。我们可以看出目录也是文件的一部分，它也扮演了“组织仓库管理员”的角色，可以对文件进行分层分类，以便用户对众多文件进行管理。
虽然这看上去好像有点复杂、是个技术活，但是别怕，毕竟我们不是干这事的第一批人，可以参考别人的设计与实现。好了，废话不多说，难不难，要做了才知道……
文件系统设计既然要实现一个文件系统，还是要好好设计一下，我们首先从三个问题出发对文件系统设计方面的思考。
文件系统为什么可以是一个设备开始，以及它在整个Cosmos内核中的位置格局？ 文件数据的格式以及储存介质的最小单位是什么？ 如何组织越来越多的文件。 搞清楚这三大问题的过程，就是设计文件系统的过程，这里是重点中的重点，你可以停下来好好揣摩，然后再继续往下学习。
文件系统只是一个设备HD机械硬盘、SSD固态硬盘、U盘、各种TF卡等都属于存储设备，这些设备上的文件储存格式都不相同，甚至同一个硬盘上不同的分区的储存格式也不同。这个储存格式就是相应文件系统在储存设备上组织储存文件的方式。
例如我们经常看到的：FAT32、NTFS、Ext4、Btrfs、ZFS、HPFS等，这些都是不同的文件系统建立的文件系统格式。
看到上面储存设备与文件系统多样性的情况之后，不难发现让文件系统成为Cosmos内核中一部分，是个非常愚蠢的想法。那怎么解决这个困难呢，你可以先自己想一想，然后再参考我后面的分析。
针对前面的困难，我们不难提出这样两点设想：第一，文件系统组件是独立的与内核分开的；第二，操作系统需要动态加载和删除不同的文件系统组件，这样就可以适应复杂的情况了。例如，硬盘上不同的分区有不同的文件系统格式，还可以拔插U盘、TF卡等。
你还记得前面Cosmos内核的设备驱动的设计吗？如果文件系统也是Cosmos内核下的一个设备，那就好办多了，因为不同的设备驱动程序可以动态加载，而且可以建立多个文件系统设备，而对各个文件系统设备驱动程序的实现，就是各个文件系统的实现。
刚好前面的驱动模型中（第30节课），定义了文件系统的设备类型。这个架构我给你画一幅图，你看一下就明白了。
这里我不仅给出了文件系统设备的架构，还简单地梳理了内核中其它组件与文件系统的关系。
如图所示，文件系统下面有诸如U盘、硬盘、SSD、CD、TF卡等储存设备。文件系统一定要有储存设备，这个储存设备可以是硬盘，也可以是TF卡，总之能储存数据的设备就行。
为了减小程序的复杂程度，我们使用一块4MB大小的内存空间来模拟储存设备，何况又不是我们第一次建造内存文件系统（ramfs），只是我们做得更小。在文件系统设备驱动程序的入口函数中，分配4MB大小的内存空间。
相信即使如此，也能让我们清楚地看到文件系统的实现。等哪天有时间了，写好了硬盘驱动程序，也可以让文件系统设备驱动程序处理好了数据，然后发送给硬盘设备驱动程序，让其写入到硬盘中去。
这在我们设计的驱动模型中是完全允许的，这就形成了储存系统的“I/O栈”。
文件格式与储存块通常说的文件，都是一堆数据，当我们把这堆数据组织成一个文件，储存在储存介质上时，就有了一个问题：我们按什么格式把这些数据存放在储存介质上。
当然，这个格式是指文件系统存放文件数据的格式。文件数据本身的格式，文件系统不该多管，例如MP3、Word文档的内部格式，各不相同。
关于文件系统存放文件数据的格式，类UNIX系统和Windows系统都采用了相同的方案，那就是逻辑上认为一个文件就是一个可以动态增加、减少的线性字节数组，即文件数据的每个字节都一一对应到这个线性数组中的每个元素。
那么我们也和它们一样，我来给你画个图梳理逻辑关系。
图中的文件数据字节数组，终究是逻辑上的，所以问题又来了，我们如何把这个逻辑上的文件数据字节数组，映射到具体的储存设备上呢？只有解决了这个问题，才能真正储存数据。
现在的机械硬盘、SSD固态硬盘、TF卡，它们都是以储存块为单位储存数据的，一个储存块的大小可以是512、1024、2048、4096字节，访问这些储存设备的最小单位也是一个储存块，不像内存设备可以最少访问一个字节。
文件系统把文件数据定义成一个动态的线性字节数组，可是一开始我们不知道这个数组是多大，需要分配多少个物理储存块，最好是把这个动态的线性字节数组分成一个个数据块。
然而，不同的储存设备的物理储存块的大小不同，有的是512字节，而有的是4096字节，我们为了文件系统能工作在不同的储存设备上，所以我们把这里的数据块定义为文件系统逻辑块，其大小为4096字节，最后把这个逻辑块映射到一个或多个物理储存块。
为了让你更好地理解这个过程，我为你准备了一幅图，如下所示。
从这幅图里，我们可以看到从文件这个抽象概念，它是如何一步步从文件字节数组，整合形成文件数据逻辑块，最后映射到储存介质上的物理储存块。你需要先掌握整个演变过程的逻辑，具体怎么实现我们后面继续讲。
如何组织文件现在PC机上的文件数量都已经上十万的数量级了，网络服务器上更是不止这个数量。
我们不难想到，如果把十万个文件顺序地排列在一起，要找出其中一个文件，那是非常困难的，即使是计算机程序查找起来也是相当慢的，加上硬盘、TF卡之类的储存设备比内存慢得多，因此会变得更慢。
所以，需要一个叫文件目录或者叫文件夹的东西，我们习惯称其为目录。这样我们就可以用不同的目录来归纳不同的文件，例如在MP3目录下存放MP3音乐文件，或者在MP4目录下存放视频文件。同时，目录之下还可以创建目录，这样就建立了非常好的层次关系。
你可能经常在LINUX系统中看到如：“/dev/kvm，/user/bin/gcc”之类的东西，其中dev、user、bin它们就是目录，kvm、gcc它们就是文件，“/”符号就是文件路径分隔符，它们合起来就是文件路径名。
可以看出，整个文件层次结构就像是一棵倒挂的树。前面那幅图已经显示出了这种结构。后面我们的文件系统也会采用目录来组织文件。这里你只要明白，文件数量多了就出现了目录，而目录是用来帮助用户组织或归纳文件的就行了。
文件系统数据结构一路走来，不难发现操作系统内核的任何组件的实现，都需要设计一套相应的数据结构，文件系统也不例外。
根据前面我们对文件系统的设计，我们至少需要表示文件和目录的数据结构，除此之外，还需要表示文件系统本身的一些数据结构，这些数据结构我们称为文件系统元数据。下面我们先从文件系统元数据开始吧！
设计超级块一个文件系统有很多重要的信息，例如文件系统标识、版本、状态，储存介质大小，文件系统逻辑储存块大小，位图所在的储存块，还有根目录等。因为这些信息很重要，没有它们就等于没有文件系统，所以包含这些信息的数据结构，就叫做文件系统的超级块或者文件系统描述块。
下面我们就来设计超级块的数据结构，先在cosmos/include/drvinc/目录下建立一个drvrfs_t.h文件，写下rfssublk_t结构，代码如下所示。
typedef struct s_RFSSUBLK { spinlock_t rsb_lock;//超级块在内存中使用的自旋锁 uint_t rsb_mgic;//文件系统标识 uint_t rsb_vec;//文件系统版本 uint_t rsb_flg;//标志 uint_t rsb_stus;//状态 size_t rsb_sz;//该数据结构本身的大小 size_t rsb_sblksz;//超级块大小 size_t rsb_dblksz;//文件系统逻辑储存块大小，我们这里用的是4KB uint_t rsb_bmpbks;//位图的开始逻辑储存块 uint_t rsb_bmpbknr;//位图占用多少个逻辑储存块 uint_t rsb_fsysallblk;//文件系统有多少个逻辑储存块 rfsdir_t rsb_rootdir;//根目录，后面会看到这个数据结构的 }rfssublk_t; 我们文件系统的超级块，保存在储存设备的第一个4KB大小的逻辑储存块中，但是它本身的大小没有4KB，多余的空间用于以后扩展。rfsdir_t数据结构是一个目录数据结构，你先有个印象，后面我们会有介绍的。</description></item><item><title>33_仓库划分：文件系统的格式化操作</title><link>https://artisanbox.github.io/9/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/33/</guid><description>你好，我是LMOS。
上一节课中，我们已经设计好了文件系统数据结构，相当于建好了仓库的基本结构。
今天，我将和你一起探索仓库的划分，即什么地方存放仓库的管理信息，什么地方存放进程的“劳动成果”（也就是文件），对应于文件系统就是文件系统的格式化操作。
具体我是这样安排的，我们先来实现文件系统设备驱动，接着建立文件系统超级块，然后建立根目录，最后建立文件系统的位图。下面，我们先从建立文件系统设备开始。
这节课的配套代码，你可以从这里获取。
文件系统设备根据我们前面的设计，文件系统并不是Cosmos的一部分，它只是Cosmos下的一个设备。
既然是设备，那就要编写相应的设备驱动程序。我们首先得编写文件系统设备的驱动程序。由于前面已经写过驱动程序了，你应该对驱动程序框架已经很熟悉了。
我们先在cosmos/drivers/目录下建立一个drvrfs.c文件，在里面写下文件系统驱动程序框架代码，如下所示。
drvstus_t rfs_entry(driver_t* drvp,uint_t val,void* p){……} drvstus_t rfs_exit(driver_t* drvp,uint_t val,void* p){……} drvstus_t rfs_open(device_t* devp,void* iopack){……} drvstus_t rfs_close(device_t* devp,void* iopack){……} drvstus_t rfs_read(device_t* devp,void* iopack){……} drvstus_t rfs_write(device_t* devp,void* iopack){……} drvstus_t rfs_lseek(device_t* devp,void* iopack){……} drvstus_t rfs_ioctrl(device_t* devp,void* iopack){……} drvstus_t rfs_dev_start(device_t* devp,void* iopack){……} drvstus_t rfs_dev_stop(device_t* devp,void* iopack){……} drvstus_t rfs_set_powerstus(device_t* devp,void* iopack){……} drvstus_t rfs_enum_dev(device_t* devp,void* iopack){……} drvstus_t rfs_flush(device_t* devp,void* iopack){……} drvstus_t rfs_shutdown(device_t* devp,void* iopack){……} 这个框架代码我们已经写好了，是不是感觉特别熟悉？这就是我们开发驱动程序的规范操作。下面，我们来建立文件系统设备。
按照之前的设计（如果不熟悉可以回顾第32课），我们将使用4MB内存空间来模拟真实的储存设备，在建立文件系统设备的时候分配一块4MB大小的内存空间，这个内存空间我们用一个数据结构来描述，这个数据结构的分配内存空间的代码如下所示。
typedef struct s_RFSDEVEXT { spinlock_t rde_lock;//自旋锁 list_h_t rde_list;//链表 uint_t rde_flg;//标志 uint_t rde_stus;//状态 void* rde_mstart;//用于模拟储存介质的内存块的开始地址 size_t rde_msize;//内存块的大小 void* rde_ext;//扩展所用 }rfsdevext_t; drvstus_t new_rfsdevext_mmblk(device_t* devp,size_t blksz) { //分配模拟储存介质的内存空间，大小为4MB adr_t blkp= krlnew(blksz); //分配rfsdevext_t结构实例的内存空间 rfsdevext_t* rfsexp=(rfsdevext_t*)krlnew(sizeof(rfsdevext_t)); //初始化rfsdevext_t结构 rfsdevext_t_init(rfsexp); rfsexp-&amp;gt;rde_mstart=(void*)blkp; rfsexp-&amp;gt;rde_msize=blksz; //把rfsdevext_t结构的地址放入device_t 结构的dev_extdata字段中，这里dev_extdata字段就起作用了 devp-&amp;gt;dev_extdata=(void*)rfsexp;.</description></item><item><title>34_仓库管理：如何实现文件的六大基本操作？</title><link>https://artisanbox.github.io/9/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/34/</guid><description>你好，我是LMOS。
我们在上一节课中，已经建立了仓库，并对仓库进行了划分，就是文件系统的格式化。有了仓库就需要往里面存取东西，对于我们的仓库来说，就是存取应用程序的文件。
所以今天我们要给仓库增加一些相关的操作，这些操作主要用于新建、打开、关闭、读写文件，它们也是文件系统的标准功能，自然即使我们这个最小的文件系统，也必须要支持。
好了，话不多说，我们开始吧。这节课的配套代码，你可以从这里下载。
辅助操作通过上一节课的学习，我们了解了文件系统格式化操作，不难发现文件系统格式化并不复杂，但是它们需要大量的辅助函数。同样的，完成文件相关的操作，我们也需要大量的辅助函数。为了让你更加清楚每个实现细节，这里我们先来实现文件操作相关的辅助函数。
操作根目录文件根据我们文件系统的设计，不管是新建、删除、打开一个文件，首先都要找到与该文件对应的rfsdir_t结构。
在我们的文件系统中，一个文件的rfsdir_t结构就储存在根目录文件中，所以想要读取文件对应的rfsdir_t结构，首先就要获取和释放根目录文件。
下面我们来实现获取和释放根目录文件的函数，代码如下所示。
//获取根目录文件 void* get_rootdirfile_blk(device_t* devp) { void* retptr = NULL; rfsdir_t* rtdir = get_rootdir(devp);//获取根目录文件的rfsdir_t结构 //分配4KB大小的缓冲区并清零 void* buf = new_buf(FSYS_ALCBLKSZ); hal_memset(buf, FSYS_ALCBLKSZ, 0); //读取根目录文件的逻辑储存块到缓冲区中 read_rfsdevblk(devp, buf, rtdir-&amp;gt;rdr_blknr) retptr = buf;//设置缓冲区的首地址为返回值 goto errl1; errl: del_buf(buf, FSYS_ALCBLKSZ); errl1: del_rootdir(devp, rtdir);//释放根目录文件的rfsdir_t结构 return retptr; } //释放根目录文件 void del_rootdirfile_blk(device_t* devp,void* blkp) { //因为逻辑储存块的头512字节的空间中，保存的就是fimgrhd_t结构 fimgrhd_t* fmp = (fimgrhd_t*)blkp; //把根目录文件回写到储存设备中去，块号为fimgrhd_t结构自身所在的块号 write_rfsdevblk(devp, blkp, fmp-&amp;gt;fmd_sfblk) //释放缓冲区 del_buf(blkp, FSYS_ALCBLKSZ); return; } 上述代码中，get_rootdir函数的作用就是读取文件系统超级块中rfsdir_t结构到一个缓冲区中，del_rootdir函数则是用来释放这个缓冲区，其代码非常简单，我已经帮你写好了。
获取根目录文件的方法也很容易，根据超级块中的rfsdir_t结构中的信息，读取根目录文件的逻辑储存块就行了。而释放根目录文件，就是把根目录文件的储存块回写到储存设备中去，最后释放对应的缓冲区就可以了。</description></item><item><title>35_瞧一瞧Linux：虚拟文件系统如何管理文件？</title><link>https://artisanbox.github.io/9/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/35/</guid><description>你好，我是LMOS。
在前面的课程中，我们已经实现了Cosmos下的文件系统rfs，相信你已经感受到了一个文件系统是如何管理文件的。今天我们一起来瞧一瞧Linux是如何管理文件，也验证一下Linux那句口号：一切皆为文件。
为此，我们需要首先搞清楚什么是VFS，接着理清为了实现VFS所用到的数据结构，然后看看一个文件的打开、读写、关闭的过程，最后我们还要亲自动手实践，在VFS下实现一个“小”且“能跑”的文件系统。
下面让我们开始吧！这节课的配套代码，你可以从这里下载。
什么是VFSVFS（Virtual Filesystem）就像伙伴系统、SLAB内存管理算法一样，也是SUN公司最早在Sloaris上实现的虚拟文件系统，也可以理解为通用文件系统抽象层。Linux又一次“白嫖”了Sun公司的技术。
在Linux中，支持EXT、XFS、JFS、BTRFS、FAT、NTFS等多达十几种不同的文件系统，但不管在什么储存设备上使用什么文件系统，也不管访问什么文件，都可以统一地使用一套open(), read()、write()、close()这样的接口。
这些接口看上去都很简单，但要基于不同的存储设备设计，还要适应不同的文件系统，这并不容易。这就得靠优秀的VFS了，它提供了一个抽象层，让不同的文件系统表现出一致的行为。
对于用户空间和内核空间的其他部分，这些文件系统看起来都是一样的：文件都有目录，都支持建立、打开，读写、关闭和删除操作，不用关注不同文件系统的细节。
我来给你画张图，你一看就明白了。
你有没有发现，在计算机科学领域的很多问题，都可以通过增加一个中间的抽象层来解决，上图中Linux的VFS层就是应用和许多文件系统之间的抽象层。VFS向上对应用提供了操作文件的标准接口，向下规范了一个文件系统要接入VFS必需要实现的机制。
后面我们就会看到，VFS提供一系列数据结构和具体文件系统应该实现的回调函数。这样，一个文件系统就可以被安装到VFS中了。操作具体文件时，VFS会根据需要调用具体文件系统的函数。从此文件系统的细节就被VFS屏蔽了，应用程序只需要调用标准的接口就行了。
VFS数据结构VFS为了屏蔽各个文件系统的差异，就必须要定义一组通用的数据结构，规范各个文件系统的实现，每种结构都对应一套回调函数集合，这是典型的面向对象的设计方法。
这些数据结构包含描述文件系统信息的超级块、表示文件名称的目录结构、描述文件自身信息的索引节点结构、表示打开一个文件的实例结构。下面我们依次探讨这些结构。
超级块结构首先我们来看一看超级块结构，这个结构用于一个具体文件系统的相关信息，其中包含了VFS规定的标准信息，也有具体文件系统的特有信息，Linux系统中的超级块结构是一个文件系统安装在VFS中的标识。我们来看看代码，如下所示。
struct super_block { struct list_head s_list; //超级块链表 dev_t s_dev; //设备标识 unsigned char s_blocksize_bits;//以位为单位的块大小 unsigned long s_blocksize;//以字节为单位的块大小 loff_t s_maxbytes; //一个文件最大多少字节 struct file_system_type *s_type; //文件系统类型 const struct super_operations *s_op;//超级块函数集合 const struct dquot_operations *dq_op;//磁盘限额函数集合 unsigned long s_flags;//挂载标志 unsigned long s_magic;//文件系统魔数 struct dentry *s_root;//挂载目录 struct rw_semaphore s_umount;//卸载信号量 int s_count;//引用计数 atomic_t s_active;//活动计数 struct block_device *s_bdev;//块设备 void *s_fs_info;//文件系统信息 time64_t s_time_min;//最小时间限制 time64_t s_time_max;//最大时间限制 char s_id[32]; //标识名称 uuid_t s_uuid; //文件系统的UUID struct list_lru s_dentry_lru;//LRU方式挂载的目录 struct list_lru s_inode_lru;//LRU方式挂载的索引结点 struct mutex s_sync_lock;//同步锁 struct list_head s_inodes; //所有的索引节点 spinlock_t s_inode_wblist_lock;//回写索引节点的锁 struct list_head s_inodes_wb; //挂载所有要回写的索引节点 } __randomize_layout; 上述代码中我删除了我们现在不用关注的代码，在文件系统被挂载到VFS的某个目录下时，VFS会调用获取文件系统自己的超级块的函数，用具体文件系统的信息构造一个上述结构的实例，有了这个结构实例，VFS就能感知到一个文件系统插入了。</description></item><item><title>36_从URL到网卡：如何全局观察网络数据流动？</title><link>https://artisanbox.github.io/9/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/36/</guid><description>你好，我是 LMOS。
从这节课起，我们就要开始学习网络篇的内容了。网络是一个极其宏大的知识结构，我会通过五节课带你了解计算机网络的关键内容。
具体我们是这样安排的。作为网络篇的开始，今天这节课我会从一个面试中高频出现的问题切入，带你梳理从输入URL到网卡的网络数据流动过程中都发生了什么事。如果你真正理解了这个过程，相信你对整个网络架构的认知也会有质的飞跃。
网络篇的第二节课，我会带你分析网络数据包在内核中如何流转；第三节课，我们一起探讨互联网架构演进过程，并动手做一次协议栈移植；最后两节课，我还是照例带你看看Linux，让你理解套接字在Linux内核中怎样实现。
从一道经典面试题说起下面我们一起来看看一个问题，估计你多多少少会觉得熟悉。
输入URL，从一个请求到响应都发生了什么事？
没错，这是一道非常经典的面试题，你在网上随便一搜，也会找到各种各样的资料解答这道题。
不过啊，那些答案都有一些笼统，今天我会尽量详细地为你梳理一下这个过程。跟着我学完这节课，你就能明白，为什么面试官对这道题青睐有加了。
这里我先给你概括一下全过程，让你有个整体印象。
1.常规的网络交互过程是从客户端发起网络请求，用户态的应用程序（浏览器）会生成HTTP请求报文、并通过DNS协议查找到对应的远端IP地址。
2.在套接字生成之后进入内核态，浏览器会委托操作系统内核协议栈中的上半部分，也就是TCP/UDP协议发起连接请求。
3.然后经由协议栈下半部分的IP协议进行封装，使数据包具有远程定位能力。
4.经过MAC层处理，找到接收方的目标MAC地址。
5.最终数据包在经过网卡转化成电信号经过交换机、路由器发送到服务端，服务端经过处理拿到数据，再通过各种网络协议把数据响应给客户端。
6.客户端拿到数据进行渲染。
7.客户端和服务端之间反复交换数据，客户端的页面数据就会发生变化。
你有没有发现，刚才的过程中，我们提到了多个层级和网络协议，那么网络为什么要分层呢？网络协议又是什么呢？请听我给你一一道来。
前置知识：网络分层和网络协议在计算机网络时代初期，各大厂商推出了不同的网络架构和标准，为统一标准，国际标准化组织ISO推出了统一的OSI参考模型。
当前网络主要遵循的IEEE 802.3标准，就是基于OSI模型提出的，主要定义的是物理层和数据链路层有线物理数据流传输的标准。
那么问题来了，网络为什么要分层呢？
我们都知道网络是复杂的。对于复杂的问题，我们自然要通过分层处理简化问题难度，降低复杂度，由于分层后的各层之间相互独立，我们可以把大问题分割成小问题。同样，分层也保证了网络的松耦合和相对的灵活，分层拆分后易于各层的实现和维护，也方便了各层的后续扩展。
网络分层解决了网络复杂的问题，在网络中传输数据中，我们对不同设备之间的传输数据的格式，需要定义一个数据标准，所以就有了网络协议。
网络协议是双方通信的一种约定，以便双方都可以理解对方的信息。接下来我们就用OSI协议体系中广泛应用的TCP/IP层的体系结构来分析整个过程，你重点需要关注的是数据处理的过程和网络协议。
发起请求阶段（应用层）下面我们首先来看看网络应用层，它是最上层的，也是我们能直接接触到的。
我们的电脑或⼿机使⽤的应⽤软件都是在应⽤层实现，所以应⽤层只需要专注于为⽤户提供应⽤功能，不⽤去关⼼数据是如何传输的。你可以这样理解，应⽤层是⼯作在操作系统中的⽤户态。
我们依然从浏览器中输入URL，开始了解网络应用层的工作流程。
用户输入：在浏览器中输入URL我们在浏览器中输入URL的时候，浏览器已经开始工作了。浏览器会根据我们的输入内容，先匹配对应的URL以及关键词，给出输入建议，同时校验URL的合法性，并且会在URL前后补全URL。
为了帮你更好地理解，我给你举个例子说明。我们以输入cosmos.com为例，首先浏览器会判断出这是一个合法的URL，并且会补全为http://www.cosmos.com。
其中http为协议，cosmos.com为网络地址，每个网络栏的地址都符合通用 URI 的语法。URI 一般语法由五个分层序列组成。后面的第一行内容我给你列了URL的格式，第二行做了行为说明。
URI = scheme:[//authority]path[?query][#fragment] URI = 方案:[//授权]路径[?查询][#片段ID] 接着，浏览器从URL中会提取出网络的地址，也叫做主机名（host），一般主机名可以为域名或IP地址，此处使用域名。
对URL进行解析之后，浏览器确定了服务器的主机名和请求路径，接下来就是根据这些信息来生成HTTP请求消息了，那么到现在为止，我们的HTTP请求是否已经发出了呢？并不是这样的，我们接着往下看。
网络请求前：查看浏览器缓存浏览器在HTTP报文生成完成后，它并不是马上就开始网络请求的。
在请求发出之前，浏览器首先会检查保存在本地计算机中的缓存，如果访问过当前的URL，会先进入缓存中查询是否有要请求的文件。此时存在的缓存有路由器缓存、DNS缓存、浏览器缓存、Service Worker、Memory Cache、Disk Cache、Push Cache、系统缓存等。
在这里我们看一下系统缓存，如果在浏览器缓存里没有命中缓存，浏览器会做一个系统调用获得系统缓存中的记录，就是我们的gethostbyname方法，它的作用是通过域名获取IP地址。这个方法会返回如下结构。
struct hostent { char *h_name;// 主机的别名.www.cosmos.com就是google他自己的别名
char **h_aliases;// 主机ip地址的类型，到底是ipv4(AF_INET)，还是pv6(AF_INET6) int h_addrtype;// 主机ip地址的长度 int h_length;// 主机ip地址的长度 char **h_addr_list; // 主机的ip地址，注意，这个是以网络字节序存储的 #define h_addr h_addr_list[0] 这个函数，是将类型为af的网络地址结构src，转换成主机序的字符串形式，存放在长度为cnt的字符串中。返回指向dst的一个指针。如果函数调用错误，返回值是NULL }; 如果没有访问过当前的URL，就会跳过缓存这一步，这时我们就会进入网络操作了。</description></item><item><title>37_从内核到应用：网络数据在内核中如何流转</title><link>https://artisanbox.github.io/9/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/37/</guid><description>你好，我是 LMOS。
上节课我们对一次请求到响应的过程积累了一些宏观认识，相信你已经对整个网络架构有了一个整体蓝图。这节课，让我们来仔细研究一下网络数据是如何在内核中流转的，让你开阔视野，真正理解底层工程的实现思路。
凡事先问目的，在网络数据在内核中的流转，最终要服务于网络收发功能。所以，我会先带你了解一次具体的网络发收过程，然后带你了解lwIP的网络数据收发。有了这些基础，我还会示范一下如何实现协议栈移植，你可以在课后自行动手拓展。
好，让我们正式开始今天的学习吧。课程配套代码，你可以点击这里获取。
先看看一次具体的网络发收过程理解软件的设计思想，最重要的是先要理解需求。而内核中的数据流转也只是为了满足网络收发的需求而进行的设计。
发送过程总览下面我们一起来看看应用程序通过网络发送数据的全过程。
应用程序首先会准备好数据，调用用户态下的库函数。接着调用系统API接口函数，进入到内核态。
内核态对应的系统服务函数会复制应用程序的数据到内核的内存空间中，然后将数据移交给网络协议栈，在网络协议栈中将数据层层打包。
最后，包装好的数据会交给网卡驱动，网卡驱动程序负责将打包好的数据写入网卡并让其发送出去。
我为你准备了一张流程图供你参考，如下所示。
上图中，只是展示了大致流程，其中还有DMA处理、CRC校验、出错处理等细节，但对于我们理解梳理发送流程，这些就够了。
接收过程总览了解了发送数据的过程以后，掌握接收数据的过程就更容易了，因为它就是发送数据的逆过程。
首先，网卡接收到数据，通过DMA复制到指定的内存，接着发送中断，以便通知网卡驱动，由网卡驱动处理中断复制数据。然后网络协议收到网卡驱动传过来的数据，层层解包，获取真正的有效数据。最后，这个数据会发送给用户态监听的应用进程。
为了让你更加直观地了解这一过程，我特意准备了一张流程图供你参考，如下所示。
前面只是帮你梳理一下数据的发送与接收的流程，其实我们真正要关注的是网络协议。可是我们若手动实现一个完整的网络协议，不太现实，网络协议的复杂度大到也许要重新开一门课程，才可以完全解决，所以下面我们借用一下lwIP项目，以这个为基础来讨论网络协议。
认识一下lwIP架构现在我们清楚了一次具体网络发收过程是怎么回事，那怎么让Cosmos实现网络通信呢？这里我们选择lwIP这个TCP/IP协议的轻量级开源项目，让它成为Cosmos的网络部的战略合作伙伴。
lwIP是由瑞典计算机科学研究院（SICS）的Adam Dunkels开发的小型开源TCP/IP协议栈。它是一个用C语言实现的软件组件，一共有两套接口层，向下是操作系统要提供的，向上是提供给应用程序的。这样lwIP就能嵌入到任何操作系统之中工作，并为这个操作系统上的应用软件提供网络功能支持了。
为啥说lwIP是轻量级的呢？很简单，跟Linux比，从代码行数上就能看得出。lwIP的设计目标就是尽量用少量资源消耗，实现一个相对完整的TCP/IP协议栈。
这里的“完整性”主要是指TCP协议的完整性，实现的关键点就是在保持TCP协议主要功能的基础上减少对RAM的占用。同时，lwIP还支持IPv6的标准实现，这也让我们与现代交换设备的对接变得非常方便。
这里额外提供你一份扩展阅读资料，lwIP的项目主页链接，这里包含了大量相关资料，感兴趣的同学可以课后深入了解。另外，lwIP既可以移植到操作系统上运行，也可以在没有操作系统的情况下独立运行。
lwIP在结构上可分为四层：OS层、API层、核心层、硬件驱动层，如下图所示。
第一层
MCU的业务层是lwIP的服务对象，也是其自身代码使用lwIP的地方。大部分时候我们都是从这里入手，通过netconn或lwip_api使用lwIP的各种功能函数。
在典型的TCP通信的客户端应用程序中，一般先要通过netconn_new创建一个struct netconn对象，然后调用netconn_connect连接到服务器，并返回成功或失败。成功后，可以调用netconn_write向服务器发送数据，也可以调用netconn_recv接收数据。最后，关闭连接并通过netconn_close释放资源。
第二层
lwIP的api层是netconn的功能代码所在的层，负责为上层代码提供netconn的api。习惯使用socket的同学也可以使用lwip_socket等函数，以标准的socket方式调用lwIP。新版本增加了http、mqtt等应用的代码，这些额外的应用对目前的物联网通信来说确实很方便。
第三层
lwIP的核心层存放了TCP/IP协议栈的核心代码，它不仅实现了大部分的TCP和UDP功能，还实现了DNS、ICMP、IGMP等协议，同时也实现了内存管理和网络接口功能。
该层提供了sys_arch模块设计，便于将lwIP移植到不同的操作系统，如线程创建、信号量、消息队列等功能。和操作系统相关的真正定义写在了lwip/include/sys.h文件中。
第四层
硬件驱动层提供PHY芯片驱动，用来匹配lwIP的使用。lwIP会调用该层的代码将组装好的数据包发送到网络，同时从网络接收数据包并进行分析，实现通信功能。
lwIP的三套应用程序编程接口理清了架构，我们再说一说lwIP的应用程序编程接口，一共有三套。
原始API：原始的lwIP API。它通过事件回调机制开发应用程序。该应用编程接口提供了最佳的性能和优化的代码长度，但它增加了应用程序开发的复杂性。
Netconn API：是高级的有序API、需要实时操作系统（RTOS）的支持（提供进程间通信的方法）。Netconn API支持多线程。
BSD套接字API：类似伯克利的套接字API（在Netconn API上开发，需要注意NETCONN API 即为 Sequential API）。
对于以上三种接口，前者只需要裸机调用，后两种需要操作系统调用。因此，移植lwIP有两种方法，一种是只移植内核，不过这样之后只能基于RAW/Callback API编写应用程序。第二种是移植内核和上层API。这时应用程序编程可以使用三种API，即RAW/Callback API、顺序API和Socket API。
lwIP执行流程现在，想必聪明的你已经理解了前文中的网络收发过程。
接下来，让我们顺着之前的思路来对应到lwIP在收发过程中的核心函数，具体过程我同样梳理了流程图。你可以结合图里关键的函数名以及步骤顺序，按这个顺序在IwIP代码中检索阅读。
数据发送首先要说的是数据发送过程。
由于我们把lwIP作为Cosmos的一个内核组件来工作，自然要由lwIP接收来自内核上层发来的数据。内核上层首先会调用lwIP的netconn层的接口函数netconn_write，通过这个函数，数据正式流进lwIP组件层。
接着，netconn层调用lwIP组件的TCP层的接口函数tcp_write，在TCP层对数据首次进行打包。然后，TCP层将打包好的数据通过调用io_output函数，向下传递给lwIP组件的IP层，进行打包。
最后，IP层将打包好的数据发送给网卡驱动接口层netif，这里调用了实际的网卡驱动程序，将数据发送出去。
数据接收数据接收的步骤相比数据发送稍微多一些，但也不用害怕，跟住我的讲解思路一定可以理清这个过程。
数据接收需要应用程序首先调用lwIP的netconn层的netconn_recv接口。然后由netconn层调用sys_arch_mbox_fetch函数，进入监听等待相关的mbox。
接着，数据会进入网卡，驱动程序相关的函数负责把它复制到内存。再然后是调用ethernet_input函数，进入ethernet层。完成相关处理后，调用ip4_input函数，数据在lwIP组件的IP层对数据解包，进行相应处理之后，还会调用tcp_input函数，进入lwIP组件的TCP层对数据解包。
最后，调用sys_mbox_trypost函数把数据放入特定的mbox，也就是消息盒子里，这样等待监听的应用程序就能得到数据了。
在了解了lwIP组件收发数据的过程之后，就可以进行移植的相关工作了。lwIP的结构设计非常优秀，这让移植工作变得很容易。我们这里只要了解lwIP组件的sys_arch层的接口函数即可。
下面我们一起了解lwIP的移植细节。
协议栈移植lwIP有两种移植模式，一种是NO_SYS，无操作系统模式，一种是有操作系统模式。用NO_SYS模式比较简单，你可以自行探索。
操作系统模式主要需要基于操作系统的 IPC 机制，对网络连接进行了抽象（信号量、邮箱/队列、互斥体等机制），从而保证内核与应用层API的通讯，这样做的好处是lwIP 内核线程可以只负责数据包的 TCP/IP 封装和拆封，而不用进行数据的应用层处理，从而极大地提高系统对网络数据包的处理效率。</description></item><item><title>38_从单排到团战：详解操作系统的宏观网络架构</title><link>https://artisanbox.github.io/9/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/38/</guid><description>你好，我是 LMOS。
上节课我们学习了单机状态下网络数据在内核中流转的全过程，并且带你一起梳理了网络栈移植的关键步骤。
这节课我会带你看看，现实世界中网络请求是如何穿过重重网络设备，实现大规模组网的。同时，我还会给你讲解网络架构的过去、现在，并展望一下将来的发展趋势。最后我会带你动手搭建一个现代互联网实验环境，通过实际的组网实践加深对网络架构的理解。
从传统网络架构聊起你是否好奇过，我们目前用的互联网是如何做到互联互通的呢？
让我们先来看看传统的三层网络架构，著名的通信设备厂商思科把这种架构叫做分级的互联网络模型（Hierarchical Inter-networking Model）。这种架构的优点是，可以把复杂的网络设计问题抽象为几个层面来解决，每个层面又聚焦于某些特定的功能。这样就能把复杂而庞大的网络问题拆解成比较好解决的子问题。
如下图所示，三层网络架构设计主要包括核心层、汇聚层、接入层这三个层。下面我分别给你说一说。
首先是核心层。交换层的核心交换机为进出数据中心的数据包提供高速转发的功能，为多个汇聚层提供连通性，同时也为整个网络提供灵活的L3路由网络。
然后是汇聚层。汇聚交换机与接入交换机相连，提供防火墙、SSL卸载、入侵检测、网络分析等其他服务。
最后我们来看接入层。接入交换机通常位于机架的顶部，因此它们也被称为ToR交换机，并且它们与服务器物理连接。
当然，观察这个架构我们可以发现，核心层和汇聚层这种骨干网络需要承担的流量是蛮大的，流量大意味着对交换性能、效率有更高的要求。所以为了解决性能、效率等问题，我们需要在OSI的1、2、3层上分别做优化。
这里要说到传统网络架构的不足之处，我们发现经典的IP网络是逐跳转发数据的。转发数据时，每台路由器都要根据包头的目的地址查询路由表，以获得下一跳的出口。这个过程显然是繁琐低效的。
另外，转发路径也不够灵活，为了加以改善，我们在第二层之上、第三层之下引入一个2.5层的技术方案，即多协议标签交换（MPLS）技术。
优化与迭代：MPLS技术目前MPLS技术在国内应用广泛，无论是BAT等互联网巨头，还是运营商建设骨干网都在应用这种技术。MPLS的核心结构如下。
MPLS通过LDP标签分发协议。我来举个例子吧，这相当于把快递标签“贴在”了快递盒子上了，后续只需要读取标签，就能知道这个数据要转发到哪里去了。这样就避免了传统路由网络中每路过一个经手人（每一跳），都要把快递盒子打开看一看的额外开销。
而路径计算元素协议（RSVP-TE）最大的优点是收集整个网络的拓扑和链路状态信息。通过扩展的资源预留协议，可以实现灵活的转发路径选择和规划。这就好比双十一了，物流公司根据物流大数据收集到的路网和拥堵状态等信息，自动规划出性价比最高的路径，显然快递配送效率会得到很大提升。
当然，只在OSI的2、3层之间做优化是远远不够的，为了满足动辄数百G传输需求，物理层也经历了从DWDM（Dense Wavelength Division Multiplexing）波分复用系统这种波分复用技术到OTN（Iptical Transport Network，光传送网）的技术演进。感兴趣的同学可以搜索光传送网和波分复用相关的资料，这里我就不展开了。
根据前面的讲解我们发现，传统网络基础架构确实可以解决不少问题，但这样真的完美了么？其实不然，比如前面的MPLS技术虽然也解决了问题，但也加重了耦合，并且存在资源利用率低、复杂度高、价格昂贵等缺点。
所以后来SR（Segment Routing）技术又应运而生，而随着IPv6的演进，我们用SRv6替代MPLS技术也是大势所趋。
另外，我们还要注意到业务需求的变化。比如随着云与5G等移动通信的发展，流量除了以前客户端和服务端的南北向通信之外，服务端分布式服务之间也会引入了大量的通信流量。甚至随着云与容器的演进，服务端会存在大量的虚拟机迁移等动作。这些对传统网络中STP拓扑变化、收敛以及网络规模都带来了巨大的挑战。
那么如何解决传统三层网络架构带来的挑战呢？答案其实在贝尔实验室的Charles Clos博士在1953年的《无阻塞交换网络研究》之中。论文中提到的核心思想是：用多个小规模、低成本的单元，构建复杂、大规模的网络。
论文中提到的简单的CLOW网络是包含输入级别、中间级别和输出级别的三级互连体系结构。
下图中的矩形表示规模较小的转发单元，其成本显然也相对较低。CLOS的本质可以简单理解为是一种多级交换的架构思想，并且这种架构很适合在输入和输出持续增加的情况下将中间交叉数降至最低。
下图中，m是每个子模块的输入端口数，n是每个子模块的输出端口数，r是每一级的子模块数，经过合理的重排，只要满足公式：
r2≥max(m1,n3) 那么，对于任意的输入到输出，总是能找到一条无阻塞的通路。
直到1990年代，CLOS架构被应用到Switch Fabric。应用CLOS架构的交换机的开关密度，与交换机端口数量N的关系如下。
O(N^(3/2)) 可以看到，在N较大时，CLOS模型能降低交换机内部的开关密度。由此可见，越来越多的人发现了传统三层网络架构下的痛点，于是一种叫做胖树的网络架构应运而生（感兴趣的同学可以在搜索《A Scalable, Commodity Data Center Network Architecture》这篇论文）。
而借鉴Fattree和CLOS模型的思想，目前业界衍生出了叶脊（Spine-Leaf）网络架构。目前通过FaceBook、Google等公司大量实践的事实已经证明，Spine-Leaf网络架构可以提供高带宽、低延迟、非阻塞、可扩展的服务器到服务器连接。
这种新一代架构在工程实践中的代表之一，则正是Google的B4网络，接下来就让我们一起看一下Google B4网络的架构。
谈谈Google B4Google的研究员Amin Vahdat曾经说过：“如果没有软件定义网络，那Google就不会是今天的Google。”
为了实现实现数据中心的互联互通，谷歌设计并搭建了B4网络，实现了数据在各个公司园区之间的实时复制。
B4网络的核心架构由Google设计的控制软件和白盒交换机构成。谷歌的目标是建立一个类似于广域网的镜像网络，随着网络规模的不断扩展，目前谷歌的大部分业务都已经运行在B4上了。
接下来让我们来看一下Google Google B4的架构图（下面4张图出自Google B4网络论文）：
B4网络的其实也是由三层构成，但这个和传统网络的“三层架构”又不太一样。这里指的是物理设备层（Switch Hardware）、局部网络控制层（Site Controllers）和全局控制层（Global）。
全局控制层中的SDN网关和TE服务器会在全局进行统一控制，而每个数据中心（Site）则会通过Site Controller来控制物理交换机，从而实现将网络的控制面和数据面分离的效果。
第一层：物理设备层我们首先来看第一层的物理交换设备，它是Google自研并请ODM厂商代工的白盒交换机。这个自研的交换机使用了24颗16×10Gb的芯片，还携带了128个10Gb网口。
交换机里面运行的是OpenFlow协议。但众所周知，交换机内的专用芯片从研发设计到最终流片其实周期和成本还是很高的。
那如何让专用的交换机芯片跟OpenFlow更好地进行协同呢？为了解决这个问题，Google采用了TTP方案。实际运行时交换机则会把像访问控制列表（ACL）、路由表、隧道表之类的关键数据通过BGP/IS-IS协议报文送到Controller，由Controller进行处理。
第二层：局部网络控制层B4网络中，一个Controller服务可以控制多个交换机。而为了保证可用性，一个交换机是可以连接多个Controller服务的，而同一时间只会有一个Controller服务为这台交换机提供服务，并且一个数据中心中会包含由多个Controller服务实例构成的服务集群。
在局部网络控制层中，还会使用Paxos协议负责所有控制功能的领导者（leader）选举。
具体过程是这样的，每个节点上的Paxos实例对给定控制功能的可用副本集做应用程序级别的健康检测。当大多数的Paxos实例检测到故障时，他们就会从剩余的可用服务器集中选出一个新的负责人。然后，Paxos会将递增的ID号回调给当选的leader。leader使用这个ID来向客户表明自己的身份。
第三层全局控制层（Global）负责全局控制的TE Server通过SDN Gateway从各个数据中心的控制器收集链路信息，从而掌握路径状态。这些路径以IP-In-IP隧道的方式创建，通过SDN网关到达Onix控制器，最后下达到交换机。</description></item><item><title>39_瞧一瞧Linux：详解socket实现与网络编程接口</title><link>https://artisanbox.github.io/9/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/39/</guid><description>你好，我是LMOS。
前面我们了解了网络的宏观架构，建立了网络模块知识的大局观，也进行了实际的组网实践。现在我们来瞧一瞧Linux的网络程序，不过想要入门Linux的网络编程，套接字也是一个绕不开的重要知识点，正是有了套接字，Linux系统才拥有了网络通信的能力。而且网络协议的最底层也是套接字，有了这个基础，你再去看相关的网络协议的时候也会更加轻松。
我会通过两节课的内容带你了解套接字的原理和具体实现。这节课，我们先来了解套接字的作用、工作原理和关键数据结构。下一节课，我们再一起研究它在Linux内核中的设计与实现。
好，让我们开始今天的学习吧。
如何理解套接字根据底层网络机制的差异，计算机网络世界中定义了不同协议族的套接字（socket），比如DARPA Internet地址（Internet套接字）、本地节点的路径名（Unix套接字）、CCITT X.25地址（X.25 套接字）等。
我们会重点讲解跟网络子系统和TCP/IP协议栈息息相关的一种套接字——Internet 套接字。如果你对其他类型的套接字有兴趣，可以自行阅读这里的资料。
Internet套接字是TCP/IP协议栈中传输层协议的接口，也是传输层以上所有协议的实现。
同时，套接字接口在网络程序功能中是内核与应用层之间的接口。TCP/IP协议栈的所有数据和控制功能都来自于套接字接口，与OSI网络分层模型相比，TCP/IP协议栈本身在传输层以上就不包含任何其他协议。
在Linux操作系统中，替代传输层以上协议实体的标准接口，称为套接字，它负责实现传输层以上所有的功能，可以说套接字是TCP/IP协议栈对外的窗口。
Linux套接字API适合所有的应用标准，现在的应用层协议也全部移植到了Linux系统中。但请你注意，在套接字层下的基础体系结构实现却是Linux系统独有的，Linux内核支持的套接字结构如图所示。
我们创建套接字时，可以通过参数选择协议族，为应用程序指定不同的网络机制。如果指定为PF_INET协议族，这里的套接字就叫做INET套接字，它的套接字接口函数提供了TCP/IP网络服务功能。现在我先带你了解一下套接字的数据结构。
套接字的数据结构在Linux操作系统下，对套接字、套接字的属性、套接字传输的数据格式还有管理套接字连接状态的数据结构分别做了一系列抽象定义。
每个程序使用的套接字都有一个struct socket数据结构与struct sock数据结构的实例。
Linux内核在套接字层定义了包含套接字通用属性的数据结构，分别是struct socket与struct sock，它们独立于具体协议；而具体的协议族与协议实例继承了通用套接字的属性，加入协议相关属性，就形成了管理协议本身套接字的结构。
struct socket数据结构struct socket是套接字结构类型，每个套接字在内核中都对应唯一的struct socket结构（用户程序通过唯一的套接字描述符来表示套接字，且描述符与struct socket结构一一对应）。
我们来看看struct socket数据结构是什么样，代码如下，我相信配合注释你有能力理解它。
struct socket { socket_state state; // 套接字的状态 unsigned long flags; // 套接字的设置标志。存放套接字等待缓冲区的状态信息，其值的形式如SOCK_ASYNC_NOSPACE等 struct fasync_struct *fasync_list; // 等待被唤醒的套接字列表，该链表用于异步文件调用 struct file *file; // 套接字所属的文件描述符 struct sock *sk; // 指向存放套接字属性的结构指针 wait_queue_head_t wait; //套接字的等待队列 short type; // 套接字的类型。其取值为SOCK_XXXX形式 const struct proto_ops *ops; // 套接字层的操作函数块 } struct sock数据结构在Linux内核的早期版本中，struct sock数据结构非常复杂。从Linux2.</description></item><item><title>40_瞧一瞧Linux：详解socket的接口实现</title><link>https://artisanbox.github.io/9/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/40/</guid><description>你好，我是LMOS。
上节课，我们一起了解了套接字的工作机制和数据结构，但套接字有哪些基本接口实现呢？相信学完这节课，你就能够解决这个问题了。
今天我会和你探讨套接字从创建、协议接口注册与初始化过程，还会为你深入分析套接字系统，是怎样调用各个功能函数的。通过这节课，相信你可以学会基于套接字来编写网络应用程序。有了之前的基础，想理解这节课并不难，让我们正式开始吧。
套接字接口套接字接口最初是BSD操作系统的一部分，在应用层与TCP/IP协议栈之间接供了一套标准的独立于协议的接口。
Linux内核实现的套接字接口，将UNIX的“一切都是文件操作”的概念应用在了网络连接访问上，让应用程序可以用常规文件操作API访问网络连接。
从TCP/IP协议栈的角度来看，传输层以上的都是应用程序的一部分，Linux与传统的UNIX类似，TCP/IP协议栈驻留在内核中，与内核的其他组件共享内存。传输层以上执行的网络功能，都是在用户地址空间完成的。
Linux使用内核套接字概念与用户空间套接字通信，这样可以让实现和操作变得更简单。Linux提供了一套API和套接字数据结构，这些服务向下与内核接口，向上与用户空间接口，应用程序正是使用这一套API访问内核中的网络功能。
套接字的创建在应用程序使用TCP/IP协议栈的功能之前，我们必须调用套接字库函数API创建一个新的套接字，创建好以后，对库函数创建套接字的调用，就会转换为内核套接字创建函数的系统调用。
这时，完成的是通用套接字创建的初始化功能，跟具体的协议族并不相关。
这个过程具体是这样的，在应用程序中执行socket函数，socket产生系统调用中断执行内核的套接字分路函数sys_socketcall，在sys_socketcall套接字函数分路器中将调用传送到sys_socket函数，由sys_socket函数调用套接字的通用创建函数sock_create。
sock_create函数完成通用套接字创建、初始化任务后，再调用特定协议族的套接字创建函数。
这样描述你可能还没有直观感受，我特意画了图，帮你梳理socket创建的流程，你可以对照图片仔细体会调用过程。
结合图解，我再用一个具体例子帮你加深理解，比如由AF_INET协议族的inet_create函数完成套接字与特定协议族的关联。
一个新的struct socket数据结构起始由sock_create函数创建，该函数直接调用__sock_create函数，__sock_create函数的任务是为套接字预留需要的内存空间，由sock_alloc函数完成这项功能。
这个sock_alloc函数不仅会为struct socket数据结构实例预留空间，也会为struct inode数据结构实例分配需要的内存空间，这样可以使两个数据结构的实例相关联。__sock_create函数代码如下。
static int __sock_create(struct net *net, int family, int type, int protocol, struct socket **res, int kern) { int err; struct socket *sock; const struct net_proto_family *pf; // 首先检验是否支持协议族 /* * 检查是否在内核支持的socket范围内 */ if (family &amp;lt; 0 || family &amp;gt;= NPROTO) return -EAFNOSUPPORT; if (type &amp;lt; 0 || type &amp;gt;= SOCK_MAX) return -EINVAL; /* * 为新的套接字分配内存空间，分配成功后返回新的指针 */ sock = sock_alloc(); } sock_alloc函数如下所示。</description></item><item><title>41_服务接口：如何搭建沟通桥梁？</title><link>https://artisanbox.github.io/9/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/41/</guid><description>你好，我是LMOS。
一路走来，咱们的Cosmos系统已经有内存管理，进程、文件、I/O了，这些重要的组件已经建立了，也就是说它们可以向应用程序提供服务了。
但就好像你去各政府部门办理业务证件一样，首先是前台工作人员接待你，对你的业务需求进行初级预判，然后后台人员进行审核并进行业务办理，最后由前台人员回复，并且给你开好相关业务证件。
今天，我们就来实现Cosmos下的“前台工作人员”，我们称之为服务接口，也可以说是Cosmos的API。代码你可以从这里下载。
服务接口的结构我们先来设计一下服务接口的整体结构，即Cosmos的API结构。因为Cosmos的API数量很多，所以我们先来分个类，它们分别是进程类、内存类、文件类和时间类的API。这些API还会被上层C库封装，方便应用程序调用。
为了帮你理解它们之间的关系，我为你准备了一幅图，如下所示。
结合上图可以看到，我们的应用程序库分为时间库、进程库、内存库、文件库这几种类型。
通常情况下，应用程序中调用的是一些库函数。库函数是对系统服务的封装，有的库函数是直接调用相应的系统服务；而有的库函数为了完成特定的功能，则调用了几个相应的系统服务；还有一些库函数完成的功能不需要调用相应的系统调用，这时前台接待人员也就是“库函数”，可以自行处理。
如何进入内核由上图我们还可以看出，应用程序和库函数都在用户空间中，而系统服务却在内核空间中，想要让代码控制流从用户空间进入到内核空间中，如何穿过CPU保护模式的“铜墙铁壁”才是关键。下面我们就一起来探索这个问题。
软中断指令请你回忆下，CPU长模式下如何处理中断的（不熟悉的可以回看第5课和第13课）？
设备向CPU发送一个中断信号，CPU接受到这个电子信号后，在允许响应中断的情况下，就会中断当前正在运行的程序，自动切换到相应的CPU R0特权级，并跳转到中断门描述符中相应的地址上运行中断处理代码。
当然，这里的中断处理代码就是操作系统内核的代码，这样CPU的控制权就转到操作系统内核的手中了。
其实，应用软件也可以给CPU发送中断。现代CPU设计时都会设计这样一条指令，一旦执行该指令，CPU就要中断当前正在运行的程序，自动跳转到相应的固定地址上运行代码。当然这里的代码也就是操作系统内核的代码，就这样CPU的控制权同样会回到操作系统内核的手中。
因为这条指令模拟了中断的电子信号，所以称为软中断指令。在x86 CPU上这条指令是int指令。例如int255。int指令后面需要跟一个常数，这个常数表示CPU从中断表描述符表中取得第几个中断描述符进入内核。
传递参数虽然int指令提供了应用程序进入操作系统内核函数的底层机制，但是我们还需要解决参数传递的问题。
因为你必须要告诉操作系统你要干什么，系统才能做出相应的反馈。比如你要分配内存，分配多大的内存，这些信息必须要以参数的形式传递给操作系统内核。
因为应用程序运行在用户空间时，用的是用户栈，当它切换到内核空间时，用的是内核栈。所以参数的传递，就需要硬性地规定一下，要么所有的参数都用寄存器传递，要么所有的参数都保存在用户栈中。
显然，第一种用寄存器传递所有参数的方法要简单得多，事实上有很多操作系统就是用寄存器传递参数的。
我们使用RBX、RCX、RDX、RDI、RSI这5个寄存器来传递参数，事实上一个系统服务接口函数不会超过5个参数，所以这是足够的。而RAX寄存器中保存着一个整数，称为系统服务号。在系统服务分发器中，会根据这个系统服务号调用相应的函数。
因为C编译器不能处理这种参数传递形式，另外C编译器也不支持int指令，所以要用汇编代码来处理这种问题。
下面我们来建立一个cosmos/include/libinc/lapinrentry.h文件，在这里写上后面的代码。
//传递一个参数所用的宏 #define API_ENTRY_PARE1(intnr,rets,pval1) \ __asm__ __volatile__(\ &amp;quot;movq %[inr],%%rax\n\t&amp;quot;\//系统服务号 &amp;quot;movq %[prv1],%%rbx\n\t&amp;quot;\//第一个参数 &amp;quot;int $255 \n\t&amp;quot;\//触发中断 &amp;quot;movq %%rax,%[retval] \n\t&amp;quot;\//处理返回结果 :[retval] &amp;quot;=r&amp;quot; (rets)\ :[inr] &amp;quot;r&amp;quot; (intnr),[prv1]&amp;quot;r&amp;quot; (pval1)\ :&amp;quot;rax&amp;quot;,&amp;quot;rbx&amp;quot;,&amp;quot;cc&amp;quot;,&amp;quot;memory&amp;quot;\ ) //传递四个参数所用的宏 #define API_ENTRY_PARE4(intnr,rets,pval1,pval2,pval3,pval4) \ __asm__ __volatile__(\ &amp;quot;movq %[inr],%%rax \n\t&amp;quot;\//系统服务号 &amp;quot;movq %[prv1],%%rbx \n\t&amp;quot;\//第一个参数 &amp;quot;movq %[prv2],%%rcx \n\t&amp;quot;\//第二个参数 &amp;quot;movq %[prv3],%%rdx \n\t&amp;quot;\//第三个参数 &amp;quot;movq %[prv4],%%rsi \n\t&amp;quot;\//第四个参数 &amp;quot;int $255 \n\t&amp;quot;\//触发中断 &amp;quot;movq %%rax,%[retval] \n\t&amp;quot;\//处理返回结果 :[retval] &amp;quot;=r&amp;quot; (rets)\ :[inr] &amp;quot;r&amp;quot; (intnr),[prv1]&amp;quot;g&amp;quot; (pval1),\ [prv2] &amp;quot;g&amp;quot; (pval2),[prv3]&amp;quot;g&amp;quot; (pval3),\ [prv4] &amp;quot;g&amp;quot; (pval4)\ :&amp;quot;rax&amp;quot;,&amp;quot;rbx&amp;quot;,&amp;quot;rcx&amp;quot;,&amp;quot;rdx&amp;quot;,&amp;quot;rsi&amp;quot;,&amp;quot;cc&amp;quot;,&amp;quot;memory&amp;quot;\ ) 上述代码中只展示了两个宏。其实是有四个，在代码文件中我已经帮你写好了，主要功能是用来解决传递参数和触发中断问题，并且还需要处理系统返回的结果。这些都是用C语言中嵌入汇编代码的方式来实现的。</description></item><item><title>42_瞧一瞧Linux：如何实现系统API？</title><link>https://artisanbox.github.io/9/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/42/</guid><description>你好，我是LMOS。
上节课，我们通过实现一个获取时间的系统服务，学习了Cosmos里如何建立一个系统服务接口。Cosmos为应用程序提供服务的过程大致是这样的：应用程序先设置服务参数，然后通过int指令进入内核，由Cosmos内核运行相应的服务函数，最后为应用程序提供所需服务。
不知道你是否好奇过业内成熟的Linux内核，又是怎样为应用程序提供服务的呢？
这节课我们就来看看Linux内核是如何实现这一过程的，我们首先了解一下Linux内核有多少API接口，然后了解一下Linux内核API接口的架构，最后，我们动手为Linux内核增加一个全新的API，并实现相应的功能。
下面让我们开始吧！这节课的配套代码你可以从这里下载。
Linux内核API接口的架构在上节课中，我们已经熟悉了我们自己的Cosmos内核服务接口的架构，由应用程序调用库函数，再由库函数调用API入口函数，进入内核函数执行系统服务。
其实对于Linux内核也是一样，应用程序会调用库函数，在库函数中调用API入口函数，触发中断进入Linux内核执行系统调用，完成相应的功能服务。
在Linux内核之上，使用最广泛的C库是glibc，其中包括C标准库的实现，也包括所有和系统API对应的库接口函数。几乎所有C程序都要调用glibc的库函数，所以glibc是Linux内核上C程序运行的基础。
下面我们以open库函数为例分析一下，看看open是如何进入Linux内核调用相关的系统调用的。glibc虽然开源了，但是并没有在Linux内核代码之中，你需要从这里下载并解压，open函数代码如下所示。
//glibc/intl/loadmsgcat.c #ifdef _LIBC # define open(name, flags) __open_nocancel (name, flags) # define close(fd) __close_nocancel_nostatus (fd) #endif //glibc/sysdeps/unix/sysv/linux/open_nocancel.c int __open_nocancel (const char *file, int oflag, ...) { int mode = 0; if (__OPEN_NEEDS_MODE (oflag)) { va_list arg; va_start (arg, oflag);//解决可变参数 mode = va_arg (arg, int); va_end (arg); } return INLINE_SYSCALL_CALL (openat, AT_FDCWD, file, oflag, mode); } //glibc/sysdeps/unix/sysdep.h //这是为了解决不同参数数量的问题 #define __INLINE_SYSCALL0(name) \ INLINE_SYSCALL (name, 0) #define __INLINE_SYSCALL1(name, a1) \ INLINE_SYSCALL (name, 1, a1) #define __INLINE_SYSCALL2(name, a1, a2) \ INLINE_SYSCALL (name, 2, a1, a2) #define __INLINE_SYSCALL3(name, a1, a2, a3) \ INLINE_SYSCALL (name, 3, a1, a2, a3) #define __INLINE_SYSCALL_NARGS_X(a,b,c,d,e,f,g,h,n,.</description></item><item><title>43_虚拟机内核：KVM是什么？</title><link>https://artisanbox.github.io/9/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/43/</guid><description>你好，我是LMOS。
上节课，我们理解了Linux里要如何实现系统API。可是随着云计算、大数据和分布式技术的演进，我们需要在一台服务器上虚拟化出更多虚拟机，还要让这些虚拟机能够弹性伸缩，实现跨主机的迁移。
而虚拟化技术正是这些能力的基石。这一节课，就让我们一起探索一下，亚马逊、阿里、腾讯等知名公司用到的云虚拟主机，看看其中的核心技术——KVM虚拟化技术。
理解虚拟化的定义什么是虚拟化？在我看来，虚拟化的本质是一种资源管理的技术，它可以通过各种技术手段把计算机的实体资源（如：CPU、RAM、存储、网络、I/O等等）进行转换和抽象，让这些资源可以重新分割、排列与组合，实现最大化使用物理资源的目的。
虚拟化的核心思想学习了前面的课程我们发现，操作系统的设计很高明，已经帮我们实现了单机的资源配置需求，具体就是在一台物理机上把CPU、内存资源抽象成进程，把磁盘等资源抽象出存储、文件、I/O等特性，方便之后的资源调度和管理工作。
但随着时间的推移，我们做个统计就会发现，其实现在的PC机平常可能只有50%的时间处于工作状态，剩下的一半时间都是在闲置资源，甚至要被迫切换回低功耗状态。这显然是对资源的严重浪费，那么我们如何解决资源复用的问题呢？
这个问题确实很复杂，但根据我们的工程经验，但凡遇到不太好解决的问题，我们就可以考虑抽象出一个新的层次来解决。于是我们在已有的OS经验之上，进行了后面这样的设计。
结合图解，可以看出最大的区别就是后者额外引入了一个叫Hypervisor/Virtual Machine Monitor（VMM）的层。在这个层里面我们就可以做一些“无中生有”的事情，向下统一管理和调度真实的物理资源，向上“骗”虚拟机，让每个虚拟机都以为自己都独享了独立的资源。
而在这个过程中，我们既然作为一个“两头骗的中间商”，显然要做一些瞒天过海的事情（访问资源的截获与重定向）。那么让我们先暂停两分钟，思考一下具体如何设计，才能实现这个“两头骗”的目标呢？
用赵高矫诏谈理解虚拟化说起欺上瞒下，有个历史人物很有代表性，他就是赵高。始皇三十七年（前210年），统一了天下的秦始皇（OS）在生平最后一次出巡路上去世了，管理诏书的赵高（Hypervisor/VMM）却趁机发动了阴谋，威胁丞相李斯，矫诏处死扶苏与蒙恬。
赵高隐瞒秦始皇死讯，还伪造了诏书，回到了咸阳最终一顿忽悠立了胡亥为为帝。这段故事后世称为沙丘之变。
作为一个成功瞒天过海，实现了偷梁换柱的中间人赵高，他成事的关键要点包括这些，首先要像咸阳方向伪造一切正常的假象（让被虚拟化的机器看起来和平常一样），其次还要把真正核心的权限获取到手（Hypervisor/VMM要想办法调度真正的物理资源）。
所以以史为鉴。在具体实现的层面，我们会发现，这个瞒天过海的目标其实有几种实现方式。
一种思路是赵高一个人全权代理，全部模拟和代理出所有的资源（软件虚拟化技术），另一种思路是朝中有人（胡亥）配合赵高控制、调度各种资源的使用，真正执行的时候，再转发给胡亥去处理（硬件虚拟化技术）。
我们发现如果如果是前者，显然赵高会消耗大量资源，并且还可能会遇到一些安全问题，所以他选择了后者。
历史总是惊人地相似，在软件虚拟化遇到了无法根治的性能瓶颈和安全等问题的时候，软件工程师就开始给硬件工程师提需求了，需求背后的核心想法是这样的：能不能让朝中有人，有问题交给他，软件中间层只管调度资源之类的轻量级工作呢？
KVM架构梳理答案显然是可以的，根据我们对计算机的了解就会发现，计算机最重要几种资源分别是：计算（CPU）、存储（RAM、ROM），以及为了连接各种设备抽象出的I/O资源。
所以Intel分别设计出了VT-x指令集、VT-d指令集、VT-c指令集等技术来实现硬件虚拟化，让CPU配合我们来实现这个目标，了解了核心思想之后，让我们来看一看KVM的架构图。（图片出自论文《Residency-Aware Virtual Machine Communication Optimization: Design Choices and Techniques》）
是不是看起来比较复杂？别担心，我用大白话帮你梳理一下。
首先，客户机（咸阳）看到的硬件资源基本都是由Hypervisor（赵高）模拟出来的。当客户机对模拟设备进行操作时，命令就会被截获并转发给实际设备/内核模块（胡亥）去处理。
通过这种架构设计Hypervisor层，最终实现了把一个客户机映射到宿主机OS系统的一个进程，而一个客户机的vCPU则映射到这个进程下的独立的线程中。同理，I/O也可以映射到同一个线程组内的独立线程中。
这样，我们就可以基于物理机OS的进程等资源调度能力，实现不同虚拟机的权限限定、优先级管理等功能了。
KVM核心原理通过前面的知识，我们发现，要实现成功的虚拟化，核心是要对资源进行“欺上瞒下”。而对应到我们计算机内的最重要的资源，可以简单抽象成为三大类，分别是：CPU、内存、I/O。接下来，我们就来看看如何让这三大类资源做好虚拟化。
CPU虚拟化原理众所周知，CPU是我们计算机最重要的模块，让我们先看看Intel CPU是如何跟Hypervisor/VMM“里应外合”的。
Intel定义了Virtual Machine Extension（VMX）这个处理器特性，也就是传说中的VT-x指令集，开启了这个特性之后，就会存在两种操作模式。它们分别是：根操作（VMX root operation）和非根操作（VMX non-root operation）。
我们之前说的Hypervisor/VMM，其实就运行在根操作模式下，这种模式下的系统对处理器和平台硬件具有完全的控制权限。
而客户软件（Guest software）包括虚拟机内的操作系统和应用程序，则运行在非根操作模式下。当客户软件执行一些特殊的敏感指令或者一些异常（如CPUID、INVD、INVEPT指令，中断、故障、或者一些寄存器操作等）时，则会触发VM-Exit指令切换回根操作模式，从而让Hypervisor/VMM完全接管控制权限。
下面这张图画出了模式切换的过程，想在这两种模式之间切换，就要通过VM-Entry和VM-Exit实现进入和退出。而在这个切换过程中，你要留意一个非常关键的数据结构，它就是VMCS（Virtual Machine Control Structure）数据结构控制（下文也会讲到）。
内存虚拟化原理内存虚拟化的核心目的是“骗”客户机，给每个虚拟客户机都提供一个从0开始的连续的物理内存空间的假象，同时又要保障各个虚拟机之间内存的隔离和调度能力。
可能有同学已经联想到，我们之前实现实现虚拟内存的时候，不也是在“骗”应用程序每个程序都有连续的物理内存，为此还设计了一大堆“转换表”的数据结构和转换、调度机制么？
没错，其实内存虚拟化也借鉴了相同的思想，只不过问题更复杂些，因为我们发现我们的内存从原先的虚拟地址、物理地址突然变成了后面这四种内存地址。
1.客户机虚拟地址GVA（Guest Virtual Address）
2.客户机物理地址GPA（Guest Physical Address）
3.宿主机虚拟地址HVA（Host Virtual Address）
4.宿主机物理地址HPA（Host Physical Address）
一看到有这么多种地址，又需要进行地址转换，想必转换时的映射关系表是少不掉的。
确实，早期我们主要是基于影子页表（Shadow Page Table）来进行转换的，缺点就是性能有不小的损耗。所以，后来Intel在硬件上就设计了EPT（Extended Page Tables）机制，用来提升内存地址转换效率。</description></item><item><title>44_容器：如何理解容器的实现机制？</title><link>https://artisanbox.github.io/9/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/44/</guid><description>你好，我是LMOS。
上节课我带你通过KVM技术打开了计算机虚拟化技术的大门，KVM技术是基于内核的虚拟机，同样的KVM和传统的虚拟化技术一样，需要虚拟出一台完整的计算机，对于某些场景来说成本会比较高，其实还有比KVM更轻量化的虚拟化技术，也就是今天我们要讲的容器。
这节课我会先带你理解容器的概念，然后把它跟虚拟机作比较，之后为你讲解容器的基础架构跟基础技术，虽然这样安排有点走马观花，但这些内容都是我精选的核心知识，相信会为你以后继续探索容器打下一个良好的基础。
什么是容器容器的名词源于container，但不得不说我们再次被翻译坑了。相比“容器”，如果翻译成“集装箱”会更加贴切。为啥这么说呢？
我们先从“可复用”说起，现实里我们如果有一个集装箱的模具和原材料，很容易就能批量生产出多个规格相同的集装箱。从功能角度看，集装箱可以用来打包和隔离物品。不同类型的物品放在不同的集装箱里，这样东西就不会混在一起。
而且，集装箱里的物品在运输过程中不易损坏，具体说就是不管集装箱里装了什么东西，被送到哪里，只要集装箱没破坏，再次开箱时放在里面的东西就是完好无损的。
因此，我们可以这样来理解，容器是这样一种工作模式：轻量、拥有一个模具（镜像），既可以规模生产出多个相同集装箱（运行实例），又可以和外部环境（宿主机）隔离，最终实现对“内容”的打包隔离，方便其运输传送。
如果把容器看作集装箱，那内部运行的进程/应用就应该是集装箱里的物品了，类比来看，容器的目的就是提供一个独立的运行环境。
和虚拟机的对比我们传统的虚拟化技术可以通过硬件模拟来实现，也可以通过操作系统软件来实现，比如上节课提到的KVM。
为了让虚拟的应用程序达到和物理机相近的效果，我们使用了Hypervisor/VMM（虚拟机监控器），它允许多个操作系统共享一个或多个CPU，但是却带来了很大的开销，由于虚拟机中包括全套的OS，调度与资源占用都非常重。
容器（container）是一种更加轻量级的操作系统虚拟化技术，它将应用程序，依赖包，库文件等运行依赖环境打包到标准化的镜像中，通过容器引擎提供进程隔离、资源可限制的运行环境，实现应用与OS平台及底层硬件的解耦。
为了大大降低我们的计算成本，节省物理资源，提升计算机资源的利用率，让虚拟技术更加轻量化，容器技术应运而生。那么如何实现一个容器程序呢？我们需要先看看容器的基础架构。
看一看容器基础架构容器概念的起源是哪里？其实是从UNIX系统的chroot这个系统调用开始的。
在Linux系统上，LXC是第一个比较完整的容器，但是功能上还存在一定的不足，例如缺少可移植性，不够标准化。后面Docker的出现解决了容器标准化与可移植性问题，成为现在应用最广泛的容器技术。
Docker是最经典，使用范围最广，最具有代表性的容器技术。所以我们就以它为例，先对容器架构进行分析，Docker应用是一种C/S架构，包括3个核心部分。
容器客户端（Client）首先来看Docker的客户端，其主要任务是接收并解析用户的操作指令和执行参数，收集所需要的配置信息，根据相应的Docker命令通过HTTP或REST API等方式与Docker daemon（守护进程）进行交互，并将处理结果返回给用户，实现Docker服务使用与管理。
当然我们也可以使用其他工具通过Docker提供的API与daemon通信。
容器镜像仓库（Registry）Registry就是存储容器镜像的仓库，在容器的运行过程中，Client在接受到用户的指令后转发给Host下的Daemon，它会通过网络与Registry进行通信，例如查询镜像（search），下载镜像（pull），推送镜像（push）等操作。
镜像仓库可以部署在公网环境，如Docker Hub，我们也可以私有化部署到内网，通过局域网对镜像进行管理。
容器管理引擎进程（Host）容器引擎进程是Docker架构的核心，包括运行Docker Daemon（守护进程）、Image（镜像）、驱动（Driver）、Libcontainer（容器管理）等。
接下来，我们详细说说守护进程、镜像、驱动和容器管理这几个模块的运作机制/实现原理。
Docker Daemon详解
首先来看Docker Daemon进程，它是一个常驻后台的系统进程，也是Docker架构中非常重要的一环。Docker Daemon负责监听客户端请求，然后执行后续的对应逻辑，还能管理Docker对象（容器、镜像、网络、磁盘等）。
我们可以把Daemon分为三大部分，分别是Server、Job、Engine。
Server负责接收客户端发来的请求（由Daemon在后台启动Server）。接受请求以后Server通过路由与分发调度找到相应的Handler执行请求，然后与容器镜像仓库交互（查询、拉取、推送）镜像并将结果返回给Docker Client。
而Engine是Daemon架构中的运行引擎，同时也是Docker运行的核心模块。Engine扮演了Docker container存储仓库的角色。Engine执行的每一项工作，都可以拆解成多个最小动作——Job，这是Engine最基本的工作执行单元。
其实，Job不光能用在Engine内部，Docker内部每一步操作，都可以抽象为一个Job。Job负责执行各项操作时，如储存拉取的镜像，配置容器网络环境等，会使用下层的Driver（驱动）来完成。
Docker Driver
Driver顾名思义就是Docker中的驱动。设计驱动这一层依旧是解耦，将容器管理的镜像、网络和隔离执行逻辑从Docker Daemon的逻辑中剥离。
在Docker Driver的实现中，可以分为以下三类驱动。
graphdriver负责容器镜像的管理，主要就是镜像的存储和获取，当镜像下载的时候，会将镜像持久化存储到本地的指定目录； networkdriver主要负责Docker容器网络环境的配置，如Docker运行时进行IP分配端口映射以及启动时创建网桥和虚拟网卡； execdriver是Docker的执行驱动，通过操作Lxc或者libcontainer实现资源隔离。它负责创建管理容器运行命名空间、管理分配资源和容器内部真实进程的运行； libcontainer
上面我们提到execdriver，通过调用libcontainer来完成对容器的操作，加载容器配置container，继而创建真正的Docker容器。libcontainer提供了访问内核中和容器相关的API，负责对容器进行具体操作。
容器可以创建出一个相对隔离的环境，就容器技术本身来说，容器的核心部分是利用了我们操作系统内核的虚拟化技术，那么libcontainer中到底用到了哪些操作系统内核中的基础能力呢？
容器基础技术我们经常听到，Docker是一个基于Linux操作系统下的Namespace和Cgroups和UnionFS的虚拟化工具，下面我带你看一下这几个容器用到的内核中的基础能力。
Linux NameSpace容器的一大特点就是创造出一个相对隔离的环境。在Linux内核中，实现各种资源隔离功能的技术就叫Linux Namespace，它可以隔离一系列的系统资源，比如PID（进程ID）、UID（用户ID）、Network等。
看到这里，你很容易就会想到开头讲的chroot系统调用。类似于chroot把当前目录变成被隔离出的根目录，使得当前目录无法访问到外部的内容，Namespace在基于chroot扩展升级的基础上，也可以分别将一些资源隔离起来，限制每个进程能够访问的资源。
Linux内核提供了7类Namespace，以下是不同Namespace的隔离资源和系统调用参数。
1.PID Namespace：保障进程隔离，每个容器都以PID=1的init进程来启动。PID Namespace使用了的参数CLONE_NEWPID。类似于单独的Linux系统一样，每个NameSpace都有自己的初始化进程，PID为1，作为所有进程的父进程，父进程拥有很多特权。其他进程的PID会依次递增，子NameSpace的进程映射到父NameSpace的进程上，父NameSpace可以拿到全部子NameSpace的状态，但是每个子NameSpace之间是互相隔离的。
2.User Namespace：用于隔离容器中UID、GID以及根目录等。User Namespace使用了CLONE_NEWUSER的参数，可配置映射宿主机和容器中的UID、GID。某一个UID的用户，虚拟化出来一个Namespace，在当前的Namespace下，用户是具有root权限的。但是，在宿主机上面，他还是那个用户，这样就解决了用户之间隔离的问题。
3.UTS Namespace：保障每个容器都有独立的主机名或域名。UTS Namespace使用了的参数CLONE_NEWUTS，用来隔离hostname 和 NIS Domain name 两个系统标识，在UTS Namespace里面，每个Namespace允许有自己的主机名，作用就是可以让不同namespace中的进程看到不同的主机名。</description></item><item><title>45_ARM新宠：苹果的M1芯片因何而快？</title><link>https://artisanbox.github.io/9/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/45/</guid><description>你好，我是 LMOS。
前面两节课，我们一起学习了虚拟机和容器的原理，这些知识属于向上延展。而这节课我们要向下深挖，看看操作系统下面的硬件层面，重点研究一下CPU的原理和它的加速套路。
有了这些知识的加持，我还会给你说说，为什么去年底发布的苹果M1芯片可以实现高性能、低功耗。你会发现，掌握了硬件的知识，很多“黑科技”就不再那么神秘了。
好，让我们正式开始今天的学习！
CPU的原理初探经过前面的学习，我们已经对操作系统原理建立了一定认知。从操作系统的位置来看，它除了能够向上封装，为软件调用提供API（也就是系统调用），向下又对硬件资源进行了调度和抽象。我们通常更为关注系统调用，但为了更好地设计实现一个OS，我们当然也要对硬件足够了解。
接下来，我们一起看一看硬件中最重要的一个硬件——CPU是怎么工作的。让我们拆开CPU这个黑盒子，看一看一个最小的CPU应该包含哪些部分。不同架构的CPU，具体设计还是有很大差异的。为了方便你理解，我这里保留了CPU里的共性部分，给你抽象出了CPU的最小组成架构。
对照上图描绘的基本模块，我们可以把CPU运行过程抽象成这样6步。
1.众所周知，CPU的指令是以二进制形式存储在存储器中的（这里把寄存器、RAM统一抽象成了存储器），所以当CPU执行指令的时候，第一步就要先从存储器中取出（fetch）指令。
2.CPU将取出的指令通过硬件的指令解码器进行解码。
3.CPU根据指令解码出的功能，决定是否还要从存储器中取出需要处理的数据。
4.控制单元（CU）根据解码出的指令决定要进行哪些相应的计算，这部分工作由算术逻辑单元（ALU）完成。
5.控制单元（CU）根据前边解码出的指令决定是否将计算结果存入存储器。
6.修改程序计数器（PC）的指针，为下一次取指令做准备，以上整体执行过程由控制单元（CU）在时钟信号的驱动之下，周而复始地有序运行。
看了CPU核心组件执行的这6个步骤，不知道你有没有联想到第一节课的图灵机的执行原理？没错，现代CPU架构与实现虽然千差万别，但核心思想都是一致的。
ALU的需求梳理与方案设计通过研究CPU核心组件的运行过程，我们发现，原来CPU也可以想象成我们熟悉的软件，一样能抽象成几大模块，然后再进行模块化开发。
因为从零开始实现一款CPU的工程量还是不小的，所以在这里我带你使用Verilog语言实现一个可以运行简单计算的ALU，从而对CPU具体模块的设计与实现加深一下认知。
首先，我们来思考一下，对于一个最简单的ALU这个模块，我们的核心需求是什么？
没错，聪明的你可能已经脱口而出了，我需要能对两个N位的二进制数进行加减、比较运算。等等，为啥这里没有乘除？还记得学生时代初学乘除法的时候，老师也同样先简化为加减法，方便我们理解。
这里也一样，因为乘除也可以转换为循环的加减运算，比如2*3可以转换成2+2+2，6/2可以转换成6-2-2-2。所以，只需要实现了加减运算之后，我们就可以通过软件操作CPU，让它实现更复杂的运算了，这也正是软件扩展硬件能力的魅力。
好了，搞清楚需求之后，先不用着急编码，我们先来根据需求梳理一下ALU模块功能简图。
首先，我们在模块左侧（也就是输入侧）抽象出了5根引脚，这五根引脚的作用分别是：
ena：表示使能信号，它的取值是0或1可以分别控制ALU关闭或开启。 clk：表示时钟信号，时钟信号也是01交替运行的方波，时钟信号会像人的心跳一样驱动ALU的电路稳定可靠地运行。 opcode：表示操作码，取值范围是00、01、10这三种值，用来区分这一次计算到底是加法、减法还是比较运算。 data1、data2：表示参与运算的两个N位数据总线。 现在我们再来看图片右侧，也就是输出侧的y，它表示输出结果，如果是加减运算，则直接输出运算后的数值，而比较运算，则要输出0、1、2，分别表示等于、大于、小于。
好了，有了方案，接下来就让我们想办法把方案变成可落地的实践吧。
自己动手用Verilog实现一个ALUVerilog是一种优秀的硬件描述语言，它可以用类似C语言的高级语言设计芯片，从而免去了徒手画门电路的烦恼。
目前Intel等很多著名芯片公司都在使用Verilog进行芯片设计。我们为了和业界保持一致，也采用了这种Verilog来设计我们的ALU。
在开发之前，你需要先进行一些准备工作，安装VSCode的Verilog语言支持插件、iverilog、gtkwave，这些工具安装比较简单，你可以自行Google搜索。
接下来，我们就来实现一下ALU的代码，也就是alu.v，代码如下。
/*---------------------------------------------------------------- Filename: alu.v Function: 设计一个N位的ALU(实现两个N位有符号整数加 减 比较运算) -----------------------------------------------------------------*/ module alu(ena, clk, opcode, data1, data2, y); //定义alu位宽 parameter N = 32; //输入范围[-128, 127] //定义输入输出端口 input ena, clk; input [1 : 0] opcode; input signed [N - 1 : 0] data1, data2; //输入有符号整数范围为[-128, 127] output signed [N : 0] y; //输出范围有符号整数范围为[-255, 255] //内部寄存器定义 reg signed [N : 0] y; //状态编码 parameter ADD = 2'b00, SUB = 2'b01, COMPARE = 2'b10; //逻辑实现 always@(posedge clk) begin if(ena) begin casex(opcode) ADD: y &amp;amp;lt;= data1 + data2; //实现有符号整数加运算 SUB: y &amp;amp;lt;= data1 - data2; //实现有符号数减运算 COMPARE: y &amp;amp;lt;= (data1 &amp;amp;gt; data2) ?</description></item><item><title>46_AArch64体系：ARM最新编程架构模型剖析</title><link>https://artisanbox.github.io/9/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/46/</guid><description>你好，我是LMOS。
在今天，Andriod+ARM已经成了移动领域的霸主，这与当年的Windows+Intel何其相似。之前我们已经在Intel的x86 CPU上实现了Cosmos，今天我会给你讲讲ARM的AArch64体系结构，带你扩展一下视野。
首先，我们来看看什么是AArch64体系，然后分析一下AArch64体系有什么特点，最后了解一下AArch64体系下运行程序的基础，包括AArch64体系下的寄存器、运行模式、异常与中断处理，以及AArch64体系的地址空间与内存模型。
话不多说，下面我们进入正题。
什么是AArch64体系ARM架构在不断发展，现在它在各个领域都得到了非常广泛地应用。
自从Acorn公司于1983年开始发布第一个版本，到目前为止，有九个主要版本，版本号由1到9表示。2011年，Acorn公司发布了ARMv8版本。
ARMv8是首款支持64位指令集的ARM处理器架构，它兼容了ARMv7与之前处理器的技术基础，同样它也兼容现有的A32（ARM 32bit）指令集，还扩充了基于64bit的AArch64架构。
下面我们一起来看看ARMv8一共定义了哪几种架构，一共有三种。
1.ARMv8-A（Application）架构，支持基于内存管理的虚拟内存系统体系结构（VMSA），支持A64、A32和T32指令集，主打高性能，在我们的移动智能设备中广泛应用。
2.ARMv8-R（Real-time）架构，支持基于内存保护的受保护内存系统架构（PMSA），支持A32和T32指令集，一般用于实时计算系统。
3.ARMv8-M（Microcontroller架构），是一个压缩成本的嵌入式架构，而且需要极低延迟中断处理。它支持T32指令集的变体，主打低功耗，一般用于物联网设备。
今天我们要讨论的AArch64，它只是ARMv8-A架构下的一种执行状态，“64”表示内存或者数据都保存在64位的寄存器中，并且它的基本指令集可以用64位寄存器进行数据运算处理。
AArch64体系的寄存器一款处理器要运行程序和处理数据，必须要有一定数量的寄存器。特别是基于RISC（精简指令集）架构的ARM处理器，寄存器数量非常之多，因为大量的指令操作的就是寄存器。
ARMv8-AArch64体系下的寄存器简单可以分为以下几类。
1.通用寄存器
2.特殊寄存器
3.系统寄存器
下面我们分别来看看这三类寄存器。
通用寄存器R0-R30首先来看通用寄存器（general-purpose registers），通用寄存器一共为31个，从R0到R30，这个31个寄存器可以作为全64位使用，也可以只使用其中的低32位。
全64位的寄存器以x0到x30名称进行引用，用于32位或者64位的整数运算或者64位的寻址；低32位寄存器以W0到W30名称进行引用，只能用于32位的整数运算或者32位的寻址。为了帮你理解，我还在后面画了示意图。
通用寄存器中还有32个向量寄存器（SIMD），编号从V0到V31。因为向量计算依然是数据运算类的，所以要把它们归纳到通用寄存器中。每个向量寄存器都是128位的，但是它们可以单独使用其中的8位、16位、32位、64位，它们的访问方式和索引名称如下所示。
Q0到Q31为一个128-bit的向量寄存器 ； D0到D31为一个64-bit的向量寄存器； S0到S31为一个32-bit的向量寄存器； H0到H31为一个16-bit的向量寄存器； B0到B31为一个8-bit的向量寄存器； 特殊寄存器特殊寄存器（spseical registers）比通用寄存器稍微复杂一些，它还可以细分，包括程序计数寄存器（PC），栈指针寄存器（SP），异常链接寄存器（ELR_ELx），程序状态寄存器（PSTATE、SPSR_ELx）等。
PC寄存器
PC寄存器，保存当前指令地址的64位程序计数器，指向即将要执行的下一条指令，CPU正是在这个寄存器的指引下，一条一条地运行代码指令。在ARMv7上，PC寄存器就是通用寄存器R15，而在ARMv8上，PC寄存器不再是通用寄存器，不能直接被修改，只可以通过隐式的指令来改变，例如PC-relative load。
SP寄存器
SP是64位的栈指针寄存器，可以通过WSP寄存器访问低32位，在指令中使用SP作为操作数，表示使用当前栈指针。C语言调用函数和分配局部变量都需要用栈，栈是一种后进先出的内存空间，而SP寄存器中保存的就是栈顶的内存地址。
ELR_ELx异常链接寄存器
每个异常状态下都有一个ELR_EL寄存器，ELR_ELx 寄存器是异常综合寄存器或者异常状态寄存器 ，负责保存异常进入Elx的地址和发生异常的原因等信息。
该寄存器只有ELR_EL1、ELR_EL2、ELR_EL3这几种，没用ELR_EL0寄存器，因为异常不会routing(target)到EL0。例如：16bit指令的异常、32bit指令的异常、simd浮点运算的异常、MSR/MRS的异常。
PSTATE
PSTATE不是单独的一个寄存器，而是保存当前PE（Processing Element）状态的一组寄存器统称，其中可访问寄存器有：NZCV、DAIF、CurrentEL（）、SPSel。这些属于ARMv8新增内容，在64bit下可以代替CPSR（32位系统下的PE信息）。
type ProcState is ( // PSTATE.{N, Z, C, V}： 条件标志位，这些位的含义跟之前AArch32位一样，分别表示补码标志，运算结果为0标志，进位标志，带符号位溢出标志 bits (1) N, // Negative condition flag bits (1) Z, // Zero condition flag bits (1) C, // Carry condition flag bits (1) V, // oVerflow condition flag // D表示debug异常产生，比如软件断点指令/断点/观察点/向量捕获/软件单步 等； // A, I, F表示异步异常标志，异步异常会有两种类型：一种是物理中断产生的，包括SError（系统错误类型，包括外部数据终止），IRQ或者FIQ； // 另一种是虚拟中断产生的，这种中断发生在运行在EL2管理者enable的情况下：vSError，vIRQ，vFIQ； bits (1) D, // Debug mask bit [AArch64 only] bits (1) A, // Asynchronous abort mask bit bits (1) I, // IRQ mask bit bits (1) F, // FIQ mask bit // 异常发生的时候，通过设置MDSCR_EL1.</description></item><item><title>参考答案_对答案，是再次学习的一个机会</title><link>https://artisanbox.github.io/9/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/48/</guid><description>你好，我是编辑宇新。
春节将至，先给你拜个早年：愿你2022年工期变长，需求变少，技术水平更加硬核。
距离我们专栏更新结束已经过去了不少时间，给坚持学习的你点个赞。学习操作系统是一个长期投资，需要持之以恒，才能见效。无论你是二刷、三刷的朋友，还是刚买课的新同学，都建议你充分利用留言区，给自己的学习加个增益buff。这种学习讨论的氛围，也会激励你持续学习。
今天这期加餐，我们整理了课程里的思考题答案，一次性发布出来，供你对照参考，查漏补缺。
建议你一定要先自己学习理解，动脑思考、动手训练，有余力还可以看看其他小伙伴的解题思路，之后再来对答案。
第1节课Q：为了实现C语言中函数的调用和返回功能，CPU实现了函数调用和返回指令，即上图汇编代码中的“call”，“ret”指令，请你思考一下：call和ret指令在逻辑上执行的操作是怎样的呢？
A：一般函数调用的情况下call和ret指令在逻辑上执行的操作如下：
1.将call指令的下一条指令的地址压入栈中；
2.将call指令数据中的地址送入IP寄存器中（指令指针寄存器），该地址就是被调用函数的地址；
3.由于IP寄存器地址设置成为被调用函数的地址，CPU自然跳转到被调用函数处开始执行指令；
4.在被调用函数的最后都有一条ret指令，当CPU执行到ret指令时，就从栈中弹出一个数据到IP寄存器，而这个数据通常是先前执行call指令的下一条指令的地址，即实现了函数返回功能。
第2节课Q：以上printf函数定义，其中有个形式参数很奇怪，请你思考下：为什么是“…”形式参数，这个形式参数有什么作用？
A：在C语言中经常使用printf(“%s :%d”,“number is :”,20);printf(“%x :%d”,0x10,20);printf(“%x,%x :%d”,0xba,0xff,20);可以看出，这些printf函数参数个数都不同，因为C语言的特性支持变参函数。而“…”表示支持0个和多个参数，C语言是通过调用者传递参数的，刚好支持这种变参函数。
第3节课Q：其实我们的内核架构不是我们首创的，它是属于微内核、宏内核之外的第三种架构，请问这是什么架构？
A：我们的内核架构是混合内核架构，是介于微、宏架构之间的一种架构，这种架构保证了宏架构的高性能又兼顾了微架构的可移植、可扩展性。
第4节课Q：Windows NT内核属于哪种架构类型？
A：Windows NT内核架构其实既不属于传统的宏内核架构，也不是新的微内核架构，说NT是微内核架构是错误的，NT这种内核架构其实是宏内核的变种——混合内核。
第5节课Q：请问实模式下能寻址多大的内存空间？
A：由于实模式下访问内存的地址是这样产生的：16位段寄存器左移4位，加一个16位通用寄存器，最后形成了20位地址，所以只能访问1MB大的内存空间。
第6节课Q：分页模式下，操作系统是如何对应用程序的地址空间进行隔离的？
A：操作系统会给每个应用程序都配置独立的一套页表数据。应用程序运行时，就让CR3寄存器指向该应用程序的页表数据。运行下一个应用程序时，则会执行同样的操作。
第7节课Q：请你思考一下，如何写出让CPU跑得更快的代码？由于Cache比内存快几个数量级，所以这个问题也可以转换成：如何写出提高Cache命中率的代码？
A：第一，定义变量时，尽量让其地址与Cache行大小对齐。
int a __attribute__((aligned (64)));&amp;nbsp; int b __attribute__((aligned (64)));&amp;nbsp; 第二，操作数据时的顺序，尽量和数据在内存中布局顺序保持一致。
int arr[M][M]; for(int i = 0; i &amp;lt; M; i++) { for(int k = 0; k &amp;lt; M; k++) { arr[i][k] = 0; } } //而非这样 for(int i = 0; i &amp;lt; M; i++) { for(int k = 0; k &amp;lt; M; k++) { arr[k][i] = 0; } } 第三，尽量少用全局变量。</description></item><item><title>开篇词_为什么要学写一个操作系统？</title><link>https://artisanbox.github.io/9/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/51/</guid><description>你好，我是彭东，网名LMOS，欢迎加入我的专栏，跟我一起开启操作系统的修炼之路。
先来介绍一下我自己。我是Intel 傲腾项目开发者之一，也是《深度探索嵌入式操作系统》这本书的作者。
我曾经为Intel做过内核层面的开发工作，也对Linux、BSD、SunOS等开源操作系统，还有Windows的NT内核很熟悉。这十几年来，我一直专注于操作系统内核研发。
LMOS（基于x86平台支持多进程、多CPU、虚拟化等技术的全64位操作系统内核）跟LMOSEM（基于ARM处理器平台的嵌入式操作系统内核）是我独立开发的两套全新的操作系统内核，其中LMOS的代码规模达到了数十万行，两个系统现在仍在更新。
当时是基于兴趣和学习的目的开始了这两套操作系统，在这个过程中，我遇到了各种各样的技术问题，解决了诸多疑难杂症，总结了大量的开发操作系统的方法和经验。非常希望能在这个专栏与你一起交流。
每个工程师都有必要学好操作系统吗？经常会有同学问我这样一些问题：我是一个做应用层开发的工程师，有必要学习操作系统吗？我的日常工作中，好像用不到什么深奥的操作系统内核知识，而且大学时已经学过了操作系统课程，还有必要再学吗？
对于这些问题，我的答案当然是“有必要”。至于理由么，请听我慢慢为你道来。
你是否也跟我一样，曾经在一个数千万行代码的大项目中茫然失措？一次次徘徊在内存为什么会泄漏、服务进程为什么会dang掉、文件为什么打不开等一系列“基础”问题的漩涡中？
你是否惊叹于Nginx的高并发性？是不是感觉Golang的垃圾回收器真的很垃圾？除了这样的感叹，你也许还好奇过这样一些问题：MySQL的I/O性能还能不能再提升？网络服务为什么会掉线？Redis中经典的Reactor设计模式靠什么技术支撑？Node.js 的 I/O 模型长什么模样……
如果你也追问过上面的这些问题，那这会儿我也差不多可以给充满求知欲的你指一条“明路”了。这些都将在后面的学习中，找到答案。
为什么说操作系统很重要？首先我们都知道，操作系统是所有软件的基础，所有上层软件都要依赖于操作系统提供的各种机制，才能运行。
而我在工作中也认识了很多技术大牛，根据我的观察，他们的基本功往往十分扎实，这对他们的架构视野、技术成长都十分有帮助。
如果你是后端工程师，在做高性能服务端编程的时候，内存、进程、线程、I/O相关的知识就会经常用到。还有，在做一些前端层面的性能调优时，操作系统相关的一些知识更是必不可少。
除了Web开发，做高性能计算超级计算机的时候，操作系统内核相关的开发能力也至关重要。其实，即使单纯的操作系统内核相关的开发能力，对于工程师来说也是绕不过的基本功。
对于运维、测试同学，你要维护和测试的任何产品，其实是基于操作系统的。比如给服务配置多大的内存、多大的缓存空间？怎样根据操作系统给出的信息，判断服务器的问题出现在哪里。随着你对操作系统的深入理解和掌握，你才能透过现象看本质，排查监控思路也会更开阔。
除了工作，操作系统离我们的生活也并不遥远，甚至可以说是息息相关。要知道，操作系统其实不仅仅局限于手机和电脑，你的智能手表、机顶盒、路由器，甚至各种家电中都运行着各种各样的操作系统。
可以说，操作系统作为计算机的灵魂，眼前的工作、日常的生活，甚至这个行业未来的“诗与远方”都离不开它。
操作系统很难，我能学得会么？但即使是大学时期就学过操作系统的同学，也可能会感觉学得云里雾里。更别说非科班的一些人，难度更甚，甚至高不可攀。那为什么我这么有信心，给你讲好操作系统这门课呢？这还要从我自己的学习经历说起。
跟许多人一样，我看的第一本C教程就是那本“老谭C”。看了之后，除了能写出那个家喻户晓的“hello world”程序，其它什么也干不了。接着我又开始折腾C++、Java，结果如出一辙，还是只能写个“hello world”程序。
还好我有互联网，它让我发现了数据结构与算法，经过一番学习，后来我总算可以写一些小功能的软件了，但或许那根本就称不上功能。既然如此，我就继续折腾，继续学习微机原理、汇编语言这些内容。
最后我终于发现，操作系统才是我最想写的软件。我像着了魔一样，一切操作系统、硬件层相关的书籍都找来看。
有了这么多的“输入”，我就想啊，既然是写操作系统，为什么不能把这些想法用代码实现出来，放在真正的计算机上验证一下呢？
LMOS的雏形至此诞生。从第一行引导代码开始，一次又一次代码重构，一次又一次地面对莫名的死机而绝望，倒逼我不断改进，最终才有了现在的LMOS。因为一个人从零开始，独立开发操作系统这种行为有点疯狂，我索性就用LMOS（liberty，madness，operating，system）来命名了我的操作系统。
经过我这几年的独立开发，现在LMOS已经发布了8个测试版本。先后从32位单CPU架构发展到64位多CPU架构，现在的LMOS已经是多进程、多线程、多CPU、支持虚拟内存的x86_64体系下的全64位操作系统内核，代码量已经有10万多行了。
后来，我又没忍住自己的好奇心，写了个嵌入式操作系统——LMOSEM。由于有了先前的功底，加上ARM体系很简单，所以我再学习和实现嵌入式操作系统时，就感觉驾轻就熟了。
经过跋山涉水，我再回头来看，很容易就发现了为什么操作系统很难学。
操作系统需要你有大量的知识储备，但是现在大多的课程、学习资料，往往都是根据目前已有的一些操作系统，做局部解读。所以，我们学的时候，前后的知识是无法串联在一起的。结果就会越看越迷惑，不去查吧，看不懂，再去搜索又加重了学习负担，最后只能遗憾放弃。
那怎样学习操作系统才是最高效的呢？理论基础是要补充的，但相对来说，实践更为重要。我认为，千里之行还得始于足下。
所以，通过这个专栏，我会带你从无到有实现一个自己的操作系统。
我会使用大量的插图代码和风趣幽默的段子，来帮助你更好地理解操作系统内核的本质。同时在介绍每个内核组件实现时，都会先给你说明白为什么，带着你基于设计理解去动手实现；然后，再给你详细描述Linux内核对应的实现，做前后对比。这样既能让你边学边练，又能帮你从“上帝视角”审视Linux内核。
我们课程怎么安排的？操作系统作为计算机王国的权力中枢，我们的课程就是讲解如何实现它。
为此，我们将从了解计算机王国的资源开始，如CPU、MMU、内存和Cache。其次要为这个权力中枢设计基本法，即各种同步机制，如信号量与自旋锁。接着进行夺权，从固件程序的手中抢过计算机并进行初始化，其中包含初始化CPU、内存、中断、显示等。
然后，开始建设中枢的各级部门，它们分别是内存管理部门、进程管理部门、I/O管理部门、文件管理部门、通信管理部门。最后将这些部门组合在一起，就形成了计算机王国的权力中枢——操作系统。
我们的课程就是按照上述逻辑，依次为你讲解这些部门的实现过程和细节。每节课都配有可以工作的代码，让你能跟着课程一步步实现。你也可以直接使用我提供的代码一步步调试，直到最终实现一个基于x86平台的64位多进程的操作系统——Cosmos。
你能获得什么？走这样一条“明路”，一步一个脚印，最终你会到达这样一个目的地：拥有一个属于自己的操作系统内核，同时收获对Linux内核更深入的理解。
学完这门课，你会明显提升操作系统架构设计能力，并且可以学会系统级别的软件编程技巧。我相信，这对你拓展技术深度和广度是大有裨益的。之后你在日常开发中遇到问题的时候，就可以尝试用更多维度的能力去解决问题了。
同时，由于操作系统内核是有核心竞争力的高技术含量软件，这能给你职业生涯的成长带来长远的帮助。如今，在任何一家中大型互联网公司都使用大量的Linux服务器。
操作系统相关的内容，已经成为你涨薪、晋升的必考项，比如 Linux 内核相关的技术，中断、I/O、网络、多线程、并发、性能、内存管理、系统稳定性、文件系统、容器和虚拟化等等，这些核心知识都来源于操作系统。
而跳出个人，从大局观出发的话，计算机作为20世纪以来人类最伟大的发明之一，已经深入人们生活的方方面面，而计算机系统作为国家级战略基础软件，却受制于人，这关系到整个国家的信息安全，也关系到互联网信息行业以及其它相关基础行业的前途和未来。
而要改变这一困局，就要从培养技术人才开始。对于我们工程师来说，树高叶茂，系于根深，只有不断升级自己的认知，才能让你的技术之路行稳致远。
下面，我给出一个简化的操作系统知识体系图，也是后面课程涉及到的所有知识点。尽管图中只是最简短的一些词汇，但随着课程的展开，你会发现图中的每一小块，都犹如一片汪洋。
现在让我们一起带着好奇，带着梦想，向星辰大海进发！
课程交流群点这里加入。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>用户故事_yiyang：我的上机实验“爬坑指南”</title><link>https://artisanbox.github.io/9/52/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/52/</guid><description>你好，我是yiyang。
先简单说说我自己吧，我是一名编程爱好者，这个爱好从小学就已经播下了种子。我从求学到就业，有过很多次机会接触计算机方面的学习和相关工作，可是一直没有真正动手编程。这次能接触到LMOS老师的《操作系统实战45课》，让我眼前一亮，当时就报名了这门课。
都说编程需要能掌握一些基础的编程语言，但在这门编写操作系统的这门面前，我属于“三零基础”：Linux是零基础、汇编语言是零基础，C语言也是零基础。但这一点也没有影响我学习这门课的热情，因为我从报名那一刻，就站在了LMOS老师的肩膀上;-)
我的学习思路简单粗暴，就是先跟着老师的整个课程跑一遍，拿下整体框架。我自己也清楚，看不懂代码是我目前的一大劣势，而这不是三天两天能速成的，那我就不纠结在这方面，先跟着老师的每节课的讲解把大概意思给硬啃下来。
至于代码部分，老师在教学辅助石墨文档里已经给大家推荐了非常好的学习资料。因为明白自己当前的情况，也明确了自己的目标，所以并没有出现计划容易落地难的问题，我也一直是按计划学习的。在时间安排上，我是每天安排一课，跟着老师课程里的语音，同步看文字、插图及配套的代码。
这里有一个关键点，那就是每节课后都有编程大神们的精选留言，这些是已经学完课程的同学留下的宝贵学习经验。对于正在学习的我们，也可以用来辅助参考，这是我每节课必看的内容。
专栏里大部分内容都可以实际上机体验，LMOS老师都把配套的课程代码链接传到了Gitee上。我在学完每节课之后，一定会亲自上机运行一遍老师的代码。虽然我目前还写不出这些代码，但每次遇到有代码的课都自己跑一遍，也能更直观感受到当前这节课最终可以实现出什么样的结果。以我目前的实操能力，现在就能把这件事做到。
就从第一次上机运行的经验开始说起吧，也就是第二课“几行代码几行C，实现一个最简单的内核”。先说结果：我印象里，开始动手是夜里11点半，一直搞到了凌晨3、4点才最终完成。虽然只是运行了老师写的代码，但把自己的运行结果晒到打卡群时，内心还是感受到满满的成就感。
我在这将近4个小时的折腾里，因为这样那样的问题，不断掉坑、爬坑。那我究竟是如何解决，最终才完成了第2课的 Hello OS 呢？
兵马未动，配置先行之前在介绍里给自己的标签是“三无基础”，所以上机实践的每一步都是新的尝试和探索。运行课程代码前，自然要先搞定运行环境，主要分两个部分：安装Ubuntu和各种编译连接工具。
虽说是第一次安装Ubuntu，经过学习石墨文档里每一条和我的运行环境相关的文档链接以及在百度里大致搜索了一些安装教程，基本就能搞定这一关，安装过程中除了自己指定安装路径，其余的安装设置基本上都是选择默认的选项。
安装中途遇到第一个大坑，估计大部分初次安装Ubuntu的同学都可能遇到过：初次安装时，系统会在线安装系统的一些升级程序，具体是哪些先不深究，这个环节我至少等了45分钟还多。相比之前安装Windows操作系统区别很大，毕竟虚拟机里安装Win10，安装程序基本上是直接从iOS系统安装压缩包里把需要安装的程序文件全部解压出来，感觉整个过程十几分钟就完成了。
后来再去百度搜了一些资料才知道，安装中最好关闭本机网络。其中的关键点就是，我们电脑主机或虚拟机在安装Ubuntu系统时，有一部分程序需要从服务器下载，而默认的下载服务器传输速度缓慢是这个坑的直接产生原因。这个不光会影响Ubuntu安装，后续使用时安装一些程序还会给我们带来困扰：你会发现，下载程序速度非常慢，甚至经常中断无法完成下载。
找到了原因，对症下药就能根治问题。办法就是安装Ubuntu时，记得要关闭网络或虚拟机的网络功能，然后在安装完成后，找一个Ubuntu大陆地区镜像站点，配置到Ubuntu的设置里。
这里我推荐清华大学开源软件镜像站，超链接里有具体的使用帮助，选择跟你安装的ubuntu相同的版本，跟着帮助指导就能完成配置。重启后，再去下载和升级程序，你会发现下载速度就变得非常快了。
其实镜像站还有很多，你可以自己搜一搜，比如还有阿里云的镜像站，有兴趣的可以多找一些备用，使用和配置方法都是相似的，只要学会配置一个，其他的也都差不多。唯一不同的就是镜像站的链接网址这方面的区别。
解决了Ubuntu的下载问题，接下来就是安装课程里需要用到的代码编译等程序，把它们安装或升级到最新版本。
接下来我们就安装编译链接等工具：nasm、gcc、make。具体操作时，在Ubuntu系统里，进入终端 Terminal，在命令行中输入下面这条指令：
sudo apt install nasm gcc make 输入指令后，系统就会帮我们下载并安装这几个编译链接工具。完成安装后，第二课配套代码的程序编译环境我们就搭建好了。
上机运行的“爬坑指南”第二课的上机经验第二课的上机代码,老师已经帮我们写好了，我们只需要下载到Ubuntu里，然后进入终端Terminal ，在lesson02/HelloOS目录下，运行下面这条指令：
make -f Makefile 经过上述流程，我们就会得到 HelloOS.bin 文件。当然，这条指令的执行过程中，整个过程里生成了好几个文件，这几个文件生成的具体流程和介绍，专栏里都有详细说明，这里我们主要是讲如何运行代码，需要的就是最终生成的这个HelloOS.bin文件。
得到HelloOS.bin后，我们需要手动修改两个地方，手动选择启动项，还有把 Hello OS 添加进GRUB开机启动菜单。
手动修改第一关首先，我们要修改/etc/default/grub，把GRUB启动菜单配置改成启动时“显示”，可以让我们手动选择启动项。
这里我额外分享一个我的技巧，在对这类系统文件进行任何改动前，建议都先做一个备份，这样备份后，即使修改发生了错误后，还能用这个备份文件还原恢复。修改grub的具体操作是，在Ubuntu系统的终端 Terminal里，进入 /etc/default/ 目录，使用指令修改grub配置文件，代码如下：
sudo gedit grub 输入之上述指令后，编辑器里会显示grub配置文件，大约33行左右。
首先我们要用#号注销掉hidden行。我这个Ubuntu版本是在第7行，只需要在前面加上#号，也就是“#GRUB_TIMEOUT_STYLE=hidden”。
有的Ubuntu版本里是第7和第8行里都有hidden，那就把这两行前面都加上#号注释掉。为啥要注释掉呢？hidden的作用是启动时不显示GRUB启动菜单，而我们需要在启动时显示GRUB菜单选项，所以需要用#号注释掉。如果实验后你不需要显示GRUB启动菜单，逆向操作设置即可。
接下来，需要设置GRUB启动菜单的默认等待时间。代码如下：
GRUB_TIMEOUT=30 这里的参数30表示Ubuntu启动，进入GRUB启动菜单后，倒计时30秒，如果没有任何手动操作，就会直接进入第一个默认的启动选项系统。
接着，我们需要把GRUB_CMDLINE_LINUX_DEFAUL设置为text，也就是打开启动菜单时默认使用文本模式，代码如下：
GRUB_CMDLINE_LINUX_DEFAULT="text" 完成grub文件的这三处修改，记得按右上角的Save保存，然后关闭grub文件。
grub文件并不是修改后就完事了，还需要提示系统，我们已经更新了grub文件。操作也很简单，只需要在命令行输入如下指令：
sudo update-grub 这样，grub文件的配置修改我们就搞定了。
手动修改第二关接下来我们看看添加 Hello OS 的操作 。老师在课里“安装 Hello OS”这部分提到：</description></item><item><title>用户故事_成为面向“知识库”的工程师</title><link>https://artisanbox.github.io/9/54/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/54/</guid><description>你好，我是pedro，目前是一名后端小研发。
很早的时候，就收到了小编的邀请，让我来写一写用户故事。但是因为我手上有很多事情，这事儿就被耽搁了下来，所以导致这篇小故事迟到了很久。
虽然是在操作系统这个专栏下，但是我不想受到领域的限制，我想和你们分享一下我的学习思路、学习方法和收获，真诚地和你说说话，唠唠嗑，吹吹水。
学习思路你自己知道你需要什么，这才是最重要的！
我想能来这里学习的人，大多数都是希望提升自己的小伙伴，我也和你们一样，都遇到这样的问题，那就是——好书这么多，视频这么多，专栏这么多，博文又这么多，我缺的真的不是资源，而是时间！
几年前，我想要提升自己的心态十分迫切，在B站上收藏了N多视频，在浏览器主页上收藏了 N多博文，也买了很多好书和极客专栏。然而，这一堆接着一堆的东西，让我感到焦虑和茫然，实在是太多了，我哪里学得完呀。
而且我还时不时接到各式各样的推送，告诉我：你要学习数据库，这很重要；你要学习编译原理，这很重要；你要学习这个框架，面试必考；你要学习这个技术，工作必备；你要学习如何看画，审美很重要；你要学习如何读诗，远方很重要……
可我就是一个普通人，哪能学这么多？即使是时间管理大师罗志祥也办不到。我们不妨仔细想想，这些东西真的有这么重要吗？可能很重要，但是对我们来说，我觉得辨识力最重要，知道你自己需要什么，才最重要！
什么都舍弃不了的人，什么也改变不了！
这是《巨人》里面有名的金句。我之所以放到这里和你分享，是因为我觉得把这句话放在学习上同样很有效。
聊到这里，我想说说我自己的学习思路，其实也很简单。那就是，二八定律，80%的功利主义，学对工作最有帮助的，20%的情怀主义，学自己最感兴趣的。
结合你自身的工作情况和个人爱好，选择那么几门去开始学习，不要贪多，不要把买了就当成学了，用这样的方式来缓解自己的焦虑。
以我个人为例，我自校招入职以来，主要在学习与工作相关的知识，但也没有放弃个人兴趣。这里我把我正在学习探索的方向整理成了一张导图，也分享给你做参考。
在我看来，功利主义和情怀主义二者并不冲突，相反二者是相得益彰的，可以共同帮助你成长。因为工作以后，解决工作问题是最主要的事情，所以把大部分时间花在上面是值得的，这属于功利主义。
但是，工作内容并不一定只是为了解决工作问题，在工作中也可以找到有趣的事情。比如Go 语言底层的调度实现其实是非常有意思的，也可以本着情怀主义来学习，但同时在未来这部分知识又可以帮你解决更多的工作问题。
其实我也是出于情怀来学习操作系统的。操作系统可以说是打开技术底层大门的钥匙，一方面可以开拓视野，另一方面恰好也能在工作需要的时候帮助我们解决困难。
学习方法输出是学习的最佳途径！
光有学习思路是不够的，我曾遇到过这些问题：一个Bug遇到了两次，可是每次都得去 Google上搜，下次遇到了还是忘了。或者明明看了相关的视频，可是一到用的时候，突然发现自己好像只记得几个名词。
你看，明明花费了时间，却收获极小，这会严重打击我们的学习积极性。究其根本，是因为学习方法不对，导致学不到东西。
几年前，我刚步入这行的时候，由于原来没有接触过计算机，每次都是对着黑框框终端一顿操作，遇到问题到处百度（后来才转向Google），虽然稀里糊涂地解决了问题，可是下次遇到这个问题的时候，又得再百度，知识毫无积累，水平毫无提升，成了名副其实的面向“浏览器”工程师。
后面，我发现记笔记是一个有效的学习方法，可以直接提高对知识的熟练度。
因为在记笔记的过程里，我们会思考步骤、流程的合理性，重新审视这个知识点，同时记笔记也需要我们在内心里面揉碎这个知识点，加以消化，然后重新写出来。这是极佳的思考和输出的过程，有了这个过程，你不再是走马观花，而是经过了自己大脑的“解码”和“编码”，学习自然就会变得高效起来。
我记笔记最开始使用纸来写，但是效率太低，容易丢失；再后来，我学会了Markdown，开始在Markdown上记下自己踩坑的过程，写下自己的心得体会；可是很多时候我一会儿在笔记本上，一会儿又在台式机上，也有时候我需要和别人分享，甚至邀请别人一起来协作记笔记，于是我又将记笔记的地方转向了云端，开始使用石墨文档。
石墨文档支持多人协作，而且个人就算多PC、终端也可以登录，很好地解决了我的问题。下面附上我石墨文档的桌面截图，也推荐你使用。
慢慢地，我开始有了自己的积累，因为输出是更深层次的理解过程，很多坑点，我都能记下来，下次直接解决，即使遗忘了，我也能搜索自己的笔记。渐渐地我开始有了自己的知识库。从面向浏览器工程师变成了面向知识库工程师。这样的成长蜕变绝非朝夕之功，但我相信点滴的积累，终会聚沙成塔。
当然记笔记只是输出的一种，你也可以选择其它方式，比如技术分享，和同事、同学之间进行讨论，甚至给专栏留言。这里我就不得不骄傲一把了，操作系统专栏每一个小节，我都认真阅读了，思考和回答了问题，并且做了输出——留言，所以这个专栏让我收获巨大。你也可以借鉴！
收获技术能力应该是最基础的收获，收获更多的应该是生态！
开始时，我把学习和工作的目标定为提升技术能力，一路坚持下来，我的技术确实有了进步，但是我更大的收获是生态。
这个生态可能你不太理解，我来详细解释一下，我把因为学习和工作而结交到的朋友、业务理解、商业模式和思考方式等等统称为生态。
拿这个专栏来说，我重新对操作系统进行了梳理和复盘，把很多原来一知半解的知识彻底弄懂了，这只是第一层的收获。
更上层的是，我认识了大佬东哥（作者）和他的一些朋友，可爱又有责任心的小编 Sara，人美心善的小运营洁仔，还有一堆天天在群里吹水的小伙伴，他们在群里分享了很多实用的知识，我也订阅了好几个公众号。
我们因为这个专栏而认识，我们志同道合，我们一起努力来完善这个专栏，用反馈去给专栏增值，这个因大家一起努力贡献而组建起来的生态，才是我本次最大的收获。
我希望你在学习和工作的时候，不要仅仅着眼于技术本身，而是要试着切换视角，跳脱出固有的框架，并且尝试鸟瞰全局，这样你才能收获更多。同时也建议你把专栏当作学习交友的平台，希望你能在本次专栏的学习中能够与我们成为好朋友，鼓励更多的人加入进来。
除了课程正文的干货，我总是能在课程留言区发现惊喜。其实我们才是专栏真正的主人，也是专栏增值的核心力量，专栏是我们跟作者共同的作品。
还是拿我自己来说吧，加入专栏成为助教后，我的学习激情一下子就“膨胀“了。认真学习专栏不仅仅只是兴趣，还有责任感与使命感，仿佛不追完就觉得白来了一趟。也正因如此，我才能收获如此巨大，相信你也可以。
写在最后今天的分享，我从思路、方法和收获三个方面跟你聊了聊学习这件事情，下面我来谈一谈我对操作系统的看法。
操作系统是我个人认为最应该掌握的计算机必修课！因为我们的每个程序、每个应用以及每个服务都跑在操作系统这个地基上面，可以说现代互联网完全构建在了操作系统上。
操作系统是计算机软件的集大成者，是架构的极致！无论是Windows、Linux还是macOS都有几百万行代码，在保证高效运行的同时，又能将各种能力通过开放接口提供给我们，这是优良架构才能带来的能力。
东哥将操作系统的精华浓缩，并将其实现为Cosmos，用专栏的形式提供给我们，让我们有机会去一睹操作系统的风采，去汲取最有营养的养料，让你在学习操作系统的路上少走弯路，少走弯路就是走捷径。
希望每个看到这篇用户故事的小伙伴，重新拿起这个专栏。行百里者半九十，很多人行了十里就落下了，专栏行程虽然过半，但仍然可以赶上，大家，加油！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>用户故事_技术人如何做选择，路才越走越宽？</title><link>https://artisanbox.github.io/9/55/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/55/</guid><description>你好，我是宇新。
作为《操作系统实战45讲》的编辑。从专栏上线到现在已经有3个多月的时间了，感谢你一直坚持到现在。
留意过课程评论区的同学都知道，我们有几位常驻的同学一直在主动输出。那这些“课代表”是怎样学习专栏，又有什么学习诀窍？
为了满足咱们的好奇心，我特意策划了这次特别的采访，请到了在专栏里留下很多精彩足迹的neohope同学，我会代表好奇的小伙伴向他提问，希望这次的分享能够带给你一些启发。
首先让我介绍一下neohope，他是一个技术爱好者，年龄就不说了。neohope做过很多的岗位，像是软件工程师、项目经理、项目总监、产品经理、架构师、研发总监等，现在他在医疗健康行业工作。
让我们正式开始这次采访吧！
如何搭建自己的学习体系Q1：你好，neohope。你的课程笔记帮到了不少人，看得出你学得很认真，能给同学们说说，对于学好《操作系统实战45讲》这个专栏，你有哪些建议么？
A1：你好，关于怎么学好这门课程。我有这样几个建议作为参考。
第一个建议是多动动手：前期看到有些小伙伴不会用虚拟机，也不会命令行，但其实大部分同学花上几个小时也就搞定了。有了感性的认识，后面学习就不那么抽象了；
第二个建议就是够用就好：不要一看里面有汇编语言，就去从头学汇编；也不要一看C语言，就去学C。我的建议是，能读懂就够了，不会的命令网上找一下就可以了。其实，我们不妨想一下，自己小时候是如何读书的？有些看不懂的字，其实可以跳过去，这不影响理解的；
然后，我建议你多看源码：其实我的方法很笨，就是把老师的注释先拷贝到课程源码里，再结合自己的思考理解补充一些注释，这样读起来还是很简单的；
接着，就是要多理资料：有些地方看不太懂的，就去查资料，建议你把看完的资料，用自己的方式整理出来，然后分享，这样会有很好的效果；
最后，要找到组织：不要自己孤军奋战，找几个小伙伴定期聊聊，参与一些好的技术群，多交流会让你提升很快。自己迷惑的问题，可以问一下，看看别人如何理解的，不要害羞。
Q2：感谢neohope的建议。从你的留言里可以看出你对操作系统认识很深，即使是比较复杂的调用过程，你也总能很快地理清脉络，把握全局。这是怎么做到的呢？
A2：其实操作系统也好，其他技术也好，想要理解透彻都需要一个过程，而非一蹴而就。我觉得建立自己的知识体系，是一种很好的方法。
Q3：这个知识体系你是怎么建立的呢？可否分享一下，让入行不久或即将入行的小伙伴做个参考么？
A3：在我日常工作中，经常会遇到要教新人的情况，每一次带一位新人，我都会要求他/她做这样几件事情。
1.首先，我会请他用图解的方式，画一下自己会哪些技术；
2.然后，跟他深入聊几个常见问题，比如下面这些问题：
用谷歌浏览器打开一个登录页面，输入用户名、密码，当用鼠标点击登录按钮时，究竟发生了什么？ 如何自己做一个框架，去实现Spring Boot、Flask或WCF等相关功能；自己平时用框架有没有不爽的地方，想要如何改进它？ 找一个大家都熟的业务场景，聊一聊如何在技术或非技术层面进行改进…… 3.在技术上，我还会问问他，后续的学习发展计划是怎样的，自己想学什么，优先要学什么？
4.最后，我会帮他/她去逐步建立一个技术栈，并以此为出发点，做一个为期1到3年的技术规划。
Q4：前面几步听起来有点像面试的场景。你是怎样想到用这个方法呢？
A4：我接着刚才知识体系说，因为工作以后，相比在学校系统学习，我们现在接触的信息大多都是碎片化的，对自己掌握了什么技术，我们并没有清晰的了解。而且根据我多年观察，即使是一些平时工作很认真的人，都没有去好好整理过自己的知识体系，这很可惜。
我第一次跟新入职的同学沟通时，可能最开始往往得到的是一堆的技术名词。
这个时候，我会根据小伙伴自己的技术栈，帮他/她搭一个简单的体系框架，把上面的技术名词归类放好，这里我以后端工程师为例。
然后，对于重点关注的层，还可以进一步展开。咱们是自学操作系统，那这里就把OS层展开。
之后，可以把自己整理的图和可信度高的资料进行对比。咱们这里就把上图和Cosmos、Linux进行一下对比。根据对比，摘取自己需要的内容，对自己的图进行补充。
参考：https://makelinux.github.io/kernel/map/
这样，你自己的知识体系就有了雏形。接着，对于自己要重点学的内容，进一步展开，比如说，对于锁这个知识点，我是这样拆分的。
乐观锁、悲观锁 公平锁、非公平锁 重入锁、不可重入锁 自旋锁、非自旋锁 独享锁、共享锁、读写锁 分段锁、行锁、表锁 分布式锁、共识算法 …… 之后，对于这些知识点，我们可以用不同颜色进行标记（后面我列出了我自己习惯用的标记方式）。标记好了以后，你可以把“必须，未掌握，红色”的内容，整理一个清单，排个优先级，作为未来一段时间学习计划的参考。
A、必须，已掌握，绿色
B、必须，未掌握，红色
C、非必须，已掌握，绿色
D、非必须，未掌握，黄色
其实，这个知识体系就像是一张藏宝图，上面的一个个知识点就是一个个宝藏。实际使用的时候，我们不用花很大精力去做这个图，也不用限制是何种模式，一个markdown文件足够了，对自己有帮助就好。
随着你的积累和进步，每经过一个时期，都可以重新看下这个藏宝图，常看常新。
如果你特别喜欢自己的藏宝图，但图中有不少盲点，那就先找最基础的东西看，探索一段时间，迷雾自然就少了；如果你的藏宝图虽然很大，但能挖掘的精华有限，建议先找一张对你最有用的图，精力不要过于分散。如果这张图的要点你都掌握了，就需要扩展知识面，再去开个副本吧！
Q5：你的藏宝图方法听起来很酷，看得出你对不同的技术栈都比较熟悉，可以说说你的思考么？比如，不同技术栈怎样找共同点？
A5：随着不断的学习，我发现不同的技术栈，的确有很多相似的地方，就像是同一类型的宝藏。然后去看细节，又会发现不一样的地方，就像每个宝石，纹理都不一样。
以操作系统及虚拟机为例，你有没有想过Linux、Windows、Android、iOS、Docker、VritualBox、JVM、CLR、V8，都在管理哪些事情呢？
虽然这些技术并不在一个层面，其实很多要做的事情，却是很相似的。比如，都需要CPU管理、内存管理、任务管理、处理同步问题、文件管理、I/O管理、资源隔离、提供统一而稳定的API等。
然后，从任务管理这个角度再去看，还能看到优先级、时间片、抢占式、沙盒、命名空间、配额、欺上瞒下、甩手掌柜、单脑回路等等精彩的宝石纹理。
Q6：刚才说了不少寻找共性的思路，掌握了很多技术以后，你会怎么去分析它们呢？
A6：技术千千万，但追究其本质，技术都是为了解决具体问题的，这里我举三个例子吧。
以远程调用为例（远程调用推荐你看下公开课《周志明的软件架构课》），CORBA、DCOM、EJB、Webservice、REST、Thrift、ProtocolBuffer、Dubbo等，这些技术都在解决什么问题呢？这些技术的流行和没落的原因是什么呢？
我们想要解决类似RPC的问题，都是定义了一套规范要调用方和被调用方共同遵守，而且都提供了代码的辅助生成工具。那为何至今还会有很多新的技术出来，要解决这个问题呢？咱们就又要去观察“纹理”了。
以任务调度为例，从操作系统进程调度，到线程池、Socket连接池、DB连接池、对象池，再到F5、Nginx、Dubbo的流量控制，以及到大数据的Yarn、容器的编排，它们都在解决哪些问题？
再以低代码为例，ESB、OA（流程编辑器+表单设计器）、FaaS平台、SaaS平台，都在解决什么问题，给出的答案又有什么差异？这种思考方式还有很多例子，我就不一一列举了。
随着不断的学习，你会发现，不同的技术栈，有很多重叠的地方。比如，数据结构与算法、网络、数据库、文件处理、加密解密、系统调用等。一旦一次学会，就像打通任督二脉，在另外的地方，遇到类似问题的时候，就无师自通了。
Q7：那不同的技术栈，你会怎么样做对比呢？
A7：不同的技术栈，有很多不同的思路。就拿泛型为例，每种语言各有不同。
C语言，可以通过函数指针或宏来实现，需要一定的编程技巧； C++语言，一般通过STL来实现，在编译时实现，会造成代码膨胀； Java语言，通过类型擦除实现，编译时擦除，JVM运行时并不知道处理的是什么类型； C#语言，在编译生成IL中间码时，通用类型T只是一个占位符；在实例化时，根据实际类型进行替代，并通过JIT生成本地代码，不同类型的泛型类是不一样的； Go语言，当前版本，并不支持泛型，可以通过interface强制转换，需要一些编程技巧； JavaScript语言，动态类型，天生支持泛型。 Q8：感觉这样做了对比之后，确实更容易加深理解。这个方法只能用在分析泛型么，可以不可以再举个例子？</description></item><item><title>用户故事_操作系统发烧友：看不懂？因为你没动手</title><link>https://artisanbox.github.io/9/53/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/53/</guid><description>你好，我是spring Xu。我平时的工作就是做实时嵌入式系统，坐标上海。
写操作系统这件事一直是我的兴趣，我之前写过引导器，也有移植过uboot的基础，还读了不少操作系统的书。作为一名操作系统“发烧友”，我是怎样跟操作系统、跟LMOS这门课程结缘的呢？请你听我慢慢道来。
我是怎样与操作系统结缘的？其实我并非计算机专业出身，也没有系统地学过操作系统。不过出于兴趣，我早在大学时就自学了微机原理，当时记得还在x86实模式下写了些汇编程序，但还是有很多迷惑的地方。
于是，我跑到图书馆找了本Intel的芯片手册自己随意翻看，发现x86的保护模式寻址方式好奇怪，还有调用权限的知识也弄不太懂。后来我还试着询问老师，结果当时没得到什么满意的答案，这事儿也就不了了之了。
直到我工作了，接触的是嵌入式系统ucos-II。感觉这样的系统有点简单，因为它无法动态加载外部应用程序，还是想搞个更高级点的，自己写一个操作系统的想法从此埋下种子。
于是我购买了潘爱民老师的《程序员的自我修养》阅读。潘老师的那本书是讲C语言的编译链接和运行环境，也就是C语言文件如何编译、如何链接到生成程序的过程，还有该程序如何在操作系统上加载和运行以及程序加载的知识，都可以从这本书里学到。
我还根据于渊老师的《一个操作系统的实现》这本书，试着写了一个开机引导程序。这次再看到x86的保护模式，我觉得更容易看懂了，脑子里出现了一个念头，这是用硬件提供应用程序与操作系统内核之间的隔离。其实这并不是我的阅读理解水平有了多快的“飞升”，而是因为许多知识的领悟，都要经历大量的实践才会产生。
二级引导器的编写需要包含文件系统，还要有硬盘或者软盘的读取操作。工作一忙，时间久了自己也渐渐热情冷却，只是开了个头，就这样停工了。
但我的写操作系统梦想，并没有止步。后来偶然的机会，我看到周自恒老师翻译了一位日本人川合秀实写的《30天自制操作系统》。
哈哈，真正中我下怀！我心里盘算着，只要30天就可以写个操作系统出来了，那应该挺简单的。就按一个星期七天，我只读一天的内容，一年我也能完成我的操作系统了。就这样我又开始捣鼓自己的操作系统了。
这本书面对的读者算是小白，没有啥学术名词概念，用通俗的语言把许多知识都讲解了。而且按作者的工具和思路，我确实实现了一个带图形的系统，但我按照他的步骤，没觉得自己水平有啥提升，还是觉得有点不过瘾。
于是为了补充一下理论知识，我又买了好多操作系统的书，甚至入手了一块三星的s5pv210的ARM CPU的开发板，这次是想实现个3D界面效果的操作系统，像iOS的4.3风格的那种图形，选择这块开发板，是因为iPhone4的CPU还没有这个强大。这样，我的自制操作系统之路再次启动。
工作的忙碌让我只能停停走走，只把uboot移植了，可以点亮并可以在那块s700的屏幕上输出打印字符信息，并实现了一个没有2D加速的16位5:6:5格式，24位888格式的小型点阵图形库。但很遗憾，由于那个芯片的3d图形芯片是PowerVR，根本不可能有任何开放的资料指导我写出驱动它的程序，最多用CPU模拟3D功能，就这样实验再度搁置了。
现在回想起来，当时是对操作系统的图形界面感兴趣，而并不是操作系统本身的知识。关注点不一样，导致许多知识并没有进一步学习。
真正开始学习写操作系统今年年初，我无意中看到了B站哈工大李治军老师的操作系统课程，这个课程是用Linux0.11作为实验代码，更具有实战性，于是我开始边刷课程边看代码，但进展十分缓慢。
直到五月份，在微信群中看到了极客时间有操作系统实战课程。想到写操作系统的难度大，一般不太容易自己写出操作系统，而且不光写操作系统还能教别人写，对这位作者有些钦佩。我又查了下这门课程的作者——彭东，再看开篇词里他自己学操作系统、写操作系统的故事，深深被他的这份执着精神所鼓舞。
想想自己每次都想着要写一个自己的操作系统，但真正到实施时就退缩了，直到现在我也没有实现，真是太惭愧了。于是我购买了课程，还加入了东哥的操作系统实战课程群。因为之前看书跟实操积累的基础，这门课程跟起来就更加顺畅了。所以我经常在交流群里催更，还提问过CPU多核方面的问题，也会经常在课程中留言。
如果你也跟我一样，也想自己动手写操作系统，我从一个写操作系统的爱好者角度，建议大家先学习CPU的体系结构知识，这个是在硬件上为操作系统所做的准备，比如内存的访问、中断以及异常的调用。
这个基础很有必要，目的是让你在没有模拟器的环境，又不能真机调试时，也有能力定位问题，还能锻炼如何用大脑模拟运行汇编代码。这里我顺便提一个问题，你感受下，如果写1个循环100次的代码，用累加1的操作，与用递减1的操作，哪个快？为什么？欢迎留言写出你的答案。
然后是C语言和编译链接方面的知识，这个是用来生成操作系统程序的。其他知识就是在实践中，根据自己情况主动探索、获取。这个课程的代码也可以拿来使用，也可以在这个代码上做二次开发。期待我们一起为这个Cosmos操作系统添砖加瓦！
这门课带给我的收获跟着东哥学习操作系统，我感觉收获颇多，每一节课都有许多感悟，有一些我已经在课程留言区记录了，还有一些正在整理酝酿中。这里我就从整体课程出发，简单为你描述一下课程中操作系统的知识分布。
基础铺垫阶段
前两节课是热身阶段，这是为了带我们了解，从源代码文件到可运行的程序文件的编译链接过程，以及这个程序文件如何在PC上运行。这部分包含了C语言、编译、汇编还有链接的相关知识点，这也是现在许多程序员感到神秘和陌生的部分。
课程里的Hello OS实验，相当于一个PC硬件平台上的裸机程序（这里“裸机”二字我可能用得不太严谨）。现在的开发工具IDE，都把这部分工作自动化完成了。我觉得讲这些还是很有必要的，能让我们能清晰地了解程序生成过程，也能方便我们知道无调试环境下要如何看汇编代码。毕竟太依赖IDE的话，水平不会提高。
第三、第四节课对操作系统的宏内核与微内核架构做了介绍，也是Linus与Andrew Tanenbaum争论的话题。
第五到第九节课为后面Cosmos搭建铺垫了基础知识。硬件模块是对x86硬件编程的规范说明。CPU如何寻址访问内存的，也包括硬件上如何支持操作系统内核与应用程序的分级管理、x86的中断机制、cache的运行机制。
而硬件资源有限的情况下，不同程序访问同一资源，又涉及到后面各种锁的介绍。 通常操作系统课本上，这部分是放到进程中一并描述的知识点。而这里却单独拎出来，有助于我们关注本质。
初始化、内存管理与进程模块
之后第十节课开始，我们进入到启动初始化模块。一起探索真实开发环境的搭建与初始化。在二级引导器的帮助下，可以加载Cosmos的内核系统，最为直观的体会就是显示图片跟点阵字体。整个引导完成之后进入到操作系统核心，实现了各种资源的初始化过程（包括中断框架初始化）。
接着就到了最硬核的内存管理模块，这里的确有难度，一方面代码量不少，另一方面内存设计是老师自研，我们乍一看有点陌生。我现在也处于看懂了代码，但还需要进一步分析的阶段。所以也建议你对照课程讲解慢慢揣摩。
该模块老师用四节课讲物理内存管理，也就是操作系统下，内存管理中的物理内存分配管理。 这里的物理内存是指在硬件的地址空间中可以找到该数据的内存。而区别内存的前三节课，第十九节课是讲小于4k的内存分配是如何实现的。
之后四节课，两节课讲解了进程访问虚拟内存的相关知识。所谓的虚拟就是不真实的。为什么不真实呢？因为虚拟内存的大小，是由当前CPU最大可以访问的内存大小决定的，不是当前计算机安装的物理内存大小。
比如32位的CPU，虚拟内存大小是4G，但该计算机的内存配置1G，对于进程来说，还是会认为有4G的空间可以使用。
也正是这个原因，所以访问内存时，要把虚拟内存映射到真实的物理内存上，映射过程中CPU就会产生缺页中断异常，然后需要测试中断框架的代码，处理了该中断异常后，虚拟内存就可以访问到物理内存了。
之后两节课又讲了Linux的内存管理，关于Cosmos跟Linux的内存管理做对比，这里我也在摸索，等我理清楚了再分享出来。
因为之前看过不少图书，所以老师课程里进程模块的设计让我眼前一亮。因为进程的结构与调度，在各种操作系统的书里经常是讲得最复杂难懂的部分，一般课本上是把进程相关的都讲述一遍，每个知识点一次性都提及，我们不知道来龙去脉，就容易一头雾水。这不是劝退的思路么？
但这门课安排就巧妙得多，先讲内存管理再讲进程。程序的运行第一件事就是需要内存空间。有了这个基础，你再学习进程的时候也会觉得没那么难。
所以我从这两个模块学习中，得到的最大感悟就是，做什么事，把目标定好后，别考虑那么多，要设计多么的完美，而是把任务分解开，一点点来实现。
先实现一个雏形，知道会有问题，找出问题，解决问题。通俗点，哪怕起初挖了许多个坑也可以逐步完善，慢慢把坑填了，系统也就健壮了。
驱动模型、文件系统与网络
第二十八到三十课是Cosmos操作系统的驱动模型。由于操作系统是应用程序与硬件之间的桥梁。操作系统为简化应用程序开发难度，把硬件操作做了统一规划。
做驱动的开发只要根据操作系统提供的驱动模型，实现操作硬件的代码，这样就可以让应用程序调用操作系统提供的统一接口来操控不同的硬件了。课程里用的是定时中断的例子，如果想驱动键盘和鼠标的话，你可以重点看这个部分。
第三十二到三十四节课讲的是文件系统。这部分只是在内存上建立的文件系统。虽然简易，但也构建出了一个自己的文件系统。有能力的同学还可以写个磁盘驱动进行完善。
其实在等更新的时候，我也很好奇操作系统的网络模块要怎么讲。后来真的看了内容后发现，这部分其实是讲计算机网络与操作系统的关系。首先是传统单机中，计算机的网络是什么样的。 然后扩展到集群下的、超大宽带的计算机网络与操作系统的关系。
这里我整理了一张导图，整理了课程脉络。这里先说明一下，课程里还有不少关于Linux的内核的分析，这里为了凸显Cosmos主线我没有过多涉及Linux，如果你有兴趣可以试着自己动手总结归纳。
希望我对课程的内容梳理对你有帮助。操作系统里面包含的知识可以说是博大精深，真要完全掌握，需要大量的时间和精力的投入。
所以，我想结合我个人经验跟你聊一聊，我们怎么从自身工作技术栈中的底层技术点这个维度入手，深挖出与操作系统相关的知识点，通过对底层机制的思考学习，加深自己对技术的理解。
比如做Java开发，那自然深挖技术知识点，就会挖出JVM虚拟机的内存管理原理。 内存管理在操作系统中也有，但操作系统做成了谁申请、谁释放的原则，让应用程序自己来负责。
而JVM这个应用程序，它向操作系统申请了超大的堆内存作自己的虚拟机管理的内存，并根据对象的引用计数器来判断对象的生命周期是否结束，这样就可以自动回收垃圾对象所分配的内存了。对于操作系统来说，JVM仍然是占用着那块超大的堆内存的。
我们进一步思考下，如果把这部分机制放到内核中，是不是就可以做出带垃圾回收机制的C语言了呢？
这个思路其实是可以有的，但为了兼容考虑，解决思路是放到了编译阶段了。你可以了解一下Apple系统的object-c语言的ARC机制。这里你可以想想，为什么不能在Windows或者Linux、iOS上把这个功能实现了，只能从编译阶段做成这个样子？
再比如使用Golang做开发，你会发现协程其实是在Golang的运行环境里提供了协程和协程调度，协程调度器按照调度策略，把协程调度到线程中运行。协程的调度过程由用户态程序负责，也就是golang应用程序，由M个协程对应N个内核线程，这个调度算法比较复杂，但对于开发者来说是无感知的。这样带来的好处又是什么？当然协程不光Golang提供了。
再比如做前端Web开发。微信是一个IM的应用软件，但微信可以浏览网页、公众号，甚至加载小程序，小程序的开发语言是JavaScript，它的运行环境是JavaScript虚拟机。这个不是和多年前Palm公司推出的WebOS系统很像吗？
操作系统的桌面用浏览器来代替，不需要用C++或者Java语言来开发应用，直接用JavaScript语言就可以开发所谓的桌面应用程序了。如果用浏览器的插件技术，开发的语言是C++就会导致开发人员的学习成本升高，但性能是强劲的。
通过这三个例子，你有没有发现，跟原来的技术实现相比，开发应用的难度是在降低的。而现代新出的技术有不少是操作系统里做的一些功能，换到应用程序里提供的功能，又在编程语言上提供了语法糖（在编程语言中，增加新的关键字来简化原来的写法），再通过开发工具生成应用程序。
核心思想就是屏蔽复杂的知识，降低开发难度，开发人员不用太了解底层知识，就可以快速上手开发应用程序，让更多的开发者更关心所谓的业务开发。
如果只关心业务开发，你会发现今天出了一门语言，明天又出了一个框架，你感觉好像有学不完的知识。一直在学习，但感觉学不动了，要休息了。所以，我学习新技术，或者新框架，会先试着理解这个新技术是为了解决什么问题而产生的，原来的技术是否可以这样做，新技术与原技术在开发效率与运行效率上是不是有优势？
总之，我认为无论你是不是内核开发者，都有必要了解操作系统的相关知识。如果操作系统的知识你掌握了，就相当于掌握了内功心法，学习新的语言或者新的技术，只要看看官方的文档，就可以很快开始运用该技术做项目了。
学技术不动手，就好比游泳只看理论不下水。希望你也能认识到，动手去写代码、改代码很重要，让我们跟着LMOS，在操作系统的实践中不断精进！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>用户故事_用好动态调试，助力课程学习</title><link>https://artisanbox.github.io/9/56/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/56/</guid><description>你好，我是leveryd。
先做个自我介绍，我在网络安全行业从事技术工作，目前在负责安全产品的研发工作，工作六年。
虽然在研发工作中，我们通常是遇到什么问题就去查，边查边学。虽然这样的学习方式能快速解决问题，但有时候这种方法也不灵，比方说学习语义分析时，就必须要把词法分析、语法分析先学了，一通搜索、查阅、汇总和学习，回头一看，需要花费的时间和精力还是不少的。
显然，只靠自己在网上搜索，学到的常常是零零散散，效率太低。尤其是和工作的关联程度很高的必修知识，我觉得不太适合边查边学，更需要系统学习。结合自己的工作需要，今年年初的时候，我给自己安排了近期学习计划，定下了相应的学习的优先级。
其中，补充操作系统的专业知识就是高优先级的一项。近期学习《操作系统实战45讲》的过程中，我也跟着课程内容开始动手实践，还在课程群里分享了自己的调试经验。接到LMOS老师的邀请，今天我就和你聊聊我是怎样学习这门课程，以及我是如何调试课程代码的。
我是怎么学习《操作系统实战45讲》的根据我的学习需求，我给自己立下了两个学习目标：
第一，理解第十三课的代码：第十三课之前的内容包括了整个机器初始化过程；
第二，理解第二十六课的代码：比第十三课内容多了“内存”和“进程”。
在这个过程中，我会遇到一些问题，我把解决这些问题的实践经验写到公众号（公众号上我记录了这门课的学习实验笔记，以及关于安全业务和技术的一些案例）上，以此加深自己的理解。
就目前我自己的学习经验来看，“内核实验”比较复杂。这主要是因为内核涉及的知识较多，比如C语言、汇编、硬件知识；而且这方面内容比较底层，某些概念我们平时接触得比较少，比如汇编层面的函数调用细节。
另外，部分算法乍一看确实有点难理解，比如第二十五课中进程的切换是利用“栈上的函数返回地址”，而“返回地址”包括初始化和后面被进程调度器更新这两种场景。我们需要弄清楚这两个场景都是怎么更新的，才能更好理解进程是如何切换运行的。
Cosmos调试思路因为刚才说的这些原因，当我们遇到疑问时，往往无法从网络上直接搜到答案。这个时候，就可以通过调试来辅助我们分析问题。
接下来，我就说一说我是怎么调试课程代码的，后面还会再分享一下我通过动态调试解决疑问的例子。
虽然我们可以在代码中打印日志，但这种方式效率不高，因为每次都需要编写代码、重新编译运行。我更喜欢用GDB和QEMU动态调试Cosmos。
结合下图中我们可以看到：使用GDB在Cosmos内核函数下了断点，并且断点生效。如果我想观察copy_pages_data的逻辑，就只需要在单步调试过程中观察内存的变化，这样就能知道copy_pages_data建立的页表数据长什么样子。
总的来说，想要动态调试，我们首先需要编译一个带调试符号的elf文件出来，然后更新hd.img镜像文件。
接着我们用QEMU启动内核，具体命令如下：
➜ myos qemu-system-x86_64 -drive format=raw,file=hd.img -m 512M -cpu kvm64,smep,smap -s // 一定要加-s参数，此参数可以打开调试服务。 最后，我们用GDB加载调试符号并调试，具体命令如下：
(gdb) symbol-file ./initldr/build/initldrkrl.elf // 加载调试符号，这样才能在显示源码、可以用函数名下断点 Reading symbols from /root/cosmos/lesson13/Cosmos/initldr/build/initldrkrl.elf...done. (gdb) target remote :1234 // 连接qemu-system-x86_64 -s选项打开的1234端口进行调试 Remote debugging using :1234 0x000000000000e82e in ?? () 我已经将编译好的带调试符号的elf文件，以及对应的hd.img镜像文件放在了GitHub上，你可以直接用这些文件和上面的命令来调试。仓库中目前我只放了对应第十三课和第二十六课的调试文件，如果你想要调试其他课的代码，不妨继续往下看。
制作“带调试符号的elf文件"的详细步骤如果你调试过Linux内核，应该比较熟悉上面的流程。不过在制作“带调试符号的elf文件”时，Cosmos和Linux内核有些不同，下面我就详细说明一下。
先说说整体思路：通过修改编译选项，即可生成“带调试符号的elf文件”。然后再生成Cosmos.eki内核文件，最后替换hd.img镜像文件中的Cosmos.eki文件。这样，我们就可以用“带调试符号的elf文件”和hd.img来调试代码了。
修复两个bug只有先修复后面这两个bug，才能成功编译，并且运行Cosmos内核代码。
第一个问题是：编译第十三课的代码时遇到一个报错，报错截图如下。
解决办法很简单：将kernel.asm文件中的“kernel.inc”修改成“/kernel.inc”，你可以对照后面的截图看一下。
第二个问题是第二十六课遇到的运行时报错，如下图所示。
因为acpi是和“电源管理”相关的模块，这里并没有用到，所以我们可以注释掉 initldr/ldrkrl/chkcpmm.c 文件中的init_acpi 函数调用。
解决掉这两个问题，就可以成功编译第十三课和第二十六课的代码了。
修改“编译选项"修复bug后，我们虽然能够成功编译运行，但是因为文件没有调试符号，所以我们在GDB调试时无法对应到c源码，也无法用函数名下断点。因此，我们需要通过修改编译选项来生成带调试符号的elf文件。
为了编译出带调试符号的执行文件，需要对编译脚本做两处修改。
第一处修改，GCC的-O2参数要修改成O0 -g参数：-O0是告诉GCC编译器，在编译时不要对代码做优化，这么做的原因是避免在GDB调试时源码和实际程序对应不上的情况；-g参数是为了告诉编译器带上调试符号。</description></item><item><title>结束语_生活可以一地鸡毛，但操作系统却是心中的光</title><link>https://artisanbox.github.io/9/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/50/</guid><description>你好，我是LMOS。
感谢你的一路相伴，我们的《操作系统实战45讲》专栏写到此处，你亦能学至此处，多半是出于兴趣，出于一种对操作系统的热爱，出于一种对事物本质发自内心的苛求……
如果是这样，请你永远保持这份心性，它会给你带来更多意想不到的结果。走到这里，也让我们先停住前进的脚步，回忆一下这一路走来都做了些什么事情，收获了什么，有什么让我们印象深刻的体会？
我作为Cosmos和《操作系统实战45讲》专栏这两大作品的作者先来开个头，跟你说说我自己的感受和体会，可以用两个“出乎意料”来表示。
第一个“出乎意料”，是课程出乎意料的难写。我之前写过书，也写过多个操作系统内核，更是做过业界重量级的傲腾项目，但是专栏之难，超过我之前做过所有的项目之难。
一开始，我也不明白为什么写专栏比写代码难？但经历了整个专栏的筹备、备稿、修改，一直到更新和答疑的各个环节，我才深刻地体会到这点。
我最初设计整个专栏的时候，就想兼顾宏观思路和细节实现，既带你领略操作系统的壮观风景，也能作为指导手册让你跟着我动手实现。但是写起来才发现，为了完成这两点，我实际花的时间跟精力，远远超过了预估。
写专栏，难就难在要用通俗的大白话，把复杂的操作系统“讲”出来，而不只是写出来；难就难在细节与重点的把握和梳理。如果只有细节，就难以体现出重点。可是如果只有重点思路，我又担心内容会让你觉得过于抽象；难就难在，我要交付的对象，不再是编译器，而是各个不同思想层次、不同思维方式的人。
第二个“出我意料”，是出我意料的“热”。我搞了很多年的操作系统，感觉操作系统在整个行业之中非常冷，操作系统之冷，是那种高处不胜寒的“冷”，是学校老师都只愿意从理论上一笔带过的“冷” ，是互联网时代的创新企业无法触及，也不敢触及的“冷”。
但是出我意料的是，专栏刚刚上线不久就引起了业界广泛关注，其热度超出了我的想像。我以为在业务为王的今天，很少有人会关注这么底层的操作系统。不得不说，这些关注从侧面说明了操作系统在各从业人员心中的重要性，同时也说明了我们对亲手实现一个操作系统这件事充满好奇。
前面这些是对专栏的体会和感受，下面我想谈一谈写Cosmos的感受。相信看过专栏的同学，对操作系统工程之浩大，代码之精微，都有了深切的体会和认知。说开发成熟操作系统之难，难于上青天，这绝不是夸张和开玩笑。
在互联网时代，我可能比围观的同学更清楚，不能基于功利的目的去开发Cosmos，在今天它无法直接给我们产生价值，我开发Cosmos是基于兴趣，是对技术的探索和追求。我就是那种人——生活可以一地鸡毛，但操作系统却是心中的光。
Cosmos断断续续开发很多年，几次推倒重来，正是在这种一次次重构之下，摸索、总结，才设计出了今天Cosmos的架构。很多代码要反复测试验证，对于没有达到预期的代码，我需要对其算法进行分析，找出原因。
Cosmos的调试是最难的，往往需要查找其文件的反汇编代码，然后一条一条对比，在脑中模拟指令的执行过程和结果，并发现隐藏其中的Bug，这些都是极其烦琐的事情。不瞒你说，我也会一个bug卡好几天，感觉写内核仿佛是一场“法事”，一个人念咒、画符，请神跳舞……可以说，若没有“爱”的加持，真的很难坚持下来。
其实，我们写专栏的顺序正是我开发Cosmos过程的顺序，只有这样，才能把我的经验原样分享给你们。
因为我就是从Hello World应用程序开始，探索计算机是如何运行一个应用程序的，进而一步步了解了操作系统内核中的所有组件，在心中建立了一个现代操作系统内核的模型。
因为操作系统内核必须要运行在具体的计算平台上，所以我研读了大量的芯片手册，并且着重了解了其中CPU和内存的细节。接着，我又学习了编译工具集。有了这些基础，我开始写引导器和初始化代码，逐步实现了内存管理、进程调度、设备I/O、文件系统、网络和若干设备驱动程序，最后实现了系统调用和应用程序库。
虽然这些组件比成熟的操作系统内核中的组件简单得多，但实现的都是最关键、最核心、最必要的功能机制，简小而全面一直是我的思想，而这也正是我们这些操作系统初学者想要的。
我们的专栏虽然结束了，但是我们的Cosmos才刚刚开始。不知道你是否也在思考，我们亲自建造的Cosmos，为什么没有强大的文件系统和网络组件，为什么没有精美且高性能的图形界面，为什么没有工业级的安全性？
如果你真的在思考、在好奇，如果你真的有兴趣，还想继续探索，我真诚地希望你能再次阅读更多的书籍，或者借助万能的互联网，去搜寻资料，去寻找答案。
相信以这份好奇和兴趣为动力，必定会从一无所知，到知道一点点，再到知道一部分，慢慢地积累，也许有一天你会惊奇地跳起来，用尽全身力气喊出来：“原来我也能了”，“我真的能了”！
到了那一天，想必我们也已经有了全新的开始，那一定将是真正具备创造性的开始。
如果你愿意，也可以加入我们的Cosmos开源社区，让我们再续前缘，一起开发Cosmos操作系统，让我们一起开始创造性的工作。
Cosmos开源社区以Cosmos的“Ψ(Psi)”架构为基础进行展开，Ψ(Psi)内核架构有别于微内核、宏内核、混合内核，它吸收了其它内核架构的优势，完全摒弃了其它内核架构的劣势，这就导致了现有的硬件架构体系，不适应运行这样的Cosmos。
为此，我们将以RISCV处理器为基础进行扩展，形成“Ψ(Psi)”架构的RISCV处理器，这个处理器将成为运行Cosmos特有的处理器，同时这个“Ψ(Psi)”架构的RISCV处理器也会开源，形成硬件、软件双开源的方式，欢迎各方勇士加入，一起迎接挑战，一起开创IT新纪元。
也许有一天，人们会用着我们建造的操作系统，在我们自己设计的计算机上，听着杜比级别的音乐、看着4K画质的高清电影、玩着如梦如幻的3D游戏、和远方的恋人进行视频通话、进行超大规模的科学计算、处理着海量级的网络数据……
但是别忘了，这仅仅是因为我们最初那一点点求知欲和兴趣……
虽然暂时需要告别，但我期待后会有期。感谢3个多月的同行，真心希望我的专栏对你有所帮助。
我知道，很多同学总是默默潜水，默默学习。所以在专栏即将结束的今天，我希望听听你学习这个专栏的感受。这里我为你准备了一份毕业问卷，题目不多，希望你可以花两分钟填一下。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>结课测试｜这些操作系统的问题，你都掌握了么？</title><link>https://artisanbox.github.io/9/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/49/</guid><description>你好，我是LMOS。
《操作系统实战45讲》已经完结了。在这段时间里，我依然收到了很多用户的留言，很感谢你一直以来的认真学习和支持！
为了帮助你检验自己的学习效果，我特别给你准备了一套结课测试题。这套测试题共有 20道题目，包括 10 道单选题，10 道多选题，满分 100 分，系统会自动评分。点击下面按钮，马上开始测试吧！
最后，我很希望听听你学习这个专栏的感受。这里我为你准备了一份毕业问卷，题目不多，希望你可以花两分钟填一下。</description></item><item><title>编辑手记_升级认知，迭代自己的操作系统</title><link>https://artisanbox.github.io/9/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/9/47/</guid><description>你好，我是宇新，《操作系统实战45讲》的专栏编辑。
除了负责更新课程里的内容，我也一直关注着小伙伴们的留言。这次，终于有机会自己也留一回言了，很开心能用编辑手记的方式，和你聊一聊我的想法。
这门课的独特之处细心的小伙伴可能发现了，我们的开篇词标题是“为什么要学写一个操作系统？”注意，不只是学操作系统，而是学着去“写”一个操作系统。
你可能还会想，平时我们接触不到的“黑盒子”，现在却要我们自己写代码实现，听起来很有挑战啊？为什么会这样设计呢，且听我慢慢道来。
操作系统博大精深，甚至每个子模块单拿出来讲，都有无数的知识点，太容易只见树木不见森林。但用一个实战项目连起来的话，就能很好地帮助我们聚焦关键问题。
看似“写”操作系统，这是把难度升级了，其实是为了控制我们的作战范围。写操作系统的时候，涉及哪些关键要点，我们就相应地学习研究这部分内容。
现在成熟的操作系统，像是Linux系统，它的源码量级已是今非昔比，我们去看源代码总会晕头转向。但老师的课程像是一条线，把实战需要的东西都展示出来，想要深入研究的同学建议对照查漏补缺，然后继续跟着课程走，这样才能实现“螺旋式”进步。
如果你也喜欢玩游戏的话，估计有这样的体验，把游戏调成了无敌模式，很容易就会索然无味。没错，有挑战的游戏才好玩。有时候卡在某一处确实很痛苦，但是突破以后也会爽。你不妨把自己当作玩家，去攻克一个个操作系统的关卡。当然了，你也不是孤军奋战，遇到疑问，还可以通过学习、交流和讨论去解决。
课程的思路我就说到这里，如果你感兴趣，还可以看看我们的课程设计文档。
更多课程设计的缘起，也可以看看LMOS老师好友Yason Lee的解读：《大咖助场｜以无法为有法，以无限为有限》。
怎样学习这门课课程上线以后啊，LMOS老师跟我都在关注大家的留言反馈。
学习这门课的同学身份各异，从学生党到已经退休的朋友都有，但共同特点就是对操作系统充满热情，因为这样一个专栏而结缘。无论是在课程交流群，还是课程留言区里，这两个疑问算是高频出现的。
学习这门课，我需要什么前置知识？ 某个问题/知识点好难啊，我该怎么办？ 这里我就从编辑的视角说说我的看法吧。
先说第一个问题，需要什么基础。我一直在琢磨这个问题背后的含义。同学们的水平参差不齐，有畏难心理这很正常，你学习课程的时候，其实是明确了自己哪里“不会”，换个角度想，这样学习的时候不就能有的放矢了么？
不少同学担心自己不是科班出身，其实LMOS也不是科班出身的，这些历史问题还是翻篇更好，你过去怎么样，并不代表你之后不可以学习、研究操作系统。而且，就算是计算机相关专业的同学，可能学生时代上的操作系统课程也没留下特别深刻的印象，考完试就还给老师的，也大有人在。
现在还没有看完的同学也不要着急，因为更新的速度肯定要比你们的学习速度快上不少。你需要做的是按照课程顺序持续学习，慢慢来，遇到不懂的，就多看几篇，多看几遍。
课代表陈诚同学说过一句话，我记得特别深，他是这么说的：
“其实，我觉得我们想学写操作系统，有时候是为了一碟醋包了一顿饺子，但是最终饺子是自己的了。”
我注意到有不少小伙伴为了打牢基础，为了跟上课程，去补充了汇编、C语言，以及计算机组成原理方面的知识，我要给这些人点赞。
但是，就算你没有把那些图书从头看到尾，其实也同样可以跟着课程，循序渐进地学习。建议你边学边练，动手跑起来。哪怕最初你只能复制老师给的配套代码，但是只要肯用心，也会对操作系统有更深的理解。与其苦恼于自己基础不行，不如踏踏实实去学习精进。
为了让你明确每个模块的内容重点和难易程度，我为你整理了一张表格，你可以做个参考。
如果你还是想把操作系统的相关资料也都一并啃下来，那可以看看LMOS提供的参考书单，在学有余力的情况下拓展阅读。
1.关于编译工具：LD手册、GAS手册、GCC手册、nasm手册、make手册；
2.关于GRUB：GRUB手册；
3.关于CPU：Intel手册；
4.关于汇编：《汇编程序设计》；
5.关于C语言：《C语言程序设计现代方法》；
6.关于操作系统：《操作系统设计与实现》。
如果你想参考优秀课代表的学习经验与方法，可以参考后面这些用户故事。
1.零基础yiyang同学的课程实战经验；
2.优秀课代表pedro的技术学习方法；
3.技术发烧友spring Xu的课程学习思考；
4.安全产品研发leveryd的动态调试学习法；
5.课程优质笔记分享达人neohope的访谈加餐：技术学习与职业成长方法论。
下面我再说说第二个问题，当你具体学习的时候，觉得某个知识点很难，应该怎么办？对于这个问题我想给你分享三个小建议。
第一个建议就是做好心理建设。
就拿不少同学都觉得头疼的内存管理来说吧。其实当时我在看这部分稿件的时候，也觉得压力山大。记得当时LMOS老师还鼓励我说，挺过去就好了。现在你看到的内存章节，其中16～18讲原先是一整块的内容，我们经过讨论优化，考虑让大伙儿更容易跟上，才拆分细化成了三节课。
内存是内核的内核，肯定很难。不过就像英语单词不能永远背到“abandon”一样，想要深入地探索操作系统，这关必须迎难而上。
以第19课如何管理内存对象为例，不知道你看没看到置顶评论中“neohope”同学的学习笔记，建议重点关注一下他抓“关键”内容的能力。
古语说，不积跬步无以至千里。你可能会怀疑自己，但不必过度焦虑。如果咱们因为差距过大，而陷入弃疗状态，那就太可惜了。哪怕是“大佬”，也曾有萌新时期，基础不好就慢慢跟进。
第二个建议就是明确自己的需求，按需学习。
虽然没有什么“跳关”秘籍，但还是有些技巧让你快速掌握一节课内容的。没错，就像数据结构一样，每节课也有“内容结构”，想要快速消化，可以着重理一理后面这几点：
这个模块/这节课要解决什么问题（What） 思路是什么/ 为什么要这么解决（Why） 具体如何用代码实现（How） 你还可以自己动手用流程图画一下（pedro同学推荐了此方法，你可以试试绘图工具 Graph-Easy），这样不容易迷路。
当然，如果你已经有不少的学习积累，或者目的不在于“全景浏览”和“扫盲”，而是想要更加深入，那你必然要花费更多苦工。操作系统是星辰大海，建议以你困惑的问题为导向，进行专项突破。
比如，第23节课 Slab 内存对象，来自课程交流 1 群的zzyj同学就分享了Slab作者写的参考文献，你不妨搭配使用。
我的第三个建议是，积极交流，在反馈和记录中激励自己。
虽然学习方法重要，但我们也不要沉迷于把时间消耗在“找方法”上。很可能“优质方法”给你节省的时间，还赶不上你在找方法这件事上花掉的时间。
一人计短，众人计长，我们课程开设留言区，在部落开话题（推荐你在话题下分享自己的学习收获，晒一晒实战截图），搭建用户交流群，就是为了让你的学习之旅不再孤单，让我们在分享交流中一同进步。
除了多交流，我也强烈推荐你学习留痕，把你的阶段性学习成果、经验记录下来，这些都能激励自己坚持学习。都说闻道有先后，术业有专攻。百科全书式的人毕竟是少数，但爱学习的小伙伴总会遇到志同道合的朋友。
今天的提问者，也许明天就有能力给别人解答问题了，这就是教学相长。我们的助教 Jason提到：
“教别人是个沟通的过程，各种感官都会调度起来。调度越多，大脑参与理解记忆的部分就越多，以后回忆起来，搜索路径就越多。光看的话，只是眼睛。这跟实践出真知，道理类似。”</description></item></channel></rss>