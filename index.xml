<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gen 的学习笔记</title><link>https://artisanbox.github.io/</link><description>Recent content on Gen 的学习笔记</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 08 Mar 2022 18:37:53 +0800</lastBuildDate><atom:link href="https://artisanbox.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>01_为什么要学习数据结构和算法？</title><link>https://artisanbox.github.io/2/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/2/</guid><description>你是不是觉得数据结构和算法，跟操作系统、计算机网络一样，是脱离实际工作的知识？可能除了面试，这辈子也用不着？
尽管计算机相关专业的同学在大学都学过这门课程，甚至很多培训机构也会培训这方面的知识，但是据我了解，很多程序员对数据结构和算法依旧一窍不通。还有一些人也只听说过数组、链表、快排这些最最基本的数据结构和算法，稍微复杂一点的就完全没概念。
当然，也有很多人说，自己实际工作中根本用不到数据结构和算法。所以，就算不懂这块知识，只要Java API、开发框架用得熟练，照样可以把代码写得“飞”起来。事实真的是这样吗？
今天我们就来详细聊一聊，为什么要学习数据结构和算法。
想要通关大厂面试，千万别让数据结构和算法拖了后腿很多大公司，比如BAT、Google、Facebook，面试的时候都喜欢考算法、让人现场写代码。有些人虽然技术不错，但每次去面试都会“跪”在算法上，很是可惜。那你有没有想过，为什么这些大公司都喜欢考算法呢？
校招的时候，参加面试的学生通常没有实际项目经验，公司只能考察他们的基础知识是否牢固。社招就更不用说了，越是厉害的公司，越是注重考察数据结构与算法这类基础知识。相比短期能力，他们更看中你的长期潜力。
你可能要说了，我不懂数据结构与算法，照样找到了好工作啊。那我是不是就不用学数据结构和算法呢？当然不是，你别忘了，我们学任何知识都是为了“用”的，是为了解决实际工作问题的，学习数据结构和算法自然也不例外。
业务开发工程师，你真的愿意做一辈子CRUD boy吗？如果你是一名业务开发工程师，你可能要说，我整天就是做数据库CRUD（增删改查），哪里用得到数据结构和算法啊？
是的，对于大部分业务开发来说，我们平时可能更多的是利用已经封装好的现成的接口、类库来堆砌、翻译业务逻辑，很少需要自己实现数据结构和算法。但是，不需要自己实现，并不代表什么都不需要了解。
如果不知道这些类库背后的原理，不懂得时间、空间复杂度分析，你如何能用好、用对它们？存储某个业务数据的时候，你如何知道应该用ArrayList，还是Linked List呢？调用了某个函数之后，你又该如何评估代码的性能和资源的消耗呢？
作为业务开发，我们会用到各种框架、中间件和底层系统，比如Spring、RPC框架、消息中间件、Redis等等。在这些基础框架中，一般都揉和了很多基础数据结构和算法的设计思想。
比如，我们常用的Key-Value数据库Redis中，里面的有序集合是用什么数据结构来实现的呢？为什么要用跳表来实现呢？为什么不用二叉树呢？
如果你能弄明白这些底层原理，你就能更好地使用它们。即便出现问题，也很容易就能定位。因此，掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。
在平时的工作中，数据结构和算法的应用到处可见。我来举一个你非常熟悉的例子：如何实时地统计业务接口的99%响应时间？
你可能最先想到，每次查询时，从小到大排序所有的响应时间，如果总共有1200个数据，那第1188个数据就是99%的响应时间。很显然，每次用这个方法查询的话都要排序，效率是非常低的。但是，如果你知道“堆”这个数据结构，用两个堆可以非常高效地解决这个问题。
基础架构研发工程师，写出达到开源水平的框架才是你的目标！现在互联网上的技术文章、架构分享、开源项目满天飞，照猫画虎做一套基础框架并不难。我就拿RPC框架举例。
不同的公司、不同的人做出的RPC框架，架构设计思路都差不多，最后实现的功能也都差不多。但是有的人做出来的框架，Bug很多、性能一般、扩展性也不好，只能在自己公司仅有的几个项目里面用一下。而有的人做的框架可以开源到GitHub上给很多人用，甚至被Apache收录。为什么会有这么大的差距呢？
我觉得，高手之间的竞争其实就在细节。这些细节包括：你用的算法是不是够优化，数据存取的效率是不是够高，内存是不是够节省等等。这些累积起来，决定了一个框架是不是优秀。所以，如果你还不懂数据结构和算法，没听说过大O复杂度分析，不知道怎么分析代码的时间复杂度和空间复杂度，那肯定说不过去了，赶紧来补一补吧！
对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！何为编程能力强？是代码的可读性好、健壮？还是扩展性好？我觉得没法列，也列不全。但是，在我看来，性能好坏起码是其中一个非常重要的评判标准。但是，如果你连代码的时间复杂度、空间复杂度都不知道怎么分析，怎么写出高性能的代码呢？
你可能会说，我在小公司工作，用户量很少，需要处理的数据量也很少，开发中不需要考虑那么多性能的问题，完成功能就可以，用什么数据结构和算法，差别根本不大。但是你真的想“十年如一日”地做一样的工作吗？
经常有人说，程序员35岁之后很容易陷入瓶颈，被行业淘汰，我觉得原因其实就在此。有的人写代码的时候，从来都不考虑非功能性的需求，只是完成功能，凑合能用就好；做事情的时候，也从来没有长远规划，只把眼前事情做好就满足了。
我曾经面试过很多大龄候选人，简历能写十几页，经历的项目有几十个，但是细看下来，每个项目都是重复地堆砌业务逻辑而已，完全没有难度递进，看不出有能力提升。久而久之，十年的积累可能跟一年的积累没有任何区别。这样的人，怎么不会被行业淘汰呢？
如果你在一家成熟的公司，或者BAT这样的大公司，面对的是千万级甚至亿级的用户，开发的是TB、PB级别数据的处理系统。性能几乎是开发过程中时刻都要考虑的问题。一个简单的ArrayList、Linked List的选择问题，就可能会产生成千上万倍的性能差别。这个时候，数据结构和算法的意义就完全凸显出来了。
其实，我觉得，数据结构和算法这个东西，如果你不去学，可能真的这辈子都用不到，也感受不到它的好。但是一旦掌握，你就会常常被它的强大威力所折服。之前你可能需要费很大劲儿来优化的代码，需要花很多心思来设计的架构，用了数据结构和算法之后，很容易就可以解决了。
内容小结我们学习数据结构和算法，并不是为了死记硬背几个知识点。我们的目的是建立时间复杂度、空间复杂度意识，写出高质量的代码，能够设计基础架构，提升编程技能，训练逻辑思维，积攒人生经验，以此获得工作回报，实现你的价值，完善你的人生。
所以，不管你是业务开发工程师，还是基础架构工程师；不管你是初入职场的初级工程师，还是工作多年的资深架构师，又或者是想转人工智能、区块链这些热门领域的程序员，数据结构与算法作为计算机的基础知识、核心知识，都是必须要掌握的。
掌握了数据结构与算法，你看待问题的深度，解决问题的角度就会完全不一样。因为这样的你，就像是站在巨人的肩膀上，拿着生存利器行走世界。数据结构与算法，会为你的编程之路，甚至人生之路打开一扇通往新世界的大门。
课后思考你为什么要学习数据结构和算法呢？在过去的软件开发中，数据结构和算法在哪些地方帮到了你？
欢迎留言和我分享，我会第一时间给你反馈。如果你的朋友也在学习算法这个问题上犹豫不决，欢迎你把这篇文章分享给他！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>01_冯·诺依曼体系结构：计算机组成的金字塔</title><link>https://artisanbox.github.io/4/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/1/</guid><description>学习计算机组成原理，到底是在学些什么呢？这个事儿，一两句话还真说不清楚。不过没关系，我们先从“装电脑”这个看起来没有什么技术含量的事情说起，来弄清楚计算机到底是由什么组成的。
不知道你有没有自己搞过“装机”这回事儿。在2019年的今天，大部分人用的计算机，应该都已经是组装好的“品牌机”。如果我们把时钟拨回到上世纪八九十年代，不少早期的电脑爱好者，都是自己采购各种电脑配件，来装一台自己的计算机的。
计算机的基本硬件组成早年，要自己组装一台计算机，要先有三大件，CPU、内存和主板。
在这三大件中，我们首先要说的是CPU，它是计算机最重要的核心配件，全名你肯定知道，叫中央处理器（Central Processing Unit）。为什么说CPU是“最重要”的呢？因为计算机的所有“计算”都是由CPU来进行的。自然，CPU也是整台计算机中造价最昂贵的部分之一。
第二个重要的配件，就是内存（Memory）。你撰写的程序、打开的浏览器、运行的游戏，都要加载到内存里才能运行。程序读取的数据、计算得到的结果，也都要放在内存里。内存越大，能加载的东西自然也就越多。
存放在内存里的程序和数据，需要被CPU读取，CPU计算完之后，还要把数据写回到内存。然而CPU不能直接插到内存上，反之亦然。于是，就带来了最后一个大件——主板（Motherboard）。
主板是一个有着各种各样，有时候多达数十乃至上百个插槽的配件。我们的CPU要插在主板上，内存也要插在主板上。主板的芯片组（Chipset）和总线（Bus）解决了CPU和内存之间如何通信的问题。芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题。总线则是实际数据传输的高速公路。因此，总线速度（Bus Speed）决定了数据能传输得多快。
有了三大件，只要配上电源供电，计算机差不多就可以跑起来了。但是现在还缺少各类输入（Input）/输出（Output）设备，也就是我们常说的I/O设备。如果你用的是自己的个人电脑，那显示器肯定必不可少，只有有了显示器我们才能看到计算机输出的各种图像、文字，这也就是所谓的输出设备。
同样的，鼠标和键盘也都是必不可少的配件。这样我才能输入文本，写下这篇文章。它们也就是所谓的输入设备。
最后，你自己配的个人计算机，还要配上一个硬盘。这样各种数据才能持久地保存下来。绝大部分人都会给自己的机器装上一个机箱，配上风扇，解决灰尘和散热的问题。不过机箱和风扇，算不上是计算机的必备硬件，我们拿个纸板或者外面放个电风扇，也一样能用。
说了这么多，其实你应该有感觉了，显示器、鼠标、键盘和硬盘这些东西并不是一台计算机必须的部分。你想一想，我们其实只需要有I/O设备，能让我们从计算机里输入和输出信息，是不是就可以了？答案当然是肯定的。
你肯定去过网吧吧？不知道你注意到没有，很多网吧的计算机就没有硬盘，而是直接通过局域网，读写远程网络硬盘里面的数据。我们日常用的各类云服务器，只要让计算机能通过网络，SSH远程登陆访问就好了，因此也没必要配显示器、鼠标、键盘这些东西。这样不仅能够节约成本，还更方便维护。
还有一个很特殊的设备，就是显卡（Graphics Card）。现在，使用图形界面操作系统的计算机，无论是Windows、Mac OS还是Linux，显卡都是必不可少的。有人可能要说了，我装机的时候没有买显卡，计算机一样可以正常跑起来啊！那是因为，现在的主板都带了内置的显卡。如果你用计算机玩游戏，做图形渲染或者跑深度学习应用，你多半就需要买一张单独的显卡，插在主板上。显卡之所以特殊，是因为显卡里有除了CPU之外的另一个“处理器”，也就是GPU（Graphics Processing Unit，图形处理器），GPU一样可以做各种“计算”的工作。
鼠标、键盘以及硬盘，这些都是插在主板上的。作为外部I/O设备，它们是通过主板上的南桥（SouthBridge）芯片组，来控制和CPU之间的通信的。“南桥”芯片的名字很直观，一方面，它在主板上的位置，通常在主板的“南面”。另一方面，它的作用就是作为“桥”，来连接鼠标、键盘以及硬盘这些外部设备和CPU之间的通信。
有了南桥，自然对应着也有“北桥”。是的，以前的主板上通常也有“北桥”芯片，用来作为“桥”，连接CPU和内存、显卡之间的通信。不过，随着时间的变迁，现在的主板上的“北桥”芯片的工作，已经被移到了CPU的内部，所以你在主板上，已经看不到北桥芯片了。
冯·诺依曼体系结构刚才我们讲了一台计算机的硬件组成，这说的是我们平时用的个人电脑或者服务器。那我们平时最常用的智能手机的组成，也是这样吗？
我们手机里只有SD卡（Secure Digital Memory Card）这样类似硬盘功能的存储卡插槽，并没有内存插槽、CPU插槽这些东西。没错，因为手机尺寸的原因，手机制造商们选择把CPU、内存、网络通信，乃至摄像头芯片，都封装到一个芯片，然后再嵌入到手机主板上。这种方式叫SoC，也就是System on a Chip（系统芯片）。
这样看起来，个人电脑和智能手机的硬件组成方式不太一样。可是，我们写智能手机上的App，和写个人电脑的客户端应用似乎没有什么差别，都是通过“高级语言”这样的编程语言撰写、编译之后，一样是把代码和数据加载到内存里来执行。这是为什么呢？因为，无论是个人电脑、服务器、智能手机，还是Raspberry Pi这样的微型卡片机，都遵循着同一个“计算机”的抽象概念。这是怎么样一个“计算机”呢？这其实就是，计算机祖师爷之一冯·诺依曼（John von Neumann）提出的冯·诺依曼体系结构（Von Neumann architecture），也叫存储程序计算机。
什么是存储程序计算机呢？这里面其实暗含了两个概念，一个是“可编程”计算机，一个是“存储”计算机。
说到“可编程”，估计你会有点懵，你可以先想想，什么是“不可编程”。计算机是由各种门电路组合而成的，然后通过组装出一个固定的电路板，来完成一个特定的计算程序。一旦需要修改功能，就要重新组装电路。这样的话，计算机就是“不可编程”的，因为程序在计算机硬件层面是“写死”的。最常见的就是老式计算器，电路板设好了加减乘除，做不了任何计算逻辑固定之外的事情。
我们再来看“存储”计算机。这其实是说，程序本身是存储在计算机的内存里，可以通过加载不同的程序来解决不同的问题。有“存储程序计算机”，自然也有不能存储程序的计算机。典型的就是早年的“Plugboard”这样的插线板式的计算机。整个计算机就是一个巨大的插线板，通过在板子上不同的插头或者接口的位置插入线路，来实现不同的功能。这样的计算机自然是“可编程”的，但是编写好的程序不能存储下来供下一次加载使用，不得不每次要用到和当前不同的“程序”的时候，重新插板子，重新“编程”。
可以看到，无论是“不可编程”还是“不可存储”，都会让使用计算机的效率大大下降。而这个对于效率的追求，也就是“存储程序计算机”的由来。
于是我们的冯祖师爷，基于当时在秘密开发的EDVAC写了一篇报告First Draft of a Report on the EDVAC，描述了他心目中的一台计算机应该长什么样。这篇报告在历史上有个很特殊的简称，叫First Draft，翻译成中文，其实就是《第一份草案》。这样，现代计算机的发展就从祖师爷写的一份草案开始了。
First Draft里面说了一台计算机应该有哪些部分组成，我们一起来看看。
首先是一个包含算术逻辑单元（Arithmetic Logic Unit，ALU）和处理器寄存器（Processor Register）的处理器单元（Processing Unit），用来完成各种算术和逻辑运算。因为它能够完成各种数据的处理或者计算工作，因此也有人把这个叫作数据通路（Datapath）或者运算器。
然后是一个包含指令寄存器（Instruction Register）和程序计数器（Program Counter）的控制器单元（Control Unit/CU），用来控制程序的流程，通常就是不同条件下的分支和跳转。在现在的计算机里，上面的算术逻辑单元和这里的控制器单元，共同组成了我们说的CPU。
接着是用来存储数据（Data）和指令（Instruction）的内存。以及更大容量的外部存储，在过去，可能是磁带、磁鼓这样的设备，现在通常就是硬盘。
最后就是各种输入和输出设备，以及对应的输入和输出机制。我们现在无论是使用什么样的计算机，其实都是和输入输出设备在打交道。个人电脑的鼠标键盘是输入设备，显示器是输出设备。我们用的智能手机，触摸屏既是输入设备，又是输出设备。而跑在各种云上的服务器，则是通过网络来进行输入和输出。这个时候，网卡既是输入设备又是输出设备。
任何一台计算机的任何一个部件都可以归到运算器、控制器、存储器、输入设备和输出设备中，而所有的现代计算机也都是基于这个基础架构来设计开发的。
而所有的计算机程序，也都可以抽象为从输入设备读取输入信息，通过运算器和控制器来执行存储在存储器里的程序，最终把结果输出到输出设备中。而我们所有撰写的无论高级还是低级语言的程序，也都是基于这样一个抽象框架来进行运作的。
总结延伸可以说，冯·诺依曼体系结构确立了我们现在每天使用的计算机硬件的基础架构。因此，学习计算机组成原理，其实就是学习和拆解冯·诺依曼体系结构。
具体来说，学习组成原理，其实就是学习控制器、运算器的工作原理，也就是CPU是怎么工作的，以及为何这样设计；学习内存的工作原理，从最基本的电路，到上层抽象给到CPU乃至应用程序的接口是怎样的；学习CPU是怎么和输入设备、输出设备打交道的。
学习组成原理，就是在理解从控制器、运算器、存储器、输入设备以及输出设备，从电路这样的硬件，到最终开放给软件的接口，是怎么运作的，为什么要设计成这样，以及在软件开发层面怎么尽可能用好它。
好了，这一讲说到这儿就结束了。你应该已经理解了计算机的硬件是由哪些设备组成的，以及冯·诺依曼体系结构是什么样的了。下一讲，我会带你看一张地图，也是计算机组成原理的知识地图。我们一起来看一看怎么样才是学习组成原理的好方法。
推荐阅读我一直认为，读读经典的论文，是从一个普通工程师迈向优秀工程师必经的一步。如果你有时间，不妨去读一读First Draft of a Report on the EDVAC。对于工程师来说，直接读取英文论文的原文，既可以搞清楚、弄明白对应的设计及其背后的思路来源，还可以帮你破除对于论文或者核心技术的恐惧心理。</description></item><item><title>01_基础架构：一条SQL查询语句是如何执行的？</title><link>https://artisanbox.github.io/1/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/1/</guid><description>你好，我是林晓斌。
这是专栏的第一篇文章，我想来跟你聊聊MySQL的基础架构。我们经常说，看一个事儿千万不要直接陷入细节里，你应该先鸟瞰其全貌，这样能够帮助你从高维度理解问题。同样，对于MySQL的学习也是这样。平时我们使用数据库，看到的通常都是一个整体。比如，你有个最简单的表，表里只有一个ID字段，在执行下面这个查询语句时：
mysql&amp;gt; select * from T where ID=10； 我们看到的只是输入一条语句，返回一个结果，却不知道这条语句在MySQL内部的执行过程。
所以今天我想和你一起把MySQL拆解一下，看看里面都有哪些“零件”，希望借由这个拆解过程，让你对MySQL有更深入的理解。这样当我们碰到MySQL的一些异常或者问题时，就能够直戳本质，更为快速地定位并解决问题。
下面我给出的是MySQL的基本架构示意图，从中你可以清楚地看到SQL语句在MySQL的各个功能模块中的执行过程。
MySQL的逻辑架构图大体来说，MySQL可以分为Server层和存储引擎层两部分。
Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。
而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。
也就是说，你执行create table建表的时候，如果不指定引擎类型，默认使用的就是InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在create table语句中使用engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同，在后面的文章中，我们会讨论到引擎的选择。
从图中不难看出，不同的存储引擎共用一个Server层，也就是从连接器到执行器的部分。你可以先对每个组件的名字有个印象，接下来我会结合开头提到的那条SQL语句，带你走一遍整个执行流程，依次看下每个组件的作用。
连接器第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：
mysql -h$ip -P$port -u$user -p 输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在-p后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。
连接命令中的mysql是客户端工具，用来跟服务端建立连接。在完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。
如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。
连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在show processlist命令中看到它。文本中这个图是show processlist的结果，其中的Command列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。
客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时。
如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。
数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。
建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。
但是全部使用长连接后，你可能会发现，有些时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。
怎么解决这个问题呢？你可以考虑以下两种方案。
定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。
查询缓存连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步：查询缓存。
MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。
如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。
但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。
查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。
好在MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：
mysql&amp;gt; select SQL_CACHE * from T where ID=10； 需要注意的是，MySQL 8.</description></item><item><title>01｜实现一门超简单的语言最快需要多久？</title><link>https://artisanbox.github.io/3/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/3/</guid><description>你好，我是宫文学。
说到实现一门计算机语言，你肯定觉得这是一个庞大又复杂的工程，工作量巨大！
这个理解，我只能说部分正确。其实，有的时候，实现一门语言的速度也可以很快。比如，当年兰登·艾克（Brendan Eich）只花了10天时间就把JavaScript语言设计出来了。当然，语言跟其他软件一样，也需要不断迭代，至今JS的标准和实现仍在不停的演化。
如果我说，你也完全可以在这么短的时间内实现一门语言，甚至都不需要那么长时间，你一定会觉得我是在哗众取宠、标题党。
别急，我再补充说明一下，你马上就会认可我的说法了。这个让你一开始实现的版本，只是为了去探索计算机语言的原理，是高度简化的版本，并不要求马上能实用。你可以把它看做是一个原型系统，仅此而已，实现起来不会太复杂。
好吧，我知道你肯定还在心里打鼓：再简单的计算机语言，那也是一门语言呀，难度又能低到哪里去？
这样，先保留你的疑虑，我们进入今天的课程。今天我就要带你挑战，仅仅只用一节课时间，就实现一门超简洁的语言。我会暂时忽略很多的技术细节，带你抓住实现一门计算机语言的骨干部分，掌握其核心原理。在这节课中，你会快速获得两个技能：
如何通过编译器来理解某个程序； 如何解释执行这个程序。 这两个点，分别是编译时的核心和运行时的核心。理解了这两个知识点，你就大致理解计算机语言是如何工作的了！
我们的任务这节课，我们要让下面这个程序运行起来：
//一个函数的声明，这个函数很简单，只打印"Hello World!" function sayHello(){ println("Hello World!"); } //调用刚才声明的函数 sayHello(); 这个程序做了两件事：第一件是声明了一个函数，叫做sayHello；第二件事，就是调用sayHello()函数。运行这个程序的时候，我们期待它会输出“Hello World！”。
这个程序看上去还挺像那么回事的，但其实为降低难度，我们对JavaScript/TypeScript做了极度的简化：它只支持声明函数和调用函数，在我们的sayHello()函数里，它只能调用函数。你可以调用一个自己声明的函数，如foo，也可以调用语言内置的函数，如示例中的println()。
这还不够，为了进一步降低难度，我们的编译器是从一个数组里读取程序，而不是读一个文本文件。这个数组的每一个元素是一个单词，分别是function、sayHello、左括号、右括号等等，这些单词，我们叫它Token，它们是程序的最小构成单位。注意，最后一个Token比较特殊，它叫做EOF，有时会记做$，表示程序的结尾。
好了，现在任务清楚了，那我们开始第一步，解析这个程序。
解析这个程序解析，英文叫做Parse，是指读入程序，并形成一个计算机可以理解的数据结构的过程。能够完成解析工作的程序，就叫做解析器（Parser），它是编译器的组成部分之一。
那么，什么数据结构是计算机能够理解的呢？很简单，其实就是一棵树，这棵树叫做抽象语法树，英文缩写是AST（Abstract Syntax Tree）。针对我们的例子，这棵AST是下面的样子：
你仔细看一下这棵树，你会发现它跟我们程序想表达的思想是一样的：
根节点代表了整个程序； 根节点有两个子节点，分别代表一个函数声明和一个函数调用语句； 函数声明节点，又包含了两个子节点，一个是函数名称，一个是函数体； 函数体中又包含一个函数调用语句； 而函数调用语句呢，则是由函数名称和参数列表构成的； …… 通过这样自顶向下的层层分析，你会发现这棵树确实体现了我们原来的程序想表达的意思。其实，这就跟我们自己在阅读文章的时候是一样的，我们的大脑也是把段落分解成句子，再把句子分解成主语、谓语、宾语等一个个语法单元，最终形成一棵树型的结构，我们的大脑是能够理解这种树型结构的。
总结起来，解析器的工作，就是要读取一个Token串，然后把它转换成一棵AST。
好了，知道了解析的工作目标后，我们就来实现这个解析器吧！
可是，这怎么下手呢？
你可以琢磨一下，你的大脑是如何理解这些程序，并且把它们在不知不觉之间转化成一棵树的。我们假设，人类的大脑采用了一种自顶向下的分析方式，也就是把一个大的解析任务逐步分解成小的任务，落实到解析器的实现上也是如此。
首先，我们的目的是识别Token串的特征，并把它转换成AST。我们暂时忽略细节，假设我们能够成功地完成这个解析，那么把这个解析动作写成代码，就是：
prog = parseProg()； 我们再具体一点，看看实现parseProg()需要做什么事情。
parseProg()需要建立程序的子节点，也就是函数声明或者函数调用。我们规定一个程序可以有零到多个函数声明或函数调用。我们把这个语法规则用比较严谨的方法表达出来，是这样的：
prog = (functionDecl | functionCall)* ; 咦？这个表达方式看上去有点熟悉呀？这个式子的格式叫做EBNF格式（扩展巴科斯范式）。你可以看到，它的左边是一个语法成份的名称，右边说的是这个语法成份是由哪些子成分构成的。这样，整个式子就构成了一条语法规则。
不过，编译原理的教科书里，有时会用产生式的格式。EBNF格式和产生式是等价的，区别是产生式不允许使用？、*和+号，而是用递归来表示多个元素的重复，用ε来表示不生成任何字符串。如果我们把上述语法规则用产生式来表示的话，相当于下面四条：
prog -&amp;gt; statement prog prog -&amp;gt; ε statement -&amp;gt; functionDecl statement -&amp;gt; functionCall 你马上就能看出来，还是EBNF格式更简洁吧？</description></item><item><title>02_如何抓住重点，系统高效地学习数据结构与算法？</title><link>https://artisanbox.github.io/2/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/3/</guid><description>你是否曾跟我一样，因为看不懂数据结构和算法，而一度怀疑是自己太笨？实际上，很多人在第一次接触这门课时，都会有这种感觉，觉得数据结构和算法很抽象，晦涩难懂，宛如天书。正是这个原因，让很多初学者对这门课望而却步。
我个人觉得，其实真正的原因是你没有找到好的学习方法，没有抓住学习的重点。实际上，数据结构和算法的东西并不多，常用的、基础的知识点更是屈指可数。只要掌握了正确的学习方法，学起来并没有看上去那么难，更不需要什么高智商、厚底子。
还记得大学里每次考前老师都要划重点吗？今天，我就给你划划我们这门课的重点，再告诉你一些我总结的学习小窍门。相信有了这些之后，你学起来就会有的放矢、事半功倍了。
什么是数据结构？什么是算法？大部分数据结构和算法教材，在开篇都会给这两个概念下一个明确的定义。但是，这些定义都很抽象，对理解这两个概念并没有实质性的帮助，反倒会让你陷入死抠定义的误区。毕竟，我们现在学习，并不是为了考试，所以，概念背得再牢，不会用也就没什么用。
虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。 下面我就从广义和狭义两个层面，来帮你理解数据结构与算法这两个概念。
从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。
图书馆储藏书籍你肯定见过吧？为了方便查找，图书管理员一般会将书籍分门别类进行“存储”。按照一定规律编号，就是书籍这种“数据”的存储结构。
那我们如何来查找一本书呢？有很多种办法，你当然可以一本一本地找，也可以先根据书籍类别的编号，是人文，还是科学、计算机，来定位书架，然后再依次查找。笼统地说，这些查找方法都是算法。
从狭义上讲，也就是我们专栏要讲的，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。这些都是前人智慧的结晶，我们可以直接拿来用。我们要讲的这些经典数据结构和算法，都是前人从很多实际操作场景中抽象出来的，经过非常多的求证和检验，可以高效地帮助我们解决很多实际的开发问题。
那数据结构和算法有什么关系呢？为什么大部分书都把这两个东西放到一块儿来讲呢？
这是因为，数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。 因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。
比如，因为数组具有随机访问的特点，常用的二分查找算法需要用数组来存储数据。但如果我们选择链表这种数据结构，二分查找算法就无法工作了，因为链表并不支持随机访问。
数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。
现在你对数据结构与算法是不是有了比较清晰的理解了呢？有了这些储备，下面我们来看看，究竟该怎么学数据结构与算法。
学习这个专栏需要什么基础？看到数据结构和算法里的“算法”两个字，很多人就会联想到“数学”，觉得算法会涉及到很多深奥的数学知识。那我数学基础不是很好，学起来会不会很吃力啊？
数据结构和算法课程确实会涉及一些数学方面的推理、证明，尤其是在分析某个算法的时间、空间复杂度的时候，但是这个你完全不需要担心。
这个专栏不会像《算法导论》那样，里面有非常复杂的数学证明和推理。我会由浅入深，从概念到应用，一点一点给你解释清楚。你只要有高中数学水平，就完全可以学习。
当然，我希望你最好有些编程基础，如果有项目经验就更好了。这样我给你讲数据结构和算法如何提高效率、如何节省存储空间，你就会有很直观的感受。因为，对于每个概念和实现过程，我都会从实际场景出发，不仅教你“是什么”，还会教你“为什么”，并且告诉你遇到同类型问题应该“怎么做”。
学习的重点在什么地方？提到数据结构和算法，很多人就很头疼，因为这里面的内容实在是太多了。这里，我就帮你梳理一下，应该先学什么，后学什么。你可以对照看看，你属于哪个阶段，然后有针对性地进行学习。
想要学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。
这个概念究竟有多重要呢？可以这么说，它几乎占了数据结构和算法这门课的半壁江山，是数据结构和算法学习的精髓。
数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无招胜有招！
所以，复杂度分析这个内容，我会用很大篇幅给你讲透。你也一定要花大力气来啃，必须要拿下，并且要搞得非常熟练。否则，后面的数据结构和算法也很难学好。
搞定复杂度分析，下面就要进入数据结构与算法的正文内容了。
为了让你对数据结构和算法能有个全面的认识，我画了一张图，里面几乎涵盖了所有数据结构和算法书籍中都会讲到的知识点。
（图谱内容较多，建议长按保存后浏览）
但是，作为初学者，或者一个非算法工程师来说，你并不需要掌握图里面的所有知识点。很多高级的数据结构与算法，比如二分图、最大流等，这些在我们平常的开发中很少会用到。所以，你暂时可以不用看。我还是那句话，咱们学习要学会找重点。如果不分重点地学习，眉毛胡子一把抓，学起来肯定会比较吃力。
所以，结合我自己的学习心得，还有这些年的面试、开发经验，我总结了20个最常用的、最基础数据结构与算法，不管是应付面试还是工作需要，只要集中精力逐一攻克这20个知识点就足够了。
这里面有10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树；10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。
掌握了这些基础的数据结构和算法，再学更加复杂的数据结构和算法，就会非常容易、非常快。
在学习数据结构和算法的过程中，你也要注意，不要只是死记硬背，不要为了学习而学习，而是要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”。对于每一种数据结构或算法，我都会从这几个方面进行详细讲解。只要你掌握了我每节课里讲的内容，就能在开发中灵活应用。
学习数据结构和算法的过程，是非常好的思维训练的过程，所以，千万不要被动地记忆，要多辩证地思考，多问为什么。如果你一直这么坚持做，你会发现，等你学完之后，写代码的时候就会不由自主地考虑到很多性能方面的事情，时间复杂度、空间复杂度非常高的垃圾代码出现的次数就会越来越少。你的编程内功就真正得到了修炼。
一些可以让你事半功倍的学习技巧前面我给你划了学习的重点，也讲了学习这门课需要具备的基础。作为一个过来人，现在我就给你分享一下，专栏学习的一些技巧。掌握了这些技巧，可以让你化被动为主动，学起来更加轻松，更加有动力！
1.边学边练，适度刷题“边学边练”这一招非常有用。建议你每周花1～2个小时的时间，集中把这周的三节内容涉及的数据结构和算法，全都自己写出来，用代码实现一遍。这样一定会比单纯地看或者听的效果要好很多！
有面试需求的同学，可能会问了，那我还要不要去刷题呢？
我个人的观点是可以“适度”刷题，但一定不要浪费太多时间在刷题上。我们学习的目的还是掌握，然后应用。除非你要面试Google、Facebook这样的公司，它们的算法题目非常非常难，必须大量刷题，才能在短期内提升应试正确率。如果是应对国内公司的技术面试，即便是BAT这样的公司，你只要彻底掌握这个专栏的内容，就足以应对。
2.多问、多思考、多互动学习最好的方法是，找到几个人一起学习，一块儿讨论切磋，有问题及时寻求老师答疑。 但是，离开大学之后，既没有同学也没有老师，这个条件就比较难具备了。
不过，这也就是咱们专栏学习的优势。专栏里有很多跟你一样的学习者。你可以多在留言区写下自己的疑问、思考和总结，也可以经常看看别人的留言，和他们进行互动。
除此之外，如果你有疑问，你可以随时在留言区给我留言，我只要有空就会及时回复你。你不要担心问的问题太小白。因为我初学的时候，也常常会被一些小白问题困扰。不懂一点都不丢人，只要你勇敢提出来，我们一起解决了就可以了。
我也会力争每节课都最大限度地给你讲透，帮你扫除知识盲点，而你要做的就是，避免一知半解，要想尽一切办法去搞懂我讲的所有内容。
3.打怪升级学习法学习的过程中，我们碰到最大的问题就是，坚持不下来。 是的，很多基础课程学起来都非常枯燥。为此，我自己总结了一套“打怪升级学习法”。
游戏你肯定玩过吧？为什么很多看起来非常简单又没有乐趣的游戏，你会玩得不亦乐乎呢？这是因为，当你努力打到一定级别之后，每天看着自己的经验值、战斗力在慢慢提高，那种每天都在一点一点成长的成就感就不由自主地产生了。
所以，我们在枯燥的学习过程中，也可以给自己设立一个切实可行的目标，就像打怪升级一样。
比如，针对这个专栏，你就可以设立这样一个目标：每节课后的思考题都认真思考，并且回复到留言区。当你看到很多人给你点赞之后，你就会为了每次都能发一个漂亮的留言，而更加认真地学习。
当然，还有很多其他的目标，比如，每节课后都写一篇学习笔记或者学习心得；或者你还可以每节课都找一下我讲得不对、不合理的地方……诸如此类，你可以总结一个适合你的“打怪升级攻略”。
如果你能这样学习一段时间，不仅能收获到知识，你还会有意想不到的成就感。因为，这其实帮你改掉了一点学习的坏习惯。这个习惯一旦改掉了，你的人生也会变得不一样。
4.知识需要沉淀，不要想试图一下子掌握所有在学习的过程中，一定会碰到“拦路虎”。如果哪个知识点没有怎么学懂，不要着急，这是正常的。因为，想听一遍、看一遍就把所有知识掌握，这肯定是不可能的。学习知识的过程是反复迭代、不断沉淀的过程。
如果碰到“拦路虎”，你可以尽情地在留言区问我，也可以先沉淀一下，过几天再重新学一遍。所谓，书读百遍其义自见，我觉得是很有道理的！
我讲的这些学习方法，不仅仅针对咱们这一个课程的学习，其实完全适用任何知识的学习过程。你可以通过这个专栏的学习，实践一下这些方法。如果效果不错，再推广到之后的学习过程中。
内容小结今天，我带你划了划数据结构和算法的学习重点，复杂度分析，以及10个数据结构和10个算法。
这些内容是我根据平时的学习和工作、面试经验积累，精心筛选出来的。只要掌握这些内容，应付日常的面试、工作，基本不会有问题。
除此之外，我还给你分享了我总结的一些学习技巧，比如边学边练、多问、多思考，还有两个比较通用的学习方法，打怪升级法和沉淀法。掌握了这些学习技巧，可以让你学习过程中事半功倍。所以，你一定要好好实践哦！
课后思考今天的内容是一个准备课，从下节开始，我们就要正式开始学习精心筛选出的这20个数据结构和算法了。所以，今天给你布置一个任务，对照我上面讲的“打怪升级学习法”，请思考一下你自己学习这个专栏的方法，让我们一起在留言区立下Flag，相互鼓励！
另外，你在之前学习数据结构和算法的过程中，遇到过什么样的困难或者疑惑吗？
欢迎留言和我分享，我会第一时间给你反馈。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>02_日志系统：一条SQL更新语句是如何执行的？</title><link>https://artisanbox.github.io/1/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/2/</guid><description>前面我们系统了解了一个查询语句的执行流程，并介绍了执行过程中涉及的处理模块。相信你还记得，一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。
那么，一条更新语句的执行流程又是怎样的呢？
之前你可能经常听DBA同事说，MySQL可以恢复到半个月内任意一秒的状态，惊叹的同时，你是不是心中也会不免会好奇，这是怎样做到的呢？
我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键ID和一个整型字段c：
mysql&amp;gt; create table T(ID int primary key, c int); 如果要将ID=2这一行的值加1，SQL语句就会这么写：
mysql&amp;gt; update T set c=c+1 where ID=2; 前面我有跟你介绍过SQL语句基本的执行链路，这里我再把那张图拿过来，你也可以先简单看看这个图回顾下。首先，可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。
MySQL的逻辑架构图你执行语句前要先连接数据库，这是连接器的工作。
前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。
接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引。然后，执行器负责具体执行，找到这一行，然后更新。
与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。如果接触MySQL，那这两个词肯定是绕不过的，我后面的内容里也会不断地和你强调。不过话说回来，redo log和binlog在设计上有很多有意思的地方，这些设计思路也可以用到你自己的程序里。
重要的日志模块：redo log不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。
如果有人要赊账或者还账的话，掌柜一般有两种做法：
一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉； 另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算。 在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。
这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的帮助，每次记账都得翻账本，效率是不是低得让人难以忍受？
同样，在MySQL里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。为了解决这个问题，MySQL的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。
而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。
具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。
如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。
与此类似，InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。
write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。
write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。
有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。
要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。
重要的日志模块：binlog前面我们讲过，MySQL整体来看，其实就有两块：一块是Server层，它主要做的是MySQL功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。
我想你肯定会问，为什么会有两份日志呢？
因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。
这两种日志有以下三点不同。
redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。
redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</description></item><item><title>02_给你一张知识地图，计算机组成原理应该这么学</title><link>https://artisanbox.github.io/4/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/2/</guid><description>了解了现代计算机的基本硬件组成和背后最基本的冯·诺依曼体系结构，我们就可以正式进入计算机组成原理的学习了。在学习一个一个零散的知识点之前，我整理了一份学习地图，好让你对将要学习的内容有一个总纲层面的了解。
从这张图可以看出来，整个计算机组成原理，就是围绕着计算机是如何组织运作展开的。
计算机组成原理知识地图计算机组成原理的英文叫Computer Organization。这里的Organization是“组织机构”的意思。计算机由很多个不同的部件放在一起，变成了一个“组织机构”。这个组织机构最终能够进行各种计算、控制、读取输入，进行输出，达成各种强大的功能。
在这张图里面，我们把整个计算机组成原理的知识点拆分成了四大部分，分别是计算机的基本组成、计算机的指令和计算、处理器设计，以及存储器和I/O设备。
首先，我们来看计算机的基本组成。
这一部分，你需要学习计算机是由哪些硬件组成的。这些硬件，又是怎么对应到经典的冯·诺依曼体系结构中的，也就是运算器、控制器、存储器、输入设备和输出设备这五大基本组件。除此之外，你还需要了解计算机的两个核心指标，性能和功耗。性能和功耗也是我们在应用和设计五大基本组件中需要重点考虑的因素。
了解了组成部分，接下来你需要掌握计算机的指令和计算。
在计算机指令部分，你需要搞明白，我们每天撰写的一行行C、Java、PHP程序，是怎么在计算机里面跑起来的。这里面，你既需要了解我们的程序是怎么通过编译器和汇编器，变成一条条机器指令这样的编译过程（如果把编译过程展开的话，可以变成一门完整的编译原理课程），还需要知道我们的操作系统是怎么链接、装载、执行这些程序的（这部分知识如果再深入学习，又可以变成一门操作系统课程）。而这一条条指令执行的控制过程，就是由计算机五大组件之一的控制器来控制的。
在计算机的计算部分，你要从二进制和编码开始，理解我们的数据在计算机里的表示，以及我们是怎么从数字电路层面，实现加法、乘法这些基本的运算功能的。实现这些运算功能的ALU（Arithmetic Logic Unit/ALU），也就是算术逻辑单元，其实就是我们计算机五大组件之一的运算器。
这里面有一个在今天看起来特别重要的知识点，就是浮点数（Floating Point）。浮点数是我们在日常运用中非常容易用错的一种数据表示形式。掌握浮点数能让你对数据的编码、存储和计算能够有一个从表到里的深入理解。尤其在AI火热的今天，浮点数是机器学习中重度使用的数据表示形式，掌握它更是非常有必要。
明白计算机指令和计算是如何运转的，我们就可以深入到CPU的设计中去一探究竟了。
CPU时钟可以用来构造寄存器和内存的锁存器和触发器，因此，CPU时钟应该是我们学习CPU的前导知识。搞明白我们为什么需要CPU时钟（CPU Clock），以及寄存器和内存是用什么样的硬件组成的之后，我们可以再来看看，整个计算机的数据通路是如何构造出来的。
数据通路，其实就是连接了整个运算器和控制器，并最终组成了CPU。而出于对于性能和功耗的考虑，你要进一步理解和掌握面向流水线设计的CPU、数据和控制冒险，以及分支预测的相关技术。
既然CPU作为控制器要和输入输出设备通信，那么我们就要知道异常和中断发生的机制。在CPU设计部分的最后，我会讲一讲指令的并行执行，看看如何直接在CPU层面，通过SIMD来支持并行计算。
最后，我们需要看一看，计算机五大组成部分之一，存储器的原理。通过存储器的层次结构作为基础的框架引导，你需要掌握从上到下的CPU高速缓存、内存、SSD硬盘和机械硬盘的工作原理，它们之间的性能差异，以及实际应用中利用这些设备会遇到的挑战。存储器其实很多时候又扮演了输入输出设备的角色，所以你需要进一步了解，CPU和这些存储器之间是如何进行通信的，以及我们最重视的性能问题是怎么一回事；理解什么是IO_WAIT，如何通过DMA来提升程序性能。
对于存储器，我们不仅需要它们能够正常工作，还要确保里面的数据不能丢失。于是你要掌握我们是如何通过RAID、Erasure Code、ECC以及分布式HDFS，这些不同的技术，来确保数据的完整性和访问性能。
学习计算机组成原理，究竟有没有好办法？相信这个学习地图，应该让你对计算机组成这门课要学些什么，有了一些了解。不过这个地图上的知识点繁多，应该也给你带来了不小的挑战。
我上一节也说过，相较于整个计算机科学中的其他科目，计算机组成原理更像是整个计算机学科里的“纲要”。这门课里任何一个知识点深入挖下去，都可以变成计算机科学里的一门核心课程。
比如说，程序怎样从高级代码变成指令在计算机里面运行，对应着“编译原理”和“操作系统”这两门课程；计算实现背后则是“数字电路”；如果要深入CPU和存储器系统的优化，必然要深入了解“计算机体系结构”。
因此，为了帮你更快更好地学计算机组成，我为你总结了三个学习方法，帮你更好地掌握这些知识点，并且能够学为所用，让你在工作中能够用得上。
首先，学会提问自己来串联知识点。学完一个知识点之后，你可以从下面两个方面，问一下自己。
我写的程序，是怎样从输入的代码，变成运行的程序，并得到最终结果的？
整个过程中，计算器层面到底经历了哪些步骤，有哪些地方是可以优化的？
无论是程序的编译、链接、装载和执行，以及计算时需要用到的逻辑电路、ALU，乃至CPU自发为你做的流水线、指令级并行和分支预测，还有对应访问到的硬盘、内存，以及加载到高速缓存中的数据，这些都对应着我们学习中的一个个知识点。建议你自己脑子里过一遍，最好是口头表述一遍或者写下来，这样对你彻底掌握这些知识点都会非常有帮助。
其次，写一些示例程序来验证知识点。计算机科学是一门实践的学科。计算机组成中的大量原理和设计，都对应着“性能”这个词。因此，通过把对应的知识点，变成一个个性能对比的示例代码程序记录下来，是把这些知识点融汇贯通的好方法。因为，相比于强记硬背知识点，一个有着明确性能对比的示例程序，会在你脑海里留下更深刻的印象。当你想要回顾这些知识点的时候，一个程序也更容易提示你把它从脑海深处里面找出来。
最后，通过和计算机硬件发展的历史做对照。计算机的发展并不是一蹴而就的。从第一台电子计算机ENIAC（Electronic Numerical Integrator And Computer，电子数值积分计算机）的发明到现在，已经有70多年了。现代计算机用的各个技术，都是跟随实际应用中遇到的挑战，一个个发明、打磨，最后保留下来的。这当中不仅仅有学术层面的碰撞，更有大量商业层面的交锋。通过了解充满戏剧性和故事性的计算机硬件发展史，让你更容易理解计算机组成中各种原理的由来。
比如说，奔腾4和SPARC的失败，以及ARM的成功，能让我们记住CPU指令集的繁与简、权衡性能和功耗的重要性，而现今高速发展的机器学习和边缘计算，又给计算机硬件设计带来了新的挑战。
给松鼠症患者的学习资料学习总是要花点笨功夫的。最有效的办法还是“读书百遍，其义自见”。对于不够明白的知识点，多搜索，多看不同来源的资料，多和朋友、同事、老师一起交流，一定能够帮你掌握好想要学习的知识点。
在这个专栏之前，计算机组成原理，已经有很多优秀的图书和课程珠玉在前了。为了覆盖更多知识点的细节，这些书通常都有点厚，课程都会有点长。不过作为专栏的补充阅读材料，却是最合适不过了。
因此，每一讲里，我都会留下一些“补充阅读”的材料。如果你想更进一步理解更多深入的计算机组成原理的知识，乃至更多相关的其他核心课程的知识，多用一些业余时间来看一看，读一读这些“补充阅读”也一定不会让你对花在上面的时间后悔的。
下面给你推荐一些我自己看过、读过的内容。我在之后的文章里推荐的“补充阅读”，大部分都是来自这些资料。你可以根据自己的情况来选择学习。
入门书籍我知道，订阅这个专栏的同学，有很多是非计算机科班出身，我建议你先对计算机组成原理这门课有个基本概念。建立这个概念，有两种方法，第一，你可以把我上面那张地图的核心内容记下来，对这些内容之间的关系先有个大致的了解。
第二，我推荐你阅读两本书，准确地说，这其实是两本小册子，因为它们非常轻薄、好读，而且图文并茂，非常适合初学者和想要入门组成原理的同学。一本是《计算机是怎样跑起来的》，另一本是《程序是怎样跑起来的》。我要特别说一下后面这本，它可以说是一个入门微缩版本的“计算机组成原理”。
除此之外，计算机组成中，硬件层面的基础实现，比如寄存器、ALU这些电路是怎么回事，你可以去看一看Coursera上的北京大学免费公开课《Computer Organization》。这个视频课程的视频部分也就10多个小时。在学习专栏相应章节的前后去浏览一遍，相信对你了解程序在电路层面会变成什么样子有所帮助。
深入学习书籍对于想要深入掌握计算机组成的同学，我推荐你去读一读《计算机组成与设计：硬件/软件接口》和经典的《深入理解计算机系统》这两本书。后面这本被称为CSAPP的经典教材，网上也有配套的视频课程。我在这里给你推荐两个不同版本的链接（Bilibili版和Youtube版 ）。不过这两本都在500页以上，坚持啃下来需要不少实践经验。
计算机组成原理还有一本的经典教材，就是来自操作系统大神塔能鲍姆（Andrew S. Tanenbaum）的《计算机组成：结构化方法》。这本书的组织结构和其他教材都不太一样，适合作为一个辅助的参考书来使用。
如果在学习这个专栏的过程中，引发了你对于计算机体系结构的兴趣，你还可以深入读一读《计算机体系结构：量化研究方法》。
课外阅读在上面这些教材之外，对于资深程序员来说，来自Redhat的What Every Programmer Should Know About Memory是写出高性能程序不可不读的经典材料。而LMAX开源的Disruptor，则是通过实际应用程序，来理解计算机组成原理中各个知识点的最好范例了。
《编码：隐匿在计算机软硬件背后的语言》和《程序员的自我修养：链接、装载和库》是理解计算机硬件和操作系统层面代码执行的优秀阅读材料。
总结延伸学习不是死记硬背，学习材料也不是越多越好。到了这里，希望你不要因为我给出了太多可以学习的材料，结果成了“松鼠症”患者，光囤积材料，却没有花足够多的时间去学习这些知识。
我工作之后一直在持续学习，在这个过程中，我发现最有效的办法，不是短时间冲刺，而是有节奏地坚持，希望你能够和专栏的发布节奏同步推进，做好思考题，并且多在留言区和其他朋友一起交流，就更容易能够“积小步而至千里”，在程序员这个职业上有更长足的发展。
好了，对于学习资料的介绍就到这里了。希望在接下来的几个月里，你能和我一起走完这趟“计算机组成”之旅，从中收获到知识和成长。
课后思考今天我为你梳理计算机组成的知识地图，也讲了我认为学习这个专栏的一些方法，听了这么多，那么你打算怎么学习这个专栏呢？
欢迎你在留言区写下你的学习目标和学习计划，和大家一起交流，也欢迎你把今天的文章分享给你的朋友，互相督促，共同成长。</description></item><item><title>02｜词法分析：识别Token也可以很简单吗？</title><link>https://artisanbox.github.io/3/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/4/</guid><description>你好，我是宫文学。
上一节课，我们用了很简单的方法就实现了语法分析。但当时，我们省略了词法分析的任务，使用了一个Token的列表作为语法分析阶段的输入，而这个Token列表呢，就是词法分析的结果。
其实，编译器的第一项工作就是词法分析，也是你实现一门计算机语言的头一项基本功。今天，我们就来补补课，学习一下怎么实现词法分析功能，词法分析也就是把程序从字符串转换成Token串的过程。
词法分析难不难呢？我们来对比一下，语法分析的结果是一棵AST树，而词法分析的结果是一个列表。直观上看，列表就要比树结构简单一些，所以你大概会猜想到，词法分析应该会更简单一些。
那么，具体来说，词法分析要用什么算法呢？词法是不是也像语法一样有规则？词法规则又是如何表达的？这一节课，我会带着你实现一个词法分析器，来帮你掌握这些技能。
在这里，我有个好消息告诉你。你在上一节课学到的语法分析的技能，很多可以用在词法分析中，这会大大降低你的学习难度。好了，我们开始了。
词法分析的任务你已经知道，词法分析的任务就是把程序从字符串转变成Token串，那它该怎么实现呢？我们这里先不讲具体的算法，先来看看下面这张示意图，分析一下，我们人类的大脑是如何把这个字符串断成一个个Token的？
你可能首先会想到，借助字符串中的空白字符（包括空格、回车、换行），把这个字符串截成一段段的，每一段作为一个Token，行不行？
按照这个方法，function关键字可以被单独识别出来。但是你看，我们还有一些圆括号、花括号等等，这些符号跟前一个单词之间并没有空格或回车，我们怎么把它们断开呢？
OK，你可以说，凡是遇到圆括号、花括号、加号、减号、点号等这些符号，我们把它们单独作为Token识别出来就好了。比如，对于cat.weight这样的对象属性访问的场景，点符号就是一个单独的Token。
但是，你马上会发现这个规则仍然不能处理所有的情况，例如，对于一个浮点数的字面量“3.14”的情况，点符号是浮点数的一部分，不能作为单独的Token。我稍微解释一下，这里的字面量（Literal），是指写在程序中的常量值，包括整数值、浮点数值、字符串等。
此外，还有一些难处理的情况，比如像“==”、“+=”、“-=”、“- -”、“&amp;amp;&amp;amp;”这些由两个或两个以上字符构成的运算符，程序处理时是要跟“=”、“+”、“-”等区分开的。
再比如，在JavaScript/TypeScript中，十六进制的字面量用“0x”开头，里面有a到f的字母，比如0x1F4；八进制的字面量用“0”开头，后面跟0~7的数字；而二进制的字面量用“0b”开头，后面跟着0或1。
所以，你可以看到，做词法分析需要考虑的情况还挺多，不是用简单的一两个规则就能解决的，我们必须寻找一种系统性的解决方法。
在这里，为了让你对词法分析的任务有更全面的了解，我梳理了各种不同的处理工作，你可以看看下面这张表：
那么，如何用系统性的方法进行词法分析呢？
借助这节课一开头的提示，我们试一下能否用语法分析的方法来处理词法，也就是说像做语法分析一样，我们要先用一个规则来描述每个Token的词法，然后让程序基于词法规则来做处理。
词法规则同语法规则一样，我们可以用正则表达式来描述词法。在这里，标识符的规则是用字母开头，后面的字符可以是字母、数字或下划线，标识符的词法规则可以写成下面这样：
Identifier: [a-zA-Z_][a-zA-Z0-9_]* ; 实际上，JavaScript的标识符是允许使用合法的Unicode字符的，我们这里做了简化。
看上去，词法规则跟上一节学过的语法规则没什么不同嘛，只不过词法的构成要素是字符，而语法的构成要素是Token。
表面上是这样，其实这里还是有一点不同的。实际上，词法规则使用的是正则文法（Formal Grammar），而语法规则使用的是上下文无关文法（Context-free Gammar，CFG）。正则文法是上下文无关文法的一个子集，至于对这两者差别的深入分析，我们还是放到后面的课上，这里我们先专注于完成词法分析功能。
我们再写一下前面讨论过的浮点数字面量的词法规则，这里同样是精简的版本，省略了指数部分、二进制、八进制以及十六进制的内容：
DecimalLiteral: IntegerLiteral '.' [0-9]* | '.' [0-9]+ | IntegerLiteral ; IntegerLiteral: '0' | [1-9] [0-9]* ; 对于上面这个DecimalLiteral词法规则，我们总结一下有这几个特点：
一个合法的浮点数可以是好几种格式，3.14、.14、3等等都行； 整数部分要么只有一个0，要么是1~9开头的数字； 可以没有小数点前的整数部分，但这时候小数点后至少要有一位数字，否则这就只剩了一个点号了； 也可以完全没有小数部分，只有整数部分。 好了，目前我们已经知道如何用词法规则来描述不同的Token了。接下来，我们要做的就是用程序实现这些词法规则，然后生成Token。
用程序实现词法分析上节课我们在讲语法分析的时候，提到了递归下降算法。这个算法比较让人喜欢的一点是，程序结构基本上就是对语法规则的一对一翻译。
其实词法分析程序也是一样的，比如我们要识别一个浮点数，我们照样可以根据上述DecimalLiteral的几条规则一条条地匹配过去。
首先，我们匹配第一条规则，就是既有整数部分又有小数部分的情况；如果匹配不上，就尝试第二条规则，也就是以小数点开头的情况；如果还匹配不上，就尝试第三条，即只有整数部分的情况。只要这三条匹配里有一条成功，就意味着我们匹配浮点数成功。
我们来看看具体的程序实现：
if (this.isDigit(ch)){ this.stream.next(); let ch1 = this.stream.peek(); let literal:string = ''; //首先解析整数部分 if(ch == '0'){//暂不支持八进制、二进制、十六进制 if (!</description></item><item><title>03_事务隔离：为什么你改了我还看不见？</title><link>https://artisanbox.github.io/1/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/3/</guid><description>提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转账，你要给朋友小王转100块钱，而此时你的银行卡只有100块钱。
转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这100块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。
简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事务支持是在引擎层实现的。你现在知道，MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如MySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代的重要原因之一。
今天的文章里，我将会以InnoDB为例，剖析MySQL在事务支持方面的特定实现，并基于原理给出相应的实践建议，希望这些案例能加深你对MySQL事务原理的理解。
隔离性与隔离级别提到事务，你肯定会想到ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中I，也就是“隔离性”。
当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。
在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面我逐一为你解释：
读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表T中只有一列，其中一行的值为1，下面是按照时间顺序执行两个事务的行为。
mysql&amp;gt; create table T(c int) engine=InnoDB; insert into T(c) values(1); 我们来看看在不同的隔离级别下，事务A会有哪些不同的返回结果，也就是图里面V1、V2、V3的返回值分别是什么。
若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。
我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致，你一定要记得将MySQL的隔离级别设置为“读提交”。
配置的方式是，将启动参数transaction-isolation的值设置成READ-COMMITTED。你可以用show variables来查看当前的值。
mysql&amp;gt; show variables like 'transaction_isolation'; +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
| Variable_name | Value |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
| transaction_isolation | READ-COMMITTED |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+ 总结来说，存在即合理，每种隔离级别都有自己的使用场景，你要根据自己的业务情况来定。我想你可能会问那什么时候需要“可重复读”的场景呢？我们来看一个数据校对逻辑的案例。
假设你在管理一个个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。
这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。
事务隔离的实现理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复读”。
在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。
假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。
当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-view A，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。</description></item><item><title>03_复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？</title><link>https://artisanbox.github.io/2/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/4/</guid><description>我们都知道，数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行得更快，如何让代码更省存储空间。所以，执行效率是算法一个非常重要的考量指标。那如何来衡量你编写的算法代码的执行效率呢？这里就要用到我们今天要讲的内容：时间、空间复杂度分析。
其实，只要讲到数据结构与算法，就一定离不开时间、空间复杂度分析。而且，我个人认为，复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。
复杂度分析实在太重要了，因此我准备用两节内容来讲。希望你学完这个内容之后，无论在任何场景下，面对任何代码的复杂度分析，你都能做到“庖丁解牛”般游刃有余。
为什么需要复杂度分析？你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？
首先，我可以肯定地说，你这种评估算法执行效率的方法是正确的。很多数据结构和算法书籍还给这种方法起了一个名字，叫事后统计法。但是，这种统计方法有非常大的局限性。
1. 测试结果非常依赖测试环境
测试环境中硬件的不同会对测试结果有很大的影响。比如，我们拿同样一段代码，分别用Intel Core i9处理器和Intel Core i3处理器来运行，不用说，i9处理器要比i3处理器执行的速度快很多。还有，比如原本在这台机器上a代码执行的速度比b代码要快，等我们换到另一台机器上时，可能会有截然相反的结果。
2.测试结果受数据规模的影响很大
后面我们会讲排序算法，我们先拿它举个例子。对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别。极端情况下，如果数据已经是有序的，那排序算法不需要做任何操作，执行时间就会非常短。除此之外，如果测试数据规模太小，测试结果可能无法真实地反映算法的性能。比如，对于小规模的数据排序，插入排序可能反倒会比快速排序要快！
所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是我们今天要讲的时间、空间复杂度分析方法。
大O复杂度表示法算法的执行效率，粗略地讲，就是算法代码执行的时间。但是，如何在不运行代码的情况下，用“肉眼”得到一段代码的执行时间呢？
这里有段非常简单的代码，求1,2,3...n的累加和。现在，我就带你一块来估算一下这段代码的执行时间。
int cal(int n) { int sum = 0; int i = 1; for (; i &amp;lt;= n; ++i) { sum = sum + i; } return sum; } 从CPU的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的CPU执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？
第2、3行代码分别需要1个unit_time的执行时间，第4、5行都运行了n遍，所以需要2n*unit_time的执行时间，所以这段代码总的执行时间就是(2n+2)*unit_time。可以看出来，所有代码的执行时间T(n)与每行代码的执行次数成正比。
按照这个分析思路，我们再来看这段代码。
int cal(int n) { int sum = 0; int i = 1; int j = 1; for (; i &amp;lt;= n; ++i) { j = 1; for (; j &amp;lt;= n; ++j) { sum = sum + i * j; } } } 我们依旧假设每个语句的执行时间是unit_time。那这段代码的总执行时间T(n)是多少呢？</description></item><item><title>03_通过你的CPU主频，我们来谈谈“性能”究竟是什么？</title><link>https://artisanbox.github.io/4/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/3/</guid><description>“性能”这个词，不管是在日常生活还是写程序的时候，都经常被提到。比方说，买新电脑的时候，我们会说“原来的电脑性能跟不上了”；写程序的时候，我们会说，“这个程序性能需要优化一下”。那么，你有没有想过，我们常常挂在嘴边的“性能”到底指的是什么呢？我们能不能给性能下一个明确的定义，然后来进行准确的比较呢？
在计算机组成原理乃至体系结构中，“性能”都是最重要的一个主题。我在前面说过，学习和研究计算机组成原理，就是在理解计算机是怎么运作的，以及为什么要这么运作。“为什么”所要解决的事情，很多时候就是提升“性能”。
什么是性能？时间的倒数计算机的性能，其实和我们干体力劳动很像，好比是我们要搬东西。对于计算机的性能，我们需要有个标准来衡量。这个标准中主要有两个指标。
第一个是响应时间（Response time）或者叫执行时间（Execution time）。想要提升响应时间这个性能指标，你可以理解为让计算机“跑得更快”。
第二个是吞吐率（Throughput）或者带宽（Bandwidth），想要提升这个指标，你可以理解为让计算机“搬得更多”。
服务器使用的网络带宽，通常就是一个吞吐率性能指标所以说，响应时间指的就是，我们执行一个程序，到底需要花多少时间。花的时间越少，自然性能就越好。
而吞吐率是指我们在一定的时间范围内，到底能处理多少事情。这里的“事情”，在计算机里就是处理的数据或者执行的程序指令。
和搬东西来做对比，如果我们的响应时间短，跑得快，我们可以来回多跑几趟多搬几趟。所以说，缩短程序的响应时间，一般来说都会提升吞吐率。
除了缩短响应时间，我们还有别的方法吗？当然有，比如说，我们还可以多找几个人一起来搬，这就类似现代的服务器都是8核、16核的。人多力量大，同时处理数据，在单位时间内就可以处理更多数据，吞吐率自然也就上去了。
提升吞吐率的办法有很多。大部分时候，我们只要多加一些机器，多堆一些硬件就好了。但是响应时间的提升却没有那么容易，因为CPU的性能提升其实在10年前就处于“挤牙膏”的状态了，所以我们得慎重地来分析对待。下面我们具体来看。
我们一般把性能，定义成响应时间的倒数，也就是：
性能 = 1/响应时间这样一来，响应时间越短，性能的数值就越大。同样一个程序，在Intel最新的CPU Coffee Lake上，只需要30s就能运行完成，而在5年前CPU Sandy Bridge上，需要1min才能完成。那么我们自然可以算出来，Coffee Lake的性能是1/30，Sandy Bridge的性能是1/60，两个的性能比为2。于是，我们就可以说，Coffee Lake的性能是Sandy Bridge的2倍。
过去几年流行的手机跑分软件，就是把多个预设好的程序在手机上运行，然后根据运行需要的时间，算出一个分数来给出手机的性能评估。而在业界，各大CPU和服务器厂商组织了一个叫作SPEC（Standard Performance Evaluation Corporation）的第三方机构，专门用来指定各种“跑分”的规则。
SPEC提供的CPU基准测试程序，就好像CPU届的“高考”，通过数十个不同的计算程序，对于CPU的性能给出一个最终评分。这些程序丰富多彩，有编译器、解释器、视频压缩、人工智能国际象棋等等，涵盖了方方面面的应用场景。感兴趣的话，你可以点击这个链接看看。
计算机的计时单位：CPU时钟虽然时间是一个很自然的用来衡量性能的指标，但是用时间来衡量时，有两个问题。
第一个就是时间不“准”。如果用你自己随便写的一个程序，来统计程序运行的时间，每一次统计结果不会完全一样。有可能这一次花了45ms，下一次变成了53ms。
为什么会不准呢？这里面有好几个原因。首先，我们统计时间是用类似于“掐秒表”一样，记录程序运行结束的时间减去程序开始运行的时间。这个时间也叫Wall Clock Time或者Elapsed Time，就是在运行程序期间，挂在墙上的钟走掉的时间。
但是，计算机可能同时运行着好多个程序，CPU实际上不停地在各个程序之间进行切换。在这些走掉的时间里面，很可能CPU切换去运行别的程序了。而且，有些程序在运行的时候，可能要从网络、硬盘去读取数据，要等网络和硬盘把数据读出来，给到内存和CPU。所以说，要想准确统计某个程序运行时间，进而去比较两个程序的实际性能，我们得把这些时间给刨除掉。
那这件事怎么实现呢？Linux下有一个叫time的命令，可以帮我们统计出来，同样的Wall Clock Time下，程序实际在CPU上到底花了多少时间。
我们简单运行一下time命令。它会返回三个值，第一个是real time，也就是我们说的Wall Clock Time，也就是运行程序整个过程中流逝掉的时间；第二个是user time，也就是CPU在运行你的程序，在用户态运行指令的时间；第三个是sys time，是CPU在运行你的程序，在操作系统内核里运行指令的时间。而程序实际花费的CPU执行时间（CPU Time），就是user time加上sys time。
$ time seq 1000000 | wc -l 1000000 real 0m0.101s user 0m0.031s sys 0m0.016s 在我给的这个例子里，你可以看到，实际上程序用了0.101s，但是CPU time只有0.031+0.016 = 0.047s。运行程序的时间里，只有不到一半是实际花在这个程序上的。
备注：你最好在云平台上，找一台1 CPU的机器来跑这个命令，在多CPU的机器上，seq和wc两个命令可能分配到不同的CPU上，我们拿到的user time和sys time是两个CPU上花费的时间之和，可能会导致real time可能会小于user time+sys time。</description></item><item><title>03｜支持表达式：解析表达式和解析语句有什么不同？</title><link>https://artisanbox.github.io/3/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/5/</guid><description>你好，我是宫文学。
到目前为止，我们已经学习了一些语法分析的算法。不过，我们主要是分析了如何来解析语句，比如函数声明、函数调用，没有把重点放在解析表达式上。
其实我是刻意为之的，故意把表达式的解析往后推迟一下。原因是表达式解析，特别是像“2+3*5”这样的看似特别简单的二元运算的表达式解析，涉及的语法分析技术反而是比较复杂的。所以，从循序渐进的角度来说，我们要把它们放在后面。
表达式的解析复杂在哪里呢？是这样，我们在解析二元表达式的时候，会遇到递归下降算法最大的短板，也就是不支持左递归的文法。如果遇到左递归的文法，会出现无限循环的情况。
在这一节里，我会给你分析这种左递归的困境，借此加深你对递归下降算法运算过程的理解。
同时，我也要给出避免左递归问题的方法。这里，我没有采用教科书上经常推荐的改写文法的方法，而是使用了业界实际编译器中更常用的算法：运算符优先级解析器（Operator-precedence parser）。JDK的Java编译器、V8的JaveScript编译器和Go语言的GC编译器，都毫无例外地采用了这个算法，所以这个算法非常值得我们掌握。
好了，那我们首先来了解一下用递归下降算法解析算术表达式会出现的这个左递归问题。
左递归问题我们先给出一种简化的加法表达式的语法规则：
add : add '+' IntLiteral | IntLiteral ; 对这个规则的解读是这样的：一个加法表达式，它要么是一个整型字面量，要么是另一个加法表达式再加上一个整型字面量。在这个规则下，2、2+3、2+3+4都是合格的加法表达式。
那如果用递归下降算法去解析2+3，我们会采用“add ‘+’ IntLiteral”的规则。而这个规则呢，又要求匹配出一个add来，从而算法又会递归地再次调用“add ‘+’ IntLiteral”规则，导致无限递归下去。
2+3是不是一个add表达式？ -&amp;gt;先匹配出一个add表达式来，再是+号，再是整型字面量 -&amp;gt;先匹配出一个add表达式来，再是+号，再是整型字面量 -&amp;gt;先匹配出一个add表达式来，再是+号，再是整型字面量 -&amp;gt;无限递归... 这就是著名的左递归问题，是递归下降算法或者LL算法都无法解决的。
你可能会问，如果把产生式的写法换一下，把add放在后面，不就会避免左递归了吗？
add : IntLiteral '+' add | IntLiteral ; 这个也是不行的，因为这样会导致运算的结合性出错。如果执意按照这个语法解析，解析2+3+4这个表达式所形成的AST会是右结合的：
你会看到，基于使用右递归文法生成的AST，实际是先计算3+4，再跟2相加。这违背了加法运算的结合性的规定。正确的运算顺序，应该是先计算2+3，然后再加上4，是左结合，对应的AST应该是右边的那个。这种结合性的错误，看上去对于加法影响不大，但如果换成减法或者除法，那计算结果就完全错误了。
好了，现在你已经理解了左递归问题了。那我们要如何解决这个问题呢？一个可行的解决方法就是改写文法，并且要在解析算法上做一些特殊的处理，你可以参考《编译原理之美》课程04讲。除了改写文法的方法以外，还有一些研究者提出了其他一些算法，也能解决左递归问题。
不过，针对二元表达式的解析，今天我要采用的是被实际编译器广泛采用的运算符优先级算法。
运算符优先级算法这是个怎么样的算法呢？我想先用简单的方式帮你理解运算符优先级算法的原理，然后再一步步深化。
在01讲介绍递归下降算法的时候，我提到，它对应的是人类的一种思维方式，也就是从顶向下逐步分解。但人类还有另一种思维方式：自底向上逐步归纳。而运算符优先级算法，对应的就是自底向上的一种思维方式。
首先我们来看2+3+4这个表达式，如果我们用自底向上归纳的思路做语法分析是怎么样一个思考过程呢？
第1步，首先看到2。你心里想，这里有一个整数了，那它是不是一个算术表达式的组成部分呀？是一个加法表达式的，还是乘法表达式的一部分呢？我们再往下看一看就知道。
第2步，看到一个+号。噢，你说，原来是一个加法表达式呀。这时候，我们知道，2肯定是要参与加法运算的，所以是加法的左子树。但加法后面可以跟很多东西的，比如另一个整数，或者是一个乘法表达式什么的，都有可能。那我们继续向下看。
第3步，看到整数3。奥，你心里想，原来是2+3呀。那我现在根据这三个Token是不是可以先凑出一棵AST的子树来呢？先等一等，我们现在还不知道3后面跟的是什么。
如果3后面跟的是+号或者-号，那没问题，3是先参与前面这个+号的计算，再把2+3的结果一起，去参与后面的计算的。
但如果3后面遇到的是 * 号呢？那么3就要先参与乘法运算，计算完的才参与前面的加法的。这两个不同的计算顺序，导致AST的结构是不一样的，而影响AST结构的，其实就是3前后的两个运算符的优先级。
第4步，看到第2个+号。这个时候，你心里知道了，原来3后面的运算符的优先级跟前面的是一样的呀，那么按照结合性的规定，应该先算前面的加法，再算后面的加法，所以3应该跟前面的2和+号一起凑成一个AST子树。并且，这棵子树会作为一个稳定的单元，参与后面的AST的构建。
第5步，看到整数4。现在的情况跟第3步是一样的，我们不知道4后面跟着的是什么。如果4后面跟着一个 * 号，那么4还要先参与后面的计算，然后再跟前面这一堆做加法。如果4后面也是一个加法运算符，那4就要先参与前面的计算，4在AST中的位置也就会变得确定。
第6步，再往下看，发现后面的Token既不是+号，也不是 * 号，而是EOF，也就是Token串的结尾。这样的话，整个AST就可以确定下来了。
好了，这是一个比较简单的算法运行的场景。你可以多读几遍，借此找找自底向上分析的直观感觉。
接下来，我们再换一个任务，分析一下2+3*5。你会发现，跟前一个例子相比，一直到第5步的时候，也就是读入了5以后，仍然没有形成一棵稳定的AST子树：
这是为什么呢？
因为根据5后面读入的Token的不同，形成的AST的结构会有很大的区别。这里我们展示3种情形：
情形1，第6个Token是+号：它的优先级不高于最后一个运算符 * 号，所以3 * 5这棵子树的结构就是确定的；进一步看，它也不高于第一个运算符的优先级，所以整个2+3 * 5这棵子树的结构都可以确定下来，并且肯定是最后一个+号的左子树。</description></item><item><title>04_复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度</title><link>https://artisanbox.github.io/2/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/5/</guid><description>上一节，我们讲了复杂度的大O表示法和几个分析技巧，还举了一些常见复杂度分析的例子，比如O(1)、O(logn)、O(n)、O(nlogn)复杂度分析。掌握了这些内容，对于复杂度分析这个知识点，你已经可以到及格线了。但是，我想你肯定不会满足于此。
今天我会继续给你讲四个复杂度分析方面的知识点，最好情况时间复杂度（best case time complexity）、最坏情况时间复杂度（worst case time complexity）、平均情况时间复杂度（average case time complexity）、均摊时间复杂度（amortized time complexity）。如果这几个概念你都能掌握，那对你来说，复杂度分析这部分内容就没什么大问题了。
最好、最坏情况时间复杂度上一节我举的分析复杂度的例子都很简单，今天我们来看一个稍微复杂的。你可以用我上节教你的分析技巧，自己先试着分析一下这段代码的时间复杂度。
// n表示数组array的长度 int find(int[] array, int n, int x) { int i = 0; int pos = -1; for (; i &amp;lt; n; ++i) { if (array[i] == x) pos = i; } return pos; } 你应该可以看出来，这段代码要实现的功能是，在一个无序的数组（array）中，查找变量x出现的位置。如果没有找到，就返回-1。按照上节课讲的分析方法，这段代码的复杂度是O(n)，其中，n代表数组的长度。
我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。但是，这段代码写得不够高效。我们可以这样优化一下这段查找代码。
// n表示数组array的长度 int find(int[] array, int n, int x) { int i = 0; int pos = -1; for (; i &amp;lt; n; ++i) { if (array[i] == x) { pos = i; break; } } return pos; } 这个时候，问题就来了。我们优化完之后，这段代码的时间复杂度还是O(n)吗？很显然，咱们上一节讲的分析方法，解决不了这个问题。</description></item><item><title>04_深入浅出索引（上）</title><link>https://artisanbox.github.io/1/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/4/</guid><description>提到数据库索引，我想你并不陌生，在日常工作中会经常接触到。比如某一个SQL查询比较慢，分析完原因之后，你可能就会说“给某个字段加个索引吧”之类的解决方案。但到底什么是索引，索引又是如何工作的呢？今天就让我们一起来聊聊这个话题吧。
数据库索引的内容比较多，我分成了上下两篇文章。索引是数据库系统里面最重要的概念之一，所以我希望你能够耐心看完。在后面的实战文章中，我也会经常引用这两篇文章中提到的知识点，加深你对数据库索引的理解。
一句话简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。一本500页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。
索引的常见模型索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了索引模型的概念。可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是哈希表、有序数组和搜索树。
下面我主要从使用的角度，为你简单分析一下这三种模型的区别。
哈希表是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的键即key，就可以找到其对应的值即Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。
不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。
假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示：
图1 哈希表示意图图中，User2和User4根据身份证号算出来的值都是N，但没关系，后面还跟了一个链表。假设，这时候你要查ID_card_n2对应的名字是什么，处理步骤就是：首先，将ID_card_n2通过哈希函数算出N；然后，按顺序遍历，找到User2。
需要注意的是，图中四个ID_card_n的值并不是递增的，这样做的好处是增加新的User时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。
你可以设想下，如果你现在要找身份证号在[ID_card_X, ID_card_Y]这个区间的所有用户，就必须全部扫描一遍了。
所以，哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。
而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示：
图2 有序数组示意图这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查ID_card_n2对应的名字，用二分法就可以快速得到，这个时间复杂度是O(log(N))。
同时很显然，这个索引结构支持范围查询。你要查身份证号在[ID_card_X, ID_card_Y]区间的User，可以先用二分法找到ID_card_X（如果不存在ID_card_X，就找到大于ID_card_X的第一个User），然后向右遍历，直到查到第一个大于ID_card_Y的身份证号，退出循环。
如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。
所以，有序数组索引只适用于静态存储引擎，比如你要保存的是2017年某个城市的所有人口信息，这类不会再修改的数据。
二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示：
图3 二叉搜索树示意图二叉搜索树的特点是：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。这样如果你要查ID_card_n2的话，按照图中的搜索顺序就是按照UserA -&amp;gt; UserC -&amp;gt; UserF -&amp;gt; User2这个路径得到。这个时间复杂度是O(log(N))。
当然为了维持O(log(N))的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是O(log(N))。
树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。
你可以想象一下一棵100万节点的平衡二叉树，树高20。一次查询可能需要访问20个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的。
为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小。
以InnoDB的一个整数字段索引为例，这个N差不多是1200。这棵树高是4的时候，就可以存1200的3次方个值，这已经17亿了。考虑到树根的数据块总是在内存中的，一个10亿行的表上一个整数字段的索引，查找一个值最多只需要访问3次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。
N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。
不管是哈希还是有序数组，或者N叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM树等数据结构也被用于引擎设计中，这里我就不再一一展开了。
你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。
截止到这里，我用了半篇文章的篇幅和你介绍了不同的数据结构，以及它们的适用场景，你可能会觉得有些枯燥。但是，我建议你还是要多花一些时间来理解这部分内容，毕竟这是数据库处理数据的核心概念之一，在分析问题的时候会经常用到。当你理解了索引的模型后，就会发现在分析问题的时候会有一个更清晰的视角，体会到引擎设计的精妙之处。
现在，我们一起进入相对偏实战的内容吧。
在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于InnoDB存储引擎在MySQL数据库中使用最为广泛，所以下面我就以InnoDB为例，和你分析一下其中的索引模型。
InnoDB 的索引模型在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。
每一个索引在InnoDB里面对应一棵B+树。
假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。
这个表的建表语句是：
mysql&amp;gt; create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 表中R1~R5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意图如下。
图4 InnoDB的索引组织结构从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。
主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。</description></item><item><title>04_穿越功耗墙，我们该从哪些方面提升“性能”？</title><link>https://artisanbox.github.io/4/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/4/</guid><description>上一讲，在讲CPU的性能时，我们提到了这样一个公式：
程序的CPU执行时间 = 指令数×CPI×Clock Cycle Time这么来看，如果要提升计算机的性能，我们可以从指令数、CPI以及CPU主频这三个地方入手。要搞定指令数或者CPI，乍一看都不太容易。于是，研发CPU的硬件工程师们，从80年代开始，就挑上了CPU这个“软柿子”。在CPU上多放一点晶体管，不断提升CPU的时钟频率，这样就能让CPU变得更快，程序的执行时间就会缩短。
于是，从1978年Intel发布的8086 CPU开始，计算机的主频从5MHz开始，不断提升。1980年代中期的80386能够跑到40MHz，1989年的486能够跑到100MHz，直到2000年的奔腾4处理器，主频已经到达了1.4GHz。而消费者也在这20年里养成了“看主频”买电脑的习惯。当时已经基本垄断了桌面CPU市场的Intel更是夸下了海口，表示奔腾4所使用的CPU结构可以做到10GHz，颇有一点“大力出奇迹”的意思。
功耗：CPU的“人体极限”然而，计算机科学界从来不相信“大力出奇迹”。奔腾4的CPU主频从来没有达到过10GHz，最终它的主频上限定格在3.8GHz。这还不是最糟的，更糟糕的事情是，大家发现，奔腾4的主频虽然高，但是它的实际性能却配不上同样的主频。想要用在笔记本上的奔腾4 2.4GHz处理器，其性能只和基于奔腾3架构的奔腾M 1.6GHz处理器差不多。
于是，这一次的“大力出悲剧”，不仅让Intel的对手AMD获得了喘息之机，更是代表着“主频时代”的终结。后面几代Intel CPU主频不但没有上升，反而下降了。到如今，2019年的最高配置Intel i9 CPU，主频也只不过是5GHz而已。相较于1978年到2000年，这20年里300倍的主频提升，从2000年到现在的这19年，CPU的主频大概提高了3倍。
奔腾4的主频为什么没能超过3.8GHz的障碍呢？答案就是功耗问题。什么是功耗问题呢？我们先看一个直观的例子。
一个3.8GHz的奔腾4处理器，满载功率是130瓦。这个130瓦是什么概念呢？机场允许带上飞机的充电宝的容量上限是100瓦时。如果我们把这个CPU安在手机里面，不考虑屏幕内存之类的耗电，这个CPU满载运行45分钟，充电宝里面就没电了。而iPhone X使用ARM架构的CPU，功率则只有4.5瓦左右。
我们的CPU，一般都被叫作超大规模集成电路（Very-Large-Scale Integration，VLSI）。这些电路，实际上都是一个个晶体管组合而成的。CPU在计算，其实就是让晶体管里面的“开关”不断地去“打开”和“关闭”，来组合完成各种运算和功能。
想要计算得快，一方面，我们要在CPU里，同样的面积里面，多放一些晶体管，也就是增加密度；另一方面，我们要让晶体管“打开”和“关闭”得更快一点，也就是提升主频。而这两者，都会增加功耗，带来耗电和散热的问题。
这么说可能还是有点抽象，我还是给你举一个例子。你可以把一个计算机CPU想象成一个巨大的工厂，里面有很多工人，相当于CPU上面的晶体管，互相之间协同工作。
为了工作得快一点，我们要在工厂里多塞一点人。你可能会问，为什么不把工厂造得大一点呢？这是因为，人和人之间如果离得远了，互相之间走过去需要花的时间就会变长，这也会导致性能下降。这就好像如果CPU的面积大，晶体管之间的距离变大，电信号传输的时间就会变长，运算速度自然就慢了。
除了多塞一点人，我们还希望每个人的动作都快一点，这样同样的时间里就可以多干一点活儿了。这就相当于提升CPU主频，但是动作快，每个人就要出汗散热。要是太热了，对工厂里面的人来说会中暑生病，对CPU来说就会崩溃出错。
我们会在CPU上面抹硅脂、装风扇，乃至用上水冷或者其他更好的散热设备，就好像在工厂里面装风扇、空调，发冷饮一样。但是同样的空间下，装上风扇空调能够带来的散热效果也是有极限的。
因此，在CPU里面，能够放下的晶体管数量和晶体管的“开关”频率也都是有限的。一个CPU的功率，可以用这样一个公式来表示：
功耗 ~= 1/2 ×负载电容×电压的平方×开关频率×晶体管数量那么，为了要提升性能，我们需要不断地增加晶体管数量。同样的面积下，我们想要多放一点晶体管，就要把晶体管造得小一点。这个就是平时我们所说的提升“制程”。从28nm到7nm，相当于晶体管本身变成了原来的1/4大小。这个就相当于我们在工厂里，同样的活儿，我们要找瘦小一点的工人，这样一个工厂里面就可以多一些人。我们还要提升主频，让开关的频率变快，也就是要找手脚更快的工人。
但是，功耗增加太多，就会导致CPU散热跟不上，这时，我们就需要降低电压。这里有一点非常关键，在整个功耗的公式里面，功耗和电压的平方是成正比的。这意味着电压下降到原来的1/5，整个的功耗会变成原来的1/25。
事实上，从5MHz主频的8086到5GHz主频的Intel i9，CPU的电压已经从5V左右下降到了1V左右。这也是为什么我们CPU的主频提升了1000倍，但是功耗只增长了40倍。比如说，我写这篇文章用的是Surface Go，在这样的轻薄笔记本上，微软就是选择了把电压下降到0.25V的低电压CPU，使得笔记本能有更长的续航时间。
并行优化，理解阿姆达尔定律虽然制程的优化和电压的下降，在过去的20年里，让我们的CPU性能有所提升。但是从上世纪九十年代到本世纪初，软件工程师们所用的“面向摩尔定律编程”的套路越来越用不下去了。“写程序不考虑性能，等明年CPU性能提升一倍，到时候性能自然就不成问题了”，这种想法已经不可行了。
于是，从奔腾4开始，Intel意识到通过提升主频比较“难”去实现性能提升，边开始推出Core Duo这样的多核CPU，通过提升“吞吐率”而不是“响应时间”，来达到目的。
提升响应时间，就好比提升你用的交通工具的速度，比如原本你是开汽车，现在变成了火车乃至飞机。本来开车从上海到北京要20个小时，换成飞机就只要2个小时了，但是，在此之上，再想要提升速度就不太容易了。我们的CPU在奔腾4的年代，就好比已经到了飞机这个速度极限。
那你可能要问了，接下来该怎么办呢？相比于给飞机提速，工程师们又想到了新的办法，可以一次同时开2架、4架乃至8架飞机，这就好像我们现在用的2核、4核，乃至8核的CPU。
虽然从上海到北京的时间没有变，但是一次飞8架飞机能够运的东西自然就变多了，也就是所谓的“吞吐率”变大了。所以，不管你有没有需要，现在CPU的性能就是提升了2倍乃至8倍、16倍。这也是一个最常见的提升性能的方式，通过并行提高性能。
这个思想在很多地方都可以使用。举个例子，我们做机器学习程序的时候，需要计算向量的点积，比如向量$W = [W_0, W_1, W_2, …, W_{15}]$和向量 $X = [X_0, X_1, X_2, …, X_{15}]$，$W·X = W_0 * X_0 + W_1 * X_1 +$ $W_2 * X_2 + … + W_{15} * X_{15}$。这些式子由16个乘法和1个连加组成。如果你自己一个人用笔来算的话，需要一步一步算16次乘法和15次加法。如果这个时候我们把这个任务分配给4个人，同时去算$W_0～W_3$, $W_4～W_7$, $W_8～W_{11}$, $W_{12}～W_{15}$这样四个部分的结果，再由一个人进行汇总，需要的时间就会缩短。</description></item><item><title>04｜如何让我们的语言支持变量和类型？</title><link>https://artisanbox.github.io/3/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/6/</guid><description>你好，我是宫文学。
到目前为止，我们的语言已经能够处理语句，也能够处理表达式，并且都能够解释执行了。不过，我们目前程序能够处理的数据，还都只是字面量而已。接下来，我们要增加一个重要的能力：支持变量。
在程序中声明变量、对变量赋值、基于变量进行计算，是计算机语言的基本功能。只有支持了变量，我们才能实现那些更加强大的功能，比如，你可以用程序写一个计算物体下落的速度和位置，如何随时间变化的公式。这里的时间就是变量，通过给时间变量赋予不同的值，我们可以知道任意时间的物体速度和位置。
这一节，我就带你让我们手头上的语言能够支持变量。在这个过程中，你还会掌握语义分析的更多技能，比如类型处理等。
好了，我们已经知道了这一节的任务。那么第一步要做什么呢？你可以想到，我们首先要了解与处理变量声明和初始化有关的语法规则。
与变量有关的语法分析功能在TypeScript中，我们在声明变量的时候，可以指定类型，这样有助于在编译期做类型检查：
let myAge : number = 18; 如果编译成JavaScript，那么类型信息就会被抹掉：
var myAge = 18; 不过，因为我们的目标是教给你做类型分析的方法，以后还要静态编译成二进制的机器码，所以我们选择的是TypeScript的语法。
此外，在上面的例子中，变量在声明的时候就已经做了初始化。你还可以把这个过程拆成两步。第一步的时候，只是声明变量，之后再给它赋值：
let myAge : number; myAge = 18; 知道了如何声明变量以后，你就可以试着写出相关的语法规则。我这里给出一个示范的版本：
variableDecl : 'let' Identifier typeAnnotation？ ('=' singleExpression)?; typeAnnotation : ':' typeName; 学完了前面3节课，我相信你现在应该对阅读语法规则越来越熟悉了。接下来，就要修改我们的语法分析程序，让它能够处理变量声明语句。这里有什么关键点呢？
这里你要注意的是，我们采用的语法分析的算法是LL算法。而在02讲中，我们知道LL算法的关键是计算First和Follow集合。
首先是First集合。在变量声明语句的上一级语法规则（也就是statement）中，要通过First集合中的不同元素，准确地确定应该采用哪一条语法规则。由于变量声明语句是用let开头的，这就使它非常容易辨别。只要预读的Token是let，那就按照变量声明的语法规则来做解析就对了。
接下来是Follow集合。在上面的语法规则中你能看出，变量的类型注解和初始化部分都是可选的，它们都使用了?号。
由于类型注解是可选的，那么解析器在处理了变量名称后，就要看一下后面的Token是什么。如果是冒号，由于冒号是在typeAnnotation的First集合中，那就去解析一个类型注解；如果这个Token不是冒号，而是typeAnnotation的Follow集合中的元素，就说明当前语句里没有typeAnnotation，所以可以直接略过。
那typeAnnotation的Follow集合有哪些元素呢？我就不直说了，你自己来分析一下吧。
再往后，由于变量的初始化部分也是可选的，还要计算一下它的Follow集合。你能看出，这个Follow集合只有;号这一个元素。所以，在解析到变量声明部分的时候，我们可以通过预读准确地判断接下来该采取什么动作：
如果预读的Token是=号，那就是继续做变量初始化部分的解析； 如果预读的Token是;号，那就证明该语句没有变量初始化部分，因此可以结束变量声明语句的解析了； 如果读到的是=号和;号之外的任何Token呢，那就触发语法错误了。 相关的实现很简单，你参考一下这个示例代码：
let t1 = this.scanner.peek(); //可选的类型标注 if (t1.text == ':'){ this.scanner.next(); t1 = this.scanner.peek(); if (t1.kind == TokenKind.Identifier){ this.scanner.next(); varType = t1.</description></item><item><title>05_数组：为什么很多编程语言中数组都从0开始编号？</title><link>https://artisanbox.github.io/2/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/6/</guid><description>提到数组，我想你肯定不陌生，甚至还会自信地说，它很简单啊。
是的，在每一种编程语言中，基本都会有数组这种数据类型。不过，它不仅仅是一种编程语言中的数据类型，还是一种最基础的数据结构。尽管数组看起来非常基础、简单，但是我估计很多人都并没有理解这个基础数据结构的精髓。
在大部分编程语言中，数组都是从0开始编号的，但你是否下意识地想过，为什么数组要从0开始编号，而不是从1开始呢？ 从1开始不是更符合人类的思维习惯吗？
你可以带着这个问题来学习接下来的内容。
如何实现随机访问？什么是数组？我估计你心中已经有了答案。不过，我还是想用专业的话来给你做下解释。数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。
这个定义里有几个关键词，理解了这几个关键词，我想你就能彻底掌握数组的概念了。下面就从我的角度分别给你“点拨”一下。
第一是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。
而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。
第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。
说到数据的访问，那你知道数组是如何实现根据下标随机访问数组元素的吗？
我们拿一个长度为10的int类型的数组int[] a = new int[10]来举例。在我画的这个图中，计算机给数组a[10]，分配了一块连续内存空间1000～1039，其中，内存块的首地址为base_address = 1000。
我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：
a[i]_address = base_address + i * data_type_size 其中data_type_size表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是int类型数据，所以data_type_size就为4个字节。这个公式非常简单，我就不多做解释了。
这里我要特别纠正一个“错误”。我在面试的时候，常常会问数组和链表的区别，很多人都回答说，“链表适合插入、删除，时间复杂度O(1)；数组适合查找，查找时间复杂度为O(1)”。
实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为O(1)。
低效的“插入”和“删除”前面概念部分我们提到，数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。现在我们就来详细说一下，究竟为什么会导致低效？又有哪些改进方法呢？
我们先来看插入操作。
假设数组的长度为n，现在，如果我们需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，给新来的数据，我们需要将第k～n这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？你可以自己先试着分析一下。
如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为(1+2+...n)/n=O(n)。
如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移k之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第k个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。
为了更好地理解，我们举一个例子。假设数组a[10]中存储了如下5个元素：a，b，c，d，e。
我们现在需要将元素x插入到第3个位置。我们只需要将c放入到a[5]，将a[2]赋值为x即可。最后，数组中的元素如下： a，b，x，d，e，c。
利用这种处理技巧，在特定场景下，在第k个位置插入一个元素的时间复杂度就会降为O(1)。这个处理思想在快排中也会用到，我会在排序那一节具体来讲，这里就说到这儿。
我们再来看删除操作。
跟插入数据类似，如果我们要删除第k个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。
和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为O(1)；如果删除开头的数据，则最坏情况时间复杂度为O(n)；平均情况时间复杂度也为O(n)。
实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？
我们继续来看例子。数组a[10]中存储了8个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除a，b，c三个元素。
为了避免d，e，f，g，h这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。
如果你了解JVM，你会发现，这不就是JVM标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。
警惕数组的访问越界问题了解了数组的几个基本操作后，我们来聊聊数组访问越界的问题。
首先，我请你来分析一下这段C语言代码的运行结果：
int main(int argc, char* argv[]){ int i = 0; int arr[3] = {0}; for(; i&amp;lt;=3; i++){ arr[i] = 0; printf(&amp;quot;hello world\n&amp;quot;); } return 0; } 你发现问题了吗？这段代码的运行结果并非是打印三行“hello word”，而是会无限打印“hello world”，这是为什么呢？</description></item><item><title>05_深入浅出索引（下）</title><link>https://artisanbox.github.io/1/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/5/</guid><description>在上一篇文章中，我和你介绍了InnoDB索引的数据结构模型，今天我们再继续聊聊跟MySQL索引有关的概念。
在开始这篇文章之前，我们先来看一下这个问题：
在下面这个表T中，如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？
下面是这个表的初始化语句。
mysql&amp;gt; create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; insert into T values(100,1, &amp;lsquo;aa&amp;rsquo;),(200,2,&amp;lsquo;bb&amp;rsquo;),(300,3,&amp;lsquo;cc&amp;rsquo;),(500,5,&amp;rsquo;ee&amp;rsquo;),(600,6,&amp;lsquo;ff&amp;rsquo;),(700,7,&amp;lsquo;gg&amp;rsquo;); 图1 InnoDB的索引组织结构现在，我们一起来看看这条SQL查询语句的执行流程：
在k索引树上找到k=3的记录，取得 ID = 300；
再到ID索引树查到ID=300对应的R3；
在k索引树取下一个值k=5，取得ID=500；
再回到ID索引树查到ID=500对应的R4；
在k索引树取下一个值k=6，不满足条件，循环结束。
在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了k索引树的3条记录（步骤1、3和5），回表了两次（步骤2和4）。
在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？
覆盖索引如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。</description></item><item><title>05_计算机指令：让我们试试用纸带编程</title><link>https://artisanbox.github.io/4/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/5/</guid><description>你在学写程序的时候，有没有想过，古老年代的计算机程序是怎么写出来的？
上大学的时候，我们系里教C语言程序设计的老师说，他们当年学写程序的时候，不像现在这样，都是用一种古老的物理设备，叫作“打孔卡（Punched Card）”。用这种设备写程序，可没法像今天这样，掏出键盘就能打字，而是要先在脑海里或者在纸上写出程序，然后在纸带或者卡片上打洞。这样，要写的程序、要处理的数据，就变成一条条纸带或者一张张卡片，之后再交给当时的计算机去处理。
你看这个穿孔纸带是不是有点儿像我们现在考试用的答题卡？那个时候，人们在特定的位置上打洞或者不打洞，来代表“0”或者“1”。
为什么早期的计算机程序要使用打孔卡，而不能像我们现在一样，用C或者Python这样的高级语言来写呢？原因很简单，因为计算机或者说CPU本身，并没有能力理解这些高级语言。即使在2019年的今天，我们使用的现代个人计算机，仍然只能处理所谓的“机器码”，也就是一连串的“0”和“1”这样的数字。
那么，我们每天用高级语言的程序，最终是怎么变成一串串“0”和“1”的？这一串串“0”和“1”又是怎么在CPU中处理的？今天，我们就来仔细介绍一下，“机器码”和“计算机指令”到底是怎么回事。
在软硬件接口中，CPU帮我们做了什么事？我们常说，CPU就是计算机的大脑。CPU的全称是Central Processing Unit，中文是中央处理器。
我们上一节说了，从硬件的角度来看，CPU就是一个超大规模集成电路，通过电路实现了加法、乘法乃至各种各样的处理逻辑。
如果我们从软件工程师的角度来讲，CPU就是一个执行各种计算机指令（Instruction Code）的逻辑机器。这里的计算机指令，就好比一门CPU能够听得懂的语言，我们也可以把它叫作机器语言（Machine Language）。
不同的CPU能够听懂的语言不太一样。比如，我们的个人电脑用的是Intel的CPU，苹果手机用的是ARM的CPU。这两者能听懂的语言就不太一样。类似这样两种CPU各自支持的语言，就是两组不同的计算机指令集，英文叫Instruction Set。这里面的“Set”，其实就是数学上的集合，代表不同的单词、语法。
所以，如果我们在自己电脑上写一个程序，然后把这个程序复制一下，装到自己的手机上，肯定是没办法正常运行的，因为这两者语言不通。而一台电脑上的程序，简单复制一下到另外一台电脑上，通常就能正常运行，因为这两台CPU有着相同的指令集，也就是说，它们的语言相通的。
一个计算机程序，不可能只有一条指令，而是由成千上万条指令组成的。但是CPU里不能一直放着所有指令，所以计算机程序平时是存储在存储器中的。这种程序指令存储在存储器里面的计算机，我们就叫作存储程序型计算机（Stored-program Computer）。
说到这里，你可能要问了，难道还有不是存储程序型的计算机么？其实，在没有现代计算机之前，有着聪明才智的工程师们，早就发明了一种叫Plugboard Computer的计算设备。我把它直译成“插线板计算机”。在一个布满了各种插口和插座的板子上，工程师们用不同的电线来连接不同的插口和插座，从而来完成各种计算任务。下面这个图就是一台IBM的Plugboard，看起来是不是有一股满满的蒸汽朋克范儿？
从编译到汇编，代码怎么变成机器码？了解了计算机指令和计算机指令集，接下来我们来看看，平时编写的代码，到底是怎么变成一条条计算机指令，最后被CPU执行的呢？我们拿一小段真实的C语言程序来看看。
// test.c int main() { int a = 1; int b = 2; a = a + b; } 这是一段再简单不过的C语言程序，即便你不了解C语言，应该也可以看懂。我们给两个变量 a、b分别赋值1、2，然后再将a、b两个变量中的值加在一起，重新赋值给了a这个变量。
要让这段程序在一个Linux操作系统上跑起来，我们需要把整个程序翻译成一个汇编语言（ASM，Assembly Language）的程序，这个过程我们一般叫编译（Compile）成汇编代码。
针对汇编代码，我们可以再用汇编器（Assembler）翻译成机器码（Machine Code）。这些机器码由“0”和“1”组成的机器语言表示。这一条条机器码，就是一条条的计算机指令。这样一串串的16进制数字，就是我们CPU能够真正认识的计算机指令。
在一个Linux操作系统上，我们可以简单地使用gcc和objdump这样两条命令，把对应的汇编代码和机器码都打印出来。
$ gcc -g -c test.c $ objdump -d -M intel -S test.o 可以看到，左侧有一堆数字，这些就是一条条机器码；右边有一系列的push、mov、add、pop等，这些就是对应的汇编代码。一行C语言代码，有时候只对应一条机器码和汇编代码，有时候则是对应两条机器码和汇编代码。汇编代码和机器码之间是一一对应的。
test.o: file format elf64-x86-64 Disassembly of section .text: 0000000000000000 &amp;lt;main&amp;gt;: int main() { 0: 55 push rbp 1: 48 89 e5 mov rbp,rsp int a = 1; 4: c7 45 fc 01 00 00 00 mov DWORD PTR [rbp-0x4],0x1 int b = 2; b: c7 45 f8 02 00 00 00 mov DWORD PTR [rbp-0x8],0x2 a = a + b; 12: 8b 45 f8 mov eax,DWORD PTR [rbp-0x8] 15: 01 45 fc add DWORD PTR [rbp-0x4],eax } 18: 5d pop rbp 19: c3 ret 这个时候你可能又要问了，我们实际在用GCC（GUC编译器套装，GNU Compiler Collectipon）编译器的时候，可以直接把代码编译成机器码呀，为什么还需要汇编代码呢？原因很简单，你看着那一串数字表示的机器码，是不是摸不着头脑？但是即使你没有学过汇编代码，看的时候多少也能“猜”出一些这些代码的含义。</description></item><item><title>05｜函数实现：是时候让我们的语言支持函数和返回值了</title><link>https://artisanbox.github.io/3/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/7/</guid><description>你好，我是宫文学。
不知道你还记不记得，我们在第一节课就支持了函数功能。不过那个版本的函数功能是被高度简化了的，比如，它不支持声明函数的参数，也不支持函数的返回值。
在上一节课实现了对变量的支持以后，我们终于可以进一步升级我们的函数功能了。为什么要等到这个时候呢？因为其实函数的参数的实现机制跟变量是很类似的。
为了升级我们的函数功能，我们需要完成几项任务：
参考变量的机制实现函数的参数机制； 支持在函数内部声明和使用本地变量，这个时候，我们需要能够区分函数作用域和全局作用域，还要能够在退出函数的时候，让本地变量的生命期随之结束； 要支持函数的返回值。 你可以想象到，在实现了这节课的功能以后，我们的语言就越来像样了。你甚至可以用这个语言来实现一点复杂的功能了，比如设计个函数，用来计算圆的周长、面积什么的。
好吧，让我们赶紧动手吧。首先，像上节课一样，我们还是要增强一下语法分析功能，以便解析函数的参数和返回值，并支持在函数内部声明本地变量。
增强语法分析功能我们原来的函数声明的语法比较简陋，现在我们采用一下TypeScript完整的函数声明语法。采用这个语法，函数可以有0到多个参数，每个参数都可以指定类型，就像变量一样，还可以指定函数返回值的类型。
//函数声明，由'function'关键字、函数名、函数签名和函数体构成。 functionDeclaration : 'function' Identifier callSignature '{' functionBody '}'; //函数签名，也就是参数数量和类型正确，以及函数的返回值类型正确 callSignature : &amp;lsquo;(&amp;rsquo; parameterList? &amp;lsquo;)&amp;rsquo; typeAnnotation? ;
//参数列表，由1到多个参数声明构成。 parameterList : parameter (&amp;rsquo;,&amp;rsquo; parameter)* ;
//参数，由参数名称和可选的类型标注构成 parameter : Identifier typeAnnotation? ;
//返回语句 returnStatement: &amp;lsquo;return&amp;rsquo; expression? &amp;lsquo;;&amp;rsquo; ; 采用该规则以后，你可以声明一个像下面的函数，比如，你给这个函数传入圆的半径的值，它会给你计算出圆的面积：
//计算圆的面积 function circleArea(r : number):number{ let area : number = 3.14rr; return area; } let r:number =4; println(&amp;ldquo;r=&amp;rdquo; + r +&amp;quot;, area=&amp;quot;+circleArea(r)); r = 5; println(&amp;ldquo;r=&amp;rdquo; + r +&amp;quot;, area=&amp;quot;+circleArea(r)); 好了，修改好语法规则以后，我们就按照该语法规则来升级一下语法分析程序，跟04讲一样，我们同样需要计算一下相关元素的First和Follow集合。在这里，我就不再演示计算First集合和Follow集合了，而是把它们留到了思考题的部分，让你自己来计算一个语法成分的Follow集合，这样能让你对LL算法理解得更加深入。</description></item><item><title>06_全局锁和表锁：给表加个字段怎么有这么多阻碍？</title><link>https://artisanbox.github.io/1/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/6/</guid><description>今天我要跟你聊聊MySQL的锁。数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。
根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类。今天这篇文章，我会和你分享全局锁和表级锁。而关于行锁的内容，我会留着在下一篇文章中再和你详细介绍。
这里需要说明的是，锁的设计比较复杂，这两篇文章不会涉及锁的具体实现细节，主要介绍的是碰到锁时的现象和其背后的原理。
全局锁顾名思义，全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。
全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。
以前有一种做法，是通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。
但是让整库都只读，听上去就很危险：
如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。 看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？我们来看一下不加锁会有什么问题。
假设你现在要维护“极客时间”的购买系统，关注的是用户账户余额表和用户课程表。
现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉他的余额，然后往已购课程里面加上一门课。
如果时间顺序上是先备份账户余额表(u_account)，然后用户购买，然后备份用户课程表(u_course)，会怎么样呢？你可以看一下这个图：
图1 业务和备份状态图可以看到，这个备份结果里，用户A的数据状态是“账户余额没扣，但是用户课程表里面已经多了一门课”。如果后面用这个备份来恢复数据的话，用户A就发现，自己赚了。
作为用户可别觉得这样可真好啊，你可以试想一下：如果备份表的顺序反过来，先备份用户课程表再备份账户余额表，又可能会出现什么结果？
也就是说，不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。
说到视图你肯定想起来了，我们在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视图的，对吧？
是的，就是在可重复读隔离级别下开启一个事务。
备注：如果你对事务隔离级别的概念不是很清晰的话，可以再回顾一下第3篇文章《事务隔离：为什么你改了我还看不见？》中的相关内容。
官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。
你一定在疑惑，有了这个功能，为什么还需要FTWRL呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。
所以，single-transaction方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。
你也许会问，既然要全库只读，为什么不使用set global readonly=true的方式呢？确实readonly方式也可以让全库进入只读状态，但我还是会建议你用FTWRL方式，主要有两个原因：
一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大，我不建议你使用。 二是，在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。 业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。
但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍的表级锁。
表级锁MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。
表锁的语法是 lock tables … read/write。与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
举个例子, 如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。
在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。
另一类表级的锁是MDL（metadata lock)。MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。
因此，在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。
读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。</description></item><item><title>06_指令跳转：原来if...else就是goto</title><link>https://artisanbox.github.io/4/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/6/</guid><description>上一讲，我们讲解了一行代码是怎么变成计算机指令的。你平时写的程序中，肯定不只有int a = 1这样最最简单的代码或者指令。我们总是要用到if…else这样的条件判断语句、while和for这样的循环语句，还有函数或者过程调用。
对应的，CPU执行的也不只是一条指令，一般一个程序包含很多条指令。因为有if…else、for这样的条件和循环存在，这些指令也不会一路平铺直叙地执行下去。
今天我们就在上一节的基础上来看看，一个计算机程序是怎么被分解成一条条指令来执行的。
CPU是如何执行指令的？拿我们用的Intel CPU来说，里面差不多有几百亿个晶体管。实际上，一条条计算机指令执行起来非常复杂。好在CPU在软件层面已经为我们做好了封装。对于我们这些做软件的程序员来说，我们只要知道，写好的代码变成了指令之后，是一条一条顺序执行的就可以了。
我们先不管几百亿的晶体管的背后是怎么通过电路运转起来的，逻辑上，我们可以认为，CPU其实就是由一堆寄存器组成的。而寄存器就是CPU内部，由多个触发器（Flip-Flop）或者锁存器（Latches）组成的简单电路。
触发器和锁存器，其实就是两种不同原理的数字电路组成的逻辑门。这块内容并不是我们这节课的重点，所以你只要了解就好。如果想要深入学习的话，你可以学习数字电路的相关课程，这里我们不深入探讨。
好了，现在我们接着前面说。N个触发器或者锁存器，就可以组成一个N位（Bit）的寄存器，能够保存N位的数据。比方说，我们用的64位Intel服务器，寄存器就是64位的。
一个CPU里面会有很多种不同功能的寄存器。我这里给你介绍三种比较特殊的。
一个是PC寄存器（Program Counter Register），我们也叫指令地址寄存器（Instruction Address Register）。顾名思义，它就是用来存放下一条需要执行的计算机指令的内存地址。
第二个是指令寄存器（Instruction Register），用来存放当前正在执行的指令。
第三个是条件码寄存器（Status Register），用里面的一个一个标记位（Flag），存放CPU进行算术或者逻辑计算的结果。
除了这些特殊的寄存器，CPU里面还有更多用来存储数据和内存地址的寄存器。这样的寄存器通常一类里面不止一个。我们通常根据存放的数据内容来给它们取名字，比如整数寄存器、浮点数寄存器、向量寄存器和地址寄存器等等。有些寄存器既可以存放数据，又能存放地址，我们就叫它通用寄存器。
实际上，一个程序执行的时候，CPU会根据PC寄存器里的地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。可以看到，一个程序的一条条指令，在内存里面是连续保存的，也会一条条顺序加载。
而有些特殊指令，比如上一讲我们讲到J类指令，也就是跳转指令，会修改PC寄存器里面的地址值。这样，下一条要执行的指令就不是从内存里面顺序加载的了。事实上，这些跳转指令的存在，也是我们可以在写程序的时候，使用if…else条件语句和while/for循环语句的原因。
从if…else来看程序的执行和跳转我们现在就来看一个包含if…else的简单程序。
// test.c #include &amp;lt;time.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt;
int main() { srand(time(NULL)); int r = rand() % 2; int a = 10; if (r == 0) { a = 1; } else { a = 2; } 我们用rand生成了一个随机数r，r要么是0，要么是1。当r是0的时候，我们把之前定义的变量a设成1，不然就设成2。
$ gcc -g -c test.c $ objdump -d -M intel -S test.</description></item><item><title>06_链表（上）：如何实现LRU缓存淘汰算法</title><link>https://artisanbox.github.io/2/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/7/</guid><description>今天我们来聊聊“链表（Linked list）”这个数据结构。学习链表有什么用呢？为了回答这个问题，我们先来讨论一个经典的链表应用场景，那就是LRU缓存淘汰算法。
缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的CPU缓存、数据库缓存、浏览器缓存等等。
缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略FIFO（First In，First Out）、最少使用策略LFU（Least Frequently Used）、最近最少使用策略LRU（Least Recently Used）。
这些策略你不用死记，我打个比方你很容易就明白了。假如说，你买了很多本技术书，但有一天你发现，这些书太多了，太占书房空间了，你要做个大扫除，扔掉一些书籍。那这个时候，你会选择扔掉哪些书呢？对应一下，你的选择标准是不是和上面的三种策略神似呢？
好了，回到正题，我们今天的开篇问题就是：如何用链表来实现LRU缓存淘汰策略呢？ 带着这个问题，我们开始今天的内容吧！
五花八门的链表结构相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍难一些。这两个非常基础、非常常用的数据结构，我们常常会放到一块儿来比较。所以我们先来看，这两者有什么区别。
我们先从底层的存储结构上来看一看。
为了直观地对比，我画了一张图。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个100MB大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于100MB，仍然会申请失败。
而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是100MB大小的链表，根本不会有问题。
链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。我们首先来看最简单、最常用的单链表。
我们刚刚讲到，链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针next。
从我画的单链表图中，你应该可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址NULL，表示这是链表上最后一个结点。
与数组一样，链表也支持数据的查找、插入和删除操作。
我们知道，在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。
为了方便你理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是O(1)。
但是，有利就有弊。链表要想随机访问第k个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。
你可以把链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第k位的人是谁的时候，我们就需要从第一个人开始，一个一个地往下数。所以，链表随机访问的性能没有数组好，需要O(n)的时间复杂度。
好了，单链表我们就简单介绍完了，接着来看另外两个复杂的升级版，循环链表和双向链表。
循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。
和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。
单链表和循环链表是不是都不难？接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。
单向链表只有一个方向，结点只有一个后继指针next指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。
从我画的图中可以看出来，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。那相比单链表，双向链表适合解决哪种问题呢？
从结构上来看，双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。
你可能会说，我刚讲到单链表的插入、删除操作的时间复杂度已经是O(1)了，双向链表还能再怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。我再来带你分析一下链表的两个操作。
我们先来看删除操作。
在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：
删除结点中“值等于某个给定值”的结点；
删除给定指针指向的结点。
对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。
尽管单纯的删除操作时间复杂度是O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。
对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点q需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到p-&amp;gt;next=q，说明p是q的前驱结点。
但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定了！
同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。你可以参照我刚刚讲过的删除操作自己分析一下。
除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。
现在，你有没有觉得双向链表要比单链表更加高效呢？这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉Java语言，你肯定用过LinkedHashMap这个容器。如果你深入研究LinkedHashMap的实现原理，就会发现其中就用到了双向链表这种数据结构。
实际上，这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。
还是开篇缓存的例子。缓存实际上就是利用了空间换时间的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。
所以我总结一下，对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？
了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不用我多讲，你应该知道双向循环链表长什么样子了吧？你可以自己试着在纸上画一画。
链表VS数组性能大比拼通过前面内容的学习，你应该已经知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。
不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。
数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存不友好，没办法有效预读。
数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。
你可能会说，我们Java中的ArrayList容器，也可以支持动态扩容啊？我们上一节课讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。
我举一个稍微极端的例子。如果我们用ArrayList存储了了1GB大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList会申请一个1.5GB大小的存储空间，并且把原来那1GB的数据拷贝到新申请的空间上。听起来是不是就很耗时？
除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是Java语言，就有可能会导致频繁的GC（Garbage Collection，垃圾回收）。
所以，在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。
解答开篇好了，关于链表的知识我们就讲完了。我们现在回过头来看下开篇留给你的思考题。如何基于链表实现LRU缓存淘汰算法？
我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。
1.如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
2.如果此数据没有在缓存链表中，又可以分为两种情况：
如果此时缓存未满，则将此结点直接插入到链表的头部；</description></item><item><title>06｜怎么支持条件语句和循环语句？</title><link>https://artisanbox.github.io/3/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/8/</guid><description>你好，我是宫文学。
我们现在的语言已经支持表达式、变量和函数了。可是你发现没有，到现在为止，我们还没有支持流程控制类的语句，比如条件语句和循环语句。如果再加上这两类语句的话，我们的语言就能做很复杂的事情了，甚至你会觉得它已经是一门比较完整的语言了。
那么今天，我们就来加上条件语句和循环语句。在这个过程中，我们会加深对作用域和栈桢的理解，包括跨作用域的变量引用、词法作用域的概念，以及如何在运行时访问其他作用域的变量。这些知识点会帮助你加深对计算机语言的运行机制的理解。
而这些理解和认知，会有助于我们后面把基于AST的解释器升级成基于字节码的解释器，也有助于我们理解编译成机器码后的运行时机制。
好了，首先我们先从语法层面支持一下这两种语句。
语法分析：支持一元表达式按照惯例，我们首先要写下新的语法规则，然后使用LL算法来升级语法分析程序。新的语法规则如下：
ifStatement &amp;nbsp; &amp;nbsp; : If '(' expression ')' statement (Else statement)? &amp;nbsp; &amp;nbsp; ; forStatement :For '(' expression? ';' expression? ';' expression? ')' statement ; statement: : block | functionDecl | varaibleStatement | expressionStatement | returnStatement | ifStatement | forStatement | emptyStatement ; 你从上面的语法可以得到这几个信息：
首先，if语句中，else部分是可选的。这样，我们在解析完if条件后面的语句以后，要去看看后面跟着的是不是’else’关键字，从而决定是否解析else后面的语句块。更具体的你可以参见parseIfStatement函数的代码。
第二，在for循环语句中，for括号里用分号分割的三个表达式都是可选的，在解析的时候也要根据Follow集合来判断是否需要解析这三个表达式。这点你具体可以参见parseForStatement函数的代码。
最后，从statement的语法规则中，我们也可以发现，我们的语言所支持的语句越来越多了，这也使得语言特性越来越丰富了。
现在，升级我们的语法解析程序，对你来说已经没有太大的困难了，你可以参照我的参考实现动手自己做一下。
不过，为了实现for语句，我们还有一个语言特性需要升级一下，这就是对一元运算的支持。
哪些是一元运算呢？比如，在for语句中，我们经常会使用下面的写法：
for(i = 0; i&amp;lt; 10; i++) 其中i++就是使用了一元运算。在这里，为了方便，我们干脆就让程序支持所有的一元运算！
一元运算符除了++以外，还有–、~、!等。甚至还有更复杂一点的情况，+号和-号除了作为二元运算符以外，还可以作为一元运算符使用，比如下面这个例子：
myAge = +myAge + -10; 你甚至可以将多个一元运算符叠加使用，比如我们把上面的例子修改一下，仍然和原来的计算结果相同：</description></item><item><title>07_函数调用：为什么会发生stackoverflow？</title><link>https://artisanbox.github.io/4/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/7/</guid><description>在开发软件的过程中我们经常会遇到错误，如果你用Google搜过出错信息，那你多少应该都访问过Stack Overflow这个网站。作为全球最大的程序员问答网站，Stack Overflow的名字来自于一个常见的报错，就是栈溢出（stack overflow）。
今天，我们就从程序的函数调用开始，讲讲函数间的相互调用，在计算机指令层面是怎么实现的，以及什么情况下会发生栈溢出这个错误。
为什么我们需要程序栈？和前面几讲一样，我们还是从一个非常简单的C程序function_example.c看起。
// function_example.c #include &amp;lt;stdio.h&amp;gt; int static add(int a, int b) { return a+b; } int main() { int x = 5; int y = 10; int u = add(x, y); } 这个程序定义了一个简单的函数add，接受两个参数a和b，返回值就是a+b。而main函数里则定义了两个变量x和y，然后通过调用这个add函数，来计算u=x+y，最后把u的数值打印出来。
$ gcc -g -c function_example.c $ objdump -d -M intel -S function_example.o 我们把这个程序编译之后，objdump出来。我们来看一看对应的汇编代码。
int static add(int a, int b) { 0: 55 push rbp 1: 48 89 e5 mov rbp,rsp 4: 89 7d fc mov DWORD PTR [rbp-0x4],edi 7: 89 75 f8 mov DWORD PTR [rbp-0x8],esi return a+b; a: 8b 55 fc mov edx,DWORD PTR [rbp-0x4] d: 8b 45 f8 mov eax,DWORD PTR [rbp-0x8] 10: 01 d0 add eax,edx } 12: 5d pop rbp 13: c3 ret 0000000000000014 &amp;lt;main&amp;gt;: int main() { 14: 55 push rbp 15: 48 89 e5 mov rbp,rsp 18: 48 83 ec 10 sub rsp,0x10 int x = 5; 1c: c7 45 fc 05 00 00 00 mov DWORD PTR [rbp-0x4],0x5 int y = 10; 23: c7 45 f8 0a 00 00 00 mov DWORD PTR [rbp-0x8],0xa int u = add(x, y); 2a: 8b 55 f8 mov edx,DWORD PTR [rbp-0x8] 2d: 8b 45 fc mov eax,DWORD PTR [rbp-0x4] 30: 89 d6 mov esi,edx 32: 89 c7 mov edi,eax 34: e8 c7 ff ff ff call 0 &amp;lt;add&amp;gt; 39: 89 45 f4 mov DWORD PTR [rbp-0xc],eax 3c: b8 00 00 00 00 mov eax,0x0 } 41: c9 leave</description></item><item><title>07_行锁功过：怎么减少行锁对性能的影响？</title><link>https://artisanbox.github.io/1/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/7/</guid><description>在上一篇文章中，我跟你介绍了MySQL的全局锁和表级锁，今天我们就来讲讲MySQL的行锁。
MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。
我们今天就主要来聊聊InnoDB的行锁，以及如何通过减少锁冲突来提升业务并发度。
顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务A更新了一行，而这时候事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。
当然，数据库中还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导致程序出现非预期行为，比如两阶段锁。
从两阶段锁说起我先给你举个例子。在下面的操作序列中，事务B的update语句执行时会是什么现象呢？假设字段id是表t的主键。
这个问题的结论取决于事务A在执行完两条update语句后，持有哪些锁，以及在什么时候释放。你可以验证一下：实际上事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。
知道了这个答案，你一定知道了事务A持有的两个记录的行锁，都是在commit的时候才释放的。
也就是说，在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。
知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。我给你举个例子。
假设你负责实现一个电影票在线交易业务，顾客A要在影院B购买电影票。我们简化一点，这个业务需要涉及到以下操作：
从顾客A账户余额中扣除电影票价；
给影院B的账户余额增加这张电影票价；
记录一条交易日志。
也就是说，要完成这个交易，我们需要update两条记录，并insert一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？
试想如果同时有另外一个顾客C要在影院B买票，那么这两个事务冲突的部分就是语句2了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。
根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。
好了，现在由于你的正确设计，影院余额这一行的行锁在一个事务中不会停留很长时间。但是，这并没有完全解决你的困扰。
如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的MySQL就挂了。你登上服务器一看，CPU消耗接近100%，但整个数据库每秒就执行不到100个事务。这是什么原因呢？
这里，我就要说到死锁和死锁检测了。
死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。
这时候，事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：
一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。 在InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。
但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。
所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。
你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。
那如果是我们上面说到的所有事务都要更新同一行的场景呢？
每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到CPU利用率很高，但是每秒却执行不了几个事务。
根据上面的分析，我们来讨论一下，怎么解决由这种热点行更新导致的性能问题呢？问题的症结在于，死锁检测要耗费大量的CPU资源。
一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。
另一个思路是控制并发度。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有10个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有600个客户端，这样即使每个客户端控制到只有5个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到3000。
因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改MySQL源码的人，也可以做在MySQL里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。
可能你会问，如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？
你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗。
这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成0的时候，代码要有特殊处理。
小结今天，我和你介绍了MySQL的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。
其中，我以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句。这里的原则/我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。
但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事务量。
最后，我给你留下一个问题吧。如果你要删除一个表里面的前10000行数据，有以下三种方法可以做到：
第一种，直接执行delete from T limit 10000; 第二种，在一个连接中循环执行20次 delete from T limit 500; 第三种，在20个连接中同时执行delete from T limit 500。 你会选择哪一种方法呢？为什么呢？
你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾和你讨论这个问题。感谢你的收听，也欢迎你把这篇文章分享给更多的朋友一起阅读。</description></item><item><title>07_链表（下）：如何轻松写出正确的链表代码？</title><link>https://artisanbox.github.io/2/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/8/</guid><description>上一节我讲了链表相关的基础知识。学完之后，我看到有人留言说，基础知识我都掌握了，但是写链表代码还是很费劲。哈哈，的确是这样的！
想要写好链表代码并不是容易的事儿，尤其是那些复杂的链表操作，比如链表反转、有序链表合并等，写的时候非常容易出错。从我上百场面试的经验来看，能把“链表反转”这几行代码写对的人不足10%。
为什么链表代码这么难写？究竟怎样才能比较轻松地写出正确的链表代码呢？
只要愿意投入时间，我觉得大多数人都是可以学会的。比如说，如果你真的能花上一个周末或者一整天的时间，就去写链表反转这一个代码，多写几遍，一直练到能毫不费力地写出Bug free的代码。这个坎还会很难跨吗？
当然，自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要一些方法和技巧。我根据自己的学习经历和工作经验，总结了几个写链表代码技巧。如果你能熟练掌握这几个技巧，加上你的主动和坚持，轻松拿下链表代码完全没有问题。
技巧一：理解指针或引用的含义事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以，要想写对链表代码，首先就要理解好指针。
我们知道，有些语言有“指针”的概念，比如C语言；有些语言没有指针，取而代之的是“引用”，比如Java、Python。不管是“指针”还是“引用”，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。
接下来，我会拿C语言中的“指针”来讲解，如果你用的是Java或者其他没有指针的语言也没关系，你把它理解成“引用”就可以了。
实际上，对于指针的理解，你只需要记住下面这句话就可以了：
将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。
这句话听起来还挺拗口的，你可以先记住。我们回到链表代码的编写过程中，我来慢慢给你解释。
在编写链表代码的时候，我们经常会有这样的代码：p-&amp;gt;next=q。这行代码是说，p结点中的next指针存储了q结点的内存地址。
还有一个更复杂的，也是我们写链表代码经常会用到的：p-&amp;gt;next=p-&amp;gt;next-&amp;gt;next。这行代码表示，p结点的next指针存储了p结点的下下一个结点的内存地址。
掌握了指针或引用的概念，你应该可以很轻松地看懂链表代码。恭喜你，已经离写出链表代码近了一步！
技巧二：警惕指针丢失和内存泄漏不知道你有没有这样的感觉，写链表代码的时候，指针指来指去，一会儿就不知道指到哪里了。所以，我们在写的时候，一定注意不要弄丢了指针。
指针往往都是怎么弄丢的呢？我拿单链表的插入操作为例来给你分析一下。
如图所示，我们希望在结点a和相邻的结点b之间插入结点x，假设当前指针p指向结点a。如果我们将代码实现变成下面这个样子，就会发生指针丢失和内存泄露。
p-&amp;gt;next = x; // 将p的next指针指向x结点； x-&amp;gt;next = p-&amp;gt;next; // 将x的结点的next指针指向b结点； 初学者经常会在这儿犯错。p-&amp;gt;next指针在完成第一步操作之后，已经不再指向结点b了，而是指向结点x。第2行代码相当于将x赋值给x-&amp;gt;next，自己指向自己。因此，整个链表也就断成了两半，从结点b往后的所有结点都无法访问到了。
对于有些语言来说，比如C语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄露。所以，我们插入结点时，一定要注意操作的顺序，要先将结点x的next指针指向结点b，再把结点a的next指针指向结点x，这样才不会丢失指针，导致内存泄漏。所以，对于刚刚的插入代码，我们只需要把第1行和第2行代码的顺序颠倒一下就可以了。
同理，删除链表结点时，也一定要记得手动释放内存空间，否则，也会出现内存泄漏的问题。当然，对于像Java这种虚拟机自动管理内存的编程语言来说，就不需要考虑这么多了。
技巧三：利用哨兵简化实现难度首先，我们先来回顾一下单链表的插入和删除操作。如果我们在结点p后面插入一个新的结点，只需要下面两行代码就可以搞定。
new_node-&amp;gt;next = p-&amp;gt;next; p-&amp;gt;next = new_node; 但是，当我们要向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。我们需要进行下面这样的特殊处理，其中head表示链表的头结点。所以，从这段代码，我们可以发现，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不一样的。
if (head == null) { head = new_node; } 我们再来看单链表结点删除操作。如果要删除结点p的后继结点，我们只需要一行代码就可以搞定。
p-&amp;gt;next = p-&amp;gt;next-&amp;gt;next; 但是，如果我们要删除链表中的最后一个结点，前面的删除代码就不work了。跟插入类似，我们也需要对于这种情况特殊处理。写成代码是这样子的：
if (head-&amp;gt;next == null) { head = null; } 从前面的一步一步分析，我们可以看出，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。如何来解决这个问题呢？
技巧三中提到的哨兵就要登场了。哨兵，解决的是国家之间的边界问题。同理，这里说的哨兵也是解决“边界问题”的，不直接参与业务逻辑。
还记得如何表示一个空链表吗？head=null表示链表中没有结点了。其中head表示头结点指针，指向链表中的第一个结点。
如果我们引入哨兵结点，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。
我画了一个带头链表，你可以发现，哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。
实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。这些内容我们后面才会讲，现在为了让你感受更深，我再举一个非常简单的例子。代码我是用C语言实现的，不涉及语言方面的高级语法，很容易看懂，你可以类比到你熟悉的语言。
代码一：</description></item><item><title>07｜怎么设计属于我们自己的虚拟机和字节码？</title><link>https://artisanbox.github.io/3/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/9/</guid><description>你好，我是宫文学。
到目前为止，我们的语言看上去已经有点像模像样了。但是有一个地方，我还一直是用比较凑合的方式来实现的，这就是解释器，这节课我想带你把它升级一下。
在之前的内容中，我们用的解释器都是基于AST执行的，而实际上，你所能见到的大多数解释执行的脚本语言，比如Python、PHP、JavaScript等，内部都是采用了一个虚拟机，用字节码解释执行的。像Java这样的语言，虽然是编译执行，但编译的结果也是字节码，JVM虚拟机也能够解释执行它。
为什么这些语言都广泛采用虚拟机呢？这主要是由基于AST的解释器的不足导致的。今天这节课，我就带你先来分析一下AST解释器的缺陷，然后了解一下像JVM这样的虚拟机的运行原理，特别是栈机和寄存器机的差别，以便确定我们自己的虚拟机的设计思路。
看上去任务有点多，没关系，我们一步一步来，我们先来分析一下基于AST的解释器。
基于AST的解释器的缺陷其实，我们目前采用的解释器，是一种最简单的解释器。它有时被称为“树遍历解释器”(Tree-walking Interpreter)，或者更简单一点，也被叫做“AST解释器”。
为什么我刚刚会说我们这个基于AST的解释器有点凑合呢？你可能会想通过遍历AST来执行程序不是挺好的吗？
确实，AST解释器虽然简单，但很有用。比如，最常见的就是对一个表达式做求值，实现类似公式计算的功能，这在电子表格等系统里很常见。甚至在MySQL中，也是基于AST做表达式的计算，还有一些计算机语言的早期版本（如Ruby），以及一些产品中自带的脚本功能，也是用AST解释器就足够了！
不过，虽然AST解释器很有用，但它仍然有明显的缺陷。最主要的问题，就是性能差，在运行时需要消耗更多的CPU时间和内存。在上一节课里，你可能使用过我们的函数特性计算过斐波那契数列，在参数值比较大的情况下（比如n大于30以后），你会看到程序的运行速度确实比较慢。
为什么会这样呢？你再来看我们的解释器，会发现它哪怕只是做一个表达式求值，也要层层做很多次的函数调用。比如，简单的计算2+3*5，需要做的调用包括：
visitBlock() visitStatement() visitExpressionStatement() visitBinary() //+号 visitIntegerLiteral() //2 visitBinary() //*号 visitIntegerLiteral() //3 visitIntegerLiteral() //5 从表面上看起来，这只是做了8次的函数调用。其实，如果你仔细看我们代码的细节，就会发现，由于我们的程序采用了Visitor模式，每一次调用还都有一个Visitor.visit(AstNode)和AstNode.accept(Visitor)来回握手的过程，所以加起来一共做了24次函数调用。
这样的函数调用的开销是很大的。在上一节课，你已经知道了，每次函数调用都需要创建一个栈桢，这会导致内存的消耗。调用函数和从函数返回，也都需要耗费额外的CPU时间。在后面的课程里，等我们对程序的运行时机制的细节了解得更清楚以后，你会更加理解这些额外的开销发生在什么地方。
除了性能问题，AST解释器还有其他的问题。比如，我们已经看到，在实现Return语句的时候，需要额外的冗余处理，以便跳过Return后面的语句，类似的情况还发生在Break、Continue等语句中。总的来说，在控制流的跳转方面，用AST都不方便。
还有，我们执行函数调用的时候，需要从函数调用的AST节点跳到函数声明的节点，这让人有点眼花缭乱。如果我们后面支持类、Lambda等更加丰富的特性，需要运行类的构造函数、进行类成员的初始化，查找并执行正确的父类或子类的方法，那么程序的执行过程会更加让人难以理解。
而且，从根本上来说，AST这种数据结构，比较忠实地体现了高级语言的语法特征。而高级语言呢，是设计用来方便人类理解的，并不是适合机器执行的。把计算机语言从适合人类阅读，变成适合机器执行，本来就是编译器要做的事情。不过，把高级语言变成AST，我们叫做解析（Parse），还不能称上是编译（Compile）。要称得上编译，要对程序的表示方式做更多的变换才行。
那接下来呢，我们就探讨一下如何把程序编译成对机器更友好的方式。按照循序渐进的学习原则，我们不会一下子就编译成机器码，而是先编译成字节码，并试着实现一个虚拟机。
初步了解虚拟机说到虚拟机，我们大多数人都不陌生。比如Java语言最显著的特征之一，就是运行在虚拟机上，而不是像C语言或Go语言那样，编译成可执行文件直接运行；.NET也是用了类似的架构。就算现在Java支持AOT（Ahead of Time）编译方式了，它的可执行文件中仍然打包了一个小型的虚拟机，以便支持某些特别的语言特性。
至于其他语言，如Python、JavaScript等，虽然我们不怎么提及它们的虚拟机，但它们通常都是基于虚拟机来运行的。
虚拟机如此流行，不是偶然现象，因为它提供了一些明显的优点。其中最值得注意的，就是程序可以独立于具体的计算机硬件和操作系统而运行。所以，我们在任何设备上，无论是Mac电脑、Windows电脑、安卓手机还是iPad，都可以用浏览器打开一个页面，并运行里面的JavaScript。而如果采用C语言那样的运行方式，那么针对每种不同的CPU和操作系统的组合，都要生成不同的可执行文件，这对于像浏览器一样的很多应用场景，显然都是很麻烦的。
虚拟机还能提供其他好处，比如通过托管运行提供更高的安全性，还有通过标准的方式使用不同计算机上的文件资源、网络资源、线程资源和图形资源等等。
实际上，虚拟化是计算机领域的一个基本思路。比如，云计算平台能够虚拟出很多不同的操作系统，在基于ARM芯的Mac上可以仿真运行基于X86的Windows等等，都是不同角度的虚拟化。
所以说，要实现一门现代计算机语言，就不能忽视虚拟机方面的知识。
那语言的虚拟机都要包含哪些功能呢？又是由哪些部分构成的呢？
介绍虚拟机的文章很多，特别是针对像JVM和安卓的ART这样广泛使用的平台，有专门的书籍和课程来深入剖析。在我们的课程里，由于我们自己要实现一下虚拟机，因此我也会简单介绍一下虚拟机的原理和构成，当然更多的就要靠你动手实践来掌握虚拟机的精髓了。
总的来说，虚拟机可以看做是一台虚拟的计算机，它能像物理计算机一样，给程序提供一个完整的运行环境。
首先，虚拟机像物理计算机一样，会支持一个指令集。我们的程序按照这个指令集编译成目标代码后，就可以在虚拟机上执行了。
第二，虚拟机也像物理计算机一样，提供内存资源。比如在JVM中，你可以使用栈和堆两种内存。根据不同语言的内存管理机制的不同，在虚拟机里通常还要集成垃圾收集机制。
第三，虚拟机要像物理计算机一样，能够加载程序的代码并执行。程序的目标代码文件中，除了可执行的代码（也就是虚拟机指令），还会包含一些静态的数据，比如程序的符号表，这会让你的程序支持元编程功能，比如运行时的类型判断、基于反射来运行代码等等。静态数据还包括程序中使用的一些常量，比如字符串常量、整数常量等等。对于代码和静态数据，会被虚拟机放在特定的区域，并且能够被指令访问。
此外，虚拟机还要在IO、并发等方面提供相应的支持。这样，我们才可以实现像在终端打印字符这样的功能。
好了，我们已经大概了解了虚拟机相关的概念。不过，不同的虚拟机在运行代码的机制方面是有所区别的，这也会影响到字节码的设计和算法的实现，所以我们现在展开介绍一下。
你可能听说过寄存器机和栈机，这就是比较流行两种程序运行机制。
栈机和寄存器机使用栈机的典型代表，就是JVM，它能够运行Java的字节码。Web Assembly是为浏览器设计的字节码，它的设计也是栈机的架构。
使用寄存器机的典型代表是能够运行JavaScript的V8，V8里面有一个基于寄存器机的字节码解释器。而Lua和Erlang内部也是采用了寄存器机作为程序的运行机制，其实我们现在使用物理计算机，也是寄存器机。
栈机和寄存器机的主要区别，是获取操作数的方式不同：栈机是从操作数栈里获取操作数，而寄存器机是从寄存器里获取。比如要计算“a+3”这个表达式，虚拟机通常都提供了一个指令用来做加法。a和3呢，则是加法指令的操作数，其中a是一个本地变量，其值为2，另一个操作数是常量3。那怎么完成这个加法操作呢？
栈机的运行方式，是先把a的值从内存取出来，压到一个叫做操作栈的区域（使用load指令），然后把常量3压到操作数栈里（使用push指令），接着执行add指令。在执行add指令的时候，就从操作数栈里弹出a和3，做完加法以后，再把结果5压到栈里。
而寄存器机的运行方式，是先把a的值加载到寄存器，在执行add指令的时候，从这寄存器取数，加上常量3以后，再把结果5放回到寄存器。
总结起来，栈机和寄存器机的区别有这三个方面：
第一，操作数的存储机制不同。栈机的操作数放在操作栈，操作数栈的大小几乎不受限制；而寄存器机的操作数是放在寄存器里，寄存器的数量是有限的。需要说明的是，对于物理机来说，寄存器指的是物理寄存器；而对于虚拟机来说，寄存器通常只是几个本地变量。但因为这些变量被频繁访问，根据寄存器分配算法，它们有比较大的概率被映射成物理寄存器，从而大大提高运行性能。
第二，指令的格式不同。寄存器机的指令，需要在操作数里指定运算所需的源和目的操作数。而栈机的运算性的指令，比如加减乘除等，是不需要带操作数的，因为操作数就在栈顶；而像push、load这样的指令，是把数据压到栈里，也不需要指定目的地，因为这个数据也一定是存到栈顶的。
第三，生成字节码的难度不同。从AST生成栈机的代码是比较容易的，你在后面就可以体会到。而生成寄存器机的代码的难度就更高一些，因为寄存器的数量是有限的，我们必须要添加寄存器分配算法。
了解了栈机和寄存器机的这些差别以后，我们就可以根据自己的需求做取舍。比如，如果要更关注运行性能，就选用寄存器机；而如果想实现起来简单一点，并且指令数量更少，便于通过网络传输，我们就可以用栈机。
好了，现在我们已经初步了解了两种运行机制和两类指令集的特点了，是时候设计我们自己的虚拟机和字节码了！
设计我们自己的虚拟机设计一个虚拟机是一项挺有挑战的工作，不过，如果我们仍然采取先迈出一小步，然后慢慢迭代的思路，就没那么复杂了。
在实现一个虚拟机之前，有些关键的技术决策是要确定一下的，这些决策影响到虚拟机的特性和我们所采用的技术。
决策1：选择栈机还是寄存器机？
其实，栈机和寄存器都能满足我们对于程序运行的核心需求，因为我们目前对性能、字节码的大小都没有什么特别的要求。不过，经过思考，我最终选择了栈机，主要有这几个考虑：
首先，Java的JVM用的就是栈机，而且讲述JVM的资料也很多，方便我们借鉴和学习它成熟的设计思路。
第二，由于我们最后是要生成面向物理机的机器码，而物理机就是寄存器机，所以我们肯定会学到这方面的知识。从扩大知识面的角度，我们在虚拟机层面熟悉一下栈机就更好了。
第三，Web Assembly是一个很有前途的技术领域，目前各门语言都在添加编译成Web Assembly的工作。而自己动手实现一个栈机的经验，有助于我们理解Web Assembly，为未来支持Web Assembly打下基础。</description></item><item><title>08_ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？</title><link>https://artisanbox.github.io/4/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/8/</guid><description>过去的三节，你和我一起通过一些简单的代码，看到了我们写的程序，是怎么变成一条条计算机指令的；if…else这样的条件跳转是怎么样执行的；for/while这样的循环是怎么执行的；函数间的相互调用是怎么发生的。
我记得以前，我自己在了解完这些知识之后，产生了一个非常大的疑问。那就是，既然我们的程序最终都被变成了一条条机器码去执行，那为什么同一个程序，在同一台计算机上，在Linux下可以运行，而在Windows下却不行呢？反过来，Windows上的程序在Linux上也是一样不能执行的。可是我们的CPU并没有换掉，它应该可以识别同样的指令呀？
如果你和我有同样的疑问，那这一节，我们就一起来解开。
编译、链接和装载：拆解程序执行第5节我们说过，写好的C语言代码，可以通过编译器编译成汇编代码，然后汇编代码再通过汇编器变成CPU可以理解的机器码，于是CPU就可以执行这些机器码了。你现在对这个过程应该不陌生了，但是这个描述把过程大大简化了。下面，我们一起具体来看，C语言程序是如何变成一个可执行程序的。
不知道你注意到没有，过去几节，我们通过gcc生成的文件和objdump获取到的汇编指令都有些小小的问题。我们先把前面的add函数示例，拆分成两个文件add_lib.c和link_example.c。
// add_lib.c int add(int a, int b) { return a+b; } // link_example.c #include &amp;lt;stdio.h&amp;gt; int main() { int a = 10; int b = 5; int c = add(a, b); printf(&amp;quot;c = %d\n&amp;quot;, c); } 我们通过gcc来编译这两个文件，然后通过objdump命令看看它们的汇编代码。
$ gcc -g -c add_lib.c link_example.c $ objdump -d -M intel -S add_lib.o $ objdump -d -M intel -S link_example.o add_lib.o: file format elf64-x86-64 Disassembly of section .</description></item><item><title>08_事务到底是隔离的还是不隔离的？</title><link>https://artisanbox.github.io/1/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/8/</guid><description>你好，我是林晓斌。
你现在看到的这篇文章是我重写过的。在第一版文章发布之后，我发现在介绍事务可见性规则时，由于引入了太多概念，导致理解起来很困难。随后，我索性就重写了这篇文章。
现在的用户留言中，还能看到第一版文章中引入的up_limit_id的概念，为了避免大家产生误解，再此特地和大家事先说明一下。
我在第3篇文章和你讲事务隔离级别的时候提到过，如果是可重复读隔离级别，事务T启动的时候会创建一个视图read-view，之后事务T执行期间，即使有其他事务修改了数据，事务T看到的仍然跟在启动时看到的一样。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。
但是，我在上一篇文章中，和你分享行锁的时候又提到，一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？
我给你举一个例子吧。下面是一个只有两行的表的初始化语句。
mysql&amp;gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 图1 事务A、B、C的执行流程这里，我们需要注意的是事务的启动时机。
begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。
第一种启动方式，一致性视图是在执行第一个快照读语句时创建的；
第二种启动方式，一致性视图是在执行start transaction with consistent snapshot时创建的。
还需要注意的是，在整个专栏里面，我们的例子中如果没有特别说明，都是默认autocommit=1。
在这个例子中，事务C没有显式地使用begin/commit，表示这个update语句本身就是一个事务，语句完成的时候会自动提交。事务B在更新了行之后查询; 事务A在一个只读事务中查询，并且时间顺序上是在事务B的查询之后。
这时，如果我告诉你事务B查到的k的值是3，而事务A查到的k的值是1，你是不是感觉有点晕呢？
所以，今天这篇文章，我其实就是想和你说明白这个问题，希望借由把这个疑惑解开的过程，能够帮助你对InnoDB的事务和锁有更进一步的理解。
在MySQL里，有两个“视图”的概念：
一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view … ，而它的查询方法与表一样。 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。 它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。
在第3篇文章《事务隔离：为什么你改了我还看不见？》中，我跟你解释过一遍MVCC的实现逻辑。今天为了说明查询和更新的区别，我换一个方式来说明，把read view拆开。你可以结合这两篇文章的说明来更深一步地理解MVCC。
“快照”在MVCC里是怎么工作的？在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。
这时，你会说这看上去不太现实啊。如果一个库有100G，那么我启动一个事务，MySQL就要拷贝100G的数据出来，这个过程得多慢啊。可是，我平时的事务执行起来很快啊。
实际上，我们并不需要拷贝出这100G的数据。我们先来看看这个快照是怎么实现的。</description></item><item><title>08_栈：如何实现浏览器的前进和后退功能？</title><link>https://artisanbox.github.io/2/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/9/</guid><description>浏览器的前进、后退功能，我想你肯定很熟悉吧？
当你依次访问完一串页面a-b-c之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面b和a。当你后退到页面a，点击前进按钮，就可以重新查看页面b和c。但是，如果你后退到页面b后，点击了新的页面d，那就无法再通过前进、后退功能查看页面c了。
假设你是Chrome浏览器的开发工程师，你会如何实现这个功能呢？
这就要用到我们今天要讲的“栈”这种数据结构。带着这个问题，我们来学习今天的内容。
如何理解“栈”？关于“栈”，我有一个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出。后进者先出，先进者后出，这就是典型的“栈”结构。
从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。
我第一次接触这种数据结构的时候，就对它存在的意义产生了很大的疑惑。因为我觉得，相比数组和链表，栈带给我的只有限制，并没有任何优势。那我直接使用数组或者链表不就好了吗？为什么还要用这个“操作受限”的“栈”呢？
事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。
当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时我们就应该首选“栈”这种数据结构。
如何实现一个“栈”？从刚才栈的定义里，我们可以看出，栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。理解了栈的定义之后，我们来看一看如何用代码实现一个栈。
实际上，栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。
我这里实现一个基于数组的顺序栈。基于链表实现的链式栈的代码，你可以自己试着写一下。我会将我写好的代码放到GitHub上，你可以去看一下自己写的是否正确。
我这段代码是用Java来实现的，但是不涉及任何高级语法，并且我还用中文做了详细的注释，所以你应该是可以看懂的。
// 基于数组实现的顺序栈 public class ArrayStack { private String[] items; // 数组 private int count; // 栈中元素个数 private int n; //栈的大小 // 初始化数组，申请一个大小为n的数组空间 public ArrayStack(int n) { this.items = new String[n]; this.n = n; this.count = 0; }
// 入栈操作 public boolean push(String item) { // 数组空间不够了，直接返回false，入栈失败。 if (count == n) return false; // 将item放到下标为count的位置，并且count加一 items[count] = item; ++count; return true; }</description></item><item><title>08｜基于TypeScript的虚拟机（一）：实现一个简单的栈机</title><link>https://artisanbox.github.io/3/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/10/</guid><description>你好，我是宫文学。
上一节课，我们已经探讨了设计一个虚拟机所要考虑的那些因素，并做出了一些设计决策。那么今天这一节课，我们就来实现一个初级的虚拟机。
要实现这个初级虚拟机，具体来说，我们要完成下面三方面的工作：
首先，我们要设计一些字节码，来支持第一批语言特性，包括支持函数、变量和整型数据的运算。也就是说，我们的虚拟机要能够支持下面程序的正确运行：
//一个简单的函数，把输入的参数加10，然后返回 function foo(x:number):number{ return x + 10; } //调用foo，并输出结果 println(foo(18)); 第二，我们要做一个字节码生成程序，基于当前的AST生成正确的字节码。
第三，使用TypeScript，实现一个虚拟机的原型系统，验证相关设计概念。
话不多说，开搞，让我们先设计一下字节码吧！
“设计”字节码说是设计，其实我比较懒，更愿意抄袭现成的设计，比如Java的字节码设计。因为Java字节码的资料最充分，比较容易研究，不像V8等的字节码，只有很少的文档资料，探讨的人也很少。另外，学会手工生成Java字节码还有潜在的实用价值，比如你可以把自己的语言编译后直接在JVM上运行。那么我们就先来研究一下Java字节码的特点。
上面的用TypeScript编写的示例代码，如果用Java改写，会变成下面的程序：
//实现同样功能的Java程序。 public class A{ public static int foo(int a){ return a + 10; } public static void main(String args[]){ System.out.println(foo(8)); } } 我们首先把这个Java程序编译成字节码。
javac A.java 这个文件是一个二进制文件。我们可以用hexdump命令查看它的内容。
hexdump -C A.class 从hexdump显示的信息中，你能看到一些可以阅读的字符，比如“java/lang/Object”、"java/lang/System"等等，这些是常量表中的内容。还有一些内容显然不是字符，没法在屏幕上显示，所以hexdump就用一个.号来表示。其中某些字节，代表的是指令，我在图中把代表foo函数、main函数和构造函数的指令标注了出来，这些都是。用于运行的字节码指令，其他都是一些符号表等描述性的信息。
通过上图，你还能得到一个直观的印象：字节码文件并不都是由指令构成的。
没错，指令只是一个程序文件的一部分。除了指令以外，在字节码文件中还要存储不少其他内容，才能保证程序的正常运行，比如类和方法的符号信息、字符串和数字常量，等等。至于字节码文件的格式，是由字节码的规范来规定的，你有兴趣的话，可以按照规范生成这样的字节码文件。这样的话，我们的程序就可以在JVM上运行了。
不过，我现在不想陷入字节码文件格式的细节里，而是想用自己的方式生成字节码文件，够支持现在的语言特性，能够在我们自己的虚拟机上运行就行了。
上面这张图显示的字节码文件不是很容易阅读和理解。所以，我们用javap命令把它转化成文本格式来看看。
javap -v A.class &amp;gt; A.bc 在这个新生成的文件里，我们可以清晰地看到每个函数的定义以及指令，我也在图里标注了主要的指令的含义。
看到这个字节码文件的内容，你可能会直观地觉得：这看上去跟我们的高级语言也没有那么大的区别嘛。程序照样划分成几个函数，只不过每个函数里的语句变成了栈机的指令而已，函数之间照样需要互相调用。
实际上也确实没错。字节码文件里本来就存储了各个类和方法的符号信息，相当于保存了高级语言里的主体框架。当然，每个方法体里的代码就看不出if语句、循环语句这样的结构了，而是变成了字节码的指令。
通过研究这些指令，加上查阅JVM规则中对于字节码的规定，你会发现为了实现上面示例代码中的功能，我们目前只需要这几个指令就够了：
你先花一两分钟看一下这些指令，看上去挺多，其实可以分为几组。
首先是iload系列，这是把指定下标的本地变量入栈。注意，变量的下标是由声明的顺序决定的，参数也算本地变量，并且排在最前面。所以，iload 0的意思，就是把第一个参数入栈。如果没有参数，就是把第一个本地变量入栈。
iload后面的那几个指令，是压缩格式的指令，也就是利用指令末尾富余的位，把操作数和指令压缩在了一起，这样可以少一个字节码，能够缩小最后生成的字节码文件的大小。从这里面，你能借鉴到字节码设计的一些好的实践。所以你看，学习成熟的设计是有好处的吧？
第二组是istore系列，它做的工作刚好跟iload相反，是把栈顶的值存到指定下标的变量里去。
第三组，是对常数做入栈的操作。对于0~5这几个数字，Java字节码也是提供了压缩格式的指令。对于8位整数（-128~127），使用bipush指令。对于16位整数（-32768~32767），使用sipush指令。而对于更大的常数，则要使用ldc指令，从常量池里去取。
第四组，是几个二元运算的指令。它们都是从栈里取两个操作数，计算完毕之后，再压回栈里。</description></item><item><title>09_普通索引和唯一索引，应该怎么选择？</title><link>https://artisanbox.github.io/1/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/9/</guid><description>今天的正文开始前，我要特意感谢一下评论区几位留下高质量留言的同学。
用户名是 @某、人 的同学，对文章的知识点做了梳理，然后提了关于事务可见性的问题，就是先启动但是后提交的事务，对数据可见性的影响。@夏日雨同学也提到了这个问题，我在置顶评论中回复了，今天的文章末尾也会再展开说明。@Justin和@倪大人两位同学提了两个好问题。
对于能够引发更深一步思考的问题，我会在回复的内容中写上“好问题”三个字，方便你搜索，你也可以去看看他们的留言。
非常感谢大家很细致地看文章，并且留下了那么多和很高质量的留言。知道文章有给大家带来一些新理解，对我来说是一个很好的鼓励。同时，也让其他认真看评论区的同学，有机会发现一些自己还没有意识到的、但可能还不清晰的知识点，这也在总体上提高了整个专栏的质量。再次谢谢你们。
好了，现在就回到我们今天的正文内容。
在前面的基础篇文章中，我给你介绍过索引的基本概念，相信你已经了解了唯一索引和普通索引的区别。今天我们就继续来谈谈，在不同的业务场景下，应该选择普通索引，还是唯一索引？
假设你在维护一个市民系统，每个人都有一个唯一的身份证号，而且业务代码已经保证了不会写入两个重复的身份证号。如果市民系统需要按照身份证号查姓名，就会执行类似这样的SQL语句：
select name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz'; 所以，你一定会考虑在id_card字段上建索引。
由于身份证号字段比较大，我不建议你把身份证号当做主键，那么现在你有两个选择，要么给id_card字段创建唯一索引，要么创建一个普通索引。如果业务代码已经保证了不会写入重复的身份证号，那么这两个选择逻辑上都是正确的。
现在我要问你的是，从性能的角度考虑，你选择唯一索引还是普通索引呢？选择的依据是什么呢？
简单起见，我们还是用第4篇文章《深入浅出索引（上）》中的例子来说明，假设字段 k 上的值都不重复。
图1 InnoDB的索引组织结构接下来，我们就从这两种索引对查询语句和更新语句的性能影响来进行分析。
查询过程假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的过程，先是通过B+树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。
对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。
你知道的，InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB。
因为引擎是按页读写的，所以说，当找到k=5的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。
当然，如果k=5这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。
但是，我们之前计算过，对于整型字段，一个数据页可以放近千个key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的CPU来说可以忽略不计。
更新过程为了说明普通索引和唯一索引对更新语句性能的影响这个问题，我需要先跟你介绍一下change buffer。
当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。
需要说明的是，虽然名字叫作change buffer，实际上它是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。
将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。
显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。
那么，什么条件下可以使用change buffer呢？
对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入(4,400)这个记录，就要先判断现在表中是否已经存在k=4的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了。
因此，唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。
change buffer用的是buffer pool里的内存，因此不能无限增大。change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。
现在，你已经理解了change buffer的机制，那么我们再一起来看看如果要在这张表中插入一个新记录(4,400)的话，InnoDB的处理流程是怎样的。
第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB的处理流程如下：
对于唯一索引来说，找到3和5之间的位置，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，找到3和5之间的位置，插入这个值，语句执行结束。 这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的CPU时间。</description></item><item><title>09_程序装载：“640K内存”真的不够用么？</title><link>https://artisanbox.github.io/4/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/9/</guid><description>计算机这个行业的历史上有过很多成功的预言，最著名的自然是“摩尔定律”。当然免不了的也有很多“失败”的预测，其中一个最著名的就是，比尔·盖茨在上世纪80年代说的“640K ought to be enough for anyone”，也就是“640K内存对哪个人来说都够用了”。
那个年代，微软开发的还是DOS操作系统，程序员们还在绞尽脑汁，想要用好这极为有限的640K内存。而现在，我手头的开发机已经是16G内存了，上升了一万倍还不止。那比尔·盖茨这句话在当时也是完全的无稽之谈么？有没有哪怕一点点的道理呢？这一讲里，我就和你一起来看一看。
程序装载面临的挑战上一讲，我们看到了如何通过链接器，把多个文件合并成一个最终可执行文件。在运行这些可执行文件的时候，我们其实是通过一个装载器，解析ELF或者PE格式的可执行文件。装载器会把对应的指令和数据加载到内存里面来，让CPU去执行。
说起来只是装载到内存里面这一句话的事儿，实际上装载器需要满足两个要求。
第一，可执行程序加载后占用的内存空间应该是连续的。我们在第6讲讲过，执行指令的时候，程序计数器是顺序地一条一条指令执行下去。这也就意味着，这一条条指令需要连续地存储在一起。
第二，我们需要同时加载很多个程序，并且不能让程序自己规定在内存中加载的位置。虽然编译出来的指令里已经有了对应的各种各样的内存地址，但是实际加载的时候，我们其实没有办法确保，这个程序一定加载在哪一段内存地址上。因为我们现在的计算机通常会同时运行很多个程序，可能你想要的内存地址已经被其他加载了的程序占用了。
要满足这两个基本的要求，我们很容易想到一个办法。那就是我们可以在内存里面，找到一段连续的内存空间，然后分配给装载的程序，然后把这段连续的内存空间地址，和整个程序指令里指定的内存地址做一个映射。
我们把指令里用到的内存地址叫作虚拟内存地址（Virtual Memory Address），实际在内存硬件里面的空间地址，我们叫物理内存地址（Physical Memory Address）。
程序里有指令和各种内存地址，我们只需要关心虚拟内存地址就行了。对于任何一个程序来说，它看到的都是同样的内存地址。我们维护一个虚拟内存到物理内存的映射表，这样实际程序指令执行的时候，会通过虚拟内存地址，找到对应的物理内存地址，然后执行。因为是连续的内存地址空间，所以我们只需要维护映射关系的起始地址和对应的空间大小就可以了。
内存分段这种找出一段连续的物理内存和虚拟内存地址进行映射的方法，我们叫分段（Segmentation）。这里的段，就是指系统分配出来的那个连续的内存空间。
分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处，第一个就是内存碎片（Memory Fragmentation）的问题。
我们来看这样一个例子。我现在手头的这台电脑，有1GB的内存。我们先启动一个图形渲染程序，占用了512MB的内存，接着启动一个Chrome浏览器，占用了128MB内存，再启动一个Python程序，占用了256MB内存。这个时候，我们关掉Chrome，于是空闲内存还有1024 - 512 - 256 = 256MB。按理来说，我们有足够的空间再去装载一个200MB的程序。但是，这256MB的内存空间不是连续的，而是被分成了两段128MB的内存。因此，实际情况是，我们的程序没办法加载进来。
当然，这个我们也有办法解决。解决的办法叫内存交换（Memory Swapping）。
我们可以把Python程序占用的那256MB内存写到硬盘上，然后再从硬盘上读回来到内存里面。不过读回来的时候，我们不再把它加载到原来的位置，而是紧紧跟在那已经被占用了的512MB内存后面。这样，我们就有了连续的256MB内存空间，就可以去加载一个新的200MB的程序。如果你自己安装过Linux操作系统，你应该遇到过分配一个swap硬盘分区的问题。这块分出来的磁盘空间，其实就是专门给Linux操作系统进行内存交换用的。
虚拟内存、分段，再加上内存交换，看起来似乎已经解决了计算机同时装载运行很多个程序的问题。不过，你千万不要大意，这三者的组合仍然会遇到一个性能瓶颈。硬盘的访问速度要比内存慢很多，而每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。所以，如果内存交换的时候，交换的是一个很占内存空间的程序，这样整个机器都会显得卡顿。
内存分页既然问题出在内存碎片和内存交换的空间太大上，那么解决问题的办法就是，少出现一些内存碎片。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决这个问题。这个办法，在现在计算机的内存管理里面，就叫作内存分页（Paging）。
和分段这样分配一整段连续的空间给到程序相比，分页是把整个物理内存空间切成一段段固定尺寸的大小。而对应的程序所需要占用的虚拟内存空间，也会同样切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。从虚拟内存到物理内存的映射，不再是拿整段连续的内存的物理地址，而是按照一个一个页来的。页的尺寸一般远远小于整个程序的大小。在Linux下，我们通常只设置成4KB。你可以通过命令看看你手头的Linux系统设置的页的大小。
$ getconf PAGE_SIZE 由于内存空间都是预先划分好的，也就没有了不能使用的碎片，而只有被释放出来的很多4KB的页。即使内存空间不够，需要让现有的、正在运行的其他程序，通过内存交换释放出一些内存的页出来，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，让整个机器被内存交换的过程给卡住。
更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是只在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。
实际上，我们的操作系统，的确是这么做的。当要读取特定的页，却发现数据并没有加载到物理内存里的时候，就会触发一个来自于CPU的缺页错误（Page Fault）。我们的操作系统会捕捉到这个错误，然后将对应的页，从存放在硬盘上的虚拟内存里读取出来，加载到物理内存里。这种方式，使得我们可以运行那些远大于我们实际物理内存的程序。同时，这样一来，任何程序都不需要一次性加载完所有指令和数据，只需要加载当前需要用到就行了。
通过虚拟内存、内存交换和内存分页这三个技术的组合，我们最终得到了一个让程序不需要考虑实际的物理内存地址、大小和当前分配空间的解决方案。这些技术和方法，对于我们程序的编写、编译和链接过程都是透明的。这也是我们在计算机的软硬件开发中常用的一种方法，就是加入一个间接层。
通过引入虚拟内存、页映射和内存交换，我们的程序本身，就不再需要考虑对应的真实的内存地址、程序加载、内存管理等问题了。任何一个程序，都只需要把内存当成是一块完整而连续的空间来直接使用。
总结延伸现在回到开头我问你的问题，我们的电脑只要640K内存就够了吗？很显然，现在来看，比尔·盖茨的这个判断是不合理的，那为什么他会这么认为呢？因为他也是一个很优秀的程序员啊！
在虚拟内存、内存交换和内存分页这三者结合之下，你会发现，其实要运行一个程序，“必需”的内存是很少的。CPU只需要执行当前的指令，极限情况下，内存也只需要加载一页就好了。再大的程序，也可以分成一页。每次，只在需要用到对应的数据和指令的时候，从硬盘上交换到内存里面来就好了。以我们现在4K内存一页的大小，640K内存也能放下足足160页呢，也无怪乎在比尔·盖茨会说出“640K ought to be enough for anyone”这样的话。
不过呢，硬盘的访问速度比内存慢很多，所以我们现在的计算机，没有个几G的内存都不好意思和人打招呼。
那么，除了程序分页装载这种方式之外，我们还有其他优化内存使用的方式么？下一讲，我们就一起来看看“动态装载”，学习一下让两个不同的应用程序，共用一个共享程序库的办法。
推荐阅读想要更深入地了解代码装载的详细过程，推荐你阅读《程序员的自我修养——链接、装载和库》的第1章和第6章。
课后思考请你想一想，在Java这样使用虚拟机的编程语言里面，我们写的程序是怎么装载到内存里面来的呢？它也和我们讲的一样，是通过内存分页和内存交换的方式加载到内存里面来的么？
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>09_队列：队列在线程池等有限资源池中的应用</title><link>https://artisanbox.github.io/2/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/10/</guid><description>我们知道，CPU资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致CPU频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。
当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？
实际上，这些问题并不复杂，其底层的数据结构就是我们今天要学的内容，队列（queue）。
如何理解“队列”？队列这个概念非常好理解。你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的“队列”。
我们知道，栈只支持两个基本操作：入栈push()和出栈pop()。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队enqueue()，放一个数据到队列尾部；出队dequeue()，从队列头部取一个元素。
所以，队列跟栈一样，也是一种操作受限的线性表数据结构。
队列的概念很好理解，基本操作也很容易掌握。作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列Disruptor、Linux环形缓存，都用到了循环并发队列；Java concurrent并发包利用ArrayBlockingQueue来实现公平锁等。
顺序队列和链式队列我们知道了，队列跟栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持在队尾插入元素，在队头删除元素，那究竟该如何实现一个队列呢？
跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。同样，用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。
我们先来看下基于数组的实现方法。我用Java语言实现了一下，不过并不包含Java语言的高级语法，而且我做了比较详细的注释，你应该可以看懂。
// 用数组实现的队列 public class ArrayQueue { // 数组：items，数组大小：n private String[] items; private int n = 0; // head表示队头下标，tail表示队尾下标 private int head = 0; private int tail = 0; // 申请一个大小为capacity的数组 public ArrayQueue(int capacity) { items = new String[capacity]; n = capacity; }
// 入队 public boolean enqueue(String item) { // 如果tail == n 表示队列已经满了 if (tail == n) return false; items[tail] = item; ++tail; return true; }</description></item><item><title>09｜基于TypeScript的虚拟机（二）：丰富特性，支持跳转语句</title><link>https://artisanbox.github.io/3/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/11/</guid><description>你好，我是宫文学。
在上一节课里，我们已经实现了一个简单的虚拟机。不过，这个虚拟机也太简单了，实在是不够实用啊。
那么，今天这节课，我们就来增强一下当前的虚拟机，让它的特性更丰富一些，也为我们后续的工作做好铺垫，比如用C语言实现一个更强的虚拟机。
我们在这一节课有两项任务要完成：
首先，要支持if语句和for循环语句。这样，我们就能熟悉与程序分支有关的指令，并且还能让虚拟机支持复杂一点的程序，比如我们之前写过的生成斐波那契数列的程序。
第二，做一下性能比拼。既然我们已经完成了字节码虚拟机的开发，那就跟AST解释器做一些性能测试，看看性能到底差多少。
话不多说，开干！首先，我们来实现一下if语句和for循环语句。而实现这两个语句的核心，就是要支持跳转指令。
了解跳转指令if语句和for循环语句，有一个特点，就是让程序根据一定的条件执行不同的代码。这样一个语法，比较适合我们人类阅读，但是对于机器执行并不方便。机器执行的代码，都是一条条指令排成的直线型的代码，但是可以根据需要跳转到不同的指令去执行。
针对这样的差异，编译器就需要把if和for这样结构化编程的代码，转变成通过跳转指令跳转的代码，其中的关键是计算出正确的跳转地址。
我们举个例子来说明一下，下面是我用Java写的一个示例程序，它有一个if语句。
public static int foo3(int a){ int b; if (a &amp;gt; 10){ b = a + 8; } else{ b = a - 8; } return b; } 我们先用javac命令编译成.class文件，然后再用javap命令以文本方式显示生成的字节码：
我主要是想通过这个示例程序，给你展现两条跳转指令。
一条是if_icmple，它是一条有条件的跳转指令。它给栈顶的两个元素做&amp;lt;=（less equal，缩写为le）运算。如果计算结果为真，那么就跳转到分支地址14。
你可能会发现一个问题，为什么我们的源代码是&amp;gt;号，翻译成字节码却变成了&amp;lt;=号了呢？没错，虽然符号变了，但其实我们的语义并没有发生变化。
我给你分析一下，在源代码里面，程序用&amp;gt;号计算为真，就执行if下面的块；那就意味着如果&amp;gt;号计算为假，或者说&amp;lt;=号为真，则跳转到else下面的那个块，这两种说法是等价的。
但现在我们要生成的是跳转指令，所以用&amp;lt;=做判断，然后再跳转，就是比较自然了。具体你可以看看我们下文中为if语句生成代码的逻辑和相关的图，就更容易理解了。
你在字节码中还会看到另一个跳转指令，是goto指令，它是一个无条件跳转指令。
在计算机语言发展的早期，人们用高级语言写程序的时候，也会用很多goto语句，导致程序非常难以阅读，程序的控制流理解起来困难。虽然直到今天，C和C++语言里还保留了goto语句。不过，一般不到迫不得已，你不应该使用goto语句。
这种迫不得已的情况，我指的是使用goto语句实现一些奇特的效果，这些效果是用结构化编程方式（也就是不用goto语句，而是用条件语句和循环语句表达程序分支）无法完成的。比如，采用goto语句能够从一个嵌套很深的语句块，一下子跳到外面，然后还能再跳进去，接着继续执行！这相当于能够暂停一个执行到一半的程序，然后需要时再恢复上下文，接着执行。
我在说什么呢？这可以跟协程的实现机制关联起来，协程要求在应用层把一个程序停止下来，然后在需要的时候再继续执行。那么利用C/C++的goto语句的无条件跳转能力，你其实就可以实现一个协程库，如果你想了解得更具体一些，可以看看我之前的《编译原理实战课》。
总结起来，goto的这种跳转方式，是更加底层的一种机制。所以，在编译程序的过程中，我们会多次变换程序的表达方式，让它越来越接近计算机容易理解的形式，这个过程叫做Lower过程。而Lower到一定程度，就会形成线性代码加跳转语句的代码格式，我们有时候就会把这种格式的IR叫做“goto格式（goto form）”。
好了，刚才聊的关于goto语句的这些知识点，是为了加深大家对它的认识，希望能够对你的编程思想有所启发。
回到正题，现在我们已经对跳转指令有了基本的认识，那么我就把接下来要用到的跳转指令列出来，你可以看看下面这两张表：
如果后面要增加对浮点数和对象引用的比较功能，我们可以再增加一些指令。但由于目前我们还是只处理整数，所以这些指令就够了。
接着，我们就修改一下字节码生成程序和虚拟机中的执行引擎，让它们能够支持if语句和for语句。
为if语句和for循环语句生成字节码让if语句生成字节码的代码你可以参考visitIfStatement方法。在这个方法里，我们首先为if条件、if后面的块、else块分别生成了字节码。
//条件表达式的代码 let code_condition:number[] = this.visit(ifstmt.condition); //if块的代码 let code_ifBlock:number[] = this.visit(ifstmt.stmt); //else块的代码 let code_elseBlock:number[] = (ifstmt.</description></item><item><title>10_MySQL为什么有时候会选错索引？</title><link>https://artisanbox.github.io/1/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/10/</guid><description>前面我们介绍过索引，你已经知道了在MySQL中一张表其实是可以支持多个索引的。但是，你写SQL语句的时候，并没有主动指定使用哪个索引。也就是说，使用哪个索引是由MySQL来确定的。
不知道你有没有碰到过这种情况，一条本来可以执行得很快的语句，却由于MySQL选错了索引，而导致执行速度变得很慢？
我们一起来看一个例子吧。
我们先建一个简单的表，表里有a、b两个字段，并分别建上索引：
CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `b` (`b`) ) ENGINE=InnoDB; 然后，我们往表t中插入10万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到(100000,100000,100000)。
我是用存储过程来插入数据的，这里我贴出来方便你复现：
delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=100000)do insert into t values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 接下来，我们分析一条SQL语句：
mysql&amp;gt; select * from t where a between 10000 and 20000; 你一定会说，这个语句还用分析吗，很简单呀，a上有索引，肯定是要使用索引a的。</description></item><item><title>10_动态链接：程序内部的“共享单车”</title><link>https://artisanbox.github.io/4/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/10/</guid><description>我们之前讲过，程序的链接，是把对应的不同文件内的代码段，合并到一起，成为最后的可执行文件。这个链接的方式，让我们在写代码的时候做到了“复用”。同样的功能代码只要写一次，然后提供给很多不同的程序进行链接就行了。
这么说来，“链接”其实有点儿像我们日常生活中的标准化、模块化生产。我们有一个可以生产标准螺帽的生产线，就可以生产很多个不同的螺帽。只要需要螺帽，我们都可以通过链接的方式，去复制一个出来，放到需要的地方去，大到汽车，小到信箱。
但是，如果我们有很多个程序都要通过装载器装载到内存里面，那里面链接好的同样的功能代码，也都需要再装载一遍，再占一遍内存空间。这就好比，假设每个人都有骑自行车的需要，那我们给每个人都生产一辆自行车带在身边，固然大家都有自行车用了，但是马路上肯定会特别拥挤。
链接可以分动、静，共享运行省内存我们上一节解决程序装载到内存的时候，讲了很多方法。说起来，最根本的问题其实就是内存空间不够用。如果我们能够让同样功能的代码，在不同的程序里面，不需要各占一份内存空间，那该有多好啊！就好比，现在马路上的共享单车，我们并不需要给每个人都造一辆自行车，只要马路上有这些单车，谁需要的时候，直接通过手机扫码，都可以解锁骑行。
这个思路就引入一种新的链接方法，叫作动态链接（Dynamic Link）。相应的，我们之前说的合并代码段的方法，就是静态链接（Static Link）。
在动态链接的过程中，我们想要“链接”的，不是存储在硬盘上的目标文件代码，而是加载到内存中的共享库（Shared Libraries）。顾名思义，这里的共享库重在“共享“这两个字。
这个加载到内存中的共享库会被很多个程序的指令调用到。在Windows下，这些共享库文件就是.dll文件，也就是Dynamic-Link Libary（DLL，动态链接库）。在Linux下，这些共享库文件就是.so文件，也就是Shared Object（一般我们也称之为动态链接库）。这两大操作系统下的文件名后缀，一个用了“动态链接”的意思，另一个用了“共享”的意思，正好覆盖了两方面的含义。
地址无关很重要，相对地址解烦恼不过，要想要在程序运行的时候共享代码，也有一定的要求，就是这些机器码必须是“地址无关”的。也就是说，我们编译出来的共享库文件的指令代码，是地址无关码（Position-Independent Code）。换句话说就是，这段代码，无论加载在哪个内存地址，都能够正常执行。如果不是这样的代码，就是地址相关的代码。
如果还不明白，我给你举一个生活中的例子。如果我们有一个骑自行车的程序，要“前进500米，左转进入天安门广场，再前进500米”。它在500米之后要到天安门广场了，这就是地址相关的。如果程序是“前进500米，左转，再前进500米”，无论你在哪里都可以骑车走这1000米，没有具体地点的限制，这就是地址无关的。
你可以想想，大部分函数库其实都可以做到地址无关，因为它们都接受特定的输入，进行确定的操作，然后给出返回结果就好了。无论是实现一个向量加法，还是实现一个打印的函数，这些代码逻辑和输入的数据在内存里面的位置并不重要。
而常见的地址相关的代码，比如绝对地址代码（Absolute Code）、利用重定位表的代码等等，都是地址相关的代码。你回想一下我们之前讲过的重定位表。在程序链接的时候，我们就把函数调用后要跳转访问的地址确定下来了，这意味着，如果这个函数加载到一个不同的内存地址，跳转就会失败。
对于所有动态链接共享库的程序来讲，虽然我们的共享库用的都是同一段物理内存地址，但是在不同的应用程序里，它所在的虚拟内存地址是不同的。我们没办法、也不应该要求动态链接同一个共享库的不同程序，必须把这个共享库所使用的虚拟内存地址变成一致。如果这样的话，我们写的程序就必须明确地知道内部的内存地址分配。
那么问题来了，我们要怎么样才能做到，动态共享库编译出来的代码指令，都是地址无关码呢？
动态代码库内部的变量和函数调用都很容易解决，我们只需要使用相对地址（Relative Address）就好了。各种指令中使用到的内存地址，给出的不是一个绝对的地址空间，而是一个相对于当前指令偏移量的内存地址。因为整个共享库是放在一段连续的虚拟内存地址中的，无论装载到哪一段地址，不同指令之间的相对地址都是不变的。
PLT和GOT，动态链接的解决方案要实现动态链接共享库，也并不困难，和前面的静态链接里的符号表和重定向表类似，还是和前面一样，我们还是拿出一小段代码来看一看。
首先，lib.h 定义了动态链接库的一个函数 show_me_the_money。
// lib.h #ifndef LIB_H #define LIB_H void show_me_the_money(int money);
#endif lib.c包含了lib.h的实际实现。
// lib.c #include &amp;lt;stdio.h&amp;gt;
void show_me_the_money(int money) { printf(&amp;quot;Show me USD %d from lib.c \n&amp;quot;, money); } 然后，show_me_poor.c 调用了 lib 里面的函数。
// show_me_poor.c #include &amp;quot;lib.h&amp;quot; int main() { int money = 5; show_me_the_money(money); } 最后，我们把 lib.</description></item><item><title>10_递归：如何用三行代码找到“最终推荐人”？</title><link>https://artisanbox.github.io/2/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/11/</guid><description>推荐注册返佣金的这个功能我想你应该不陌生吧？现在很多App都有这个功能。这个功能中，用户A推荐用户B来注册，用户B又推荐了用户C来注册。我们可以说，用户C的“最终推荐人”为用户A，用户B的“最终推荐人”也为用户A，而用户A没有“最终推荐人”。
一般来说，我们会通过数据库来记录这种推荐关系。在数据库表中，我们可以记录两行数据，其中actor_id表示用户id，referrer_id表示推荐人id。
基于这个背景，我的问题是，给定一个用户ID，如何查找这个用户的“最终推荐人”？ 带着这个问题，我们来学习今天的内容，递归（Recursion）！
如何理解“递归”？从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。
递归是一种应用非常广泛的算法（或者编程技巧）。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如DFS深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。
不过，别看我说了这么多，递归本身可是一点儿都不“高冷”，咱们生活中就有很多用到递归的例子。
周末你带着女朋友去电影院看电影，女朋友问你，咱们现在坐在第几排啊？电影院里面太黑了，看不清，没法数，现在你怎么办？
别忘了你是程序员，这个可难不倒你，递归就开始排上用场了。于是你就问前面一排的人他是第几排，你想只要在他的数字上加一，就知道自己在哪一排了。但是，前面的人也看不清啊，所以他也问他前面的人。就这样一排一排往前问，直到问到第一排的人，说我在第一排，然后再这样一排一排再把数字传回来。直到你前面的人告诉你他在哪一排，于是你就知道答案了。
这就是一个非常标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。基本上，所有的递归问题都可以用递推公式来表示。刚刚这个生活中的例子，我们用递推公式将它表示出来就是这样的：
f(n)=f(n-1)+1 其中，f(1)=1 f(n)表示你想知道自己在哪一排，f(n-1)表示前面一排所在的排数，f(1)=1表示第一排的人知道自己在第一排。有了这个递推公式，我们就可以很轻松地将它改为递归代码，如下：
int f(int n) { if (n == 1) return 1; return f(n-1) + 1; } 递归需要满足的三个条件刚刚这个例子是非常典型的递归，那究竟什么样的问题可以用递归来解决呢？我总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决。
1.一个问题的解可以分解为几个子问题的解
何为子问题？子问题就是数据规模更小的问题。比如，前面讲的电影院的例子，你要知道，“自己在哪一排”的问题，可以分解为“前一排的人在哪一排”这样一个子问题。
2.这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
比如电影院那个例子，你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。
3.存在递归终止条件
把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。
还是电影院的例子，第一排的人不需要再继续询问任何人，就知道自己在哪一排，也就是f(1)=1，这就是递归的终止条件。
如何编写递归代码？刚刚铺垫了这么多，现在我们来看，如何来写递归代码？我个人觉得，写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。
你先记住这个理论。我举一个例子，带你一步一步实现一个递归代码，帮你理解。
假如这里有n个台阶，每次你可以跨1个台阶或者2个台阶，请问走这n个台阶有多少种走法？如果有7个台阶，你可以2，2，2，1这样子上去，也可以1，2，1，1，2这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？
我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了1个台阶，另一类是第一步走了2个台阶。所以n个台阶的走法就等于先走1阶后，n-1个台阶的走法 加上先走2阶后，n-2个台阶的走法。用公式表示就是：
f(n) = f(n-1)+f(n-2) 有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以f(1)=1。这个递归终止条件足够吗？我们可以用n=2，n=3这样比较小的数试验一下。
n=2时，f(2)=f(1)+f(0)。如果递归终止条件只有一个f(1)=1，那f(2)就无法求解了。所以除了f(1)=1这一个递归终止条件外，还要有f(0)=1，表示走0个台阶有一种走法，不过这样子看起来就不符合正常的逻辑思维了。所以，我们可以把f(2)=2作为一种终止条件，表示走2个台阶，有两种走法，一步走完或者分两步来走。
所以，递归终止条件就是f(1)=1，f(2)=2。这个时候，你可以再拿n=3，n=4来验证一下，这个终止条件是否足够并且正确。
我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：
f(1) = 1; f(2) = 2; f(n) = f(n-1)+f(n-2) 有了这个公式，我们转化成递归代码就简单多了。最终的递归代码是这样的：
int f(int n) { if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2); } 我总结一下，写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。</description></item><item><title>10｜基于C语言的虚拟机（一）：实现一个简单的栈机</title><link>https://artisanbox.github.io/3/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/12/</guid><description>你好，我是宫文学。
到目前为止，我们已经用TypeScript实现了一个小而全的虚拟机，也在这个过程中稍微体会了一下虚拟机设计的一些要点，比如字节码的设计、指令的生成和栈机的运行机理等等，而且我们还通过性能测试，也看到了栈机确实比AST解释器的性能更高。
虽然，上面这些工作我们都是用TypeScript实现的，但既然我们已经生成了字节码，我不由地产生了一个想法：我们能不能用C语言这样的更基础的语言来实现一个虚拟机，同样来运行这些字节码呢？
我这样的想法可不是凭空产生的。你看，字节码最大的好处，就是和平台无关的能力。不管什么平台，只要有个虚拟机，就可以运行字节码，这也是安卓平台一开始选择字节码作为运行机制的原因。你甚至也可以来试一试，假设现在时间回到智能手机刚出现的时代，你是否也能够快速设计一个虚拟机，来运行手机上的应用呢？
那么进一步，在这种移动设备上运行的应用，很重要的功能就是去调用底层操作系统的API。用C和C++实现的虚拟机，显然在这方面有优势，能够尽量降低由于ABI转换所带来的性能损失。
所以，这一节课，我就带你用C语言重新实现一遍虚拟机。在这个过程中，你会对字节码文件的设计有更细致的体会，对于符号表的作用的理解也会加深，也会掌握如何用C语言设计栈桢的知识点。
好了，我们首先实现第一步的目标，把程序保存成字节码文件，再把字节码文件加载到内存。
读写字节码文件在TypeScript版的虚拟机中，我们用了模块（BCModule）来保存程序的相关信息，有了这样一个模块，程序就可以生成一个独立的字节码文件，就像Java语言里，每个.java文件会编译生成一个.class文件那样。
首先，我们要先来定字节码的文件格式。
我们也说过，Java语言的字节码文件，是依据了专门的技术规范。其实我们仍然可以采用Java字节码文件的格式，你查阅相应的技术规格就可以。这样的话，我们编译后的结果，就直接可以用Java虚拟机来运行了！
有时间的同学可以做一下这个尝试。这项工作在某些场景下会很有意义。你可以定义自己的DSL，直接生成字节码，跟Java编写的程序一起混合运行。我认识的一位极客朋友就做了类似的使用，用低代码的编程界面直接生成了.NET的字节码，形成了一个基于Unity的游戏开发平台。
不过，我们目前的语言比较简单，所以不用遵循那么复杂的规范，我们就设计自己的文件格式就好了。
第二，我们需要考虑：保存什么信息到字节码文件里，才足够用于程序的运行？
从我们目前实现的虚拟机来看，其实不需要太多的信息。你可以回忆一下，其实要保证程序的运行，只需要能够从常量表里查找到函数的一些基本信息即可，最重要的信息包括：
这个函数的字节码； 这个函数有几个本地变量？我们需要在栈桢里保留存储位置； 这个函数的操作数栈的最大尺寸是多少？也就是最多的时候，需要在栈里保存几个操作数，以便我们预留存储空间。 除了这些信息外，再就是我们的代码里用到了的部分数字常量，也需要从常量表里加载，就像ldc指令那样。
所以说，只要把函数常量、数字常量存成字节码文件，就足够我们现在的虚拟机使用了。你也可以看到，我们现在甚至连函数名称、函数的签名都不需要，如果需要的话，也是为了在运行期来显示错误信息而已。
不过，如果把函数名称和函数签名的信息加进去，会有利于我们实现多模块的运行机制。也就是说，如果一个模块中的函数要调用另一个模块中的函数，那么我们可以创造一种机制，实现模块之间的代码查找。
这其实就是Java的类加载机制和多个.class文件之间互相调用的机制，我们仍然可以借鉴。而且，即使像C语言那样的编译成本地代码的语言，也是通过暴露出函数签名的信息，来实现多个模块之间的静态链接和动态链接机制的。
那再进一步，既然需要函数签名，那么我们就需要知道一些类型信息，比如往函数里要传递什么类型的参数，返回的是什么类型的数据，这样调用者和被调用者之间才能无缝衔接到一起。
像C语言这样的系统语言，以及在操作系统的ABI里，支持的都是一些基础的数据类型的信息，比如整数、浮点数、整数指针、字符串指针之类的。而像Java等语言，它们建立了具有较高抽象度的类型体系，还可以包含这些高级的类型信息，从而实现像运行时的类型判断、通过反省的方式动态运行程序等高级功能。
上面这几段的分析总结起来，就是我们需要往目标文件里或多或少地保存一些类型信息。
那么现在就清楚了，我们需要把常量信息和类型信息写到字节码里，就足够程序运行了。
现在，我们来到了第三个步骤：序列化。
具体来说，序列化就是把这些信息以一定的格式写到文件里，再从文件里恢复的过程，是一个比较啰嗦的、充满细节的过程。也就是说，我们在内存里是一种比较结构化的数据，而在文件里保存，或者通过网络传输，都是采用一个线性的数据结构。
在我的编程经验里，所有这些序列化的工作都比较繁琐，但大致的实现方式都是一样的。无论是保存成二进制格式、XML格式、json格式，还是基于一种网络协议在网络上传输，都是一个把内存中的数据结构变成线性的数据结构，然后再从线性的数据结构中恢复的过程。
你可以看看BCModuleWriter和BCModuleReader中的代码，实现的技巧也很简单，最重要的就是你要知道每个数据占了多少个字节。比如，当你向文件里写一个字符串的时候，你先要写下字符串的长度，再写字符串的实际数据，用这样的方法，当你读文件的时候，就能把相关信息顺利还原了。
这类程序中稍微有点难的地方，是保证对象之间正确的引用关系。比如，函数引用了变量和类型，而高级的类型之间也是互相有引用关系的，比如子类型的关系等等，这样就构成了一张网状的数据结构，相互之间有引用。
当你写入文件的时候，要注意，这个网的每个节点只能写一次，不能因为两个函数的返回值都引用了某个类型，就把这个类型写了两次。在读的时候呢，则要重新建立起对象之间正确的引用关系。
好了，了解了实现思路以后，再阅读相应的示例代码就很容易了。在TypeScript中，我用BCModuleWriter把斐波那契数列程序的字节码写成了文件，然后用hexdump命令来显示一下看看：
乍一看，这个跟Java的字节码文件还挺像的，不过我们用的是自己的简单格式。我在图中做了标注，标明了字节是什么含义。其中_main函数和fibonacci函数的字节码指令，我也标了出来。
之后，我可以用BCModuleReader把这个字节码文件再读入内存，重建BCModule，包括里面的符号信息。如果基于这个新的BCModule，程序同样可以顺畅地运行，那就说明我们的字节码文件里面确实包含了足够的运行信息。
好了，现在我们的字节码文件以及相应的读写机制已经设计成功，也用TypeScript做完了所有的设计验证。在这个基础上，重新用C语言实现一个虚拟机，就是一个比较简单的事情了。你可以发现，虽然我们的语言换了，但虚拟机的实现机制没有变。
接下来，我们就需要用C语言把字节码文件读到内存，并在内存重建BCModule相关的各种对象结构。
用C语言读入字节码文件关于C语言版本的字节码读取程序，你可以参考一下readBCModule函数的代码。在读取了字节码文件以后，我还写了一个dumpBCModule的函数，可以在控制台显示BCModule的信息，如下图所示：
你可能会注意到，我们最后的模型里的类型信息和函数都比字节码文件里的要多。不要担心，多出来的其实是系统内置的类型（比如number类型）和内置函数（比如println），它们不需要被保存在字节码文件里，但是会被我们的程序引用到，所以我们要在内存的数据结构中体现。
在这里，我重新梳理一下内存里的对象模型，这个对象模型就是我们运行时所需要的所有信息。我们读取了字节码文件以后，会在内存里形成这个结构化的对象模型，来代表一个程序的信息。
这里我再讲一个小技术点，看着上面的类图，你可能会问：C语言不是不支持面向对象吗？你为什么还能用面向对象的方式来保存这些信息？
其实，用C语言也能模拟类似面向对象的机制。以符号为例，我们是这样声明Symbol和FunctionSymbol的，让FunctionSymbol包含基类Symbol中的数据：
typedef struct _Symbol{ char* name; //符号名称 Type* theType; //类型 SymKind kind; //符号种类 } Symbol; typedef struct _FunctionSymbol{ Symbol symbol; //基类数据 int numVars; //本地变量数量 VarSymbol ** vars; //本地变量信息 int opStackSize; //操作数栈大小 int numByteCodes; //字节码数量 unsigned char* byteCode; //字节码指令 } FunctionSymbol; 在内存里，FunctionSymbol最前面的字段，就是Symbol的字段，因此你可以把FunctionSymbol的指针强制转换成Symbol的指针，从而访问Symbol的字段。这种编程方式在一些用C语言编写的系统软件里非常普遍，包括其他作者写的一些编译器的代码，以及Linux操作系统内核中的代码中都有体现。</description></item><item><title>11_二进制编码：“手持两把锟斤拷，口中疾呼烫烫烫”？</title><link>https://artisanbox.github.io/4/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/11/</guid><description>上算法和数据结构课的时候，老师们都会和你说，程序 = 算法 + 数据结构。如果对应到组成原理或者说硬件层面，算法就是我们前面讲的各种计算机指令，数据结构就对应我们接下来要讲的二进制数据。
众所周知，现代计算机都是用0和1组成的二进制，来表示所有的信息。前面几讲的程序指令用到的机器码，也是使用二进制表示的；我们存储在内存里面的字符串、整数、浮点数也都是用二进制表示的。万事万物在计算机里都是0和1，所以呢，搞清楚各种数据在二进制层面是怎么表示的，是我们必备的一课。
大部分教科书都会详细地从整数的二进制表示讲起，相信你在各种地方都能看到对应的材料，所以我就不再啰啰嗦嗦地讲这个了，只会快速地浏览一遍整数的二进制表示。
然后呢，我们重点来看一看，大家在实际应用中最常遇到的问题，也就是文本字符串是怎么表示成二进制的，特别是我们会遇到的乱码究竟是怎么回事儿。我们平时在开发的时候，所说的Unicode和UTF-8之间有什么关系。理解了这些，相信以后遇到任何乱码问题，你都能手到擒来了。
理解二进制的“逢二进一”二进制和我们平时用的十进制，其实并没有什么本质区别，只是平时我们是“逢十进一”，这里变成了“逢二进一”而已。每一位，相比于十进制下的0～9这十个数字，我们只能用0和1这两个数字。
任何一个十进制的整数，都能通过二进制表示出来。把一个二进制数，对应到十进制，非常简单，就是把从右到左的第N位，乘上一个2的N次方，然后加起来，就变成了一个十进制数。当然，既然二进制是一个面向程序员的“语言”，这个从右到左的位置，自然是从0开始的。
比如0011这个二进制数，对应的十进制表示，就是$0×2^3+0×2^2+1×2^1+1×2^0$
$=3$，代表十进制的3。
对应地，如果我们想要把一个十进制的数，转化成二进制，使用短除法就可以了。也就是，把十进制数除以2的余数，作为最右边的一位。然后用商继续除以2，把对应的余数紧靠着刚才余数的右侧，这样递归迭代，直到商为0就可以了。
比如，我们想把13这个十进制数，用短除法转化成二进制，需要经历以下几个步骤：
因此，对应的二进制数，就是1101。
刚才我们举的例子都是正数，对于负数来说，情况也是一样的吗？我们可以把一个数最左侧的一位，当成是对应的正负号，比如0为正数，1为负数，这样来进行标记。
这样，一个4位的二进制数， 0011就表示为+3。而1011最左侧的第一位是1，所以它就表示-3。这个其实就是整数的原码表示法。原码表示法有一个很直观的缺点就是，0可以用两个不同的编码来表示，1000代表0， 0000也代表0。习惯万事一一对应的程序员看到这种情况，必然会被“逼死”。
于是，我们就有了另一种表示方法。我们仍然通过最左侧第一位的0和1，来判断这个数的正负。但是，我们不再把这一位当成单独的符号位，在剩下几位计算出的十进制前加上正负号，而是在计算整个二进制值的时候，在左侧最高位前面加个负号。
比如，一个4位的二进制补码数值1011，转换成十进制，就是$-1×2^3+0×2^2+1×2^1+1×2^0$
$=-5$。如果最高位是1，这个数必然是负数；最高位是0，必然是正数。并且，只有0000表示0，1000在这样的情况下表示-8。一个4位的二进制数，可以表示从-8到7这16个整数，不会白白浪费一位。
当然更重要的一点是，用补码来表示负数，使得我们的整数相加变得很容易，不需要做任何特殊处理，只是把它当成普通的二进制相加，就能得到正确的结果。
我们简单一点，拿一个4位的整数来算一下，比如 -5 + 1 = -4，-5 + 6 = 1。我们各自把它们转换成二进制来看一看。如果它们和无符号的二进制整数的加法用的是同样的计算方式，这也就意味着它们是同样的电路。
字符串的表示，从编码到数字不仅数值可以用二进制表示，字符乃至更多的信息都能用二进制表示。最典型的例子就是字符串（Character String）。最早计算机只需要使用英文字符，加上数字和一些特殊符号，然后用8位的二进制，就能表示我们日常需要的所有字符了，这个就是我们常常说的ASCII码（American Standard Code for Information Interchange，美国信息交换标准代码）。
图片来源ASCII码就好比一个字典，用8位二进制中的128个不同的数，映射到128个不同的字符里。比如，小写字母a在ASCII里面，就是第97个，也就是二进制的0110 0001，对应的十六进制表示就是 61。而大写字母 A，就是第65个，也就是二进制的0100 0001，对应的十六进制表示就是41。
在ASCII码里面，数字9不再像整数表示法里一样，用0000 1001来表示，而是用0011 1001 来表示。字符串15也不是用0000 1111 这8位来表示，而是变成两个字符1和5连续放在一起，也就是 0011 0001 和 0011 0101，需要用两个8位来表示。
我们可以看到，最大的32位整数，就是2147483647。如果用整数表示法，只需要32位就能表示了。但是如果用字符串来表示，一共有10个字符，每个字符用8位的话，需要整整80位。比起整数表示法，要多占很多空间。
这也是为什么，很多时候我们在存储数据的时候，要采用二进制序列化这样的方式，而不是简单地把数据通过CSV或者JSON，这样的文本格式存储来进行序列化。不管是整数也好，浮点数也好，采用二进制序列化会比存储文本省下不少空间。
ASCII码只表示了128个字符，一开始倒也堪用，毕竟计算机是在美国发明的。然而随着越来越多的不同国家的人都用上了计算机，想要表示譬如中文这样的文字，128个字符显然是不太够用的。于是，计算机工程师们开始各显神通，给自己国家的语言创建了对应的字符集（Charset）和字符编码（Character Encoding）。
字符集，表示的可以是字符的一个集合。比如“中文”就是一个字符集，不过这样描述一个字符集并不准确。想要更精确一点，我们可以说，“第一版《新华字典》里面出现的所有汉字”，这是一个字符集。这样，我们才能明确知道，一个字符在不在这个集合里面。比如，我们日常说的Unicode，其实就是一个字符集，包含了150种语言的14万个不同的字符。
而字符编码则是对于字符集里的这些字符，怎么一一用二进制表示出来的一个字典。我们上面说的Unicode，就可以用UTF-8、UTF-16，乃至UTF-32来进行编码，存储成二进制。所以，有了Unicode，其实我们可以用不止UTF-8一种编码形式，我们也可以自己发明一套 GT-32 编码，比如就叫作Geek Time 32好了。只要别人知道这套编码规则，就可以正常传输、显示这段代码。
同样的文本，采用不同的编码存储下来。如果另外一个程序，用一种不同的编码方式来进行解码和展示，就会出现乱码。这就好像两个军队用密语通信，如果用错了密码本，那看到的消息就会不知所云。在中文世界里，最典型的就是“手持两把锟斤拷，口中疾呼烫烫烫”的典故。
我曾经听说过这么一个笑话，没有经验的同学，在看到程序输出“烫烫烫”的时候，以为是程序让CPU过热发出报警，于是尝试给CPU降频来解决问题。
既然今天要彻底搞清楚编码知识，我们就来弄清楚“锟斤拷”和“烫烫烫”的来龙去脉。</description></item><item><title>11_怎么给字符串字段加索引？</title><link>https://artisanbox.github.io/1/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/11/</guid><description>现在，几乎所有的系统都支持邮箱登录，如何在邮箱这样的字段上建立合理的索引，是我们今天要讨论的问题。
假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的：
mysql&amp;gt; create table SUser( ID bigint unsigned primary key, email varchar(64), ... )engine=innodb; 由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句：
mysql&amp;gt; select f1, f2 from SUser where email='xxx'; 从第4和第5篇讲解索引的文章中，我们可以知道，如果email这个字段上没有索引，那么这个语句就只能做全表扫描。
同时，MySQL是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。
比如，这两个在email字段上创建索引的语句：
mysql&amp;gt; alter table SUser add index index1(email); 或 mysql&amp;gt; alter table SUser add index index2(email(6)); 第一个语句创建的index1索引里面，包含了每个记录的整个字符串；而第二个语句创建的index2索引里面，对于每个记录都是只取前6个字节。
那么，这两种不同的定义在数据结构和存储上有什么区别呢？如图2和3所示，就是这两个索引的示意图。
图1 email 索引结构图2 email(6) 索引结构从图中你可以看到，由于email(6)这个索引结构中每个邮箱字段都只取前6个字节（即：zhangs），所以占用的空间会更小，这就是使用前缀索引的优势。
但，这同时带来的损失是，可能会增加额外的记录扫描次数。
接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。
select id,name,email from SUser where email='zhangssxyz@xxx.com'; 如果使用的是index1（即email整个字符串的索引结构），执行顺序是这样的：
从index1索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得ID2的值；
到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集；
取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足email='zhangssxyz@xxx.com’的条件了，循环结束。
这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。</description></item><item><title>11_排序（上）：为什么插入排序比冒泡排序更受欢迎？</title><link>https://artisanbox.github.io/2/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/12/</guid><description>排序对于任何一个程序员来说，可能都不会陌生。你学的第一个算法，可能就是排序。大部分编程语言中，也都提供了排序函数。在平常的项目中，我们也经常会用到排序。排序非常重要，所以我会花多一点时间来详细讲一讲经典的排序算法。
排序算法太多了，有很多可能你连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。我只讲众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。我按照时间复杂度把它们分成了三类，分三节课来讲解。
带着问题去学习，是最有效的学习方法。所以按照惯例，我还是先给你出一个思考题：插入排序和冒泡排序的时间复杂度相同，都是O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？
你可以先思考一两分钟，带着这个问题，我们开始今天的内容！
如何分析一个“排序算法”？学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。那分析一个排序算法，要从哪几个方面入手呢？
排序算法的执行效率对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：
1.最好情况、最坏情况、平均情况时间复杂度
我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。
为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。
2.时间复杂度的系数、常数 、低阶
我们知道，时间复杂度反映的是数据规模n很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是10个、100个、1000个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。
3.比较次数和交换（或移动）次数
这一节和下一节讲的都是基于比较的排序算法。基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。
排序算法的内存消耗我们前面讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是O(1)的排序算法。我们今天讲的三种排序算法，都是原地排序算法。
排序算法的稳定性仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。
我通过一个例子来解释一下。比如我们有一组数据2，9，3，4，8，3，按照大小排序之后就是2，3，3，4，8，9。
这组数据里有两个3。经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法；如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。
你可能要问了，两个3哪个在前，哪个在后有什么关系啊，稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？
很多数据结构和算法课程，在讲排序的时候，都是用整数来举例，但在真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。
比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有10万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？
最先想到的方法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。
借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？
稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。
冒泡排序（Bubble Sort）我们从冒泡排序开始，学习今天的三种排序算法。
冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序工作。
我用一个例子，带你看下冒泡排序的整个过程。我们要对一组数据4，5，6，3，2，1，从小到大进行排序。第一次冒泡操作的详细过程就是这样：
可以看出，经过一次冒泡操作之后，6这个元素已经存储在正确的位置上。要想完成所有数据的排序，我们只要进行6次这样的冒泡操作就行了。
实际上，刚讲的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。我这里还有另外一个例子，这里面给6个元素排序，只需要4次冒泡操作就可以了。
冒泡排序算法的原理比较容易理解，具体的代码我贴到下面，你可以结合着代码来看我前面讲的原理。
// 冒泡排序，a表示数组，n表示数组大小 public void bubbleSort(int[] a, int n) { if (n &amp;lt;= 1) return; for (int i = 0; i &amp;lt; n; ++i) { // 提前退出冒泡循环的标志位 boolean flag = false; for (int j = 0; j &amp;lt; n - i - 1; ++j) { if (a[j] &amp;gt; a[j+1]) { // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true; // 表示有数据交换 } } if (!</description></item><item><title>11｜基于C语言的虚拟机（二）：性能增长10倍的秘密</title><link>https://artisanbox.github.io/3/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/13/</guid><description>你好，我是宫文学。
上一节课，我们初步实现了一个C语言版本的虚拟机，让它顺利地跑起来了。你想想看，用TypeScript生成字节码文件，然后在一个C语言实现的虚拟机上去运行，这个设计，其实和Java应用、Andorid应用、Erlang应用、Lua应用等的运行机制是一样的。也就是说，如果退回到智能手机刚诞生的年代，你完全可以像Android的发明人一样，用这种方式提供一个移动应用开发工具。
其实，我国最新的自主操作系统HarmonyOS，也是采用了像我们这门课一样的虚拟机设计机制，而且用的就是TypeScript语言，这也是我这门课采用TypeScript作为教学语言的原因之一。虽然我还没有看到HarmonyOS的虚拟机代码，但并不妨碍我去理解它的实现原理。当然了，你在学完这门课以后，也会更容易理解HarmonyOS的开发方式，而且也有助于你阅读它的虚拟机的代码。
好了，对于我们当前成果的吹捧到此打住。让我们回到现实，现实有点残酷：我们当前实现的基于C语言的虚拟机，在上一节课的性能测试中，竟然排名倒数第一。这显然不正常，这也说明了在虚拟机的设计中，我们还有一些重要的设计考虑被忽视了。
那这一节课呢，我们就来分析一下导致我们虚拟机性能不高的原因，并且针对性地解决掉这个问题。在这个过程中，你会加深对计算机语言的运行时技术的理解，特别是对内存管理的理解。
那首先，让我们把产生性能问题的可能原因分析一下。
性能问题的分析首先，我们应该了解到一点，现代的JavaScript引擎，性能确实挺高的。
在我们TypeScript版本的虚拟机中，TypeScript被编译成了JavaScript，并在Node.js中运行，而Node.js又是基于V8引擎的。
在互联网的早期，JavaScript的运行效率比较低。但是后来，以V8为代表的JavaScript引擎，性能有了大幅度的提升，使得现在的Web前端可以实现很复杂的功能。
V8在运行JavaScript的时候，会做即时编译（JIT）。V8的即时编译器，能够根据运行时收集的信息对类型做推测，这也就避免了由于运行时的类型判断而产生的额外开销，从而生成了跟提前编译（AOT）差不多的代码。如果你想了解更多细节，你可以去看看我在《编译原理实战课》中对V8的剖析。
从原理上来说，运行时的推测机制，甚至会生成比提前编译（AOT）更高效的代码。因为它拥有运行时的统计信息，并通过某些优化算法（参考JVM的局部逃逸分析算法）实现了更好的编译优化。
换一句话说，V8也是编译生成了机器码，甚至有时候会生成更高效的机器码。仅从这一点看，它并不会比C语言的提前编译差。
不过，JavaScript毕竟是动态类型的语言，它的编译和运行过程会有一些额外的开销。
比如，编译后的目标代码总要留出一些口子，用来处理类型预测失效的情况。这个时候，它会从运行本地代码的状态退回到解释器去执行。
所以，平均来说，JavaScript编写的程序，性能不会比C/C++更高。它在某些场景下能接近C/C++的性能，已经相当惊人了。
可是，在上一节中TypeScript版本虚拟机的性能居然是C语言版本的2倍半，这就太不正常了。一定还有别的因素在起作用。
所以，我们来看看第二方面的因素，就是运行时的设计。
在前面的讨论中，我们比较关注的是编译技术与性能的关系。不过，在一个虚拟机中，还会有其他影响性能的因素，这就是语言的运行时。运行时就是支撑我们的应用程序运行所需要的一些软件功能，最常见的运行时功能就是内存管理机制和并发机制。
在这里，我们重点要看一下内存管理机制。通常我们提到内存管理的时候，一下子就想到垃圾收集机制去了。其实，这只是内存管理的一半工作，完整的内存管理功能还要包括内存的申请机制。
在像Java、JavaScript这样的语言中，语言的运行时需要根据程序的指令，随时在内存中创建对象，然后在程序用不到这些对象的时候，再使用垃圾收集机制，把这些对象所占据的内存释放掉。
那么重点就来了：申请和释放内存，有时会导致巨大的性能开销。一个好的运行时，必须想办法降低这些开销。
我们初版的C语言虚拟机可能就存在这方面的问题。不过，计算机语言的运行时，都是从堆里申请和管理内存的。为了让你理解内存管理和性能的关系，更好地排查出影响C语言虚拟机性能的原因，我们首先回顾一下栈和堆这两种基础的内存管理机制。
两种内存管理机制：栈和堆在现代的操作系统中，为了支持应用的运行，通常会提供栈和堆这两种内存管理机制。当我们在一个C语言的函数里使用本地变量时，这些本地变量所需的内存是在栈里申请的，这也就是这个函数所使用的栈桢。而当我们用C语言的malloc函数申请一块内存的时候，这块内存就是从堆里申请的。
不过，只有像C/C++这样直接编译成本地代码的语言，才可以使用操作系统的栈来保存栈桢。在后面的课程中，我们也会生成与栈桢管理有关的汇编代码，管理栈桢通常需要修改特定寄存器的值，以及使用push、pop等辅助的指令。
在我们的解释器所使用的栈桢是自己管理的，本质都是从堆里申请的。从栈里和堆里申请内存的开销是不一样的。
从栈里申请内存很简单，基本上只需要修改栈顶指针，也就是某个特定寄存器的值就行了，栈就会自动地伸缩，整个栈的地址空间始终是连续的一整块内存。
而堆就不是了。从堆里申请的内存，由于每个对象的生存期是不一样的，所以就会形成很多的“空洞”，导致内存碎片化。这样，再次申请内存的时候，操作系统需要找到一块大小合适的自由内存空间。这个过程，就需要消耗一定的计算量。在内存碎片化越来越严重的情况下，找到一块可用内存空间的开销会越来越大。
另外，程序的并发也会为堆的内存申请带来额外的开销。在现代操作系统中，每个线程都有自己独享的栈，相互之间不会干扰，但堆却是各个线程所共享的。所以，在分配内存的时候，操作系统会进行线程间的同步，每次只能为一个线程分配内存，避免同一块内存被分配给多个线程。这显然也会降低系统的性能。
现在你再回头来看看我们的C语言虚拟机的实现。在栈桢和操作数栈这两个数据结构中，有好几个地方都是指针，比如本地变量的数组、操作数栈，以及操作数栈中的数据区。按照常规的编程方法，我们为每个指针都单独申请了内存。
typedef struct _StackFrame{ //本栈桢对应的函数，用来找到代码 FunctionSymbol* functionSym; //返回地址 int returnIndex; //本地变量数组 NUMBER* localVars; //操作数栈 OprandStack* oprandStack; //指向前一个栈桢的链接 struct _StackFrame * prev; }StackFrame; /**
操作数栈 当栈为空的时候，top = -1; */ typedef struct _OprandStack{ NUMBER * data; //数组 int top; //栈顶的索引值 }OprandStack; 这样就导致我们一个栈桢的内存布局被切成了4小块：</description></item><item><title>12_为什么我的MySQL会“抖”一下？</title><link>https://artisanbox.github.io/1/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/12/</guid><description>平时的工作中，不知道你有没有遇到过这样的场景，一条SQL语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。
看上去，这就像是数据库“抖”了一下。今天，我们就一起来看一看这是什么原因。
你的SQL语句为什么变“慢”了在前面第2篇文章《日志系统：一条SQL更新语句是如何执行的？》中，我为你介绍了WAL机制。现在你知道了，InnoDB在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作redo log（重做日志），也就是《孔乙己》里咸亨酒店掌柜用来记账的粉板，在更新内存写完redo log后，就返回给客户端，本次更新成功。
做下类比的话，掌柜记账的账本是数据文件，记账用的粉板是日志文件（redo log），掌柜的记忆就是内存。
掌柜总要找时间把账本更新一下，这对应的就是把内存里的数据写入磁盘的过程，术语就是flush。在这个flush操作执行之前，孔乙己的赊账总额，其实跟掌柜手中账本里面的记录是不一致的。因为孔乙己今天的赊账金额还只在粉板上，而账本里的记录是老的，还没把今天的赊账算进去。
当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。
不论是脏页还是干净页，都在内存中。在这个例子里，内存对应的就是掌柜的记忆。
接下来，我们用一个示意图来展示一下“孔乙己赊账”的整个操作过程。假设原来孔乙己欠账10文，这次又要赊9文。
图1 “孔乙己赊账”更新和flush过程回到文章开头的问题，你不难想象，平时执行很快的更新操作，其实就是在写内存和日志，而MySQL偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）。
那么，什么情况会引发数据库的flush过程呢？
我们还是继续用咸亨酒店掌柜的这个例子，想一想：掌柜在什么情况下会把粉板上的赊账记录改到账本上？
第一种场景是，粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活儿，将粉板上的记录擦掉一些，留出空位以便继续记账。当然在擦掉之前，他必须先将正确的账目记录到账本中才行。
这个场景，对应的就是InnoDB的redo log写满了。这时候系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写。我在第二讲画了一个redo log的示意图，这里我改成环形，便于大家理解。 图2 redo log状态图checkpoint可不是随便往前修改一下位置就可以的。比如图2中，把checkpoint位置从CP推进到CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都flush到磁盘上。之后，图中从write pos到CP’之间就是可以再写入的redo log的区域。
第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出账本把孔乙己这笔账先加进去。
这种场景，对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。
你一定会说，这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿redo log出来应用不就行了？这里其实是从性能考虑的。如果刷脏页一定会写盘，就保证了每个数据页有两种状态：
一种是内存里存在，内存里就肯定是正确的结果，直接返回； 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。
这样的效率最高。 第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不如更新账本。
这种场景，对应的就是MySQL认为系统“空闲”的时候。当然，MySQL“这家酒店”的生意好起来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，即使是“生意好”的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”。
第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。
这种场景，对应的就是MySQL正常关闭的情况。这时候，MySQL会把内存的脏页都flush到磁盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快。
接下来，你可以分析一下上面四种场景对性能的影响。
其中，第三种情况是属于MySQL空闲时的操作，这时系统没什么压力，而第四种场景是数据库本来就要关闭了。这两种情况下，你不会太关注“性能”问题。所以这里，我们主要来分析一下前两种场景下的性能问题。
第一种是“redo log写满了，要flush脏页”，这种情况是InnoDB要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为0。
第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：
第一种是，还没有使用的； 第二种是，使用了并且是干净页； 第三种是，使用了并且是脏页。 InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。
而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。
所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：
一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。
所以，InnoDB需要有控制脏页比例的机制，来尽量避免上面的这两种情况。
InnoDB刷脏页的控制策略接下来，我就来和你说说InnoDB脏页的控制策略，以及和这些策略相关的参数。
首先，你要正确地告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时候，可以刷多快。</description></item><item><title>12_排序（下）：如何用快排思想在O(n)内查找第K大元素？</title><link>https://artisanbox.github.io/2/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/13/</guid><description>上一节我讲了冒泡排序、插入排序、选择排序这三种排序算法，它们的时间复杂度都是O(n2)，比较高，适合小规模数据的排序。今天，我讲两种时间复杂度为O(nlogn)的排序算法，归并排序和快速排序。这两种排序算法适合大规模的数据排序，比上一节讲的那三种排序算法要更常用。
归并排序和快速排序都用到了分治思想，非常巧妙。我们可以借鉴这个思想，来解决非排序的问题，比如：如何在O(n)的时间复杂度内查找一个无序数组中的第K大元素？ 这就要用到我们今天要讲的内容。
归并排序的原理我们先来看归并排序（Merge Sort）。
归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。
归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。
从我刚才的描述，你有没有感觉到，分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。分治算法的思想我后面会有专门的一节来讲，现在不展开讨论，我们今天的重点还是排序算法。
前面我通过举例让你对归并有了一个感性的认识，又告诉你，归并排序用的是分治思想，可以用递归来实现。我们现在就来看看如何用递归代码来实现归并排序。
我在第10节讲的递归代码的编写技巧你还记得吗？写递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，我们先写出归并排序的递推公式。
递推公式： merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r)) 终止条件： p &amp;gt;= r 不用再继续分解 我来解释一下这个递推公式。
merge_sort(p…r)表示，给下标从p到r之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p…q)和merge_sort(q+1…r)，其中下标q等于p和r的中间位置，也就是(p+r)/2。当下标从p到q和从q+1到r这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从p到r之间的数据就也排好序了。
有了递推公式，转化成代码就简单多了。为了阅读方便，我这里只给出伪代码，你可以翻译成你熟悉的编程语言。
// 归并排序算法, A是数组，n表示数组大小 merge_sort(A, n) { merge_sort_c(A, 0, n-1) }
// 递归调用函数 merge_sort_c(A, p, r) { // 递归终止条件 if p &amp;gt;= r then return
// 取p到r之间的中间位置q q = (p+r) / 2 // 分治递归 merge_sort_c(A, p, q) merge_sort_c(A, q+1, r) // 将A[p&amp;hellip;q]和A[q+1&amp;hellip;r]合并为A[p&amp;hellip;r] merge(A[p&amp;hellip;r], A[p&amp;hellip;q], A[q+1&amp;hellip;r]) } 你可能已经发现了，merge(A[p&amp;hellip;r], A[p&amp;hellip;q], A[q+1&amp;hellip;r])这个函数的作用就是，将已经有序的A[p&amp;hellip;q]和A[q+1&amp;hellip;.</description></item><item><title>12_理解电路：从电报机到门电路，我们如何做到“千里传信”？</title><link>https://artisanbox.github.io/4/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/12/</guid><description>我们前面讲过机器指令，你应该知道，所有最终执行的程序其实都是使用“0”和“1”这样的二进制代码来表示的。上一讲里，我也向你展示了，对应的整数和字符串，其实也是用“0”和“1”这样的二进制代码来表示的。
那么你可能要问了，我知道了这个有什么用呢？毕竟我们人用纸和笔来做运算，都是用十进制，直接用十进制和我们最熟悉的符号不是最简单么？为什么计算机里我们最终要选择二进制呢？
这一讲，我和你一起来看看，计算机在硬件层面究竟是怎么表示二进制的，以此你就会明白，为什么计算机会选择二进制。
从信使到电报，我们怎么做到“千里传书”？马拉松的故事相信你听说过。公元前490年，在雅典附近的马拉松海边，发生了波斯和希腊之间的希波战争。雅典和斯巴达领导的希腊联军胜利之后，雅典飞毛腿菲迪皮德斯跑了历史上第一个马拉松，回雅典报喜。这个时候，人们在远距离报信的时候，采用的是派人跑腿，传口信或者送信的方式。
但是，这样靠人传口信或者送信的方式，实在是太慢了。在军事用途中，信息能否更早更准确地传递出去经常是事关成败的大事。所以我们看到中国古代的军队有“击鼓进军”和“鸣金收兵”，通过打鼓和敲钲发出不同的声音，来传递军队的号令。
如果我们把军队当成一台计算机，那“金”和“鼓”就是这台计算机的“1”和“0”。我们可以通过不同的编码方式，来指挥这支军队前进、后退、转向、追击等等。
“金”和“鼓”比起跑腿传口信，固然效率更高了，但是能够传递的范围还是非常有限，超出个几公里恐怕就听不见了。于是，人们发明了更多能够往更远距离传信的方式，比如海上的灯塔、长城上的烽火台。因为光速比声速更快，传的距离也可以更远。
图片来源亚历山大港外的法罗斯灯塔，位列世界七大奇迹之一，可惜现在只剩下遗迹了。可见人类社会很早就学会使用类似二进制信号的方式来传输信息但是，这些传递信息的方式都面临一个问题，就是受限于只有“1”和“0”这两种信号，不能传递太复杂的信息，那电报的发明就解决了这个问题。
从信息编码的角度来说，金、鼓、灯塔、烽火台类似电报的二进制编码。电报传输的信号有两种，一种是短促的点信号（dot信号），一种是长一点的划信号（dash信号）。我们把“点”当成“1”，把“划”当成“0”。这样一来，我们的电报信号就是另一种特殊的二进制编码了。电影里最常见的电报信号是“SOS”，这个信号表示出来就是 “点点点划划划点点点”。
比起灯塔和烽火台这样的设备，电报信号有两个明显的优势。第一，信号的传输距离迅速增加。因为电报本质上是通过电信号来进行传播的，所以从输入信号到输出信号基本上没有延时。第二，输入信号的速度加快了很多。电报机只有一个按钮，按下就是输入信号，按的时间短一点，就是发出了一个“点”信号；按的时间长一些，就是一个“划”信号。只要一个手指，就能快速发送电报。
图片来源一个摩尔斯电码的电报机而且，制造一台电报机也非常容易。电报机本质上就是一个“蜂鸣器+长长的电线+按钮开关”。蜂鸣器装在接收方手里，开关留在发送方手里。双方用长长的电线连在一起。当按钮开关按下的时候，电线的电路接通了，蜂鸣器就会响。短促地按下，就是一个短促的点信号；按的时间稍微长一些，就是一个稍长的划信号。
有了电池开关和铃铛，你就有了最简单的摩尔斯电码发报机理解继电器，给跑不动的信号续一秒有了电报机，只要铺设好电报线路，就可以传输我们需要的讯息了。但是这里面又出现了一个新的挑战，就是随着电线的线路越长，电线的电阻就越大。当电阻很大，而电压不够的时候，即使你按下开关，蜂鸣器也不会响。
你可能要说了，我们可以提高电压或者用更粗的电线，使得电阻更小，这样就可以让整个线路铺得更长一些。但是这个再长，也没办法从北京铺设到上海吧。要想从北京把电报发到上海，我们还得想些别的办法。
对于电报来说，电线太长了，使得线路接通也没有办法让蜂鸣器响起来。那么，我们就不要一次铺太长的线路，而把一小段距离当成一个线路。我们也可以跟驿站建立一个小电报站，在小电报站里面安排一个电报员。他听到上一个小电报站发来的信息，然后原样输入，发到下一个电报站去。这样，我们的信号就可以一段段传输下去，而不会因为距离太长，导致电阻太大，没有办法成功传输信号。为了能够实现这样接力传输信号，在电路里面，工程师们造了一个叫作继电器（Relay）的设备。
中继，其实就是不断地通过新的电源重新放大已经开始衰减的原有信号事实上，这个过程中，我们需要在每一阶段原样传输信号，所以你可以想想，我们是不是可以设计一个设备来代替这个电报员？相比使用人工听蜂鸣器的声音，来重复输入信号，利用电磁效应和磁铁，来实现这个事情会更容易。
我们把原先用来输出声音的蜂鸣器，换成一段环形的螺旋线圈，让电路封闭通上电。因为电磁效应，这段螺旋线圈会产生一个带有磁性的电磁场。我们原本需要输入的按钮开关，就可以用一块磁力稍弱的磁铁把它设在“关”的状态。这样，按下上一个电报站的开关，螺旋线圈通电产生了磁场之后，磁力就会把开关“吸”下来，接通到下一个电报站的电路。
如果我们在中间所有小电报站都用这个“螺旋线圈+磁性开关”的方式，来替代蜂鸣器和普通开关，而只在电报的始发和终点用普通的开关和蜂鸣器，我们就有了一个拆成一段一段的电报线路，接力传输电报信号。这样，我们就不需要中间安排人力来听打电报内容，也不需要解决因为线缆太长导致的电阻太大或者电压不足的问题了。我们只要在终点站安排电报员，听写最终的电报内容就可以了。这样是不是比之前更省事了？
事实上，继电器还有一个名字就叫作电驿，这个“驿”就是驿站的驿，可以说非常形象了。这个接力的策略不仅可以用在电报中，在通信类的科技产品中其实都可以用到。
比如说，你在家里用WiFi，如果你的屋子比较大，可能某些房间的信号就不好。你可以选用支持“中继”的WiFi路由器，在信号衰减的地方，增加一个WiFi设备，接收原来的WiFi信号，再重新从当前节点传输出去。这种中继对应的英文名词和继电器是一样的，也叫Relay。
再比如说，我们现在互联网使用的光缆，是用光信号来传输数据。随着距离的增长、反射次数的增加，信号也会有所衰减，我们同样要每隔一段距离，来增加一个用来重新放大信号的中继。
有了继电器之后，我们不仅有了一个能够接力传输信号的方式，更重要的是，和输入端通过开关的“开”和“关”来表示“1”和“0”一样，我们在输出端也能表示“1”和“0”了。
输出端的作用，不仅仅是通过一个蜂鸣器或者灯泡，提供一个供人观察的输出信号，通过“螺旋线圈 + 磁性开关”，使得我们有“开”和“关”这两种状态，这个“开”和“关”表示的“1”和“0”，还可以作为后续线路的输入信号，让我们开始可以通过最简单的电路，来组合形成我们需要的逻辑。
通过这些线圈和开关，我们也可以很容易地创建出 “与（AND）”“或（OR）”“非（NOT）”这样的逻辑。我们在输入端的电路上，提供串联的两个开关，只有两个开关都打开，电路才接通，输出的开关也才能接通，这其实就是模拟了计算机里面的“与”操作。
我们在输入端的电路，提供两条独立的线路到输出端，两条线路上各有一个开关，那么任何一个开关打开了，到输出端的电路都是接通的，这其实就是模拟了计算机中的“或”操作。
当我们把输出端的“螺旋线圈+磁性开关”的组合，从默认关掉，只有通电有了磁场之后打开，换成默认是打开通电的，只有通电之后才关闭，我们就得到了一个计算机中的“非”操作。输出端开和关正好和输入端相反。这个在数字电路中，也叫作反向器（Inverter）。
反向器的电路，其实就是开关从默认关闭变成默认开启而已与、或、非的电路都非常简单，要想做稍微复杂一点的工作，我们需要很多电路的组合。不过，这也彰显了现代计算机体系中一个重要的思想，就是通过分层和组合，逐步搭建起更加强大的功能。
回到我们前面看的电报机原型，虽然一个按钮开关的电报机很“容易”操作，但是却不“方便”操作。因为电报员要熟记每一个字母对应的摩尔斯电码，并且需要快速按键来进行输入，一旦输错很难纠正。但是，因为电路之间可以通过与、或、非组合完成更复杂的功能，我们完全可以设计一个和打字机一样的电报机，每按下一个字母按钮，就会接通一部分电路，然后把这个字母的摩尔斯电码输出出去。
虽然在电报机时代，我们没有这么做，但是在计算机时代，我们其实就是这样做的。我们不再是给计算机“0”和“1”，而是通过千万个晶体管组合在一起，最终使得我们可以用“高级语言”，指挥计算机去干什么。
总结延伸可以说，电报是现代计算机的一个最简单的原型。它和我们现在使用的现代计算机有很多相似之处。我们通过电路的“开”和“关”，来表示“1”和“0”。就像晶体管在不同的情况下，表现为导电的“1”和绝缘的“0”的状态。
我们通过电报机这个设备，看到了如何通过“螺旋线圈+开关”，来构造基本的逻辑电路，我们也叫门电路。一方面，我们可以通过继电器或者中继，进行长距离的信号传输。另一方面，我们也可以通过设置不同的线路和开关状态，实现更多不同的信号表示和处理方式，这些线路的连接方式其实就是我们在数字电路中所说的门电路。而这些门电路，也是我们创建CPU和内存的基本逻辑单元。我们的各种对于计算机二进制的“0”和“1”的操作，其实就是来自于门电路，叫作组合逻辑电路。
推荐阅读《编码：隐匿在计算机软硬件背后的语言》的第6～11章，是一个很好的入门材料，可以帮助你深入理解数字电路，值得你花时间好好读一读。
课后思考除了与、或、非之外，还有很多基础的门电路，比如“异或（XOR）门”。你可以想一想，试着搜索一些资料，设计一个异或门的电路。
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>12｜物理机上程序运行的硬件环境是怎么样的？</title><link>https://artisanbox.github.io/3/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/14/</guid><description>你好，我是宫文学。
在经过了几节课的努力以后，我们的语言运行引擎，从AST解释器升级成了TypeScript版的虚拟机，又升级成了C语言版的虚拟机。这个过程中，我们的语言的性能在不断地提升。并且，我们的关注点，也越来越从高层的语法语义处理层面，往底层技术方向靠拢了。
虽然我们现在的语言特性还不够丰富，但我还是想先带你继续往下钻。我们的目标是先把技术栈钻透，然后再在各个层次上扩大战果。
所以，在接下来的几节课里，我们会把程序编译成汇编代码，然后再生成二进制的可执行程序。在这个过程中，你会把很多过去比较模糊的底层机制搞清楚，我也会带你去除一些知识点的神秘面纱，让你不再畏惧它们。
在此之前，为了让你编译后的程序能够在计算机上跑起来，你必须把物理计算机上程序的运行机制搞清楚，特别是要搞清楚应用程序、操作系统和底层硬件的互动关系。这里面的一些知识点，通常很多程序员都理解得似是而非，不是太透彻。而理解了这些程序运行机制，除了能够让我们的语言在计算机上顺利地运行，还能够帮助你胜任一些系统级软件的开发任务。
今天这节课，我想先带你透彻了解程序运行的硬件环境，以及硬件架构跟我们实现计算机语言的关系。在下节课，我则会带你透彻了解程序运行的软件环境。
硬件环境和程序的运行机制其实，我们现在用的计算机、手机、物联网等大部分智能设备，它们的硬件架构都是差不多的，基本遵循下面这张图所展示的架构。而这张图上画出来的部分，都是需要我们在实现一门计算机语言的时候需要了解的。
首先我们从整体向部分逐个击破，先来看看计算机的总体架构和程序运行的原理。
对于计算机，我们最关心的是两个硬件，一个是CPU，一个是内存。它们通过计算机的总线连接在一起，这样CPU就可以读取内存中的数据和程序，并把数据写回内存。而CPU内部还会细分成更多的成分，包括高速缓存、寄存器和各种处理单元。
那在这种硬件环境下，程序是怎么运行起来的呢？通常，CPU上会有个寄存器，叫做PC计数器。通过PC计数器的值，CPU能够计算出下一条需要执行的代码的地址，然后读取这个代码并执行（根据不同的CPU架构，PC计数器中的值可能不是直接的内存地址，而需要进行一点转换和计算）。通常情况下，程序都是顺序执行的。但当遇到跳转指令时，PC计数器就会指向新的代码地址，从新的地址开始执行代码。
除了跳转指令会改变PC计数器的值，CPU的异常机制也会改变PC计数器的值，跳转到异常处理程序，处理完毕之后再回来。CPU的异常机制是CPU架构设计的一个重要组成部分。典型的异常是由硬件触发的中断。我们每次敲打键盘，都会触发一个中断，处理完毕以后会再接着运行原来的程序。也有的时候，中断可以由软件触发，比如当你Debug程序的时候，你可以控制着程序一条一条代码的执行，这也是利用了中断机制。
了解了程序总体的运行原理后，我们再通过一段代码的执行过程，深入了解一下其中的机理，并了解各个硬件成分是如何协同工作的。
一段代码的执行过程这段示例代码是三条汇编代码，你先看一下：
movl -4(%rbp), %eax addl $10, %eax movl %eax, -r(%rbp) 这三条代码都采用了统一的格式：操作码的助记符、源操作数和目标操作数。注意，不同CPU的指令集和不同的汇编器，会采用不同的格式，我这里只是举个例子。
这三条代码的意思也很简单，我来解释一下：
第1条是一个movl指令，movl能够把一个整数从一个地方拷贝到另一个地方。这里是从一个内存地址取出一个值，放到%eax寄存器，而这个内存地址是%rpb寄存器的值减去4； 第2条是一个addl指令，它把常数10加到%eax寄存器上； 第3条又是一个movl指令，这次是把%eax寄存器的值又写回第一行的那个内存地址。 理解了这三条代码的意思以后，我们来看看具体执行的时候都发生了些什么。
第一步，CPU读入第一行代码。
我们这三条代码都是存在内存里的。CPU会根据PC计数器的值，从内存里把第一条代码读进CPU里。
这里你要注意，我们刚才使用了汇编代码来表示程序，但内存里保存的，实际上是机器码。汇编代码通过汇编器可以转换成机器码。
在设计CPU的指令集的时候，我们会设计机器码的格式。比如，下图是我在RISC-V手册中找到的一张图，描述了RISC-V指令的几种编码方式。你能看到，每条指令占用32位，也就是一个整数的长度。其中opcode的意思是操作码，占用低7位，rs是源寄存器器，rd的意思是目的寄存器，imm是立即数，也就是常数。
这些指令被读入内存以后，会有一个解码的过程，也就是把操作码、源操作数、目标操作数这些信息从一条指令里拆解出来，用于后续的处理。这个解码的功能，是由CPU内部的一个功能单元完成的。
那么CPU是直接从内存中读入代码的吗？
不是的，其实CPU是从高速缓存中读入代码和数据的。通常代码和数据的高速缓存是分开的，分别叫做Instruction Cache和Data Cache。只有高速缓存中没有这些代码或数据的时候，才会从内存中读取。
高速缓存是内存和CPU之间的缓冲区。高速缓存的读写速度比内存快，能够减少CPU在读写内存过程中的等待时间。当CPU从内存里读一个数据的时候，它其实是从高速缓存中读到的；如果在高速缓存里没有，术语叫做没有命中，CPU会把这个数据旁白的一批数据都读到高速缓存，这样再读下一个数据的时候，又可以直接从高速缓存中读取了。
高速缓存可能分多级，比如叫做L1~L3，速度从高到底，容量则反过来，从低到高。并且，一般较低速的缓存是多个核共享的，而更高速的是每个核独享的。
那高速缓存的相关知识对我们实现计算机语言有什么帮助呢？
有一类优化技术，是提高程序数据的局部性，也就是把代码前后需要用到的数据，尽量都聚集在一起，这样便于一次性地加载到高速缓存。在读取下一个数据的时候，就不需要访问内存了，直接从高速缓存就可以获得了，从而提高了系统的性能。这就是数据局部性的好处。
从这个角度看，你回想一下，上一节课我们就是把栈桢的数据都放在一个连续的内存块里，也是在不经意间提高了数据的局部性。
不过，高速缓存也会带来一些麻烦。比如，当两个内核都去读写同一个内存数据的时候，它们各自使用自己的高速缓存，可能就会出现数据不一致的情况。所以，如果我们在语言层面上支持并发编程的特性，就像Java那样，那么在生成指令时就要保证数据的一致性。如果你想具体了解一下这些技术，可以再去看一下《编译原理实战课》。
理解了高速缓存以后，我们接着继续看第一条指令的执行过程。在这条指令里，目标操作数，也就是数据加载的目的地是一个寄存器。那我们再了解一下寄存器。
寄存器是CPU做运算的操作区。在典型的情况下，CPU都是把数据加载到寄存器，然后再在寄存器里做各种运算。
相比高速缓存来说，寄存器的读写速度更高，大约是内存的100倍。整体来说，寄存器、高速缓存和内存的读写速度是寄存器&amp;gt;高速缓存&amp;gt;内存。
在CPU的设计中，有些寄存器是有特定用途的，比如PC计数器用于计算代码地址，EFlags寄存器用于保存一些运算结果产生的状态等。
还有一些寄存器叫做通用寄存器，它们可以被我们的代码所使用，进行加减乘除等各种计算。在把程序编译成汇编代码的时候，我们要尽量去利用这些通用寄存器来运算。但如果寄存器不够用，就需要临时保存到内存中，把寄存器的空间腾出来。
好，现在我们对寄存器也有了基本的了解了，我们接着往下分析。在第一条指令里，还有一个源操作数，是-4(%rbp)，这代表了一个内存地址。CPU需要从内存地址里获取数据。
那CPU是如何从内存里获取数据的呢？这个过程其实比较复杂，是由多个步骤构成的，并不是一蹴而就的。
首先，CPU需要计算出内存地址。也就是从%rbp寄存器中取出现在的值，再减去4，得到要访问的数据的内存地址。这个地址计算的过程，通常也是由CPU内部一个单独的功能模块负责的。
那是不是从这个地址读取数据就行了呢？还不行，因为这个地址可能是个逻辑地址。现代CPU一般都有一个MMU单元。MMU是Memory Management Unit的缩写，也就是内存管理单元。它提供了虚拟内存管理的功能。也就是说，我们刚才计算出来的地址可能只是个逻辑地址，要经过MMU的翻译，才能获得物理的内存地址。
要实现完整的虚拟内存管理功能，还需要操作系统的支持，这个我们在下一节课还会探讨。
那现在，CPU终于得到了物理内存的地址。那么它会先从高速缓存中读数据，如果高速缓存中没有这个数据，才从内存加载。
你看，一个简单的内存访问功能，竟然涉及到这么多的细节。
解析完毕第一条指令之后，你大致也能理解第二条、第三条指令是如何执行的了。其中第二条指令，是做了一个加法运算，在这个过程中，会用到CPU内部的另一个功能单元：ALU，也就是算术逻辑运算单元。
到这里为止，我们已经提到了计算机内部的多个功能单元了，所以我们再把CPU内部的功能单元和流水线功能给总结一下。
CPU内部的功能单元和流水线对于CPU内部的结构，我们已经了解了高速缓存和寄存器。除此之外，CPU内部还包含了很多的功能单元，每个单元负责不同的功能。比如，有的单元负责获取指令，有的单元负责对指令译码，有的单元负责真正的运算，有的单元负责读取数据，有的单元负责写入数据，等等。
在阅读CPU的手册的时候，你会看到关于这个CPU的内部结构的一些信息，这个内部结构也被叫做微架构。你可以多看看这些图，即使你不能完全理解其中每个单元的含义，这也会有助于你理解CPU到底是如何运作的。下面这张图是我从Intel的手册中看到的Ice Lake型号的CPU的微架构的示意图：
我稍微解释一下这个微架构。你会看到，在图的左上角，指令高速缓存中的指令会被解码，解码后变成微指令。这里就涉及到了X86设计上的一些细节。X86使用的指令属于复杂指令集（CISC），CISC会针对特定的功能来设计一些指令，所以指令的执行效率会比较高，就像我们为了某个应用目的专门写一个程序来处理那样。
但复杂指令集也有坏处，就是指令的条数太多了，导致硬件设计会变得复杂，也不容易利用我们下面将要讲到的流水线的优势。所以，其实现代使用CISC的CPU，在内部设计上也借鉴了RISC的优点，把复杂的指令拆解成了简单的指令，或者叫做微指令，也就是图中的uop。
微指令会排成队列去执行任务，它们会到达一个调度器，由调度器调度不同的处理单元去完成不同的任务。调度器通过不同的端口（Port）来调度任务，不同的功能单元则在端口上接收任务。有的单元负责保存数据，有的单元负责加载数据，这些单元都会接到高速缓存上。还有几个端口是专门做计算的。不同的计算任务又分别由不同的计算单元承担，比如ALU是做算术运算的，LEA是做地址运算的，FMA是做浮点数运算的，等等。
不过，不同的CPU，其内部功能单元的划分是不同的。但总的来说，在执行一条指令的时候，CPU内部实际是多个单元按顺序去处理的，这被叫做指令流水线。不同CPU的流水线设计是不同的，有的分5个步骤，有的分成8个、10个甚至更多个步骤。
采用流水线技术最大的好处，就是我们不用等一条指令完全执行完毕，才去执行第二条指令。假设每条指令需要用到5个功能单元，分成5个步骤。那么在第一条指令的第一个步骤执行完毕以后，第一个功能单元就空出来了，就可以处理第二条指令了。总的来说，相当于有5条指令在并行运行。
当然了，实际上的执行过程并没有这么理想，因为不同的指令会用到不同的功能单元。比如上面示例程序的三条指令中，addl指令用到了ALU单元，而其他两条指令就没用到。而且，每个功能单元所需要的时钟周期也是不同的。所以，各条指令在执行过程中就会出现等待的情况。</description></item><item><title>13_为什么表数据删掉一半，表文件大小不变？</title><link>https://artisanbox.github.io/1/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/13/</guid><description>经常会有同学来问我，我的数据库占用空间太大，我把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？
那么今天，我就和你聊聊数据库表的空间回收，看看如何解决这个问题。
这里，我们还是针对MySQL中应用最广泛的InnoDB引擎展开讨论。一个InnoDB表包含两部分，即：表结构定义和数据。在MySQL 8.0版本以前，表结构是存在以.frm为后缀的文件里。而MySQL 8.0版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。
接下来，我会先和你说明为什么简单地删除表数据达不到表空间回收的效果，然后再和你介绍正确回收空间的方法。
参数innodb_file_per_table表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数innodb_file_per_table控制的：
这个参数设置为OFF表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；
这个参数设置为ON表示的是，每个InnoDB表数据存储在一个以 .ibd为后缀的文件中。
从MySQL 5.6.6版本开始，它的默认值就是ON了。
我建议你不论使用MySQL的哪个版本，都将这个值设置为ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过drop table命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。
所以，将innodb_file_per_table设置为ON，是推荐做法，我们接下来的讨论都是基于这个设置展开的。
我们在删除整个表的时候，可以使用drop table命令回收表空间。但是，我们遇到的更多的删除数据的场景是删除某些行，这时就遇到了我们文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。
我们要彻底搞明白这个问题的话，就要从数据删除流程说起了。
数据删除流程我们先再来看一下InnoDB中一个索引的示意图。在前面第4和第5篇文章中，我和你介绍索引时曾经提到过，InnoDB里的数据都是用B+树的结构组织的。
图1 B+树索引示意图假设，我们要删掉R4这个记录，InnoDB引擎只会把R4这个记录标记为删除。如果之后要再插入一个ID在300和600之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。
现在，你已经知道了InnoDB的数据是按页存储的，那么如果我们删掉了一个数据页上的所有记录，会怎么样？
答案是，整个数据页就可以被复用了。
但是，数据页的复用跟记录的复用是不同的。
记录的复用，只限于符合范围条件的数据。比如上面的这个例子，R4这条记录被删除后，如果插入一个ID是400的行，可以直接复用这个空间。但如果插入的是一个ID是800的行，就不能复用这个位置了。
而当整个页从B+树里面摘掉以后，可以复用到任何位置。以图1为例，如果将数据页page A上的所有记录删除以后，page A会被标记为可复用。这时候如果要插入一条ID=50的记录需要使用新页的时候，page A是可以被复用的。
如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。
进一步地，如果我们用delete命令把整个表的数据删除呢？结果就是，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。
你现在知道了，delete命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过delete命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。
实际上，不止是删除数据会造成空洞，插入数据也会。
如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。
假设图1中page A已经满了，这时我要再插入一行数据，会怎样呢？
图2 插入数据导致页分裂可以看到，由于page A满了，再插入一个ID是550的数据时，就不得不再申请一个新的页面page B来保存数据了。页分裂完成后，page A的末尾就留下了空洞（注意：实际上，可能不止1个记录的位置是空洞）。
另外，更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的。
也就是说，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。
而重建表，就可以达到这样的目的。
重建表试想一下，如果你现在有一个表A，需要做空间收缩，为了把表中存在的空洞去掉，你可以怎么做呢？
你可以新建一个与表A结构相同的表B，然后按照主键ID递增的顺序，把数据一行一行地从表A里读出来再插入到表B中。
由于表B是新建的表，所以表A主键索引上的空洞，在表B中就都不存在了。显然地，表B的主键索引更紧凑，数据页的利用率也更高。如果我们把表B作为临时表，数据从表A导入表B的操作完成后，用表B替换A，从效果上看，就起到了收缩表A空间的作用。
这里，你可以使用alter table A engine=InnoDB命令来重建表。在MySQL 5.5版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表B不需要你自己创建，MySQL会自动完成转存数据、交换表名、删除旧表的操作。
图3 改锁表DDL显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表A的话，就会造成数据丢失。因此，在整个DDL过程中，表A中不能有更新。也就是说，这个DDL不是Online的。
而在MySQL 5.6版本开始引入的Online DDL，对这个操作流程做了优化。
我给你简单描述一下引入了Online DDL之后，重建表的流程：
建立一个临时文件，扫描表A主键的所有数据页；
用数据页中表A的记录生成B+树，存储到临时文件中；</description></item><item><title>13_加法器：如何像搭乐高一样搭电路（上）？</title><link>https://artisanbox.github.io/4/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/13/</guid><description>上一讲，我们看到了如何通过电路，在计算机硬件层面设计最基本的单元，门电路。我给你看的门电路非常简单，只能做简单的 “与（AND）”“或（OR）”“NOT（非）”和“异或（XOR）”，这样最基本的单比特逻辑运算。下面这些门电路的标识，你需要非常熟悉，后续的电路都是由这些门电路组合起来的。
这些基本的门电路，是我们计算机硬件端的最基本的“积木”，就好像乐高积木里面最简单的小方块。看似不起眼，但是把它们组合起来，最终可以搭出一个星球大战里面千年隼这样的大玩意儿。我们今天包含十亿级别晶体管的现代CPU，都是由这样一个一个的门电路组合而成的。
图片来源异或门和半加器我们看到的基础门电路，输入都是两个单独的bit，输出是一个单独的bit。如果我们要对2个8 位（bit）的数，计算与、或、非这样的简单逻辑运算，其实很容易。只要连续摆放8个开关，来代表一个8位数。这样的两组开关，从左到右，上下单个的位开关之间，都统一用“与门”或者“或门”连起来，就是两个8位数的AND或者OR的运算了。
比起AND或者OR这样的电路外，要想实现整数的加法，就需要组建稍微复杂一点儿的电路了。
我们先回归一个最简单的8位的无符号整数的加法。这里的“无符号”，表示我们并不需要使用补码来表示负数。无论高位是“0”还是“1”，这个整数都是一个正数。
我们很直观就可以想到，要表示一个8位数的整数，简单地用8个bit，也就是8个像上一讲的电路开关就好了。那2个8位整数的加法，就是2排8个开关。加法得到的结果也是一个8位的整数，所以又需要1排8位的开关。要想实现加法，我们就要看一下，通过什么样的门电路，能够连接起加数和被加数，得到最后期望的和。
其实加法器就是想一个办法把这三排开关电路连起来要做到这一点，我们先来看看，我们人在计算加法的时候一般会怎么操作。二进制的加法和十进制没什么区别，所以我们一样可以用列竖式来计算。我们仍然是从右到左，一位一位进行计算，只是把从逢10进1变成逢2进1。
你会发现，其实计算一位数的加法很简单。我们先就看最简单的个位数。输入一共是4种组合，00、01、10、11。得到的结果，也不复杂。
一方面，我们需要知道，加法计算之后的个位是什么，在输入的两位是00和11的情况下，对应的输出都应该是0；在输入的两位是10和01的情况下，输出都是1。结果你会发现，这个输入和输出的对应关系，其实就是我在上一讲留给你的思考题里面的“异或门（XOR）”。
讲与、或、非门的时候，我们很容易就能和程序里面的“AND（通常是&amp;amp;符号）”“ OR（通常是 | 符号）”和“ NOT（通常是 !符号）”对应起来。可能你没有想过，为什么我们会需要“异或（XOR）”，这样一个在逻辑运算里面没有出现的形式，作为一个基本电路。其实，异或门就是一个最简单的整数加法，所需要使用的基本门电路。
算完个位的输出还不算完，输入的两位都是11的时候，我们还需要向更左侧的一位进行进位。那这个就对应一个与门，也就是有且只有在加数和被加数都是1的时候，我们的进位才会是1。
所以，通过一个异或门计算出个位，通过一个与门计算出是否进位，我们就通过电路算出了一个一位数的加法。于是，我们把两个门电路打包，给它取一个名字，就叫作半加器（Half Adder）。
半加器的电路演示全加器你肯定很奇怪，为什么我们给这样的电路组合，取名叫半加器（Half Adder）？莫非还有一个全加器（Full Adder）么？你猜得没错。半加器可以解决个位的加法问题，但是如果放到二位上来说，就不够用了。我们这里的竖式是个二进制的加法，所以如果从右往左数，第二列不是十位，我称之为“二位”。对应的再往左，就应该分别是四位、八位。
二位用一个半加器不能计算完成的原因也很简单。因为二位除了一个加数和被加数之外，还需要加上来自个位的进位信号，一共需要三个数进行相加，才能得到结果。但是我们目前用到的，无论是最简单的门电路，还是用两个门电路组合而成的半加器，输入都只能是两个bit，也就是两个开关。那我们该怎么办呢？
实际上，解决方案也并不复杂。我们用两个半加器和一个或门，就能组合成一个全加器。第一个半加器，我们用和个位的加法一样的方式，得到是否进位X和对应的二个数加和后的结果Y，这样两个输出。然后，我们把这个加和后的结果Y，和个位数相加后输出的进位信息U，再连接到一个半加器上，就会再拿到一个是否进位的信号V和对应的加和后的结果W。
全加器就是两个半加器加上一个或门这个W就是我们在二位上留下的结果。我们把两个半加器的进位输出，作为一个或门的输入连接起来，只要两次加法中任何一次需要进位，那么在二位上，我们就会向左侧的四位进一位。因为一共只有三个bit相加，即使3个bit都是1，也最多会进一位。
这样，通过两个半加器和一个或门，我们就得到了一个，能够接受进位信号、加数和被加数，这样三个数组成的加法。这就是我们需要的全加器。
有了全加器，我们要进行对应的两个8 bit数的加法就很容易了。我们只要把8个全加器串联起来就好了。个位的全加器的进位信号作为二位全加器的输入信号，二位全加器的进位信号再作为四位的全加器的进位信号。这样一层层串接八层，我们就得到了一个支持8位数加法的算术单元。如果要扩展到16位、32位，乃至64位，都只需要多串联几个输入位和全加器就好了。
8位加法器可以由8个全加器串联而成唯一需要注意的是，对于这个全加器，在个位，我们只需要用一个半加器，或者让全加器的进位输入始终是0。因为个位没有来自更右侧的进位。而最左侧的一位输出的进位信号，表示的并不是再进一位，而是表示我们的加法是否溢出了。
这也是很有意思的一点。以前我自己在了解二进制加法的时候，一直有这么个疑问，既然int这样的16位的整数加法，结果也是16位数，那我们怎么知道加法最终是否溢出了呢？因为结果也只存得下加法结果的16位数。我们并没有留下一个第17位，来记录这个加法的结果是否溢出。
看到全加器的电路设计，相信你应该明白，在整个加法器的结果中，我们其实有一个电路的信号，会标识出加法的结果是否溢出。我们可以把这个对应的信号，输出给到硬件中其他标志位里，让我们的计算机知道计算的结果是否溢出。而现代计算机也正是这样做的。这就是为什么你在撰写程序的时候，能够知道你的计算结果是否溢出在硬件层面得到的支持。
总结延伸相信到这里，你应该已经体会到了，通过门电路来搭建算术计算的一个小功能，就好像搭乐高积木一样。
我们用两个门电路，搭出一个半加器，就好像我们拿两块乐高，叠在一起，变成一个长方形的乐高，这样我们就有了一个新的积木组件，柱子。我们再用两个柱子和一个长条的积木组合一下，就变成一个积木桥。然后几个积木桥串接在一起，又成了积木楼梯。
当我们想要搭建一个摩天大楼，我们需要很多很多楼梯。但是这个时候，我们已经不再关注最基础的一节楼梯是怎么用一块块积木搭建起来的。这其实就是计算机中，无论软件还是硬件中一个很重要的设计思想，分层。
从简单到复杂，我们一层层搭出了拥有更强能力的功能组件。在上面的一层，我们只需要考虑怎么用下一层的组件搭建出自己的功能，而不需要下沉到更低层的其他组件。就像你之前并没有深入学习过计算机组成原理，一样可以直接通过高级语言撰写代码，实现功能。
在硬件层面，我们通过门电路、半加器、全加器一层层搭出了加法器这样的功能组件。我们把这些用来做算术逻辑计算的组件叫作ALU，也就是算术逻辑单元。当进一步打造强大的CPU时，我们不会再去关注最细颗粒的门电路，只需要把门电路组合而成的ALU，当成一个能够完成基础计算的黑盒子就可以了。
以此类推，后面我们讲解CPU的设计和数据通路的时候，我们以ALU为一个基础单元来解释问题，也就够了。
补充阅读出于性能考虑，实际CPU里面使用的加法器，比起我们今天讲解的电路还有些差别，会更复杂一些。真实的加法器，使用的是一种叫作超前进位加法器的东西。你可以找到北京大学在Coursera上开设的《计算机组成》课程中的Video-306 “加法器优化”一节，了解一下超前进位加法器的实现原理，以及我们为什么要使用它。
课后思考这一讲，我给你详细讲解了无符号数的加法器是怎么通过电路搭建出来的。那么，如果是使用补码表示的有符号数，这个加法器是否可以实现正数加负数这样的运算呢？如果不行，我们应该怎么搭建对应的电路呢？
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>13_线性排序：如何根据年龄给100万用户数据排序？</title><link>https://artisanbox.github.io/2/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/14/</guid><description>上两节中，我带你着重分析了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。今天，我会讲三种时间复杂度是O(n)的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。
这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以我们今天的学习重点是掌握这些排序算法的适用场景。
按照惯例，我先给你出一道思考题：如何根据年龄给100万用户排序？ 你可能会说，我用上一节课讲的归并、快排就可以搞定啊！是的，它们也可以完成功能，但是时间复杂度最低也是O(nlogn)。有没有更快的排序方法呢？让我们一起进入今天的内容！
桶排序（Bucket sort）首先，我们来看桶排序。桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。
桶排序的时间复杂度为什么是O(n)呢？我们一块儿来分析一下。
如果要排序的数据有n个，我们把它们均匀地划分到m个桶内，每个桶里就有k=n/m个元素。每个桶内部使用快速排序，时间复杂度为O(k * logk)。m个桶排序的时间复杂度就是O(m * k * logk)，因为k=n/m，所以整个桶排序的时间复杂度就是O(n*log(n/m))。当桶的个数m接近数据个数n时，log(n/m)就是一个非常小的常量，这个时候桶排序的时间复杂度接近O(n)。
桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？
答案当然是否定的。为了让你轻松理解桶排序的核心思想，我刚才做了很多假设。实际上，桶排序对要排序数据的要求是非常苛刻的。
首先，要排序的数据需要很容易就能划分成m个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。
其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为O(nlogn)的排序算法了。
桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。
比如说我们有10GB的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百MB，没办法一次性把10GB的数据都加载到内存中。这个时候该怎么办呢？
现在我来讲一下，如何借助桶排序的处理思想来解决这个问题。
我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是1元，最大是10万元。我们将所有订单根据金额划分到100个桶里，第一个桶我们存储金额在1元到1000元之内的订单，第二桶存储金额在1001元到2000元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02...99）。
理想的情况下，如果订单金额在1到10万之间均匀分布，那订单会被均匀划分到100个文件中，每个小文件中存储大约100MB的订单数据，我们就可以将这100个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。
不过，你可能也发现了，订单按照金额在1元到10万元之间并不一定是均匀分布的 ，所以10GB订单数据是无法均匀地被划分到100个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该怎么办呢？
针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在1元到1000元之间的比较多，我们就将这个区间继续划分为10个小区间，1元到100元，101元到200元，201元到300元....901元到1000元。如果划分之后，101元到200元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。
计数排序（Counting sort）我个人觉得，计数排序其实是桶排序的一种特殊情况。当要排序的n个数据，所处的范围并不大的时候，比如最大值是k，我们就可以把数据划分成k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。
我们都经历过高考，高考查分数系统你还记得吗？我们查分数的时候，系统会显示我们的成绩以及所在省的排名。如果你所在的省有50万考生，如何通过成绩快速排序得出名次呢？
考生的满分是900分，最小是0分，这个数据的范围很小，所以我们可以分成901个桶，对应分数从0分到900分。根据考生的成绩，我们将这50万考生划分到这901个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了50万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是O(n)。
计数排序的算法思想就是这么简单，跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？
想弄明白这个问题，我们就要来看计数排序算法的实现方法。我还拿考生那个例子来解释。为了方便说明，我对数据规模做了简化。假设只有8个考生，分数在0到5分之间。这8个考生的成绩我们放在一个数组A[8]中，它们分别是：2，5，3，0，2，3，0，3。
考生的成绩从0到5分，我们使用大小为6的数组C[6]表示桶，其中下标对应分数。不过，C[6]内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到C[6]的值。
从图中可以看出，分数为3分的考生有3个，小于3分的考生有4个，所以，成绩为3分的考生在排序之后的有序数组R[8]中，会保存下标4，5，6的位置。
那我们如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？这个处理方法非常巧妙，很不容易想到。
思路是这样的：我们对C[6]数组顺序求和，C[6]存储的数据就变成了下面这样子。C[k]里存储小于等于分数k的考生个数。
有了前面的数据准备之后，现在我就要讲计数排序中最复杂、最难理解的一部分了，请集中精力跟着我的思路！
我们从后到前依次扫描数组A。比如，当扫描到3时，我们可以从数组C中取出下标为3的值7，也就是说，到目前为止，包括自己在内，分数小于等于3的考生有7个，也就是说3是数组R中的第7个元素（也就是数组R中下标为6的位置）。当3放入到数组R中后，小于等于3的元素就只剩下了6个了，所以相应的C[3]要减1，变成6。
以此类推，当我们扫描到第2个分数为3的考生的时候，就会把它放入数组R中的第6个元素的位置（也就是下标为5的位置）。当我们扫描完整个数组A后，数组R内的数据就是按照分数从小到大有序排列的了。
上面的过程有点复杂，我写成了代码，你可以对照着看下。
// 计数排序，a是数组，n是数组大小。假设数组中存储的都是非负整数。 public void countingSort(int[] a, int n) { if (n &amp;lt;= 1) return; // 查找数组中数据的范围 int max = a[0]; for (int i = 1; i &amp;lt; n; ++i) { if (max &amp;lt; a[i]) { max = a[i]; } }</description></item><item><title>13｜物理机上程序运行的软件环境是怎么样的？</title><link>https://artisanbox.github.io/3/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/15/</guid><description>你好，我是宫文学。
上一节课，我们主要讨论了程序运行的硬件环境。在某些编程场景下（比如嵌入式编程），我们的语言需要直接跑在裸设备上。所以，你要能够理解在裸设备上运行某个语言的程序所需要的技术。
不过，现代语言大部分情况下都运行在某个操作系统里，操作系统为语言提供了基础的运行环境，比如定义了可执行文件的格式、程序在内存中的布局、内存管理机制，还有并发机制等等。计算机语言需要跟操作系统紧密配合，才能更好地运行。
今天这节课，我们就来讨论一下操作系统中与计算机语言有关的那些知识点，包括内存管理、任务管理和ABI，为进一步实现我们的计算机语言打下良好的基础。
首先我们来看看操作系统的内存管理功能和程序的关系。
内存管理操作系统的一个重要功能就是管理内存，它会把内存虚拟化，并进行内存访问的权限管理。
那什么是虚拟化呢？
虚拟化就是让每个进程都有一个自己可以用的寻址空间，不过这些地址是假的地址。但就这些假地址，我们通过指令发给CPU，CPU也是认的。因为CPU中有一个内存管理单元，缩写是MMU，它能够根据这个逻辑地址，计算出在内存里真实的物理地址。MMU还可以跟操作系统配合，设置每个内存页面的权限，包括是否可读、可写和可执行。
这种逻辑地址与物理地址转换的功能，对我们做编译很有用。我们的程序编译成目标代码的时候，里面的每个函数、常量和全局变量的地址，都是确定的。这样，在操作系统加载了可执行程序以后，就可以正确地调用每个函数，访问每个数据了。
如果没有这个逻辑地址的功能，那运行多个程序就困难了。我们要保证每个程序使用的地址都互相不冲突才行，否则大家就乱了套了。在不使用操作系统的编程领域（比如某些嵌入式编程），重点就要解决好这些问题。
在虚拟化机制下，只有你用到某个地址，操作系统才会为这个地址分配真实的物理内存。这些物理内存一般划分成页来管理，MMU会根据这些分页信息把逻辑地址转换成物理地址。
这里我们横向延展一下，我们在第11节课说过，在栈里申请内存的时候很简单，只需要移动一下栈顶指针就行了，其实这个内部机制和我们上面说的虚拟化也是有关的。
在X64架构下，栈顶指针使用的是rsp寄存器。但其实，这时候并没有真正分配内存，你只是改变了寄存器的值而已。但如果你访问栈里的某个地址，而且这个地址又没有被分配物理的内存页，那么CPU在访问内存的时候就会知道这里出错了。它就会触发一个缺页中断，跳转到中断处理程序，去分配页面。分配完毕以后，又跳回原来的程序接着执行，我们的程序并不知道背后发生了这么多的事情。
说到中断，我再额外跟你补充一点。CPU在处理中断的时候，要保护当前的现场，比如各个寄存器的状态。否则，如果各个寄存器的值被弄乱了，原来的程序就没法执行了。然后在返回原来的程序的时候，CPU要恢复现场。整个过程对我们的程序是透明的。
所以说，这个rsp只是起到一个标记作用，是我们的程序跟操作系统之间的一个约定。我们只要修改了rsp里的值，操作系统就要保证给我们提供足够的内存。如果你违背了这个约定去乱访问一些地址，在MMU和操作系统的配合下，也会被识别出来，就会报内存访问的错误。
既然这只是个约定，那么有的操作系统就比较为程序着想了，它规定你可以访问栈顶之外的一定范围内的内存。比如，Linux和大多数类Unix的系统都遵循System V AMD64 ABI，它规定可以访问栈顶之外的128个字节范围内的内存。
这有什么好处呢？好处是，对于程序中的叶子函数，也就是这个函数没有调用其他函数，并且它所使用的数据不会超过128个字节的情况，我们根本不需要去建立栈桢，也就省去了把栈顶指针的值保存到内存，修改栈顶指针，最后再从内存中恢复栈顶指针这一系列操作，这样就节省了大量的内存读写时间，让系统性能得到优化。
在堆中申请内存也是一样，也不是真实的分配，只是提供了一些标记信息，之后可供MMU使用而已。
好了，关于从栈和堆里申请内存的延伸就到这里，我们回归主线，继续来看虚拟化机制带来的结果。简单的说，虚拟化机制，可以让运行中的各种程序，使用相同的逻辑地址，但实际上对应的是不同的物理地址。这样各个程序加载到内存后，就都可以使用标准的内存布局。
那一个可执行程序在内存中的布局情况是怎样的呢？我们用一个C语言的程序的例子来分析一下。
在Linux或macOS系统中，一个C语言的程序加载到内存以后，它的内存布局大概是下面的样子：
你可以看到，其中代码段（.text）和数据段（包括.data和.bss）是从可执行文件直接加载进内存的。可执行文件中提前计算好的函数、常量和全局变量的地址，也就变成了内存中的地址。
另外两个重要的区域是栈和堆。栈是从高地址向低地址延伸的，而堆则是从低地址向高地址延伸的。
但是，在不同的操作系统中，上图中每个部分的具体地址都是不大相同的，比如，macOS和Linux的就不同。不过，你可以写个程序，打印出不同区域中的地址。我写了个示例程序address.c，你可以参考一下，这里你要好好琢磨一下示例代码中每个地址是如何获取的。
另外，我是在macOS上运行的，如果你用的是不同的操作系统，也可以运行一下，看看打印出来的地址跟我的有什么区别。
这个程序是这样的：
#include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;stdio.h&amp;gt; //全局变量 int global_a = 10; char * global_b = &amp;quot;hello&amp;quot;;
int main(int argc, char** argv){ printf(&amp;quot;命令行参数/&amp;amp;quot;address&amp;amp;quot;的地址:\t0x%12lX\n&amp;quot;, (size_t)argv[0]); printf(&amp;quot;命令行参数/argv数组的地址: \t0x%12lX\n&amp;quot;, (size_t)argv);
printf(&amp;amp;quot;栈/参数argc的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;argc); printf(&amp;amp;quot;栈/参数argv的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;argv); int local_a = 20; printf(&amp;amp;quot;栈/local_a的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;local_a); int * local_b = (int*)malloc(sizeof(int)); printf(&amp;amp;quot;栈/local_b的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;local_b); printf(&amp;amp;quot;堆/local_b指向的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)local_b); free(local_b); printf(&amp;amp;quot;data段/global_b的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;global_b); printf(&amp;amp;quot;data段/global_a的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)&amp;amp;amp;global_a); printf(&amp;amp;quot;text段/\&amp;amp;quot;hello\&amp;amp;quot;的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)global_b); printf(&amp;amp;quot;text段/main函数的地址: \t0x%12lX\n&amp;amp;quot;, (size_t)main); }</description></item><item><title>14_count(x)这么慢，我该怎么办？</title><link>https://artisanbox.github.io/1/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/14/</guid><description>在开发系统的时候，你可能经常需要计算一个表的行数，比如一个交易系统的所有变更记录总数。这时候你可能会想，一条select count(*) from t 语句不就解决了吗？
但是，你会发现随着系统中记录数越来越多，这条语句执行得也会越来越慢。然后你可能就想了，MySQL怎么这么笨啊，记个总数，每次要查的时候直接读出来，不就好了吗。
那么今天，我们就来聊聊count(*)语句到底是怎样实现的，以及MySQL为什么会这么实现。然后，我会再和你说说，如果应用中有这种频繁变更并需要统计表行数的需求，业务设计上可以怎么做。
count(*)的实现方式你首先要明确的是，在不同的MySQL引擎中，count(*)有不同的实现方式。
MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高； 而InnoDB引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。 这里需要注意的是，我们在这篇文章里讨论的是没有过滤条件的count(*)，如果加了where 条件的话，MyISAM表也是不能返回得这么快的。
在前面的文章中，我们一起分析了为什么要使用InnoDB，因为不论是在事务支持、并发能力还是在数据安全方面，InnoDB都优于MyISAM。我猜你的表也一定是用了InnoDB引擎。这就是当你的记录数越来越多的时候，计算一个表的总行数会越来越慢的原因。
那为什么InnoDB不跟MyISAM一样，也把数字存起来呢？
这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。这里，我用一个算count(*)的例子来为你解释一下。
假设表t中现在有10000条记录，我们设计了三个用户并行的会话。
会话A先启动事务并查询一次表的总行数； 会话B启动事务，插入一行后记录后，查询表的总行数； 会话C先启动一个单独的语句，插入一行记录后，查询表的总行数。 我们假设从上到下是按照时间顺序执行的，同一行语句是在同一时刻执行的。
图1 会话A、B、C的执行流程你会看到，在最后一个时刻，三个会话A、B、C会同时查询表t的总行数，但拿到的结果却不同。
这和InnoDB的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是MVCC来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于count(*)请求来说，InnoDB只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。
备注：如果你对MVCC记忆模糊了，可以再回顾下第3篇文章《事务隔离：为什么你改了我还看不见？》和第8篇文章《事务到底是隔离的还是不隔离的？》中的相关内容。
当然，现在这个看上去笨笨的MySQL，在执行count(*)操作的时候还是做了优化的。
你知道的，InnoDB是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于count(*)这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。
如果你用过show table status 命令的话，就会发现这个命令的输出结果里面也有一个TABLE_ROWS用于显示这个表当前有多少行，这个命令执行挺快的，那这个TABLE_ROWS能代替count(*)吗？
你可能还记得在第10篇文章《 MySQL为什么有时候会选错索引？》中我提到过，索引统计的值是通过采样来估算的。实际上，TABLE_ROWS就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到40%到50%。所以，show table status命令显示的行数也不能直接使用。
到这里我们小结一下：
MyISAM表虽然count(*)很快，但是不支持事务； show table status命令虽然返回很快，但是不准确； InnoDB表直接count(*)会遍历全表，虽然结果准确，但会导致性能问题。 那么，回到文章开头的问题，如果你现在有一个页面经常要显示交易系统的操作记录总数，到底应该怎么办呢？答案是，我们只能自己计数。
接下来，我们讨论一下，看看自己计数有哪些方法，以及每种方法的优缺点有哪些。
这里，我先和你说一下这些方法的基本思路：你需要自己找一个地方，把操作记录表的行数存起来。
用缓存系统保存计数对于更新很频繁的库来说，你可能会第一时间想到，用缓存系统来支持。
你可以用一个Redis服务来保存这个表的总行数。这个表每被插入一行Redis计数就加1，每被删除一行Redis计数就减1。这种方式下，读和更新操作都很快，但你再想一下这种方式存在什么问题吗？
没错，缓存系统可能会丢失更新。
Redis的数据不能永久地留在内存里，所以你会找一个地方把这个值定期地持久化存储起来。但即使这样，仍然可能丢失更新。试想如果刚刚在数据表中插入了一行，Redis中保存的值也加了1，然后Redis异常重启了，重启后你要从存储redis数据的地方把这个值读回来，而刚刚加1的这个计数操作却丢失了。
当然了，这还是有解的。比如，Redis异常重启以后，到数据库里面单独执行一次count(*)获取真实的行数，再把这个值写回到Redis里就可以了。异常重启毕竟不是经常出现的情况，这一次全表扫描的成本，还是可以接受的。
但实际上，将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使Redis正常工作，这个值还是逻辑上不精确的。
你可以设想一下有这么一个页面，要显示操作记录的总数，同时还要显示最近操作的100条记录。那么，这个页面的逻辑就需要先到Redis里面取出计数，再到数据表里面取数据记录。
我们是这么定义不精确的：
一种是，查到的100行结果里面有最新插入记录，而Redis的计数里还没加1；
另一种是，查到的100行结果里没有最新插入的记录，而Redis的计数里已经加了1。
这两种情况，都是逻辑不一致的。
我们一起来看看这个时序图。
图2 会话A、B执行时序图图2中，会话A是一个插入交易记录的逻辑，往数据表里插入一行R，然后Redis计数加1；会话B就是查询页面显示时需要的数据。
在图2的这个时序里，在T3时刻会话B来查询的时候，会显示出新插入的R这个记录，但是Redis的计数还没加1。这时候，就会出现我们说的数据不一致。
你一定会说，这是因为我们执行新增记录逻辑时候，是先写数据表，再改Redis计数。而读的时候是先读Redis，再读数据表，这个顺序是相反的。那么，如果保持顺序一样的话，是不是就没问题了？我们现在把会话A的更新顺序换一下，再看看执行结果。
图3 调整顺序后，会话A、B的执行时序图你会发现，这时候反过来了，会话B在T3时刻查询的时候，Redis计数加了1了，但还查不到新插入的R这一行，也是数据不一致的情况。</description></item><item><title>14_乘法器：如何像搭乐高一样搭电路（下）？</title><link>https://artisanbox.github.io/4/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/14/</guid><description>和学习小学数学一样，学完了加法之后，我们自然而然就要来学习乘法。既然是退回到小学，我们就把问题搞得简单一点，先来看两个4位数的乘法。这里的4位数，当然还是一个二进制数。我们是人类而不是电路，自然还是用列竖式的方式来进行计算。
十进制中的13乘以9，计算的结果应该是117。我们通过转换成二进制，然后列竖式的办法，来看看整个计算的过程是怎样的。
顺序乘法的实现过程从列出竖式的过程中，你会发现，二进制的乘法有个很大的优点，就是这个过程你不需要背九九乘法口诀表了。因为单个位置上，乘数只能是0或者1，所以实际的乘法，就退化成了位移和加法。
在13×9这个例子里面，被乘数13表示成二进制是1101，乘数9在二进制里面是1001。最右边的个位是1，所以个位乘以被乘数，就是把被乘数1101复制下来。因为二位和四位都是0，所以乘以被乘数都是0，那么保留下来的都是0000。乘数的八位是1，我们仍然需要把被乘数1101复制下来。不过这里和个位位置的单纯复制有一点小小的差别，那就是要把复制好的结果向左侧移三位，然后把四位单独进行乘法加位移的结果，再加起来，我们就得到了最终的计算结果。
对应到我们之前讲的数字电路和ALU，你可以看到，最后一步的加法，我们可以用上一讲的加法器来实现。乘法因为只有“0”和“1”两种情况，所以可以做成输入输出都是4个开关，中间用1个开关，同时来控制这8个开关的方式，这就实现了二进制下的单位的乘法。
我们可以用一个开关来决定，下面的输出是完全复制输入，还是将输出全部设置为0至于位移也不麻烦，我们只要不是直接连线，把正对着的开关之间进行接通，而是斜着错开位置去接就好了。如果要左移一位，就错开一位接线；如果要左移两位，就错开两位接线。
把对应的线路错位连接，就可以起到位移的作用这样，你会发现，我们并不需要引入任何新的、更复杂的电路，仍然用最基础的电路，只要用不同的接线方式，就能够实现一个“列竖式”的乘法。而且，因为二进制下，只有0和1，也就是开关的开和闭这两种情况，所以我们的计算机也不需要去“背诵”九九乘法口诀表，不需要单独实现一个更复杂的电路，就能够实现乘法。
为了节约一点开关，也就是晶体管的数量。实际上，像13×9这样两个四位数的乘法，我们不需要把四次单位乘法的结果，用四组独立的开关单独都记录下来，然后再把这四个数加起来。因为这样做，需要很多组开关，如果我们计算一个32位的整数乘法，就要32组开关，太浪费晶体管了。如果我们顺序地来计算，只需要一组开关就好了。
我们先拿乘数最右侧的个位乘以被乘数，然后把结果写入用来存放计算结果的开关里面，然后，把被乘数左移一位，把乘数右移一位，仍然用乘数去乘以被乘数，然后把结果加到刚才的结果上。反复重复这一步骤，直到不能再左移和右移位置。这样，乘数和被乘数就像两列相向而驶的列车，仅仅需要简单的加法器、一个可以左移一位的电路和一个右移一位的电路，就能完成整个乘法。
乘法器硬件结构示意图你看这里画的乘法器硬件结构示意图。这里的控制测试，其实就是通过一个时钟信号，来控制左移、右移以及重新计算乘法和加法的时机。我们还是以计算13×9，也就是二进制的1101×1001来具体看。
这个计算方式虽然节约电路了，但是也有一个很大的缺点，那就是慢。
你应该很容易就能发现，在这个乘法器的实现过程里，我们其实就是把乘法展开，变成了“加法+位移”来实现。我们用的是4位数，所以要进行4组“位移+加法”的操作。而且这4组操作还不能同时进行。因为下一组的加法要依赖上一组的加法后的计算结果，下一组的位移也要依赖上一组的位移的结果。这样，整个算法是“顺序”的，每一组加法或者位移的运算都需要一定的时间。
所以，最终这个乘法的计算速度，其实和我们要计算的数的位数有关。比如，这里的4位，就需要4次加法。而我们的现代CPU常常要用32位或者是64位来表示整数，那么对应就需要32次或者64次加法。比起4位数，要多花上8倍乃至16倍的时间。
换个我们在算法和数据结构中的术语来说就是，这样的一个顺序乘法器硬件进行计算的时间复杂度是 O(N)。这里的N，就是乘法的数里面的位数。
并行加速方法那么，我们有没有办法，把时间复杂度上降下来呢？研究数据结构和算法的时候，我们总是希望能够把O(N)的时间复杂度，降低到O(logN)。办法还真的有。和软件开发里面改算法一样，在涉及CPU和电路的时候，我们可以改电路。
32位数虽然是32次加法，但是我们可以让很多加法同时进行。回到这一讲开始，我们把位移和乘法的计算结果加到中间结果里的方法，32位整数的乘法，其实就变成了32个整数相加。
前面顺序乘法器硬件的实现办法，就好像体育比赛里面的单败淘汰赛。只有一个擂台会存下最新的计算结果。每一场新的比赛就来一个新的选手，实现一次加法，实现完了剩下的还是原来那个守擂的，直到其余31个选手都上来比过一场。如果一场比赛需要一天，那么一共要比31场，也就是31天。
目前的乘法实现就像是单败淘汰赛加速的办法，就是把比赛变成像世界杯足球赛那样的淘汰赛，32个球队捉对厮杀，同时开赛。这样一天一下子就淘汰了16支队，也就是说，32个数两两相加后，你可以得到16个结果。后面的比赛也是一样同时开赛捉对厮杀。只需要5天，也就是O(log2N)的时间，就能得到计算的结果。但是这种方式要求我们得有16个球场。因为在淘汰赛的第一轮，我们需要16场比赛同时进行。对应到我们CPU的硬件上，就是需要更多的晶体管开关，来放下中间计算结果。
通过并联更多的ALU，加上更多的寄存器，我们也能加速乘法电路并行上面我们说的并行加速的办法，看起来还是有点儿笨。我们回头来做一个抽象的思考。之所以我们的计算会慢，核心原因其实是“顺序”计算，也就是说，要等前面的计算结果完成之后，我们才能得到后面的计算结果。
最典型的例子就是我们上一讲讲的加法器。每一个全加器，都要等待上一个全加器，把对应的进入输入结果算出来，才能算下一位的输出。位数越多，越往高位走，等待前面的步骤就越多，这个等待的时间有个专门的名词，叫作门延迟（Gate Delay）。
每通过一个门电路，我们就要等待门电路的计算结果，就是一层的门电路延迟，我们一般给它取一个“T”作为符号。一个全加器，其实就已经有了3T的延迟（进位需要经过3个门电路）。而4位整数，最高位的计算需要等待前面三个全加器的进位结果，也就是要等9T的延迟。如果是64位整数，那就要变成63×3=189T的延迟。这可不是个小数字啊！
除了门延迟之外，还有一个问题就是时钟频率。在上面的顺序乘法计算里面，如果我们想要用更少的电路，计算的中间结果需要保存在寄存器里面，然后等待下一个时钟周期的到来，控制测试信号才能进行下一次移位和加法，这个延迟比上面的门延迟更可观。
那么，我们有什么办法可以解决这个问题呢？实际上，在我们进行加法的时候，如果相加的两个数是确定的，那高位是否会进位其实也是确定的。对于我们人来说，我们本身去做计算都是顺序执行的，所以要一步一步计算进位。但是，计算机是连结的各种线路。我们不用让计算机模拟人脑的思考方式，来连结线路。
那怎么才能把线路连结得复杂一点，让高位和低位的计算同时出结果呢？怎样才能让高位不需要等待低位的进位结果，而是把低位的所有输入信号都放进来，直接计算出高位的计算结果和进位结果呢？
我们只要把进位部分的电路完全展开就好了。我们的半加器到全加器，再到加法器，都是用最基础的门电路组合而成的。门电路的计算逻辑，可以像我们做数学里面的多项式乘法一样完全展开。在展开之后呢，我们可以把原来需要较少的，但是有较多层前后计算依赖关系的门电路，展开成需要较多的，但是依赖关系更少的门电路。
我在这里画了一个示意图，展示了一下我们加法器。如果我们完全展开电路，高位的进位和计算结果，可以和低位的计算结果同时获得。这个的核心原因是电路是天然并行的，一个输入信号，可以同时传播到所有接通的线路当中。
C4是前4位的计算结果是否进位的门电路表示如果一个4位整数最高位是否进位，展开门电路图，你会发现，我们只需要3T的延迟就可以拿到是否进位的计算结果。而对于64位的整数，也不会增加门延迟，只是从上往下复制这个电路，接入更多的信号而已。看到没？我们通过把电路变复杂，就解决了延迟的问题。
这个优化，本质上是利用了电路天然的并行性。电路只要接通，输入的信号自动传播到了所有接通的线路里面，这其实也是硬件和软件最大的不同。
无论是这里把对应的门电路逻辑进行完全展开以减少门延迟，还是上面的乘法通过并行计算多个位的乘法，都是把我们完成一个计算的电路变复杂了。而电路变复杂了，也就意味着晶体管变多了。
之前很多同学在我们讨论计算机的性能问题的时候，都提到，为什么晶体管的数量增加可以优化计算机的计算性能。实际上，这里的门电路展开和上面的并行计算乘法都是很好的例子。我们通过更多的晶体管，就可以拿到更低的门延迟，以及用更少的时钟周期完成一个计算指令。
总结延伸讲到这里，相信你已经发现，我们通过之前两讲的ALU和门电路，搭建出来了乘法器。如果愿意的话，我们可以把很多在生活中不得不顺序执行的事情，通过简单地连结一下线路，就变成并行执行了。这是因为，硬件电路有一个很大的特点，那就是信号都是实时传输的。
我们也看到了，通过精巧地设计电路，用较少的门电路和寄存器，就能够计算完成乘法这样相对复杂的运算。是用更少更简单的电路，但是需要更长的门延迟和时钟周期；还是用更复杂的电路，但是更短的门延迟和时钟周期来计算一个复杂的指令，这之间的权衡，其实就是计算机体系结构中RISC和CISC的经典历史路线之争。
推荐阅读如果还有什么细节你觉得还没有彻底弄明白，我推荐你看一看《计算机组成与设计：硬件/软件接口》的3.3节。
课后思考这一讲里，我为你讲解了乘法器是怎么实现的。那么，请你想一想，如果我们想要用电路实现一个除法器，应该怎么做呢？需要注意一下，除法器除了要计算除法的商之外，还要计算出对应的余数。
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>14_排序优化：如何实现一个通用的、高性能的排序函数？</title><link>https://artisanbox.github.io/2/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/15/</guid><description>几乎所有的编程语言都会提供排序函数，比如C语言中qsort()，C++ STL中的sort()、stable_sort()，还有Java语言中的Collections.sort()。在平时的开发中，我们也都是直接使用这些现成的函数来实现业务逻辑中的排序功能。那你知道这些排序函数是如何实现的吗？底层都利用了哪种排序算法呢？
基于这些问题，今天我们就来看排序这部分的最后一块内容：如何实现一个通用的、高性能的排序函数？
如何选择合适的排序算法？如果要实现一个通用的、高效率的排序函数，我们应该选择哪种排序算法？我们先回顾一下前面讲过的几种排序算法。
我们前面讲过，线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。
如果对小规模数据进行排序，可以选择时间复杂度是O(n2)的算法；如果对大规模数据进行排序，时间复杂度是O(nlogn)的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是O(nlogn)的排序算法来实现排序函数。
时间复杂度是O(nlogn)的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。
不知道你有没有发现，使用归并排序的情况其实并不多。我们知道，快排在最坏情况下的时间复杂度是O(n2)，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是O(nlogn)，从这点上看起来很诱人，那为什么它还是没能得到“宠信”呢？
还记得我们上一节讲的归并排序的空间复杂度吗？归并排序并不是原地排序算法，空间复杂度是O(n)。所以，粗略点、夸张点讲，如果要排序100MB的数据，除了数据本身占用的内存之外，排序算法还要额外再占用100MB的内存空间，空间耗费就翻倍了。
前面我们讲到，快速排序比较适合来实现排序函数，但是，我们也知道，快速排序在最坏情况下的时间复杂度是O(n2)，如何来解决这个“复杂度恶化”的问题呢？
如何优化快速排序？我们先来看下，为什么最坏情况下快速排序的时间复杂度是O(n2)呢？我们前面讲过，如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为O(n2)。实际上，这种O(n2)时间复杂度出现的主要原因还是因为我们分区点选得不够合理。
那什么样的分区点是好的分区点呢？或者说如何来选择分区点呢？
最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。
如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现之前讲的那样，在某些情况下，排序的最坏情况时间复杂度是O(n2)。为了提高排序算法的性能，我们也要尽可能地让每次分区都比较平均。
我这里介绍两个比较常用、比较简单的分区算法，你可以直观地感受一下。
1.三数取中法我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这3个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。
2.随机法随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选得很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的O(n2)的情况，出现的可能性不大。
好了，我这里也只是抛砖引玉，如果想了解更多寻找分区点的方法，你可以自己课下深入去学习一下。
我们知道，快速排序是用递归来实现的。我们在递归那一节讲过，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。
举例分析排序函数为了让你对如何实现一个排序函数有一个更直观的感受，我拿Glibc中的qsort()函数举例说明一下。虽说qsort()从名字上看，很像是基于快速排序算法实现的，实际上它并不仅仅用了快排这一种算法。
如果你去看源码，你就会发现，qsort()会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是O(n)，所以对于小数据量的排序，比如1KB、2KB等，归并排序额外需要1KB、2KB的内存空间，这个问题不大。现在计算机的内存都挺大的，我们很多时候追求的是速度。还记得我们前面讲过的用空间换时间的技巧吗？这就是一个典型的应用。
但如果数据量太大，就跟我们前面提到的，排序100MB的数据，这个时候我们再用归并排序就不合适了。所以，要排序的数据量比较大的时候，qsort()会改为用快速排序算法来排序。
那qsort()是如何选择快速排序算法的分区点的呢？如果去看源码，你就会发现，qsort()选择分区点的方法就是“三数取中法”。是不是也并不复杂？
还有我们前面提到的递归太深会导致堆栈溢出的问题，qsort()是通过自己实现一个堆上的栈，手动模拟递归来解决的。我们之前在讲递归那一节也讲过，不知道你还有没有印象？
实际上，qsort()并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于4时，qsort()就退化为插入排序，不再继续用递归来做快速排序，因为我们前面也讲过，在小规模数据面前，O(n2)时间复杂度的算法并不一定比O(nlogn)的算法执行时间长。我们现在就来分析下这个说法。
我们在讲复杂度分析的时候讲过，算法的性能可以通过时间复杂度来分析，但是，这种复杂度分析是比较偏理论的，如果我们深究的话，实际上时间复杂度并不等于代码实际的运行时间。
时间复杂度代表的是一个增长趋势，如果画成增长曲线图，你会发现O(n2)比O(nlogn)要陡峭，也就是说增长趋势要更猛一些。但是，我们前面讲过，在大O复杂度表示法中，我们会省略低阶、系数和常数，也就是说，O(nlogn)在没有省略低阶、系数、常数之前可能是O(knlogn + c)，而且k和c有可能还是一个比较大的数。
假设k=1000，c=200，当我们对小规模数据（比如n=100）排序时，n2的值实际上比knlogn+c还要小。
knlogn+c = 1000 * 100 * log100 + 200 远大于10000 n^2 = 100*100 = 10000 所以，对于小规模数据的排序，O(n2)的排序算法并不一定比O(nlogn)排序算法执行的时间长。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法。
还记得我们之前讲到的哨兵来简化代码，提高执行效率吗？在qsort()插入排序的算法实现中，也利用了这种编程技巧。虽然哨兵可能只是少做一次判断，但是毕竟排序函数是非常常用、非常基础的函数，性能的优化要做到极致。
好了，C语言的qsort()我已经分析完了，你有没有觉得其实也不是很难？基本上都是用了我们前面讲到的知识点，有了前面的知识积累，看一些底层的类库的时候是不是也更容易了呢？
内容小结今天我带你分析了一下如何来实现一个工业级的通用的、高效的排序函数，内容比较偏实战，而且贯穿了一些前面几节的内容，你要多看几遍。我们大部分排序函数都是采用O(nlogn)排序算法来实现，但是为了尽可能地提高性能，会做很多优化。
我还着重讲了快速排序的一些优化策略，比如合理选择分区点、避免递归太深等等。最后，我还带你分析了一个C语言中qsort()的底层实现原理，希望你对此能有一个更加直观的感受。
课后思考在今天的内容中，我分析了C语言的中的qsort()的底层排序算法，你能否分析一下你所熟悉的语言中的排序函数都是用什么排序算法实现的呢？都有哪些优化技巧？
欢迎留言和我分享，我会第一时间给你反馈。
特别说明：
专栏已经更新一月有余，我在留言区看到很多同学说，希望给出课后思考题的标准答案。鉴于留言区里本身就有很多非常好的答案，之后我会将我认为比较好的答案置顶在留言区，供需要的同学参考。
如果文章发布一周后，留言里依旧没有比较好的答案，我会把我的答案写出来置顶在留言区。
最后，希望你把思考的过程看得比标准答案更重要。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>14｜汇编代码学习（一）：熟悉CPU架构和指令集</title><link>https://artisanbox.github.io/3/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/16/</guid><description>你好，我是宫文学。
经过了上一节课的学习，你已经对物理机的运行期机制有了一定的了解。其中最重要的知识点就是，为了让一个程序运行起来，硬件架构、操作系统和计算机语言分别起到了什么作用？对这些知识的深入理解，是让你进入高手行列的关键。
接下来，就让我们把程序编译成汇编代码，从而生成在物理机上运行的可执行程序吧！
慢着，不要太着急。为了让你打下更好的基础，我决定再拿出一节课来，带你了解一下CPU架构和指令集，特别是ARM和X86这两种使用最广泛的CPU架构，为你学习汇编语言打下良好的基础。
首先，我们讨论一下什么是CPU架构，以及它对学习汇编语言的作用。
掌握汇编语言的关键，是了解CPU架构提到汇编语言，很多同学都会觉得很高深、很难学。其实这是个误解，汇编语言并不难掌握。
为什么这么说呢？其实前面在实现虚拟机的时候，我们已经接触了栈机的字节码。你觉得它难吗？JVM的字节码理论上不会超过128条，而我们通过前面几节课已经了解了其中的好几十条指令，并且已经让他们顺利地运转起来了。
而且，汇编代码作为物理机的指令，也不可能有多么复杂。因为CPU的设计，就是要去快速地执行一条条简单的指令，所以这些指令不可能像高级语言那样充满复杂的语义。
我们可以把学习汇编语言跟学习高级语言做一下类比。如果你接触过多门计算机语言，很快就能找到这些计算机语言的相似性，比如都有基础数据类型、都支持加减乘除等各种运算、都支持各种表达式和语句等等。抓住这些基本规律以后，学习一门新语言的速度会很快。
汇编语言也是一样的。不同的CPU有不同的指令集，但它们的指令的格式有一些共性，比如都包含操作码的助记符和操作数，而操作数通常也都包含立即数、寄存器和内存地址这三个类别。它们也都包含数据处理、内存读写、跳转、子过程调用等几种大类的指令。所以，你也可以很快掌握这些规律或模式，做到触类旁通，游刃有余。
说了那么多，那么学习汇编语言的关键是什么呢？
由于汇编语言是跟具体的CPU打交道的，所以不同的CPU架构，它们的汇编语言就会有所差别。如果你能够深刻了解某款CPU的架构，自然也就会为它编写汇编代码了。
这里提到了一个词，架构。所谓架构，是指一个处理器的功能规范，定义了CPU在什么情况下会产生什么行为。你也可以把它理解成软件和硬件之间的一个桥梁，规定了硬件如何提供功能被软件所调用。所以，如果你要搞编译技术，就必须要了解目标CPU的架构。
我把CPU架构里的内容整合在这张表里，你可以保存起来：
看上去，要深入了解一个CPU架构，涉及的知识还蛮多的。不过，作为初学者，我们最重要的是关注两个方面就行了：指令集和寄存器集，因为它们跟汇编代码的关系最密切。
那么如何了解一个CPU的架构呢？正如我在第12节课说的那样，其实最重要的方式，就是阅读CPU的手册。
那么这节课我们就分析两种主流的CPU架构，看看能获得哪些知识点。首先我们就来看一下ARM架构的CPU的特点，它是目前大多数智能手机所采用的CPU。
了解ARM架构ARM处理器是ARM公司推出的一系列处理器的名称。ARMv8是它比较新的架构，当前大多数高端智能手机都是采用这个架构。了解这个架构的方法呢，当然是下载ARMv8的手册。
不过这本手册比较厚，有8000多页。为了加速你的理解，我挑其中最有用、跟编写汇编代码最相关的几个知识点跟你聊聊。
首先，ARMv8支持32位和64位运行模式，分别叫做AArch32和AArch64。在64位模式下，它的指令集叫做A64。
接着，我们看看ARMv8的寄存器。在AArch64架构下，它的寄存器有下面这几个（参见手册的B1.2.1部分）。
R0-R30：是31个通用寄存器。当它们被用于64位计算或32位计算的时候，分别被叫做X0-X30（X表示64位），以及W0-W30（W是Word的意思，表示32位）。这31个处理器是我们用做数据处理的主力。 SP寄存器：64位的栈指针寄存器，用于指向栈顶的地址。 PC寄存器：64位的程序代码寄存器，PC是Program Code的意思。这个寄存器记录了内存中当前指令的地址，CPU会从这个地址读取指令并执行。当程序执行跳转指令、进入异常或退出异常的时候，这个寄存器的值会被自动修改。 另外，还有一组寄存器是用于处理浮点数运算和矢量计算的，你可以去官方手册看看。 这些就是我们的汇编代码中会涉及到的寄存器，还有一些寄存器是系统级的寄存器，我们平常的应用代码用不上，就先不管了。
谈到寄存器，我插个话题。我注意到，你如果在网上搜索某个CPU架构的文章，往往得到的是模棱两可的、甚至是错误的信息，你千万要注意不要被它们误导了。
比如，我看到有的文章说在某个架构的CPU中，哪些寄存器是用来放返回值的，哪些寄存器是用来传参数的，而哪些寄存器又分别是由调用者或被调用者保护的，等等，还配了图做说明。
但如果你对调用约定或ABI的概念有所了解的话马上就会知道，这些其实都是软件层面上的一些约定，不是CPU架构层面上的规定。如果你还想了解得更具体一些，可以参考涉及到ARM架构的一些ABI规范文档，特别是其中的“Procedure Call Standard for the Arm® 64-bit”，这篇文档就规定了如何使用这些通用寄存器等信息。但作为语言的作者，你其实可以设计自己的ABI，你拥有更大的自由度。
与寄存器相关的一个概念，叫做Process State，或者PSTATE，它是CPU在执行指令时形成的一些状态信息，这些状态信息在物理层面是保存在一些特殊目的寄存器里。
PSTATE有什么用呢？比如，它的用途之一是辅助跳转指令的运行。当我们执行一个条件跳转指令之前，会先执行一个比较指令，这个比较指令就会设置某个状态信息，而后续的跳转指令就可以基于这个状态信息进行正确的跳转了。
此外，PSTATE还可以用于判断算术运算是否溢出、是否需要进位等等。
最后，我们终于谈到CPU架构中的主角，指令集了。你先看一下A64指令集中的一些常见的指令：
你看这些指令，是不是跟前面学过的Java字节码的指令集有很多相似之处？我们来比较一下看，上面的指令大概可以分为四组。
第一组指令，是加减乘除等算术运算的指令。
回忆一下，在我们之前学栈机的时候，是不是也有这些运算指令？但栈机和寄存器机的指令有一些差别。栈机的运算指令，是不需要带操作数的，因为操作数已经在操作数栈里。这几个指令会从栈顶取出两个操作数，做完加减乘除运算后，再放回栈顶。这样，下一条指令就可以把这个值作为操作数，继续进行计算。
但寄存器机上没有操作数栈，典型的寄存器机，所有运算都发生在寄存器里。我们来看看下面这个示例程序：
int foo(int a){ return a + 10; } 你可以用下面这个命令生成ARM64指令集的汇编代码：
clang -arch arm64 -S&amp;nbsp; foo.c -o foo_arm64.s -O2 汇编代码的主要部分是下面这几行：
_foo: ; @foo add w0, w0, #10 ; =10 ret 其中“add w0, w0, #10”的意思，是把w0寄存器的值加上10，结果仍然放到w0。</description></item><item><title>15_二分查找（上）：如何用最省内存的方式实现快速查找功能？</title><link>https://artisanbox.github.io/2/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/16/</guid><description>今天我们讲一种针对有序数据集合的查找算法：二分查找（Binary Search）算法，也叫折半查找算法。二分查找的思想非常简单，很多非计算机专业的同学很容易就能理解，但是看似越简单的东西往往越难掌握好，想要灵活应用就更加困难。
老规矩，我们还是来看一道思考题。
假设我们有1000万个整数数据，每个数据占8个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这1000万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过100MB，你会怎么做呢？带着这个问题，让我们进入今天的内容吧！
无处不在的二分思想二分查找是一种非常简单易懂的快速查找算法，生活中到处可见。比如说，我们现在来做一个猜字游戏。我随机写一个0到99之间的数字，然后你来猜我写的是什么。猜的过程中，你每猜一次，我就会告诉你猜的大了还是小了，直到猜中为止。你来想想，如何快速猜中我写的数字呢？
假设我写的数字是23，你可以按照下面的步骤来试一试。（如果猜测范围的数字有偶数个，中间数有两个，就选择较小的那个。）
7次就猜出来了，是不是很快？这个例子用的就是二分思想，按照这个思想，即便我让你猜的是0到999的数字，最多也只要10次就能猜中。不信的话，你可以试一试。
这是一个生活中的例子，我们现在回到实际的开发场景中。假设有1000条订单数据，已经按照订单金额从小到大排序，每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于19元的订单。如果存在，则返回订单数据，如果不存在则返回null。
最简单的办法当然是从第一个订单开始，一个一个遍历这1000个订单，直到找到金额等于19元的订单为止。但这样查找会比较慢，最坏情况下，可能要遍历完这1000条记录才能找到。那用二分查找能不能更快速地解决呢？
为了方便讲解，我们假设只有10个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98。
还是利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。为了更加直观，我画了一张查找过程的图。其中，low和high表示待查找区间的下标，mid表示待查找区间的中间元素下标。
看懂这两个例子，你现在对二分的思想应该掌握得妥妥的了。我这里稍微总结升华一下，二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0。
O(logn)惊人的查找速度二分查找是一种非常高效的查找算法，高效到什么程度呢？我们来分析一下它的时间复杂度。
我们假设数据大小是n，每次查找后数据都会缩小为原来的一半，也就是会除以2。最坏情况下，直到查找区间被缩小为空，才停止。
可以看出来，这是一个等比数列。其中n/2k=1时，k的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了k次区间缩小操作，时间复杂度就是O(k)。通过n/2k=1，我们可以求得k=log2n，所以时间复杂度就是O(logn)。
二分查找是我们目前为止遇到的第一个时间复杂度为O(logn)的算法。后面章节我们还会讲堆、二叉树的操作等等，它们的时间复杂度也是O(logn)。我这里就再深入地讲讲O(logn)这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级O(1)的算法还要高效。为什么这么说呢？
因为logn是一个非常“恐怖”的数量级，即便n非常非常大，对应的logn也很小。比如n等于2的32次方，这个数很大了吧？大约是42亿。也就是说，如果我们在42亿个数据中用二分查找一个数据，最多需要比较32次。
我们前面讲过，用大O标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1)有可能表示的是一个非常大的常量值，比如O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有O(logn)的算法执行效率高。
反过来，对数对应的就是指数。有一个非常著名的“阿基米德与国王下棋的故事”，你可以自行搜索一下，感受一下指数的“恐怖”。这也是为什么我们说，指数时间复杂度的算法在大规模数据面前是无效的。
二分查找的递归与非递归实现实际上，简单的二分查找并不难写，注意我这里的“简单”二字。下一节，我们会讲到二分查找的变体问题，那才是真正烧脑的。今天，我们来看如何来写最简单的二分查找。
最简单的情况就是有序数组中不存在重复元素，我们在其中用二分查找值等于给定值的数据。我用Java代码实现了一个最简单的二分查找算法。
public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &amp;lt;= high) { int mid = (low + high) / 2; if (a[mid] == value) { return mid; } else if (a[mid] &amp;lt; value) { low = mid + 1; } else { high = mid - 1; } }</description></item><item><title>15_浮点数和定点数（上）：怎么用有限的Bit表示尽可能多的信息？</title><link>https://artisanbox.github.io/4/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/15/</guid><description>在我们日常的程序开发中，不只会用到整数。更多情况下，我们用到的都是实数。比如，我们开发一个电商App，商品的价格常常会是9块9；再比如，现在流行的深度学习算法，对应的机器学习里的模型里的各个权重也都是1.23这样的数。可以说，在实际的应用过程中，这些有零有整的实数，是和整数同样常用的数据类型，我们也需要考虑到。
浮点数的不精确性那么，我们能不能用二进制表示所有的实数，然后在二进制下计算它的加减乘除呢？先不着急，我们从一个有意思的小案例来看。
你可以在Linux下打开Python的命令行Console，也可以在Chrome浏览器里面通过开发者工具，打开浏览器里的Console，在里面输入“0.3 + 0.6”，然后看看你会得到一个什么样的结果。
&amp;gt;&amp;gt;&amp;gt; 0.3 + 0.6 0.8999999999999999 不知道你有没有大吃一惊，这么简单的一个加法，无论是在Python还是在JavaScript里面，算出来的结果居然不是准确的0.9，而是0.8999999999999999这么个结果。这是为什么呢？
在回答为什么之前，我们先来想一个更抽象的问题。通过前面的这么多讲，你应该知道我们现在用的计算机通常用16/32个比特（bit）来表示一个数。那我问你，我们用32个比特，能够表示所有实数吗？
答案很显然是不能。32个比特，只能表示2的32次方个不同的数，差不多是40亿个。如果表示的数要超过这个数，就会有两个不同的数的二进制表示是一样的。那计算机可就会一筹莫展，不知道这个数到底是多少。
40亿个数看似已经很多了，但是比起无限多的实数集合却只是沧海一粟。所以，这个时候，计算机的设计者们，就要面临一个问题了：我到底应该让这40亿个数映射到实数集合上的哪些数，在实际应用中才能最划得来呢？
定点数的表示有一个很直观的想法，就是我们用4个比特来表示0～9的整数，那么32个比特就可以表示8个这样的整数。然后我们把最右边的2个0～9的整数，当成小数部分；把左边6个0～9的整数，当成整数部分。这样，我们就可以用32个比特，来表示从0到999999.99这样1亿个实数了。
这种用二进制来表示十进制的编码方式，叫作BCD编码（Binary-Coded Decimal）。其实它的运用非常广泛，最常用的是在超市、银行这样需要用小数记录金额的情况里。在超市里面，我们的小数最多也就到分。这样的表示方式，比较直观清楚，也满足了小数部分的计算。
不过，这样的表示方式也有几个缺点。
第一，这样的表示方式有点“浪费”。本来32个比特我们可以表示40亿个不同的数，但是在BCD编码下，只能表示1亿个数，如果我们要精确到分的话，那么能够表示的最大金额也就是到100万。如果我们的货币单位是人民币或者美元还好，如果我们的货币单位变成了津巴布韦币，这个数量就不太够用了。
第二，这样的表示方式没办法同时表示很大的数字和很小的数字。我们在写程序的时候，实数的用途可能是多种多样的。有时候我们想要表示商品的金额，关心的是9.99这样小的数字；有时候，我们又要进行物理学的运算，需要表示光速，也就是$3×10^8$这样很大的数字。那么，我们有没有一个办法，既能够表示很小的数，又能表示很大的数呢？
浮点数的表示答案当然是有的，就是你可能经常听说过的浮点数（Floating Point），也就是float类型。
我们先来想一想。如果我们想在一张便签纸上，用一行来写一个十进制数，能够写下多大范围的数？因为我们要让人能够看清楚，所以字最小也有一个限制。你会发现一个和上面我们用BCD编码表示数一样的问题，就是纸张的宽度限制了我们能够表示的数的大小。如果宽度只放得下8个数字，那么我们还是只能写下最大到99999999这样的数字。
有限宽度的便签，只能写下有限大小的数字其实，这里的纸张宽度，就和我们32个比特一样，是在空间层面的限制。那么，在现实生活中，我们是怎么表示一个很大的数的呢？比如说，我们想要在一本科普书里，写一下宇宙内原子的数量，莫非是用一页纸，用好多行写下很多个0么？
当然不是了，我们会用科学计数法来表示这个数字。宇宙内的原子的数量，大概在 10的82次方左右，我们就用$1.0×10^82$这样的形式来表示这个数值，不需要写下82个0。
在计算机里，我们也可以用一样的办法，用科学计数法来表示实数。浮点数的科学计数法的表示，有一个IEEE的标准，它定义了两个基本的格式。一个是用32比特表示单精度的浮点数，也就是我们常常说的float或者float32类型。另外一个是用64比特表示双精度的浮点数，也就是我们平时说的double或者float64类型。
双精度类型和单精度类型差不多，这里，我们来看单精度类型，双精度你自然也就明白了。
单精度的32个比特可以分成三部分。
第一部分是一个符号位，用来表示是正数还是负数。我们一般用s来表示。在浮点数里，我们不像正数分符号数还是无符号数，所有的浮点数都是有符号的。
接下来是一个8个比特组成的指数位。我们一般用e来表示。8个比特能够表示的整数空间，就是0～255。我们在这里用1～254映射到-126～127这254个有正有负的数上。因为我们的浮点数，不仅仅想要表示很大的数，还希望能够表示很小的数，所以指数位也会有负数。
你发现没，我们没有用到0和255。没错，这里的 0（也就是8个比特全部为0） 和 255 （也就是8个比特全部为1）另有它用，我们等一下再讲。
最后，是一个23个比特组成的有效数位。我们用f来表示。综合科学计数法，我们的浮点数就可以表示成下面这样：
$(-1)^s×1.f×2^e$
你会发现，这里的浮点数，没有办法表示0。的确，要表示0和一些特殊的数，我们就要用上在e里面留下的0和255这两个表示，这两个表示其实是两个标记位。在e为0且f为0的时候，我们就把这个浮点数认为是0。至于其它的e是0或者255的特殊情况，你可以看下面这个表格，分别可以表示出无穷大、无穷小、NAN以及一个特殊的不规范数。
我们可以以0.5为例子。0.5的符号为s应该是0，f应该是0，而e应该是-1，也就是
$0.5= (-1)^0×1.0×2^{-1}=0.5$，对应的浮点数表示，就是32个比特。
$s=0，e = 2^{-1}$，需要注意，e表示从-126到127个，-1是其中的第126个数，这里的e如果用整数表示，就是$2^6+2^5+2^4+2^3+2^2+2^1=126$，$1.f=1.0$。
在这样的浮点数表示下，不考虑符号的话，浮点数能够表示的最小的数和最大的数，差不多是$1.17×10^{-38}$和$3.40×10^{38}$。比前面的BCD编码能够表示的范围大多了。
总结延伸你会看到，在这样的表示方式下，浮点数能够表示的数据范围一下子大了很多。正是因为这个数对应的小数点的位置是“浮动”的，它才被称为浮点数。随着指数位e的值的不同，小数点的位置也在变动。对应的，前面的BCD编码的实数，就是小数点固定在某一位的方式，我们也就把它称为定点数。
回到我们最开头，为什么我们用0.3 + 0.6不能得到0.9呢？这是因为，浮点数没有办法精确表示0.3、0.6和0.9。事实上，我们拿出0.1～0.9这9个数，其中只有0.5能够被精确地表示成二进制的浮点数，也就是s = 0、e = -1、f = 0这样的情况。
而0.3、0.6乃至我们希望的0.9，都只是一个近似的表达。这个也为我们带来了一个挑战，就是浮点数无论是表示还是计算其实都是近似计算。那么，在使用过程中，我们该怎么来使用浮点数，以及使用浮点数会遇到些什么问题呢？下一讲，我会用更多的实际代码案例，来带你看看浮点数计算中的各种“坑”。
推荐阅读如果对浮点数的表示还不是很清楚，你可以仔细阅读一下《计算机组成与设计：硬件/软件接口》的3.5.1节。
课后思考对于BCD编码的定点数，如果我们用7个比特来表示连续两位十进制数，也就是00～99，是不是可以让32比特表示更大一点的数据范围？如果我们还需要表示负数，那么一个32比特的BCD编码，可以表示的数据范围是多大？
欢迎你在留言区写下你的思考和疑问，和大家一起探讨。你也可以把今天的文章分享给你朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>15_答疑文章（一）：日志和索引相关问题</title><link>https://artisanbox.github.io/1/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/15/</guid><description>在今天这篇答疑文章更新前，MySQL实战这个专栏已经更新了14篇。在这些文章中，大家在评论区留下了很多高质量的留言。现在，每篇文章的评论区都有热心的同学帮忙总结文章知识点，也有不少同学提出了很多高质量的问题，更有一些同学帮忙解答其他同学提出的问题。
在浏览这些留言并回复的过程中，我倍受鼓舞，也尽我所知地帮助你解决问题、和你讨论。可以说，你们的留言活跃了整个专栏的氛围、提升了整个专栏的质量，谢谢你们。
评论区的大多数留言我都直接回复了，对于需要展开说明的问题，我都拿出小本子记了下来。这些被记下来的问题，就是我们今天这篇答疑文章的素材了。
到目前为止，我已经收集了47个问题，很难通过今天这一篇文章全部展开。所以，我就先从中找了几个联系非常紧密的问题，串了起来，希望可以帮你解决关于日志和索引的一些疑惑。而其他问题，我们就留着后面慢慢展开吧。
日志相关问题我在第2篇文章《日志系统：一条SQL更新语句是如何执行的？》中，和你讲到binlog（归档日志）和redo log（重做日志）配合崩溃恢复的时候，用的是反证法，说明了如果没有两阶段提交，会导致MySQL出现主备数据不一致等问题。
在这篇文章下面，很多同学在问，在两阶段提交的不同瞬间，MySQL如果发生异常重启，是怎么保证数据完整性的？
现在，我们就从这个问题开始吧。
我再放一次两阶段提交的图，方便你学习下面的内容。
图1 两阶段提交示意图这里，我要先和你解释一个误会式的问题。有同学在评论区问到，这个图不是一个update语句的执行流程吗，怎么还会调用commit语句？
他产生这个疑问的原因，是把两个“commit”的概念混淆了：
他说的“commit语句”，是指MySQL语法中，用于提交一个事务的命令。一般跟begin/start transaction 配对使用。 而我们图中用到的这个“commit步骤”，指的是事务提交过程中的一个小步骤，也是最后一步。当这个步骤执行完成后，这个事务就提交完成了。 “commit语句”执行的时候，会包含“commit 步骤”。 而我们这个例子里面，没有显式地开启事务，因此这个update语句自己就是一个事务，在执行完成后提交事务时，就会用到这个“commit步骤“。
接下来，我们就一起分析一下在两阶段提交的不同时刻，MySQL异常重启会出现什么现象。
如果在图中时刻A的地方，也就是写入redo log 处于prepare阶段之后、写binlog之前，发生了崩溃（crash），由于此时binlog还没写，redo log也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog还没写，所以也不会传到备库。到这里，大家都可以理解。
大家出现问题的地方，主要集中在时刻B，也就是binlog写完，redo log还没commit前发生crash，那崩溃恢复的时候MySQL会怎么处理？
我们先来看一下崩溃恢复时的判断规则。
如果redo log里面的事务是完整的，也就是已经有了commit标识，则直接提交；
如果redo log里面的事务只有完整的prepare，则判断对应的事务binlog是否存在并完整：
a. 如果是，则提交事务；
b. 否则，回滚事务。
这里，时刻B发生crash对应的就是2(a)的情况，崩溃恢复过程中事务会被提交。
现在，我们继续延展一下这个问题。
追问1：MySQL怎么知道binlog是完整的?回答：一个事务的binlog是有完整格式的：
statement格式的binlog，最后会有COMMIT； row格式的binlog，最后会有一个XID event。 另外，在MySQL 5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQL可以通过校验checksum的结果来发现。所以，MySQL还是有办法验证事务binlog的完整性的。
追问2：redo log 和 binlog是怎么关联起来的?回答：它们有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log：
如果碰到既有prepare、又有commit的redo log，就直接提交； 如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。 追问3：处于prepare阶段的redo log加上完整binlog，重启就能恢复，MySQL为什么要这么设计?回答：其实，这个问题还是跟我们在反证法中说到的数据与备份的一致性有关。在时刻B，也就是binlog写完以后MySQL发生崩溃，这时候binlog已经写入了，之后就会被从库（或者用这个binlog恢复出来的库）使用。
所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。
追问4：如果这样的话，为什么还要两阶段提交呢？干脆先redo log写完，再写binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？回答：其实，两阶段提交是经典的分布式系统问题，并不是MySQL独有的。
如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。
对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回滚不了，数据和binlog日志又不一致了。</description></item><item><title>15｜汇编语言学习（二）：熟悉X86汇编代码</title><link>https://artisanbox.github.io/3/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/17/</guid><description>你好，我是宫文学。
上一节课，在开始写汇编代码之前，我先带着你在CPU架构方面做了一些基础的铺垫工作。我希望能让你有个正确的认知：其实汇编语言的语法等层面的知识是很容易掌握的。但要真正学懂汇编语言，关键还是要深入了解CPU架构。
今天这一节课，我们会再进一步，特别针对X86汇编代码来近距离分析一下。我会带你吃透一个汇编程序的例子，在这个过程中，你会获得关于汇编程序构成、指令构成、内存访问方式、栈桢维护，以及汇编代码优化等方面的知识点。掌握这些知识点之后，我们后面生成汇编代码的工作就会顺畅很多了！
好了，我们开始第一步，通过实际的示例程序，看看X86的汇编代码是什么样子的。
学习编译器生成的汇编代码按我个人的经验来说，学习汇编最快的方法，就是让别的编译器生成汇编代码给我们看。
比如，你可以用C语言写出表达式计算、函数调用、条件分支等不同的逻辑，然后让C语言的编译器编译一下，就知道这些逻辑对应的汇编代码是什么样子了，而且你还可以分析每条代码的作用。这样看多了、分析多了以后，你自然就会对汇编语言越来越熟悉，也敢自己上手写了。
我们还是采用上一节课那个用C语言写的示例函数foo，我们让这个函数接受一个整型的参数，把它加上10以后返回：
int foo(int a){ return a+10; } 接着，再输入下面的clang或gcc命令：
clang -S foo.c -o foo.s 或 gcc -S foo.c -o foo.s 然后我们用一个文本编辑器打开foo.s，你就会看到下面这些汇编代码：
.section __TEXT,__text,regular,pure_instructions .build_version macos, 11, 0 sdk_version 11, 3 .globl _foo ## -- Begin function foo .p2align 4, 0x90 _foo: ## @foo .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp movl %edi, -4(%rbp) movl -4(%rbp), %eax addl $10, %eax popq %rbp retq .</description></item><item><title>16_“orderby”是怎么工作的？</title><link>https://artisanbox.github.io/1/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/16/</guid><description>在你开发应用的时候，一定会经常碰到需要根据指定的字段排序来显示结果的需求。还是以我们前面举例用过的市民表为例，假设你要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前1000个人的姓名、年龄。
假设这个表的部分定义是这样的：
CREATE TABLE `t` ( `id` int(11) NOT NULL, `city` varchar(16) NOT NULL, `name` varchar(16) NOT NULL, `age` int(11) NOT NULL, `addr` varchar(128) DEFAULT NULL, PRIMARY KEY (`id`), KEY `city` (`city`) ) ENGINE=InnoDB; 这时，你的SQL语句可以这么写：
select city,name,age from t where city='杭州' order by name limit 1000 ; 这个语句看上去逻辑很清晰，但是你了解它的执行流程吗？今天，我就和你聊聊这个语句是怎么执行的，以及有什么参数会影响执行的行为。
全字段排序前面我们介绍过索引，所以你现在就很清楚了，为避免全表扫描，我们需要在city字段加上索引。
在city字段上创建索引之后，我们用explain命令来看看这个语句的执行情况。
图1 使用explain命令查看语句的执行情况Extra这个字段中的“Using filesort”表示的就是需要排序，MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。
为了说明这个SQL查询语句的执行过程，我们先来看一下city这个索引的示意图。
图2 city字段的索引示意图从图中可以看到，满足city='杭州’条件的行，是从ID_X到ID_(X+N)的这些记录。
通常情况下，这个语句执行流程如下所示 ：
初始化sort_buffer，确定放入name、city、age这三个字段；
从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X；
到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中；
从索引city取下一个记录的主键id；</description></item><item><title>16_二分查找（下）：如何快速定位IP对应的省份地址？</title><link>https://artisanbox.github.io/2/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/17/</guid><description>通过IP地址来查找IP归属地的功能，不知道你有没有用过？没用过也没关系，你现在可以打开百度，在搜索框里随便输一个IP地址，就会看到它的归属地。
这个功能并不复杂，它是通过维护一个很大的IP地址库来实现的。地址库中包括IP地址范围和归属地的对应关系。
当我们想要查询202.102.133.13这个IP地址的归属地时，我们就在地址库中搜索，发现这个IP地址落在[202.102.133.0, 202.102.133.255]这个地址范围内，那我们就可以将这个IP地址范围对应的归属地“山东东营市”显示给用户了。
[202.102.133.0, 202.102.133.255] 山东东营市 [202.102.135.0, 202.102.136.255] 山东烟台 [202.102.156.34, 202.102.157.255] 山东青岛 [202.102.48.0, 202.102.48.255] 江苏宿迁 [202.102.49.15, 202.102.51.251] 江苏泰州 [202.102.56.0, 202.102.56.255] 江苏连云港 现在我的问题是，在庞大的地址库中逐一比对IP地址所在的区间，是非常耗时的。假设我们有12万条这样的IP区间与归属地的对应关系，如何快速定位出一个IP地址的归属地呢？
是不是觉得比较难？不要紧，等学完今天的内容，你就会发现这个问题其实很简单。
上一节我讲了二分查找的原理，并且介绍了最简单的一种二分查找的代码实现。今天我们来讲几种二分查找的变形问题。
不知道你有没有听过这样一个说法：“十个二分九个错”。二分查找虽然原理极其简单，但是想要写出没有Bug的二分查找并不容易。
唐纳德·克努特（Donald E.Knuth）在《计算机程序设计艺术》的第3卷《排序和查找》中说到：“尽管第一个二分查找算法于1946年出现，然而第一个完全正确的二分查找算法实现直到1962年才出现。”
你可能会说，我们上一节学的二分查找的代码实现并不难写啊。那是因为上一节讲的只是二分查找中最简单的一种情况，在不存在重复元素的有序数组中，查找值等于给定值的元素。最简单的二分查找写起来确实不难，但是，二分查找的变形问题就没那么好写了。
二分查找的变形问题很多，我只选择几个典型的来讲解，其他的你可以借助我今天讲的思路自己来分析。
需要特别说明一点，为了简化讲解，今天的内容，我都以数据是从小到大排列为前提，如果你要处理的数据是从大到小排列的，解决思路也是一样的。同时，我希望你最好先自己动手试着写一下这4个变形问题，然后再看我的讲述，这样你就会对我说的“二分查找比较难写”有更加深的体会了。
变体一：查找第一个值等于给定值的元素上一节中的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。如果我们将这个问题稍微修改下，有序数据集合中存在重复的数据，我们希望找到第一个值等于给定值的数据，这样之前的二分查找代码还能继续工作吗？
比如下面这样一个有序数组，其中，a[5]，a[6]，a[7]的值都等于8，是重复的数据。我们希望查找第一个等于8的数据，也就是下标是5的元素。
如果我们用上一节课讲的二分查找的代码实现，首先拿8与区间的中间值a[4]比较，8比6大，于是在下标5到9之间继续查找。下标5和9的中间位置是下标7，a[7]正好等于8，所以代码就返回了。
尽管a[7]也等于8，但它并不是我们想要找的第一个等于8的元素，因为第一个值等于8的元素是数组下标为5的元素。我们上一节讲的二分查找代码就无法处理这种情况了。所以，针对这个变形问题，我们可以稍微改造一下上一节的代码。
100个人写二分查找就会有100种写法。网上有很多关于变形二分查找的实现方法，有很多写得非常简洁，比如下面这个写法。但是，尽管简洁，理解起来却非常烧脑，也很容易写错。
public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &amp;lt;= high) { int mid = low + ((high - low) &amp;gt;&amp;gt; 1); if (a[mid] &amp;gt;= value) { high = mid - 1; } else { low = mid + 1; } } if (low &amp;lt; n &amp;amp;&amp;amp; a[low]==value) return low; else return -1; } 看完这个实现之后，你是不是觉得很不好理解？如果你只是死记硬背这个写法，我敢保证，过不了几天，你就会全都忘光，再让你写，90%的可能会写错。所以，我换了一种实现方法，你看看是不是更容易理解呢？</description></item><item><title>16_浮点数和定点数（下）：深入理解浮点数到底有什么用？</title><link>https://artisanbox.github.io/4/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/16/</guid><description>上一讲，我们讲了用“浮点数”这样的数据形式，来表示一个不能确定大小的数据范围。浮点数可以大到$3.40×10^{38}$，也可以小到$1.17×10^{-38}$这样的数值。同时，我们也发现，其实我们平时写的0.1、0.2并不是精确的数值，只是一个近似值。只有0.5这样，可以表示成$2^{-1}$这种形式的，才是一个精确的浮点数。
你是不是感到很疑惑，浮点数的近似值究竟是怎么算出来的？浮点数的加法计算又是怎么回事儿？在实践应用中，我们怎么才用好浮点数呢？这一节，我们就一起来看这几个问题。
浮点数的二进制转化我们首先来看，十进制的浮点数怎么表示成二进制。
我们输入一个任意的十进制浮点数，背后都会对应一个二进制表示。比方说，我们输入了一个十进制浮点数9.1。那么按照之前的讲解，在二进制里面，我们应该把它变成一个“符号位s+指数位e+有效位数f”的组合。第一步，我们要做的，就是把这个数变成二进制。
首先，我们把这个数的整数部分，变成一个二进制。这个我们前面讲二进制的时候已经讲过了。这里的9，换算之后就是1001。
接着，我们把对应的小数部分也换算成二进制。小数怎么换成二进制呢？我们先来定义一下，小数的二进制表示是怎么回事。我们拿0.1001这样一个二进制小数来举例说明。和上面的整数相反，我们把小数点后的每一位，都表示对应的2的-N次方。那么0.1001，转化成十进制就是：
$1×2^{-1}+0×2^{-2}+0×2^{-3}+$
$1×2^{-4}=0.5625$
和整数的二进制表示采用“除以2，然后看余数”的方式相比，小数部分转换成二进制是用一个相似的反方向操作，就是乘以2，然后看看是否超过1。如果超过1，我们就记下1，并把结果减去1，进一步循环操作。在这里，我们就会看到，0.1其实变成了一个无限循环的二进制小数，0.000110011。这里的“0011”会无限循环下去。
然后，我们把整数部分和小数部分拼接在一起，9.1这个十进制数就变成了1001.000110011…这样一个二进制表示。
上一讲我们讲过，浮点数其实是用二进制的科学计数法来表示的，所以我们可以把小数点左移三位，这个数就变成了：
$1.0010$$0011$$0011… × 2^3$
那这个二进制的科学计数法表示，我们就可以对应到了浮点数的格式里了。这里的符号位s = 0，对应的有效位f=001000110011…。因为f最长只有23位，那这里“0011”无限循环，最多到23位就截止了。于是，f=00100011001100110011 001。最后的一个“0011”循环中的最后一个“1”会被截断掉。对应的指数为e，代表的应该是3。因为指数位有正又有负，所以指数位在127之前代表负数，之后代表正数，那3其实对应的是加上127的偏移量130，转化成二进制，就是130，对应的就是指数位的二进制，表示出来就是10000010。
然后，我们把“s+e+f”拼在一起，就可以得到浮点数9.1的二进制表示了。最终得到的二进制表示就变成了：
010000010 0010 0011001100110011 001
如果我们再把这个浮点数表示换算成十进制， 实际准确的值是9.09999942779541015625。相信你现在应该不会感觉奇怪了。
我在这里放一个链接，这里提供了直接交互式地设置符号位、指数位和有效位数的操作。你可以直观地看到，32位浮点数每一个bit的变化，对应的有效位数、指数会变成什么样子以及最后的十进制的计算结果是怎样的。
这个也解释了为什么，在上一讲一开始，0.3+0.6=0.899999。因为0.3转化成浮点数之后，和这里的9.1一样，并不是精确的0.3了，0.6和0.9也是一样的，最后的计算会出现精度问题。
浮点数的加法和精度损失搞清楚了怎么把一个十进制的数值，转化成IEEE-754标准下的浮点数表示，我们现在来看一看浮点数的加法是怎么进行的。其实原理也很简单，你记住六个字就行了，那就是先对齐、再计算。
两个浮点数的指数位可能是不一样的，所以我们要把两个的指数位，变成一样的，然后只去计算有效位的加法就好了。
比如0.5，表示成浮点数，对应的指数位是-1，有效位是00…（后面全是0，记住f前默认有一个1）。0.125表示成浮点数，对应的指数位是-3，有效位也还是00…（后面全是0，记住f前默认有一个1）。
那我们在计算0.5+0.125的浮点数运算的时候，首先要把两个的指数位对齐，也就是把指数位都统一成两个其中较大的-1。对应的有效位1.00…也要对应右移两位，因为f前面有一个默认的1，所以就会变成0.01。然后我们计算两者相加的有效位1.f，就变成了有效位1.01，而指数位是-1，这样就得到了我们想要的加法后的结果。
实现这样一个加法，也只需要位移。和整数加法类似的半加器和全加器的方法就能够实现，在电路层面，也并没有引入太多新的复杂性。
同样的，你可以用刚才那个链接来试试看，我们这个加法计算的浮点数的结果是不是正确。
回到浮点数的加法过程，你会发现，其中指数位较小的数，需要在有效位进行右移，在右移的过程中，最右侧的有效位就被丢弃掉了。这会导致对应的指数位较小的数，在加法发生之前，就丢失精度。两个相加数的指数位差的越大，位移的位数越大，可能丢失的精度也就越大。当然，也有可能你的运气非常好，右移丢失的有效位都是0。这种情况下，对应的加法虽然丢失了需要加的数字的精度，但是因为对应的值都是0，实际的加法的数值结果不会有精度损失。
32位浮点数的有效位长度一共只有23位，如果两个数的指数位差出23位，较小的数右移24位之后，所有的有效位就都丢失了。这也就意味着，虽然浮点数可以表示上到$3.40×10^{38}$，下到$1.17×10^{-38}$这样的数值范围。但是在实际计算的时候，只要两个数，差出$2^{24}$，也就是差不多1600万倍，那这两个数相加之后，结果完全不会变化。
你可以试一下，我下面用一个简单的Java程序，让一个值为2000万的32位浮点数和1相加，你会发现，+1这个过程因为精度损失，被“完全抛弃”了。
public class FloatPrecision { public static void main(String[] args) { float a = 20000000.0f; float b = 1.0f; float c = a + b; System.out.println(&amp;quot;c is &amp;quot; + c); float d = c - a; System.</description></item><item><title>16｜生成本地代码第1关：先把基础搭好</title><link>https://artisanbox.github.io/3/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/18/</guid><description>你好，我是宫文学。
到目前为止，我们已经初步了解了CPU架构和X86汇编代码的相关知识点，为我们接下来的让编译器生成汇编代码的工作打下了不错的基础。
不过，我总是相信最好的学习方法就是实践。因为，只有你自己动手尝试过用编译器生成汇编代码，你才会对CPU架构和汇编的知识有更深刻的了解，总之，遇到问题，解决问题就好了。
所以呢，今天这节课，我们就开始着手生成X86汇编代码。我会带你分析生成汇编代码的算法思路，理解寄存器机与栈机在生成代码上的差别，以及了解如何在内存里表示汇编代码。
看上去工作有点多，不着急，我们一步步来。那首先，我们就通过一个实例，让你对生成汇编代码的算法思路有一个直觉上的认知。
生成汇编代码的算法思路在前面的课里，我们已经学会了如何生成字节码。你也知道了，基本上，我们只需要通过遍历AST就能生成栈机的字节码。
但当时我们也说过，为栈机生成代码是比较简单的，比寄存器机要简单，这具体是为什么呢？
你可以保留这个疑问，先来跟我分析一个例子，看看我们要怎么把它转化成汇编代码，以及在这个过程中会遇到什么问题：
function foo(a:number, b:number):number{ let c = a+b+10; return a+c; } 你可以看到，这个函数有两个参数。为了提高程序的运行效率，在参数数量不多的情况下，参数通常都是通过寄存器传递的，我们暂且把传递这两个参数的两个寄存器叫做r1和r2。
接下来，我们要执行运算，也就是a+b+10。这该怎么做呢？这里你要注意的是，在生成指令的时候，我们通常不能直接把b的值加到a上，也就是从r2加到r1上。因为这样就破坏了r1原来的值，而这个值后面的代码有可能用到。
所以呢，我们这里就要生成两条指令。第一条指令，是把a的值从r1拷贝到一个新的寄存器r3；第2条指令，是把b的值从r2加到r3上，最终结果也就保存到了r3。
mov r1, r3 add r2, r3 之后，我们要再加10。这个时候，我们可以放心地把10加到r3上。因为r3是我们自己生成的一个临时变量，我们可以确保其他代码不会用到它，所以我们可以放心地改变它的值。
add 10, r3 接着，我们要把表达式a+b+10的值赋给本地变量c。对于本地变量，我们也是尽可能地把它放到寄存器里。不过呢，由于r3作为临时变量的任务已经圆满完成了，所以这个时候，我们可以用r3来表示c的值，这下我们就节省了一个寄存器。因此，这里我们不需要添加任何新的指令。
再接着，我们要计算a+c的值。为了不影响已有寄存器的值，我们又使用了一个新的寄存器r4，用来保存计算结果：
mov r1, r4 add r3, r4 最后，我们要把计算结果返回。通常，根据调用约定，返回值也是放在某个寄存器里的。我们这里假设这个寄存器是r0。所以，我们可以用这两条指令返回：
mov r4, r0 ret 到此为止，我们就已经成功地为foo函数生成了汇编代码。当然，这个汇编代码只是示意性的、逻辑性的，如果要让它成为真正可用的汇编代码，还要做一些调整，比如把寄存器的名称换成正式的物理寄存器的名称，如rax等。我这样叙述，是为了尽量保持简洁，避免你过早陷入到具体CPU架构的细节中去，增加认知负担。
好了，回顾我们的工作成果，你可能很快会发现一个问题：这么小的一个程序就占据了4个寄存器，如果我再多加点参数或者本地变量，那寄存器岂不会很快就会被用光？
这个担心是很有道理的，你可以先看看下面的例子：
function foo(a:number, b:number, d:number, f:number):number{ let c = a+b+10; let g = ... let h = ... return g+h; } 在这个例子中，参数a、b、d、f和本地变量c、g、h，都会额外占用一个寄存器，并且每个变量的计算过程都有可能消耗额外的寄存器来保存临时变量，所以寄存器很快就会被用光。
到这里，你可能已经体会到了为什么给寄存器机生成代码会更难了。在栈机里，根本没有这样的问题，因为我们可以用操作数栈来保存中间结果，而操作数栈的大小是没有限制的。</description></item><item><title>17_如何正确地显示随机消息？</title><link>https://artisanbox.github.io/1/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/17/</guid><description>我在上一篇文章，为你讲解完order by语句的几种执行模式后，就想到了之前一个做英语学习App的朋友碰到过的一个性能问题。今天这篇文章，我就从这个性能问题说起，和你说说MySQL中的另外一种排序需求，希望能够加深你对MySQL排序逻辑的理解。
这个英语学习App首页有一个随机显示单词的功能，也就是根据每个用户的级别有一个单词表，然后这个用户每次访问首页的时候，都会随机滚动显示三个单词。他们发现随着单词表变大，选单词这个逻辑变得越来越慢，甚至影响到了首页的打开速度。
现在，如果让你来设计这个SQL语句，你会怎么写呢？
为了便于理解，我对这个例子进行了简化：去掉每个级别的用户都有一个对应的单词表这个逻辑，直接就是从一个单词表中随机选出三个单词。这个表的建表语句和初始数据的命令如下：
mysql&amp;gt; CREATE TABLE `words` ( `id` int(11) NOT NULL AUTO_INCREMENT, `word` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; delimiter ;; create procedure idata() begin declare i int; set i=0; while i&amp;lt;10000 do insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10)))); set i=i+1; end while; end;; delimiter ;
call idata(); 为了便于量化说明，我在这个表里面插入了10000行记录。接下来，我们就一起看看要随机选择3个单词，有什么方法实现，存在什么问题以及如何改进。
内存临时表首先，你会想到用order by rand()来实现这个逻辑。</description></item><item><title>17_建立数据通路（上）：指令+运算=CPU</title><link>https://artisanbox.github.io/4/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/17/</guid><description>前面几讲里，我从两个不同的部分为你讲解了CPU的功能。
在“指令”部分，我为你讲解了计算机的“指令”是怎么运行的，也就是我们撰写的代码，是怎么变成一条条的机器能够理解的指令的，以及是按照什么样的顺序运行的。
在“计算”部分，我为你讲解了计算机的“计算”部分是怎么执行的，数据的二进制表示是怎么样的，我们执行的加法和乘法又是通过什么样的电路来实现的。
然而，光知道这两部分还不能算是真正揭开了CPU的秘密，只有把“指令”和“计算”这两部分功能连通起来，我们才能构成一个真正完整的CPU。这一讲，我们就在前面知识的基础上，来看一个完整的CPU是怎么运转起来的。
指令周期（Instruction Cycle）前面讲计算机机器码的时候，我向你介绍过PC寄存器、指令寄存器，还介绍过MIPS体系结构的计算机所用到的R、I、J类指令。如果我们仔细看一看，可以发现，计算机每执行一条指令的过程，可以分解成这样几个步骤。
1.Fetch（取得指令），也就是从PC寄存器里找到对应的指令地址，根据指令地址从内存里把具体的指令，加载到指令寄存器中，然后把PC寄存器自增，好在未来执行下一条指令。
2.Decode（指令译码），也就是根据指令寄存器里面的指令，解析成要进行什么样的操作，是R、I、J中的哪一种指令，具体要操作哪些寄存器、数据或者内存地址。
3.Execute（执行指令），也就是实际运行对应的R、I、J这些特定的指令，进行算术逻辑操作、数据传输或者直接的地址跳转。
4.重复进行1～3的步骤。
这样的步骤，其实就是一个永不停歇的“Fetch - Decode - Execute”的循环，我们把这个循环称之为指令周期（Instruction Cycle）。
指令周期（Instruction Cycle）在这个循环过程中，不同部分其实是由计算机中的不同组件完成的。不知道你还记不记得，我们在专栏一开始讲的计算机组成的五大组件？
在取指令的阶段，我们的指令是放在存储器里的，实际上，通过PC寄存器和指令寄存器取出指令的过程，是由控制器（Control Unit）操作的。指令的解码过程，也是由控制器进行的。一旦到了执行指令阶段，无论是进行算术操作、逻辑操作的R型指令，还是进行数据传输、条件分支的I型指令，都是由算术逻辑单元（ALU）操作的，也就是由运算器处理的。不过，如果是一个简单的无条件地址跳转，那么我们可以直接在控制器里面完成，不需要用到运算器。
不同步骤在不同组件之内完成除了Instruction Cycle这个指令周期，在CPU里面我们还会提到另外两个常见的Cycle。一个叫Machine Cycle，机器周期或者CPU周期。CPU内部的操作速度很快，但是访问内存的速度却要慢很多。每一条指令都需要从内存里面加载而来，所以我们一般把从内存里面读取一条指令的最短时间，称为CPU周期。
还有一个是我们之前提过的Clock Cycle，也就是时钟周期以及我们机器的主频。一个CPU周期，通常会由几个时钟周期累积起来。一个CPU周期的时间，就是这几个Clock Cycle的总和。
对于一个指令周期来说，我们取出一条指令，然后执行它，至少需要两个CPU周期。取出指令至少需要一个CPU周期，执行至少也需要一个CPU周期，复杂的指令则需要更多的CPU周期。
三个周期（Cycle）之间的关系所以，我们说一个指令周期，包含多个CPU周期，而一个CPU周期包含多个时钟周期。
建立数据通路在专栏一开始，不少同学留言问到，ALU就是运算器吗？在讨论计算机五大组件的运算器的时候，我们提到过好几个不同的相关名词，比如ALU、运算器、处理器单元、数据通路，它们之间到底是什么关系呢？
名字是什么其实并不重要，一般来说，我们可以认为，数据通路就是我们的处理器单元。它通常由两类原件组成。
第一类叫操作元件，也叫组合逻辑元件（Combinational Element），其实就是我们的ALU。在前面讲ALU的过程中可以看到，它们的功能就是在特定的输入下，根据下面的组合电路的逻辑，生成特定的输出。
第二类叫存储元件，也有叫状态元件（State Element）的。比如我们在计算过程中需要用到的寄存器，无论是通用寄存器还是状态寄存器，其实都是存储元件。
我们通过数据总线的方式，把它们连接起来，就可以完成数据的存储、处理和传输了，这就是所谓的建立数据通路了。
下面我们来说控制器。它的逻辑就没那么复杂了。我们可以把它看成只是机械地重复“Fetch - Decode - Execute“循环中的前两个步骤，然后把最后一个步骤，通过控制器产生的控制信号，交给ALU去处理。
听起来是不是很简单？实际上，控制器的电路特别复杂。下面我给你详细解析一下。
一方面，所有CPU支持的指令，都会在控制器里面，被解析成不同的输出信号。我们之前说过，现在的Intel CPU支持2000个以上的指令。这意味着，控制器输出的控制信号，至少有2000种不同的组合。
运算器里的ALU和各种组合逻辑电路，可以认为是一个固定功能的电路。控制器“翻译”出来的，就是不同的控制信号。这些控制信号，告诉ALU去做不同的计算。可以说正是控制器的存在，让我们可以“编程”来实现功能，能让我们的“存储程序型计算机”名副其实。
指令译码器将输入的机器码，解析成不同的操作码和操作数，然后传输给ALU进行计算CPU所需要的硬件电路那么，要想搭建出来整个CPU，我们需要在数字电路层面，实现这样一些功能。
首先，自然是我们之前已经讲解过的ALU了，它实际就是一个没有状态的，根据输入计算输出结果的第一个电路。
第二，我们需要有一个能够进行状态读写的电路元件，也就是我们的寄存器。我们需要有一个电路，能够存储到上一次的计算结果。这个计算结果并不一定要立刻拿到电路的下游去使用，但是可以在需要的时候拿出来用。常见的能够进行状态读写的电路，就有锁存器（Latch），以及我们后面要讲的D触发器（Data/Delay Flip-flop）的电路。
第三，我们需要有一个“自动”的电路，按照固定的周期，不停地实现PC寄存器自增，自动地去执行“Fetch - Decode - Execute“的步骤。我们的程序执行，并不是靠人去拨动开关来执行指令的。我们希望有一个“自动”的电路，不停地去一条条执行指令。
我们看似写了各种复杂的高级程序进行各种函数调用、条件跳转。其实只是修改PC寄存器里面的地址。PC寄存器里面的地址一修改，计算机就可以加载一条指令新指令，往下运行。实际上，PC寄存器还有一个名字，就叫作程序计数器。顾名思义，就是随着时间变化，不断去数数。数的数字变大了，就去执行一条新指令。所以，我们需要的就是一个自动数数的电路。
第四，我们需要有一个“译码”的电路。无论是对于指令进行decode，还是对于拿到的内存地址去获取对应的数据或者指令，我们都需要通过一个电路找到对应的数据。这个对应的自然就是“译码器”的电路了。
好了，现在我们把这四类电路，通过各种方式组合在一起，就能最终组成功能强大的CPU了。但是，要实现这四种电路中的中间两种，我们还需要时钟电路的配合。下一节，我们一起来看一看，这些基础的电路功能是怎么实现的，以及怎么把这些电路组合起来变成一个CPU。
总结延伸好了，到这里，我们已经把CPU运转需要的数据通路和控制器介绍完了，也找出了需要完成这些功能，需要的4种基本电路。它们分别是，ALU这样的组合逻辑电路、用来存储数据的锁存器和D触发器电路、用来实现PC寄存器的计数器电路，以及用来解码和寻址的译码器电路。
虽然CPU已经是由几十亿个晶体管组成的及其复杂的电路，但是它仍然是由这样一个个基本功能的电路组成的。只要搞清楚这些电路的运作原理，你自然也就弄明白了CPU的工作原理。
推荐阅读如果想要了解数据通路，可以参看《计算机组成与设计 硬件软件接口》的第5版的4.1到4.4节。专栏里的内容是从更高一层的抽象逻辑来解释这些问题，而教科书里包含了更多电路的技术细节。这两者结合起来学习，能够帮助你更深入地去理解数据通路。
课后思考这一讲，我们说CPU好像一个永不停歇的机器，一直在不停地读取下一条指令去运行。那为什么CPU还会有满载运行和Idle闲置的状态呢？请你自己搜索研究一下这是为什么，并在留言区写下你的思考和答案。
欢迎你留言和我分享，你也可以把今天的文章分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>17_跳表：为什么Redis一定要用跳表来实现有序集合？</title><link>https://artisanbox.github.io/2/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/18/</guid><description>上两节我们讲了二分查找算法。当时我讲到，因为二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，就真的没法用二分查找算法了吗？
实际上，我们只需要对链表稍加改造，就可以支持类似“二分”的查找算法。我们把改造之后的数据结构叫做跳表（Skip list），也就是今天要讲的内容。
跳表这种数据结构对你来说，可能会比较陌生，因为一般的数据结构和算法书籍里都不怎么会讲。但是它确实是一种各方面性能都比较优秀的动态数据结构，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。
Redis中的有序集合（Sorted Set）就是用跳表来实现的。如果你有一定基础，应该知道红黑树也可以实现快速地插入、删除和查找操作。那Redis为什么会选择用跳表来实现有序集合呢？ 为什么不用红黑树呢？学完今天的内容，你就知道答案了。
如何理解“跳表”？对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是O(n)。
那怎么来提高查找效率呢？如果像图中那样，对链表建立一级“索引”，查找起来是不是就会更快一些呢？每两个结点提取一个结点到上一级，我们把抽出来的那一级叫做索引或索引层。你可以看我画的图。图中的down表示down指针，指向下一级结点。
如果我们现在要查找某个结点，比如16。我们可以先在索引层遍历，当遍历到索引层中值为13的结点时，我们发现下一个结点是17，那要查找的结点16肯定就在这两个结点之间。然后我们通过索引层结点的down指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历2个结点，就可以找到值等于16的这个结点了。这样，原来如果要查找16，需要遍历10个结点，现在只需要遍历7个结点。
从这个例子里，我们看出，加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。那如果我们再加一级索引呢？效率会不会提升更多呢？
跟前面建立第一级索引的方式相似，我们在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在我们再来查找16，只需要遍历6个结点了，需要遍历的结点数量又减少了。
我举的例子数据量不大，所以即便加了两级索引，查找效率的提升也并不明显。为了让你能真切地感受索引提升查询效率。我画了一个包含64个结点的链表，按照前面讲的这种思路，建立了五级索引。
从图中我们可以看出，原来没有索引的时候，查找62需要遍历62个结点，现在只需要遍历11个结点，速度是不是提高了很多？所以，当链表的长度n比较大时，比如1000、10000的时候，在构建索引之后，查找效率的提升就会非常明显。
前面讲的这种链表加多级索引的结构，就是跳表。我通过例子给你展示了跳表是如何减少查询次数的，现在你应该比较清晰地知道，跳表确实是可以提高查询效率的。接下来，我会定量地分析一下，用跳表查询到底有多快。
用跳表查询到底有多快？前面我讲过，算法的执行效率可以通过时间复杂度来度量，这里依旧可以用。我们知道，在一个单链表中查询某个数据的时间复杂度是O(n)。那在一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢？
这个时间复杂度的分析方法比较难想到。我把问题分解一下，先来看这样一个问题，如果链表里有n个结点，会有多少级索引呢？
按照我们刚才讲的，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2，第二级索引的结点个数大约就是n/4，第三级索引的结点个数大约就是n/8，依次类推，也就是说，第k级索引的结点个数是第k-1级索引的结点个数的1/2，那第k级索引结点的个数就是n/(2k)。
假设索引有h级，最高级的索引有2个结点。通过上面的公式，我们可以得到n/(2h)=2，从而求得h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历m个结点，那在跳表中查询一个数据的时间复杂度就是O(m*logn)。
那这个m的值是多少呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历3个结点，也就是说m=3，为什么是3呢？我来解释一下。
假设我们要查找的数据是x，在第k级索引中，我们遍历到y结点之后，发现x大于y，小于后面的结点z，所以我们通过y的down指针，从第k级索引下降到第k-1级索引。在第k-1级索引中，y和z之间只有3个结点（包含y和z），所以，我们在K-1级索引中最多只需要遍历3个结点，依次类推，每一级索引都最多只需要遍历3个结点。
通过上面的分析，我们得到m=3，所以在跳表中查询任意数据的时间复杂度就是O(logn)。这个查找的时间复杂度跟二分查找是一样的。换句话说，我们其实是基于单链表实现了二分查找，是不是很神奇？不过，天下没有免费的午餐，这种查询效率的提升，前提是建立了很多级索引，也就是我们在第6节讲过的空间换时间的设计思路。
跳表是不是很浪费内存？比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。那到底需要消耗多少额外的存储空间呢？我们来分析一下跳表的空间复杂度。
跳表的空间复杂度分析并不难，我在前面说了，假设原始链表大小为n，那第一级索引大约有n/2个结点，第二级索引大约有n/4个结点，以此类推，每上升一级就减少一半，直到剩下2个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。
这几级索引的结点总和就是n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是O(n)。也就是说，如果将包含n个结点的单链表构造成跳表，我们需要额外再用接近n个结点的存储空间。那我们有没有办法降低索引占用的内存空间呢？
我们前面都是每两个结点抽一个结点到上级索引，如果我们每三个结点或五个结点，抽一个结点到上级索引，是不是就不用那么多索引结点了呢？我画了一个每三个结点抽一个的示意图，你可以看下。
从图中可以看出，第一级索引需要大约n/3个结点，第二级索引需要大约n/9个结点。每往上一级，索引结点个数都除以3。为了方便计算，我们假设最高一级的索引结点个数是1。我们把每级索引的结点个数都写下来，也是一个等比数列。
通过等比数列求和公式，总的索引结点大约就是n/3+n/9+n/27+...+9+3+1=n/2。尽管空间复杂度还是O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。
实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。
高效的动态插入和删除跳表长什么样子我想你应该已经很清楚了，它的查找操作我们刚才也讲过了。实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是O(logn)。
我们现在来看下， 如何在跳表中插入一个数据，以及它是如何做到O(logn)的时间复杂度的？
我们知道，在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。
对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的时间复杂度是O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是O(logn)。我画了一张图，你可以很清晰地看到插入的过程。
好了，我们再来看删除操作。
如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。
跳表索引动态更新当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某2个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。
作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。
如果你了解红黑树、AVL树这样平衡二叉树，你就知道它们是通过左右旋的方式保持左右子树的大小平衡（如果不了解也没关系，我们后面会讲），而跳表是通过随机函数来维护前面提到的“平衡性”。
当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。如何选择加入哪些索引层呢？
我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值K，那我们就将这个结点添加到第一级到第K级这K级索引中。
随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。至于随机函数的选择，我就不展开讲解了。如果你感兴趣的话，可以看看我在GitHub上的代码或者Redis中关于有序集合的跳表实现。
跳表的实现还是稍微有点复杂的，我将Java实现的代码放到了GitHub中，你可以根据我刚刚的讲解，对照着代码仔细思考一下。你不用死记硬背代码，跳表的实现并不是我们这节的重点。
解答开篇今天的内容到此就讲完了。现在，我来讲解一下开篇的思考题：为什么Redis要用跳表来实现有序集合，而不是红黑树？
Redis中的有序集合是通过跳表来实现的，严格点讲，其实还用到了散列表。不过散列表我们后面才会讲到，所以我们现在暂且忽略这部分。如果你去查看Redis的开发手册，就会发现，Redis中的有序集合支持的核心操作主要有下面这几个：
插入一个数据；
删除一个数据；
查找一个数据；
按照区间查找数据（比如查找值在[100, 356]之间的数据）；
迭代输出有序序列。
其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。
对于按照区间查找数据这个操作，跳表可以做到O(logn)的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。
当然，Redis之所以用跳表来实现有序集合，还有其他原因，比如，跳表更容易代码实现。虽然跳表的实现也不简单，但比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。
不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的Map类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果你想使用跳表，必须要自己实现。
内容小结今天我们讲了跳表这种数据结构。跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是O(logn)。
跳表的空间复杂度是O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。</description></item><item><title>17｜生成本地代码第2关：变量存储、函数调用和栈帧维护</title><link>https://artisanbox.github.io/3/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/19/</guid><description>你好，我是宫文学。
在上一节课里，我们已经初步生成了汇编代码和可执行文件。不过，很多技术细节我还没有来得及给你介绍，而且我们支持的语言特性也比较简单。
那么，这一节课，我就来给你补上这些技术细节。比如，我们要如何把逻辑寄存器映射到物理寄存器或内存地址、如何管理栈桢，以及如何让程序符合调用约定等等。
好了，我们开始吧。先让我们解决逻辑寄存器的映射问题，这其中涉及一个简单的寄存器分配算法。
给变量分配物理寄存器或内存在上一节课，我们在生成汇编代码的时候，给参数、本地变量和临时变量使用的都是逻辑寄存器，也就是只保存了变量的下标。那么我们要怎么把这些逻辑寄存器对应到物理的存储方式上来呢？
我们还是先来梳理一下实现思路吧。
其实，我们接下来要实现的寄存器分配算法，是一个比较初级的算法。你如果用clang或gcc把一个C语言的文件编译成汇编代码，并且不带-O1、-O2这样的优化选项，生成出来的汇编代码就是采用了类似的寄存器分配算法。现在我们就来看看这种汇编代码在实际存储变量上的特点。
首先，程序的参数都被保存到了内存里。具体是怎么来保存的呢？你可以先看看示例程序param.c：
void println(int a); int foo(int p1, int p2, int p3, int p4, int p5, int p6, int p7, int p8){ int x1 = p1p2; int x2 = p3p4; return x1 + x2 + p5p6 + p7p8; }
int main(){ int a = 10; int b = 12; int c = ab + foo(a,b,1,2,3,4,5,6) + foo(b,a,7,8,9,10,11,12); println(c); return 0; } &amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;这个示例程序所对应的汇编代码是param.</description></item><item><title>18_为什么这些SQL语句逻辑相同，性能却差异巨大？</title><link>https://artisanbox.github.io/1/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/18/</guid><description>在MySQL中，有很多看上去逻辑相同，但性能却差异巨大的SQL语句。对这些语句使用不当的话，就会不经意间导致整个数据库的压力变大。
我今天挑选了三个这样的案例和你分享。希望再遇到相似的问题时，你可以做到举一反三、快速解决问题。
案例一：条件字段函数操作假设你现在维护了一个交易系统，其中交易记录表tradelog包含交易流水号（tradeid）、交易员id（operator）、交易时间（t_modified）等字段。为了便于描述，我们先忽略其他字段。这个表的建表语句如下：
mysql&amp;gt; CREATE TABLE `tradelog` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `operator` int(11) DEFAULT NULL, `t_modified` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`), KEY `t_modified` (`t_modified`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 假设，现在已经记录了从2016年初到2018年底的所有数据，运营部门有一个需求是，要统计发生在所有年份中7月份的交易记录总数。这个逻辑看上去并不复杂，你的SQL语句可能会这么写：
mysql&amp;gt; select count(*) from tradelog where month(t_modified)=7; 由于t_modified字段上有索引，于是你就很放心地在生产库中执行了这条语句，但却发现执行了特别久，才返回了结果。
如果你问DBA同事为什么会出现这样的情况，他大概会告诉你：如果对字段做了函数计算，就用不上索引了，这是MySQL的规定。
现在你已经学过了InnoDB的索引结构了，可以再追问一句为什么？为什么条件是where t_modified='2018-7-1’的时候可以用上索引，而改成where month(t_modified)=7的时候就不行了？
下面是这个t_modified索引的示意图。方框上面的数字就是month()函数对应的值。
图1 t_modified索引示意图如果你的SQL语句条件用的是where t_modified='2018-7-1’的话，引擎就会按照上面绿色箭头的路线，快速定位到 t_modified='2018-7-1’需要的结果。
实际上，B+树提供的这个快速定位能力，来源于同一层兄弟节点的有序性。
但是，如果计算month()函数的话，你会看到传入7的时候，在树的第一层就不知道该怎么办了。
也就是说，对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。
需要注意的是，优化器并不是要放弃使用这个索引。
在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引t_modified，优化器对比索引大小后发现，索引t_modified更小，遍历这个索引比遍历主键索引来得更快。因此最终还是会选择索引t_modified。
接下来，我们使用explain命令，查看一下这条SQL语句的执行结果。
图2 explain 结果key="t_modified"表示的是，使用了t_modified这个索引；我在测试表数据中插入了10万行数据，rows=100335，说明这条语句扫描了整个索引的所有值；Extra字段的Using index，表示的是使用了覆盖索引。
也就是说，由于在t_modified字段加了month()函数操作，导致了全索引扫描。为了能够用上索引的快速定位能力，我们就要把SQL语句改成基于字段本身的范围查询。按照下面这个写法，优化器就能按照我们预期的，用上t_modified索引的快速定位能力了。
mysql&amp;gt; select count(*) from tradelog where -&amp;gt; (t_modified &amp;gt;= '2016-7-1' and t_modified&amp;lt;'2016-8-1') or -&amp;gt; (t_modified &amp;gt;= '2017-7-1' and t_modified&amp;lt;'2017-8-1') or -&amp;gt; (t_modified &amp;gt;= '2018-7-1' and t_modified&amp;lt;'2018-8-1'); 当然，如果你的系统上线时间更早，或者后面又插入了之后年份的数据的话，你就需要再把其他年份补齐。</description></item><item><title>18_建立数据通路（中）：指令+运算=CPU</title><link>https://artisanbox.github.io/4/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/18/</guid><description>上一讲，我们看到，要能够实现一个完整的CPU功能，除了加法器这样的电路之外，我们还需要实现其他功能的电路。其中有一些电路，和我们实现过的加法器一样，只需要给定输入，就能得到固定的输出。这样的电路，我们称之为组合逻辑电路（Combinational Logic Circuit）。
但是，光有组合逻辑电路是不够的。你可以想一下，如果只有组合逻辑电路，我们的CPU会是什么样的？电路输入是确定的，对应的输出自然也就确定了。那么，我们要进行不同的计算，就要去手动拨动各种开关，来改变电路的开闭状态。这样的计算机，不像我们现在每天用的功能强大的电子计算机，反倒更像古老的计算尺或者机械计算机，干不了太复杂的工作，只能协助我们完成一些计算工作。
这样，我们就需要引入第二类的电路，也就是时序逻辑电路（Sequential Logic Circuit）。时序逻辑电路可以帮我们解决这样几个问题。
第一个就是自动运行的问题。时序电路接通之后可以不停地开启和关闭开关，进入一个自动运行的状态。这个使得我们上一讲说的，控制器不停地让PC寄存器自增读取下一条指令成为可能。
第二个是存储的问题。通过时序电路实现的触发器，能把计算结果存储在特定的电路里面，而不是像组合逻辑电路那样，一旦输入有任何改变，对应的输出也会改变。
第三个本质上解决了各个功能按照时序协调的问题。无论是程序实现的软件指令，还是到硬件层面，各种指令的操作都有先后的顺序要求。时序电路使得不同的事件按照时间顺序发生。
时钟信号的硬件实现想要实现时序逻辑电路，第一步我们需要的就是一个时钟。我在第3讲说过，CPU的主频是由一个晶体振荡器来实现的，而这个晶体振荡器生成的电路信号，就是我们的时钟信号。
实现这样一个电路，和我们之前讲的，通过电的磁效应产生开关信号的方法是一样的。只不过，这里的磁性开关，打开的不再是后续的线路，而是当前的线路。
在下面这张图里你可以看到，我们在原先一般只放一个开关的信号输入端，放上了两个开关。一个开关A，一开始是断开的，由我们手工控制；另外一个开关B，一开始是合上的，磁性线圈对准一开始就合上的开关B。
于是，一旦我们合上开关A，磁性线圈就会通电，产生磁性，开关B就会从合上变成断开。一旦这个开关断开了，电路就中断了，磁性线圈就失去了磁性。于是，开关B又会弹回到合上的状态。这样一来，电路接通，线圈又有了磁性。我们的电路就会来回不断地在开启、关闭这两个状态中切换。
开关A闭合（也就是相当于接通电路之后），开关B就会不停地在开和关之间切换，生成对应的时钟信号这个不断切换的过程，对于下游电路来说，就是不断地产生新的0和1这样的信号。如果你在下游的电路上接上一个灯泡，就会发现这个灯泡在亮和暗之间不停切换。这个按照固定的周期不断在0和1之间切换的信号，就是我们的时钟信号（Clock Signal）。
一般这样产生的时钟信号，就像你在各种教科书图例中看到的一样，是一个振荡产生的0、1信号。
时钟信号示意图这种电路，其实就相当于把电路的输出信号作为输入信号，再回到当前电路。这样的电路构造方式呢，我们叫作反馈电路（Feedback Circuit）。
接下来，我们还会看到更多的反馈电路。上面这个反馈电路一般可以用下面这个示意图来表示，其实就是一个输出结果接回输入的反相器（Inverter），也就是我们之前讲过的非门。
通过一个反相器实现时钟信号通过D触发器实现存储功能有了时钟信号，我们的系统里就有了一个像“自动门”一样的开关。利用这个开关和相同的反馈电路，我们就可以构造出一个有“记忆”功能的电路。这个有记忆功能的电路，可以实现在CPU中用来存储计算结果的寄存器，也可以用来实现计算机五大组成部分之一的存储器。
我们先来看下面这个RS触发器电路。这个电路由两个或非门电路组成。我在图里面，把它标成了A和B。
或非门的真值表 在这个电路一开始，输入开关都是关闭的，所以或非门（NOR）A的输入是0和0。对应到我列的这个真值表，输出就是1。而或非门B的输入是0和A的输出1，对应输出就是0。B的输出0反馈到A，和之前的输入没有变化，A的输出仍然是1。而整个电路的输出Q，也就是0。
当我们把A前面的开关R合上的时候，A的输入变成了1和0，输出就变成了0，对应B的输入变成0和0，输出就变成了1。B的输出1反馈给到了A，A的输入变成了1和1，输出仍然是0。所以把A的开关合上之后，电路仍然是稳定的，不会像晶振那样振荡，但是整个电路的输出Q变成了1。
这个时候，如果我们再把A前面的开关R打开，A的输入变成和1和0，输出还是0，对应的B的输入没有变化，输出也还是1。B的输出1反馈给到了A，A的输入变成了1和0，输出仍然是0。这个时候，电路仍然稳定。开关R和S的状态和上面的第一步是一样的，但是最终的输出Q仍然是1，和第1步里Q状态是相反的。我们的输入和刚才第二步的开关状态不一样，但是输出结果仍然保留在了第2步时的输出没有发生变化。
这个时候，只有我们再去关闭下面的开关S，才可以看到，这个时候，B有一个输入必然是1，所以B的输出必然是0，也就是电路的最终输出Q必然是0。
这样一个电路，我们称之为触发器（Flip-Flop）。接通开关R，输出变为1，即使断开开关，输出还是1不变。接通开关S，输出变为0，即使断开开关，输出也还是0。也就是，当两个开关都断开的时候，最终的输出结果，取决于之前动作的输出结果，这个也就是我们说的记忆功能。
这里的这个电路是最简单的RS触发器，也就是所谓的复位置位触发器（Reset-Set Flip Flop) 。对应的输出结果的真值表，你可以看下面这个表格。可以看到，当两个开关都是0的时候，对应的输出不是1或者0，而是和Q的上一个状态一致。
再往这个电路里加两个与门和一个小小的时钟信号，我们就可以实现一个利用时钟信号来操作一个电路了。这个电路可以帮我们实现什么时候可以往Q里写入数据。
我们看看下面这个电路，这个在我们的上面的R-S触发器基础之上，在R和S开关之后，加入了两个与门，同时给这两个与门加入了一个时钟信号CLK作为电路输入。
这样，当时钟信号CLK在低电平的时候，与门的输入里有一个0，两个实际的R和S后的与门的输出必然是0。也就是说，无论我们怎么按R和S的开关，根据R-S触发器的真值表，对应的Q的输出都不会发生变化。
只有当时钟信号CLK在高电平的时候，与门的一个输入是1，输出结果完全取决于R和S的开关。我们可以在这个时候，通过开关R和S，来决定对应Q的输出。
通过一个时钟信号，我们可以在特定的时间对输出的Q进行写入操作如果这个时候，我们让R和S的开关，也用一个反相器连起来，也就是通过同一个开关控制R和S。只要CLK信号是1，R和S就可以设置输出Q。而当CLK信号是0的时候，无论R和S怎么设置，输出信号Q是不变的。这样，这个电路就成了我们最常用的D型触发器。用来控制R和S这两个开关的信号呢，我们视作一个输入的数据信号D，也就是Data，这就是D型触发器的由来。
把R和S两个信号通过一个反相器合并，我们可以通过一个数据信号D进行Q的写入操作一个D型触发器，只能控制1个比特的读写，但是如果我们同时拿出多个D型触发器并列在一起，并且把用同一个CLK信号控制作为所有D型触发器的开关，这就变成了一个N位的D型触发器，也就可以同时控制N位的读写。
CPU里面的寄存器可以直接通过D型触发器来构造。我们可以在D型触发器的基础上，加上更多的开关，来实现清0或者全部置为1这样的快捷操作。
总结延伸好了，到了这里，我们可以顺一顺思路了。通过引入了时序电路，我们终于可以把数据“存储”下来了。我们通过反馈电路，创建了时钟信号，然后再利用这个时钟信号和门电路组合，实现了“状态记忆”的功能。
电路的输出信号不单单取决于当前的输入信号，还要取决于输出信号之前的状态。最常见的这个电路就是我们的D触发器，它也是我们实际在CPU内实现存储功能的寄存器的实现方式。
这也是现代计算机体系结构中的“冯·诺伊曼”机的一个关键，就是程序需要可以“存储”，而不是靠固定的线路连接或者手工拨动开关，来实现计算机的可存储和可编程的功能。
有了时钟信号和触发器之后，我们还差一个“自动”需求没有实现。我们的计算机还不能做到自动地不停地从内存里面读取指令去执行。这一部分，我们留在下一讲。下一讲里，我们看看怎么让程序自动运转起来。
推荐阅读想要深入了解计算机里面的各种功能组件，是怎么通过电路来实现的，推荐你去阅读《编码：隐匿在计算机软硬件背后的语言》这本书的第14章和16章。
如果对于数字电路和数字逻辑特别感兴趣，想要彻底弄清楚数字电路、时序逻辑电路，也可以看一看计算机学科的一本专业的教科书《数字逻辑应用与设计》。
课后思考现在我们的CPU主频非常高了，通常在几GHz了，但是实际上我们的晶振并不能提供这么高的频率，而是通过“外频+倍频“的方式来实现高频率的时钟信号。请你研究一下，倍频和分频的信号是通过什么样的电路实现的？
欢迎留言和我分享你的疑惑和见解，也欢迎你把今天的内容分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>18_散列表（上）：Word文档中的单词拼写检查功能是如何实现的？</title><link>https://artisanbox.github.io/2/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/19/</guid><description>Word这种文本编辑器你平时应该经常用吧，那你有没有留意过它的拼写检查功能呢？一旦我们在Word里输入一个错误的英文单词，它就会用标红的方式提示“拼写错误”。Word的这个单词拼写检查功能，虽然很小但却非常实用。你有没有想过，这个功能是如何实现的呢？
其实啊，一点儿都不难。只要你学完今天的内容，散列表（Hash Table）。你就能像微软Office的工程师一样，轻松实现这个功能。
散列思想散列表的英文叫“Hash Table”，我们平时也叫它“哈希表”或者“Hash表”，你一定也经常听过它，我在前面的文章里，也不止一次提到过，但是你是不是真的理解这种数据结构呢？
散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。
我用一个例子来解释一下。假如我们有89名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。这89名选手的编号依次是1到89。现在我们希望编程实现这样一个功能，通过编号快速找到对应的选手信息。你会怎么做呢？
我们可以把这89名选手的信息放在数组里。编号为1的选手，我们放到数组中下标为1的位置；编号为2的选手，我们放到数组中下标为2的位置。以此类推，编号为k的选手放到数组中下标为k的位置。
因为参赛编号跟数组下标一一对应，当我们需要查询参赛编号为x的选手的时候，我们只需要将下标为x的数组元素取出来就可以了，时间复杂度就是O(1)。这样按照编号查找选手信息，效率是不是很高？
实际上，这个例子已经用到了散列的思想。在这个例子里，参赛编号是自然数，并且与数组的下标形成一一映射，所以利用数组支持根据下标随机访问的时候，时间复杂度是O(1)这一特性，就可以实现快速查找编号对应的选手信息。
你可能要说了，这个例子中蕴含的散列思想还不够明显，那我来改造一下这个例子。
假设校长说，参赛编号不能设置得这么简单，要加上年级、班级这些更详细的信息，所以我们把编号的规则稍微修改了一下，用6位数字来表示。比如051167，其中，前两位05表示年级，中间两位11表示班级，最后两位还是原来的编号1到89。这个时候我们该如何存储选手信息，才能够支持通过编号来快速查找选手信息呢？
思路还是跟前面类似。尽管我们不能直接把编号作为数组下标，但我们可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。当通过参赛编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。
这就是典型的散列思想。其中，参赛选手的编号我们叫做键（key）或者关键字。我们用它来标识一个选手。我们把参赛编号转化为数组下标的映射方法就叫作散列函数（或“Hash函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash值”“哈希值”）。
通过这个例子，我们可以总结出这样的规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是O(1)的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。
散列函数从上面的例子我们可以看到，散列函数在散列表中起着非常关键的作用。现在我们就来学习下散列函数。
散列函数，顾名思义，它是一个函数。我们可以把它定义成hash(key)，其中key表示元素的键值，hash(key)的值表示经过散列函数计算得到的散列值。
那第一个例子中，编号就是数组下标，所以hash(key)就等于key。改造后的例子，写成散列函数稍微有点复杂。我用伪代码将它写成函数就是下面这样：
int hash(String key) { // 获取后两位字符 string lastTwoChars = key.substr(length-2, length); // 将后两位字符转换为整数 int hashValue = convert lastTwoChas to int-type; return hashValue; } 刚刚举的学校运动会的例子，散列函数比较简单，也比较容易想到。但是，如果参赛选手的编号是随机生成的6位数字，又或者用的是a到z之间的字符串，该如何构造散列函数呢？我总结了三点散列函数设计的基本要求：
散列函数计算得到的散列值是一个非负整数；
如果key1 = key2，那hash(key1) == hash(key2)；
如果key1 ≠ key2，那hash(key1) ≠ hash(key2)。
我来解释一下这三点。其中，第一点理解起来应该没有任何问题。因为数组下标是从0开始的，所以散列函数生成的散列值也要是非负整数。第二点也很好理解。相同的key，经过散列函数得到的散列值也应该是相同的。
第三点理解起来可能会有问题，我着重说一下。这个要求看起来合情合理，但是在真实的情况下，要想找到一个不同的key对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。
所以我们几乎无法找到一个完美的无冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，我们需要通过其他途径来解决。
散列冲突再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。
1.开放寻址法开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法，线性探测（Linear Probing）。
当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。
我说的可能比较抽象，我举一个例子具体给你说明一下。这里面黄色的色块表示空闲位置，橙色的色块表示已经存储了数据。</description></item><item><title>18｜生成本地代码第3关：实现完整的功能</title><link>https://artisanbox.github.io/3/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/20/</guid><description>你好，我是宫文学。
到目前为止，我们已经把挑战生成本地代码的过程中会遇到的各种难点都解决了，也就是说，我们已经实现基本的寄存器分配算法，并维护好了栈桢。在这个基础上，我们只需要再实现其他的语法特性就行了。
所以，在今天这节课，我们要让编译器支持条件语句和循环语句。这样的话，我们就可以为前面一直在使用的一些例子，比如生成斐波那契数列的程序，生成本地代码了。然后，我们可以再比较一次不同运行时机制下的性能表现。
还记得吗？我们在前面已经分别使用了AST解释器、基于JavaScript的虚拟机和基于C语言的虚拟机来生成斐波那契数列。现在我们就看看用我们自己生成的本地代码，性能上是否会有巨大的变化。
在这个过程中，我们还会再次认识CFG这种数据结构，也会考察一下如何支持一元运算，让我们的语言特性更加丰富。
那首先我们来看一下，如何实现if语句和for循环语句。
支持if语句和for循环语句在前面的课程中，我们曾经练习过为if语句和for循环语句生成字节码。不知道你还记不记得，其中的难点就是生成跳转指令。在今天这节课，我们会完成类似的任务，但采用的是一个稍微不同的方法。
我们还是先来研究一下if语句，看看针对if语句，编译器需要生成什么代码。我们采用下面一个C语言的示例程序，更详细的代码你可以参见代码库中的if.c：
int foo(int a){ if (a &amp;gt; 10) return a + 8; else return a - 8; } C语言的编译器针对这段示例程序，会生成下面的汇编代码（参见代码库中的if.s），我对汇编代码进行了整理，并添加了注释。
这段汇编代码是未经优化的。不过，我相信你经过前面课程的训练，应该可以看出来很多可以用手工优化的地方。不过现在我们关注的重点是跳转指令，所以你可以重点看一下代码中的cmpl指令、jle指令和jmp指令。
我们现在来分析一下。第一个cmpl指令的作用是比较两个整数的大小。在这个例子中，是比较a和10的大小，计算结果会设置到eflags寄存器中相应的标志位。
第二个jle指令，它的作用是根据eflags寄存器中标志位决定是否进行跳转，如果发现是小于等于的结果，那么就进行跳转，这里是跳转到else块。如果是大于呢，就会顺着执行下面的指令，也就是if块的内容。
最后我们来看jmp指令，这是无条件跳转指令，相当于我们前面学过的字节码中的goto指令。
认识了这三个指令以后，我们就知道程序的跳转逻辑了。在这个C语言的示例程序中，一共有四个基本块，我把它们之间的跳转关系画成了图，可以更加直观一些：
在分析清楚了整个思路以后，为if语句生成本地代码的逻辑也就很清楚了，我们现在就动手吧。完整的代码你可以查看代码库里的visitIfStmt方法，我这里挑重点和你分析一下。
首先，我们要生成4个基本块：
//条件 let bbCondition = this.getCurrentBB(); let compOprand = this.visit(ifStmt.condition) as Oprand; //if块 let bbIfBlcok = this.newBlock(); this.visit(ifStmt.stmt);
//else块 let bbElseBlock:BasicBlock|null = null if (ifStmt.elseStmt != null){ bbElseBlock = this.newBlock(); this.visit(ifStmt.elseStmt); }
//最后，要新建一个基本块,用于If后面的语句。 let bbFollowing = this.newBlock(); 接着，我们要添加跳转指令，在4个基本块之间建立正确的跳转关系。这其中，最关键的就是我们怎么来为基本块0，也就是if条件所在的基本块生成跳转指令。</description></item><item><title>19_为什么我只查一行的语句，也执行这么慢？</title><link>https://artisanbox.github.io/1/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/19/</guid><description>一般情况下，如果我跟你说查询性能优化，你首先会想到一些复杂的语句，想到查询需要返回大量的数据。但有些情况下，“查一行”，也会执行得特别慢。今天，我就跟你聊聊这个有趣的话题，看看什么情况下，会出现这个现象。
需要说明的是，如果MySQL数据库本身就有很大的压力，导致数据库服务器CPU占用率很高或ioutil（IO利用率）很高，这种情况下所有语句的执行都有可能变慢，不属于我们今天的讨论范围。
为了便于描述，我还是构造一个表，基于这个表来说明今天的问题。这个表有两个字段id和c，并且我在里面插入了10万行记录。
mysql&amp;gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=100000) do insert into t values(i,i); set i=i+1; end while; end;; delimiter ;
call idata(); 接下来，我会用几个不同的场景来举例，有些是前面的文章中我们已经介绍过的知识点，你看看能不能一眼看穿，来检验一下吧。
第一类：查询长时间不返回如图1所示，在表t执行下面的SQL语句：
mysql&amp;gt; select * from t where id=1; 查询结果长时间不返回。
图1 查询长时间不返回一般碰到这种情况的话，大概率是表t被锁住了。接下来分析原因的时候，一般都是首先执行一下show processlist命令，看看当前语句处于什么状态。
然后我们再针对每种状态，去分析它们产生的原因、如何复现，以及如何处理。
等MDL锁&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;如图2所示，就是使用show processlist命令查看Waiting for table metadata lock的示意图。</description></item><item><title>19_建立数据通路（下）：指令+运算=CPU</title><link>https://artisanbox.github.io/4/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/19/</guid><description>上一讲，我们讲解了时钟信号是怎么实现的，以及怎么利用这个时钟信号，来控制数据的读写，可以使得我们能把需要的数据“存储”下来。那么，这一讲，我们要让计算机“自动”跑起来。
通过一个时钟信号，我们可以实现计数器，这个会成为我们的PC寄存器。然后，我们还需要一个能够帮我们在内存里面寻找指定数据地址的译码器，以及解析读取到的机器指令的译码器。这样，我们就能把所有学习到的硬件组件串联起来，变成一个CPU，实现我们在计算机指令的执行部分的运行步骤。
PC寄存器所需要的计数器我们常说的PC寄存器，还有个名字叫程序计数器。下面我们就来看看，它为什么叫作程序计数器。
有了时钟信号，我们可以提供定时的输入；有了D型触发器，我们可以在时钟信号控制的时间点写入数据。我们把这两个功能组合起来，就可以实现一个自动的计数器了。
加法器的两个输入，一个始终设置成1，另外一个来自于一个D型触发器A。我们把加法器的输出结果，写到这个D型触发器A里面。于是，D型触发器里面的数据就会在固定的时钟信号为1的时候更新一次。
这样，我们就有了一个每过一个时钟周期，就能固定自增1的自动计数器了。这个自动计数器，可以拿来当我们的PC寄存器。事实上，PC寄存器的这个PC，英文就是Program Counter，也就是程序计数器的意思。
每次自增之后，我们可以去对应的D型触发器里面取值，这也是我们下一条需要运行指令的地址。前面第5讲我们讲过，同一个程序的指令应该要顺序地存放在内存里面。这里就和前面对应上了，顺序地存放指令，就是为了让我们通过程序计数器就能定时地不断执行新指令。
加法计数、内存取值，乃至后面的命令执行，最终其实都是由我们一开始讲的时钟信号，来控制执行时间点和先后顺序的，这也是我们需要时序电路最核心的原因。
在最简单的情况下，我们需要让每一条指令，从程序计数，到获取指令、执行指令，都在一个时钟周期内完成。如果PC寄存器自增地太快，程序就会出错。因为前一次的运算结果还没有写回到对应的寄存器里面的时候，后面一条指令已经开始读取里面的数据来做下一次计算了。这个时候，如果我们的指令使用同样的寄存器，前一条指令的计算就会没有效果，计算结果就错了。
在这种设计下，我们需要在一个时钟周期里，确保执行完一条最复杂的CPU指令，也就是耗时最长的一条CPU指令。这样的CPU设计，我们称之为单指令周期处理器（Single Cycle Processor）。
很显然，这样的设计有点儿浪费。因为即便只调用一条非常简单的指令，我们也需要等待整个时钟周期的时间走完，才能执行下一条指令。在后面章节里我们会讲到，通过流水线技术进行性能优化，可以减少需要等待的时间，这里我们暂且说到这里。
读写数据所需要的译码器现在，我们的数据能够存储在D型触发器里了。如果我们把很多个D型触发器放在一起，就可以形成一块很大的存储空间，甚至可以当成一块内存来用。像我现在手头这台电脑，有16G内存。那我们怎么才能知道，写入和读取的数据，是在这么大的内存的哪几个比特呢？
于是，我们就需要有一个电路，来完成“寻址”的工作。这个“寻址”电路，就是我们接下来要讲的译码器。
在现在实际使用的计算机里面，内存所使用的DRAM，并不是通过上面的D型触发器来实现的，而是使用了一种CMOS芯片来实现的。不过，这并不影响我们从基础原理方面来理解译码器。在这里，我们还是可以把内存芯片，当成是很多个连在一起的D型触发器来实现的。
如果把“寻址”这件事情退化到最简单的情况，就是在两个地址中，去选择一个地址。这样的电路，我们叫作2-1选择器。我把它的电路实现画在了这里。
我们通过一个反相器、两个与门和一个或门，就可以实现一个2-1选择器。通过控制反相器的输入是0还是1，能够决定对应的输出信号，是和地址A，还是地址B的输入信号一致。
2-1选择器电路示意图一个反向器只能有0和1这样两个状态，所以我们只能从两个地址中选择一个。如果输入的信号有三个不同的开关，我们就能从$2^3$，也就是8个地址中选择一个了。这样的电路，我们就叫3-8译码器。现代的计算机，如果CPU是64位的，就意味着我们的寻址空间也是$2^{64}$，那么我们就需要一个有64个开关的译码器。
当我们把译码器和内存连到一起时，通常会组成这样一个电路所以说，其实译码器的本质，就是从输入的多个位的信号中，根据一定的开关和电路组合，选择出自己想要的信号。除了能够进行“寻址”之外，我们还可以把对应的需要运行的指令码，同样通过译码器，找出我们期望执行的指令，也就是在之前我们讲到过的opcode，以及后面对应的操作数或者寄存器地址。只是，这样的“译码器”，比起2-1选择器和3-8译码器，要复杂的多。
建立数据通路，构造一个最简单的CPUD触发器、自动计数以及译码器，再加上一个我们之前说过的ALU，我们就凑齐了一个拼装一个CPU必须要的零件了。下面，我们就来看一看，怎么把这些零件组合起来，才能实现指令执行和算术逻辑计算的CPU。
CPU实现的抽象逻辑图 首先，我们有一个自动计数器。这个自动计数器会随着时钟主频不断地自增，来作为我们的PC寄存器。 在这个自动计数器的后面，我们连上一个译码器。译码器还要同时连着我们通过大量的D触发器组成的内存。 自动计数器会随着时钟主频不断自增，从译码器当中，找到对应的计数器所表示的内存地址，然后读取出里面的CPU指令。 读取出来的CPU指令会通过我们的CPU时钟的控制，写入到一个由D触发器组成的寄存器，也就是指令寄存器当中。 在指令寄存器后面，我们可以再跟一个译码器。这个译码器不再是用来寻址的了，而是把我们拿到的指令，解析成opcode和对应的操作数。 当我们拿到对应的opcode和操作数，对应的输出线路就要连接ALU，开始进行各种算术和逻辑运算。对应的计算结果，则会再写回到D触发器组成的寄存器或者内存当中。 这样的一个完整的通路，也就完成了我们的CPU的一条指令的执行过程。在这个过程中，你会发现这样几个有意思的问题。
第一个，是我们之前在第6讲讲过的程序跳转所使用的条件码寄存器。那时，讲计算机的指令执行的时候，我们说高级语言中的if…else，其实是变成了一条cmp指令和一条jmp指令。cmp指令是在进行对应的比较，比较的结果会更新到条件码寄存器当中。jmp指令则是根据条件码寄存器当中的标志位，来决定是否进行跳转以及跳转到什么地址。
不知道你当时看到这个知识点的时候，有没有一些疑惑，为什么我们的if…else会变成这样两条指令，而不是设计成一个复杂的电路，变成一条指令？到这里，我们就可以解释了。这样分成两个指令实现，完全匹配好了我们在电路层面，“译码-执行-更新寄存器“这样的步骤。
cmp指令的执行结果放到了条件码寄存器里面，我们的条件跳转指令也是在ALU层面执行的，而不是在控制器里面执行的。这样的实现方式在电路层面非常直观，我们不需要一个非常复杂的电路，就能实现if…else的功能。
第二个，是关于我们在第17讲里讲到的指令周期、CPU周期和时钟周期的差异。在上面的抽象的逻辑模型中，你很容易发现，我们执行一条指令，其实可以不放在一个时钟周期里面，可以直接拆分到多个时钟周期。
我们可以在一个时钟周期里面，去自增PC寄存器的值，也就是指令对应的内存地址。然后，我们要根据这个地址从D触发器里面读取指令，这个还是可以在刚才那个时钟周期内。但是对应的指令写入到指令寄存器，我们可以放在一个新的时钟周期里面。指令译码给到ALU之后的计算结果，要写回到寄存器，又可以放到另一个新的时钟周期。所以，执行一条计算机指令，其实可以拆分到很多个时钟周期，而不是必须使用单指令周期处理器的设计。
因为从内存里面读取指令时间很长，所以如果使用单指令周期处理器，就意味着我们的指令都要去等待一些慢速的操作。这些不同指令执行速度的差异，也正是计算机指令有指令周期、CPU周期和时钟周期之分的原因。因此，现代我们优化CPU的性能时，用的CPU都不是单指令周期处理器，而是通过流水线、分支预测等技术，来实现在一个周期里同时执行多个指令。
总结延伸好了，今天我们讲完了，怎么通过连接不同功能的电路，实现出一个完整的CPU。
我们可以通过自动计数器的电路，来实现一个PC寄存器，不断生成下一条要执行的计算机指令的内存地址。然后通过译码器，从内存里面读出对应的指令，写入到D触发器实现的指令寄存器中。再通过另外一个译码器，把它解析成我们需要执行的指令和操作数的地址。这些电路，组成了我们计算机五大组成部分里面的控制器。
我们把opcode和对应的操作数，发送给ALU进行计算，得到计算结果，再写回到寄存器以及内存里面来，这个就是我们计算机五大组成部分里面的运算器。
我们的时钟信号，则提供了协调这样一条条指令的执行时间和先后顺序的机制。同样的，这也带来了一个挑战，那就是单指令周期处理器去执行一条指令的时间太长了。而这个挑战，也是我们接下来的几讲里要解答的问题。
推荐阅读《编码：隐匿在计算机软硬件背后的语言》的第17章，用更多细节的流程来讲解了CPU的数据通路。《计算机组成与设计 硬件/软件接口》的4.1到4.4小节，从另外一个层面和角度讲解了CPU的数据通路的建立，推荐你阅读一下。
课后思考CPU在执行无条件跳转的时候，不需要通过运算器以及ALU，可以直接在控制器里面完成，你能说说这是为什么吗？
欢迎在留言区写下你的思考和疑惑，你也可以把今天的内容分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>19_散列表（中）：如何打造一个工业级水平的散列表？</title><link>https://artisanbox.github.io/2/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/20/</guid><description>通过上一节的学习，我们知道，散列表的查询效率并不能笼统地说成是O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。
在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果我们使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从O(1)急剧退化为O(n)。
如果散列表中有10万个数据，退化后的散列表查询的效率就下降了10万倍。更直接点说，如果之前运行100次查询只需要0.1秒，那现在就需要1万秒。这样就有可能因为查询操作消耗大量CPU或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就是散列表碰撞攻击的基本原理。
今天，我们就来学习一下，如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？
如何设计散列函数？散列函数设计的好坏，决定了散列表冲突的概率大小，也直接决定了散列表的性能。那什么才是好的散列函数呢？
首先，散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接地影响到散列表的性能。其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。
实际工作中，我们还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。散列函数各式各样，我举几个常用的、简单的散列函数的设计方法，让你有个直观的感受。
第一个例子就是我们上一节的学生运动会的例子，我们通过分析参赛编号的特征，把编号中的后两位作为散列值。我们还可以用类似的散列函数处理手机号码，因为手机号码前几位重复的可能性很大，但是后面几位就比较随机，我们可以取手机号的后四位作为散列值。这种散列函数的设计方法，我们一般叫做“数据分析法”。
第二个例子就是上一节的开篇思考题，如何实现Word拼写检查功能。这里面的散列函数，我们就可以这样设计：将单词中每个字母的ASCll码值“进位”相加，然后再跟散列表的大小求余、取模，作为散列值。比如，英文单词nice，我们转化出来的散列值就是下面这样：
hash(&amp;quot;nice&amp;quot;)=((&amp;quot;n&amp;quot; - &amp;quot;a&amp;quot;) * 26*26*26 + (&amp;quot;i&amp;quot; - &amp;quot;a&amp;quot;)*26*26 + (&amp;quot;c&amp;quot; - &amp;quot;a&amp;quot;)*26+ (&amp;quot;e&amp;quot;-&amp;quot;a&amp;quot;)) / 78978 实际上，散列函数的设计方法还有很多，比如直接寻址法、平方取中法、折叠法、随机数法等，这些你只要了解就行了，不需要全都掌握。
装载因子过大了怎么办？我们上一节讲到散列表的装载因子的时候说过，装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。
对于没有频繁插入和删除的静态数据集合来说，我们很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数，因为毕竟之前数据都是已知的。
对于动态散列表来说，数据集合是频繁变动的，我们事先无法预估将要加入的数据个数，所以我们也无法事先申请一个足够大的散列表。随着数据慢慢加入，装载因子就会慢慢变大。当装载因子大到一定程度之后，散列冲突就会变得不可接受。这个时候，我们该如何处理呢？
还记得我们前面多次讲的“动态扩容”吗？你可以回想一下，我们是如何做数组、栈、队列的动态扩容的。
针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了0.4。
针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。
你可以看我图里这个例子。在原来的散列表中，21这个元素原来存储在下标为0的位置，搬移到新的散列表中，存储在下标为7的位置。
对于支持动态扩容的散列表，插入操作的时间复杂度是多少呢？前面章节我已经多次分析过支持动态扩容的数组、栈等数据结构的时间复杂度了。所以，这里我就不啰嗦了，你要是还不清楚的话，可以回去复习一下。
插入一个数据，最好情况下，不需要扩容，最好时间复杂度是O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是O(1)。
实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。
我们前面讲到，当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。
装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于1。
如何避免低效的扩容？我们刚刚分析得到，大部分情况下，动态扩容的散列表插入一个数据都很快，但是在特殊情况下，当装载因子已经到达阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢，甚至会无法接受。
我举一个极端的例子，如果散列表当前大小为1GB，要想扩容为原来的两倍大小，那就需要对1GB的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表，听起来就很耗时，是不是？
如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，“一次性”扩容的机制就不合适了。
为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。
当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。
这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。
通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是O(1)。
如何选择冲突解决方法？上一节我们讲了两种主要的散列冲突的解决办法，开放寻址法和链表法。这两种冲突解决办法在实际的软件开发中都非常常用。比如，Java中LinkedHashMap就采用了链表法解决冲突，ThreadLocalMap是通过线性探测的开放寻址法来解决冲突。那你知道，这两种冲突解决方法各有什么优势和劣势，又各自适用哪些场景吗？
1.开放寻址法我们先来看看，开放寻址法的优点有哪些。
开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用CPU缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。你可不要小看序列化，很多场合都会用到的。我们后面就有一节会讲什么是数据结构序列化、如何序列化，以及为什么要序列化。
我们再来看下，开放寻址法有哪些缺点。
上一节我们讲到，用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。
所以，我总结一下，当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。
2.链表法首先，链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。
链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于1的情况。接近1时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。
还记得我们之前在链表那一节讲的吗？链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对CPU缓存是不友好的，这方面对于执行效率也有一定的影响。
当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4个字节或者8个字节），那链表中指针的内存消耗在大对象面前就可以忽略了。
实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。
所以，我总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。
工业级散列表举例分析刚刚我讲了实现一个工业级散列表需要涉及的一些关键技术，现在，我就拿一个具体的例子，Java中的HashMap这样一个工业级的散列表，来具体看下，这些技术是怎么应用的。
1.初始大小HashMap默认的初始大小是16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高HashMap的性能。
2.装载因子和动态扩容最大装载因子默认是0.75，当HashMap中元素个数超过0.75*capacity（capacity表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。
3.散列冲突解决方法HashMap底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。
于是，在JDK1.8版本中，为了对HashMap做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高HashMap的性能。当红黑树结点个数少于8个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。
4.散列函数散列函数的设计并不复杂，追求的是简单高效、分布均匀。我把它摘抄出来，你可以看看。
int hash(Object key) { int h = key.</description></item><item><title>19｜怎么实现一个更好的寄存器分配算法：原理篇</title><link>https://artisanbox.github.io/3/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/21/</guid><description>你好，我是宫文学。
到目前为止，我们的语言已经能够生成机器码了，并且性能确实还挺高的。不过我们也知道，现在我们采用的寄存器分配算法呀，还是很初级的。
那这个初级的寄存器分配算法会遇到什么问题呢？我们还有更优化的分配寄存器的思路吗？
当然是有的。接下来的这两节课，我们就会来回答这两个问题，我会带你从原理到实操，理解和实现一个更好的算法，叫做线性扫描算法，让寄存器的分配获得更好的优化效果。
首先，我们来分析一下当前寄存器分配算法的局限性。
初级算法的不足在前两节课中，我们实现了一个初级的寄存器分配算法。这个算法的特点呢，是主要的数据都保存在内存的栈桢中，包括参数和本地变量。而临时变量，则是映射到寄存器，从而保证各类运算指令的合法性，因为像加减乘数这种运算，不能两个操作数都是内存地址。
这个算法有什么不足呢？你可以暂停一会儿，先自己想一下，大概有两点。
我们现在来揭晓答案。
第一点不足在生成的代码性能上。
你知道，我们做编译的目标，是要让生成的代码的性能最高，但这个算法在这方面显然是不合格的。因为参数和本地变量都是从内存中访问的，这会导致代码的性能大大降低。
第二点不足就在对需要Caller保护的寄存器的处理上。
在上一节课后面的性能比拼中，我们发现，其实我们自己的语言编译生成的可执行程序，它的性能还略低于C语言生成的、同样未经优化的版本，按理说它们的性能应该是一样的才对。
深究原因，还是在调用函数的时候，程序需要保存那些需要Caller保护的寄存器。而我们的算法，多保护了一些其实已经不需要被保护的寄存器，从而拖累了性能。
不过，这两个方面的局限性，我们通过今天的算法，都可以很好地解决。我们现在就通过一个示例程序来找一下更好的寄存器分配算法的思路。
寄存器分配算法的改进思路你先看看我们下面这个示例程序：
function foo(p1:number,p2:number,p3:number,p4:number,p5:number,p6:number){ let x7 = p1; let x8 = p2; let x9 = p3; let x10 = p4; let x11 = p5; let x12 = p6 + x7 + x8 + x9 + x10 + x11; let sum = x12; for (let i:number = 0; i&amp;amp;lt; 10000; i++){ sum += i; } return sum; } 你看这里有p1~p6共6个参数，还有x7~x12这6个本地变量。但在变量x12的计算过程中，我们还需要用到1个临时变量t1。接下来是一个循环语句，这个语句又涉及到sum和i两个本地变量。</description></item><item><title>20_幻读是什么，幻读有什么问题？</title><link>https://artisanbox.github.io/1/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/20/</guid><description>在上一篇文章最后，我给你留了一个关于加锁规则的问题。今天，我们就从这个问题说起吧。
为了便于说明问题，这一篇文章，我们就先使用一个小一点儿的表。建表和初始化语句如下（为了便于本期的例子说明，我把上篇文章中用到的表结构做了点儿修改）：
CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 这个表除了主键id外，还有一个索引c，初始化语句在表中插入了6行数据。
上期我留给你的问题是，下面的语句序列，是怎么加锁的，加的锁又是什么时候释放的呢？
begin; select * from t where d=5 for update; commit; 比较好理解的是，这个语句会命中d=5的这一行，对应的主键id=5，因此在select 语句执行完成后，id=5这一行会加一个写锁，而且由于两阶段锁协议，这个写锁会在执行commit语句的时候释放。
由于字段d上没有索引，因此这条查询语句会做全表扫描。那么，其他被扫描到的，但是不满足条件的5行记录上，会不会被加锁呢？
我们知道，InnoDB的默认事务隔离级别是可重复读，所以本文接下来没有特殊说明的部分，都是设定在可重复读隔离级别下。
幻读是什么？现在，我们就来分析一下，如果只在id=5这一行加锁，而其他行的不加锁的话，会怎么样。
下面先来看一下这个场景（注意：这是我假设的一个场景）：
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;图 1 假设只在id=5这一行加行锁可以看到，session A里执行了三次查询，分别是Q1、Q2和Q3。它们的SQL语句相同，都是select * from t where d=5 for update。这个语句的意思你应该很清楚了，查所有d=5的行，而且使用的是当前读，并且加上写锁。现在，我们来看一下这三条SQL语句，分别会返回什么结果。
Q1只返回id=5这一行；
在T2时刻，session B把id=0这一行的d值改成了5，因此T3时刻Q2查出来的是id=0和id=5这两行；</description></item><item><title>20_散列表（下）：为什么散列表和链表经常会一起使用？</title><link>https://artisanbox.github.io/2/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/21/</guid><description>我们已经学习了20节内容，你有没有发现，有两种数据结构，散列表和链表，经常会被放在一起使用。你还记得，前面的章节中都有哪些地方讲到散列表和链表的组合使用吗？我带你一起回忆一下。
在链表那一节，我讲到如何用链表来实现LRU缓存淘汰算法，但是链表实现的LRU缓存淘汰算法的时间复杂度是O(n)，当时我也提到了，通过散列表可以将这个时间复杂度降低到O(1)。
在跳表那一节，我提到Redis的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表。当时我们也提到，Redis有序集合不仅使用了跳表，还用到了散列表。
除此之外，如果你熟悉Java编程语言，你会发现LinkedHashMap这样一个常用的容器，也用到了散列表和链表两种数据结构。
今天，我们就来看看，在这几个问题中，散列表和链表都是如何组合起来使用的，以及为什么散列表和链表会经常放到一块使用。
LRU缓存淘汰算法在链表那一节中，我提到，借助散列表，我们可以把LRU缓存淘汰算法的时间复杂度降低为O(1)。现在，我们就来看看它是如何做到的。
首先，我们来回顾一下当时我们是如何通过链表实现LRU缓存淘汰算法的。
我们需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。
当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的LRU缓存淘汰算法的时间复杂很高，是O(n)。
实际上，我总结一下，一个缓存（cache）系统主要包含下面这几个操作：
往缓存中添加一个数据；
从缓存中删除一个数据；
在缓存中查找一个数据。
这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是O(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到O(1)。具体的结构就是下面这个样子：
我们使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段hnext。这个hnext有什么作用呢？
因为我们的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext指针是为了将结点串在散列表的拉链中。
了解了这个散列表和双向链表的组合存储结构之后，我们再来看，前面讲到的缓存的三个操作，是如何做到时间复杂度是O(1)的？
首先，我们来看如何查找一个数据。我们前面讲过，散列表中查找数据的时间复杂度接近O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。
其次，我们来看如何删除一个数据。我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在O(1)时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针O(1)时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要O(1)的时间复杂度。
最后，我们来看如何添加一个数据。添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。
这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在O(1)的时间复杂度内完成。所以，这三个操作的时间复杂度都是O(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持LRU缓存淘汰算法的缓存系统原型。
Redis有序集合在跳表那一节，讲到有序集合的操作时，我稍微做了些简化。实际上，在有序集合中，每个成员对象有两个重要的属性，key（键值）和score（分值）。我们不仅会通过score来查找数据，还会通过key来查找数据。
举个例子，比如用户积分排行榜有这样一个功能：我们可以通过用户的ID来查找积分信息，也可以通过积分区间来查找用户ID或者姓名信息。这里包含ID、姓名和积分的用户信息，就是成员对象，用户ID就是key，积分就是score。
所以，如果我们细化一下Redis有序集合的操作，那就是下面这样：
添加一个成员对象；
按照键值来删除一个成员对象；
按照键值来查找一个成员对象；
按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象；
按照分值从小到大排序成员变量；
如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢，解决方法与LRU缓存淘汰算法的解决方法类似。我们可以再按照键值构建一个散列表，这样按照key来删除、查找一个成员对象的时间复杂度就变成了O(1)。同时，借助跳表结构，其他操作也非常高效。
实际上，Redis有序集合的操作还有另外一类，也就是查找成员对象的排名（Rank）或者根据排名区间查找成员对象。这个功能单纯用刚刚讲的这种组合结构就无法高效实现了。这块内容我后面的章节再讲。
Java LinkedHashMap前面我们讲了两个散列表和链表结合的例子，现在我们再来看另外一个，Java中的LinkedHashMap这种容器。
如果你熟悉Java，那你几乎天天会用到这个容器。我们之前讲过，HashMap底层是通过散列表这种数据结构实现的。而LinkedHashMap前面比HashMap多了一个“Linked”，这里的“Linked”是不是说，LinkedHashMap是一个通过链表法解决散列冲突的散列表呢？
实际上，LinkedHashMap并没有这么简单，其中的“Linked”也并不仅仅代表它是通过链表法解决散列冲突的。关于这一点，在我是初学者的时候，也误解了很久。
我们先来看一段代码。你觉得这段代码会以什么样的顺序打印3，1，5，2这几个key呢？原因又是什么呢？
HashMap&amp;lt;Integer, Integer&amp;gt; m = new LinkedHashMap&amp;lt;&amp;gt;(); m.put(3, 11); m.put(1, 12); m.put(5, 23); m.put(2, 22); for (Map.</description></item><item><title>20_面向流水线的指令设计（上）：一心多用的现代CPU</title><link>https://artisanbox.github.io/4/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/20/</guid><description>前面我们用了三讲，用一个个的电路组合，制作出了一个完整功能的CPU。这里面一下子给你引入了三个“周期”的概念，分别是指令周期、机器周期（或者CPU周期）以及时钟周期。
你可能会有点摸不着头脑了，为什么小小一个CPU，有那么多的周期（Cycle）呢？我们在专栏一开始，不是把CPU的性能定义得非常清楚了吗？我们说程序的性能，是由三个因素相乘来衡量的，我们还专门说过“指令数×CPI×时钟周期”这个公式。这里面和周期相关的只有一个时钟周期，也就是我们CPU的主频倒数。当时讲的时候我们说，一个CPU的时钟周期可以认为是可以完成一条最简单的计算机指令的时间。
那么，为什么我们在构造CPU的时候，一下子出来了那么多个周期呢？这一讲，我就来为你说道说道，带你更深入地看看现代CPU是怎么一回事儿。
愿得一心人，白首不相离：单指令周期处理器学过前面三讲，你现在应该知道，一条CPU指令的执行，是由“取得指令（Fetch）-指令译码（Decode）-执行指令（Execute） ”这样三个步骤组成的。这个执行过程，至少需要花费一个时钟周期。因为在取指令的时候，我们需要通过时钟周期的信号，来决定计数器的自增。
那么，很自然地，我们希望能确保让这样一整条指令的执行，在一个时钟周期内完成。这样，我们一个时钟周期可以执行一条指令，CPI也就是1，看起来就比执行一条指令需要多个时钟周期性能要好。采用这种设计思路的处理器，就叫作单指令周期处理器（Single Cycle Processor），也就是在一个时钟周期内，处理器正好能处理一条指令。
不过，我们的时钟周期是固定的，但是指令的电路复杂程度是不同的，所以实际一条指令执行的时间是不同的。在第13讲和第14讲讲加法器和乘法器电路的时候，我给你看过，随着门电路层数的增加，由于门延迟的存在，位数多、计算复杂的指令需要的执行时间会更长。
不同指令的执行时间不同，但是我们需要让所有指令都在一个时钟周期内完成，那就只好把时钟周期和执行时间最长的那个指令设成一样。这就好比学校体育课1000米考试，我们要给这场考试预留的时间，肯定得和跑得最慢的那个同学一样。因为就算其他同学先跑完，也要等最慢的同学跑完间，我们才能进行下一项活动。
快速执行完成的指令，需要等待满一个时钟周期，才能执行下一条指令所以，在单指令周期处理器里面，无论是执行一条用不到ALU的无条件跳转指令，还是一条计算起来电路特别复杂的浮点数乘法运算，我们都等要等满一个时钟周期。在这个情况下，虽然CPI能够保持在1，但是我们的时钟频率却没法太高。因为太高的话，有些复杂指令没有办法在一个时钟周期内运行完成。那么在下一个时钟周期到来，开始执行下一条指令的时候，前一条指令的执行结果可能还没有写入到寄存器里面。那下一条指令读取的数据就是不准确的，就会出现错误。
前一条指令的写入，在后一条指令的读取之前到这里你会发现，这和我们之前第3讲和第4讲讲时钟频率时候的说法不太一样。当时我们说，一个CPU时钟周期，可以认为是完成一条简单指令的时间。为什么到了这里，单指令周期处理器，反而变成了执行一条最复杂的指令的时间呢？
这是因为，无论是PC上使用的Intel CPU，还是手机上使用的ARM CPU，都不是单指令周期处理器，而是采用了一种叫作指令流水线（Instruction Pipeline）的技术。
无可奈何花落去，似曾相识燕归来：现代处理器的流水线设计其实，CPU执行一条指令的过程和我们开发软件功能的过程很像。
如果我们想开发一个手机App上的功能，并不是找来一个工程师，告诉他“你把这个功能开发出来”，然后他就吭哧吭哧把功能开发出来。真实的情况是，无论只有一个工程师，还是有一个开发团队，我们都需要先对开发功能的过程进行切分，把这个过程变成“撰写需求文档、开发后台API、开发客户端App、测试、发布上线”这样多个独立的过程。每一个后面的步骤，都要依赖前面的步骤。
我们的指令执行过程也是一样的，它会拆分成“取指令、译码、执行”这样三大步骤。更细分一点的话，执行的过程，其实还包含从寄存器或者内存中读取数据，通过ALU进行运算，把结果写回到寄存器或者内存中。
如果我们有一个开发团队，我们不会让后端工程师开发完API之后，就歇着等待前台App的开发、测试乃至发布，而是会在客户端App开发的同时，着手下一个需求的后端API开发。那么，同样的思路我们可以一样应用在CPU执行指令的过程中。
通过过去三讲，你应该已经知道了，CPU的指令执行过程，其实也是由各个电路模块组成的。我们在取指令的时候，需要一个译码器把数据从内存里面取出来，写入到寄存器中；在指令译码的时候，我们需要另外一个译码器，把指令解析成对应的控制信号、内存地址和数据；到了指令执行的时候，我们需要的则是一个完成计算工作的ALU。这些都是一个一个独立的组合逻辑电路，我们可以把它们看作一个团队里面的产品经理、后端工程师和客户端工程师，共同协作来完成任务。
流水线执行示意图这样一来，我们就不用把时钟周期设置成整条指令执行的时间，而是拆分成完成这样的一个一个小步骤需要的时间。同时，每一个阶段的电路在完成对应的任务之后，也不需要等待整个指令执行完成，而是可以直接执行下一条指令的对应阶段。
这就好像我们的后端程序员不需要等待功能上线，就会从产品经理手中拿到下一个需求，开始开发API。这样的协作模式，就是我们所说的指令流水线。这里面每一个独立的步骤，我们就称之为流水线阶段或者流水线级（Pipeline Stage）。
如果我们把一个指令拆分成“取指令-指令译码-执行指令”这样三个部分，那这就是一个三级的流水线。如果我们进一步把“执行指令”拆分成“ALU计算（指令执行）-内存访问-数据写回”，那么它就会变成一个五级的流水线。
五级的流水线，就表示我们在同一个时钟周期里面，同时运行五条指令的不同阶段。这个时候，虽然执行一条指令的时钟周期变成了5，但是我们可以把CPU的主频提得更高了。我们不需要确保最复杂的那条指令在时钟周期里面执行完成，而只要保障一个最复杂的流水线级的操作，在一个时钟周期内完成就好了。
如果某一个操作步骤的时间太长，我们就可以考虑把这个步骤，拆分成更多的步骤，让所有步骤需要执行的时间尽量都差不多长。这样，也就可以解决我们在单指令周期处理器中遇到的，性能瓶颈来自于最复杂的指令的问题。像我们现代的ARM或者Intel的CPU，流水线级数都已经到了14级。
虽然我们不能通过流水线，来减少单条指令执行的“延时”这个性能指标，但是，通过同时在执行多条指令的不同阶段，我们提升了CPU的“吞吐率”。在外部看来，我们的CPU好像是“一心多用”，在同一时间，同时执行5条不同指令的不同阶段。在CPU内部，其实它就像生产线一样，不同分工的组件不断处理上游传递下来的内容，而不需要等待单件商品生产完成之后，再启动下一件商品的生产过程。
超长流水线的性能瓶颈既然流水线可以增加我们的吞吐率，你可能要问了，为什么我们不把流水线级数做得更深呢？为什么不做成20级，乃至40级呢？这个其实有很多原因，我在之后几讲里面会详细讲解。这里，我先讲一个最基本的原因，就是增加流水线深度，其实是有性能成本的。
我们用来同步时钟周期的，不再是指令级别的，而是流水线阶段级别的。每一级流水线对应的输出，都要放到流水线寄存器（Pipeline Register）里面，然后在下一个时钟周期，交给下一个流水线级去处理。所以，每增加一级的流水线，就要多一级写入到流水线寄存器的操作。虽然流水线寄存器非常快，比如只有20皮秒（ps，$10^{-12}$秒）。
但是，如果我们不断加深流水线，这些操作占整个指令的执行时间的比例就会不断增加。最后，我们的性能瓶颈就会出现在这些overhead上。如果我们指令的执行有3纳秒，也就是3000皮秒。我们需要20级的流水线，那流水线寄存器的写入就需要花费400皮秒，占了超过10%。如果我们需要50级流水线，就要多花费1纳秒在流水线寄存器上，占到25%。这也就意味着，单纯地增加流水线级数，不仅不能提升性能，反而会有更多的overhead的开销。所以，设计合理的流水线级数也是现代CPU中非常重要的一点。
总结延伸讲到这里，相信你已经能够理解，为什么我们的CPU需要流水线设计了，也能把每一个流水线阶段在干什么，和上一讲的整个CPU的数据通路的连接过程对上了。
可以看到，为了能够不浪费CPU的性能，我们通过把指令的执行过程，切分成一个一个流水线级，来提升CPU的吞吐率。而我们本身的CPU的设计，又是由一个个独立的组合逻辑电路串接起来形成的，天然能够适合这样采用流水线“专业分工”的工作方式。
因为每一级的overhead，一味地增加流水线深度，并不能无限地提高性能。同样地，因为指令的执行不再是顺序地一条条执行，而是在上一条执行到一半的时候，下一条就已经启动了，所以也给我们的程序带来了很多挑战。这些挑战和对应的解决方案，就要请你坚持关注后面的几讲，我们一起来揭开答案了。
推荐阅读想要了解CPU的流水线设计，可以参看《深入理解计算机系统》的4.4章节，以及《计算机组成与设计 硬件/软件接口》的4.5章节。
课后思考我们在前面讲过，一个CPU的时钟周期，可以认为是完成一条简单指令的时间。在这一讲之后，你觉得这句话正确吗？为什么？在了解了CPU的流水线设计之后，你是怎么理解这句话的呢？
欢迎留言和我分享你的疑惑和见解。你也可以把今天的内容，分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>20｜怎么实现一个更好的寄存器分配算法：实现篇</title><link>https://artisanbox.github.io/3/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/22/</guid><description>你好，我是宫文学。
在上一节课，我们已经介绍了寄存器分配算法的原理。不过呢，我们这门课，不是停留在对原理的理解上就够了，还要把它具体实现出来才行。在实现的过程中，你会发现有不少实际的具体问题要去解决。而你一旦解决好了它们，你对寄存器分配相关原理的理解也会变得更加通透和深入。
所以，今天这一节课，我就会带你具体实现寄存器分配算法。在这个过程中，你会解决这些具体的技术问题：
首先，我们会了解如何基于我们现在的LIR来具体实现变量活跃性分析。特别是，当程序中存在多个基本块的时候，分析算法该如何设计。 第二，我们也会学习到在实现线性扫描算法中的一些技术点，包括如何分配寄存器、在调用函数时如何保存Caller需要保护的寄存器，以及如何正确的维护栈桢。 解决了这些问题之后，我们会对我们的语言再做一次性能测试，看看这次性能的提升有多大。那么接下来，就让我们先看看实现变量活跃性分析，需要考虑哪些技术细节吧。
实现变量活跃性分析我们先来总结一下，在实现变量活跃性分析的时候，我们会遇到哪几个技术点。我们一般要考虑如何保存变量活跃性分析的结果、如何表达变量的定义，以及如何基于CFG来做变量活跃性分析这三个方面。
现在我们就一一来分析一下。
首先，我们要设计一个数据结构，把活跃性分析的结果保存下来，方便我们后面在寄存器分配算法中使用。
这个数据结构很简单，我们使用一个Map即可。这个Map的key是指令，而value是一个数组，也就是执行当前指令时，活跃变量的集合。
liveVars:Map&amp;lt;Inst, number[]&amp;gt; = new Map(); 确定了数据结构以后，我们再讨论一下算法的实现。在算法的执行过程中呢，我们倒着扫描一条条指令。对于每条指令，我们要分析它的操作数。如果操作数是一个变量下标，那我们就把这个变量加到活跃变量的集合中。所以，往集合里加变量实现起来很简单。
可是，从集合里减变量就不那么简单了。为什么呢？根据我们上一节课讲过的算法，我们需要在变量声明的时候，把这个变量从集合里去掉。可是，我们当前的LIR中并没有记录哪个变量是在什么时候声明的，也就没办法知道变量的生存期是从什么时候开始的了。
那怎么来解决这个问题呢？我的办法是，向LIR里再加一条指令，这条指令专门用来指示变量的声明。我把这条指令的OpCode叫做declVar。
由于这条指令并不能转化成具体的可执行的指令，所以你可以把它叫做伪指令。它仅用于我们的寄存器分配算法。
好了，在加入了这条指令以后，我们就能对一个基本块进行变量活跃性分析了。具体实现你可以参考代码LivenessAnalyzer，其中的核心逻辑我放在下面了：
//为每一条指令计算活跃变量集合 for (let i = bb.insts.length - 1; i &amp;gt;=0; i--){ let inst = bb.insts[i]; if (inst.numOprands == 1){ let inst_1 = inst as Inst_1; //变量声明伪指令，从liveVars集合中去掉该变量 if (inst_1.op == OpCode.declVar){ let varIndex = inst_1.oprand.value as number; let indexInArray = vars.indexOf(varIndex); if (indexInArray != -1){ vars.splice(indexInArray,1); } } //查看指令中引用了哪个变量，就加到liveVars集合中去 else{ this.</description></item><item><title>21_为什么我只改一行的语句，锁这么多？</title><link>https://artisanbox.github.io/1/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/21/</guid><description>在上一篇文章中，我和你介绍了间隙锁和next-key lock的概念，但是并没有说明加锁规则。间隙锁的概念理解起来确实有点儿难，尤其在配合上行锁以后，很容易在判断是否会出现锁等待的问题上犯错。
所以今天，我们就先从这个加锁规则开始吧。
首先说明一下，这些加锁规则我没在别的地方看到过有类似的总结，以前我自己判断的时候都是想着代码里面的实现来脑补的。这次为了总结成不看代码的同学也能理解的规则，是我又重新刷了代码临时总结出来的。所以，这个规则有以下两条前提说明：
MySQL后面的版本可能会改变加锁策略，所以这个规则只限于截止到现在的最新版本，即5.x系列&amp;lt;=5.7.24，8.0系列 &amp;lt;=8.0.13。
如果大家在验证中有发现bad case的话，请提出来，我会再补充进这篇文章，使得一起学习本专栏的所有同学都能受益。
因为间隙锁在可重复读隔离级别下才有效，所以本篇文章接下来的描述，若没有特殊说明，默认是可重复读隔离级别。
我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。
原则1：加锁的基本单位是next-key lock。希望你还记得，next-key lock是前开后闭区间。
原则2：查找过程中访问到的对象才会加锁。
优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。
我还是以上篇文章的表t为例，和你解释一下这些规则。表t的建表语句和初始化语句如下。
CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 接下来的例子基本都是配合着图片说明的，所以我建议你可以对照着文稿看，有些例子可能会“毁三观”，也建议你读完文章后亲手实践一下。
案例一：等值查询间隙锁第一个例子是关于等值条件操作间隙：
图1 等值查询的间隙锁由于表t中没有id=7的记录，所以用我们上面提到的加锁规则判断一下的话：</description></item><item><title>21_哈希算法（上）：如何防止数据库中的用户信息被脱库？</title><link>https://artisanbox.github.io/2/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/22/</guid><description>还记得2011年CSDN的“脱库”事件吗？当时，CSDN网站被黑客攻击，超过600万用户的注册邮箱和密码明文被泄露，很多网友对CSDN明文保存用户密码行为产生了不满。如果你是CSDN的一名工程师，你会如何存储用户密码这么重要的数据吗？仅仅MD5加密一下存储就够了吗？ 要想搞清楚这个问题，就要先弄明白哈希算法。
哈希算法历史悠久，业界著名的哈希算法也有很多，比如MD5、SHA等。在我们平时的开发中，基本上都是拿现成的直接用。所以，我今天不会重点剖析哈希算法的原理，也不会教你如何设计一个哈希算法，而是从实战的角度告诉你，在实际的开发中，我们该如何用哈希算法解决问题。
什么是哈希算法？我们前面几节讲到“散列表”“散列函数”，这里又讲到“哈希算法”，你是不是有点一头雾水？实际上，不管是“散列”还是“哈希”，这都是中文翻译的差别，英文其实就是“Hash”。所以，我们常听到有人把“散列表”叫作“哈希表”“Hash表”，把“哈希算法”叫作“Hash算法”或者“散列算法”。那到底什么是哈希算法呢？
哈希算法的定义和原理非常简单，基本上一句话就可以概括了。将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。但是，要想设计一个优秀的哈希算法并不容易，根据我的经验，我总结了需要满足的几点要求：
从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
对输入数据非常敏感，哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同；
散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。
这些定义和要求都比较理论，可能还是不好理解，我拿MD5这种哈希算法来具体说明一下。
我们分别对“今天我来讲哈希算法”和“jiajia”这两个文本，计算MD5哈希值，得到两串看起来毫无规律的字符串（MD5的哈希值是128位的Bit长度，为了方便表示，我把它们转化成了16进制编码）。可以看出来，无论要哈希的文本有多长、多短，通过MD5哈希之后，得到的哈希值的长度都是相同的，而且得到的哈希值看起来像一堆随机数，完全没有规律。
MD5(&amp;quot;今天我来讲哈希算法&amp;quot;) = bb4767201ad42c74e650c1b6c03d78fa MD5(&amp;quot;jiajia&amp;quot;) = cd611a31ea969b908932d44d126d195b 我们再来看两个非常相似的文本，“我今天讲哈希算法！”和“我今天讲哈希算法”。这两个文本只有一个感叹号的区别。如果用MD5哈希算法分别计算它们的哈希值，你会发现，尽管只有一字之差，得到的哈希值也是完全不同的。
MD5(&amp;quot;我今天讲哈希算法！&amp;quot;) = 425f0d5a917188d2c3c3dc85b5e4f2cb MD5(&amp;quot;我今天讲哈希算法&amp;quot;) = a1fb91ac128e6aa37fe42c663971ac3d 我在前面也说了，通过哈希算法得到的哈希值，很难反向推导出原始数据。比如上面的例子中，我们就很难通过哈希值“a1fb91ac128e6aa37fe42c663971ac3d”反推出对应的文本“我今天讲哈希算法”。
哈希算法要处理的文本可能是各种各样的。比如，对于非常长的文本，如果哈希算法的计算时间很长，那就只能停留在理论研究的层面，很难应用到实际的软件开发中。比如，我们把今天这篇包含4000多个汉字的文章，用MD5计算哈希值，用不了1ms的时间。
哈希算法的应用非常非常多，我选了最常见的七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。这节我们先来看前四个应用。
应用一：安全加密说到哈希算法的应用，最先想到的应该就是安全加密。最常用于加密的哈希算法是MD5（MD5 Message-Digest Algorithm，MD5消息摘要算法）和SHA（Secure Hash Algorithm，安全散列算法）。
除了这两个之外，当然还有很多其他加密算法，比如DES（Data Encryption Standard，数据加密标准）、AES（Advanced Encryption Standard，高级加密标准）。
前面我讲到的哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。
第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。所以我着重讲一下第二点。实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说呢？
这里就基于组合数学中一个非常基础的理论，鸽巢原理（也叫抽屉原理）。这个原理本身很简单，它是说，如果有10个鸽巢，有11只鸽子，那肯定有1个鸽巢中的鸽子数量多于1个，换句话说就是，肯定有2只鸽子在1个鸽巢内。
有了鸽巢原理的铺垫之后，我们再来看，为什么哈希算法无法做到零冲突？
我们知道，哈希算法产生的哈希值的长度是固定且有限的。比如前面举的MD5的例子，哈希值是固定的128位二进制串，能表示的数据是有限的，最多能表示2^128个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对2^128+1个数据求哈希值，就必然会存在哈希值相同的情况。这里你应该能想到，一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。
2^128=340282366920938463463374607431768211456 为了让你能有个更加直观的感受，我找了两段字符串放在这里。这两段字符串经过MD5哈希算法加密之后，产生的哈希值是相同的。
不过，即便哈希算法存在散列冲突的情况，但是因为哈希值的范围很大，冲突的概率极低，所以相对来说还是很难破解的。像MD5，有2^128个不同的哈希值，这个数据已经是一个天文数字了，所以散列冲突的概率要小于1/2^128。
如果我们拿到一个MD5哈希值，希望通过毫无规律的穷举的方法，找到跟这个MD5值相同的另一个数据，那耗费的时间应该是个天文数字。所以，即便哈希算法存在冲突，但是在有限的时间和资源下，哈希算法还是很难被破解的。
除此之外，没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长。比如SHA-256比SHA-1要更复杂、更安全，相应的计算时间就会比较长。密码学界也一直致力于找到一种快速并且很难被破解的哈希算法。我们在实际的开发过程中，也需要权衡破解难度和计算时间，来决定究竟使用哪种加密算法。
应用二：唯一标识我先来举一个例子。如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那我们该如何搜索呢？
我们知道，任何文件在计算中都可以表示成二进制码串，所以，比较笨的办法就是，拿要查找的图片的二进制码串与图库中所有图片的二进制码串一一比对。如果相同，则说明图片在图库中存在。但是，每个图片小则几十KB、大则几MB，转化成二进制是一个非常长的串，比对起来非常耗时。有没有比较快的方法呢？
我们可以给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取100个字节，从中间取100个字节，从最后再取100个字节，然后将这300个字节放到一块，通过哈希算法（比如MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。
如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。
如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。
应用三：数据校验电驴这样的BT下载软件你肯定用过吧？我们知道，BT下载的原理是基于P2P协议的。我们从多个机器上并行下载一个2GB的电影，这个电影文件可能会被分割成很多文件块（比如可以分成100块，每块大约20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。
我们知道，网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？
具体的BT协议很复杂，校验方法也有很多，我来说其中的一种思路。
我们通过哈希算法，对100个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。
应用四：散列函数前面讲了很多哈希算法的应用，实际上，散列函数也是哈希算法的一种应用。
我们前两节讲到，散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。
不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。
解答开篇好了，有了前面的基础，现在你有没有发现开篇的问题其实很好解决？</description></item><item><title>21_面向流水线的指令设计（下）：奔腾4是怎么失败的？</title><link>https://artisanbox.github.io/4/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/21/</guid><description>上一讲，我给你初步介绍了CPU的流水线技术。乍看起来，流水线技术是一个提升性能的灵丹妙药。它通过把一条指令的操作切分成更细的多个步骤，可以避免CPU“浪费”。每一个细分的流水线步骤都很简单，所以我们的单个时钟周期的时间就可以设得更短。这也变相地让CPU的主频提升得很快。
这一系列的优点，也引出了现代桌面CPU的最后一场大战，也就是Intel的Pentium 4和AMD的Athlon之间的竞争。在技术上，这场大战Intel可以说输得非常彻底，Pentium 4系列以及后续Pentium D系列所使用的NetBurst架构被完全抛弃，退出了历史舞台。但是在商业层面，Intel却通过远超过AMD的财力、原本就更大的市场份额、无所不用的竞争手段，以及最终壮士断腕般放弃整个NetBurst架构，最终依靠新的酷睿品牌战胜了AMD。
在此之后，整个CPU领域竞争的焦点，不再是Intel和AMD之间的桌面CPU之战。在ARM架构通过智能手机的快速普及，后来居上，超越Intel之后，移动时代的CPU之战，变成了高通、华为麒麟和三星之间的“三国演义”。
“主频战争”带来的超长流水线我们在第3讲里讲过，我们其实并不能简单地通过CPU的主频，就来衡量CPU乃至计算机整机的性能。因为不同的CPU实际的体系架构和实现都不一样。同样的CPU主频，实际的性能可能差别很大。所以，在工业界，更好的衡量方式通常是，用SPEC这样的跑分程序，从多个不同的实际应用场景，来衡量计算机的性能。
但是，跑分对于消费者来说还是太复杂了。在Pentium 4的CPU面世之前，绝大部分消费者并不是根据跑分结果来判断CPU的性能的。大家判断一个CPU的性能，通常只看CPU的主频。而CPU的厂商们也通过不停地提升主频，把主频当成技术竞赛的核心指标。
Intel一向在“主频战争”中保持领先，但是到了世纪之交的1999年到2000年，情况发生了变化。
1999年，AMD发布了基于K7架构的Athlon处理器，其综合性能超越了当年的Pentium III。2000年，在大部分CPU还在500～850MHz的频率下运行的时候，AMD推出了第一代Athlon 1000处理器，成为第一款1GHz主频的消费级CPU。在2000年前后，AMD的CPU不但性能和主频比Intel的要强，价格还往往只有Intel的2/3。
在巨大的外部压力之下，Intel在2001年推出了新一代的NetBurst架构CPU，也就是Pentium 4和Pentium D。Pentium 4的CPU有个最大的特点，就是高主频。2000年的Athlon 1000的主频在当时是最高的，1GHz，然而Pentium 4设计的目标最高主频是10GHz。
为了达到这个10GHz，Intel的工程师做出了一个重大的错误决策，就是在NetBurst架构上，使用超长的流水线。这个超长流水线有多长呢？我们拿在Pentium 4之前和之后的CPU的数字做个比较，你就知道了。
Pentium 4之前的Pentium III CPU，流水线的深度是11级，也就是一条指令最多会拆分成11个更小的步骤来操作，而CPU同时也最多会执行11条指令的不同Stage。随着技术发展到今天，你日常用的手机ARM的CPU或者Intel i7服务器的CPU，流水线的深度是14级。
可以看到，差不多20年过去了，通过技术进步，现代CPU还是增加了一些流水线深度的。那2000年发布的Pentium 4的流水线深度是多少呢？答案是20级，比Pentium III差不多多了一倍，而到了代号为Prescott的90纳米工艺处理器Pentium 4，Intel更是把流水线深度增加到了31级。
要知道，增加流水线深度，在同主频下，其实是降低了CPU的性能。因为一个Pipeline Stage，就需要一个时钟周期。那么我们把任务拆分成31个阶段，就需要31个时钟周期才能完成一个任务；而把任务拆分成11个阶段，就只需要11个时钟周期就能完成任务。在这种情况下，31个Stage的3GHz主频的CPU，其实和11个Stage的1GHz主频的CPU，性能是差不多的。事实上，因为每个Stage都需要有对应的Pipeline寄存器的开销，这个时候，更深的流水线性能可能还会更差一些。
我在上一讲也说过，流水线技术并不能缩短单条指令的响应时间这个性能指标，但是可以增加在运行很多条指令时候的吞吐率。因为不同的指令，实际执行需要的时间是不同的。我们可以看这样一个例子。我们顺序执行这样三条指令。
一条整数的加法，需要200ps。 一条整数的乘法，需要300ps。 一条浮点数的乘法，需要600ps。 如果我们是在单指令周期的CPU上运行，最复杂的指令是一条浮点数乘法，那就需要600ps。那这三条指令，都需要600ps。三条指令的执行时间，就需要1800ps。
如果我们采用的是6级流水线CPU，每一个Pipeline的Stage都只需要100ps。那么，在这三个指令的执行过程中，在指令1的第一个100ps的Stage结束之后，第二条指令就开始执行了。在第二条指令的第一个100ps的Stage结束之后，第三条指令就开始执行了。这种情况下，这三条指令顺序执行所需要的总时间，就是800ps。那么在1800ps内，使用流水线的CPU比单指令周期的CPU就可以多执行一倍以上的指令数。
虽然每一条指令从开始到结束拿到结果的时间并没有变化，也就是响应时间没有变化。但是同样时间内，完成的指令数增多了，也就是吞吐率上升了。
新的挑战：冒险和分支预测那到这里可能你就要问了，这样看起来不是很好么？Intel的CPU支持的指令集很大，我们之前说过有2000多条指令。有些指令很简单，执行也很快，比如无条件跳转指令，不需要通过ALU进行任何计算，只要更新一下PC寄存器里面的内容就好了。而有些指令很复杂，比如浮点数的运算，需要进行指数位比较、对齐，然后对有效位进行移位，然后再进行计算。两者的执行时间相差二三十倍也很正常。
既然这样，Pentium 4的超长流水线看起来很合理呀，为什么Pentium 4最终成为Intel在技术架构层面的大失败呢？
第一个，自然是我们在第3讲里讲过的功耗问题。提升流水线深度，必须要和提升CPU主频同时进行。因为在单个Pipeline Stage能够执行的功能变简单了，也就意味着单个时钟周期内能够完成的事情变少了。所以，只有提升时钟周期，CPU在指令的响应时间这个指标上才能保持和原来相同的性能。
同时，由于流水线深度的增加，我们需要的电路数量变多了，也就是我们所使用的晶体管也就变多了。
主频的提升和晶体管数量的增加都使得我们CPU的功耗变大了。这个问题导致了Pentium 4在整个生命周期里，都成为了耗电和散热的大户。而Pentium 4是在2000～2004年作为Intel的主打CPU出现在市场上的。这个时间段，正是笔记本电脑市场快速发展的时间。在笔记本电脑上，功耗和散热比起台式机是一个更严重的问题了。即使性能更好，别人的笔记本可以用上2小时，你的只能用30分钟，那谁也不爱买啊！
更何况，Pentium 4的性能还更差一些。这个就要我们说到第二点了，就是上面说的流水线技术带来的性能提升，是一个理想情况。在实际的程序执行中，并不一定能够做得到。
还回到我们刚才举的三条指令的例子。如果这三条指令，是下面这样的三条代码，会发生什么情况呢？
int a = 10 + 5; // 指令1 int b = a * 2; // 指令2 float c = b * 1.</description></item><item><title>21｜加深对栈的理解：实现尾递归和尾调用优化</title><link>https://artisanbox.github.io/3/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/23/</guid><description>你好，我是宫文学。
前面几节课，我们在实现生成本地代码的过程中，对汇编语言、栈和栈桢有关的知识点都进行了比较深入的了解。通过这些学习，你应该对程序的运行机制有了更加透彻的理解。
那么今天这节课，作为第一部分起步篇的结尾，我们就来检验一下自己的学习成果吧！具体一点，我们就是要运用我们前面已经学过的知识点，特别是关于栈和栈桢的知识点，来实现两个有用的优化功能，也就是尾递归和尾调用的优化。
这两个优化有助于我们更好地利用栈里的内存空间，也能够提高程序的性能，对于我们后面实现函数式编程特性也具有很重要的意义。另外，这样的练习，也会加深我们对栈和栈桢、对程序的控制流，还有对程序的运行机制的理解。
好了，我们先从尾递归入手吧，说说尾递归是怎么回事，还有它有怎样的运行特点，看看我们为什么需要去优化它。
递归函数和尾递归学习编程的同学都应该知道，递归是一种重要的思维方式。我们现实世界的很多事物，用递归来表达是非常自然的。在递归的思维里，解决整体的问题和解决局部问题的思路是相同的。
在我们这个课程里，我们学习的语法分析的方法，也采用了递归的思维：我们给一个大程序做语法分析，会分解成给每一个函数、每一条语句做语法分析。不管在哪个颗粒度上，算法的思路都是相同的。
递归思想的一个更具体的使用方式，就是递归函数。当前的各种高级语言，都会支持递归函数，也就是允许在一个函数内部调用自身。
你可以看看下面这个例子，这个例子是用来实现阶乘的计算的。
function factorial (n:number):number{ if (n &amp;lt; 1) return 1; else return n * factorial(n-1); } 在这里，n的阶乘f(n)，就等于n * f(n-1)。这是典型的递归思维，解决一个整体问题f(n)，能够被转化为解决其局部问题f(n-1)。n的值变化了，但解决问题的思路是一致的。
最近几年，函数式编程的思想又重新流行起来。在一些纯函数式的编程语言中，递归是其核心编程机制，被大量使用。
不过，递归函数的大量使用，对程序的运行时机制是一个挑战。因为我们已经知道，在标准的程序运行模式下，每一次函数调用，都要为这个函数创建一个栈桢。如果递归的层次很深，那么栈桢的数量就会非常多，最终引起“stack overflow”，也就是栈溢出的错误，这是我们在使用栈的时候最怕遇到的问题。
另外，我们还知道，当我们在进行函数调用的时候，还会产生比较大的性能开销。这些开销包括：设置参数、设置返回地址、移动栈顶指针、保护相关的寄存器，等等。特别是，在这个过程中，一般都会产生内存读写的动作，这会对性能产生比较大的影响。
所以说，虽然递归函数很有用，但你在学习编程的时候，可能你的老师会告诉你，如果对性能和内存占用有较高的要求，那么我们尽量不用递归算法实现，而是把递归算法改成等价的非递归算法。
不过，现代编译器也在努力帮助解决这个问题。比如在上一节课中，我们就已经见到了C语言编译器的一个功能，它在编译斐波那契数列的过程中，能够把其中一半的递归调用转变成一个循环语句，从而减少了递归调用导致的开销。
但在这一节课呢，我们不会试图一下子就实现这么复杂的编译优化功能，而是先针对递归调用中的一个特殊情况而进行优化，这个特殊情况就是尾递归。
那什么是尾递归呢？尾递归就是在return语句中，return后面只跟了一个递归调用的情况。在上面的例子中，你会看到return后面跟着的是n * factorial(n-1)，这种情况不是尾递归。不过，我们可以把示例程序改写成尾递归的情形，我写在了下面：
function factorial(n:number, total:number):number{ if (n &amp;lt;= 1) return total; else return factorial(n-1, n*total); } 这个新的阶乘函数使用了两个参数，其中第二个参数保存的是阶乘的累积值。如果要计算10的阶乘，那么我们需要函数factorial(10, 1)。你可以仔细看一下factorial函数的两个不同的版本，它们确实是等价的。但第二个版本中的第二个return语句呢，就是一个标准的尾递归调用。
我们为什么要谈论尾递归呢？这是因为尾递归在栈桢的使用上有其独特的特点，使得我们可以用很简单的方法就能实现优化。
那么接下来，我们就分析一下递归函数在栈的使用上的特点，这有利于我们制定优化策略。
递归函数对栈的使用你可以用我们上一节课的PlayScript版本，使用make example_fact命令来生成上面示例程序的汇编代码和可执行文件。
这个汇编文件是没有做尾递归优化的，你可以看一下它的内容，看看它的栈桢是什么结构。
_factorial: .cfi_startproc ## bb.0 pushq %rbp movq %rsp, %rbp cmpl $1, %edi # cmpl $1, var0 jg LBB0_2 ## bb.</description></item><item><title>22_MySQL有哪些“饮鸩止渴”提高性能的方法？</title><link>https://artisanbox.github.io/1/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/22/</guid><description>不知道你在实际运维过程中有没有碰到这样的情景：业务高峰期，生产环境的MySQL压力太大，没法正常响应，需要短期内、临时性地提升一些性能。
我以前做业务护航的时候，就偶尔会碰上这种场景。用户的开发负责人说，不管你用什么方案，让业务先跑起来再说。
但，如果是无损方案的话，肯定不需要等到这个时候才上场。今天我们就来聊聊这些临时方案，并着重说一说它们可能存在的风险。
短连接风暴正常的短连接模式就是连接到数据库后，执行很少的SQL语句就断开，下次需要的时候再重连。如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。
我在第1篇文章《基础架构：一条SQL查询语句是如何执行的？》中说过，MySQL建立连接的过程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的数据读写权限。
在数据库压力比较小的时候，这些额外的成本并不明显。
但是，短连接模型存在一个风险，就是一旦数据库处理得慢一些，连接数就会暴涨。max_connections参数，用来控制一个MySQL实例同时存在的连接数的上限，超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。对于被拒绝连接的请求来说，从业务角度看就是数据库不可用。
在机器负载比较高的时候，处理现有请求的时间变长，每个连接保持的时间也更长。这时，再有新建连接的话，就可能会超过max_connections的限制。
碰到这种情况时，一个比较自然的想法，就是调高max_connections的值。但这样做是有风险的。因为设计max_connections这个参数的目的是想保护MySQL，如果我们把它改得太大，让更多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑上，结果可能是适得其反，已经连接的线程拿不到CPU资源去执行业务的SQL请求。
那么这种情况下，你还有没有别的建议呢？我这里还有两种方法，但要注意，这些方法都是有损的。
第一种方法：先处理掉那些占着连接但是不工作的线程。
max_connections的计算，不是看谁在running，是只要连着就占用一个计数位置。对于那些不需要保持的连接，我们可以通过kill connection主动踢掉。这个行为跟事先设置wait_timeout的效果是一样的。设置wait_timeout参数表示的是，一个线程空闲wait_timeout这么多秒之后，就会被MySQL直接断开连接。
但是需要注意，在show processlist的结果里，踢掉显示为sleep的线程，可能是有损的。我们来看下面这个例子。
图1 sleep线程的两种状态在上面这个例子里，如果断开session A的连接，因为这时候session A还没有提交，所以MySQL只能按照回滚事务来处理；而断开session B的连接，就没什么大影响。所以，如果按照优先级来说，你应该优先断开像session B这样的事务外空闲的连接。
但是，怎么判断哪些是事务外空闲的呢？session C在T时刻之后的30秒执行show processlist，看到的结果是这样的。
图2 sleep线程的两种状态，show processlist结果图中id=4和id=5的两个会话都是Sleep 状态。而要看事务具体状态的话，你可以查information_schema库的innodb_trx表。
图3 从information_schema.innodb_trx查询事务状态这个结果里，trx_mysql_thread_id=4，表示id=4的线程还处在事务中。
因此，如果是连接数过多，你可以优先断开事务外空闲太久的连接；如果这样还不够，再考虑断开事务内空闲太久的连接。
从服务端断开连接使用的是kill connection + id的命令， 一个客户端处于sleep状态时，它的连接被服务端主动断开后，这个客户端并不会马上知道。直到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。
从数据库端主动断开连接可能是有损的，尤其是有的应用端收到这个错误后，不重新连接，而是直接用这个已经不能用的句柄重试查询。这会导致从应用端看上去，“MySQL一直没恢复”。
你可能觉得这是一个冷笑话，但实际上我碰到过不下10次。
所以，如果你是一个支持业务的DBA，不要假设所有的应用代码都会被正确地处理。即使只是一个断开连接的操作，也要确保通知到业务开发团队。
第二种方法：减少连接过程的消耗。
有的业务代码会在短时间内先大量申请数据库连接做备用，如果现在数据库确认是被连接行为打挂了，那么一种可能的做法，是让数据库跳过权限验证阶段。
跳过权限验证的方法是：重启数据库，并使用–skip-grant-tables参数启动。这样，整个MySQL会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。
但是，这种方法特别符合我们标题里说的“饮鸩止渴”，风险极高，是我特别不建议使用的方案。尤其你的库外网可访问的话，就更不能这么做了。
在MySQL 8.0版本里，如果你启用–skip-grant-tables参数，MySQL会默认把 --skip-networking参数打开，表示这时候数据库只能被本地的客户端连接。可见，MySQL官方对skip-grant-tables这个参数的安全问题也很重视。
除了短连接数暴增可能会带来性能问题外，实际上，我们在线上碰到更多的是查询或者更新语句导致的性能问题。其中，查询问题比较典型的有两类，一类是由新出现的慢查询导致的，一类是由QPS（每秒查询数）突增导致的。而关于更新语句导致的性能问题，我会在下一篇文章和你展开说明。
慢查询性能问题在MySQL中，会引发性能问题的慢查询，大体有以下三种可能：
索引没有设计好；
SQL语句没写好；
MySQL选错了索引。
接下来，我们就具体分析一下这三种可能，以及对应的解决方案。</description></item><item><title>22_冒险和预测（一）：hazard是“危”也是“机”</title><link>https://artisanbox.github.io/4/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/22/</guid><description>过去两讲，我为你讲解了流水线设计CPU所需要的基本概念。接下来，我们一起来看看，要想通过流水线设计来提升CPU的吞吐率，我们需要冒哪些风险。
任何一本讲解CPU的流水线设计的教科书，都会提到流水线设计需要解决的三大冒险，分别是结构冒险（Structural Hazard）、数据冒险（Data Hazard）以及控制冒险（Control Hazard）。
这三大冒险的名字很有意思，它们都叫作hazard（冒险）。喜欢玩游戏的话，你应该知道一个著名的游戏，生化危机，英文名就叫Biohazard。的确，hazard还有一个意思就是“危机”。那为什么在流水线设计里，hazard没有翻译成“危机”，而是要叫“冒险”呢？
在CPU的流水线设计里，固然我们会遇到各种“危险”情况，使得流水线里的下一条指令不能正常运行。但是，我们其实还是通过“抢跑”的方式，“冒险”拿到了一个提升指令吞吐率的机会。流水线架构的CPU，是我们主动进行的冒险选择。我们期望能够通过冒险带来更高的回报，所以，这不是无奈之下的应对之举，自然也算不上什么危机了。
事实上，对于各种冒险可能造成的问题，我们其实都准备好了应对的方案。这一讲里，我们先从结构冒险和数据冒险说起，一起来看看这些冒险及其对应的应对方案。
结构冒险：为什么工程师都喜欢用机械键盘？我们先来看一看结构冒险。结构冒险，本质上是一个硬件层面的资源竞争问题，也就是一个硬件电路层面的问题。
CPU在同一个时钟周期，同时在运行两条计算机指令的不同阶段。但是这两个不同的阶段，可能会用到同样的硬件电路。
最典型的例子就是内存的数据访问。请你看看下面这张示意图，其实就是第20讲里对应的5级流水线的示意图。
可以看到，在第1条指令执行到访存（MEM）阶段的时候，流水线里的第4条指令，在执行取指令（Fetch）的操作。访存和取指令，都要进行内存数据的读取。我们的内存，只有一个地址译码器的作为地址输入，那就只能在一个时钟周期里面读取一条数据，没办法同时执行第1条指令的读取内存数据和第4条指令的读取指令代码。
同一个时钟周期，两个不同指令访问同一个资源类似的资源冲突，其实你在日常使用计算机的时候也会遇到。最常见的就是薄膜键盘的“锁键”问题。常用的最廉价的薄膜键盘，并不是每一个按键的背后都有一根独立的线路，而是多个键共用一个线路。如果我们在同一时间，按下两个共用一个线路的按键，这两个按键的信号就没办法都传输出去。
这也是为什么，重度键盘用户，都要买贵一点儿的机械键盘或者电容键盘。因为这些键盘的每个按键都有独立的传输线路，可以做到“全键无冲”，这样，无论你是要大量写文章、写程序，还是打游戏，都不会遇到按下了键却没生效的情况。
“全键无冲”这样的资源冲突解决方案，其实本质就是增加资源。同样的方案，我们一样可以用在CPU的结构冒险里面。对于访问内存数据和取指令的冲突，一个直观的解决方案就是把我们的内存分成两部分，让它们各有各的地址译码器。这两部分分别是存放指令的程序内存和存放数据的数据内存。
这样把内存拆成两部分的解决方案，在计算机体系结构里叫作哈佛架构（Harvard Architecture），来自哈佛大学设计Mark I型计算机时候的设计。对应的，我们之前说的冯·诺依曼体系结构，又叫作普林斯顿架构（Princeton Architecture）。从这些名字里，我们可以看到，早年的计算机体系结构的设计，其实产生于美国各个高校之间的竞争中。
不过，我们今天使用的CPU，仍然是冯·诺依曼体系结构的，并没有把内存拆成程序内存和数据内存这两部分。因为如果那样拆的话，对程序指令和数据需要的内存空间，我们就没有办法根据实际的应用去动态分配了。虽然解决了资源冲突的问题，但是也失去了灵活性。
现代CPU架构，借鉴了哈佛架构，在高速缓存层面拆分成指令缓存和数据缓存不过，借鉴了哈佛结构的思路，现代的CPU虽然没有在内存层面进行对应的拆分，却在CPU内部的高速缓存部分进行了区分，把高速缓存分成了指令缓存（Instruction Cache）和数据缓存（Data Cache）两部分。
内存的访问速度远比CPU的速度要慢，所以现代的CPU并不会直接读取主内存。它会从主内存把指令和数据加载到高速缓存中，这样后续的访问都是访问高速缓存。而指令缓存和数据缓存的拆分，使得我们的CPU在进行数据访问和取指令的时候，不会再发生资源冲突的问题了。
数据冒险：三种不同的依赖关系结构冒险是一个硬件层面的问题，我们可以靠增加硬件资源的方式来解决。然而还有很多冒险问题，是程序逻辑层面的事儿。其中，最常见的就是数据冒险。
数据冒险，其实就是同时在执行的多个指令之间，有数据依赖的情况。这些数据依赖，我们可以分成三大类，分别是先写后读（Read After Write，RAW）、先读后写（Write After Read，WAR）和写后再写（Write After Write，WAW）。下面，我们分别看一下这几种情况。
先写后读（Read After Write）我们先来一起看看先写后读这种情况。这里有一段简单的C语言代码编译出来的汇编指令。这段代码简单地定义两个变量 a 和 b，然后计算 a = a + 2。再根据计算出来的结果，计算 b = a + 3。
int main() { int a = 1; int b = 2; a = a + 2; b = a + 3; } int main() { 0: 55 push rbp 1: 48 89 e5 mov rbp,rsp int a = 1; 4: c7 45 fc 01 00 00 00 mov DWORD PTR [rbp-0x4],0x1 int b = 2; b: c7 45 f8 02 00 00 00 mov DWORD PTR [rbp-0x8],0x2 a = a + 2; 12: 83 45 fc 02 add DWORD PTR [rbp-0x4],0x2 b = a + 3; 16: 8b 45 fc mov eax,DWORD PTR [rbp-0x4] 19: 83 c0 03 add eax,0x3 1c: 89 45 f8 mov DWORD PTR [rbp-0x8],eax } 1f: 5d pop rbp 20: c3 ret 你可以看到，在内存地址为12的机器码，我们把0x2添加到 rbp-0x4 对应的内存地址里面。然后，在紧接着的内存地址为16的机器码，我们又要从rbp-0x4这个内存地址里面，把数据写入到eax这个寄存器里面。</description></item><item><title>22_哈希算法（下）：哈希算法在分布式系统中有哪些应用？</title><link>https://artisanbox.github.io/2/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/23/</guid><description>上一节，我讲了哈希算法的四个应用，它们分别是：安全加密、数据校验、唯一标识、散列函数。今天，我们再来看剩余三种应用：负载均衡、数据分片、分布式存储。
你可能已经发现，这三个应用都跟分布式系统有关。没错，今天我就带你看下，哈希算法是如何解决这些分布式问题的。
应用五：负载均衡我们知道，负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。
最直接的方法就是，维护一张映射关系表，这张表的内容是客户端IP地址或者会话ID与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：
如果客户端很多，映射表可能会很大，比较浪费内存空间；
客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；
如果借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端IP地址或者会话ID计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个IP过来的所有请求，都路由到同一个后端服务器上。
应用六：数据分片哈希算法还可以用于数据的分片。我这里有两个例子。
1.如何统计“搜索关键词”出现的次数？假如我们有1T的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？
我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。
针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用n台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号。
这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。
实际上，这里的处理过程也是MapReduce的基本设计思想。
2.如何快速判断图片是否在图库中？如何快速判断图片是否在图库中？上一节我们讲过这个例子，不知道你还记得吗？当时我介绍了一种方法，即给每个图片取唯一标识（或者信息摘要），然后构建散列表。
假设现在我们的图库中有1亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而1亿张图片构建散列表显然远远超过了单台机器的内存上限。
我们同样可以对数据进行分片，然后采用多机处理。我们准备n台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数n求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。
当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数n求余取模。假设得到的值是k，那就去编号k的机器构建的散列表中查找。
现在，我们来估算一下，给这1亿张图片构建散列表大约需要多少台机器。
散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设我们通过MD5来计算哈希值，那长度就是128比特，也就是16字节。文件路径长度的上限是256字节，我们可以假设平均长度是128字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用8字节。所以，散列表中每个数据单元就占用152字节（这里只是估算，并不准确）。
假设一台机器的内存大小为2GB，散列表的装载因子为0.75，那一台机器可以给大约1000万（2GB*0.75/152）张图片构建散列表。所以，如果要对1亿张图片构建索引，需要大约十几台机器。在工程中，这种估算还是很重要的，能让我们事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性。
实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU等资源的限制。
应用七：分布式存储现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。
该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。
但是，如果数据增多，原来的10个机器已经无法承受了，我们就需要扩容了，比如扩到11个机器，这时候麻烦就来了。因为，这里并不是简单地加个机器就可以了。
原来的数据是通过与10来取模的。比如13这个数据，存储在编号为3这台机器上。但是新加了一台机器中，我们对数据按照11取模，原来13这个数据就被分配到2号这台机器上了。
因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。
所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了。
假设我们有k个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成m个小区间（m远大于k），每个机器负责m/k个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。
一致性哈希算法的基本思想就是这么简单。除此之外，它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来。这里我就不展开讲了，如果感兴趣，你可以看下这个介绍。
除了我们上面讲到的分布式缓存，实际上，一致性哈希算法的应用非常广泛，在很多分布式存储系统中，都可以见到一致性哈希算法的影子。
解答开篇&amp;amp;内容小结这两节的内容理论不多，比较贴近具体的开发。今天我讲了三种哈希算法在分布式系统中的应用，它们分别是：负载均衡、数据分片、分布式存储。
在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。
课后思考这两节我总共讲了七个哈希算法的应用。实际上，我讲的也只是冰山一角，哈希算法还有很多其他的应用，比如网络协议中的CRC校验、Git commit id等等。除了这些，你还能想到其他用到哈希算法的地方吗？
欢迎留言和我分享，我会第一时间给你反馈。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>22｜增强编译器前端功能第1步：再识数据流分析技术</title><link>https://artisanbox.github.io/3/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/24/</guid><description>你好，我是宫文学。
到目前为止，实现一门计算机语言的流程，我已经带你完整地走了一遍：从编译器前端的技术，到AST解释器，再到字节码虚拟机，最后生成了汇编代码并编译成了可执行文件。在这个过程中，我们领略了沿途的风光，初步了解了实现一门计算机语言的各种关键技术。
可是，我们在第一部分起步篇里，都是顾着奋力攀爬，去开出一条路来。可是这条路实在有点窄，是条羊肠小道，现在我们就需要把它拓宽一下。也就是把我们PlayScript语言的特性增强一下，拓宽我们的知识面。
这个拓宽的方式呢，我选择的是围绕数据类型这条主线进行。这是因为，现代计算机语言的很多特性，都是借助类型体系来呈现的。
不知道你注意到没有，到目前为止，除了最早的AST解释器以外，我们的后几个运行机制都只支持整型。所以，我们要在第二部分进阶篇中让PlayScript支持浮点型、字符串和自定义对象等数据类型。做这些工作的目的，不仅仅是增加我们语言支持的数据类型，而且，随着你对字符串类型、和自定义对象等类型的学习，你也会对对象的处理能力，包括对象的属性、方法、对象内存的自动管理等知识有更深刻的理解。
为了降低工作量，我后面的课程主要实现的是实现静态编译的版本。因为这种运行机制涉及的知识点比较广，并且我的目标之一就是要实现一个高效的、静态编译的TypeScript版本，到这里我的目标也算达到了。如果你有兴趣，你也可以把字节码虚拟机版本扩展一下，用于支持对象特性等高级特性。
不过，在我们开启第二段征程之时，我们需要回到编译器的前端部分，把词法分析、语法分析和语义分析等功能都增强一下，以便支持后面的语法特性。在这个过程中，你会学习到前端的数据流分析技术、前端的优化技术和类型计算方面的知识点，让你的编译器前端的知识得到迭代提升。
那么，首先我们就来看看，我们语言在编译器前端功能方面的现状，找找增强这些功能的办法。
编译器前端功能上的现状编译器的前端，也就是词法分析、语法分析和语义分析功能。我们逐个回顾一下，看看我们现在做到怎么样了。
首先来看我们的词法分析功能，跟语法分析和语义分析功能相比，它应该算是最完善的了。为什么这么说呢？因为我们目前的语言特性已经涉及到了大部分种类的Token，这些词法分析器都能提供。但相对来说，我们支持的语法规则还比较有限，所需要的语义分析功能也有很多缺失。不过，词法分析仍然有一些功能不够充分，比如数字字面量和字符串字面量等。
在数字字面量这边，我们目前虽然已经支持比较简单的整数和浮点数的字面量。但还有二进制、八进制和十六进制的数字、科学计数法表达的数字，我们都还没去支持。而且，字符串字面量上，我们也不支持Unicode和转义字符，并且字符串只能用双引号，还没使用单引号的版本。
接着，我们再来看语法分析功能。我们目前使用的都是一些比较高频的语法规则，忽略了一些比较低频的语法。比如，目前函数的参数只支持固定的参数数量，不支持变动数量的参数，也不支持参数的缺省值。再比如，我们目前的循环语句只支持for循环，并且不支持对集合的枚举，等等。
所以说，我们的词法分析和语法分析都还有不少功课需要去补呢。不过，我目前并不着急补这两方面。我的计划是，随着课程的推进，每当我们需要增加新特性的时候，就扩展一下这方面所需的词法和语法分析功能就好了。这样能够让你见证到像计算机语言这样的高难度软件一步步迭代成熟的过程，增强你自己驾驭类似的软件的信心。
既然词法和语法分析功能都不是我们这节课的重点，那语义分析功能自然就是重点了。
这是因为，实际上，在我们实现编译器的前端功能的时候，语义分析的工作量是最大的，但我们目前实现的功能确实有限。如果你有兴趣，可以参考我在《编译原理实战课》中对Java前端编译器的分析。在把编译工作分成的多个阶段中，大部分阶段都是去做语义分析相关的工作。
那我们现在的语义分析功能做到哪一步了呢？
在前面的课程中，我们已经实现了一些必要的语义分析功能，比如建立符号表、进行引用消解、分析哪个表达式是左值，以及进行简单的类型检查等等。不过这些功能其实还远远不够，因为还有很多潜在的语义错误没有被检查出来，因此需要我们逐步把这些工作补上。
在这个过程中，你会学习如何把数据流分析技术、类型计算技术用于语义分析工作。今天这节课，我们就先主要聚焦在数据流分析技术上。接下来，我们就举几个典型的场景，来学习如何在语义分析中使用数据流分析技术。
场景一：代码活跃性分析之程序是否return了？我们在写函数的时候，如果这个函数需要返回值，那么在编译时，编译器会检查一下，是不是你所有的程序分支都以return语句结尾了。如果没有，编译器就会报错。我们举个例子：
function foo(a:number):number{ if (a &amp;gt; 10){ let b:number = a+5; return b; b = a + 10; //这段代码不可到达。 } } 你可以看一下，这段代码有什么问题呢？
首先，你会发现，这段代码里只有在if语句块有return语句。所以，当不满足if条件的时候，程序的执行流程就不会遇到这个return语句。那根据TypeScript的语义，此时的返回值是undefined。而函数的返回值类型里呢，又不包含undefined。所以这时，如果你用“tsc --strict example_return.ts”命令去编译它，tsc会报下面的错误：
当然，如果函数的前面是下面的样子，在返回值里包含undefined，那就是正确的。
function foo(a:number):number|undefined 好，这是我们从示例代码中发现的第一个问题。那么第二个问题是什么呢？
你会看到，在return语句的下面还有一行代码“b = a + 10”，这一行代码其实是永远也不会被执行的。当然，这并不是一个错误，用tsc来编译也不会报错。但是，编译器或IDE工具最好要能够检查出这些问题，给程序员以提示。在编译生成代码的时候，编译器也可以直接把这些代码优化掉。
那如何检查出上面这些语义问题呢？那又需要用到数据流分析技术了。
到目前为止，我们已经多次接触到数据流分析技术了。在进行变量引用分析的时候，我们就曾实现过一个功能，检查出“变量是否在声明前就被引用”的错误。
它的处理逻辑是：语义分析程序遍历整个AST，相当于自上而下地分析每一条代码。当程序遇到变量声明节点的时候，就会标记该变量已经被声明了。而当程序遇到变量引用节点时，如果它发现该变量虽然属于某个作用域，但它当前还没有被声明，那么它就会报语义分析错误。具体的实现，你可以参考RefResolver类中的代码。
另外，在实现寄存器分配算法时，我们也曾经使用过数据流分析技术，来计算每个变量的生存期，从而确定多个变量如何共享寄存器。在那个时候，我们是在CFG上进行数据流分析的，并且分析方向是自下而上的顺序。
针对我们前面实操过的这两个例子，你可以总结出来数据流分析的几个特点：
首先，数据流分析技术可以用在像AST和CFG等多种数据结构上，未来你还会见到我们把它用到其他的数据结构上；
第二，针对不同的分析任务，数据流分析方向是不同的，有的是自上而下，有的是自下而上，你需要确定清楚；
第三，数据流分析的过程，都会针对一个分析变量，并会不断改变这个变量的值。分析变量可能是一个单个的值，或者叫做标量，也可能是一组数值，比如向量和集合。在我们的前面的两个例子中，这个变量都是集合。第一个例子的分析变量是“已声明的变量的集合”，第二个例子的分析变量是“活跃变量集合”。
第四，我们需要有一个规则或函数，基于这个规则来处理每行代码，从而计算新的变量值。比如，在变量活跃性分析中，这个规则是只要遇到变量使用的语句，就往集合里添加该变量，遇到变量声明的语句，就从集合中去掉该变量。
第五，要确定变量的初始值。在第一个例子中，初始值是一个空集。在第二个例子中，每个基本块可能会有一个活跃变量的初始值，这些初始值是由CFG中的其他基本块决定的。
还有最后一个共性，它们都有交汇函数。交汇函数是用来在多个控制流交汇的时候，计算出交汇的值。在第二个例子中，当两个基本块交汇的时候，活跃变量集合是取两个集合的并集。
好了，上面这些就是数据流分析技术的核心特点。抓住这些核心特点，我们可以把这个技术用于更多的场景。比如说，我们就可以用这些特性解决上面这个程序是否正确return的问题。
在开始解决这个问题之前，我们先来梳理一个分析框架，看看我们具体要从哪些方面着手。
我们可以把一个程序在执行过程中是否遇到了return语句，用一个变量来描述，就是当前执行流程是不是alive的。我们从程序的开头，一行行代码的往下分析。在一开始，alive的初始值是true。当遇到return语句以后，alive就变成了false。
对于分支语句，比如if分支语句，则需要每个分支都要遇到一个return语句。如果一个分支的alive值是alive1，另一个分支的alive值是alive2，那么合起来的alive值是什么呢？是alive1 || alive2。也就是说，必须每个分支都遇到return语句后，总的alive才是false。这就是我们的交汇函数。</description></item><item><title>23_MySQL是怎么保证数据不丢的？</title><link>https://artisanbox.github.io/1/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/23/</guid><description>今天这篇文章，我会继续和你介绍在业务高峰期临时提升性能的方法。从文章标题“MySQL是怎么保证数据不丢的？”，你就可以看出来，今天我和你介绍的方法，跟数据的可靠性有关。
在专栏前面文章和答疑篇中，我都着重介绍了WAL机制（你可以再回顾下第2篇、第9篇、第12篇和第15篇文章中的相关内容），得到的结论是：只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启后，数据可以恢复。
评论区有同学又继续追问，redo log的写入流程是怎么样的，如何保证redo log真实地写入了磁盘。那么今天，我们就再一起看看MySQL写入binlog和redo log的流程。
binlog的写入机制其实，binlog的写入逻辑比较简单：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。
一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了binlog cache的保存问题。
系统给binlog cache分配了一片内存，每个线程一个，参数 binlog_cache_size用于控制单个线程内binlog cache所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。
事务提交的时候，执行器把binlog cache里的完整事务写入到binlog中，并清空binlog cache。状态如图1所示。
图1 binlog写盘状态可以看到，每个线程有自己binlog cache，但是共用同一份binlog文件。
图中的write，指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快。 图中的fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为fsync才占磁盘的IOPS。 write 和fsync的时机，是由参数sync_binlog控制的：
sync_binlog=0的时候，表示每次提交事务都只write，不fsync；
sync_binlog=1的时候，表示每次提交事务都会执行fsync；
sync_binlog=N(N&amp;gt;1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。
因此，在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成0，比较常见的是将其设置为100~1000中的某个数值。
但是，将sync_binlog设置为N，对应的风险是：如果主机发生异常重启，会丢失最近N个事务的binlog日志。
redo log的写入机制接下来，我们再说说redo log的写入机制。
在专栏的第15篇答疑文章中，我给你介绍了redo log buffer。事务在执行过程中，生成的redo log是要先写到redo log buffer的。
然后就有同学问了，redo log buffer里面的内容，是不是每次生成后都要直接持久化到磁盘呢？
答案是，不需要。
如果事务执行期间MySQL发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。
那么，另外一个问题是，事务还没提交的时候，redo log buffer中的部分日志有没有可能被持久化到磁盘呢？
答案是，确实会有。
这个问题，要从redo log可能存在的三种状态说起。这三种状态，对应的就是图2 中的三个颜色块。
图2 MySQL redo log存储状态这三种状态分别是：
存在redo log buffer中，物理上是在MySQL进程内存中，就是图中的红色部分；</description></item><item><title>23_二叉树基础（上）：什么样的二叉树适合用数组来存储？</title><link>https://artisanbox.github.io/2/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/24/</guid><description>前面我们讲的都是线性表结构，栈、队列等等。今天我们讲一种非线性表结构，树。树这种数据结构比线性表的数据结构要复杂得多，内容也比较多，所以我会分四节来讲解。
我反复强调过，带着问题学习，是最有效的学习方式之一，所以在正式的内容开始之前，我还是给你出一道思考题：二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？
带着这些问题，我们就来学习今天的内容，树！
树（Tree）我们首先来看，什么是“树”？再完备的定义，都没有图直观。所以我在图中画了几棵“树”。你来看看，这些“树”都有什么特征？
你有没有发现，“树”这种数据结构真的很像我们现实生活中的“树”，这里面每个元素我们叫做“节点”；用来连接相邻节点之间的关系，我们叫做“父子关系”。
比如下面这幅图，A节点就是B节点的父节点，B节点是A节点的子节点。B、C、D这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫做根节点，也就是图中的节点E。我们把没有子节点的节点叫做叶子节点或者叶节点，比如图中的G、H、I、J、K、L都是叶子节点。
除此之外，关于“树”，还有三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）。它们的定义是这样的：
这三个概念的定义比较容易混淆，描述起来也比较空洞。我举个例子说明一下，你一看应该就能明白。
记这几个概念，我还有一个小窍门，就是类比“高度”“深度”“层”这几个名词在生活中的含义。
在我们的生活中，“高度”这个概念，其实就是从下往上度量，比如我们要度量第10层楼的高度、第13层楼的高度，起点都是地面。所以，树这种数据结构的高度也是一样，从最底层开始计数，并且计数的起点是0。
“深度”这个概念在生活中是从上往下度量的，比如水中鱼的深度，是从水平面开始度量的。所以，树这种数据结构的深度也是类似的，从根结点开始度量，并且计数起点也是0。
“层数”跟深度的计算类似，不过，计数起点是1，也就是说根节点位于第1层。
二叉树（Binary Tree）树结构多种多样，不过我们最常用还是二叉树。
二叉树，顾名思义，每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。我画的这几个都是二叉树。以此类推，你可以想象一下四叉树、八叉树长什么样子。
这个图里面，有两个比较特殊的二叉树，分别是编号2和编号3这两个。
其中，编号2的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做满二叉树。
编号3的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做完全二叉树。
满二叉树很好理解，也很好识别，但是完全二叉树，有的人可能就分不清了。我画了几个完全二叉树和非完全二叉树的例子，你可以对比着看看。
你可能会说，满二叉树的特征非常明显，我们把它单独拎出来讲，这个可以理解。但是完全二叉树的特征不怎么明显啊，单从长相上来看，完全二叉树并没有特别特殊的地方啊，更像是“芸芸众树”中的一种。
那我们为什么还要特意把它拎出来讲呢？为什么偏偏把最后一层的叶子节点靠左排列的叫完全二叉树？如果靠右排列就不能叫完全二叉树了吗？这个定义的由来或者说目的在哪里？
要理解完全二叉树定义的由来，我们需要先了解，如何表示（或者存储）一棵二叉树？
想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。
我们先来看比较简单、直观的链式存储法。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。
我们再来看，基于数组的顺序存储法。我们把根节点存储在下标i = 1的位置，那左子节点存储在下标2 * i = 2的位置，右子节点存储在2 * i + 1 = 3的位置。以此类推，B节点的左子节点存储在2 * i = 2 * 2 = 4的位置，右子节点存储在2 * i + 1 = 2 * 2 + 1 = 5的位置。
我来总结一下，如果节点X存储在数组中下标为i的位置，下标为2 * i 的位置存储的就是左子节点，下标为2 * i + 1的位置存储的就是右子节点。反过来，下标为i/2的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为1的位置），这样就可以通过下标计算，把整棵树都串起来。
不过，我刚刚举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为0的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。你可以看我举的下面这个例子。
所以，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。
当我们讲到堆和堆排序的时候，你会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。
二叉树的遍历前面我讲了二叉树的基本定义和存储方法，现在我们来看二叉树中非常重要的操作，二叉树的遍历。这也是非常常见的面试题。</description></item><item><title>23_冒险和预测（二）：流水线里的接力赛</title><link>https://artisanbox.github.io/4/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/23/</guid><description>上一讲，我为你讲解了结构冒险和数据冒险，以及应对这两种冒险的两个解决方案。一种方案是增加资源，通过添加指令缓存和数据缓存，让我们对于指令和数据的访问可以同时进行。这个办法帮助CPU解决了取指令和访问数据之间的资源冲突。另一种方案是直接进行等待。通过插入NOP这样的无效指令，等待之前的指令完成。这样我们就能解决不同指令之间的数据依赖问题。
着急的人，看完上一讲的这两种方案，可能已经要跳起来问了：“这也能算解决方案么？”的确，这两种方案都有点儿笨。
第一种解决方案，好比是在软件开发的过程中，发现效率不够，于是研发负责人说：“我们需要双倍的人手和研发资源。”而第二种解决方案，好比你在提需求的时候，研发负责人告诉你说：“来不及做，你只能等我们需求排期。” 你应该很清楚地知道，“堆资源”和“等排期”这样的解决方案，并不会真的提高我们的效率，只是避免冲突的无奈之举。
那针对流水线冒险的问题，我们有没有更高级或者更高效的解决方案呢？既不用简单花钱加硬件电路这样“堆资源”，也不是纯粹等待之前的任务完成这样“等排期”。
答案当然是有的。这一讲，我们就来看看计算机组成原理中，一个更加精巧的解决方案，操作数前推。
NOP操作和指令对齐要想理解操作数前推技术，我们先来回顾一下，第5讲讲过的，MIPS体系结构下的R、I、J三类指令，以及第20讲里的五级流水线“取指令（IF）-指令译码（ID）-指令执行（EX）-内存访问（MEM）-数据写回（WB） ”。
我把对应的图片放进来了，你可以看一下。如果印象不深，建议你先回到这两节去复习一下，再来看今天的内容。
在MIPS的体系结构下，不同类型的指令，会在流水线的不同阶段进行不同的操作。
我们以MIPS的LOAD，这样从内存里读取数据到寄存器的指令为例，来仔细看看，它需要经历的5个完整的流水线。STORE这样从寄存器往内存里写数据的指令，不需要有写回寄存器的操作，也就是没有数据写回的流水线阶段。至于像ADD和SUB这样的加减法指令，所有操作都在寄存器完成，所以没有实际的内存访问（MEM）操作。
有些指令没有对应的流水线阶段，但是我们并不能跳过对应的阶段直接执行下一阶段。不然，如果我们先后执行一条LOAD指令和一条ADD指令，就会发生LOAD指令的WB阶段和ADD指令的WB阶段，在同一个时钟周期发生。这样，相当于触发了一个结构冒险事件，产生了资源竞争。
所以，在实践当中，各个指令不需要的阶段，并不会直接跳过，而是会运行一次NOP操作。通过插入一个NOP操作，我们可以使后一条指令的每一个Stage，一定不和前一条指令的同Stage在一个时钟周期执行。这样，就不会发生先后两个指令，在同一时钟周期竞争相同的资源，产生结构冒险了。
流水线里的接力赛：操作数前推通过NOP操作进行对齐，我们在流水线里，就不会遇到资源竞争产生的结构冒险问题了。除了可以解决结构冒险之外，这个NOP操作，也是我们之前讲的流水线停顿插入的对应操作。
但是，插入过多的NOP操作，意味着我们的CPU总是在空转，干吃饭不干活。那么，我们有没有什么办法，尽量少插入一些NOP操作呢？不要着急，下面我们就以两条先后发生的ADD指令作为例子，看看能不能找到一些好的解决方案。
add $t0, $s2,$s1 add $s2, $s1,$t0 这两条指令很简单。
第一条指令，把 s1 和 s2 寄存器里面的数据相加，存入到 t0 这个寄存器里面。 第二条指令，把 s1 和 t0 寄存器里面的数据相加，存入到 s2 这个寄存器里面。 因为后一条的 add 指令，依赖寄存器 t0 里的值。而 t0 里面的值，又来自于前一条指令的计算结果。所以后一条指令，需要等待前一条指令的数据写回阶段完成之后，才能执行。就像上一讲里讲的那样，我们遇到了一个数据依赖类型的冒险。于是，我们就不得不通过流水线停顿来解决这个冒险问题。我们要在第二条指令的译码阶段之后，插入对应的NOP指令，直到前一天指令的数据写回完成之后，才能继续执行。
这样的方案，虽然解决了数据冒险的问题，但是也浪费了两个时钟周期。我们的第2条指令，其实就是多花了2个时钟周期，运行了两次空转的NOP操作。
不过，其实我们第二条指令的执行，未必要等待第一条指令写回完成，才能进行。如果我们第一条指令的执行结果，能够直接传输给第二条指令的执行阶段，作为输入，那我们的第二条指令，就不用再从寄存器里面，把数据再单独读出来一次，才来执行代码。
我们完全可以在第一条指令的执行阶段完成之后，直接将结果数据传输给到下一条指令的ALU。然后，下一条指令不需要再插入两个NOP阶段，就可以继续正常走到执行阶段。
这样的解决方案，我们就叫作操作数前推（Operand Forwarding），或者操作数旁路（Operand Bypassing）。其实我觉得，更合适的名字应该叫操作数转发。这里的Forward，其实就是我们写Email时的“转发”（Forward）的意思。不过现有的经典教材的中文翻译一般都叫“前推”，我们也就不去纠正这个说法了，你明白这个意思就好。
转发，其实是这个技术的逻辑含义，也就是在第1条指令的执行结果，直接“转发”给了第2条指令的ALU作为输入。另外一个名字，旁路（Bypassing），则是这个技术的硬件含义。为了能够实现这里的“转发”，我们在CPU的硬件里面，需要再单独拉一根信号传输的线路出来，使得ALU的计算结果，能够重新回到ALU的输入里来。这样的一条线路，就是我们的“旁路”。它越过（Bypass）了写入寄存器，再从寄存器读出的过程，也为我们节省了2个时钟周期。
操作数前推的解决方案不但可以单独使用，还可以和流水线冒泡一起使用。有的时候，虽然我们可以把操作数转发到下一条指令，但是下一条指令仍然需要停顿一个时钟周期。
比如说，我们先去执行一条LOAD指令，再去执行ADD指令。LOAD指令在访存阶段才能把数据读取出来，所以下一条指令的执行阶段，需要在访存阶段完成之后，才能进行。
总的来说，操作数前推的解决方案，比流水线停顿更进了一步。流水线停顿的方案，有点儿像游泳比赛的接力方式。下一名运动员，需要在前一个运动员游玩了全程之后，触碰到了游泳池壁才能出发。而操作数前推，就好像短跑接力赛。后一个运动员可以提前抢跑，而前一个运动员会多跑一段主动把交接棒传递给他。
总结延伸这一讲，我给你介绍了一个更加高级，也更加复杂的解决数据冒险问题方案，就是操作数前推，或者叫操作数旁路。
操作数前推，就是通过在硬件层面制造一条旁路，让一条指令的计算结果，可以直接传输给下一条指令，而不再需要“指令1写回寄存器，指令2再读取寄存器“这样多此一举的操作。这样直接传输带来的好处就是，后面的指令可以减少，甚至消除原本需要通过流水线停顿，才能解决的数据冒险问题。
这个前推的解决方案，不仅可以单独使用，还可以和前面讲解过的流水线冒泡结合在一起使用。因为有些时候，我们的操作数前推并不能减少所有“冒泡”，只能去掉其中的一部分。我们仍然需要通过插入一些“气泡”来解决冒险问题。
通过操作数前推，我们进一步提升了CPU的运行效率。那么，我们是不是还能找到别的办法，进一步地减少浪费呢？毕竟，看到现在，我们仍然少不了要插入很多NOP的“气泡”。那就请你继续坚持学习下去。下一讲，我们来看看，CPU是怎么通过乱序执行，进一步减少“气泡”的。
推荐阅读想要深入了解操作数前推相关的内容，推荐你读一下《计算机组成与设计：硬件/软件接口》的4.5～4.7章节。
课后思考前面讲5级流水线指令的时候，我们说，STORE指令是没有数据写回阶段的，而ADD指令是没有访存阶段的。那像CMP或者JMP这样的比较和跳转指令，5个阶段都是全的么？还是说不需要哪些阶段呢？
欢迎留言和我分享你的疑惑和见解。你也可以把今天的内容，分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>23｜增强编译器前端功能第2步：增强类型体系</title><link>https://artisanbox.github.io/3/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/25/</guid><description>你好，我是宫文学。
你可能也注意到了，我们在第二部分的主要任务，是要让PlayScript扩展到支持更多的类型。在这个任务中，对类型的处理能力就是一个很重要的功能。
其实在第一部分，我们已经实现了一定的类型处理功能，包括类型检查、类型自动推断等，但其实还有更多的类型处理能力需要支持。
对于一门语言来说，类型系统是它的核心。语言之间的差别很多时候都体现在类型系统的设计上，程序员们通常也会对类型处理的内部机制很感兴趣。而TypeScript比JavaScript语言增强的部分，恰恰就是一个强大而又灵活的类型系统，所以我们就更有必要讨论一下与类型有关的话题了。
那么通过今天这节课，我们就来增强一下PlayScript的类型处理能力，在这过程中，我们也能学习到更多与类型系统有关的知识点，特别是能对类型计算的数学实质有所认知。
首先，我们来看看TypeScript的类型系统有什么特点。
TypeScript的类型系统从TypeScript的名字上，你就可以看出来，这门语言在类型系统的设计上，一定是下了功夫的。也确实是这样，TypeScript在设计之初，就想弥补JavaScript弱类型、动态类型所带来的缺点。特别是，当程序规模变大的时候，弱类型、动态类型很容易不经意地引入一些错误，而且还比较难以发现。
所以TypeScript的设计者，希望通过提供一个强类型体系，让编译器能够检查出程序中潜在的错误，这也有助于IDE工具提供更友好的特性，比如准确提示类的属性和方法，从而帮助程序员编写更高质量的程序。
而TypeScript也确实实现了这个设计目标。它的类型系统功能很强大，表达能力很强，既有利于提高程序的正确性，同时又没有削弱程序员自由表达各种设计思想的能力。
那么我们现在就来看一看TypeScript的类型系统到底有什么特点。
首先，TypeScript继承了JavaScript的几个预定义的类型，比如number、string和boolean等。
在JavaScript中，我们不需要声明类型，比如下面两句代码就是。在程序运行的时候，系统会自动给age和name1分别关联一个number和string类型的值。
var age = 18; var name1 = "richard"; 而在TypeScript中呢，你需要用let关键字来声明变量。在下面的示例程序中，age和number被我们用let关键字分别赋予了number和string类型。
let age = 18; let name1 = "richard"; 这两行代码里的类型是被推导出来的，它们跟显式声明类型的方式是等价的。
let age:number = 18; let name1:string = "richard"; 第二，TypeScript禁止了变量类型的动态修改。
在JavaScript中，我们可以动态地修改变量的类型。比如在下面两行代码中，age一开头是number型的，后来被改成了string型，也是允许的：
var age = 18; age = "eighteen"; 但在TypeScript中，如果你一开头给age赋一个number的值，后面再赋一个string类型的值，编译器就会报错：
let age = 18; age = "eighteen"; //错误！ 这是因为，上面的第一行代码等价于显式声明age为number类型，因为TypeScript会根据变量初始化的部分，来推断出age的类型。而这个类型一旦确定，后面就不允许再修改了。
let age:number = 18; age = "eighteen"; 不过，如果完全不允许类型动态变化，可能会失去JavaScript灵活性这个优点，会让某些程序员觉得用起来不舒服。所以，TypeScript还留了一个口子，就是any类型。
第三，只有any类型允许动态修改变量的类型。
在TypeScript中，如果你声明变量的时候不指定任何类型，或者显式地指定变量类型为any，那变量的类型都是any，程序也就可以动态地修改变量的类型，我们可以看看下面这个例子：
let age; //等价于 let age:any; age = 18; console.</description></item><item><title>24_MySQL是怎么保证主备一致的？</title><link>https://artisanbox.github.io/1/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/24/</guid><description>在前面的文章中，我不止一次地和你提到了binlog，大家知道binlog可以用来归档，也可以用来做主备同步，但它的内容是什么样的呢？为什么备库执行了binlog就可以跟主库保持一致了呢？今天我就正式地和你介绍一下它。
毫不夸张地说，MySQL能够成为现下最流行的开源数据库，binlog功不可没。
在最开始，MySQL是以容易学习和方便的高可用架构，被开发人员青睐的。而它的几乎所有的高可用架构，都直接依赖于binlog。虽然这些高可用架构已经呈现出越来越复杂的趋势，但都是从最基本的一主一备演化过来的。
今天这篇文章我主要为你介绍主备的基本原理。理解了背后的设计原理，你也可以从业务开发的角度，来借鉴这些设计思想。
MySQL主备的基本原理如图1所示就是基本的主备切换流程。
图 1 MySQL主备切换流程在状态1中，客户端的读写都直接访问节点A，而节点B是A的备库，只是将A的更新都同步过来，到本地执行。这样可以保持节点B和A的数据是相同的。
当需要切换的时候，就切成状态2。这时候客户端读写访问的都是节点B，而节点A是B的备库。
在状态1中，虽然节点B没有被直接访问，但是我依然建议你把节点B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑：
有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作；
防止切换逻辑有bug，比如切换过程中出现双写，造成主备不一致；
可以用readonly状态，来判断节点的角色。
你可能会问，我把备库设置成只读了，还怎么跟主库保持同步更新呢？
这个问题，你不用担心。因为readonly设置对超级(super)权限用户是无效的，而用于同步更新的线程，就拥有超级权限。
接下来，我们再看看节点A到B这条线的内部流程是什么样的。图2中画出的就是一个update语句在节点A执行，然后同步到节点B的完整流程图。
图2 主备流程图图2中，包含了我在上一篇文章中讲到的binlog和redo log的写入机制相关的内容，可以看到：主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写binlog。
备库B跟主库A之间维持了一个长连接。主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程是这样的：
在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量。
在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和sql_thread。其中io_thread负责与主库建立连接。
主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。
备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。
sql_thread读取中转日志，解析出日志里的命令，并执行。
这里需要说明，后来由于多线程复制方案的引入，sql_thread演化成为了多个线程，跟我们今天要介绍的原理没有直接关系，暂且不展开。
分析完了这个长连接的逻辑，我们再来看一个问题：binlog里面到底是什么内容，为什么备库拿过去可以直接执行。
binlog的三种格式对比我在第15篇答疑文章中，和你提到过binlog有两种格式，一种是statement，一种是row。可能你在其他资料上还会看到有第三种格式，叫作mixed，其实它就是前两种格式的混合。
为了便于描述binlog的这三种格式间的区别，我创建了一个表，并初始化几行数据。
mysql&amp;gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `t_modified`(`t_modified`) ) ENGINE=InnoDB; insert into t values(1,1,&amp;lsquo;2018-11-13&amp;rsquo;); insert into t values(2,2,&amp;lsquo;2018-11-12&amp;rsquo;); insert into t values(3,3,&amp;lsquo;2018-11-11&amp;rsquo;); insert into t values(4,4,&amp;lsquo;2018-11-10&amp;rsquo;); insert into t values(5,5,&amp;lsquo;2018-11-09&amp;rsquo;); 如果要在表中删除一行数据的话，我们来看看这个delete语句的binlog是怎么记录的。</description></item><item><title>24_二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？</title><link>https://artisanbox.github.io/2/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/25/</guid><description>上一节我们学习了树、二叉树以及二叉树的遍历，今天我们再来学习一种特殊的二叉树，二叉查找树。二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。
我们之前说过，散列表也是支持这些操作的，并且散列表的这些操作比二叉查找树更高效，时间复杂度是O(1)。既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？
带着这些问题，我们就来学习今天的内容，二叉查找树！
二叉查找树（Binary Search Tree）二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。它是怎么做到这些的呢？
这些都依赖于二叉查找树的特殊结构。二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。 我画了几个二叉查找树的例子，你一看应该就清楚了。
前面我们讲到，二叉查找树支持快速查找、插入、删除操作，现在我们就依次来看下，这三个操作是如何实现的。
1.二叉查找树的查找操作首先，我们看如何在二叉查找树中查找一个节点。我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。
这里我把查找的代码实现了一下，贴在下面了，结合代码，理解起来会更加容易。
public class BinarySearchTree { private Node tree; public Node find(int data) { Node p = tree; while (p != null) { if (data &amp;lt; p.data) p = p.left; else if (data &amp;gt; p.data) p = p.right; else return p; } return null; }
public static class Node { private int data; private Node left; private Node right;
public Node(int data) { this.</description></item><item><title>24_冒险和预测（三）：CPU里的“线程池”</title><link>https://artisanbox.github.io/4/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/24/</guid><description>过去两讲，我为你讲解了通过增加资源、停顿等待以及主动转发数据的方式，来解决结构冒险和数据冒险问题。对于结构冒险，由于限制来自于同一时钟周期不同的指令，要访问相同的硬件资源，解决方案是增加资源。对于数据冒险，由于限制来自于数据之间的各种依赖，我们可以提前把数据转发到下一个指令。
但是即便综合运用这三种技术，我们仍然会遇到不得不停下整个流水线，等待前面的指令完成的情况，也就是采用流水线停顿的解决方案。比如说，上一讲里最后给你的例子，即使我们进行了操作数前推，因为第二条加法指令依赖于第一条指令从内存中获取的数据，我们还是要插入一次NOP的操作。
那这个时候你就会想了，那我们能不能让后面没有数据依赖的指令，在前面指令停顿的时候先执行呢？
答案当然是可以的。毕竟，流水线停顿的时候，对应的电路闲着也是闲着。那我们完全可以先完成后面指令的执行阶段。
填上空闲的NOP：上菜的顺序不必是点菜的顺序之前我为你讲解的，无论是流水线停顿，还是操作数前推，归根到底，只要前面指令的特定阶段还没有执行完成，后面的指令就会被“阻塞”住。
但是这个“阻塞”很多时候是没有必要的。因为尽管你的代码生成的指令是顺序的，但是如果后面的指令不需要依赖前面指令的执行结果，完全可以不必等待前面的指令运算完成。
比如说，下面这三行代码。
a = b + c d = a * e x = y * z 计算里面的 x ，却要等待 a 和 d 都计算完成，实在没啥必要。所以我们完全可以在 d 的计算等待 a 的计算的过程中，先把 x 的结果给算出来。
在流水线里，后面的指令不依赖前面的指令，那就不用等待前面的指令执行，它完全可以先执行。
可以看到，因为第三条指令并不依赖于前两条指令的计算结果，所以在第二条指令等待第一条指令的访存和写回阶段的时候，第三条指令就已经执行完成了。
这就好比你开了一家餐馆，顾客会排队来点菜。餐馆的厨房里会有洗菜、切菜、炒菜、上菜这样的各个步骤。后厨也是按照点菜的顺序开始做菜的。但是不同的菜需要花费的时间和工序可能都有差别。有些菜做起来特别麻烦，特别慢。比如做一道佛跳墙有好几道工序。我们没有必要非要等先点的佛跳墙上菜了，再开始做后面的炒鸡蛋。只要有厨子空出来了，就可以先动手做前面的简单菜，先给客户端上去。
这样的解决方案，在计算机组成里面，被称为乱序执行（Out-of-Order Execution，OoOE）。乱序执行，最早来自于著名的IBM 360。相信你一定听说过《人月神话》这本软件工程届的经典著作，它讲的就是IBM 360开发过程中的“人生体会”。而IBM 360困难的开发过程，也少不了第一次引入乱序执行这个新的CPU技术。
CPU里的“线程池”：理解乱序执行那么，我们的CPU怎样才能实现乱序执行呢？是不是像玩俄罗斯方块一样，把后面的指令，找一个前面的坑填进去就行了？事情并没有这么简单。其实，从今天软件开发的维度来思考，乱序执行好像是在指令的执行阶段，引入了一个“线程池”。我们下面就来看一看，在CPU里，乱序执行的过程究竟是怎样的。
使用乱序执行技术后，CPU里的流水线就和我之前给你看的5级流水线不太一样了。我们一起来看一看下面这张图。
1.在取指令和指令译码的时候，乱序执行的CPU和其他使用流水线架构的CPU是一样的。它会一级一级顺序地进行取指令和指令译码的工作。
2.在指令译码完成之后，就不一样了。CPU不会直接进行指令执行，而是进行一次指令分发，把指令发到一个叫作保留站（Reservation Stations）的地方。顾名思义，这个保留站，就像一个火车站一样。发送到车站的指令，就像是一列列的火车。
3.这些指令不会立刻执行，而要等待它们所依赖的数据，传递给它们之后才会执行。这就好像一列列的火车都要等到乘客来齐了才能出发。
4.一旦指令依赖的数据来齐了，指令就可以交到后面的功能单元（Function Unit，FU），其实就是ALU，去执行了。我们有很多功能单元可以并行运行，但是不同的功能单元能够支持执行的指令并不相同。就和我们的铁轨一样，有些从上海北上，可以到北京和哈尔滨；有些是南下的，可以到广州和深圳。
5.指令执行的阶段完成之后，我们并不能立刻把结果写回到寄存器里面去，而是把结果再存放到一个叫作重排序缓冲区（Re-Order Buffer，ROB）的地方。
6.在重排序缓冲区里，我们的CPU会按照取指令的顺序，对指令的计算结果重新排序。只有排在前面的指令都已经完成了，才会提交指令，完成整个指令的运算结果。
7.实际的指令的计算结果数据，并不是直接写到内存或者高速缓存里，而是先写入存储缓冲区（Store Buffer面，最终才会写入到高速缓存和内存里。
可以看到，在乱序执行的情况下，只有CPU内部指令的执行层面，可能是“乱序”的。只要我们能在指令的译码阶段正确地分析出指令之间的数据依赖关系，这个“乱序”就只会在互相没有影响的指令之间发生。
即便指令的执行过程中是乱序的，我们在最终指令的计算结果写入到寄存器和内存之前，依然会进行一次排序，以确保所有指令在外部看来仍然是有序完成的。
有了乱序执行，我们重新去执行上面的3行代码。
a = b + c d = a * e x = y * z 里面的 d 依赖于 a 的计算结果，不会在 a 的计算完成之前执行。但是我们的CPU并不会闲着，因为 x = y * z 的指令同样会被分发到保留站里。因为 x 所依赖的 y 和 z 的数据是准备好的， 这里的乘法运算不会等待计算 d，而会先去计算 x 的值。</description></item><item><title>24｜增强编译器前端功能第3步：全面的集合运算</title><link>https://artisanbox.github.io/3/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/26/</guid><description>你好，我是宫文学。
在上一节课，我们扩展了我们语言的类型体系，还测试了几个简单的例子。从中，我们已经能体会出一些TypeScript类型体系的特点了。
不过，TypeScript的类型体系其实比我们前面测试的还要强大得多，能够在多种场景下进行复杂的类型处理。
今天这节课，我们会通过多个实际的例子，来探索TypeScript的类型处理能力。并且，在这个过程中，你还会进一步印证我们上一节课的一个知识点，就是类型计算实际上就是集合运算。在我们今天的这些例子中，你会见到多种集合运算，包括子集判断、重叠判断，以及交集、并集和补集的计算。
首先，让我们看几个例子，来理解一下类型计算的使用场景。
类型计算的场景我们先看第一个例子：
function foo1(age : number|null){ let age1 : string|number; age1 = age; //编译器在这里会检查出错误。 console.log(age1); } 在这个例子中，我们用到了age和age1两个变量，它们都采用了联合类型。一个是number|null，一个是string|number。
如果你用–strict选项来编译这个程序，那么tsc会报错：
这个错误信息的意思是：类型number|null不能赋给类型string|number。具体来说，null是不能赋给string|number的。
这说明什么呢？这说明对于赋值语句，比如x = y来说，它会有一个默认要求，要求y的类型要么跟x一样，要么是x的子集才可以。我们把这个关系记做y.type &amp;lt;= x.type。
那么，其他的二元运算，是不是也像赋值运算那样，需要一个类型是另一个类型的子集呢？
不是的。不同的运算，做类型检查的规则是不同的。比如，对于“==”和“!=”这两个运算符，只需要两个类型有交集就可以。你可以用tsc编译一下这个例子：
function foo2(age1 : number|null, age2:string|number){ if (age1 == age2){ //OK。只要两个类型有交集就可以。 console.log("same age!"); } } 你会看到，编译器并不会报错。这说明，两个不同的类型，只要它们有交集，就可以进行等值和不等值比较。并且，即使age1的值是null，age2的值是一个字符串，等值比较仍然是有意义的，比较的结果是不相等。
那如果两个类型没有交集，会发生什么情况呢？我们看看下面的例子，参数x和y属于不同的类型，它们之间没有交集。
function foo3(x : number|null, y:string|boolean){ if (x == y){ //编译器报错：两个类型没有交集 console.log("x and y is the same"); } } 这次，如果你用tsc去编译，即使不加–strict选项，编译器也会报错：
编译器会说，这个条件表达式会永远返回false，因为这两个类型没有交集。
到此为止，我们就了解清楚等值比较的规则了，也就是要求两个类型有交集才可以，或者说两个类型要存在重叠。
那其他的比较运算符，比如&amp;gt;，&amp;gt;=，&amp;lt;，&amp;lt;=，也遵循相同的规则吗？
我们把foo2中的==运算符改为&amp;gt;=运算符，得到一个新的示例程序：</description></item><item><title>25_MySQL是怎么保证高可用的？</title><link>https://artisanbox.github.io/1/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/25/</guid><description>在上一篇文章中，我和你介绍了binlog的基本内容，在一个主备关系中，每个备库接收主库的binlog并执行。
正常情况下，只要主库执行更新生成的所有binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。
但是，MySQL要提供高可用能力，只有最终一致性是不够的。为什么这么说呢？今天我就着重和你分析一下。
这里，我再放一次上一篇文章中讲到的双M结构的主备切换流程图。
图 1 MySQL主备切换流程--双M结构主备延迟主备切换可能是一个主动运维动作，比如软件升级、主库所在机器按计划下线等，也可能是被动操作，比如主库所在机器掉电。
接下来，我们先一起看看主动切换的场景。
在介绍主动切换流程的详细步骤之前，我要先跟你说明一个概念，即“同步延迟”。与数据同步有关的时间点主要包括以下三个：
主库A执行完成一个事务，写入binlog，我们把这个时刻记为T1;
之后传给备库B，我们把备库B接收完这个binlog的时刻记为T2;
备库B执行完成这个事务，我们把这个时刻记为T3。
所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是T3-T1。
你可以在备库上执行show slave status命令，它的返回结果里面会显示seconds_behind_master，用于表示当前备库延迟了多少秒。
seconds_behind_master的计算方法是这样的：
每个事务的binlog 里面都有一个时间字段，用于记录主库上写入的时间；
备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时间的差值，得到seconds_behind_master。
可以看到，其实seconds_behind_master这个参数计算的就是T3-T1。所以，我们可以用seconds_behind_master来作为主备延迟的值，这个值的时间精度是秒。
你可能会问，如果主备库机器的系统时间设置不一致，会不会导致主备延迟的值不准？
其实不会的。因为，备库连接到主库的时候，会通过执行SELECT UNIX_TIMESTAMP()函数来获得当前主库的系统时间。如果这时候发现主库的系统时间与自己不一致，备库在执行seconds_behind_master计算的时候会自动扣掉这个差值。
需要说明的是，在网络正常的时候，日志从主库传给备库所需的时间是很短的，即T2-T1的值是非常小的。也就是说，网络正常情况下，主备延迟的主要来源是备库接收完binlog和执行完这个事务之间的时间差。
所以说，主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产binlog的速度要慢。接下来，我就和你一起分析下，这可能是由哪些原因导致的。
主备延迟的来源首先，有些部署条件下，备库所在机器的性能要比主库所在的机器性能差。
一般情况下，有人这么部署时的想法是，反正备库没有请求，所以可以用差一点儿的机器。或者，他们会把20个主库放在4台机器上，而把备库集中在一台机器上。
其实我们都知道，更新请求对IOPS的压力，在主库和备库上是无差别的。所以，做这种部署时，一般都会将备库设置为“非双1”的模式。
但实际上，更新过程中也会触发大量的读操作。所以，当备库主机上的多个备库都在争抢资源的时候，就可能会导致主备延迟了。
当然，这种部署现在比较少了。因为主备可能发生切换，备库随时可能变成主库，所以主备库选用相同规格的机器，并且做对称部署，是现在比较常见的情况。
追问1：但是，做了对称部署以后，还可能会有延迟。这是为什么呢？
这就是第二种常见的可能了，即备库的压力大。一般的想法是，主库既然提供了写能力，那么备库可以提供一些读能力。或者一些运营后台需要的分析语句，不能影响正常业务，所以只能在备库上跑。
我真就见过不少这样的情况。由于主库直接影响业务，大家使用起来会比较克制，反而忽视了备库的压力控制。结果就是，备库上的查询耗费了大量的CPU资源，影响了同步速度，造成主备延迟。
这种情况，我们一般可以这么处理：
一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。
通过binlog输出到外部系统，比如Hadoop这类系统，让外部系统提供统计类查询的能力。
其中，一主多从的方式大都会被采用。因为作为数据库系统，还必须保证有定期全量备份的能力。而从库，就很适合用来做备份。
备注：这里需要说明一下，从库和备库在概念上其实差不多。在我们这个专栏里，为了方便描述，我把会在HA过程中被选成新主库的，称为备库，其他的称为从库。
追问2：采用了一主多从，保证备库的压力不会超过主库，还有什么情况可能导致主备延迟吗？
这就是第三种可能了，即大事务。
大事务这种情况很好理解。因为主库上必须等事务执行完成才会写入binlog，再传给备库。所以，如果一个主库上的语句执行10分钟，那这个事务很可能就会导致从库延迟10分钟。
不知道你所在公司的DBA有没有跟你这么说过：不要一次性地用delete语句删除太多数据。其实，这就是一个典型的大事务场景。
比如，一些归档类的数据，平时没有注意删除历史数据，等到空间快满了，业务开发人员要一次性地删掉大量历史数据。同时，又因为要避免在高峰期操作会影响业务（至少有这个意识还是很不错的），所以会在晚上执行这些大量数据的删除操作。
结果，负责的DBA同学半夜就会收到延迟报警。然后，DBA团队就要求你后续再删除数据的时候，要控制每个事务删除的数据量，分成多次删除。
另一种典型的大事务场景，就是大表DDL。这个场景，我在前面的文章中介绍过。处理方案就是，计划内的DDL，建议使用gh-ost方案（这里，你可以再回顾下第13篇文章《为什么表数据删掉一半，表文件大小不变？》中的相关内容）。
追问3：如果主库上也不做大事务了，还有什么原因会导致主备延迟吗？
造成主备延迟还有一个大方向的原因，就是备库的并行复制能力。这个话题，我会留在下一篇文章再和你详细介绍。
其实还是有不少其他情况会导致主备延迟，如果你还碰到过其他场景，欢迎你在评论区给我留言，我来和你一起分析、讨论。
由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略。</description></item><item><title>25_冒险和预测（四）：今天下雨了，明天还会下雨么？</title><link>https://artisanbox.github.io/4/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/25/</guid><description>过去三讲，我主要为你介绍了结构冒险和数据冒险，以及增加资源、流水线停顿、操作数前推、乱序执行，这些解决各种“冒险”的技术方案。
在结构冒险和数据冒险中，你会发现，所有的流水线停顿操作都要从指令执行阶段开始。流水线的前两个阶段，也就是取指令（IF）和指令译码（ID）的阶段，是不需要停顿的。CPU会在流水线里面直接去取下一条指令，然后进行译码。
取指令和指令译码不会需要遇到任何停顿，这是基于一个假设。这个假设就是，所有的指令代码都是顺序加载执行的。不过这个假设，在执行的代码中，一旦遇到 if…else 这样的条件分支，或者 for/while 循环，就会不成立。
回顾一下第6讲的条件跳转流程我们先来回顾一下，第6讲里讲的cmp比较指令、jmp和jle这样的条件跳转指令。可以看到，在jmp指令发生的时候，CPU可能会跳转去执行其他指令。jmp后的那一条指令是否应该顺序加载执行，在流水线里面进行取指令的时候，我们没法知道。要等jmp指令执行完成，去更新了PC寄存器之后，我们才能知道，是否执行下一条指令，还是跳转到另外一个内存地址，去取别的指令。
这种为了确保能取到正确的指令，而不得不进行等待延迟的情况，就是今天我们要讲的控制冒险（Control Harzard）。这也是流水线设计里最后一种冒险。
分支预测：今天下雨了，明天还会继续下雨么？在遇到了控制冒险之后，我们的CPU具体会怎么应对呢？除了流水线停顿，等待前面的jmp指令执行完成之后，再去取最新的指令，还有什么好办法吗？当然是有的。我们一起来看一看。
缩短分支延迟第一个办法，叫作缩短分支延迟。回想一下我们的条件跳转指令，条件跳转指令其实进行了两种电路操作。
第一种，是进行条件比较。这个条件比较，需要的输入是，根据指令的opcode，就能确认的条件码寄存器。
第二种，是进行实际的跳转，也就是把要跳转的地址信息写入到PC寄存器。无论是opcode，还是对应的条件码寄存器，还是我们跳转的地址，都是在指令译码（ID）的阶段就能获得的。而对应的条件码比较的电路，只要是简单的逻辑门电路就可以了，并不需要一个完整而复杂的ALU。
所以，我们可以将条件判断、地址跳转，都提前到指令译码阶段进行，而不需要放在指令执行阶段。对应的，我们也要在CPU里面设计对应的旁路，在指令译码阶段，就提供对应的判断比较的电路。
这种方式，本质上和前面数据冒险的操作数前推的解决方案类似，就是在硬件电路层面，把一些计算结果更早地反馈到流水线中。这样反馈变得更快了，后面的指令需要等待的时间就变短了。
不过只是改造硬件，并不能彻底解决问题。跳转指令的比较结果，仍然要在指令执行的时候才能知道。在流水线里，第一条指令进行指令译码的时钟周期里，我们其实就要去取下一条指令了。这个时候，我们其实还没有开始指令执行阶段，自然也就不知道比较的结果。
分支预测所以，这个时候，我们就引入了一个新的解决方案，叫作分支预测（Branch Prediction）技术，也就是说，让我们的CPU来猜一猜，条件跳转后执行的指令，应该是哪一条。
最简单的分支预测技术，叫作“假装分支不发生”。顾名思义，自然就是仍然按照顺序，把指令往下执行。其实就是CPU预测，条件跳转一定不发生。这样的预测方法，其实也是一种静态预测技术。就好像猜硬币的时候，你一直猜正面，会有50%的正确率。
如果分支预测是正确的，我们自然赚到了。这个意味着，我们节省下来本来需要停顿下来等待的时间。如果分支预测失败了呢？那我们就把后面已经取出指令已经执行的部分，给丢弃掉。这个丢弃的操作，在流水线里面，叫作Zap或者Flush。CPU不仅要执行后面的指令，对于这些已经在流水线里面执行到一半的指令，我们还需要做对应的清除操作。比如，清空已经使用的寄存器里面的数据等等，这些清除操作，也有一定的开销。
所以，CPU需要提供对应的丢弃指令的功能，通过控制信号清除掉已经在流水线中执行的指令。只要对应的清除开销不要太大，我们就是划得来的。
动态分支预测第三个办法，叫作动态分支预测。
上面的静态预测策略，看起来比较简单，预测的准确率也许有50%。但是如果运气不好，可能就会特别差。于是，工程师们就开始思考，我们有没有更好的办法呢？比如，根据之前条件跳转的比较结果来预测，是不是会更准一点？
我们日常生活里，最经常会遇到的预测就是天气预报。如果没有气象台给你天气预报，你想要猜一猜明天是不是下雨，你会怎么办？
有一个简单的策略，就是完全根据今天的天气来猜。如果今天下雨，我们就预测明天下雨。如果今天天晴，就预测明天也不会下雨。这是一个很符合我们日常生活经验的预测。因为一般下雨天，都是连着下几天，不断地间隔地发生“天晴-下雨-天晴-下雨”的情况并不多见。
那么，把这样的实践拿到生活中来是不是有效呢？我在这里给了一张2019年1月上海的天气情况的表格。
我们用前一天的是不是下雨，直接来预测后一天会不会下雨。这个表格里一共有31天，那我们就可以预测30次。你可以数一数，按照这种预测方式，我们可以预测正确23次，正确率是76.7%，比随机预测的50%要好上不少。
而同样的策略，我们一样可以放在分支预测上。这种策略，我们叫一级分支预测（One Level Branch Prediction），或者叫1比特饱和计数（1-bit saturating counter）。这个方法，其实就是用一个比特，去记录当前分支的比较情况，直接用当前分支的比较情况，来预测下一次分支时候的比较情况。
只用一天下雨，就预测第二天下雨，这个方法还是有些“草率”，我们可以用更多的信息，而不只是一次的分支信息来进行预测。于是，我们可以引入一个状态机（State Machine）来做这个事情。
如果连续发生下雨的情况，我们就认为更有可能下雨。之后如果只有一天放晴了，我们仍然认为会下雨。在连续下雨之后，要连续两天放晴，我们才会认为之后会放晴。整个状态机的流转，可以参考我在文稿里放的图。
这个状态机里，我们一共有4个状态，所以我们需要2个比特来记录对应的状态。这样这整个策略，就可以叫作2比特饱和计数，或者叫双模态预测器（Bimodal Predictor）。
好了，现在你可以用这个策略，再去对照一下上面的天气情况。如果天气的初始状态我们放在“多半放晴”的状态下，我们预测的结果的正确率会是22次，也就是73.3%的正确率。可以看到，并不是更复杂的算法，效果一定就更好。实际的预测效果，和实际执行的指令高度相关。
如果想对各种分支预测技术有所了解，Wikipedia里面有更详细的内容和更多的分支预测算法，你可以看看。
为什么循环嵌套的改变会影响性能？说完了分支预测，现在我们先来看一个Java程序。
public class BranchPrediction { public static void main(String args[]) { long start = System.currentTimeMillis(); for (int i = 0; i &amp;lt; 100; i++) { for (int j = 0; j &amp;lt;1000; j ++) { for (int k = 0; k &amp;lt; 10000; k++) { } } } long end = System.</description></item><item><title>25_红黑树（上）：为什么工程中都用红黑树这种二叉树？</title><link>https://artisanbox.github.io/2/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/26/</guid><description>上两节，我们依次讲了树、二叉树、二叉查找树。二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是O(logn)。
不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于log2n的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到O(n)。我上一节说了，要解决这个复杂度退化的问题，我们需要设计一种平衡二叉查找树，也就是今天要讲的这种数据结构。
很多书籍里，但凡讲到平衡二叉查找树，就会拿红黑树作为例子。不仅如此，如果你有一定的开发经验，你会发现，在工程中，很多用到平衡二叉查找树的地方都会用红黑树。你有没有想过，为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？
带着这个问题，让我们一起来学习今天的内容吧！
什么是“平衡二叉查找树”？平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于1。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。
平衡二叉查找树不仅满足上面平衡二叉树的定义，还满足二叉查找树的特点。最先被发明的平衡二叉查找树是AVL树，它严格符合我刚讲到的平衡二叉查找树的定义，即任何节点的左右子树高度相差不超过1，是一种高度平衡的二叉查找树。
但是很多平衡二叉查找树其实并没有严格符合上面的定义（树中任意一个节点的左右子树的高度相差不能大于1），比如我们下面要讲的红黑树，它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。
我们学习数据结构和算法是为了应用到实际的开发中的，所以，我觉得没必去死抠定义。对于平衡二叉查找树这个概念，我觉得我们要从这个数据结构的由来，去理解“平衡”的意思。
发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。
所以，平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。
所以，如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比log2n大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。
如何定义一棵“红黑树”？平衡二叉查找树其实有很多，比如，Splay Tree（伸展树）、Treap（树堆）等，但是我们提到平衡二叉查找树，听到的基本都是红黑树。它的出镜率甚至要高于“平衡二叉查找树”这几个字，有时候，我们甚至默认平衡二叉查找树就是红黑树，那我们现在就来看看这个“明星树”。
红黑树的英文是“Red-Black Tree”，简称R-B Tree。它是一种不严格的平衡二叉查找树，我前面说了，它的定义是不严格符合平衡二叉查找树的定义的。那红黑树究竟是怎么定义的呢？
顾名思义，红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：
根节点是黑色的；
每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；
这里的第二点要求“叶子节点都是黑色的空节点”，稍微有些奇怪，它主要是为了简化红黑树的代码实现而设置的，下一节我们讲红黑树的实现的时候会讲到。这节我们暂时不考虑这一点，所以，在画图和讲解的时候，我将黑色的、空的叶子节点都省略掉了。
为了让你更好地理解上面的定义，我画了两个红黑树的图例，你可以对照着看下。
为什么说红黑树是“近似平衡”的？我们前面也讲到，平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化得太严重。
我们在上一节讲过，二叉查找树很多操作的性能都跟树的高度成正比。一棵极其平衡的二叉树（满二叉树或完全二叉树）的高度大约是log2n，所以如果要证明红黑树是近似平衡的，我们只需要分析，红黑树的高度是否比较稳定地趋近log2n就好了。
红黑树的高度不是很好分析，我带你一步一步来推导。
首先，我们来看，如果我们将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？
红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。
前面红黑树的定义里有这么一条：从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。
上一节我们说，完全二叉树的高度近似log2n，这里的四叉“黑树”的高度要低于完全二叉树，所以去掉红色节点的“黑树”的高度也不会超过log2n。
我们现在知道只包含黑色节点的“黑树”的高度，那我们现在把红色节点加回去，高度会变成多少呢？
从上面我画的红黑树的例子和定义看，在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过log2n，所以加入红色节点之后，最长路径不会超过2log2n，也就是说，红黑树的高度近似2log2n。
所以，红黑树的高度只比高度平衡的AVL树的高度（log2n）仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。
解答开篇我们刚刚提到了很多平衡二叉查找树，现在我们就来看下，为什么在工程中大家都喜欢用红黑树这种平衡二叉查找树？
我们前面提到Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。
AVL树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用AVL树的代价就有点高了。
红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比AVL树要低。
所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。
内容小结很多同学都觉得红黑树很难，的确，它算是最难掌握的一种数据结构。其实红黑树最难的地方是它的实现，我们今天还没有涉及，下一节我会专门来讲。
不过呢，我认为，我们其实不应该把学习的侧重点，放到它的实现上。那你可能要问了，关于红黑树，我们究竟需要掌握哪些东西呢？
还记得我多次说过的观点吗？我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。对于红黑树，也不例外。你如果能搞懂这几个问题，其实就已经足够了。
红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是O(logn)。
因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。
课后思考动态数据结构支持动态的数据插入、删除、查找操作，除了红黑树，我们前面还学习过哪些呢？能对比一下各自的优势、劣势，以及应用场景吗？
欢迎留言和我分享，我会第一时间给你反馈。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>25｜增强编译器前端功能第4步：综合运用多种语义分析技术</title><link>https://artisanbox.github.io/3/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/27/</guid><description>你好，我是宫文学。
在上一节课，我们比较全面地分析了怎么用集合运算的算法思路实现类型计算。不过，在实际的语义分析过程中，我们往往需要综合运用多种技术。
不知道你还记不记得，我们上一节课举了一个例子，里面涉及了数据流分析和类型计算技术。不过这还不够，今天这节课，我们还要多举几个例子，来看看如何综合运用各种技术来达到语义分析的目的。在这个过程中，你还会加深对类型计算的理解、了解常量折叠和常量传播技术，以及实现更精准的类型推导。
好，我们首先接着上一节课的思路，看一看怎么把数据流分析与类型计算结合起来。
在类型计算中使用数据流分析技术我们再用一下上节课的示例程序foo7。在这个程序中，age的类型是number|null，age1的类型是string|number。我们先让age=18，这时候把age赋给age1是合法的。之后又给age赋值为null，然后再把age赋给age1，这时编译器就会报错。
function foo7(age : number|null){ let age1 : string|number; age = 18; //age的值域现在变成了一个值类型：18 age1 = age; //OK age = null; //age的值域现在变成了null age1 = age; //错误！ console.log(age1); } 在这个过程中，age的值域是动态变化的。在这里，我用了“值域”这个词。它其实跟类型是同一个意思。我这里用值域这个词，是强调动态变化的特征。毕竟，如果说到类型，你通常会觉得变量的类型是不变的。如果你愿意，也可以直接把它叫做类型。
你马上就会想到，数据流分析技术很擅长处理这种情况。具体来说，就是在扫描程序代码的过程中，某个值会不断地变化。
提到数据流分析，那自然我们就要先来识别它的5大关键要素了。我们来分析一下。
首先是分析方向。这个场景中，分析方向显然是自上而下的。
第二，是数据流分析针对的变量。在这个场景中，我们需要分析的是变量的值域。所以，我用了一个varRanges变量，来保存每个变量的值域。varRanges是一个map，每个变量在里面有一个key。
varRanges:Map&amp;lt;VarSymbol, Type&amp;gt; = new Map(); 第三，我们要确定varRanges的初始值。在这个例子中，每个变量的值域的初始值就是它原来的类型。比如age一开始的值域就是number|null。
第四，我们要确定转换函数，也就是在什么情况下，变量的值域会发生变化。在当前的例子中，我们只需要搞清楚变量赋值的情况就可以了。如果我们要在变量声明中进行初始化，那也可以看做是变量赋值。
在变量赋值时，如果=号右边的值是一个常量，那么变量的值域都会变成一个值对象，这种情况我们已经在前一节课分析过了。
那如果=号右边的值不是常量，而是另一个变量呢？比如下面一个例子foo10，x的类型是number|string，y的类型是string。然后把y赋给x。我相信你也看出来，现在x的值域就应该跟y的一样了，都是string。
function foo10(x : number|string, y : string){ x = y; //x的值域变成了string if (typeof x == 'string'){ //其实这个条件一定为true println("x is string"); } } 研究一下这个例子，你会发现通过赋值操作，我们把x的值域收窄了。在TypeScript的文档中，这被叫做"Narrowing"。翻译成汉语的话，我们姑且称之为“窄化”吧。
不过，除了赋值语句，还有其他情况可以让变量的值域窄化，包括使用typeof运算符、真值判断、等值判断、instanceof运算符，以及使用类型断言等等。其中最后两种方法，涉及到对象，我们目前还没有支持对象特性，所以先不讨论了。我们就讨论一下typeof运算符、真值判断和等值判断这三种情况。</description></item><item><title>26_Superscalar和VLIW：如何让CPU的吞吐率超过1？</title><link>https://artisanbox.github.io/4/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/26/</guid><description>到今天为止，专栏已经过半了。过去的20多讲里，我给你讲的内容，很多都是围绕着怎么提升CPU的性能这个问题展开的。
我们先回顾一下第4讲，不知道你是否还记得这个公式：
程序的CPU执行时间 = 指令数 × CPI × Clock Cycle Time这个公式里，有一个叫CPI的指标。我们知道，CPI的倒数，又叫作IPC（Instruction Per Clock），也就是一个时钟周期里面能够执行的指令数，代表了CPU的吞吐率。那么，这个指标，放在我们前面几节反复优化流水线架构的CPU里，能达到多少呢？
答案是，最佳情况下，IPC也只能到1。因为无论做了哪些流水线层面的优化，即使做到了指令执行层面的乱序执行，CPU仍然只能在一个时钟周期里面，取一条指令。
这说明，无论指令后续能优化得多好，一个时钟周期也只能执行完这样一条指令，CPI只能是1。但是，我们现在用的Intel CPU或者ARM的CPU，一般的CPI都能做到2以上，这是怎么做到的呢？
今天，我们就一起来看看，现代CPU都使用了什么“黑科技”。
多发射与超标量：同一时间执行的两条指令之前讲CPU的硬件组成的时候，我们把所有算术和逻辑运算都抽象出来，变成了一个ALU这样的“黑盒子”。你应该还记得第13讲到第16讲，关于加法器、乘法器、乃至浮点数计算的部分，其实整数的计算和浮点数的计算过程差异还是不小的。实际上，整数和浮点数计算的电路，在CPU层面也是分开的。
一直到80386，我们的CPU都是没有专门的浮点数计算的电路的。当时的浮点数计算，都是用软件进行模拟的。所以，在80386时代，Intel给386配了单独的387芯片，专门用来做浮点数运算。那个时候，你买386芯片的话，会有386sx和386dx这两种芯片可以选择。386dx就是带了387浮点数计算芯片的，而sx就是不带浮点数计算芯片的。
其实，我们现在用的Intel CPU芯片也是一样的。虽然浮点数计算已经变成CPU里的一部分，但并不是所有计算功能都在一个ALU里面，真实的情况是，我们会有多个ALU。这也是为什么，在第24讲讲乱序执行的时候，你会看到，其实指令的执行阶段，是由很多个功能单元（FU）并行（Parallel）进行的。
不过，在指令乱序执行的过程中，我们的取指令（IF）和指令译码（ID）部分并不是并行进行的。
既然指令的执行层面可以并行进行，为什么取指令和指令译码不行呢？如果想要实现并行，该怎么办呢？
其实只要我们把取指令和指令译码，也一样通过增加硬件的方式，并行进行就好了。我们可以一次性从内存里面取出多条指令，然后分发给多个并行的指令译码器，进行译码，然后对应交给不同的功能单元去处理。这样，我们在一个时钟周期里，能够完成的指令就不只一条了。IPC也就能做到大于1了。
这种CPU设计，我们叫作多发射（Mulitple Issue）和超标量（Superscalar）。
什么叫多发射呢？这个词听起来很抽象，其实它意思就是说，我们同一个时间，可能会同时把多条指令发射（Issue）到不同的译码器或者后续处理的流水线中去。
在超标量的CPU里面，有很多条并行的流水线，而不是只有一条流水线。“超标量“这个词是说，本来我们在一个时钟周期里面，只能执行一个标量（Scalar）的运算。在多发射的情况下，我们就能够超越这个限制，同时进行多次计算。
你可以看我画的这个超标量设计的流水线示意图。仔细看，你应该能看到一个有意思的现象，每一个功能单元的流水线的长度是不同的。事实上，不同的功能单元的流水线长度本来就不一样。我们平时所说的14级流水线，指的通常是进行整数计算指令的流水线长度。如果是浮点数运算，实际的流水线长度则会更长一些。
Intel的失败之作：安腾的超长指令字设计无论是之前几讲里讲的乱序执行，还是现在更进一步的超标量技术，在实际的硬件层面，其实实施起来都挺麻烦的。这是因为，在乱序执行和超标量的体系里面，我们的CPU要解决依赖冲突的问题。这也就是前面几讲我们讲的冒险问题。
CPU需要在指令执行之前，去判断指令之间是否有依赖关系。如果有对应的依赖关系，指令就不能分发到执行阶段。因为这样，上面我们所说的超标量CPU的多发射功能，又被称为动态多发射处理器。这些对于依赖关系的检测，都会使得我们的CPU电路变得更加复杂。
于是，计算机科学家和工程师们就又有了一个大胆的想法。我们能不能不把分析和解决依赖关系的事情，放在硬件里面，而是放到软件里面来干呢？
如果你还记得的话，我在第4讲也讲过，要想优化CPU的执行时间，关键就是拆解这个公式：
程序的CPU执行时间 = 指令数 × CPI × Clock Cycle Time当时我们说过，这个公式里面，我们可以通过改进编译器来优化指令数这个指标。那接下来，我们就来看看一个非常大胆的CPU设计想法，叫作超长指令字设计（Very Long Instruction Word，VLIW）。这个设计呢，不仅想让编译器来优化指令数，还想直接通过编译器，来优化CPI。
围绕着这个设计的，是Intel一个著名的“史诗级”失败，也就是著名的IA-64架构的安腾（Itanium）处理器。只不过，这一次，责任不全在Intel，还要拉上可以称之为硅谷起源的另一家公司，也就是惠普。
之所以称为“史诗”级失败，这个说法来源于惠普最早给这个架构取的名字，显式并发指令运算（Explicitly Parallel Instruction Computer），这个名字的缩写EPIC，正好是“史诗”的意思。
好巧不巧，安腾处理器和和我之前给你介绍过的Pentium 4一样，在市场上是一个失败的产品。在经历了12年之久的设计研发之后，安腾一代只卖出了几千套。而安腾二代，在从2002年开始反复挣扎了16年之后，最终在2018年被Intel宣告放弃，退出了市场。自此，世上再也没有这个“史诗”服务器了。
那么，我们就来看看，这个超长指令字的安腾处理器是怎么回事儿。
在乱序执行和超标量的CPU架构里，指令的前后依赖关系，是由CPU内部的硬件电路来检测的。而到了超长指令字的架构里面，这个工作交给了编译器这个软件。
我从专栏第5讲开始，就给你看了不少C代码到汇编代码和机器代码的对照。编译器在这个过程中，其实也能够知道前后数据的依赖。于是，我们可以让编译器把没有依赖关系的代码位置进行交换。然后，再把多条连续的指令打包成一个指令包。安腾的CPU就是把3条指令变成一个指令包。
CPU在运行的时候，不再是取一条指令，而是取出一个指令包。然后，译码解析整个指令包，解析出3条指令直接并行运行。可以看到，使用超长指令字架构的CPU，同样是采用流水线架构的。也就是说，一组（Group）指令，仍然要经历多个时钟周期。同样的，下一组指令并不是等上一组指令执行完成之后再执行，而是在上一组指令的指令译码阶段，就开始取指令了。
值得注意的一点是，流水线停顿这件事情在超长指令字里面，很多时候也是由编译器来做的。除了停下整个处理器流水线，超长指令字的CPU不能在某个时钟周期停顿一下，等待前面依赖的操作执行完成。编译器需要在适当的位置插入NOP操作，直接在编译出来的机器码里面，就把流水线停顿这个事情在软件层面就安排妥当。
虽然安腾的设想很美好，Intel也曾经希望能够让安腾架构成为替代x86的新一代架构，但是最终安腾还是在前前后后折腾将近30年后失败了。2018年，Intel宣告安腾9500会在2021年停止供货。
安腾失败的原因有很多，其中有一个重要的原因就是“向前兼容”。
一方面，安腾处理器的指令集和x86是不同的。这就意味着，原来x86上的所有程序是没有办法在安腾上运行的，而需要通过编译器重新编译才行。
另一方面，安腾处理器的VLIW架构决定了，如果安腾需要提升并行度，就需要增加一个指令包里包含的指令数量，比方说从3个变成6个。一旦这么做了，虽然同样是VLIW架构，同样指令集的安腾CPU，程序也需要重新编译。因为原来编译器判断的依赖关系是在3个指令以及由3个指令组成的指令包之间，现在要变成6个指令和6个指令组成的指令包。编译器需要重新编译，交换指令顺序以及NOP操作，才能满足条件。甚至，我们需要重新来写编译器，才能让程序在新的CPU上跑起来。
于是，安腾就变成了一个既不容易向前兼容，又不容易向后兼容的CPU。那么，它的失败也就不足为奇了。
可以看到，技术思路上的先进想法，在实际的业界应用上会遇到更多具体的实践考验。无论是指令集向前兼容性，还是对应CPU未来的扩展，在设计的时候，都需要更多地去考虑实践因素。
总结延伸这一讲里，我和你一起向CPU的性能发起了一个新的挑战：让CPU的吞吐率，也就是IPC能够超过1。
我先是为你介绍了超标量，也就是Superscalar这个方法。超标量可以让CPU不仅在指令执行阶段是并行的，在取指令和指令译码的时候，也是并行的。通过超标量技术，可以使得你所使用的CPU的IPC超过1。
在Intel的x86的CPU里，从Pentium时代，第一次开始引入超标量技术，整个CPU的性能上了一个台阶。对应的技术，一直沿用到了现在。超标量技术和你之前看到的其他流水线技术一样，依赖于在硬件层面，能够检测到对应的指令的先后依赖关系，解决“冒险”问题。所以，它也使得CPU的电路变得更复杂了。
因为这些复杂性，惠普和Intel又共同推出了著名的安腾处理器。通过在编译器层面，直接分析出指令的前后依赖关系。于是，硬件在代码编译之后，就可以直接拿到调换好先后顺序的指令。并且这些指令中，可以并行执行的部分，会打包在一起组成一个指令包。安腾处理器在取指令和指令译码的时候，拿到的不再是单个指令，而是这样一个指令包。并且在指令执行阶段，可以并行执行指令包里所有的指令。
虽然看起来，VLIW在技术层面更具有颠覆性，不仅仅只是一个硬件层面的改造，而且利用了软件层面的编译器，来组合解决提升CPU指令吞吐率的问题。然而，最终VLIW却没有得到市场和业界的认可。
惠普和Intel强强联合开发的安腾处理器命运多舛。从1989开始研发，直到2001年才发布了第一代安腾处理器。然而12年的开发过程后，第一代安腾处理器最终只卖出了几千套。而2002年发布的安腾2处理器，也没能拯救自己的命运。最终在2018年，Intel宣布安腾退出市场。自此之后，市面上再没有能够大规模商用的VLIW架构的处理器了。</description></item><item><title>26_备库为什么会延迟好几个小时？</title><link>https://artisanbox.github.io/1/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/26/</guid><description>在上一篇文章中，我和你介绍了几种可能导致备库延迟的原因。你会发现，这些场景里，不论是偶发性的查询压力，还是备份，对备库延迟的影响一般是分钟级的，而且在备库恢复正常以后都能够追上来。
但是，如果备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成了小时级别。而且对于一个压力持续比较高的主库来说，备库很可能永远都追不上主库的节奏。
这就涉及到今天我要给你介绍的话题：备库并行复制能力。
为了便于你理解，我们再一起看一下第24篇文章《MySQL是怎么保证主备一致的？》的主备流程图。
图1 主备流程图谈到主备的并行复制能力，我们要关注的是图中黑色的两个箭头。一个箭头代表了客户端写入主库，另一箭头代表的是备库上sql_thread执行中转日志（relay log）。如果用箭头的粗细来代表并行度的话，那么真实情况就如图1所示，第一个箭头要明显粗于第二个箭头。
在主库上，影响并发度的原因就是各种锁了。由于InnoDB引擎支持行锁，除了所有并发事务都在更新同一行（热点行）这种极端场景外，它对业务并发度的支持还是很友好的。所以，你在性能测试的时候会发现，并发压测线程32就比单线程时，总体吞吐量高。
而日志在备库上的执行，就是图中备库上sql_thread更新数据(DATA)的逻辑。如果是用单线程的话，就会导致备库应用日志不够快，造成主备延迟。
在官方的5.6版本之前，MySQL只支持单线程复制，由此在主库并发高、TPS高时就会出现严重的主备延迟问题。
从单线程复制到最新版本的多线程复制，中间的演化经历了好几个版本。接下来，我就跟你说说MySQL多线程复制的演进过程。
其实说到底，所有的多线程复制机制，都是要把图1中只有一个线程的sql_thread，拆成多个线程，也就是都符合下面的这个模型：
图2 多线程模型图2中，coordinator就是原来的sql_thread, 不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了worker线程。而work线程的个数，就是由参数slave_parallel_workers决定的。根据我的经验，把这个值设置为8~16之间最好（32核物理机的情况），毕竟备库还有可能要提供读查询，不能把CPU都吃光了。
接下来，你需要先思考一个问题：事务能不能按照轮询的方式分发给各个worker，也就是第一个事务分给worker_1，第二个事务发给worker_2呢？
其实是不行的。因为，事务被分发给worker以后，不同的worker就独立执行了。但是，由于CPU的调度策略，很可能第二个事务最终比第一个事务先执行。而如果这时候刚好这两个事务更新的是同一行，也就意味着，同一行上的两个事务，在主库和备库上的执行顺序相反，会导致主备不一致的问题。
接下来，请你再设想一下另外一个问题：同一个事务的多个更新语句，能不能分给不同的worker来执行呢？
答案是，也不行。举个例子，一个事务更新了表t1和表t2中的各一行，如果这两条更新语句被分到不同worker的话，虽然最终的结果是主备一致的，但如果表t1执行完成的瞬间，备库上有一个查询，就会看到这个事务“更新了一半的结果”，破坏了事务逻辑的隔离性。
所以，coordinator在分发的时候，需要满足以下这两个基本要求：
不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个worker中。
同一个事务不能被拆开，必须放到同一个worker中。
各个版本的多线程复制，都遵循了这两条基本原则。接下来，我们就看看各个版本的并行复制策略。
MySQL 5.5版本的并行复制策略官方MySQL 5.5版本是不支持并行复制的。但是，在2012年的时候，我自己服务的业务出现了严重的主备延迟，原因就是备库只有单线程复制。然后，我就先后写了两个版本的并行策略。
这里，我给你介绍一下这两个版本的并行策略，即按表分发策略和按行分发策略，以帮助你理解MySQL官方版本并行复制策略的迭代。
按表分发策略按表分发事务的基本思路是，如果两个事务更新不同的表，它们就可以并行。因为数据是存储在表里的，所以按表分发，可以保证两个worker不会更新同一行。
当然，如果有跨表的事务，还是要把两张表放在一起考虑的。如图3所示，就是按表分发的规则。
图3 按表并行复制程模型可以看到，每个worker线程对应一个hash表，用于保存当前正在这个worker的“执行队列”里的事务所涉及的表。hash表的key是“库名.表名”，value是一个数字，表示队列中有多少个事务修改这个表。
在有事务分配给worker时，事务里面涉及的表会被加到对应的hash表中。worker执行完成后，这个表会被从hash表中去掉。
图3中，hash_table_1表示，现在worker_1的“待执行事务队列”里，有4个事务涉及到db1.t1表，有1个事务涉及到db2.t2表；hash_table_2表示，现在worker_2中有一个事务会更新到表t3的数据。
假设在图中的情况下，coordinator从中转日志中读入一个新事务T，这个事务修改的行涉及到表t1和t3。
现在我们用事务T的分配流程，来看一下分配规则。
由于事务T中涉及修改表t1，而worker_1队列中有事务在修改表t1，事务T和队列中的某个事务要修改同一个表的数据，这种情况我们说事务T和worker_1是冲突的。
按照这个逻辑，顺序判断事务T和每个worker队列的冲突关系，会发现事务T跟worker_2也冲突。
事务T跟多于一个worker冲突，coordinator线程就进入等待。
每个worker继续执行，同时修改hash_table。假设hash_table_2里面涉及到修改表t3的事务先执行完成，就会从hash_table_2中把db1.t3这一项去掉。
这样coordinator会发现跟事务T冲突的worker只有worker_1了，因此就把它分配给worker_1。
coordinator继续读下一个中转日志，继续分配事务。
也就是说，每个事务在分发的时候，跟所有worker的冲突关系包括以下三种情况：
如果跟所有worker都不冲突，coordinator线程就会把这个事务分配给最空闲的woker;
如果跟多于一个worker冲突，coordinator线程就进入等待状态，直到和这个事务存在冲突关系的worker只剩下1个；
如果只跟一个worker冲突，coordinator线程就会把这个事务分配给这个存在冲突关系的worker。</description></item><item><title>26_红黑树（下）：掌握这些技巧，你也可以实现一个红黑树</title><link>https://artisanbox.github.io/2/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/27/</guid><description>红黑树是一个让我又爱又恨的数据结构，“爱”是因为它稳定、高效的性能，“恨”是因为实现起来实在太难了。我今天讲的红黑树的实现，对于基础不太好的同学，理解起来可能会有些困难。但是，我觉得没必要去死磕它。
我为什么这么说呢？因为，即便你将左右旋背得滚瓜烂熟，我保证你过不几天就忘光了。因为，学习红黑树的代码实现，对于你平时做项目开发没有太大帮助。对于绝大部分开发工程师来说，这辈子你可能都用不着亲手写一个红黑树。除此之外，它对于算法面试也几乎没什么用，一般情况下，靠谱的面试官也不会让你手写红黑树的。
如果你对数据结构和算法很感兴趣，想要开拓眼界、训练思维，我还是很推荐你看一看这节的内容。但是如果学完今天的内容你还觉得懵懵懂懂的话，也不要纠结。我们要有的放矢去学习。你先把平时要用的、基础的东西都搞会了，如果有余力了，再来深入地研究这节内容。
好，我们现在就进入正式的内容。上一节，我们讲到红黑树定义的时候，提到红黑树的叶子节点都是黑色的空节点。当时我只是粗略地解释了，这是为了代码实现方便，那更加确切的原因是什么呢？ 我们这节就来说一说。
实现红黑树的基本思想不知道你有没有玩过魔方？其实魔方的复原解法是有固定算法的：遇到哪几面是什么样子，对应就怎么转几下。你只要跟着这个复原步骤，就肯定能将魔方复原。
实际上，红黑树的平衡过程跟魔方复原非常神似，大致过程就是：遇到什么样的节点排布，我们就对应怎么去调整。只要按照这些固定的调整规则来操作，就能将一个非平衡的红黑树调整成平衡的。
还记得我们前面讲过的红黑树的定义吗？今天的内容里，我们会频繁用到它，所以，我们现在再来回顾一下。一棵合格的红黑树需要满足这样几个要求：
根节点是黑色的；
每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点。
在插入、删除节点的过程中，第三、第四点要求可能会被破坏，而我们今天要讲的“平衡调整”，实际上就是要把被破坏的第三、第四点恢复过来。
在正式开始之前，我先介绍两个非常重要的操作，左旋（rotate left）、右旋（rotate right）。左旋全称其实是叫围绕某个节点的左旋，那右旋的全称估计你已经猜到了，就叫围绕某个节点的右旋。
我们下面的平衡调整中，会一直用到这两个操作，所以我这里画了个示意图，帮助你彻底理解这两个操作。图中的a，b，r表示子树，可以为空。
前面我说了，红黑树的插入、删除操作会破坏红黑树的定义，具体来说就是会破坏红黑树的平衡，所以，我们现在就来看下，红黑树在插入、删除数据之后，如何调整平衡，继续当一棵合格的红黑树的。
插入操作的平衡调整首先，我们来看插入操作。
红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。所以，关于插入操作的平衡调整，有这样两种特殊情况，但是也都非常好处理。
如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。
如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。
除此之外，其他情况都会违背红黑树的定义，于是我们就需要进行调整，调整的过程包含两种基础的操作：左右旋转和改变颜色。
红黑树的平衡调整过程是一个迭代的过程。我们把正在处理的节点叫做关注节点。关注节点会随着不停地迭代处理，而不断发生变化。最开始的关注节点就是新插入的节点。
新节点插入之后，如果红黑树的平衡被打破，那一般会有下面三种情况。我们只需要根据每种情况的特点，不停地调整，就可以让红黑树继续符合定义，也就是继续保持平衡。
我们下面依次来看每种情况的调整过程。提醒你注意下，为了简化描述，我把父节点的兄弟节点叫做叔叔节点，父节点的父节点叫做祖父节点。
CASE 1：如果关注节点是a，它的叔叔节点d是红色，我们就依次执行下面的操作：
将关注节点a的父节点b、叔叔节点d的颜色都设置成黑色；
将关注节点a的祖父节点c的颜色设置成红色；
关注节点变成a的祖父节点c；
跳到CASE 2或者CASE 3。
CASE 2：如果关注节点是a，它的叔叔节点d是黑色，关注节点a是其父节点b的右子节点，我们就依次执行下面的操作：
关注节点变成节点a的父节点b；
围绕新的关注节点b左旋；
跳到CASE 3。</description></item><item><title>26｜增强更丰富的类型第1步：如何支持浮点数？</title><link>https://artisanbox.github.io/3/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/28/</guid><description>你好，我是宫文学。
我们前面几节课，讲的都是编译器前端的功能。虽然，要实现完善的前端功能，我们要做的工作还有很多。不过，我们现在已经不“虚”了！因为我们已经把编译器前端部分的主要知识点都讲得差不多了，其他的我们可以慢慢完善。
所以，现在我们重新把精力放回到编译器后端功能和运行时上来，这部分的功能我们还有待加强。在第一部分起步篇中，为了尽量简化实现过程，我们的语言只支持了整数的运算，甚至都没区分整型的长度，统一使用了32位的整型。
但这在实用级的语言中可行不通，我们还需要在里面添加各种丰富的数据类型。所以，接下来，我们会花几节课的时间，丰富一下我们语言支持的数据类型。首先我们会添加一些内置的基础类型，比如浮点型、字符串和数组。之后，我们还要通过对面向对象编程特性，支持用户自定义自己的类型。
在这一节课，我们先来看一下如何让我们的语言支持浮点型数据。为实现这个目的，我们需要先了解CPU为了支持浮点数有哪些特别的设计，ABI方面又有一些什么规定，以及如何修改汇编代码生成逻辑。而且，为了正确地在汇编代码中表示浮点型字面量，你还会学到浮点数编码方面的国际标准。
首先，让我们了解一下CPU硬件和ABI对浮点数运算提供的支持。
CPU和ABI对浮点数运算的支持我们先来回顾一下起步篇中关于整数运算的知识。已经有些日子没见到它们了，不知道你还记不记得？你可以和下面我们重点讲解的浮点数的处理模式对比着来看，看看它们有怎样的不同，这也能加强你对这些重点知识的记忆。
X86架构的CPU在64位模式下对整数运算的支持，最重要的就是这两个知识点：
寄存器。整数运算可以使用16个通用寄存器； 指令。对整数进行加减乘除的指令分别是addl、subl、imull和idivl。 另外，ABI也针对整数运算做了一些规定，比如：
参数传递。根据ABI，参数传递过程中会使用6个寄存器。超过6个参数，则放在调用者的栈桢里； 寄存器保护。有一些寄存器要能够跨函数调用保存数据，也就是说函数调用者需要保护这些寄存器，而另一些寄存器则不需要保护； 返回值。根据ABI，从函数中返回整数值时，使用的是%eax寄存器； 栈桢结构。ABI对于栈桢里存放参数、返回地址做了规定，并且规定栈桢需要16字节内存对齐，还规定了如何使用栈桢外的红区等等。 那么，我们再来看看CPU对浮点数运算的支持是怎么样的。
其实，最早期的X86CPU只支持整数运算，并不支持浮点数运算。如果我们要进行浮点数运算，就要用整数运算来模拟。但是这样的话，浮点数运算的速度就会比较慢。
而现代CPU解决了这个问题，普遍从硬件层面进行浮点运算，所以编译器也要直接生成浮点运算的机器码，最大程度地发挥硬件的性能。
我们之前说过，一款CPU可能支持多个指令集。而某些指令集，就是用于支持浮点数计算的。在X86的历史上，CPU最早是通过一个协处理器来处理浮点数运算，这个协处理器叫做FPU（浮点处理单元），它采用的指令集叫做X87。后来这个协处理器就被整合到CPU中了。
再后来，为了提高对多媒体数据的处理能力，厂商往CPU里增加了新的指令集，叫做MMX指令集。MMX的具体含义，有人说是多媒体扩展（MultiMedia eXtension），有人说是矩阵数学扩展（Matrix Math eXtension）。不管缩写的含义是什么，MMX主要就是增强了对浮点数的处理能力，因为多媒体的处理主要就是浮点数运算。
并且，MMX还属于SIMD类型的指令集。SIMD（Single Instruction Multiple Data）是一条指令对多个数据完成加减乘数运算的意思，因此MMX指令能让CPU的处理效率更高。
MMX指令集后来又升级成为了SSE指令集，还形成了多个版本，每个版本都会增加一些新的指令和功能。最新的版本是SSE4.2。SSE是流式SIMD扩展（Streaming SIMD Extensions）的意思。到今天，X86计算机进行浮点数运算的时候，基本上都是采用SSE指令集，不再使用x87指令集，除非是使用那些特别早的型号的CPU。
不知道你还记不记得，我们之前提过，你可以查询自己电脑的CPU所支持的指令集。在macOS上，我用下面的命令就可以查到：
sysctl machdep.cpu.features machdep.cpu.leaf7_features 然后你会在命令行终端，得到关于CPU特性的信息。这些特性就对应着指令集。比如，出现在第一个的FPU，就对应着X87指令集。你也会从其中看到多个版本的SSE指令集。
如果你嫌上面的命令太长，那也可以使用一个短一点的命令。这个命令会打印出更多关于CPU的信息，比如CPU所支持的线程数，等等。其中也包括该CPU的指令集。
sysctl machdep.cpu 这里我插一个小知识点，不知道你会不会有这个疑惑，我们操作系统是怎么知道某CPU支持哪些指令集的呢？原来，X86架构的CPU提供了一个cpuid指令。你用这个指令就可以得到CPU类型、型号、制造商信息、商标信息、序列号、缓存，还有支持特性（也就是指令集）等一系列信息了。所以你看，要理解软件的功能，经常都需要底层硬件架构的知识。
好了，既然我们需要用到SSE指令集，那就需要了解一下SSE指令集的特点。并且，SSE其实不仅能处理浮点数，还能处理整数。不过现在我们主要关心与浮点数有关的特性。这些信息从哪里获得呢？当然是从Intel的手册。下面这些信息就来自于《Intel® 64 and IA-32 Architectures Software Developer’s Manual，Volume 1: Basic Architecture》，我给你稍微总结一下。
首先，我们看看SSE指令所使用的寄存器。
在64位模式下，SSE可以使用16个128位的寄存器，分别叫做xmm0~xmm15。
此外，SSE还会使用一个32位的MXCSR寄存器，用于保存浮点数运算时的控制信息和状态信息。比如，如果你做除法的时候，除数是0，那么就会触发一个异常。而MXCSR寄存器上的某个标志位会决定如何处理该异常：是采用内置的标准方法来处理呢，还是触发一个软件异常来处理。关于MXCSR的详细信息，你可以按需要查看一下手册。
第二，我们看一下SSE对数据类型的支持。
SSE指令支持32位的单精度数，也支持64位的双精度数。不过，单精度数和双精度数的格式，都遵循IEEE 754标准。
在SSE指令中，寄存器里可以只放一个浮点数，这个时候我们把它叫做标量（Scalar）。还可以把多个浮点数打包放在一个寄存器里，这种数据格式叫做打包格式（&amp;nbsp;Packed Data Types），或者叫做向量格式。下图就显示了在一个128位寄存器里存放4个单精度浮点数的情况。
打包格式是用于SIMD类型的指令的，这样一条指令就能处理寄存器里的4个单精度浮点数的计算。不过，我们关注的还是对标量数据的处理，所以就先忽略向量数据处理的情况，有需要我们再补充。
第三，我们看看SSE指令的情况。
SSE对处理浮点数的指令，包括向量指令和标量指令。另外，在JavaScript中，number是以双精度数来表示的，所以我们的语言也就可以忽略与单精度浮点数有关的指令，直接关注双精度浮点数指令就好了。
我在下面这张表中，列出了SSE中与标量的、双精度浮点数处理有关的一些主要的指令：
你能看到，其实这些指令数量也并不太多，很容易掌握。当然，SSE完整的指令还是不少的。SSE针对向量数据处理、整型数据处理都有单独的指令，还有一些指令是用于管理MXCSR寄存器的状态，以及对高速缓存进行管理的。如果你想了解这些，可以阅读Intel手册的第二卷：《Intel® 64 and IA-32 architectures software developer’s manual combined volumes 2A, 2B, 2C, and&amp;nbsp; 2D: Instruction set reference, A- Z》</description></item><item><title>27_SIMD：如何加速矩阵乘法？</title><link>https://artisanbox.github.io/4/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/27/</guid><description>上一讲里呢，我进一步为你讲解了CPU里的“黑科技”，分别是超标量（Superscalar）技术和超长指令字（VLIW）技术。
超标量（Superscalar）技术能够让取指令以及指令译码也并行进行；在编译的过程，超长指令字（VLIW）技术可以搞定指令先后的依赖关系，使得一次可以取一个指令包。
不过，CPU里的各种神奇的优化我们还远远没有说完。这一讲里，我就带你一起来看看，专栏里最后两个提升CPU性能的架构设计。它们分别是，你应该常常听说过的超线程（Hyper-Threading）技术，以及可能没有那么熟悉的单指令多数据流（SIMD）技术。
超线程：Intel多卖给你的那一倍CPU不知道你是不是还记得，在第21讲，我给你介绍了Intel是怎么在Pentium 4处理器上遭遇重大失败的。如果不太记得的话，你可以回过头去回顾一下。
那时我和你说过，Pentium 4失败的一个重要原因，就是它的CPU的流水线级数太深了。早期的Pentium 4的流水线深度高达20级，而后期的代号为Prescott的Pentium 4的流水线级数，更是到了31级。超长的流水线，使得之前我们讲的很多解决“冒险”、提升并发的方案都用不上。
因为这些解决“冒险”、提升并发的方案，本质上都是一种指令级并行（Instruction-level parallelism，简称IPL）的技术方案。换句话说就是，CPU想要在同一个时间，去并行地执行两条指令。而这两条指令呢，原本在我们的代码里，是有先后顺序的。无论是我们在流水线里面讲到的流水线架构、分支预测以及乱序执行，还是我们在上一讲说的超标量和超长指令字，都是想要通过同一时间执行两条指令，来提升CPU的吞吐率。
然而在Pentium 4这个CPU上，这些方法都可能因为流水线太深，而起不到效果。我之前讲过，更深的流水线意味着同时在流水线里面的指令就多，相互的依赖关系就多。于是，很多时候我们不得不把流水线停顿下来，插入很多NOP操作，来解决这些依赖带来的“冒险”问题。
不知道是不是因为当时面临的竞争太激烈了，为了让Pentium 4的CPU在性能上更有竞争力一点，2002年底，Intel在的3.06GHz主频的Pentium 4 CPU上，第一次引入了超线程（Hyper-Threading）技术。
什么是超线程技术呢？Intel想，既然CPU同时运行那些在代码层面有前后依赖关系的指令，会遇到各种冒险问题，我们不如去找一些和这些指令完全独立，没有依赖关系的指令来运行好了。那么，这样的指令哪里来呢？自然同时运行在另外一个程序里了。
你所用的计算机，其实同一个时间可以运行很多个程序。比如，我现在一边在浏览器里写这篇文章，后台同样运行着一个Python脚本程序。而这两个程序，是完全相互独立的。它们两个的指令完全并行运行，而不会产生依赖问题带来的“冒险”。
然而这个时候，你可能就会觉得奇怪了，这么做似乎不需要什么新技术呀。现在我们用的CPU都是多核的，本来就可以用多个不同的CPU核心，去运行不同的任务。即使当时的Pentium 4是单核的，我们的计算机本来也能同时运行多个进程，或者多个线程。这个超线程技术有什么特别的用处呢？
无论是上面说的多个CPU核心运行不同的程序，还是在单个CPU核心里面切换运行不同线程的任务，在同一时间点上，一个物理的CPU核心只会运行一个线程的指令，所以其实我们并没有真正地做到指令的并行运行。
超线程可不是这样。超线程的CPU，其实是把一个物理层面CPU核心，“伪装”成两个逻辑层面的CPU核心。这个CPU，会在硬件层面增加很多电路，使得我们可以在一个CPU核心内部，维护两个不同线程的指令的状态信息。
比如，在一个物理CPU核心内部，会有双份的PC寄存器、指令寄存器乃至条件码寄存器。这样，这个CPU核心就可以维护两条并行的指令的状态。在外面看起来，似乎有两个逻辑层面的CPU在同时运行。所以，超线程技术一般也被叫作同时多线程（Simultaneous Multi-Threading，简称SMT）技术。
不过，在CPU的其他功能组件上，Intel可不会提供双份。无论是指令译码器还是ALU，一个CPU核心仍然只有一份。因为超线程并不是真的去同时运行两个指令，那就真的变成物理多核了。超线程的目的，是在一个线程A的指令，在流水线里停顿的时候，让另外一个线程去执行指令。因为这个时候，CPU的译码器和ALU就空出来了，那么另外一个线程B，就可以拿来干自己需要的事情。这个线程B可没有对于线程A里面指令的关联和依赖。
这样，CPU通过很小的代价，就能实现“同时”运行多个线程的效果。通常我们只要在CPU核心的添加10%左右的逻辑功能，增加可以忽略不计的晶体管数量，就能做到这一点。
不过，你也看到了，我们并没有增加真的功能单元。所以超线程只在特定的应用场景下效果比较好。一般是在那些各个线程“等待”时间比较长的应用场景下。比如，我们需要应对很多请求的数据库应用，就很适合使用超线程。各个指令都要等待访问内存数据，但是并不需要做太多计算。
于是，我们就可以利用好超线程。我们的CPU计算并没有跑满，但是往往当前的指令要停顿在流水线上，等待内存里面的数据返回。这个时候，让CPU里的各个功能单元，去处理另外一个数据库连接的查询请求就是一个很好的应用案例。
我的移动工作站的CPU信息我这里放了一张我的电脑里运行CPU-Z的截图。你可以看到，在右下角里，我的CPU的Cores，被标明了是4，而Threads，则是8。这说明我手头的这个CPU，只有4个物理的CPU核心，也就是所谓的4核CPU。但是在逻辑层面，它“装作”有8个CPU核心，可以利用超线程技术，来同时运行8条指令。如果你用的是Windows，可以去下载安装一个CPU-Z来看看你手头的CPU里面对应的参数。
SIMD：如何加速矩阵乘法？在上面的CPU信息的图里面，你会看到，中间有一组信息叫作Instructions，里面写了有MMX、SSE等等。这些信息就是这个CPU所支持的指令集。这里的MMX和SSE的指令集，也就引出了我要给你讲的最后一个提升CPU性能的技术方案，SIMD，中文叫作单指令多数据流（Single Instruction Multiple Data）。
我们先来体会一下SIMD的性能到底怎么样。下面是两段示例程序，一段呢，是通过循环的方式，给一个list里面的每一个数加1。另一段呢，是实现相同的功能，但是直接调用NumPy这个库的add方法。在统计两段程序的性能的时候，我直接调用了Python里面的timeit的库。
$ python &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; import timeit &amp;gt;&amp;gt;&amp;gt; a = list(range(1000)) &amp;gt;&amp;gt;&amp;gt; b = np.array(range(1000)) &amp;gt;&amp;gt;&amp;gt; timeit.timeit(&amp;quot;[i + 1 for i in a]&amp;quot;, setup=&amp;quot;from __main__ import a&amp;quot;, number=1000000) 32.82800309999993 &amp;gt;&amp;gt;&amp;gt; timeit.timeit(&amp;quot;np.add(1, b)&amp;quot;, setup=&amp;quot;from __main__ import np, b&amp;quot;, number=1000000) 0.</description></item><item><title>27_主库出问题了，从库怎么办？</title><link>https://artisanbox.github.io/1/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/27/</guid><description>在前面的第24、25和26篇文章中，我和你介绍了MySQL主备复制的基础结构，但这些都是一主一备的结构。
大多数的互联网应用场景都是读多写少，因此你负责的业务，在发展过程中很可能先会遇到读性能的问题。而在数据库层解决读性能问题，就要涉及到接下来两篇文章要讨论的架构：一主多从。
今天这篇文章，我们就先聊聊一主多从的切换正确性。然后，我们在下一篇文章中再聊聊解决一主多从的查询逻辑正确性的方法。
如图1所示，就是一个基本的一主多从结构。
图1 一主多从基本结构图中，虚线箭头表示的是主备关系，也就是A和A’互为主备， 从库B、C、D指向的是主库A。一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担。
今天我们要讨论的就是，在一主多从架构下，主库故障后的主备切换问题。
如图2所示，就是主库发生故障，主备切换后的结果。
图2 一主多从基本结构--主备切换相比于一主一备的切换流程，一主多从结构在切换完成后，A’会成为新的主库，从库B、C、D也要改接到A’。正是由于多了从库B、C、D重新指向的这个过程，所以主备切换的复杂性也相应增加了。
接下来，我们再一起看看一个切换系统会怎么完成一主多从的主备切换过程。
基于位点的主备切换这里，我们需要先来回顾一个知识点。
当我们把节点B设置成节点A’的从库的时候，需要执行一条change master命令：
CHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password MASTER_LOG_FILE=$master_log_name MASTER_LOG_POS=$master_log_pos 这条命令有这么6个参数：
MASTER_HOST、MASTER_PORT、MASTER_USER和MASTER_PASSWORD四个参数，分别代表了主库A’的IP、端口、用户名和密码。 最后两个参数MASTER_LOG_FILE和MASTER_LOG_POS表示，要从主库的master_log_name文件的master_log_pos这个位置的日志继续同步。而这个位置就是我们所说的同步位点，也就是主库对应的文件名和日志偏移量。 那么，这里就有一个问题了，节点B要设置成A’的从库，就要执行change master命令，就不可避免地要设置位点的这两个参数，但是这两个参数到底应该怎么设置呢？
原来节点B是A的从库，本地记录的也是A的位点。但是相同的日志，A的位点和A’的位点是不同的。因此，从库B要切换的时候，就需要先经过“找同步位点”这个逻辑。
这个位点很难精确取到，只能取一个大概位置。为什么这么说呢？
我来和你分析一下看看这个位点一般是怎么获取到的，你就清楚其中不精确的原因了。
考虑到切换过程中不能丢数据，所以我们找位点的时候，总是要找一个“稍微往前”的，然后再通过判断跳过那些在从库B上已经执行过的事务。
一种取同步位点的方法是这样的：
等待新主库A’把中转日志（relay log）全部同步完成；
在A’上执行show master status命令，得到当前A’上最新的File 和 Position；
取原主库A故障的时刻T；
用mysqlbinlog工具解析A’的File，得到T时刻的位点。
mysqlbinlog File --stop-datetime=T --start-datetime=T 图3 mysqlbinlog 部分输出结果图中，end_log_pos后面的值“123”，表示的就是A’这个实例，在T时刻写入新的binlog的位置。然后，我们就可以把123这个值作为$master_log_pos ，用在节点B的change master命令里。
当然这个值并不精确。为什么呢？
你可以设想有这么一种情况，假设在T这个时刻，主库A已经执行完成了一个insert 语句插入了一行数据R，并且已经将binlog传给了A’和B，然后在传完的瞬间主库A的主机就掉电了。
那么，这时候系统的状态是这样的：
在从库B上，由于同步了binlog， R这一行已经存在；</description></item><item><title>27_递归树：如何借助树来求解递归算法的时间复杂度？</title><link>https://artisanbox.github.io/2/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/28/</guid><description>今天，我们来讲这种数据结构的一种特殊应用，递归树。
我们都知道，递归代码的时间复杂度分析起来很麻烦。我们在第12节《排序（下）》那里讲过，如何利用递推公式，求解归并排序、快速排序的时间复杂度，但是，有些情况，比如快排的平均时间复杂度的分析，用递推公式的话，会涉及非常复杂的数学推导。
除了用递推公式这种比较复杂的分析方法，有没有更简单的方法呢？今天，我们就来学习另外一种方法，借助递归树来分析递归算法的时间复杂度。
递归树与时间复杂度分析我们前面讲过，递归的思想就是，将大问题分解为小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止。
如果我们把这个一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫作递归树。我这里画了一棵斐波那契数列的递归树，你可以看看。节点里的数字表示数据的规模，一个节点的求解可以分解为左右子节点两个问题的求解。
通过这个例子，你对递归树的样子应该有个感性的认识了，看起来并不复杂。现在，我们就来看，如何用递归树来求解时间复杂度。
归并排序算法你还记得吧？它的递归实现代码非常简洁。现在我们就借助归并排序来看看，如何用递归树，来分析递归代码的时间复杂度。
归并排序的原理我就不详细介绍了，如果你忘记了，可以回看一下第12节的内容。归并排序每次会将数据规模一分为二。我们把归并排序画成递归树，就是下面这个样子：
因为每次分解都是一分为二，所以代价很低，我们把时间上的消耗记作常量$1$。归并算法中比较耗时的是归并操作，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记作$n$。
现在，我们只需要知道这棵树的高度$h$，用高度$h$乘以每一层的时间消耗$n$，就可以得到总的时间复杂度$O(n*h)$。
从归并排序的原理和递归树，可以看出来，归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是$\log_{2}n$，所以，归并排序递归实现的时间复杂度就是$O(n\log n)$。我这里的时间复杂度都是估算的，对树的高度的计算也没有那么精确，但是这并不影响复杂度的计算结果。
利用递归树的时间复杂度分析方法并不难理解，关键还是在实战，所以，接下来我会通过三个实际的递归算法，带你实战一下递归的复杂度分析。学完这节课之后，你应该能真正掌握递归代码的复杂度分析。
实战一：分析快速排序的时间复杂度在用递归树推导之前，我们先来回忆一下用递推公式的分析方法。你可以回想一下，当时，我们为什么说用递推公式来求解平均时间复杂度非常复杂？
快速排序在最好情况下，每次分区都能一分为二，这个时候用递推公式$T(n)=2T(\frac{n}{2})+n$，很容易就能推导出时间复杂度是$O(n\log n)$。但是，我们并不可能每次分区都这么幸运，正好一分为二。
我们假设平均情况下，每次分区之后，两个分区的大小比例为$1:k$。当$k=9$时，如果用递推公式的方法来求解时间复杂度的话，递推公式就写成$T(n)=T(\frac{n}{10})+T(\frac{9n}{10})+n$。
这个公式可以推导出时间复杂度，但是推导过程非常复杂。那我们来看看，用递归树来分析快速排序的平均情况时间复杂度，是不是比较简单呢？
我们还是取$k$等于$9$，也就是说，每次分区都很不平均，一个分区是另一个分区的$9$倍。如果我们把递归分解的过程画成递归树，就是下面这个样子：
快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是$n$。我们现在只要求出递归树的高度$h$，这个快排过程遍历的数据个数就是 $h * n$ ，也就是说，时间复杂度就是$O(h * n)$。
因为每次分区并不是均匀地一分为二，所以递归树并不是满二叉树。这样一个递归树的高度是多少呢？
我们知道，快速排序结束的条件就是待排序的小区间，大小为$1$，也就是说叶子节点里的数据规模是$1$。从根节点$n$到叶子节点$1$，递归树中最短的一个路径每次都乘以$\frac{1}{10}$，最长的一个路径每次都乘以$\frac{9}{10}$。通过计算，我们可以得到，从根节点到叶子节点的最短路径是$\log_{10}n$，最长的路径是$\log_{\frac{10}{9}}n$。
所以，遍历数据的个数总和就介于$n\log_{10}n$和$n\log_{\frac{10}{9}}n$之间。根据复杂度的大O表示法，对数复杂度的底数不管是多少，我们统一写成$\log n$，所以，当分区大小比例是$1:9$时，快速排序的时间复杂度仍然是$O(n\log n)$。
刚刚我们假设$k=9$，那如果$k=99$，也就是说，每次分区极其不平均，两个区间大小是$1:99$，这个时候的时间复杂度是多少呢？
我们可以类比上面$k=9$的分析过程。当$k=99$的时候，树的最短路径就是$\log_{100}n$，最长路径是$\log_{\frac{100}{99}}n$，所以总遍历数据个数介于$n\log_{100}n$和$n\log_{\frac{100}{99}}n$之间。尽管底数变了，但是时间复杂度也仍然是$O(n\log n)$。
也就是说，对于$k$等于$9$，$99$，甚至是$999$，$9999$……，只要$k$的值不随$n$变化，是一个事先确定的常量，那快排的时间复杂度就是$O(n\log n)$。所以，从概率论的角度来说，快排的平均时间复杂度就是$O(n\log n)$。
实战二：分析斐波那契数列的时间复杂度在递归那一节中，我们举了一个跨台阶的例子，你还记得吗？那个例子实际上就是一个斐波那契数列。为了方便你回忆，我把它的代码实现贴在这里。
int f(int n) { if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2); } 这样一段代码的时间复杂度是多少呢？你可以先试着分析一下，然后再来看，我是怎么利用递归树来分析的。
我们先把上面的递归代码画成递归树，就是下面这个样子：
这棵递归树的高度是多少呢？
$f(n)$分解为$f(n-1)$和$f(n-2)$，每次数据规模都是$-1$或者$-2$，叶子节点的数据规模是$1$或者$2$。所以，从根节点走到叶子节点，每条路径是长短不一的。如果每次都是$-1$，那最长路径大约就是$n$；如果每次都是$-2$，那最短路径大约就是$\frac{n}{2}$。
每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作$1$。所以，从上往下，第一层的总时间消耗是$1$，第二层的总时间消耗是$2$，第三层的总时间消耗就是$2^{2}$。依次类推，第$k$层的时间消耗就是$2^{k-1}$，那整个算法的总的时间消耗就是每一层时间消耗之和。
如果路径长度都为$n$，那这个总和就是$2^{n}-1$。
如果路径长度都是$\frac{n}{2}$ ，那整个算法的总的时间消耗就是$2^{\frac{n}{2}}-1$。
所以，这个算法的时间复杂度就介于$O(2^{n})$和$O(2^{\frac{n}{2}})$之间。虽然这样得到的结果还不够精确，只是一个范围，但是我们也基本上知道了上面算法的时间复杂度是指数级的，非常高。
实战三：分析全排列的时间复杂度前面两个复杂度分析都比较简单，我们再来看个稍微复杂的。</description></item><item><title>27｜增加更丰富的类型第2步：如何支持字符串？</title><link>https://artisanbox.github.io/3/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/29/</guid><description>你好，我是宫文学。
今天我们继续来丰富我们语言的类型体系，让它能够支持字符串。字符串是我们在程序里最常用的数据类型之一。每一门高级语言，都需要对字符串类型的数据提供充分的支持。
但是，跟我们前面讨论过的整型和浮点型数据不同，在CPU层面并没有直接支持字符串运算的指令。所以，相比我们前面讲过的这两类数据类型，要让语言支持字符串，我们需要做更多的工作才可以。
那么，在这一节课里，我们就看看要支持字符串类型的话，我们语言需要做哪些工作。在这个过程中，我们会接触到对象内存布局、内置函数（Intrinsics），以及字符串、字面量的表示等知识点。
首先，我们来分析一下，在这种情况下，我们的编译器和运行时需要完成哪些任务，然后我们再依次完成它们就可以了。
任务分析你可以看到，在一些强调易用性的脚本语言里，字符串常常作为内置的数据类型，并拥有更高优先级的支持。比如，在JavaScript里，你可以用+号连接字符串，并且，其他数据类型和字符串连接时，也会自动转换成字符串。这比在Java、C等语言使用字符串更方便。
为了支持字符串类型，实现最基础的字符串操作功能，我们就需要解决下面这几个技术问题：
第一，如何在语言内部表示一个字符串？
在像JavaScript、Java、Go和C#这样的高级语言中，所有的数据类型可以分为两大类。一类是CPU在底层就支持的，就像整数和浮点数，我们一般叫做基础类型（Primitive Type）或者叫做值类型（Value Type）。
这些类型可以直接表示为指令的操作数，在赋值、传参的时候，也是直接传递值。比如，当我们声明一个number类型的变量时，我们在语言内部用CPU支持的双精度浮点数来存储变量的值就可以了。当给变量赋值的时候，我们也是把这个double值用mov指令拷贝过去就行。
但对于基础类型之外的复杂数据类型来说，它们并不能受到CPU指令级别的直接支持。所以，我们就需要设计，当我们声明一个字符串，以及给字符串赋值的时候，它对应的确切操作是什么。
那么计算机语言的设计者，通常会怎么做呢？我们要把这些复杂数据类型在内部实现成一个内存对象，而变量赋值、传参这样的操作，实际上传递的是对象的引用，对象引用能够转换为对象的内存地址。
所以，从今天这节课开始，我们也将正式支持对象机制。其实，string也好，数组也好，还是后面的自定义类型也好，它们在内存里都是一个对象。当进行赋值操作的时候，传递的都是对象的引用。那这个时候，我们就需要设计对象的内存结构，以及确定什么是对象的引用。
第二，在运行时里提供一些内置函数，用于支持字符串的基本功能。
为了支持字符串类型的数据，我们要能够支持字符串对象的创建、字符串拼接、其他类型的数据转为字符串，还有字符串的比较，等等功能。这些功能是以内置函数（intrincics）的形式来实现的。编译成汇编代码的时候，我们要调用这些内置函数来完成相应的功能。
第三，我们还要处理一些编译器后端的工作。
在编译器的后端方面，我们要能生成对字符串进行访问和处理的汇编代码。这里面的重点就是，我们要知道如何在汇编代码里表示字符串字面量，以及如何获取字符串字面量的地址。
好了，任务安排妥当了，我们开始行动吧。首先我们来看第一个任务，如何在语言内部表示一个字符串类型的数据。
如何表示一个字符串这个问题其实又包含三个子问题：字符编码的问题、string对象的内存布局，以及如何来表示一个对象的引用。
首先，我们看看字符的编码问题。
我们知道，CPU只知道0101这些值，并不知道abcd这些概念。实际上，是我们人类给每个字符编了码，让CPU来理解的。比如规定65代表大写字母a，97代表小写字母a，而48代表字符0，这就是广为使用的ASCII编码标准。但要支持像中文这么多的字符，ASCII标准还不够用，就需要Unicode这样的编码标准。
不过，在我们当前的实现中，我们还是先做一些简化吧，先不支持Unicode，只支持ASCII码就好了。这样，在内存里，我们只需要用一个字节来表示字符就行了，这跟C语言是一样的。至于Unicode，我们后面再支持。毕竟我们的语言PlayScript，是一个开源项目，会继续扩展功能。你也可以走在我前面，自己先去思考并实现一下怎么支持Unicode编码。
第二，我们看看string的内存布局。
如何在内存里表示一个字符串呢？
我们站在巨人的肩膀上，看看C语言是怎么做的。在C语言中，字符串在内存里就相当于一个char的数组，这个数组以0结尾。所以，“Hello”在内存里大概是这样保存的，加起来一共是6个字节：
我们也可以借鉴C语言的做法，用一个数组来表示字符串。不过，C语言需要程序员自己去处理字符串使用的内存：要么通过声明一个数组，在栈里申请内存；要么在堆里申请一块内存，使用完毕以后再手工释放掉。
而JavaScript是不需要程序员来手工管理内存的，而是采用了自动内存管理机制。自动内存管理机制管理的是一个个内存对象。当对象不再被使用以后，就可以被回收。
那么我们的设计，也必须实现自动的内存管理，因为TypeScript并没有底层的内存管理能力。
说到内存对象，我们还有一个设计目标，就是在语言内部，对各种类型的对象都有统一的管理机制，包括统一的内存管理机制、统一的运行时类型查询机制等等。这样，才能铺垫好TypeScript对象化的基础，并在后面实现更丰富的语言特性。所以，我们就需要对如何在内存里表示一个对象进行一下设计。
这方面，我们又可以参考一下其他语言是怎么做的。比如，在Java等语言里，对象都有一些统一的内存布局设计。其典型特征，就是每个对象都有一个固定的对象头，对象头之后才是对象的实际数据。
对象头里面保存了一些信息，用来对这个对象进行管理。进行哪些管理呢？首先是自动内存管理。对象头里有一些标志位，是用于垃圾收集程序的。比如，通过算法来标记某个对象是否是垃圾。我们在后面会具体实现一个垃圾收集算法，那个时候就会用到这些标志位。
标志位还有一个用途就是并发管理。你可以用一些特殊的指令，锁住一个对象，使得该对象在同一时间只可以被一个线程访问。在锁住对象的时候，也要在对象头做标识。此外，对象头里还有引用了类的定义，这样我们就可以在运行时知道这个对象属于哪个类，甚至通过反射等元编程机制去动态地调用对象的方法。
我们可以参考一下Java对象头的设计。它包含类指针和标志位两个部分。类指针指向类定义的地址。标志位就是内部分割成多个部分，用来存放与锁、垃圾收集等标记，还会存放对象的哈希值。
当然，其他语言的对象，也都有类似的内存布局设计。我在《编译原理实战课》中，对Java、Python和Julia等语言的对象内存布局都做了讨论，如果你感兴趣可以去看看。
参考这些设计，我们也可以做出自己的设计。在PlayScript中，我们首先设计一个Object对象，里面有一个标志位的字段和一个指向对象的类定义的指针。我们后面再探讨它们的用途。
//所有对象的对象头。目前的设计占用16个字节。 typedef struct _Object{ //指向类的指针 struct _Object * ptrKlass; //与并发、垃圾收集有关的标志位。 unsigned long flags; }Object; 所有对象都要继承自Object对象，字符串对象也不例外。我们把字符串对象叫做PlayString，其数据结构中包含了字符串的长度。真实的字符串数据是接在PlayString之后的。而且，我们基于PlayString的地址，就能计算出字符串的存储位置，所以并不需要一个单独的指针，这样也就节省了内存空间。
typedef struct _PlayString{ Object object; //字符串的长度 size_t length;
//后面跟以0结尾的字符串，以便复用C语言的一些功能。实际占用内存是length+1。 //我们不需要保存这个指针，只需要在PlayString对象地址的基础上增加一个偏移量就行。 //char* data; }PlayString; 采用这个结构后，实际上PlayString的内存布局如下。对象头占16个字节，字符串长度占4个字节，其余的才是字符串数据，占用空间的大小是字符串的长度再加1个字节：
不过，在这里，我们还有一个技术细节需要做一下决策。</description></item><item><title>28_堆和堆排序：为什么说堆排序没有快速排序快？</title><link>https://artisanbox.github.io/2/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/29/</guid><description>我们今天讲另外一种特殊的树，“堆”（$Heap$）。堆这种数据结构的应用场景非常多，最经典的莫过于堆排序了。堆排序是一种原地的、时间复杂度为$O(n\log n)$的排序算法。
前面我们学过快速排序，平均情况下，它的时间复杂度为$O(n\log n)$。尽管这两种排序算法的时间复杂度都是$O(n\log n)$，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？
现在，你可能还无法回答，甚至对问题本身还有点疑惑。没关系，带着这个问题，我们来学习今天的内容。等你学完之后，或许就能回答出来了。
如何理解“堆”？前面我们提到，堆是一种特殊的树。我们现在就来看看，什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。
堆是一个完全二叉树；
堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。
我分别解释一下这两点。
第一点，堆必须是一个完全二叉树。还记得我们之前讲的完全二叉树的定义吗？完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。
第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。
对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫做“小顶堆”。
定义解释清楚了，你来看看，下面这几个二叉树是不是堆？
其中第$1$个和第$2$个是大顶堆，第$3$个是小顶堆，第$4$个不是堆。除此之外，从图中还可以看出来，对于同一组数据，我们可以构建多种不同形态的堆。
如何实现一个堆？要实现一个堆，我们先要知道，堆都支持哪些操作以及如何存储一个堆。
我之前讲过，完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。
我画了一个用数组存储堆的例子，你可以先看下。
从图中我们可以看到，数组中下标为$i$的节点的左子节点，就是下标为$i*2$的节点，右子节点就是下标为$i*2+1$的节点，父节点就是下标为$\frac{i}{2}$的节点。
知道了如何存储一个堆，那我们再来看看，堆上的操作有哪些呢？我罗列了几个非常核心的操作，分别是往堆中插入一个元素和删除堆顶元素。（如果没有特殊说明，我下面都是拿大顶堆来讲解）。
1.往堆中插入一个元素往堆中插入一个元素后，我们需要继续满足堆的两个特性。
如果我们把新插入的元素放到堆的最后，你可以看我画的这个图，是不是不符合堆的特性了？于是，我们就需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做堆化（heapify）。
堆化实际上有两种，从下往上和从上往下。这里我先讲从下往上的堆化方法。
堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。
我这里画了一张堆化的过程分解图。我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。
我将上面讲的往堆中插入数据的过程，翻译成了代码，你可以结合着一块看。
public class Heap { private int[] a; // 数组，从下标1开始存储数据 private int n; // 堆可以存储的最大数据个数 private int count; // 堆中已经存储的数据个数 public Heap(int capacity) { a = new int[capacity + 1]; n = capacity; count = 0; }
public void insert(int data) { if (count &amp;gt;= n) return; // 堆满了 ++count; a[count] = data; int i = count; while (i/2 &amp;gt; 0 &amp;amp;&amp;amp; a[i] &amp;gt; a[i/2]) { // 自下往上堆化 swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素 i = i/2; } } } 2.</description></item><item><title>28_异常和中断：程序出错了怎么办？</title><link>https://artisanbox.github.io/4/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/28/</guid><description>过去这么多讲，我们的程序都是自动运行且正常运行的。自动运行的意思是说，我们的程序和指令都是一条条顺序执行，你不需要通过键盘或者网络给这个程序任何输入。正常运行是说，我们的程序都是能够正常执行下去的，没有遇到计算溢出之类的程序错误。
不过，现实的软件世界可没有这么简单。一方面，程序不仅是简单的执行指令，更多的还需要和外部的输入输出打交道。另一方面，程序在执行过程中，还会遇到各种异常情况，比如除以0、溢出，甚至我们自己也可以让程序抛出异常。
那这一讲，我就带你来看看，如果遇到这些情况，计算机是怎么运转的，也就是说，计算机究竟是如何处理异常的。
异常：硬件、系统和应用的组合拳一提到计算机当中的异常（Exception），可能你的第一反应就是C++或者Java中的Exception。不过我们今天讲的，并不是这些软件开发过程中遇到的“软件异常”，而是和硬件、系统相关的“硬件异常”。
当然，“软件异常”和“硬件异常”并不是实际业界使用的专有名词，只是我为了方便给你说明，和C++、Java中软件抛出的Exception进行的人为区分，你明白这个意思就好。
尽管，这里我把这些硬件和系统相关的异常，叫作“硬件异常”。但是，实际上，这些异常，既有来自硬件的，也有来自软件层面的。
比如，我们在硬件层面，当加法器进行两个数相加的时候，会遇到算术溢出；或者，你在玩游戏的时候，按下键盘发送了一个信号给到CPU，CPU要去执行一个现有流程之外的指令，这也是一个“异常”。
同样，来自软件层面的，比如我们的程序进行系统调用，发起一个读文件的请求。这样应用程序向系统调用发起请求的情况，一样是通过“异常”来实现的。
关于异常，最有意思的一点就是，它其实是一个硬件和软件组合到一起的处理过程。异常的前半生，也就是异常的发生和捕捉，是在硬件层面完成的。但是异常的后半生，也就是说，异常的处理，其实是由软件来完成的。
计算机会为每一种可能会发生的异常，分配一个异常代码（Exception Number）。有些教科书会把异常代码叫作中断向量（Interrupt Vector）。异常发生的时候，通常是CPU检测到了一个特殊的信号。比如，你按下键盘上的按键，输入设备就会给CPU发一个信号。或者，正在执行的指令发生了加法溢出，同样，我们可以有一个进位溢出的信号。这些信号呢，在组成原理里面，我们一般叫作发生了一个事件（Event）。CPU在检测到事件的时候，其实也就拿到了对应的异常代码。
这些异常代码里，I/O发出的信号的异常代码，是由操作系统来分配的，也就是由软件来设定的。而像加法溢出这样的异常代码，则是由CPU预先分配好的，也就是由硬件来分配的。这又是另一个软件和硬件共同组合来处理异常的过程。
拿到异常代码之后，CPU就会触发异常处理的流程。计算机在内存里，会保留一个异常表（Exception Table）。也有地方，把这个表叫作中断向量表（Interrupt Vector Table），好和上面的中断向量对应起来。这个异常表有点儿像我们在第10讲里讲的GOT表，存放的是不同的异常代码对应的异常处理程序（Exception Handler）所在的地址。
我们的CPU在拿到了异常码之后，会先把当前的程序执行的现场，保存到程序栈里面，然后根据异常码查询，找到对应的异常处理程序，最后把后续指令执行的指挥权，交给这个异常处理程序。
这样“检测异常，拿到异常码，再根据异常码进行查表处理”的模式，在日常开发的过程中是很常见的。
比如说，现在我们日常进行的Web或者App开发，通常都是前后端分离的。前端的应用，会向后端发起HTTP的请求。当后端遇到了异常，通常会给到前端一个对应的错误代码。前端的应用根据这个错误代码，在应用层面去进行错误处理。在不能处理的时候，它会根据错误代码向用户显示错误信息。
public class LastChanceHandler implements Thread.UncaughtExceptionHandler { @Override public void uncaughtException(Thread t, Throwable e) { // do something here - log to file and upload to server/close resources/delete files... } } Thread.setDefaultUncaughtExceptionHandler(new LastChanceHandler()); Java里面，可以设定ExceptionHandler，来处理线程执行中的异常情况再比如说，Java里面，我们使用一个线程池去运行调度任务的时候，可以指定一个异常处理程序。对于各个线程在执行任务出现的异常情况，我们是通过异常处理程序进行处理，而不是在实际的任务代码里处理。这样，我们就把业务处理代码就和异常处理代码的流程分开了。
异常的分类：中断、陷阱、故障和中止我在前面说了，异常可以由硬件触发，也可以由软件触发。那我们平时会碰到哪些异常呢？下面我们就一起来看看。
第一种异常叫中断（Interrupt）。顾名思义，自然就是程序在执行到一半的时候，被打断了。这个打断执行的信号，来自于CPU外部的I/O设备。你在键盘上按下一个按键，就会对应触发一个相应的信号到达CPU里面。CPU里面某个开关的值发生了变化，也就触发了一个中断类型的异常。
第二种异常叫陷阱（Trap）。陷阱，其实是我们程序员“故意“主动触发的异常。就好像你在程序里面打了一个断点，这个断点就是设下的一个&amp;quot;陷阱&amp;quot;。当程序的指令执行到这个位置的时候，就掉到了这个陷阱当中。然后，对应的异常处理程序就会来处理这个&amp;quot;陷阱&amp;quot;当中的猎物。
最常见的一类陷阱，发生在我们的应用程序调用系统调用的时候，也就是从程序的用户态切换到内核态的时候。我们在第3讲讲CPU性能的时候说过，可以用Linux下的time指令，去查看一个程序运行实际花费的时间，里面有在用户态花费的时间（user time），也有在内核态发生的时间（system time）。
我们的应用程序通过系统调用去读取文件、创建进程，其实也是通过触发一次陷阱来进行的。这是因为，我们用户态的应用程序没有权限来做这些事情，需要把对应的流程转交给有权限的异常处理程序来进行。
第三种异常叫故障（Fault）。它和陷阱的区别在于，陷阱是我们开发程序的时候刻意触发的异常，而故障通常不是。比如，我们在程序执行的过程中，进行加法计算发生了溢出，其实就是故障类型的异常。这个异常不是我们在开发的时候计划内的，也一样需要有对应的异常处理程序去处理。
故障和陷阱、中断的一个重要区别是，故障在异常程序处理完成之后，仍然回来处理当前的指令，而不是去执行程序中的下一条指令。因为当前的指令因为故障的原因并没有成功执行完成。
最后一种异常叫中止（Abort）。与其说这是一种异常类型，不如说这是故障的一种特殊情况。当CPU遇到了故障，但是恢复不过来的时候，程序就不得不中止了。
在这四种异常里，中断异常的信号来自系统外部，而不是在程序自己执行的过程中，所以我们称之为“异步”类型的异常。而陷阱、故障以及中止类型的异常，是在程序执行的过程中发生的，所以我们称之为“同步“类型的异常。
在处理异常的过程当中，无论是异步的中断，还是同步的陷阱和故障，我们都是采用同一套处理流程，也就是上面所说的，“保存现场、异常代码查询、异常处理程序调用“。而中止类型的异常，其实是在故障类型异常的一种特殊情况。当故障发生，但是我们发现没有异常处理程序能够处理这种异常的情况下，程序就不得不进入中止状态，也就是最终会退出当前的程序执行。
异常的处理：上下文切换在实际的异常处理程序执行之前，CPU需要去做一次“保存现场”的操作。这个保存现场的操作，和我在第7讲里讲解函数调用的过程非常相似。
因为切换到异常处理程序的时候，其实就好像是去调用一个异常处理函数。指令的控制权被切换到了另外一个&amp;quot;函数&amp;quot;里面，所以我们自然要把当前正在执行的指令去压栈。这样，我们才能在异常处理程序执行完成之后，重新回到当前的指令继续往下执行。
不过，切换到异常处理程序，比起函数调用，还是要更复杂一些。原因有下面几点。</description></item><item><title>28_读写分离有哪些坑？</title><link>https://artisanbox.github.io/1/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/28/</guid><description>在上一篇文章中，我和你介绍了一主多从的结构以及切换流程。今天我们就继续聊聊一主多从架构的应用场景：读写分离，以及怎么处理主备延迟导致的读写分离问题。
我们在上一篇文章中提到的一主多从的结构，其实就是读写分离的基本结构了。这里，我再把这张图贴过来，方便你理解。
图1 读写分离基本结构读写分离的主要目标就是分摊主库的压力。图1中的结构是客户端（client）主动做负载均衡，这种模式下一般会把数据库的连接信息放在客户端的连接层。也就是说，由客户端来选择后端数据库进行查询。
还有一种架构是，在MySQL和客户端之间有一个中间代理层proxy，客户端只连接proxy， 由proxy根据请求类型和上下文决定请求的分发路由。
图2 带proxy的读写分离架构接下来，我们就看一下客户端直连和带proxy的读写分离架构，各有哪些特点。
客户端直连方案，因为少了一层proxy转发，所以查询性能稍微好一点儿，并且整体架构简单，排查问题更方便。但是这种方案，由于要了解后端部署细节，所以在出现主备切换、库迁移等操作的时候，客户端都会感知到，并且需要调整数据库连接信息。
你可能会觉得这样客户端也太麻烦了，信息大量冗余，架构很丑。其实也未必，一般采用这样的架构，一定会伴随一个负责管理后端的组件，比如Zookeeper，尽量让业务端只专注于业务逻辑开发。
带proxy的架构，对客户端比较友好。客户端不需要关注后端细节，连接维护、后端信息维护等工作，都是由proxy完成的。但这样的话，对后端维护团队的要求会更高。而且，proxy也需要有高可用架构。因此，带proxy架构的整体就相对比较复杂。
理解了这两种方案的优劣，具体选择哪个方案就取决于数据库团队提供的能力了。但目前看，趋势是往带proxy的架构方向发展的。
但是，不论使用哪种架构，你都会碰到我们今天要讨论的问题：由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态。
这种“在从库上会读到系统的一个过期状态”的现象，在这篇文章里，我们暂且称之为“过期读”。
前面我们说过了几种可能导致主备延迟的原因，以及对应的优化策略，但是主从延迟还是不能100%避免的。
不论哪种结构，客户端都希望查询从库的数据结果，跟查主库的数据结果是一样的。
接下来，我们就来讨论怎么处理过期读问题。
这里，我先把文章中涉及到的处理过期读的方案汇总在这里，以帮助你更好地理解和掌握全文的知识脉络。这些方案包括：
强制走主库方案； sleep方案； 判断主备无延迟方案； 配合semi-sync方案； 等主库位点方案； 等GTID方案。 强制走主库方案强制走主库方案其实就是，将查询请求做分类。通常情况下，我们可以将查询请求分为这么两类：
对于必须要拿到最新结果的请求，强制将其发到主库上。比如，在一个交易平台上，卖家发布商品以后，马上要返回主页面，看商品是否发布成功。那么，这个请求需要拿到最新的结果，就必须走主库。
对于可以读到旧数据的请求，才将其发到从库上。在这个交易平台上，买家来逛商铺页面，就算晚几秒看到最新发布的商品，也是可以接受的。那么，这类请求就可以走从库。
你可能会说，这个方案是不是有点畏难和取巧的意思，但其实这个方案是用得最多的。
当然，这个方案最大的问题在于，有时候你会碰到“所有查询都不能是过期读”的需求，比如一些金融类的业务。这样的话，你就要放弃读写分离，所有读写压力都在主库，等同于放弃了扩展性。
因此接下来，我们来讨论的话题是：可以支持读写分离的场景下，有哪些解决过期读的方案，并分析各个方案的优缺点。
Sleep 方案主库更新后，读从库之前先sleep一下。具体的方案就是，类似于执行一条select sleep(1)命令。
这个方案的假设是，大多数情况下主备延迟在1秒之内，做一个sleep可以有很大概率拿到最新的数据。
这个方案给你的第一感觉，很可能是不靠谱儿，应该不会有人用吧？并且，你还可能会说，直接在发起查询时先执行一条sleep语句，用户体验很不友好啊。
但，这个思路确实可以在一定程度上解决问题。为了看起来更靠谱儿，我们可以换一种方式。
以卖家发布商品为例，商品发布后，用Ajax（Asynchronous JavaScript + XML，异步JavaScript和XML）直接把客户端输入的内容作为“新的商品”显示在页面上，而不是真正地去数据库做查询。
这样，卖家就可以通过这个显示，来确认产品已经发布成功了。等到卖家再刷新页面，去查看商品的时候，其实已经过了一段时间，也就达到了sleep的目的，进而也就解决了过期读的问题。
也就是说，这个sleep方案确实解决了类似场景下的过期读问题。但，从严格意义上来说，这个方案存在的问题就是不精确。这个不精确包含了两层意思：
如果这个查询请求本来0.5秒就可以在从库上拿到正确结果，也会等1秒；
如果延迟超过1秒，还是会出现过期读。
看到这里，你是不是有一种“你是不是在逗我”的感觉，这个改进方案虽然可以解决类似Ajax场景下的过期读问题，但还是怎么看都不靠谱儿。别着急，接下来我就和你介绍一些更准确的方案。
判断主备无延迟方案要确保备库无延迟，通常有三种做法。
通过前面的第25篇文章，我们知道show slave status结果里的seconds_behind_master参数的值，可以用来衡量主备延迟时间的长短。
所以第一种确保主备无延迟的方法是，每次从库执行查询请求前，先判断seconds_behind_master是否已经等于0。如果还不等于0 ，那就必须等到这个参数变为0才能执行查询请求。
seconds_behind_master的单位是秒，如果你觉得精度不够的话，还可以采用对比位点和GTID的方法来确保主备无延迟，也就是我们接下来要说的第二和第三种方法。
如图3所示，是一个show slave status结果的部分截图。</description></item><item><title>28｜增加更丰富的类型第3步：支持数组</title><link>https://artisanbox.github.io/3/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/30/</guid><description>你好，我是宫文学。
前面我们已经给我们的语言，增加了两种数据类型：浮点数和字符串。那么，今天这一节课，我们再继续增加一种典型的数据类型：数组。
数组也是计算机语言中最重要的基础类型之一。像C、Java和JavaScript等各种语言，都提供了对数组的原生支持。
数组跟我们之前已经实现过的number、string类型的数据类型相比，有明显的差异。所以在这里，你也能学到一些新的知识点，包括，如何对数组做类型处理、如何设计数组的内存布局，如何正确访问数组元素，等等。
在完成这节课的任务后，我们的语言将支持在数组中保存number和string类型的数据，甚至还可以支持多维数组。是不是感觉很强大呢？那就赶紧动手试一试吧！
那么这实现的第一步，我们需要修改编译器前端的代码，来支持与数组处理有关的语法。
修改编译器前端在编译器前端，我们首先要增加与数组有关的语法规则。要增加哪些语法规则呢？我们来看看一个最常见的使用数组的例子中，都会涉及哪些语法特性。
let names:string[] = ["richard", "sam", "john"]; let ages:number[] = [8, 18, 28]; let a2:number[][] = [[1,2,3],[4,5]]; for (let i = 0; i&amp;lt; names.length; i++){ println(names[i]); } 在这个例子中，我们首先声明了一个字符串类型的数组，然后用一个数组字面量来初始化它。你还可以用同样的方法，声明并初始化一个number数组。最后，我们用names[i]这样的表达式来访问数组元素。
在这个例子中，你会发现三个与数组有关的语法现象，分别是数组类型、数组字面量和下标表达式。
首先是数组类型。在声明变量的时候，我们可以用string[]、number[]来表示一个数组类型。这个数组类型是一个基础类型再加上一对方括号[]。在这里，我们甚至还声明了一个二维数组。
所以，我们还需要扩展与类型有关的语法规则。你看看下面的语法规则，你可以这样表示数组类型：“primaryType ‘[’ ‘]’”。而primaryType本身也可以是一个数组类型，这样就能表达多维数组了，比如a[][]。
primaryType : predefinedType | literal | typeReference | '(' type_ ')' | primaryType '[' ']' ; 不过，这是一个左递归的文法，就会遇到我们之前学过的左递归问题。我们可以改写一下，变成下面的文法：
primaryType : primaryTypeLeft ('[' ']')* ; primaryTypeLeft : predefinedType | literal | typeReference | '(' type_ ')' | primaryType ; 这样的话，我们每次解析完毕一个primaryTypeLeft以后，再看看后面有没有跟着一对方括号就行了。如果出现多对方括号，就表示这是一个多维数组。</description></item><item><title>29_CISC和RISC：为什么手机芯片都是ARM？</title><link>https://artisanbox.github.io/4/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/29/</guid><description>我在第5讲讲计算机指令的时候，给你看过MIPS体系结构计算机的机器指令格式。MIPS的指令都是固定的32位长度，如果要用一个打孔卡来表示，并不复杂。
MIPS机器码的长度都是固定的32位第6讲的时候，我带你编译了一些简单的C语言程序，看了x86体系结构下的汇编代码。眼尖的话，你应该能发现，每一条机器码的长度是不一样的。
Intel x86的机器码的长度是可变的而CPU的指令集里的机器码是固定长度还是可变长度，也就是复杂指令集（Complex Instruction Set Computing，简称CISC）和精简指令集（Reduced Instruction Set Computing，简称RISC）这两种风格的指令集一个最重要的差别。那今天我们就来看复杂指令集和精简指令集之间的对比、差异以及历史纠葛。
CISC VS RISC：历史的车轮不总是向前的在计算机历史的早期，其实没有什么CISC和RISC之分。或者说，所有的CPU其实都是CISC。
虽然冯·诺依曼高屋建瓴地提出了存储程序型计算机的基础架构，但是实际的计算机设计和制造还是严格受硬件层面的限制。当时的计算机很慢，存储空间也很小。《人月神话》这本软件工程界的名著，讲的是花了好几年设计IBM 360这台计算机的经验。IBM 360的最低配置，每秒只能运行34500条指令，只有8K的内存。为了让计算机能够做尽量多的工作，每一个字节乃至每一个比特都特别重要。
所以，CPU指令集的设计，需要仔细考虑硬件限制。为了性能考虑，很多功能都直接通过硬件电路来完成。为了少用内存，指令的长度也是可变的。就像算法和数据结构里的赫夫曼编码（Huffman coding）一样，常用的指令要短一些，不常用的指令可以长一些。那个时候的计算机，想要用尽可能少的内存空间，存储尽量多的指令。
不过，历史的车轮滚滚向前，计算机的性能越来越好，存储的空间也越来越大了。到了70年代末，RISC开始登上了历史的舞台。当时，UC Berkeley的大卫·帕特森（David Patterson）教授发现，实际在CPU运行的程序里，80%的时间都是在使用20%的简单指令。于是，他就提出了RISC的理念。自此之后，RISC类型的CPU开始快速蓬勃发展。
我经常推荐的课后阅读材料，有不少是来自《计算机组成与设计：硬件/软件接口》和《计算机体系结构：量化研究方法》这两本教科书。大卫·帕特森教授正是这两本书的作者。此外，他还在2017年获得了图灵奖。
RISC架构的CPU究竟是什么样的呢？为什么它能在这么短的时间内受到如此大的追捧？
RISC架构的CPU的想法其实非常直观。既然我们80%的时间都在用20%的简单指令，那我们能不能只要那20%的简单指令就好了呢？答案当然是可以的。因为指令数量多，计算机科学家们在软硬件两方面都受到了很多挑战。
在硬件层面，我们要想支持更多的复杂指令，CPU里面的电路就要更复杂，设计起来也就更困难。更复杂的电路，在散热和功耗层面，也会带来更大的挑战。在软件层面，支持更多的复杂指令，编译器的优化就变得更困难。毕竟，面向2000个指令来优化编译器和面向500个指令来优化编译器的困难是完全不同的。
于是，在RISC架构里面，CPU选择把指令“精简”到20%的简单指令。而原先的复杂指令，则通过用简单指令组合起来来实现，让软件来实现硬件的功能。这样，CPU的整个硬件设计就会变得更简单了，在硬件层面提升性能也会变得更容易了。
RISC的CPU里完成指令的电路变得简单了，于是也就腾出了更多的空间。这个空间，常常被拿来放通用寄存器。因为RISC完成同样的功能，执行的指令数量要比CISC多，所以，如果需要反复从内存里面读取指令或者数据到寄存器里来，那么很多时间就会花在访问内存上。于是，RISC架构的CPU往往就有更多的通用寄存器。
除了寄存器这样的存储空间，RISC的CPU也可以把更多的晶体管，用来实现更好的分支预测等相关功能，进一步去提升CPU实际的执行效率。
总的来说，对于CISC和RISC的对比，我们可以一起回到第4讲讲的程序运行时间的公式：
程序的CPU执行时间=指令数 × CPI × Clock Cycle TimeCISC的架构，其实就是通过优化指令数，来减少CPU的执行时间。而RISC的架构，其实是在优化CPI。因为指令比较简单，需要的时钟周期就比较少。
因为RISC降低了CPU硬件的设计和开发难度，所以从80年代开始，大部分新的CPU都开始采用RISC架构。从IBM的PowerPC，到SUN的SPARC，都是RISC架构。所有人看到仍然采用CISC架构的Intel CPU，都可以批评一句“Complex and messy”。但是，为什么无论是在PC上，还是服务器上，仍然是Intel成为最后的赢家呢？
Intel的进化：微指令架构的出现面对这么多负面评价的Intel，自然也不能无动于衷。更何况，x86架构的问题并不能说明Intel的工程师不够厉害。事实上，在整个CPU设计的领域，Intel集中了大量优秀的人才。无论是成功的Pentium时代引入的超标量设计，还是失败的Pentium 4时代引入的超线程技术，都是异常精巧的工程实现。
而x86架构所面临的种种问题，其实都来自于一个最重要的考量，那就是指令集的向前兼容性。因为x86在商业上太成功了，所以市场上有大量的Intel CPU。而围绕着这些CPU，又有大量的操作系统、编译器。这些系统软件只支持x86的指令集，就比如著名的Windows 95。而在这些系统软件上，又有各种各样的应用软件。
如果Intel要放弃x86的架构和指令集，开发一个RISC架构的CPU，面临的第一个问题就是所有这些软件都是不兼容的。事实上，Intel并非没有尝试过在x86之外另起炉灶，这其实就是我在第26讲介绍的安腾处理器。当时，Intel想要在CPU进入64位的时代的时候，丢掉x86的历史包袱，所以推出了全新的IA-64的架构。但是，却因为不兼容x86的指令集，遭遇了重大的失败。
反而是AMD，趁着Intel研发安腾的时候，推出了兼容32位x86指令集的64位架构，也就是AMD64。如果你现在在Linux下安装各种软件包，一定经常会看到像下面这样带有AMD64字样的内容。这是因为x86下的64位的指令集x86-64，并不是Intel发明的，而是AMD发明的。
Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fontconfig amd64 2.12.6-0ubuntu2 [169 kB] 在Ubuntu下通过APT安装程序的时候，随处可见AMD64的关键字花开两朵，各表一枝。Intel在开发安腾处理器的同时，也在不断借鉴其他RISC处理器的设计思想。既然核心问题是要始终向前兼容x86的指令集，那么我们能不能不修改指令集，但是让CISC风格的指令集，用RISC的形式在CPU里面运行呢？
于是，从Pentium Pro时代开始，Intel就开始在处理器里引入了微指令（Micro-Instructions/Micro-Ops）架构。而微指令架构的引入，也让CISC和RISC的分界变得模糊了。
在微指令架构的CPU里面，编译器编译出来的机器码和汇编代码并没有发生什么变化。但在指令译码的阶段，指令译码器“翻译”出来的，不再是某一条CPU指令。译码器会把一条机器码，“翻译”成好几条“微指令”。这里的一条条微指令，就不再是CISC风格的了，而是变成了固定长度的RISC风格的了。
这些RISC风格的微指令，会被放到一个微指令缓冲区里面，然后再从缓冲区里面，分发给到后面的超标量，并且是乱序执行的流水线架构里面。不过这个流水线架构里面接受的，就不是复杂的指令，而是精简的指令了。在这个架构里，我们的指令译码器相当于变成了设计模式里的一个“适配器”（Adaptor）。这个适配器，填平了CISC和RISC之间的指令差异。
不过，凡事有好处就有坏处。这样一个能够把CISC的指令译码成RISC指令的指令译码器，比原来的指令译码器要复杂。这也就意味着更复杂的电路和更长的译码时间：本来以为可以通过RISC提升的性能，结果又有一部分浪费在了指令译码上。针对这个问题，我们有没有更好的办法呢？
我在前面说过，之所以大家认为RISC优于CISC，来自于一个数字统计，那就是在实际的程序运行过程中，有80%运行的代码用着20%的常用指令。这意味着，CPU里执行的代码有很强的局部性。而对于有着很强局部性的问题，常见的一个解决方案就是使用缓存。
所以，Intel就在CPU里面加了一层L0 Cache。这个Cache保存的就是指令译码器把CISC的指令“翻译”成RISC的微指令的结果。于是，在大部分情况下，CPU都可以从Cache里面拿到译码结果，而不需要让译码器去进行实际的译码操作。这样不仅优化了性能，因为译码器的晶体管开关动作变少了，还减少了功耗。
因为“微指令”架构的存在，从Pentium Pro开始，Intel处理器已经不是一个纯粹的CISC处理器了。它同样融合了大量RISC类型的处理器设计。不过，由于Intel本身在CPU层面做的大量优化，比如乱序执行、分支预测等相关工作，x86的CPU始终在功耗上还是要远远超过RISC架构的ARM，所以最终在智能手机崛起替代PC的时代，落在了ARM后面。
ARM和RISC-V：CPU的现在与未来2017年，ARM公司的CEO Simon Segards宣布，ARM累积销售的芯片数量超过了1000亿。作为一个从12个人起步，在80年代想要获取Intel的80286架构授权来制造CPU的公司，ARM是如何在移动端把自己的芯片塑造成了最终的霸主呢？</description></item><item><title>29_堆的应用：如何快速获取到Top10最热门的搜索关键词？</title><link>https://artisanbox.github.io/2/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/30/</guid><description>搜索引擎的热门搜索排行榜功能你用过吗？你知道这个功能是如何实现的吗？实际上，它的实现并不复杂。搜索引擎每天会接收大量的用户搜索请求，它会把这些用户输入的搜索关键词记录下来，然后再离线地统计分析，得到最热门的Top 10搜索关键词。
那请你思考下，假设现在我们有一个包含10亿个搜索关键词的日志文件，如何能快速获取到热门榜Top 10的搜索关键词呢？
这个问题就可以用堆来解决，这也是堆这种数据结构一个非常典型的应用。上一节我们讲了堆和堆排序的一些理论知识，今天我们就来讲一讲，堆这种数据结构几个非常重要的应用：优先级队列、求Top K和求中位数。
堆的应用一：优先级队列首先，我们来看第一个应用场景：优先级队列。
优先级队列，顾名思义，它首先应该是一个队列。我们前面讲过，队列最大的特性就是先进先出。不过，在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。
如何实现一个优先级队列呢？方法有很多，但是用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。
你可别小看这个优先级队列，它的应用场景非常多。我们后面要讲的很多数据结构和算法都要依赖它。比如，赫夫曼编码、图的最短路径、最小生成树算法等等。不仅如此，很多语言中，都提供了优先级队列的实现，比如，Java的PriorityQueue，C++的priority_queue等。
只讲这些应用场景比较空泛，现在，我举两个具体的例子，让你感受一下优先级队列具体是怎么用的。
1.合并有序小文件假设我们有100个小文件，每个文件的大小是100MB，每个文件中存储的都是有序的字符串。我们希望将这些100个小文件合并成一个有序的大文件。这里就会用到优先级队列。
整体思路有点像归并排序中的合并函数。我们从这100个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。
假设，这个最小的字符串来自于13.txt这个小文件，我们就再从这个小文件取下一个字符串，放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，将它从数组中删除。依次类推，直到所有的文件中的数据都放入到大文件为止。
这里我们用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。有没有更加高效方法呢？
这里就可以用到优先级队列，也可以说是堆。我们将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将100个小文件中的数据依次放入到大文件中。
我们知道，删除堆顶数据和往堆中插入数据的时间复杂度都是O(logn)，n表示堆中的数据个数，这里就是100。是不是比原来数组存储的方式高效了很多呢？
2.高性能定时器假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如1秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。
但是，这样每过1秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。
针对这些问题，我们就可以用优先级队列来解决。我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。
这样，定时器就不需要每隔1秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔T。
这个时间间隔T就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在T秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。
当T秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。
这样，定时器既不用间隔1秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。
堆的应用二：利用堆求Top K刚刚我们学习了优先级队列，我们现在来看，堆的另外一个非常重要的应用场景，那就是“求Top K问题”。
我把这种求Top K的问题抽象成两类。一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。
针对静态数据，如何在一个包含n个数据的数组中，查找前K大数据呢？我们可以维护一个大小为K的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前K大数据了。
遍历数组需要O(n)的时间复杂度，一次堆化操作需要O(logK)的时间复杂度，所以最坏情况下，n个元素都入堆一次，时间复杂度就是O(nlogK)。
针对动态数据求得Top K就是实时Top K。怎么理解呢？我举一个例子。一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前K大数据。
如果每次询问前K大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是O(nlogK)，n表示当前的数据的大小。实际上，我们可以一直都维护一个K大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前K大数据，我们都可以立刻返回给他。
堆的应用三：利用堆求中位数前面我们讲了如何求Top K的问题，现在我们来讲下，如何求动态数据集合中的中位数。
中位数，顾名思义，就是处在中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第$\frac{n}{2}+1$个数据就是中位数（注意：假设数据是从0开始编号的）；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第$\frac{n}{2}$个和第$\frac{n}{2}+1$个数据，这个时候，我们可以随意取一个作为中位数，比如取两个数中靠前的那个，就是第$\frac{n}{2}$个数据。
对于一组静态数据，中位数是固定的，我们可以先排序，第$\frac{n}{2}$个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。所以，尽管排序的代价比较大，但是边际成本会很小。但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。
借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。我们来看看，它是如何做到的？
我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。
也就是说，如果有n个数据，n是偶数，我们从小到大排序，那前$\frac{n}{2}$个数据存储在大顶堆中，后$\frac{n}{2}$个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果n是奇数，情况是类似的，大顶堆就存储$\frac{n}{2}+1$个数据，小顶堆中就存储$\frac{n}{2}$个数据。
我们前面也提到，数据是动态变化的，当新添加一个数据的时候，我们如何调整两个堆，让大顶堆中的堆顶元素继续是中位数呢？
如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。
这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果n是偶数，两个堆中的数据个数都是$\frac{n}{2}$；如果n是奇数，大顶堆有$\frac{n}{2}+1$个数据，小顶堆有$\frac{n}{2}$个数据。这个时候，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。
于是，我们就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间复杂度变成了O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是O(1)。
实际上，利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。还记得我们在“为什么要学习数据结构与算法”里的这个问题吗？“如何快速求接口的99%响应时间？”我们现在就来看下，利用两个堆如何来实现。
在开始这个问题的讲解之前，我先解释一下，什么是“99%响应时间”。
中位数的概念就是将数据从小到大排列，处于中间位置，就叫中位数，这个数据会大于等于前面50%的数据。99百分位数的概念可以类比中位数，如果将一组数据从小到大排列，这个99百分位数就是大于前面99%数据的那个数据。
如果你还是不太理解，我再举个例子。假设有100个数据，分别是1，2，3，……，100，那99百分位数就是99，因为小于等于99的数占总个数的99%。
弄懂了这个概念，我们再来看99%响应时间。如果有100个接口访问请求，每个接口请求的响应时间都不同，比如55毫秒、100毫秒、23毫秒等，我们把这100个接口的响应时间按照从小到大排列，排在第99的那个数据就是99%响应时间，也叫99百分位响应时间。
我们总结一下，如果有n个数据，将数据从小到大排列之后，99百分位数大约就是第n*99%个数据，同类，80百分位数大约就是第n*80%个数据。
弄懂了这些，我们再来看如何求99%响应时间。
我们维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是n，大顶堆中保存n*99%个数据，小顶堆中保存n*1%个数据。大顶堆堆顶的数据就是我们要找的99%响应时间。
每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。
但是，为了保持大顶堆中的数据占99%，小顶堆中的数据占1%，在每次新插入数据之后，我们都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合99:1这个比例。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法，这里我就不啰嗦了。
通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是O(logn)。每次求99%响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是O(1)。
解答开篇学懂了上面的一些应用场景的处理思路，我想你应该能解决开篇的那个问题了吧。假设现在我们有一个包含10亿个搜索关键词的日志文件，如何快速获取到Top 10最热门的搜索关键词呢？
处理这个问题，有很多高级的解决方法，比如使用MapReduce等。但是，如果我们将处理的场景限定为单机，可以使用的内存为1GB。那这个问题该如何解决呢？
因为用户搜索的关键词，有很多可能都是重复的，所以我们首先要统计每个搜索关键词出现的频率。我们可以通过散列表、平衡二叉查找树或者其他一些支持快速查找、插入的数据结构，来记录关键词及其出现的次数。
假设我们选用散列表。我们就顺序扫描这10亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为1。以此类推，等遍历完这10亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。
然后，我们再根据前面讲的用堆求Top K的方法，建立一个大小为10的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。
以此类推，当遍历完整个散列表中的搜索关键词之后，堆中的搜索关键词就是出现次数最多的Top 10搜索关键词了。
不知道你发现了没有，上面的解决思路其实存在漏洞。10亿的关键词还是很多的。我们假设10亿条搜索关键词中不重复的有1亿条，如果每个搜索关键词的平均长度是50个字节，那存储1亿个关键词起码需要5GB的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。而我们的机器只有1GB的可用内存空间，所以我们无法一次性将所有的搜索关键词加入到内存中。这个时候该怎么办呢？
我们在哈希算法那一节讲过，相同数据经过哈希算法得到的哈希值是一样的。我们可以根据哈希算法的这个特点，将10亿条搜索关键词先通过哈希算法分片到10个文件中。
具体可以这样做：我们创建10个空文件00，01，02，……，09。我们遍历这10亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同10取模，得到的结果就是这个搜索关键词应该被分到的文件编号。</description></item><item><title>29_如何判断一个数据库是不是出问题了？</title><link>https://artisanbox.github.io/1/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/29/</guid><description>我在第25和27篇文章中，和你介绍了主备切换流程。通过这些内容的讲解，你应该已经很清楚了：在一主一备的双M架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上。
主备切换有两种场景，一种是主动切换，一种是被动切换。而其中被动切换，往往是因为主库出问题了，由HA系统发起的。
这也就引出了我们今天要讨论的问题：怎么判断一个主库出问题了？
你一定会说，这很简单啊，连上MySQL，执行个select 1就好了。但是select 1成功返回了，就表示主库没问题吗？
select 1判断实际上，select 1成功返回，只能说明这个库的进程还在，并不能说明主库没问题。现在，我们来看一下这个场景。
set global innodb_thread_concurrency=3; CREATE TABLE t ( id int(11) NOT NULL, c int(11) DEFAULT NULL, PRIMARY KEY (id) ) ENGINE=InnoDB;
insert into t values(1,1) 图1 查询blocked我们设置innodb_thread_concurrency参数的目的是，控制InnoDB的并发线程上限。也就是说，一旦并发线程数达到这个值，InnoDB在接收到新请求的时候，就会进入等待状态，直到有线程退出。
这里，我把innodb_thread_concurrency设置成3，表示InnoDB只允许3个线程并行执行。而在我们的例子中，前三个session 中的sleep(100)，使得这三个语句都处于“执行”状态，以此来模拟大查询。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;你看到了， session D里面，select 1是能执行成功的，但是查询表t的语句会被堵住。也就是说，如果这时候我们用select 1来检测实例是否正常的话，是检测不出问题的。
在InnoDB中，innodb_thread_concurrency这个参数的默认值是0，表示不限制并发线程数量。但是，不限制并发线程数肯定是不行的。因为，一个机器的CPU核数有限，线程全冲进来，上下文切换的成本就会太高。
所以，通常情况下，我们建议把innodb_thread_concurrency设置为64~128之间的值。这时，你一定会有疑问，并发线程上限数设置为128够干啥，线上的并发连接数动不动就上千了。
产生这个疑问的原因，是搞混了并发连接和并发查询。
并发连接和并发查询，并不是同一个概念。你在show processlist的结果里，看到的几千个连接，指的就是并发连接。而“当前正在执行”的语句，才是我们所说的并发查询。
并发连接数达到几千个影响并不大，就是多占一些内存而已。我们应该关注的是并发查询，因为并发查询太高才是CPU杀手。这也是为什么我们需要设置innodb_thread_concurrency参数的原因。
然后，你可能还会想起我们在第7篇文章中讲到的热点更新和死锁检测的时候，如果把innodb_thread_concurrency设置为128的话，那么出现同一行热点更新的问题时，是不是很快就把128消耗完了，这样整个系统是不是就挂了呢？
实际上，在线程进入锁等待以后，并发线程的计数会减一，也就是说等行锁（也包括间隙锁）的线程是不算在128里面的。
MySQL这样设计是非常有意义的。因为，进入锁等待的线程已经不吃CPU了；更重要的是，必须这么设计，才能避免整个系统锁死。
为什么呢？假设处于锁等待的线程也占并发线程的计数，你可以设想一下这个场景：
线程1执行begin; update t set c=c+1 where id=1, 启动了事务trx1， 然后保持这个状态。这时候，线程处于空闲状态，不算在并发线程里面。
线程2到线程129都执行 update t set c=c+1 where id=1; 由于等行锁，进入等待状态。这样就有128个线程处于等待状态；</description></item><item><title>29｜面向对象编程第1步：先把基础搭好</title><link>https://artisanbox.github.io/3/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/31/</guid><description>你好，我是宫文学。
到目前为止，我们的语言已经简单支持了number类型、string类型和数组。现在，我们终于要来实现期待已久的面向对象功能了。
在我们的课程中，为了实现编译器的功能，我们使用了大量自定义的类。最典型的就是各种AST节点，它们都有共同的基类，然后各自又有自己属性或方法。这就是TypeScript面向对象特性最直观的体现。
面向对象特性是一个比较大的体系，涉及了很多知识点。我们会花两节课的时间，实现其中最关键的那些技术点，比如声明自定义类、创建对象、访问对象的属性和方法，以及对象的继承和多态，等等，让你理解面向对象的基础原理。
首先，我们仍然从编译器的前端部分改起，让它支持面向对象特性的语法和语义处理工作。
修改编译器前端首先是对语法的增强。我们还是先来看一个例子，通过这个例子看看，我们到底需要增加哪些语法特性：
class Mammal{ weight:number; color:string; constructor(weight:number, color:string){ this.weight = weight; this.color = color; } speak(){ println("Hello!"); } } let mammal = new Mammal(20,&amp;ldquo;white&amp;rdquo;); println(mammal.color); println(mammal.weight); println(mammal.speak); &amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;在这个例子中，我们声明了一个class，Mammal。这个类描述了哺乳动物的一些基础属性，包括它的体重weight、颜色color。它还提供了哺乳动物的一些行为特征，比如提供了一个speak方法。
Mammal类还有一个特殊的方法，叫做构造方法。通过调用构造方法，可以创建类的实例，也就是对象。然后，我们可以访问对象的属性和方法。
其实TypeScript的类还有很多特性，包括私有成员、静态成员等等。这里我们还是先考虑一个最小的特性集合，先让语言支持最基础的类和对象特性。
看看这个示例程序，我们能总结出多个需要增强的语法点，包括类的声明、调用类的构造方法，this关键字，以及通过点符号来引用对象的属性和方法。
我们首先看看类的声明。我们提供了下面这些语法规则，来支持类的声明：
classDecl : Class Identifier classTail ; classTail : &amp;lsquo;{&amp;rsquo; classElement* &amp;lsquo;}&amp;rsquo; ; classElement : constructorDecl| propertyMemberDecl; constructorDecl : Constructor &amp;lsquo;(&amp;rsquo; parameterList? &amp;lsquo;)&amp;rsquo; &amp;lsquo;{&amp;rsquo; functionBody &amp;lsquo;}&amp;rsquo; ; propertyMemberDecl : Identifier typeAnnotation? (&amp;rsquo;=&amp;rsquo; expression)?</description></item><item><title>30_GPU（上）：为什么玩游戏需要使用GPU？</title><link>https://artisanbox.github.io/4/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/30/</guid><description>讲完了CPU，我带你一起来看一看计算机里的另外一个处理器，也就是被称之为GPU的图形处理器。过去几年里，因为深度学习的大发展，GPU一下子火起来了，似乎GPU成了一个专为深度学习而设计的处理器。那GPU的架构究竟是怎么回事儿呢？它最早是用来做什么而被设计出来的呢？
想要理解GPU的设计，我们就要从GPU的老本行图形处理说起。因为图形处理才是GPU设计用来做的事情。只有了解了图形处理的流程，我们才能搞明白，为什么GPU要设计成现在这样；为什么在深度学习上，GPU比起CPU有那么大的优势。
GPU的历史进程GPU是随着我们开始在计算机里面需要渲染三维图形的出现，而发展起来的设备。图形渲染和设备的先驱，第一个要算是SGI（Silicon Graphics Inc.）这家公司。SGI的名字翻译成中文就是“硅谷图形公司”。这家公司从80年代起就开发了很多基于Unix操作系统的工作站。它的创始人Jim Clark是斯坦福的教授，也是图形学的专家。
后来，他也是网景公司（Netscape）的创始人之一。而Netscape，就是那个曾经和IE大战300回合的浏览器公司，虽然最终败在微软的Windows免费捆绑IE的策略下，但是也留下了Firefox这个完全由开源基金会管理的浏览器。不过这个都是后话了。
到了90年代中期，随着个人电脑的性能越来越好，PC游戏玩家们开始有了“3D显卡”的需求。那个时代之前的3D游戏，其实都是伪3D。比如，大神卡马克开发的著名Wolfenstein 3D（德军总部3D），从不同视角看到的是8幅不同的贴图，实际上并不是通过图形学绘制渲染出来的多边形。
这样的情况下，游戏玩家的视角旋转个10度，看到的画面并没有变化。但是如果转了45度，看到的画面就变成了另外一幅图片。而如果我们能实时渲染基于多边形的3D画面的话，那么任何一点点的视角变化，都会实时在画面里面体现出来，就好像你在真实世界里面看到的一样。
而在90年代中期，随着硬件和技术的进步，我们终于可以在PC上用硬件直接实时渲染多边形了。“真3D”游戏开始登上历史舞台了。“古墓丽影”“最终幻想7”，这些游戏都是在那个时代诞生的。当时，很多国内的计算机爱好者梦寐以求的，是一块Voodoo FX的显卡。
那为什么CPU的性能已经大幅度提升了，但是我们还需要单独的GPU呢？想要了解这个问题，我们先来看一看三维图像实际通过计算机渲染出来的流程。
图形渲染的流程现在我们电脑里面显示出来的3D的画面，其实是通过多边形组合出来的。你可以看看下面这张图，你在玩的各种游戏，里面的人物的脸，并不是那个相机或者摄像头拍出来的，而是通过多边形建模（Polygon Modeling）创建出来的。
图片来源3D游戏里的人脸，其实是用多边形建模创建出来的而实际这些人物在画面里面的移动、动作，乃至根据光线发生的变化，都是通过计算机根据图形学的各种计算，实时渲染出来的。
这个对于图像进行实时渲染的过程，可以被分解成下面这样5个步骤：
顶点处理（Vertex Processing） 图元处理（Primitive Processing） 栅格化（Rasterization） 片段处理（Fragment Processing） 像素操作（Pixel Operations） 我们现在来一步一步看这5个步骤。
顶点处理图形渲染的第一步是顶点处理。构成多边形建模的每一个多边形呢，都有多个顶点（Vertex）。这些顶点都有一个在三维空间里的坐标。但是我们的屏幕是二维的，所以在确定当前视角的时候，我们需要把这些顶点在三维空间里面的位置，转化到屏幕这个二维空间里面。这个转换的操作，就被叫作顶点处理。
如果你稍微学过一点图形学的话，应该知道，这样的转化都是通过线性代数的计算来进行的。可以想见，我们的建模越精细，需要转换的顶点数量就越多，计算量就越大。而且，这里面每一个顶点位置的转换，互相之间没有依赖，是可以并行独立计算的。
顶点处理就是在进行线性变换图元处理在顶点处理完成之后呢，我们需要开始进行第二步，也就是图元处理。图元处理，其实就是要把顶点处理完成之后的各个顶点连起来，变成多边形。其实转化后的顶点，仍然是在一个三维空间里，只是第三维的Z轴，是正对屏幕的“深度”。所以我们针对这些多边形，需要做一个操作，叫剔除和裁剪（Cull and Clip），也就是把不在屏幕里面，或者一部分不在屏幕里面的内容给去掉，减少接下来流程的工作量。
栅格化在图元处理完成之后呢，渲染还远远没有完成。我们的屏幕分辨率是有限的。它一般是通过一个个“像素（Pixel）”来显示出内容的。所以，对于做完图元处理的多边形，我们要开始进行第三步操作。这个操作就是把它们转换成屏幕里面的一个个像素点。这个操作呢，就叫作栅格化。这个栅格化操作，有一个特点和上面的顶点处理是一样的，就是每一个图元都可以并行独立地栅格化。
片段处理在栅格化变成了像素点之后，我们的图还是“黑白”的。我们还需要计算每一个像素的颜色、透明度等信息，给像素点上色。这步操作，就是片段处理。这步操作，同样也可以每个片段并行、独立进行，和上面的顶点处理和栅格化一样。
像素操作最后一步呢，我们就要把不同的多边形的像素点“混合（Blending）”到一起。可能前面的多边形可能是半透明的，那么前后的颜色就要混合在一起变成一个新的颜色；或者前面的多边形遮挡住了后面的多边形，那么我们只要显示前面多边形的颜色就好了。最终，输出到显示设备。
经过这完整的5个步骤之后，我们就完成了从三维空间里的数据的渲染，变成屏幕上你可以看到的3D动画了。这样5个步骤的渲染流程呢，一般也被称之为图形流水线（Graphic Pipeline）。这个名字和我们讲解CPU里面的流水线非常相似，都叫Pipeline。
解放图形渲染的GPU我们可以想一想，如果用CPU来进行这个渲染过程，需要花上多少资源呢？我们可以通过一些数据来做个粗略的估算。
在上世纪90年代的时候，屏幕的分辨率还没有现在那么高。一般的CRT显示器也就是640×480的分辨率。这意味着屏幕上有30万个像素需要渲染。为了让我们的眼睛看到画面不晕眩，我们希望画面能有60帧。于是，每秒我们就要重新渲染60次这个画面。也就是说，每秒我们需要完成1800万次单个像素的渲染。从栅格化开始，每个像素有3个流水线步骤，即使每次步骤只有1个指令，那我们也需要5400万条指令，也就是54M条指令。
90年代的CPU的性能是多少呢？93年出货的第一代Pentium处理器，主频是60MHz，后续逐步推出了66MHz、75MHz、100MHz的处理器。以这个性能来看，用CPU来渲染3D图形，基本上就要把CPU的性能用完了。因为实际的每一个渲染步骤可能不止一个指令，我们的CPU可能根本就跑不动这样的三维图形渲染。
也就是在这个时候，Voodoo FX这样的图形加速卡登上了历史舞台。既然图形渲染的流程是固定的，那我们直接用硬件来处理这部分过程，不用CPU来计算是不是就好了？很显然，这样的硬件会比制造有同样计算性能的CPU要便宜得多。因为整个计算流程是完全固定的，不需要流水线停顿、乱序执行等等的各类导致CPU计算变得复杂的问题。我们也不需要有什么可编程能力，只要让硬件按照写好的逻辑进行运算就好了。
那个时候，整个顶点处理的过程还是都由CPU进行的，不过后续所有到图元和像素级别的处理都是通过Voodoo FX或者TNT这样的显卡去处理的。也就是从这个时代开始，我们能玩上“真3D”的游戏了。
不过，无论是Voodoo FX还是NVidia TNT。整个显卡的架构还不同于我们现代的显卡，也没有现代显卡去进行各种加速深度学习的能力。这个能力，要到NVidia提出Unified Shader Archicture才开始具备。这也是我们下一讲要讲的内容。
总结延伸这一讲里，我带你了解了一个基于多边形建模的三维图形的渲染过程。这个渲染过程需要经过顶点处理、图元处理、栅格化、片段处理以及像素操作这5个步骤。这5个步骤把存储在内存里面的多边形数据变成了渲染在屏幕上的画面。因为里面的很多步骤，都需要渲染整个画面里面的每一个像素，所以其实计算量是很大的。我们的CPU这个时候，就有点跑不动了。
于是，像3dfx和NVidia这样的厂商就推出了3D加速卡，用硬件来完成图元处理开始的渲染流程。这些加速卡和现代的显卡还不太一样，它们是用固定的处理流程来完成整个3D图形渲染的过程。不过，因为不用像CPU那样考虑计算和处理能力的通用性。我们就可以用比起CPU芯片更低的成本，更好地完成3D图形的渲染工作。而3D游戏的时代也是从这个时候开始的。
推荐阅读想要了解GPU的设计构造，一个有效的办法就是回头去看看GPU的历史。我建议你好好读一读Wikipedia里面，关于GPU的条目。另外，也可以看看Techspot上的The History of the Mordern Graphics Processor的系列文章。
课后思考我们上面说的图形加速卡，可以加速3D图形的渲染。那么，这些显卡对于传统的2D图形，也能够进行加速，让CPU摆脱这些负担吗？
欢迎留言和我分享你的疑惑和见解。你也可以把今天的内容，分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>30_图的表示：如何存储微博、微信等社交网络中的好友关系？</title><link>https://artisanbox.github.io/2/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/31/</guid><description>微博、微信、LinkedIn这些社交软件我想你肯定都玩过吧。在微博中，两个人可以互相关注；在微信中，两个人可以互加好友。那你知道，如何存储微博、微信等这些社交网络的好友关系吗？
这就要用到我们今天要讲的这种数据结构：图。实际上，涉及图的算法有很多，也非常复杂，比如图的搜索、最短路径、最小生成树、二分图等等。我们今天聚焦在图存储这一方面，后面会分好几节来依次讲解图相关的算法。
如何理解“图”？我们前面讲过了树这种非线性表数据结构，今天我们要讲另一种非线性表数据结构，图（Graph）。和树比起来，这是一种更加复杂的非线性表结构。
我们知道，树中的元素我们称为节点，图中的元素我们就叫做顶点（vertex）。从我画的图中可以看出来，图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫做边（edge）。
我们生活中就有很多符合图这种结构的例子。比如，开篇问题中讲到的社交网络，就是一个非常典型的图结构。
我们就拿微信举例子吧。我们可以把每个用户看作一个顶点。如果两个用户之间互加好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。其中，每个用户有多少个好友，对应到图中，就叫做顶点的度（degree），就是跟顶点相连接的边的条数。
实际上，微博的社交关系跟微信还有点不一样，或者说更加复杂一点。微博允许单向关注，也就是说，用户A关注了用户B，但用户B可以不关注用户A。那我们如何用图来表示这种单向的社交关系呢？
我们可以把刚刚讲的图结构稍微改造一下，引入边的“方向”的概念。
如果用户A关注了用户B，我们就在图中画一条从A到B的带箭头的边，来表示边的方向。如果用户A和用户B互相关注了，那我们就画一条从A指向B的边，再画一条从B指向A的边。我们把这种边有方向的图叫做“有向图”。以此类推，我们把边没有方向的图就叫做“无向图”。
我们刚刚讲过，无向图中有“度”这个概念，表示一个顶点有多少条边。在有向图中，我们把度分为入度（In-degree）和出度（Out-degree）。
顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。对应到微博的例子，入度就表示有多少粉丝，出度就表示关注了多少人。
前面讲到了微信、微博、无向图、有向图，现在我们再来看另一种社交软件：QQ。
QQ中的社交关系要更复杂一点。不知道你有没有留意过QQ亲密度这样一个功能。QQ不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经常往来，亲密度就比较低。如何在图中记录这种好友关系的亲密度呢？
这里就要用到另一种图，带权图（weighted graph）。在带权图中，每条边都有一个权重（weight），我们可以通过这个权重来表示QQ好友间的亲密度。
关于图的概念比较多，我今天也只是介绍了几个常用的，理解起来都不复杂，不知道你都掌握了没有？掌握了图的概念之后，我们再来看下，如何在内存中存储图这种数据结构呢？
邻接矩阵存储方法图最直观的一种存储方法就是，邻接矩阵（Adjacency Matrix）。
邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点i与顶点j之间有边，我们就将A[i][j]和A[j][i]标记为1；对于有向图来说，如果顶点i到顶点j之间，有一条箭头从顶点i指向顶点j的边，那我们就将A[i][j]标记为1。同理，如果有一条箭头从顶点j指向顶点i的边，我们就将A[j][i]标记为1。对于带权图，数组中就存储相应的权重。
用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。为什么这么说呢？
对于无向图来说，如果A[i][j]等于1，那A[j][i]也肯定等于1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。
还有，如果我们存储的是稀疏图（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。比如微信有好几亿的用户，对应到图上就是好几亿的顶点。但是每个用户的好友并不会很多，一般也就三五百个而已。如果我们用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。
但这也并不是说，邻接矩阵的存储方法就完全没有优点。首先，邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。其次，用邻接矩阵存储图的另外一个好处是方便计算。这是因为，用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。比如求解最短路径问题时会提到一个Floyd-Warshall算法，就是利用矩阵循环相乘若干次得到结果。
邻接表存储方法针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，邻接表（Adjacency List）。
我画了一张邻接表的图，你可以先看下。乍一看，邻接表是不是有点像散列表？每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。另外我需要说明一下，图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点，你可以自己画下。
还记得我们之前讲过的时间、空间复杂度互换的设计思想吗？邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储起来比较节省空间，但是使用起来就比较耗时间。
就像图中的例子，如果我们要确定，是否存在一条从顶点2到顶点4的边，那我们就要遍历顶点2对应的那条链表，看链表中是否存在顶点4。而且，我们前面也讲过，链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。
在散列表那几节里，我讲到，在基于链表法解决冲突的散列表中，如果链过长，为了提高查找效率，我们可以将链表换成其他更加高效的数据结构，比如平衡二叉查找树等。我们刚刚也讲到，邻接表长得很像散列。所以，我们也可以将邻接表同散列表一样进行“改进升级”。
我们可以将邻接表中的链表改成平衡二叉查找树。实际开发中，我们可以选择用红黑树。这样，我们就可以更加快速地查找两个顶点之间是否存在边了。当然，这里的二叉查找树可以换成其他动态数据结构，比如跳表、散列表等。除此之外，我们还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边。
解答开篇有了前面讲的理论知识，现在我们回过头来看开篇的问题，如何存储微博、微信等社交网络中的好友关系？
前面我们分析了，微博、微信是两种“图”，前者是有向图，后者是无向图。在这个问题上，两者的解决思路差不多，所以我只拿微博来讲解。
数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。针对微博用户关系，假设我们需要支持下面这样几个操作：
判断用户A是否关注了用户B；
判断用户A是否是用户B的粉丝；
用户A关注用户B；
用户A取消关注用户B；
根据用户名称的首字母排序，分页获取用户的粉丝列表；
根据用户名称的首字母排序，分页获取用户的关注列表。
关于如何存储一个图，前面我们讲到两种主要的存储方法，邻接矩阵和邻接表。因为社交网络是一张稀疏图，使用邻接矩阵存储比较浪费存储空间。所以，这里我们采用邻接表来存储。
不过，用一个邻接表来存储这种有向图是不够的。我们去查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就是用户的粉丝列表，是非常困难的。
基于此，我们需要一个逆邻接表。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。对应到图上，邻接表中，每个顶点的链表中，存储的就是这个顶点指向的顶点，逆邻接表中，每个顶点的链表中，存储的是指向这个顶点的顶点。如果要查找某个用户关注了哪些用户，我们可以在邻接表中查找；如果要查找某个用户被哪些用户关注了，我们从逆邻接表中查找。
基础的邻接表不适合快速判断两个用户之间是否是关注与被关注的关系，所以我们选择改进版本，将邻接表中的链表改为支持快速查找的动态数据结构。选择哪种动态数据结构呢？红黑树、跳表、有序动态数组还是散列表呢？
因为我们需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或者关注列表，用跳表这种结构再合适不过了。这是因为，跳表插入、删除、查找都非常高效，时间复杂度是O(logn)，空间复杂度上稍高，是O(n)。最重要的一点，跳表中存储的数据本来就是有序的了，分页获取粉丝列表或关注列表，就非常高效。
如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，我们可以将整个社交关系存储在内存中，上面的解决思路是没有问题的。但是如果像微博那样有上亿的用户，数据规模太大，我们就无法全部存储在内存中了。这个时候该怎么办呢？
我们可以通过哈希算法等数据分片方式，将邻接表存储在不同的机器上。你可以看下面这幅图，我们在机器1上存储顶点1，2，3的邻接表，在机器2上，存储顶点4，5的邻接表。逆邻接表的处理方式也一样。当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找。
除此之外，我们还有另外一种解决思路，就是利用外部存储（比如硬盘），因为外部存储的存储空间要比内存会宽裕很多。数据库是我们经常用来持久化存储关系数据的，所以我这里介绍一种数据库的存储方式。
我用下面这张表来存储这样一个图。为了高效地支持前面定义的操作，我们可以在表上建立多个索引，比如第一列、第二列，给这两列都建立索引。
内容小结今天我们学习了图这种非线性表数据结构，关于图，你需要理解这样几个概念：无向图、有向图、带权图、顶点、边、度、入度、出度。除此之外，我们还学习了图的两个主要的存储方式：邻接矩阵和邻接表。
邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。
课后思考 关于开篇思考题，我们只讲了微博这种有向图的解决思路，那像微信这种无向图，应该怎么存储呢？你可以照着我的思路，自己做一下练习。
除了我今天举的社交网络可以用图来表示之外，符合图这种结构特点的例子还有很多，比如知识图谱（Knowledge Graph）。关于图这种数据结构，你还能想到其他生活或者工作中的例子吗？</description></item><item><title>30_答疑文章（二）：用动态的观点看加锁</title><link>https://artisanbox.github.io/1/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/30/</guid><description>在第20和21篇文章中，我和你介绍了InnoDB的间隙锁、next-key lock，以及加锁规则。在这两篇文章的评论区，出现了很多高质量的留言。我觉得通过分析这些问题，可以帮助你加深对加锁规则的理解。
所以，我就从中挑选了几个有代表性的问题，构成了今天这篇答疑文章的主题，即：用动态的观点看加锁。
为了方便你理解，我们再一起复习一下加锁规则。这个规则中，包含了两个“原则”、两个“优化”和一个“bug”：
原则1：加锁的基本单位是next-key lock。希望你还记得，next-key lock是前开后闭区间。 原则2：查找过程中访问到的对象才会加锁。 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 接下来，我们的讨论还是基于下面这个表t：
CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 不等号条件里的等值查询有同学对“等值查询”提出了疑问：等值查询和“遍历”有什么区别？为什么我们文章的例子里面，where条件是不等号，这个过程里也有等值查询？
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;我们一起来看下这个例子，分析一下这条查询语句的加锁范围：
begin; select * from t where id&amp;gt;9 and id&amp;lt;12 order by id desc for update; 利用上面的加锁规则，我们知道这个语句的加锁范围是主键索引上的 (0,5]、(5,10]和(10, 15)。也就是说，id=15这一行，并没有被加上行锁。为什么呢？
我们说加锁单位是next-key lock，都是前开后闭区间，但是这里用到了优化2，即索引上的等值查询，向右遍历的时候id=15不满足条件，所以next-key lock退化为了间隙锁 (10, 15)。</description></item><item><title>30｜面向对象编程第2步：剖析一些技术细节</title><link>https://artisanbox.github.io/3/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/32/</guid><description>你好，我是宫文学。
在上一节课里，我们实现了基本的面向对象特性，包括声明类、创建对象、访问对象的属性和方法等等。
本来，我想马上进入对象的继承和多态的环节。但在准备示例程序的过程中，我发现有一些技术细节还是值得单独拿出来，和你剖析一下的，以免你在看代码的时候可能会抓不住关键点，不好消化。俗话说，魔鬼都在细节中。搞技术的时候，经常一个小细节就会成为拦路虎。
我想给你剖析的技术细节呢，主要是语义分析和AST解释器方面的。通过研究这些技术细节，你会对面向对象的底层实现技术有更加细致的了解。
技术细节：语义分析语义分析方面的技术细节包括：如何设计和保存class的符号、如何设计class对应的类型、如何给This表达式做引用消解、如何消解点符号表达式中变量等等。
首先看看第一个问题，就是如何在符号表里保存class的符号。
我们知道，符号表里存储的是我们自己在程序里声明出来的那些符号。在上一节课之前，我们在符号表里主要保存了两类数据：变量和函数。而class也是我们用程序声明出来的，所以也可以被纳入到符号表里保存。
你应该还记得，我们的符号表采用的是一种层次化的数据结构，也就是Scope的层层嵌套。而且，TypeScript只允许在顶层的作用域中声明class，不允许在class内部或函数内部嵌套声明class，所以class的符号总是被保存在顶层的Scope中。
其实在TypeScript中，我们还可以在一个文件（或模块）里引用另一个文件里定义的类，这样你就能在当前文件里使用这些外部的类了。但我们其实并不是把外部类的全部代码都导入进来，而是只需要引入它们的符号就行了。在class符号里有这些类的描述信息，这些信息叫做元数据。元数据里会包括它都有哪些属性、哪些方法，分别都是什么类型的，等等。这些保存在符号里的这些信息，其实足够我们使用这个类了，我们不用去管这个类的实现细节。
你也可以对比一下FunctionSymbol的设计。FunctionSymbol里会记录函数的名称、参数个数、参数类型和返回值类型。你通过这些信息就可以调用一个函数，完全不用管这个函数的实现细节，也不用区分它是内置函数，还是你自己写的函数，调用方式都是一样的。
说完了class的符号设计和保存，我们再进入第二个技术点，讨论一下class的类型问题。
我们说过，class给我们提供了一个自定义类型的能力。那这个自定义的类型如何表达呢？
在前面的课程中，我们已经形成了自己的一套类型体系，用于进行类型计算。而在这个类型体系中，有一种类型叫做NamedType。这些类型都有名称，并且还有父类型。我们用NamedType首先表示了Number、String、Boolean这些TypeScript官方规定的类型，还用它来表示了Integer和Decimal这两个Number类型的子类型，这两个类型是我们自己设计的。
那其实NamedType就可以用来表示一个class的类型信息，以便参与类型计算。
这里你可能会提出一个问题：class本身不就是类型吗？我们在ClassSymbol里已经保存了类的各种描述信息，为什么还要用到NamedType呢？
采用这样的设计有几个原因。首先，并不是所有的类型都是用class定义出来的。比如系统里有一些内置的类型。再比如，如果你用TypeScript调用其他语言编写的库，比如一些AI库，你可以把其他语言的类型映射成TypeScript语言的类型。所以说，类型的来源并不只有自定义的class。
第二个原因是由类型计算的性质导致的。在我们目前的类型计算中，我们基本只用到了类型的名称和父子类型关系这两个信息，其他信息都没有用到，所以就不需要在类型体系中涉及。
不过，使用NamedType这种设计其实有个潜台词，就是我们类型系统是Norminal的类型系统。这是什么意思呢？Norminal的意思是说，我们在做类型比较的时候，仅仅是通过类型的名称来确定两个类型是否相同，或者是否存在父子关系。与之对应的另一种类型系统是structural的，也就是只要两个类型拥有的方法是一致的，那就认为它们是相同的类型。像Java、C++这些语言，采用的是Nominal的类型系统，而TypeScript和Go等语言，采用的是Structural的类型系统。这个话题我们就不展开了，有兴趣你可以多查阅这方面的资料。
不过，为了简单，我们目前的实现暂且采用Norminal的类型，只通过名称来区分相互之间的关系。
在分析完了class的符号和类型之后，我们再来看看它的用途。这就进入了第三个技术点，也就是如何消解This表达式。
我们知道，this表达式的使用场景，是在类的方法中指代当前对象的数据。那么它的类型是什么呢？在做引用消解的时候，应该让它引用哪个符号呢？
this的类型，不用说，肯定就是指当前的这个class对应的类型，这个不会有疑问。
那它应该被关联到什么符号上呢？我们知道，当程序中出现某个变量名称或函数名称的时候，我们会把这些AST节点关联到符号表里的VarSymbol和FunctionSymbol上，this当然也不会例外。this在被用于程序中的时候，其用法跟普通的一个对象类型的变量是没有区别的。那我们是否应该在每个用到this的方法里，创建一个this变量呢？
这样当然可以，但其实也没有必要。因为每个函数都可能用到this关键字，所以如果在每个方法里都创建一个this变量有点啰嗦。我们只需要简单地把this变量跟ClassSymbol关联起来就行了，在使用的时候也没有什么不方便的。我们下面在讲AST解释器的实现机制里，会进一步看看如何通过this来访问对象数据。
接下来，我们再看看第四个技术点：对点符号表达式的引用消解。
在上一节课的示例程序中，我们可以通过“this.weight”、“mammal.color”、“mammal.speak()”这样的点符号表达式访问对象的属性和方法。
我们知道，在做引用消解的时候，需要把这里面的this、mammal、color、speak()都关联到相应的符号上，这样我们就知道这些标识符都是在哪里声明的了。
不过，之前我们不是已经都做过引用消解了吗？为什么这里又要把点符号的引用消解单独拎出来分析呢？
这是因为，之前我们做变量和函数的引用消解的时候，只需要利用变量和函数的名称信息就行了。但在点符号这边，只依赖名称是不行的，还必须依赖类型信息。
比如，对于mammal.color这个表达式。我们在上下文里，很容易找到mammal是在哪里声明的。但color就不一样了。这个color是在哪里声明的呢？这个时候，你就必须知道mammal的类型，然后再找到mammal的定义。这样，你才能知道mammal是否有一个叫做color的属性。
那你可能说，这很简单呀，我们只需要先计算出每个表达式的类型，然后再做引用消解就可以了呀。
没那么简单。为什么呢？因为类型计算的时候，也需要用到引用消解的结果。比如在mammal.color中，如果你不知道mammal是在哪里声明的，就不能知道它的类型，那也就更没有办法去消解color属性了。
所以，在语义分析中，我们需要把类型计算和引用消解交叉着进行才行，不能分成单独的两个阶段。在《编译原理实战课》中，我曾经分析过Java的前端编译器的特点。这种多个分析工作穿插执行的情况，是Java编译器代码中最难以阅读和跟踪的部分，但你要知道这背后的原因。
我还给你提供了一个更复杂一点的例子，你可以先看一下：
class Human{ swim(){ console.log("swim"); } } class Bird{ fly(){ console.log(&amp;ldquo;fly&amp;rdquo;); } }
function foo(animal:Human|Bird){ if (animal instanceof Human){ animal.swim(); } else{ animal.fly(); } } 这个例子里有Human和Bird两个类，Human有swim()方法，而Bird有fly()方法。不过，我们可以声明一个变量animal，是Human和Bird的联合类型。那么，你什么时候可以调用animal的swim()方法，什么时候可以调用它的fly()方法呢？这个时候你就要基于数据流分析方法，先进行类型的窄化，然后才能把swim()和fly()两个方法正确地消解。
好了，关于语义分析部分的一些技术点，我就先剖析到这里。接着我们看看AST解释器中的一些技术。
技术细节：Ast解释器实现Ast解释器的时候，我们也涉及了不少的技术细节，包括如何表示对象数据、对象数据在栈桢中的存储方式、如何以左值和右值的方式访问对象的属性等。
首先我们看看如何表示对象的数据。上一节课里，我们提到用一个Map&amp;lt;Symbol, any&amp;gt;来存储对象数据就行了。我们在类中声明的每一个属性，都对应着一个Symbol，所以我们就可以用Symbol作为key，来访问对象的数据。
其实，我们的栈桢也是这样设计的。每个栈桢也是一个Map&amp;lt;Symbol, any&amp;gt;。你如果想访问哪个变量的数据，就把变量的Symbol作为key，到Map里去查找就好了。
不过，如果只用一个Map来代表对象数据，数据的接收方可能不知道该数据是属于哪个类的，在实现一些功能的时候不方便。所以我们就专门设计了一个PlayObject对象，在对象里包含了ClassSymbol和对象数据两方面的信息，具体实现如下：
class PlayObject{ classSym:ClassSymbol; data:Map&amp;lt;Symbol,any&amp;gt; = new Map(); constructor(classSym:ClassSymbol){ this.</description></item><item><title>31_GPU（下）：为什么深度学习需要使用GPU？</title><link>https://artisanbox.github.io/4/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/31/</guid><description>上一讲，我带你一起看了三维图形在计算机里的渲染过程。这个渲染过程，分成了顶点处理、图元处理、 栅格化、片段处理，以及最后的像素操作。这一连串的过程，也被称之为图形流水线或者渲染管线。
因为要实时计算渲染的像素特别地多，图形加速卡登上了历史的舞台。通过3dFx的Voodoo或者NVidia的TNT这样的图形加速卡，CPU就不需要再去处理一个个像素点的图元处理、栅格化和片段处理这些操作。而3D游戏也是从这个时代发展起来的。
你可以看这张图，这是“古墓丽影”游戏的多边形建模的变化。这个变化，则是从1996年到2016年，这20年来显卡的进步带来的。
图片来源Shader的诞生和可编程图形处理器不知道你有没有发现，在Voodoo和TNT显卡的渲染管线里面，没有“顶点处理“这个步骤。在当时，把多边形的顶点进行线性变化，转化到我们的屏幕的坐标系的工作还是由CPU完成的。所以，CPU的性能越好，能够支持的多边形也就越多，对应的多边形建模的效果自然也就越像真人。而3D游戏的多边形性能也受限于我们CPU的性能。无论你的显卡有多快，如果CPU不行，3D画面一样还是不行。
所以，1999年NVidia推出的GeForce 256显卡，就把顶点处理的计算能力，也从CPU里挪到了显卡里。不过，这对于想要做好3D游戏的程序员们还不够，即使到了GeForce 256。整个图形渲染过程都是在硬件里面固定的管线来完成的。程序员们在加速卡上能做的事情呢，只有改配置来实现不同的图形渲染效果。如果通过改配置做不到，我们就没有什么办法了。
这个时候，程序员希望我们的GPU也能有一定的可编程能力。这个编程能力不是像CPU那样，有非常通用的指令，可以进行任何你希望的操作，而是在整个的渲染管线（Graphics Pipeline）的一些特别步骤，能够自己去定义处理数据的算法或者操作。于是，从2001年的Direct3D 8.0开始，微软第一次引入了可编程管线（Programable Function Pipeline）的概念。
早期的可编程管线的GPU，提供了单独的顶点处理和片段处理（像素处理）的着色器一开始的可编程管线呢，仅限于顶点处理（Vertex Processing）和片段处理（Fragment Processing）部分。比起原来只能通过显卡和Direct3D这样的图形接口提供的固定配置，程序员们终于也可以开始在图形效果上开始大显身手了。
这些可以编程的接口，我们称之为Shader，中文名称就是着色器。之所以叫“着色器”，是因为一开始这些“可编程”的接口，只能修改顶点处理和片段处理部分的程序逻辑。我们用这些接口来做的，也主要是光照、亮度、颜色等等的处理，所以叫着色器。
这个时候的GPU，有两类Shader，也就是Vertex Shader和Fragment Shader。我们在上一讲看到，在进行顶点处理的时候，我们操作的是多边形的顶点；在片段操作的时候，我们操作的是屏幕上的像素点。对于顶点的操作，通常比片段要复杂一些。所以一开始，这两类Shader都是独立的硬件电路，也各自有独立的编程接口。因为这么做，硬件设计起来更加简单，一块GPU上也能容纳下更多的Shader。
不过呢，大家很快发现，虽然我们在顶点处理和片段处理上的具体逻辑不太一样，但是里面用到的指令集可以用同一套。而且，虽然把Vertex Shader和Fragment Shader分开，可以减少硬件设计的复杂程度，但是也带来了一种浪费，有一半Shader始终没有被使用。在整个渲染管线里，Vertext Shader运行的时候，Fragment Shader停在那里什么也没干。Fragment Shader在运行的时候，Vertext Shader也停在那里发呆。
本来GPU就不便宜，结果设计的电路有一半时间是闲着的。喜欢精打细算抠出每一分性能的硬件工程师当然受不了了。于是，统一着色器架构（Unified Shader Architecture）就应运而生了。
既然大家用的指令集是一样的，那不如就在GPU里面放很多个一样的Shader硬件电路，然后通过统一调度，把顶点处理、图元处理、片段处理这些任务，都交给这些Shader去处理，让整个GPU尽可能地忙起来。这样的设计，就是我们现代GPU的设计，就是统一着色器架构。
有意思的是，这样的GPU并不是先在PC里面出现的，而是来自于一台游戏机，就是微软的XBox 360。后来，这个架构才被用到ATI和NVidia的显卡里。这个时候的“着色器”的作用，其实已经和它的名字关系不大了，而是变成了一个通用的抽象计算模块的名字。
正是因为Shader变成一个“通用”的模块，才有了把GPU拿来做各种通用计算的用法，也就是GPGPU（General-Purpose Computing on Graphics Processing Units，通用图形处理器）。而正是因为GPU可以拿来做各种通用的计算，才有了过去10年深度学习的火热。
现代GPU的三个核心创意讲完了现代GPU的进化史，那么接下来，我们就来看看，为什么现代的GPU在图形渲染、深度学习上能那么快。
芯片瘦身我们先来回顾一下，之前花了很多讲仔细讲解的现代CPU。现代CPU里的晶体管变得越来越多，越来越复杂，其实已经不是用来实现“计算”这个核心功能，而是拿来实现处理乱序执行、进行分支预测，以及我们之后要在存储器讲的高速缓存部分。
而在GPU里，这些电路就显得有点多余了，GPU的整个处理过程是一个流式处理（Stream Processing）的过程。因为没有那么多分支条件，或者复杂的依赖关系，我们可以把GPU里这些对应的电路都可以去掉，做一次小小的瘦身，只留下取指令、指令译码、ALU以及执行这些计算需要的寄存器和缓存就好了。一般来说，我们会把这些电路抽象成三个部分，就是下面图里的取指令和指令译码、ALU和执行上下文。
多核并行和SIMT这样一来，我们的GPU电路就比CPU简单很多了。于是，我们就可以在一个GPU里面，塞很多个这样并行的GPU电路来实现计算，就好像CPU里面的多核CPU一样。和CPU不同的是，我们不需要单独去实现什么多线程的计算。因为GPU的运算是天然并行的。
我们在上一讲里面其实已经看到，无论是对多边形里的顶点进行处理，还是屏幕里面的每一个像素进行处理，每个点的计算都是独立的。所以，简单地添加多核的GPU，就能做到并行加速。不过光这样加速还是不够，工程师们觉得，性能还有进一步被压榨的空间。
我们在第27讲里面讲过，CPU里有一种叫作SIMD的处理技术。这个技术是说，在做向量计算的时候，我们要执行的指令是一样的，只是同一个指令的数据有所不同而已。在GPU的渲染管线里，这个技术可就大有用处了。
无论是顶点去进行线性变换，还是屏幕上临近像素点的光照和上色，都是在用相同的指令流程进行计算。所以，GPU就借鉴了CPU里面的SIMD，用了一种叫作SIMT（Single Instruction，Multiple Threads）的技术。SIMT呢，比SIMD更加灵活。在SIMD里面，CPU一次性取出了固定长度的多个数据，放到寄存器里面，用一个指令去执行。而SIMT，可以把多条数据，交给不同的线程去处理。
各个线程里面执行的指令流程是一样的，但是可能根据数据的不同，走到不同的条件分支。这样，相同的代码和相同的流程，可能执行不同的具体的指令。这个线程走到的是if的条件分支，另外一个线程走到的就是else的条件分支了。
于是，我们的GPU设计就可以进一步进化，也就是在取指令和指令译码的阶段，取出的指令可以给到后面多个不同的ALU并行进行运算。这样，我们的一个GPU的核里，就可以放下更多的ALU，同时进行更多的并行运算了。
GPU里的“超线程”虽然GPU里面的主要以数值计算为主。不过既然已经是一个“通用计算”的架构了，GPU里面也避免不了会有if…else这样的条件分支。但是，在GPU里我们可没有CPU这样的分支预测的电路。这些电路在上面“芯片瘦身”的时候，就已经被我们砍掉了。
所以，GPU里的指令，可能会遇到和CPU类似的“流水线停顿”问题。想到流水线停顿，你应该就能记起，我们之前在CPU里面讲过超线程技术。在GPU上，我们一样可以做类似的事情，也就是遇到停顿的时候，调度一些别的计算任务给当前的ALU。
和超线程一样，既然要调度一个不同的任务过来，我们就需要针对这个任务，提供更多的执行上下文。所以，一个Core里面的执行上下文的数量，需要比ALU多。
GPU在深度学习上的性能差异在通过芯片瘦身、SIMT以及更多的执行上下文，我们就有了一个更擅长并行进行暴力运算的GPU。这样的芯片，也正适合我们今天的深度学习的使用场景。
一方面，GPU是一个可以进行“通用计算”的框架，我们可以通过编程，在GPU上实现不同的算法。另一方面，现在的深度学习计算，都是超大的向量和矩阵，海量的训练样本的计算。整个计算过程中，没有复杂的逻辑和分支，非常适合GPU这样并行、计算能力强的架构。
我们去看NVidia 2080显卡的技术规格，就可以算出，它到底有多大的计算能力。
2080一共有46个SM（Streaming Multiprocessor，流式处理器），这个SM相当于GPU里面的GPU Core，所以你可以认为这是一个46核的GPU，有46个取指令指令译码的渲染管线。每个SM里面有64个Cuda Core。你可以认为，这里的Cuda Core就是我们上面说的ALU的数量或者Pixel Shader的数量，46x64呢一共就有2944个Shader。然后，还有184个TMU，TMU就是Texture Mapping Unit，也就是用来做纹理映射的计算单元，它也可以认为是另一种类型的Shader。
图片来源2080 Super显卡有48个SM，比普通版的2080多2个。每个SM（SM也就是GPU Core）里有64个Cuda Core，也就是Shader2080的主频是1515MHz，如果自动超频（Boost）的话，可以到1700MHz。而NVidia的显卡，根据硬件架构的设计，每个时钟周期可以执行两条指令。所以，能做的浮点数运算的能力，就是：</description></item><item><title>31_深度和广度优先搜索：如何找出社交网络中的三度好友关系？</title><link>https://artisanbox.github.io/2/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/32/</guid><description>上一节我们讲了图的表示方法，讲到如何用有向图、无向图来表示一个社交网络。在社交网络中，有一个六度分割理论，具体是说，你与世界上的另一个人间隔的关系不会超过六度，也就是说平均只需要六步就可以联系到任何两个互不相识的人。
一个用户的一度连接用户很好理解，就是他的好友，二度连接用户就是他好友的好友，三度连接用户就是他好友的好友的好友。在社交网络中，我们往往通过用户之间的连接关系，来实现推荐“可能认识的人”这么一个功能。今天的开篇问题就是，给你一个用户，如何找出这个用户的所有三度（其中包含一度、二度和三度）好友关系？
这就要用到今天要讲的深度优先和广度优先搜索算法。
什么是“搜索”算法？我们知道，算法是作用于具体数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。这是因为，图这种数据结构的表达能力很强，大部分涉及搜索的场景都可以抽象成“图”。
图上的搜索算法，最直接的理解就是，在图中找出从一个顶点出发，到另一个顶点的路径。具体方法有很多，比如今天要讲的两种最简单、最“暴力”的深度优先、广度优先搜索，还有A*、IDA*等启发式搜索算法。
我们上一节讲过，图有两种主要存储方法，邻接表和邻接矩阵。今天我会用邻接表来存储图。
我这里先给出图的代码实现。需要说明一下，深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。在今天的讲解中，我都针对无向图来讲解。
public class Graph { // 无向图 private int v; // 顶点的个数 private LinkedList&amp;lt;Integer&amp;gt; adj[]; // 邻接表 public Graph(int v) { this.v = v; adj = new LinkedList[v]; for (int i=0; i&amp;lt;v; ++i) { adj[i] = new LinkedList&amp;lt;&amp;gt;(); } }
public void addEdge(int s, int t) { // 无向图一条边存两次 adj[s].add(t); adj[t].add(s); } } 广度优先搜索（BFS）广度优先搜索（Breadth-First-Search），我们平常都简称BFS。直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。理解起来并不难，所以我画了一张示意图，你可以看下。
尽管广度优先搜索的原理挺简单，但代码实现还是稍微有点复杂度。所以，我们重点讲一下它的代码实现。
这里面，bfs()函数就是基于之前定义的，图的广度优先搜索的代码实现。其中s表示起始顶点，t表示终止顶点。我们搜索一条从s到t的路径。实际上，这样求得的路径就是从s到t的最短路径。
public void bfs(int s, int t) { if (s == t) return; boolean[] visited = new boolean[v]; visited[s]=true; Queue&amp;lt;Integer&amp;gt; queue = new LinkedList&amp;lt;&amp;gt;(); queue.</description></item><item><title>31_误删数据后除了跑路，还能怎么办？</title><link>https://artisanbox.github.io/1/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/31/</guid><description>今天我要和你讨论的是一个沉重的话题：误删数据。
在前面几篇文章中，我们介绍了MySQL的高可用架构。当然，传统的高可用架构是不能预防误删数据的，因为主库的一个drop table命令，会通过binlog传给所有从库和级联从库，进而导致整个集群的实例都会执行这个命令。
虽然我们之前遇到的大多数的数据被删，都是运维同学或者DBA背锅的。但实际上，只要有数据操作权限的同学，都有可能踩到误删数据这条线。
今天我们就来聊聊误删数据前后，我们可以做些什么，减少误删数据的风险，和由误删数据带来的损失。
为了找到解决误删数据的更高效的方法，我们需要先对和MySQL相关的误删数据，做下分类：
使用delete语句误删数据行；
使用drop table或者truncate table语句误删数据表；
使用drop database语句误删数据库；
使用rm命令误删整个MySQL实例。
误删行在第24篇文章中，我们提到如果是使用delete语句误删了数据行，可以用Flashback工具通过闪回把数据恢复回来。
Flashback恢复数据的原理，是修改binlog的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保binlog_format=row 和 binlog_row_image=FULL。
具体恢复数据时，对单个事务做如下处理：
对于insert语句，对应的binlog event类型是Write_rows event，把它改成Delete_rows event即可；
同理，对于delete语句，也是将Delete_rows event改为Write_rows event；
而如果是Update_rows的话，binlog里面记录了数据行修改前和修改后的值，对调这两行的位置即可。
如果误操作不是一个，而是多个，会怎么样呢？比如下面三个事务：
(A)delete ... (B)insert ... (C)update ... 现在要把数据库恢复回这三个事务操作之前的状态，用Flashback工具解析binlog后，写回主库的命令是：
(reverse C)update ... (reverse B)delete ... (reverse A)insert ... 也就是说，如果误删数据涉及到了多个事务的话，需要将事务的顺序调过来再执行。
需要说明的是，我不建议你直接在主库上执行这些操作。
恢复数据比较安全的做法，是恢复出一个备份，或者找一个从库作为临时库，在这个临时库上执行这些操作，然后再将确认过的临时库的数据，恢复回主库。
为什么要这么做呢？
这是因为，一个在执行线上逻辑的主库，数据状态的变更往往是有关联的。可能由于发现数据问题的时间晚了一点儿，就导致已经在之前误操作的基础上，业务代码逻辑又继续修改了其他数据。所以，如果这时候单独恢复这几行数据，而又未经确认的话，就可能会出现对数据的二次破坏。
当然，我们不止要说误删数据的事后处理办法，更重要是要做到事前预防。我有以下两个建议：
把sql_safe_updates参数设置为on。这样一来，如果我们忘记在delete或者update语句中写where条件，或者where条件里面没有包含索引字段的话，这条语句的执行就会报错。
代码上线前，必须经过SQL审计。</description></item><item><title>31｜面向对象编程第3步：支持继承和多态</title><link>https://artisanbox.github.io/3/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/33/</guid><description>你好，我是宫文学。
经过前面两节课对面向对象编程的学习，今天这节课，我们终于要来实现到面向对象中几个最核心的功能了。
面向对象编程是目前使用最广泛的编程范式之一。通常，我们说面向对象编程的核心特性有封装、继承和多态这几个方面。只要实现了这几点，就可以获得面向对象编程的各种优势，比如提高代码的可重用性、可扩展性、提高编程效率，等等。
这节课，我们就先探讨一下面向对象的这些核心特性是如何实现的，然后我会带着你动手实现一下，破解其中的技术秘密。了解了这些实现机制，能够帮助你深入理解现代计算机语言更深层次的机制。
首先，我们先来分析面向对象的几个核心特性，并梳理一下实现思路。
面向对象的核心特性及其实现机制第一，是封装特性。
封装是指我们可以把对象内部的数据和实现细节隐藏起来，只对外提供一些公共的接口。这样做的好处，是提高了代码的复用性和安全性，因为内部实现细节只有代码的作者才能够修改，并且这种修改不会影响到类的使用者。
其实封装特性，我们在上两节课已经差不多实现完了。因为我们提供了方法的机制，让方法可以访问对象的内部数据。之后，我们只需要给属性和方法添加访问权限的修饰成分就可以了。比如我们可以声明某些属性和方法是private的，这样，属性和方法就只能由内部的方法去访问了。而对访问权限的检查，我们在语义分析阶段就可以轻松做到。
上一节课，我们已经分析了如何处理点符号表达式。你在程序里可以分析出点号左边的表达式的类型信息，也可以获得对象的属性和方法。再进一步，我们可以给这些属性和方法添加上访问权限的信息，那么这些私有的属性就只可以在内部访问了，比如使用this.xxx表达式，等等。而公有的属性仍然可以在外部访问，跟现在的实现没有区别。
第二，我们看看继承。
用直白的话来说，继承指的是一个class，可以免费获得父类中的属性和方法，从而降低了开发工作量，提高了代码的复用度。
我写了一个示例程序，你可以看一下：
function println(data:any=&amp;quot;&amp;quot;){ console.log(data); } class Mammal{ weight:number = 0; // weight2; color:string; constructor(weight:number, color:string){ this.weight = weight;
this.color = color; } speak(){ println(&amp;quot;Hello, I&amp;rsquo;m a mammal, and my weight is &amp;quot; + this.weight + &amp;quot;.&amp;quot;); } }
class Human extends Mammal{ //新的语法要素：extends name:string; constructor(weight:number, color:string, name:string){ super(weight,color); //新的语法要素：super this.name = name; } swim(){ println(&amp;quot;My weight is &amp;quot; +this.</description></item><item><title>32_FPGA和ASIC：计算机体系结构的黄金时代</title><link>https://artisanbox.github.io/4/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/32/</guid><description>过去很长一段时间里，大家在讲到高科技、互联网、信息技术的时候，谈的其实都是“软件”。从1995年微软发布Windows 95开始，高科技似乎就等同于软件业和互联网。著名的风险投资基金Andreessen Horowitz的合伙人Marc Andreessen，在2011年发表了一篇博客，声称“Software is Eating the World”。Marc Andreessen，不仅是投资人，更是Netscape的创始人之一。他当时的搭档就是我们在前两讲提过的SGI创始人Jim Clark。
的确，过去20年计算机工业界的中心都在软件上。似乎硬件对大家来说，慢慢变成了一个黑盒子。虽然必要，但却显得有点无关紧要。
不过，在上世纪70～80年代，计算机的世界可不是这样的。那个时候，计算机工业届最激动人心的，是层出不穷的硬件。无论是Intel的8086，还是摩托罗拉的68000，这样用于个人电脑的CPU，还是直到今天大家还会提起的Macintosh，还有史上最畅销的计算机Commodore 64，都是在那个时代被创造出来的。
电视剧Halt and Catch Fire，灵感应该就是来自第一台笔记本电脑Compaq Portable的诞生不过，随着计算机主频提升越来越困难。这几年，计算机硬件又进入了一个新的、快速发展的时期。
从树莓派基金会这样的非盈利组织开发35美元的单片机，到Google这样的巨头为了深度学习专门开发出来的TPU，新的硬件层出不穷，也无怪乎David Patterson老爷爷，去年在拿图灵奖之后专门发表讲话，说计算机体系结构又进入了一个黄金时代。那今天我就带你一起来看看，FPGA和ASIC这两个最近比较时髦的硬件发展。
FPGA之前我们讲解CPU的硬件实现的时候说过，其实CPU其实就是一些简单的门电路像搭积木一样搭出来的。从最简单的门电路，搭建成半加器、全加器，然后再搭建成完整功能的ALU。这些电路里呢，有完成各种实际计算功能的组合逻辑电路，也有用来控制数据访问，创建出寄存器和内存的时序逻辑电路。如果你对这块儿内容印象不深，可以回顾一下第12讲到第14讲的内容，以及第17讲的内容。
好了，那现在我问你一个问题，在我们现代CPU里面，有多少个晶体管这样的电路开关呢？这个答案说出来有点儿吓人。一个四核i7的Intel CPU，上面的晶体管数量差不多有20亿个。那接着问题就来了，我们要想设计一个CPU，就要想办法连接这20亿个晶体管。
这已经够难了，后面还有更难的。就像我们写程序一样，连接晶体管不是一次就能完事儿了的。设计更简单一点儿的专用于特定功能的芯片，少不了要几个月。而设计一个CPU，往往要以“年”来计。在这个过程中，硬件工程师们要设计、验证各种各样的技术方案，可能会遇到各种各样的Bug。如果我们每次验证一个方案，都要单独设计生产一块芯片，那这个代价也太高了。
我们有没有什么办法，不用单独制造一块专门的芯片来验证硬件设计呢？能不能设计一个硬件，通过不同的程序代码，来操作这个硬件之前的电路连线，通过“编程”让这个硬件变成我们设计的电路连线的芯片呢？
图片来源XILINX的FPGA芯片这个，就是我们接下来要说的FPGA，也就是现场可编程门阵列（Field-Programmable Gate Array）。看到这个名字，你可能要说了，这里面每个单词单独我都认识，放到一起就不知道是什么意思了。
没关系，我们就从FPGA里面的每一个字符，一个一个来看看它到底是什么意思。
P代表Programmable，这个很容易理解。也就是说这是一个可以通过编程来控制的硬件。 G代表Gate也很容易理解，它就代表芯片里面的门电路。我们能够去进行编程组合的就是这样一个一个门电路。 A代表的Array，叫作阵列，说的是在一块FPGA上，密密麻麻列了大量Gate这样的门电路。 最后一个F，不太容易理解。它其实是说，一块FPGA这样的板子，可以在“现场”多次进行编程。它不像PAL（Programmable Array Logic，可编程阵列逻辑）这样更古老的硬件设备，只能“编程”一次，把预先写好的程序一次性烧录到硬件里面，之后就不能再修改了。 这么看来，其实“FPGA”这样的组合，基本上解决了我们前面说的想要设计硬件的问题。我们可以像软件一样对硬件编程，可以反复烧录，还有海量的门电路，可以组合实现复杂的芯片功能。
不过，相信你和我一样好奇，我们究竟怎么对硬件进行编程呢？我们之前说过，CPU其实就是通过晶体管，来实现各种组合逻辑或者时序逻辑。那么，我们怎么去“编程”连接这些线路呢？
FPGA的解决方案很精巧，我把它总结为这样三个步骤。
第一，用存储换功能实现组合逻辑。在实现CPU的功能的时候，我们需要完成各种各样的电路逻辑。在FPGA里，这些基本的电路逻辑，不是采用布线连接的方式进行的，而是预先根据我们在软件里面设计的逻辑电路，算出对应的真值表，然后直接存到一个叫作LUT（Look-Up Table，查找表）的电路里面。这个LUT呢，其实就是一块存储空间，里面存储了“特定的输入信号下，对应输出0还是1”。
如果还没理解，你可以想一下这个问题。假如现在我们要实现一个函数，这个函数需要返回斐波那契数列的第N项，并且限制这个N不会超过100。该怎么解决这个问题呢？
斐波那契数列的通项公式是 f(N) = f(N-1) + f(N-2) 。所以，我们的第一种办法，自然是写一个程序，从第1项开始算。但其实还有一种办法，就是我们预先用程序算好斐波那契数量前100项，然后把它预先放到一个数组里面。这个数组就像 [1, 1, 2, 3, 5…] 这样。当要计算第N项的时候呢，我们并不是去计算得到结果，而是直接查找这个数组里面的第N项。
这里面的关键就在于，这个查表的办法，不只能够提供斐波那契数列。如果我们要有一个获得N的5次方的函数，一样可以先计算好，放在表里面进行查询。这个“查表”的方法，其实就是FPGA通过LUT来实现各种组合逻辑的办法。
第二，对于需要实现的时序逻辑电路，我们可以在FPGA里面直接放上D触发器，作为寄存器。这个和CPU里的触发器没有什么本质不同。不过，我们会把很多个LUT的电路和寄存器组合在一起，变成一个叫作逻辑簇（Logic Cluster）的东西。在FPGA里，这样组合了多个LUT和寄存器的设备，也被叫做CLB（Configurable Logic Block，可配置逻辑块）。
我们通过配置CLB实现的功能有点儿像我们前面讲过的全加器。它已经在最基础的门电路上做了组合，能够提供更复杂一点的功能。更复杂的芯片功能，我们不用再从门电路搭起，可以通过CLB组合搭建出来。
第三，FPGA是通过可编程逻辑布线，来连接各个不同的CLB，最终实现我们想要实现的芯片功能。这个可编程逻辑布线，你可以把它当成我们的铁路网。整个铁路系统已经铺好了，但是整个铁路网里面，设计了很多个道岔。我们可以通过控制道岔，来确定不同的列车线路。在可编程逻辑布线里面，“编程”在做的，就是拨动像道岔一样的各个电路开关，最终实现不同CLB之间的连接，完成我们想要的芯片功能。
于是，通过LUT和寄存器，我们能够组合出很多CLB，而通过连接不同的CLB，最终有了我们想要的芯片功能。最关键的是，这个组合过程是可以“编程”控制的。而且这个编程出来的软件，还可以后续改写，重新写入到硬件里。让同一个硬件实现不同的芯片功能。从这个角度来说，FPGA也是“软件吞噬世界”的一个很好的例子。
ASIC除了CPU、GPU，以及刚刚的FPGA，我们其实还需要用到很多其他芯片。比如，现在手机里就有专门用在摄像头里的芯片；录音笔里会有专门处理音频的芯片。尽管一个CPU能够处理好手机拍照的功能，也能处理好录音的功能，但是我们直接在手机或者录音笔里塞上一个Intel CPU，显然比较浪费。
于是，我们就考虑为这些有专门用途的场景，单独设计一个芯片。这些专门设计的芯片呢，我们称之为ASIC（Application-Specific Integrated Circuit），也就是专用集成电路。事实上，过去几年，ASIC发展得特别快。因为ASIC是针对专门用途设计的，所以它的电路更精简，单片的制造成本也比CPU更低。而且，因为电路精简，所以通常能耗要比用来做通用计算的CPU更低。而我们上一讲所说的早期的图形加速卡，其实就可以看作是一种ASIC。
因为ASIC的生产制造成本，以及能耗上的优势，过去几年里，有不少公司设计和开发ASIC用来“挖矿”。这个“挖矿”，说的其实就是设计专门的数值计算芯片，用来“挖”比特币、ETH这样的数字货币。</description></item><item><title>32_为什么还有kill不掉的语句？</title><link>https://artisanbox.github.io/1/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/32/</guid><description>在MySQL中有两个kill命令：一个是kill query +线程id，表示终止这个线程中正在执行的语句；一个是kill connection +线程id，这里connection可缺省，表示断开这个线程的连接，当然如果这个线程有语句正在执行，也是要先停止正在执行的语句的。
不知道你在使用MySQL的时候，有没有遇到过这样的现象：使用了kill命令，却没能断开这个连接。再执行show processlist命令，看到这条语句的Command列显示的是Killed。
你一定会奇怪，显示为Killed是什么意思，不是应该直接在show processlist的结果里看不到这个线程了吗？
今天，我们就来讨论一下这个问题。
其实大多数情况下，kill query/connection命令是有效的。比如，执行一个查询的过程中，发现执行时间太久，要放弃继续查询，这时我们就可以用kill query命令，终止这条查询语句。
还有一种情况是，语句处于锁等待的时候，直接使用kill命令也是有效的。我们一起来看下这个例子：
图1 kill query 成功的例子可以看到，session C 执行kill query以后，session B几乎同时就提示了语句被中断。这，就是我们预期的结果。
收到kill以后，线程做什么？但是，这里你要停下来想一下：session B是直接终止掉线程，什么都不管就直接退出吗？显然，这是不行的。
我在第6篇文章中讲过，当对一个表做增删改查操作时，会在表上加MDL读锁。所以，session B虽然处于blocked状态，但还是拿着一个MDL读锁的。如果线程被kill的时候，就直接终止，那之后这个MDL读锁就没机会被释放了。
这样看来，kill并不是马上停止的意思，而是告诉执行线程说，这条语句已经不需要继续执行了，可以开始“执行停止的逻辑了”。
其实，这跟Linux的kill命令类似，kill -N pid并不是让进程直接停止，而是给进程发一个信号，然后进程处理这个信号，进入终止逻辑。只是对于MySQL的kill命令来说，不需要传信号量参数，就只有“停止”这个命令。
实现上，当用户执行kill query thread_id_B时，MySQL里处理kill命令的线程做了两件事：
把session B的运行状态改成THD::KILL_QUERY(将变量killed赋值为THD::KILL_QUERY)；
给session B的执行线程发一个信号。
为什么要发信号呢？
因为像图1的我们例子里面，session B处于锁等待状态，如果只是把session B的线程状态设置THD::KILL_QUERY，线程B并不知道这个状态变化，还是会继续等待。发一个信号的目的，就是让session B退出等待，来处理这个THD::KILL_QUERY状态。
上面的分析中，隐含了这么三层意思：
一个语句执行过程中有多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是THD::KILL_QUERY，才开始进入语句终止逻辑；
如果处于等待状态，必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处；
语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的。
到这里你就知道了，原来不是“说停就停的”。
接下来，我们再看一个kill不掉的例子，也就是我们在前面第29篇文章中提到的 innodb_thread_concurrency 不够用的例子。
首先，执行set global innodb_thread_concurrency=2，将InnoDB的并发线程上限数设置为2；然后，执行下面的序列：
图2 kill query 无效的例子可以看到：</description></item><item><title>32_字符串匹配基础（上）：如何借助哈希算法实现高效字符串匹配？</title><link>https://artisanbox.github.io/2/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/33/</guid><description>从今天开始，我们来学习字符串匹配算法。字符串匹配这样一个功能，我想对于任何一个开发工程师来说，应该都不会陌生。我们用的最多的就是编程语言提供的字符串查找函数，比如Java中的indexOf()，Python中的find()函数等，它们底层就是依赖接下来要讲的字符串匹配算法。
字符串匹配算法很多，我会分四节来讲解。今天我会讲两种比较简单的、好理解的，它们分别是：BF算法和RK算法。下一节，我会讲两种比较难理解、但更加高效的，它们是：BM算法和KMP算法。
这两节讲的都是单模式串匹配的算法，也就是一个串跟一个串进行匹配。第三节、第四节，我会讲两种多模式串匹配算法，也就是在一个串中同时查找多个串，它们分别是Trie树和AC自动机。
今天讲的两个算法中，RK算法是BF算法的改进，它巧妙借助了我们前面讲过的哈希算法，让匹配的效率有了很大的提升。那RK算法是如何借助哈希算法来实现高效字符串匹配的呢？你可以带着这个问题，来学习今天的内容。
BF算法BF算法中的BF是Brute Force的缩写，中文叫作暴力匹配算法，也叫朴素匹配算法。从名字可以看出，这种算法的字符串匹配方式很“暴力”，当然也就会比较简单、好懂，但相应的性能也不高。
在开始讲解这个算法之前，我先定义两个概念，方便我后面讲解。它们分别是主串和模式串。这俩概念很好理解，我举个例子你就懂了。
比方说，我们在字符串A中查找字符串B，那字符串A就是主串，字符串B就是模式串。我们把主串的长度记作n，模式串的长度记作m。因为我们是在主串中查找模式串，所以n&amp;gt;m。
作为最简单、最暴力的字符串匹配算法，BF算法的思想可以用一句话来概括，那就是，我们在主串中，检查起始位置分别是0、1、2....n-m且长度为m的n-m+1个子串，看有没有跟模式串匹配的。我举一个例子给你看看，你应该可以理解得更清楚。
从上面的算法思想和例子，我们可以看出，在极端情况下，比如主串是“aaaaa....aaaaaa”（省略号表示有很多重复的字符a），模式串是“aaaaab”。我们每次都比对m个字符，要比对n-m+1次，所以，这种算法的最坏情况时间复杂度是O(n*m)。
尽管理论上，BF算法的时间复杂度很高，是O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。
第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把m个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。
第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有bug也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。
所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。
RK算法RK算法的全称叫Rabin-Karp算法，是由它的两位发明者Rabin和Karp的名字来命名的。这个算法理解起来也不是很难。我个人觉得，它其实就是刚刚讲的BF算法的升级版。
我在讲BF算法的时候讲过，如果模式串长度为m，主串长度为n，那在主串中，就会有n-m+1个长度为m的子串，我们只需要暴力地对比这n-m+1个子串与模式串，就可以找出主串与模式串匹配的子串。
但是，每次检查主串与子串是否匹配，需要依次比对每个字符，所以BF算法的时间复杂度就比较高，是O(n*m)。我们对朴素的字符串匹配算法稍加改造，引入哈希算法，时间复杂度立刻就会降低。
RK算法的思路是这样的：我们通过哈希算法对主串中的n-m+1个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。
不过，通过哈希算法计算子串的哈希值的时候，我们需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。有没有方法可以提高哈希算法计算子串哈希值的效率呢？
这就需要哈希算法设计的非常有技巧了。我们假设要匹配的字符串的字符集中只包含K个字符，我们可以用一个K进制数来表示一个子串，这个K进制数转化成十进制数，作为子串的哈希值。表述起来有点抽象，我举了一个例子，看完你应该就能懂了。
比如要处理的字符串只包含a～z这26个小写字母，那我们就用二十六进制来表示一个字符串。我们把a～z这26个字符映射到0～25这26个数字，a就表示0，b就表示1，以此类推，z表示25。
在十进制的表示法中，一个数字的值是通过下面的方式计算出来的。对应到二十六进制，一个包含a到z这26个字符的字符串，计算哈希的时候，我们只需要把进位从10改成26就可以。
这个哈希算法你应该看懂了吧？现在，为了方便解释，在下面的讲解中，我假设字符串中只包含a～z这26个小写字符，我们用二十六进制来表示一个字符串，对应的哈希值就是二十六进制数转化成十进制的结果。
这种哈希算法有一个特点，在主串中，相邻两个子串的哈希值的计算公式有一定关系。我这有个例子，你先找一下规律，再来看我后面的讲解。
从这里例子中，我们很容易就能得出这样的规律：相邻两个子串s[i-1]和s[i]（i表示子串在主串中的起始位置，子串的长度都为m），对应的哈希值计算公式有交集，也就是说，我们可以使用s[i-1]的哈希值很快的计算出s[i]的哈希值。如果用公式表示的话，就是下面这个样子：
不过，这里有一个小细节需要注意，那就是26^(m-1)这部分的计算，我们可以通过查表的方法来提高效率。我们事先计算好26^0、26^1、26^2……26^(m-1)，并且存储在一个长度为m的数组中，公式中的“次方”就对应数组的下标。当我们需要计算26的x次方的时候，就可以从数组的下标为x的位置取值，直接使用，省去了计算的时间。
我们开头的时候提过，RK算法的效率要比BF算法高，现在，我们就来分析一下，RK算法的时间复杂度到底是多少呢？
整个RK算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是O(n)。
模式串哈希值与每个子串哈希值之间的比较的时间复杂度是O(1)，总共需要比较n-m+1个子串的哈希值，所以，这部分的时间复杂度也是O(n)。所以，RK算法整体的时间复杂度就是O(n)。
这里还有一个问题就是，模式串很长，相应的主串中的子串也会很长，通过上面的哈希算法计算得到的哈希值就可能很大，如果超过了计算机中整型数据可以表示的范围，那该如何解决呢？
刚刚我们设计的哈希算法是没有散列冲突的，也就是说，一个字符串与一个二十六进制数一一对应，不同的字符串的哈希值肯定不一样。因为我们是基于进制来表示一个字符串的，你可以类比成十进制、十六进制来思考一下。实际上，我们为了能将哈希值落在整型数据范围内，可以牺牲一下，允许哈希冲突。这个时候哈希算法该如何设计呢？
哈希算法的设计方法有很多，我举一个例子说明一下。假设字符串中只包含a～z这26个英文字母，那我们每个字母对应一个数字，比如a对应1，b对应2，以此类推，z对应26。我们可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值。这种哈希算法产生的哈希值的数据范围就相对要小很多了。
不过，你也应该发现，这种哈希算法的哈希冲突概率也是挺高的。当然，我只是举了一个最简单的设计方法，还有很多更加优化的方法，比如将每一个字母从小到大对应一个素数，而不是1，2，3……这样的自然数，这样冲突的概率就会降低一些。
那现在新的问题来了。之前我们只需要比较一下模式串和子串的哈希值，如果两个值相等，那这个子串就一定可以匹配模式串。但是，当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身并不匹配。
实际上，解决方法很简单。当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。当然，如果子串的哈希值与模式串的哈希值不相等，那对应的子串和模式串肯定也是不匹配的，就不需要比对子串和模式串本身了。
所以，哈希算法的冲突概率要相对控制得低一些，如果存在大量冲突，就会导致RK算法的时间复杂度退化，效率下降。极端情况下，如果存在大量的冲突，每次都要再对比子串和模式串本身，那时间复杂度就会退化成O(n*m)。但也不要太悲观，一般情况下，冲突不会很多，RK算法的效率还是比BF算法高的。
解答开篇&amp;amp;内容小结今天我们讲了两种字符串匹配算法，BF算法和RK算法。
BF算法是最简单、粗暴的字符串匹配算法，它的实现思路是，拿模式串与主串中是所有子串匹配，看是否有能匹配的子串。所以，时间复杂度也比较高，是O(n*m)，n、m表示主串和模式串的长度。不过，在实际的软件开发中，因为这种算法实现简单，对于处理小规模的字符串匹配很好用。
RK算法是借助哈希算法对BF算法进行改造，即对每个子串分别求哈希值，然后拿子串的哈希值与模式串的哈希值比较，减少了比较的时间。所以，理想情况下，RK算法的时间复杂度是O(n)，跟BF算法相比，效率提高了很多。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为O(n*m)。
课后思考我们今天讲的都是一维字符串的匹配方法，实际上，这两种算法都可以类比到二维空间。假设有下面这样一个二维字符串矩阵（图中的主串），借助今天讲的处理思路，如何在其中查找另一个二维字符串矩阵（图中的模式串）呢？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>32｜函数式编程第1关：实现高阶函数</title><link>https://artisanbox.github.io/3/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/34/</guid><description>你好，我是宫文学。
前面三节课，我们探讨了怎么在现代语言中实现面向对象编程的特性。面向对象是一种重要的编程范式。还有另一种编程范式，也同样重要，并且近年来使用得很多，这就是函数式编程。从今天这节课开始，我们就来实现一下函数式编程。
函数式编程思想其实比面向对象编程思想的历史更长，早期的Lisp等语言都是函数式编程语言。像JavaScript等后来的语言，也继承了Lisp语言在函数式编程方面的思想，对函数式编程也有不错的支持。
近年，函数式编程思想得到了一定程度的复兴，部分原因是由于函数式编程能够更好地应对大规模的并发处理。我自己最近参与的项目，也在全面使用一门函数式编程语言，这也是对函数式编程的优势的认可。此外，像Erlang这种能够开发高可靠性系统的函数式编程语言，也一直是我感兴趣的研究对象。
对于函数式编程这个话题，很多书和文章都对它有过讲解。我在《编译原理实战课》的第39节，也对函数式编程特性的一些技术点做了分析。在我们的这门课里，因为要动手实现出来，所以目标不能太大，我们就挑几个最核心的技术点来实现一下，让你对函数式编程的底层机制有一次穿透性的了解。
今天这节课，我们主要来实现高阶函数的特性。对于函数式编程来说，高阶函数是实现其他功能的基础，属于最核心的技术点。那么，我们就先分析一下什么是高阶函数。
高阶函数的例子高阶函数的核心思想，是函数本身可以当做数据来使用，就像number数据和string数据那样。那既然可以当做数据使用，那自然可以用它来声明变量、作为参数传递给另一个函数，以及作为返回值从另一个函数中返回。如果一门计算机语言把函数和数据同等对待，这时候我们说函数是一等公民（First-class Citizen）。
我用TypeScript写了一个reduce函数的例子，带你来感受一下高阶函数的特性。这个函数能够遍历一个number数组，并且返回一个number值。
//reduce函数：遍历数组中的每个元素，最后返回一个值 function reduce(numbers:number[], fun:(prev:number,cur:number)=&amp;gt;number):number{ let prev:number = 0; for (let i = 0; i &amp;lt; numbers.length; i++){ prev = fun(prev, numbers[i]); } return prev; } //累计汇总值 function sum(prev:number, cur:number):number{ return prev + cur; }
//累计最大值 function max(prev:number, cur:number):number{ if (prev &amp;gt;= cur) return prev; else return cur; }
let numbers = [2,3,4,5,7,4,5,2];
println(reduce(numbers, sum)); println(reduce(numbers, max)); 这个reduce函数很有意思的一点，是它能接受一个函数作为参数。在每遍历一个数组元素的时候，都会调用这个传进来的函数。根据传入的函数不同，reduce函数能完成不同的功能。当传入max函数的时候，reduce函数能返回数组元素的最大值；而当传入sum函数的时候，则能返回数组元素的汇总值。
这个例子能够部分体现函数式编程的优势：把系统的功能拆解成函数，再灵活组合。
那这些高阶函数的特性具体怎么实现呢？按照惯例，我们还是先看看在编译器前端方面，我们要做什么工作。
修改编译器前端要实现上面的功能，编译器前端需要增加新的语法规则，并做一些与函数类型有关的语义处理工作。</description></item><item><title>33_字符串匹配基础（中）：如何实现文本编辑器中的查找功能？</title><link>https://artisanbox.github.io/2/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/34/</guid><description>文本编辑器中的查找替换功能，我想你应该不陌生吧？比如，我们在Word中把一个单词统一替换成另一个，用的就是这个功能。你有没有想过，它是怎么实现的呢？
当然，你用上一节讲的BF算法和RK算法，也可以实现这个功能，但是在某些极端情况下，BF算法性能会退化的比较严重，而RK算法需要用到哈希算法，设计一个可以应对各种类型字符的哈希算法并不简单。
对于工业级的软件开发来说，我们希望算法尽可能的高效，并且在极端情况下，性能也不要退化的太严重。那么，对于查找功能是重要功能的软件来说，比如一些文本编辑器，它们的查找功能都是用哪种算法来实现的呢？有没有比BF算法和RK算法更加高效的字符串匹配算法呢？
今天，我们就来学习BM（Boyer-Moore）算法。它是一种非常高效的字符串匹配算法，有实验统计，它的性能是著名的KMP算法的3到4倍。BM算法的原理很复杂，比较难懂，学起来会比较烧脑，我会尽量给你讲清楚，同时也希望你做好打硬仗的准备。好，现在我们正式开始！
BM算法的核心思想我们把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF算法和RK算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。我举个例子解释一下，你可以看我画的这幅图。
在这个例子里，主串中的c，在模式串中是不存在的，所以，模式串向后滑动的时候，只要c与模式串没有重合，肯定无法匹配。所以，我们可以一次性把模式串往后多滑动几位，把模式串移动到c的后面。
由现象找规律，你可以思考一下，当遇到不匹配的字符时，有什么固定的规律，可以将模式串往后多滑动几位呢？这样一次性往后滑动好几位，那匹配的效率岂不是就提高了？
我们今天要讲的BM算法，本质上其实就是在寻找这种规律。借助这种规律，在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位。
BM算法原理分析BM算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）。我们下面依次来看，这两个规则分别都是怎么工作的。
1.坏字符规则前面两节讲的算法，在匹配的过程中，我们都是按模式串的下标从小到大的顺序，依次与主串中的字符进行匹配的。这种匹配顺序比较符合我们的思维习惯，而BM算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的。我画了一张图，你可以看下。
从模式串的末尾往前倒着匹配，当发现某个字符没法匹配的时候，我们把这个没有匹配的字符叫作坏字符（主串中的字符）。
我们拿坏字符c在模式串中查找，发现模式串中并不存在这个字符，也就是说，字符c与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位，将模式串滑动到c后面的位置，再从模式串的末尾字符开始比较。
这个时候，我们发现，模式串中最后一个字符d，还是无法跟主串中的a匹配，这个时候，还能将模式串往后滑动三位吗？答案是不行的。因为这个时候，坏字符a在模式串中是存在的，模式串中下标是0的位置也是字符a。这种情况下，我们可以将模式串往后滑动两位，让两个a上下对齐，然后再从模式串的末尾字符开始，重新匹配。
第一次不匹配的时候，我们滑动了三位，第二次不匹配的时候，我们将模式串后移两位，那具体滑动多少位，到底有没有规律呢？
当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作si。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作xi。如果不存在，我们把xi记作-1。那模式串往后移动的位数就等于si-xi。（注意，我这里说的下标，都是字符在模式串的下标）。
这里我要特别说明一点，如果坏字符在模式串里多处出现，那我们在计算xi的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。
利用坏字符规则，BM算法在最好情况下的时间复杂度非常低，是O(n/m)。比如，主串是aaabaaabaaabaaab，模式串是aaaa。每次比对，模式串都可以直接后移四位，所以，匹配具有类似特点的模式串和主串的时候，BM算法非常高效。
不过，单纯使用坏字符规则还是不够的。因为根据si-xi计算出来的移动位数，有可能是负数，比如主串是aaaaaaaaaaaaaaaa，模式串是baaa。不但不会向后滑动模式串，还有可能倒退。所以，BM算法还需要用到“好后缀规则”。
2.好后缀规则好后缀规则实际上跟坏字符规则的思路很类似。你看我下面这幅图。当模式串滑动到图中的位置的时候，模式串和主串有2个字符是匹配的，倒数第3个字符发生了不匹配的情况。
这个时候该如何滑动模式串呢？当然，我们还可以利用坏字符规则来计算模式串的滑动位数，不过，我们也可以使用好后缀处理规则。两种规则到底如何选择，我稍后会讲。抛开这个问题，现在我们来看，好后缀规则是怎么工作的？
我们把已经匹配的bc叫作好后缀，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。
如果在模式串中找不到另一个等于{u}的子串，我们就直接将模式串，滑动到主串中{u}的后面，因为之前的任何一次往后滑动，都没有匹配主串中{u}的情况。
不过，当模式串中不存在等于{u}的子串时，我们直接将模式串滑动到主串{u}的后面。这样做是否有点太过头呢？我们来看下面这个例子。这里面bc是好后缀，尽管在模式串中没有另外一个相匹配的子串{u*}，但是如果我们将模式串移动到好后缀的后面，如图所示，那就会错过模式串和主串可以匹配的情况。
如果好后缀在模式串中不存在可匹配的子串，那在我们一步一步往后滑动模式串的过程中，只要主串中的{u}与模式串有重合，那肯定就无法完全匹配。但是当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况。
所以，针对这种情况，我们不仅要看好后缀在模式串中，是否有另一个匹配的子串，我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。
所谓某个字符串s的后缀子串，就是最后一个字符跟s对齐的子串，比如abc的后缀子串就包括c, bc。所谓前缀子串，就是起始字符跟s对齐的子串，比如abc的前缀子串有a，ab。我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的，假设是{v}，然后将模式串滑动到如图所示的位置。
坏字符和好后缀的基本原理都讲完了，我现在回答一下前面那个问题。当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？
我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。
BM算法代码实现学习完了基本原理，我们再来看，如何实现BM算法？
“坏字符规则”本身不难理解。当遇到坏字符时，要计算往后移动的位数si-xi，其中xi的计算是重点，我们如何求得xi呢？或者说，如何查找坏字符在模式串中出现的位置呢？
如果我们拿坏字符，在模式串中顺序遍历查找，这样就会比较低效，势必影响这个算法的性能。有没有更加高效的方式呢？我们之前学的散列表，这里可以派上用场了。我们可以将模式串中的每个字符及其下标都存到散列表中。这样就可以快速找到坏字符在模式串的位置下标了。
关于这个散列表，我们只实现一种最简单的情况，假设字符串的字符集不是很大，每个字符长度是1字节，我们用大小为256的数组，来记录每个字符在模式串中出现的位置。数组的下标对应字符的ASCII码值，数组中存储这个字符在模式串中出现的位置。
如果将上面的过程翻译成代码，就是下面这个样子。其中，变量b是模式串，m是模式串的长度，bc表示刚刚讲的散列表。
private static final int SIZE = 256; // 全局变量或成员变量 private void generateBC(char[] b, int m, int[] bc) { for (int i = 0; i &amp;lt; SIZE; ++i) { bc[i] = -1; // 初始化bc } for (int i = 0; i &amp;lt; m; ++i) { int ascii = (int)b[i]; // 计算b[i]的ASCII值 bc[ascii] = i; } } 掌握了坏字符规则之后，我们先把BM算法代码的大框架写好，先不考虑好后缀规则，仅用坏字符规则，并且不考虑si-xi计算得到的移动位数可能会出现负数的情况。</description></item><item><title>33_我查这么多数据，会不会把数据库内存打爆？</title><link>https://artisanbox.github.io/1/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/33/</guid><description>我经常会被问到这样一个问题：我的主机内存只有100G，现在要对一个200G的大表做全表扫描，会不会把数据库主机的内存用光了？
这个问题确实值得担心，被系统OOM（out of memory）可不是闹着玩的。但是，反过来想想，逻辑备份的时候，可不就是做整库扫描吗？如果这样就会把内存吃光，逻辑备份不是早就挂了？
所以说，对大表做全表扫描，看来应该是没问题的。但是，这个流程到底是怎么样的呢？
全表扫描对server层的影响假设，我们现在要对一个200G的InnoDB表db1. t，执行一个全表扫描。当然，你要把扫描结果保存在客户端，会使用类似这样的命令：
mysql -h$host -P$port -u$user -p$pwd -e &amp;quot;select * from db1.t&amp;quot; &amp;gt; $target_file 你已经知道了，InnoDB的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表t的主键索引。这条查询语句由于没有其他的判断条件，所以查到的每一行都可以直接放到结果集里面，然后返回给客户端。
那么，这个“结果集”存在哪里呢？
实际上，服务端并不需要保存一个完整的结果集。取数据和发数据的流程是这样的：
获取一行，写到net_buffer中。这块内存的大小是由参数net_buffer_length定义的，默认是16k。
重复获取行，直到net_buffer写满，调用网络接口发出去。
如果发送成功，就清空net_buffer，然后继续取下一行，并写入net_buffer。
如果发送函数返回EAGAIN或WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。
这个过程对应的流程图如下所示。
图1 查询结果发送流程从这个流程中，你可以看到：
一个查询在发送过程中，占用的MySQL内部的内存最大就是net_buffer_length这么大，并不会达到200G；
socket send buffer 也不可能达到200G（默认定义/proc/sys/net/core/wmem_default），如果socket send buffer被写满，就会暂停读数据的流程。
也就是说，MySQL是“边读边发的”，这个概念很重要。这就意味着，如果客户端接收得慢，会导致MySQL服务端由于结果发不出去，这个事务的执行时间变长。
比如下面这个状态，就是我故意让客户端不去读socket receive buffer中的内容，然后在服务端show processlist看到的结果。
图2 服务端发送阻塞如果你看到State的值一直处于“Sending to client”，就表示服务器端的网络栈写满了。
我在上一篇文章中曾提到，如果客户端使用–quick参数，会使用mysql_use_result方法。这个方法是读一行处理一行。你可以想象一下，假设有一个业务的逻辑比较复杂，每读一行数据以后要处理的逻辑如果很慢，就会导致客户端要过很久才会去取下一行数据，可能就会出现如图2所示的这种情况。
因此，对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，我都建议你使用mysql_store_result这个接口，直接把查询结果保存到本地内存。
当然前提是查询返回结果不多。在第30篇文章评论区，有同学说到自己因为执行了一个大查询导致客户端占用内存近20G，这种情况下就需要改用mysql_use_result接口了。
另一方面，如果你在自己负责维护的MySQL里看到很多个线程都处于“Sending to client”这个状态，就意味着你要让业务开发同学优化查询结果，并评估这么多的返回结果是否合理。
而如果要快速减少处于这个状态的线程的话，将net_buffer_length参数设置为一个更大的值是一个可选方案。
与“Sending to client”长相很类似的一个状态是“Sending data”，这是一个经常被误会的问题。有同学问我说，在自己维护的实例上看到很多查询语句的状态是“Sending data”，但查看网络也没什么问题啊，为什么Sending data要这么久？</description></item><item><title>33_解读TPU：设计和拆解一块ASIC芯片</title><link>https://artisanbox.github.io/4/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/33/</guid><description>过去几年，最知名、最具有实用价值的ASIC就是TPU了。各种解读TPU论文内容的文章网上也很多。不过，这些文章更多地是从机器学习或者AI的角度，来讲解TPU。
上一讲，我为你讲解了FPGA和ASIC，讲解了FPGA如何实现通过“软件”来控制“硬件”，以及我们可以进一步把FPGA设计出来的电路变成一块ASIC芯片。
不过呢，这些似乎距离我们真实的应用场景有点儿远。我们怎么能够设计出来一块有真实应用场景的ASIC呢？如果要去设计一块ASIC，我们应该如何思考和拆解问题呢？今天，我就带着你一起学习一下，如何设计一块专用芯片。
TPU V1想要解决什么问题？黑格尔说，“世上没有无缘无故的爱，也没有无缘无故的恨”。第一代TPU的设计并不是异想天开的创新，而是来自于真实的需求。
从2012年解决计算机视觉问题开始，深度学习一下子进入了大爆发阶段，也一下子带火了GPU，NVidia的股价一飞冲天。我们在第31讲讲过，GPU天生适合进行海量、并行的矩阵数值计算，于是它被大量用在深度学习的模型训练上。
不过你有没有想过，在深度学习热起来之后，计算量最大的是什么呢？并不是进行深度学习的训练，而是深度学习的推断部分。
所谓推断部分，是指我们在完成深度学习训练之后，把训练完成的模型存储下来。这个存储下来的模型，是许许多多个向量组成的参数。然后，我们根据这些参数，去计算输入的数据，最终得到一个计算结果。这个推断过程，可能是在互联网广告领域，去推测某一个用户是否会点击特定的广告；也可能是我们在经过高铁站的时候，扫一下身份证进行一次人脸识别，判断一下是不是你本人。
虽然训练一个深度学习的模型需要花的时间不少，但是实际在推断上花的时间要更多。比如，我们上面说的高铁，去年（2018年）一年就有20亿人次坐了高铁，这也就意味着至少进行了20亿次的人脸识别“推断“工作。
所以，第一代的TPU，首先优化的并不是深度学习的模型训练，而是深度学习的模型推断。这个时候你可能要问了，那模型的训练和推断有什么不同呢？主要有三个点。
第一点，深度学习的推断工作更简单，对灵活性的要求也就更低。模型推断的过程，我们只需要去计算一些矩阵的乘法、加法，调用一些Sigmoid或者RELU这样的激活函数。这样的过程可能需要反复进行很多层，但是也只是这些计算过程的简单组合。
第二点，深度学习的推断的性能，首先要保障响应时间的指标。我们在第4讲讲过，计算机关注的性能指标，有响应时间（Response Time）和吞吐率（Throughput）。我们在模型训练的时候，只需要考虑吞吐率问题就行了。因为一个模型训练少则好几分钟，多的话要几个月。而推断过程，像互联网广告的点击预测，我们往往希望能在几十毫秒乃至几毫秒之内就完成，而人脸识别也不希望会超过几秒钟。很显然，模型训练和推断对于性能的要求是截然不同的。
第三点，深度学习的推断工作，希望在功耗上尽可能少一些。深度学习的训练，对功耗没有那么敏感，只是希望训练速度能够尽可能快，多费点电就多费点儿了。这是因为，深度学习的推断，要7×24h地跑在数据中心里面。而且，对应的芯片，要大规模地部署在数据中心。一块芯片减少5%的功耗，就能节省大量的电费。而深度学习的训练工作，大部分情况下只是少部分算法工程师用少量的机器进行。很多时候，只是做小规模的实验，尽快得到结果，节约人力成本。少数几台机器多花的电费，比起算法工程师的工资来说，只能算九牛一毛了。
这三点的差别，也就带出了第一代TPU的设计目标。那就是，在保障响应时间的情况下，能够尽可能地提高能效比这个指标，也就是进行同样多数量的推断工作，花费的整体能源要显著低于CPU和GPU。
深入理解TPU V1快速上线和向前兼容，一个FPU的设计如果你来设计TPU，除了满足上面的深度学习的推断特性之外，还有什么是你要重点考虑的呢？你可以停下来思考一下，然后再继续往下看。
不知道你的答案是什么，我的第一反应是，有两件事情必须要考虑，第一个是TPU要有向前兼容性，第二个是希望TPU能够尽早上线。我下面说说我考虑这两点的原因。
图片来源第一代的TPU就像一块显卡一样，可以直接插在主板的PCI-E口上第一点，向前兼容。在计算机产业界里，因为没有考虑向前兼容，惨遭失败的产品数不胜数。典型的有我在第26讲提过的安腾处理器。所以，TPU并没有设计成一个独立的“CPU“，而是设计成一块像显卡一样，插在主板PCI-E接口上的板卡。更进一步地，TPU甚至没有像我们之前说的现代GPU一样，设计成自己有对应的取指令的电路，而是通过CPU，向TPU发送需要执行的指令。
这两个设计，使得我们的TPU的硬件设计变得简单了，我们只需要专心完成一个专用的“计算芯片”就好了。所以，TPU整个芯片的设计上线时间也就缩短到了15个月。不过，这样一个TPU，其实是第26讲里我们提过的387浮点数计算芯片，是一个像FPU（浮点数处理器）的协处理器（Coprocessor），而不是像CPU和GPU这样可以独立工作的Processor Unit。
专用电路和大量缓存，适应推断的工作流程明确了TPU整体的设计思路之后，我们可以来看一看，TPU内部有哪些芯片和数据处理流程。我在文稿里面，放了TPU的模块图和对应的芯片布局图，你可以对照着看一下。
图片来源模块图：整个TPU的硬件，完全是按照深度学习一个层（Layer）的计算流程来设计的你可以看到，在芯片模块图里面，有单独的矩阵乘法单元（Matrix Multiply Unit）、累加器（Accumulators）模块、激活函数（Activation）模块和归一化/池化（Normalization/Pool）模块。而且，这些模块是顺序串联在一起的。
这是因为，一个深度学习的推断过程，是由很多层的计算组成的。而每一个层（Layer）的计算过程，就是先进行矩阵乘法，再进行累加，接着调用激活函数，最后进行归一化和池化。这里的硬件设计呢，就是把整个流程变成一套固定的硬件电路。这也是一个ASIC的典型设计思路，其实就是把确定的程序指令流程，变成固定的硬件电路。
接着，我们再来看下面的芯片布局图，其中控制电路（Control）只占了2%。这是因为，TPU的计算过程基本上是一个固定的流程。不像我们之前讲的CPU那样，有各种复杂的控制功能，比如冒险、分支预测等等。
你可以看到，超过一半的TPU的面积，都被用来作为Local Unified Buffer（本地统一缓冲区）（29%）和矩阵乘法单元（Matrix Mutliply Unit）了。
相比于矩阵乘法单元，累加器、实现激活函数和后续的归一/池化功能的激活管线（Activation Pipeline）也用得不多。这是因为，在深度学习推断的过程中，矩阵乘法的计算量是最大的，计算也更复杂，所以比简单的累加器和激活函数要占用更多的晶体管。
而统一缓冲区（Unified Buffer），则由SRAM这样高速的存储设备组成。SRAM一般被直接拿来作为CPU的寄存器或者高速缓存。我们在后面的存储器部分会具体讲。SRAM比起内存使用的DRAM速度要快上很多，但是因为电路密度小，所以占用的空间要大很多。统一缓冲区之所以使用SRAM，是因为在整个的推断过程中，它会高频反复地被矩阵乘法单元读写，来完成计算。
图片来源芯片布局图：从尺寸可以看出，统一缓冲区和矩阵乘法单元是TPU的核心功能组件可以看到，整个TPU里面，每一个组件的设计，完全是为了深度学习的推断过程设计出来的。这也是我们设计开发ASIC的核心原因：用特制的硬件，最大化特定任务的运行效率。
细节优化，使用8 Bits数据除了整个TPU的模块设计和芯片布局之外，TPU在各个细节上也充分考虑了自己的应用场景，我们可以拿里面的矩阵乘法单元（Matrix Multiply Unit）来作为一个例子。
如果你仔细一点看的话，会发现这个矩阵乘法单元，没有用32 Bits来存放一个浮点数，而是只用了一个8 Bits来存放浮点数。这是因为，在实践的机器学习应用中，会对数据做归一化（Normalization）和正则化（Regularization）的处理。咱们毕竟不是一个机器学习课，所以我就不深入去讲什么是归一化和正则化了，你只需要知道，这两个操作呢，会使得我们在深度学习里面操作的数据都不会变得太大。通常来说呢，都能控制在-3到3这样一定的范围之内。
因为这个数值上的特征，我们需要的浮点数的精度也不需要太高了。我们在第16讲讲解浮点数的时候说过，32位浮点数的精度，差不多可以到1/1600万。如果我们用8位或者16位表示浮点数，也能把精度放到2^6或者2^12，也就是1/64或者1/4096。在深度学习里，常常够用了。特别是在模型推断的时候，要求的计算精度，往往可以比模型训练低。所以，8 Bits的矩阵乘法器，就可以放下更多的计算量，使得TPU的推断速度更快。
用数字说话，TPU的应用效果那么，综合了这么多优秀设计点的TPU，实际的使用效果怎么样呢？不管设计得有多好，最后还是要拿效果和数据说话。俗话说，是骡子是马，总要拿出来溜溜啊。
Google在TPU的论文里面给出了答案。一方面，在性能上，TPU比现在的CPU、GPU在深度学习的推断任务上，要快15～30倍。而在能耗比上，更是好出30～80倍。另一方面，Google已经用TPU替换了自家数据中心里95%的推断任务，可谓是拿自己的实际业务做了一个明证。
总结延伸这一讲，我从第一代TPU的设计目标讲起，为你解读了TPU的设计。你可以通过这篇文章，回顾我们过去32讲提到的各种知识点。
第一代TPU，是为了做各种深度学习的推断而设计出来的，并且希望能够尽早上线。这样，Google才能节约现有数据中心里面的大量计算资源。
从深度学习的推断角度来考虑，TPU并不需要太灵活的可编程能力，只要能够迭代完成常见的深度学习推断过程中一层的计算过程就好了。所以，TPU的硬件构造里面，把矩阵乘法、累加器和激活函数都做成了对应的专门的电路。
为了满足深度学习推断功能的响应时间短的需求，TPU设置了很大的使用SRAM的Unified Buffer（UB），就好像一个CPU里面的寄存器一样，能够快速响应对于这些数据的反复读取。
为了让TPU尽可能快地部署在数据中心里面，TPU采用了现有的PCI-E接口，可以和GPU一样直接插在主板上，并且采用了作为一个没有取指令功能的协处理器，就像387之于386一样，仅仅用来进行需要的各种运算。
在整个电路设计的细节层面，TPU也尽可能做到了优化。因为机器学习的推断功能，通常做了数值的归一化，所以对于矩阵乘法的计算精度要求有限，整个矩阵乘法的计算模块采用了8 Bits来表示浮点数，而不是像Intel CPU里那样用上了32 Bits。
最终，综合了种种硬件设计点之后的TPU，做到了在深度学习的推断层面更高的能效比。按照Google论文里面给出的官方数据，它可以比CPU、GPU快上15～30倍，能耗比更是可以高出30～80倍。而TPU，也最终替代了Google自己的数据中心里，95%的深度学习推断任务。
推荐阅读既然要深入了解TPU，自然要读一读关于TPU的论文In-Datacenter Performance Analysis of a Tensor Processing Unit。
除了这篇论文之外，你也可以读一读Google官方专门讲解TPU构造的博客文章 An in-depth look at Google’s first Tensor Processing Unit(TPU)。</description></item><item><title>33｜函数式编程第2关：实现闭包特性</title><link>https://artisanbox.github.io/3/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/35/</guid><description>你好，我是宫文学。
上节课，我们实现了函数式编程的一个重要特性：高阶函数。今天这节课，我们继续来实现函数式编程的另一个重要特性，也就是闭包。
闭包机制能实现信息的封装、缓存内部状态数据等等，所以被资深程序员所喜欢，并被用于实现各种框架、类库等等。相信很多程序员都了解过闭包的概念，但往往对它的内在机制不是十分清楚。
今天这节课，我会带你了解闭包的原理和实现机制。在这个过程中，你会了解到闭包数据的形成机制、词法作用域的概念、闭包和面向对象特性的相似性，以及如何访问位于其他栈桢中的数据。
首先，让我们了解一下闭包的技术实质，从而确定如何让我们的语言支持闭包特性。
理解闭包的实质我们先通过一个例子来了解闭包的特点。在下面的示例程序中有一个ID的生成器。这个生成器是一个函数，但它把一个内部函数作为返回值来返回。这个返回值被赋给了函数类型的变量id1，然后调用这个函数。
function idGenerator():number{//()=&amp;gt;number{ let nextId = 0; function getId(){ return nextId++; //访问了外部作用域的一个变量 } return getId; }
println(&amp;quot;\nid1:&amp;quot;); let id1 = idGenerator(); println(id1()); //0 println(id1()); //1
//新创建一个闭包，重新开始编号 println(&amp;quot;\nid2:&amp;quot;); let id2 = idGenerator(); println(id2()); //0 println(id2()); //1
//闭包可以通过赋值和参数传递，在没有任何变量引用它的时候，生命周期才会结束。 println(&amp;quot;\nid3:&amp;quot;); let id3 = id1; println(id3()); //2 &amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;然后神奇的事情就发生了。每次你调用id1()，它都会返回一个不同的值，依次是0、1、2……
为什么每次返回的值会不一样呢？
你看这个代码，内部函数getId()访问了外部函数的一个本地变量nextId。当外部函数退出以后，内部函数仍然可以使用这个本地变量，并且每次调用内部函数时，都让nextId的值加1。这个现象，就体现了闭包的特点。
总结起来，闭包是这么一种现象：一个函数可以返回一个内部函数，但这个内部函数使用了它的作用域之外的数据。这些作用域之外的数据会一直伴随着该内部函数的生命周期，内部函数一直可以访问它。
说得更直白一点，就是当内部函数被返回时，它把外部作用域中的一些数据打包带走了，随身携带，便于访问。
这样分析之后你就明白了。为了支持闭包，你需要让某些函数有一个私有的数据区，用于保存一些私有数据，供这个函数访问。在我们这门课里，我们可以把这个函数专有的数据，叫做闭包数据，或者叫做闭包对象。
然后我们再运行这个示例程序，并分析它的输出结果：
你会发现这样一个事实：id1和id2分别通过调用idGenerator()函数获得了一个内部函数，而它们各自拥有自己的闭包数据，是互不干扰的。
从这种角度看，闭包有点像面向对象特性。每次new一个对象的时候，都会生成不同的对象实例。实际上，在函数式语言里，我们确实可以用闭包来模拟某些面向对象编程特性。不过这里你要注意，并不是函数所引用的外部数据，都需要放到私有的数据区中的。我们可以再通过一个例子来看一下。
我把前面的示例程序做了一点修改。这一次，我们的内部函数可以访问两个变量了。
//编号的组成部分 let segment:number = 1000;
function idGenerator():()=&amp;gt;number{ let nextId = 0;</description></item><item><title>34_到底可不可以使用join？</title><link>https://artisanbox.github.io/1/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/34/</guid><description>在实际生产中，关于join语句使用的问题，一般会集中在以下两类：
我们DBA不让使用join，使用join有什么问题呢？
如果有两个大小不同的表做join，应该用哪个表做驱动表呢？
今天这篇文章，我就先跟你说说join语句到底是怎么执行的，然后再来回答这两个问题。
为了便于量化分析，我还是创建两个表t1和t2来和你说明。
CREATE TABLE `t2` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`) ) ENGINE=InnoDB; drop procedure idata; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=1000)do insert into t2 values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata();
create table t1 like t2; insert into t1 (select * from t2 where id&amp;lt;=100) 可以看到，这两个表都有一个主键索引id和一个索引a，字段b上无索引。存储过程idata()往表t2里插入了1000行数据，在表t1里插入的是100行数据。</description></item><item><title>34_字符串匹配基础（下）：如何借助BM算法轻松理解KMP算法？</title><link>https://artisanbox.github.io/2/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/35/</guid><description>上一节我们讲了BM算法，尽管它很复杂，也不好理解，但却是工程中非常常用的一种高效字符串匹配算法。有统计说，它是最高效、最常用的字符串匹配算法。不过，在所有的字符串匹配算法里，要说最知名的一种的话，那就非KMP算法莫属。很多时候，提到字符串匹配，我们首先想到的就是KMP算法。
尽管在实际的开发中，我们几乎不大可能自己亲手实现一个KMP算法。但是，学习这个算法的思想，作为让你开拓眼界、锻炼下逻辑思维，也是极好的，所以我觉得有必要拿出来给你讲一讲。不过，KMP算法是出了名的不好懂。我会尽力把它讲清楚，但是你自己也要多动动脑子。
实际上，KMP算法跟BM算法的本质是一样的。上一节，我们讲了好后缀和坏字符规则，今天，我们就看下，如何借助上一节BM算法的讲解思路，让你能更好地理解KMP算法？
KMP算法基本原理KMP算法是根据三位作者（D.E.Knuth，J.H.Morris和V.R.Pratt）的名字来命名的，算法的全称是Knuth Morris Pratt算法，简称为KMP算法。
KMP算法的核心思想，跟上一节讲的BM算法非常相近。我们假设主串是a，模式串是b。在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候，我们希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况。
还记得我们上一节讲到的好后缀和坏字符吗？这里我们可以类比一下，在模式串和主串匹配的过程中，把不能匹配的那个字符仍然叫作坏字符，把已经匹配的那段字符串叫作好前缀。
当遇到坏字符的时候，我们就要把模式串往后滑动，在滑动的过程中，只要模式串和好前缀有上下重合，前面几个字符的比较，就相当于拿好前缀的后缀子串，跟模式串的前缀子串在比较。这个比较的过程能否更高效了呢？可以不用一个字符一个字符地比较了吗？
KMP算法就是在试图寻找一种规律：在模式串和主串匹配的过程中，当遇到坏字符后，对于已经比对过的好前缀，能否找到一种规律，将模式串一次性滑动很多位？
我们只需要拿好前缀本身，在它的后缀子串中，查找最长的那个可以跟好前缀的前缀子串匹配的。假设最长的可匹配的那部分前缀子串是{v}，长度是k。我们把模式串一次性往后滑动j-k位，相当于，每次遇到坏字符的时候，我们就把j更新为k，i不变，然后继续比较。
为了表述起来方便，我把好前缀的所有后缀子串中，最长的可匹配前缀子串的那个后缀子串，叫作最长可匹配后缀子串；对应的前缀子串，叫作最长可匹配前缀子串。
如何来求好前缀的最长可匹配前缀和后缀子串呢？我发现，这个问题其实不涉及主串，只需要通过模式串本身就能求解。所以，我就在想，能不能事先预处理计算好，在模式串和主串匹配的过程中，直接拿过来就用呢？
类似BM算法中的bc、suffix、prefix数组，KMP算法也可以提前构建一个数组，用来存储模式串中每个前缀（这些前缀都有可能是好前缀）的最长可匹配前缀子串的结尾字符下标。我们把这个数组定义为next数组，很多书中还给这个数组起了一个名字，叫失效函数（failure function）。
数组的下标是每个前缀结尾字符下标，数组的值是这个前缀的最长可以匹配前缀子串的结尾字符下标。这句话有点拗口，我举了一个例子，你一看应该就懂了。
有了next数组，我们很容易就可以实现KMP算法了。我先假设next数组已经计算好了，先给出KMP算法的框架代码。
// a, b分别是主串和模式串；n, m分别是主串和模式串的长度。 public static int kmp(char[] a, int n, char[] b, int m) { int[] next = getNexts(b, m); int j = 0; for (int i = 0; i &amp;lt; n; ++i) { while (j &amp;gt; 0 &amp;amp;&amp;amp; a[i] != b[j]) { // 一直找到a[i]和b[j] j = next[j - 1] + 1; } if (a[i] == b[j]) { ++j; } if (j == m) { // 找到匹配模式串的了 return i - m + 1; } } return -1; } 失效函数计算方法KMP算法的基本原理讲完了，我们现在来看最复杂的部分，也就是next数组是如何计算出来的？</description></item><item><title>34_理解虚拟机：你在云上拿到的计算机是什么样的？</title><link>https://artisanbox.github.io/4/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/34/</guid><description>上世纪60年代，计算机还是异常昂贵的设备，实际的计算机使用需求要面临两个挑战。第一，计算机特别昂贵，我们要尽可能地让计算机忙起来，一直不断地去处理一些计算任务。第二，很多工程师想要用上计算机，但是没有能力自己花钱买一台，所以呢，我们要让很多人可以共用一台计算机。
缘起分时系统为了应对这两个问题，分时系统的计算机就应运而生了。
无论是个人用户，还是一个小公司或者小机构，你都不需要花大价钱自己去买一台电脑。你只需要买一个输入输出的终端，就好像一套鼠标、键盘、显示器这样的设备，然后通过电话线，连到放在大公司机房里面的计算机就好了。这台计算机，会自动给程序或任务分配计算时间。你只需要为你花费的“计算时间”和使用的电话线路付费就可以了。比方说，比尔·盖茨中学时候用的学校的计算机，就是GE的分时系统。
图片来源图片里面的“计算机”其实只是一个终端而已，并没有计算能力，要通过电话线连接到实际的计算机上，才能完成运算从“黑色星期五”到公有云现代公有云上的系统级虚拟机能够快速发展，其实和分时系统的设计思路是一脉相承的，这其实就是来自于电商巨头亚马逊大量富余的计算能力。
和国内有“双十一”一样，美国会有感恩节的“黑色星期五（Black Friday）”和“网络星期一（Cyber Monday）”，这样一年一度的大型电商促销活动。几天的活动期间，会有大量的用户进入亚马逊这样的网站，看商品、下订单、买东西。这个时候，整个亚马逊需要的服务器计算资源可能是平时的数十倍。
于是，亚马逊会按照“黑色星期五”和“网络星期一”的用户访问量，来准备服务器资源。这个就带来了一个问题，那就是在一年的365天里，有360天这些服务器资源是大量空闲的。要知道，这个空闲的服务器数量不是一台两台，也不是几十几百台。根据媒体的估算，亚马逊的云服务器AWS在2014年就已经超过了150万台，到了2019年的今天，估计已经有超过千万台的服务器。
平时有这么多闲着的服务器实在是太浪费了，所以，亚马逊就想把这些服务器给租出去。出租物理服务器当然是可行的，但是却不太容易自动化，也不太容易面向中小客户。
直接出租物理服务器，意味着亚马逊只能进行服务器的“整租”，这样大部分中小客户就不愿意了。为了节约数据中心的空间，亚马逊实际用的物理服务器，大部分多半是强劲的高端8核乃至12核的服务器。想要租用这些服务器的中小公司，起步往往只需要1个CPU核心乃至更少资源的服务器。一次性要他们去租一整台服务器，就好像刚毕业想要租个单间，结果你非要整租个别墅给他。
这个“整租”的问题，还发生在“时间”层面。物理服务器里面装好的系统和应用，不租了而要再给其他人使用，就必须清空里面已经装好的程序和数据，得做一次“重装”。如果我们只是暂时不用这个服务器了，过一段时间又要租这个服务器，数据中心服务商就不得不先重装整个系统，然后租给别人。等别人不用了，再重装系统租给你，特别地麻烦。
其实，对于想要租用服务器的用户来说，最好的体验不是租房子，而是住酒店。我住一天，我就付一天的钱。这次是全家出门，一次多定几间酒店房间就好啦。
而这样的需求，用虚拟机技术来实现，再好不过了。虚拟机技术，使得我们可以在一台物理服务器上，同时运行多个虚拟服务器，并且可以动态去分配，每个虚拟服务器占用的资源。对于不运行的虚拟服务器，我们也可以把这个虚拟服务器“关闭”。这个“关闭”了的服务器，就和一个被关掉的物理服务器一样，它不会再占用实际的服务器资源。但是，当我们重新打开这个虚拟服务器的时候，里面的数据和应用都在，不需要再重新安装一次。
虚拟机的技术变迁那虚拟机技术到底是怎么一回事呢？下面我带你具体来看一看，它的技术变迁过程，好让你能更加了解虚拟机，从而更好地使用它。
虚拟机（Virtual Machine）技术，其实就是指在现有硬件的操作系统上，能够模拟一个计算机系统的技术。而模拟一个计算机系统，最简单的办法，其实不能算是虚拟机技术，而是一个模拟器（Emulator）。
解释型虚拟机要模拟一个计算机系统，最简单的办法，就是兼容这个计算机系统的指令集。我们可以开发一个应用程序，跑在我们的操作系统上。这个应用程序呢，可以识别我们想要模拟的、计算机系统的程序格式和指令，然后一条条去解释执行。
在这个过程中，我们把原先的操作系统叫作宿主机（Host），把能够有能力去模拟指令执行的软件，叫作模拟器（Emulator），而实际运行在模拟器上被“虚拟”出来的系统呢，我们叫客户机（Guest VM）。
这个方式，其实和运行Java程序的Java虚拟机很像。只不过，Java虚拟机运行的是Java自己定义发明的中间代码，而不是一个特定的计算机系统的指令。
这种解释执行另一个系统的方式，有没有真实的应用案例呢？当然是有的，如果你是一个Android开发人员，你在开发机上跑的Android模拟器，其实就是这种方式。如果你喜欢玩一些老游戏，可以注意研究一下，很多能在Windows下运行的游戏机模拟器，用的也是类似的方式。
这种解释执行方式的最大的优势就是，模拟的系统可以跨硬件。比如，Android手机用的CPU是ARM的，而我们的开发机用的是Intel X86的，两边的CPU指令集都不一样，但是一样可以正常运行。如果你想玩的街机游戏，里面的硬件早就已经停产了，那你自然只能选择MAME这样的模拟器。
图片来源MAME模拟器的界面不过这个方式也有两个明显的缺陷。第一个是，我们做不到精确的“模拟”。很多的老旧的硬件的程序运行，要依赖特定的电路乃至电路特有的时钟频率，想要通过软件达到100%模拟是很难做到的。第二个缺陷就更麻烦了，那就是这种解释执行的方式，性能实在太差了。因为我们并不是直接把指令交给CPU去执行的，而是要经过各种解释和翻译工作。
所以，虽然模拟器这样的形式有它的实际用途。甚至为了解决性能问题，也有类似于Java当中的JIT这样的“编译优化”的办法，把本来解释执行的指令，编译成Host可以直接运行的指令。但是，这个性能还是不能让人满意。毕竟，我们本来是想要把空余的计算资源租用出去的。如果我们空出来的计算能力算是个大平层，结果经过模拟器之后能够租出去的计算能力就变成了一个格子间，那我们就划不来了。
Type-1和Type-2：虚拟机的性能提升所以，我们希望我们的虚拟化技术，能够克服上面的模拟器方式的两个缺陷。同时，我们可以放弃掉模拟器方式能做到的跨硬件平台的这个能力。因为毕竟对于我们想要做的云服务里的“服务器租赁”业务来说，中小客户想要租的也是一个x86的服务器。而另外一方面，他们希望这个租用的服务器用起来，和直接买一台或者租一台物理服务器没有区别。作为出租方的我们，也希望服务器不要因为用了虚拟化技术，而在中间损耗掉太多的性能。
所以，首先我们需要一个“全虚拟化”的技术，也就是说，我们可以在现有的物理服务器的硬件和操作系统上，去跑一个完整的、不需要做任何修改的客户机操作系统（Guest OS）。那么，我们怎么在一个操作系统上，再去跑多个完整的操作系统呢？答案就是，我们自己做软件开发中很常用的一个解决方案，就是加入一个中间层。在虚拟机技术里面，这个中间层就叫作虚拟机监视器，英文叫VMM（Virtual Machine Manager）或者Hypervisor。
如果说我们宿主机的OS是房东的话，这个虚拟机监视器呢，就好像一个二房东。我们运行的虚拟机，都不是直接和房东打交道，而是要和这个二房东打交道。我们跑在上面的虚拟机呢，会把整个的硬件特征都映射到虚拟机环境里，这包括整个完整的CPU指令集、I/O操作、中断等等。
既然要通过虚拟机监视器这个二房东，我们实际的指令是怎么落到硬件上去实际执行的呢？这里有两种办法，也就是Type-1和Type-2这两种类型的虚拟机。
我们先来看Type-2类型的虚拟机。在Type-2虚拟机里，我们上面说的虚拟机监视器好像一个运行在操作系统上的软件。你的客户机的操作系统呢，把最终到硬件的所有指令，都发送给虚拟机监视器。而虚拟机监视器，又会把这些指令再交给宿主机的操作系统去执行。
那这时候你就会问了，这和上面的模拟器看起来没有那么大分别啊？看起来，我们只是把在模拟器里的指令翻译工作，挪到了虚拟机监视器里。没错，Type-2型的虚拟机，更多是用在我们日常的个人电脑里，而不是用在数据中心里。
在数据中心里面用的虚拟机，我们通常叫作Type-1型的虚拟机。这个时候，客户机的指令交给虚拟机监视器之后呢，不再需要通过宿主机的操作系统，才能调用硬件，而是可以直接由虚拟机监视器去调用硬件。
另外，在数据中心里面，我们并不需要在Intel x86上面去跑一个ARM的程序，而是直接在x86上虚拟一个x86硬件的计算机和操作系统。所以，我们的指令不需要做什么翻译工作，可以直接往下传递执行就好了，所以指令的执行效率也会很高。
所以，在Type-1型的虚拟机里，我们的虚拟机监视器其实并不是一个操作系统之上的应用层程序，而是一个嵌入在操作系统内核里面的一部分。无论是KVM、XEN还是微软自家的Hyper-V，其实都是系统级的程序。
因为虚拟机监视器需要直接和硬件打交道，所以它也需要包含能够直接操作硬件的驱动程序。所以Type-1的虚拟机监视器更大一些，同时兼容性也不能像Type-2型那么好。不过，因为它一般都是部署在我们的数据中心里面，硬件完全是统一可控的，这倒不是一个问题了。
Docker：新时代的最佳选择？虽然，Type-1型的虚拟机看起来已经没有什么硬件损耗。但是，这里面还是有一个浪费的资源。在我们实际的物理机上，我们可能同时运行了多个的虚拟机，而这每一个虚拟机，都运行了一个属于自己的单独的操作系统。
多运行一个操作系统，意味着我们要多消耗一些资源在CPU、内存乃至磁盘空间上。那我们能不能不要多运行的这个操作系统呢？
其实是可以的。因为我们想要的未必是一个完整的、独立的、全虚拟化的虚拟机。我们很多时候想要租用的不是“独立服务器”，而是独立的计算资源。在服务器领域，我们开发的程序都是跑在Linux上的。其实我们并不需要一个独立的操作系统，只要一个能够进行资源和环境隔离的“独立空间”就好了。那么，能够满足这个需求的解决方案，就是过去几年特别火热的Docker技术。使用Docker来搭建微服务，可以说是过去两年大型互联网公司的必经之路了。
在实践的服务器端的开发中，虽然我们的应用环境需要各种各样不同的依赖，可能是不同的PHP或者Python的版本，可能是操作系统里面不同的系统库，但是通常来说，我们其实都是跑在Linux内核上的。通过Docker，我们不再需要在操作系统上再跑一个操作系统，而只需要通过容器编排工具，比如Kubernetes或者Docker Swarm，能够进行各个应用之间的环境和资源隔离就好了。
这种隔离资源的方式呢，也有人称之为“操作系统级虚拟机”，好和上面的全虚拟化虚拟机对应起来。不过严格来说，Docker并不能算是一种虚拟机技术，而只能算是一种资源隔离的技术而已。
总结延伸这一讲，我从最古老的分时系统讲起，介绍了虚拟机的相关技术。我们现在的云服务平台上，你能够租到的服务器其实都是虚拟机，而不是物理机。而正是虚拟机技术的出现，使得整个云服务生态得以出现。
虚拟机是模拟一个计算机系统的技术，而其中最简单的办法叫模拟器。我们日常在PC上进行Android开发，其实就是在使用这样的模拟器技术。不过模拟器技术在性能上实在不行，所以我们才有了虚拟化这样的技术。
在宿主机的操作系统上，运行一个虚拟机监视器，然后再在虚拟机监视器上运行客户机的操作系统，这就是现代的虚拟化技术。这里的虚拟化技术可以分成Type-1和Type-2这两种类型。
Type-1类型的虚拟化机，实际的指令不需要再通过宿主机的操作系统，而可以直接通过虚拟机监视器访问硬件，所以性能比Type-2要好。而Type-2类型的虚拟机，所有的指令需要经历客户机操作系统、虚拟机监视器、宿主机操作系统，所以性能上要慢上不少。不过因为经历了宿主机操作系统的一次“翻译”过程，它的硬件兼容性往往会更好一些。
今天，即使是Type-1型的虚拟机技术，我们也会觉得有一些性能浪费。我们常常在同一个物理机上，跑上8个、10个的虚拟机。而且这些虚拟机的操作系统，其实都是同一个Linux Kernel的版本。于是，轻量级的Docker技术就进入了我们的视野。Docker也被很多人称之为“操作系统级”的虚拟机技术。不过Docker并没有再单独运行一个客户机的操作系统，而是直接运行在宿主机操作系统的内核之上。所以，Docker也是现在流行的微服务架构底层的基础设施。
推荐阅读又到了阅读英文文章的时间了。想要更多了解虚拟机、Docker这些相关技术的概念和知识，特别是进一步理解Docker的细节，你可以去读一读FreeCodeCamp里的A Beginner-Friendly Introduction to Containers, VMs and Docker这篇文章。
课后思考我们在程序开发过程中，除了会用今天讲到的系统级虚拟机之外，还会常常遇到Java虚拟机这样的进程级虚拟机。那么，JVM这个进程级虚拟机是为了解决什么问题而出现的呢？今天我们讲到的系统级虚拟机发展历程中的各种优化手段，有哪些是JVM中也可以通用的呢？
欢迎留言和我分享你的疑惑和见解。如果有收获，你也可以把今天的文章分享给你朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>34｜内存管理第1关：Arena技术和元数据</title><link>https://artisanbox.github.io/3/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/36/</guid><description>你好，我是宫文学。
通过前面8节课的学习，我们实现了对浮点数、字符串、数组、自定义对象类型和函数类型的支持，涵盖了TypeScript的一些关键数据类型，也了解了实现这些语言特性所需要的一些关键技术。
在这些数据类型中，字符串、数组、class实例，还有闭包，都需要从堆中申请内存，但我们目前还没有实现内存回收机制。所以，如果用我们现在的版本，长时间运行某些需要在堆中申请内存的程序，可能很快会就把内存耗光。
所以，接下来的两节课，我们就来补上这个缺陷，实现一个简单的内存管理模块，支持内存的申请、内存垃圾的识别和回收功能。在这个过程中，你会对内存管理的原理产生更加清晰的认识，并且能够自己动手实现基本的内存管理功能。
那么，首先我们要分析一下内存管理涉及的技术点，以此来确定我们自己的技术方案。
内存管理中的技术点计算机语言中的内存管理模块，能够对内存从申请到回收进行全生命周期的管理。
内存的申请方面，一般不会为每个对象从操作系统申请内存资源，而是要提供自己的内存分配机制。
而垃圾回收技术则是内存管理中的难点。垃圾回收有很多个技术方案，包括标记-清除、标记-整理、停止-拷贝和自动引用计数这些基础的算法。在产品级的实现里，这些算法又被进一步复杂化。比如，你可以针对老的内存对象和新内存对象，使用不同的回收算法，从而形成分代管理的方案。又比如，为了充分减少由于垃圾收集所导致的程序停顿，发展出来了增量式回收和并行回收的技术。
关于这些算法的介绍，你可以参考《编译原理之美》的33节，里面介绍了各种垃圾收集算法。还有《编译原理实战课》的第32节，里面分析了Python、Java、JavaScript、Julia、Go、Swift、Objective-C等各种语言采用的内存管理技术的特点，也讨论了这些技术与语言特性的关系。在这节课里，我就不重复介绍这些内容了。
垃圾收集对语言运行的影响是很大的，因此我们希望垃圾回收导致的程序停顿越短越好，消耗的系统资源越少越好。这些苛刻的要求，导致在很多现代语言中，垃圾回收器（GC）成了运行时中技术挑战很高的一个模块。不过，再难的技术都是一口口吃下的。在这节课里，我们先不去挑战那些特别复杂的算法，而是选择一个最容易上手的、入门级的算法，标记-清除算法来做示范。
标记-清除算法的思路比较简单，只需要简单两步：
首先，我们要找出哪些内存对象不是垃圾，并进行标记； 第二，回收掉所有没做标记的对象，也就是垃圾对象。 我们通过一个例子来看一下。在下图中，x和y变量分别指向了两个内存对象，这两个内存对象可能是自定义类的实例，也有可能是闭包、字符串或数组。这些对象中的字段，又可能会引用另外的对象。
在图中，当变量x失效以后，它直接引用和间接引用的对象就会成为内存垃圾，你就可以回收掉它了。这就是标记-清除算法的原理，非常简单。
在这个图里，变量x和y叫做GC的根（GC root）。算法需要从这些根节点出发，去遍历它直接或间接引用的对象。这个过程，实际上就是图的遍历算法。
好了，算法上大的原理我们就搞清楚了。那接下来，我们需要讨论一些实现上的技术点，包括如何管理内存的申请和释放、如何遍历所有的栈帧和内存对象，等等。
首先说一下如何管理内存的申请和释放。
内存的申请和释放在我们前面实现的、C语言版本的字节码虚拟机中，我们就曾经讨论过如何高效申请内存的问题。我们发现，如果调用操作系统的接口频繁地申请和释放小的内存块，会大大降低系统的整体性能。所以，我们采用了Arena技术，也就是一次性地从操作系统中申请比较大块的内存，然后再自行把这块大内存划分成小块的内存，给自己的语言使用。
在今天这节课，我们仍然使用Arena技术来管理内存：当我们创建新的内存对象的时候，就从Arena中找一块未被占用的内容空间；而在回收内存对象的时候，就把内存对象占的内存区域标记成自由空间。
在这里你会发现，为了记住哪些内存是被分配出去的，那些内存是可用的，我们需要一个数据结构来保存这些信息。在我的参考实现里，我用了一个简单的链表来保存这些信息。每块被分配出去的内存，都是链表的一个节点。节点里保存了当前内存对象的大小，以及下一个节点的地址。
顺着这个链表，你可以查找出自由的内存。假设节点1的地址是80，对象大小是48字节，节点2地址是180，那么节点1和2之间就有52个字节的自由空间。
当我们要申请内存的时候，如果我们要申请的对象大小低于52个字节，那就可以把这块空间分配给它。这个时候，我们就要修改链表的指针，把新的节点插入到节点1和节点2之间。
如果要回收内存呢？也比较简单，我们就从链表中去掉这个节点就好了。
了解了内存申请和释放的内容后，接下来，我们就需要查找并标记哪些内存是仍然被使用的，从而识别出内存垃圾。这就需要程序遍历所有栈帧中的GC根引用的对象，以及这些对象引用的其他对象。而要完成这样的遍历，我们需要知道函数、类和闭包等的元数据信息才可以。
管理元数据我们前面说过，GC根就是那些引用了内存对象的变量。而我们知道，我们的程序中用到的变量，有可能是在栈中的，也有可能是在寄存器里的。那到底栈里的哪个位置是变量，哪个寄存器是变量呢？另外，如何遍历所有的栈帧呢？如何知道每个栈帧的开头和结尾位置？又如何知道哪个栈帧是第一个栈帧，从而结束遍历呢？这些都是需要解决的技术问题，我们一个一个来看。
首先，我们要确定栈帧和寄存器里，哪些是变量，也就是GC根。
这就需要我们保存变量在栈帧中的布局信息。对于每个函数来说，这些布局信息都是唯一的。这些信息可以看做是函数的元数据的一部分。其他元数据信息包括函数的名称，等等。
我们用一个例子来分析一下变量布局情况。下面的foo函数的栈帧里，包括几个本地变量和几个临时变量。基于我们的寄存器分配算法，这些变量有些会被Spill到栈帧中。比如，如果某个变量使用的寄存器是需要Caller保护的，那么在调用另一个函数的时候，这些变量就会被Spill到内存中。
function foo(b:number):number{ let a:number[] = [1,2,b]; let s:string = "Hello PlayScript!"; println(s); println(a[2]); return b*10; } println(foo(2)); 另外，如果一个函数用到了需要Callee保护的寄存器，那么这些寄存器的信息也会被写入到栈帧，这些寄存器的值也可能是调用者的某个变量。算法可以查询调用者的变量布局信息来确认这一点。
最终，对于foo函数来说，这些变量在栈帧中的布局如下：
那包含了变量布局的元数据信息，应该保存到哪里呢？你可能已经想到了，它们可以被保存在可执行文件的数据区呀，就像之前我们保存vtable那样。
在具体实现的时候，这个数据区可以分成多个组成部分。像vtable这样的数据，出于性能上的要求，我们最好能够比较快捷地访问，所以我们让程序通过“1跳”，也就是只做一次获取地址的操作，就能到查到方法的入口地址。而对于其他元数据信息，由于数据类型跟vtable的不一样，可以安排到另一个数据区中，并从第一个数据区链接过去。元数据在静态数据区的布局如下图所示：
它们在汇编代码中可以写成下面的样子：
.section __DATA,__const .globl _foo.meta ## can be accessed globally .p2align 3 ## 8 byte alignment _foo.meta: .</description></item><item><title>35_join语句怎么优化？</title><link>https://artisanbox.github.io/1/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/35/</guid><description>在上一篇文章中，我和你介绍了join语句的两种算法，分别是Index Nested-Loop Join(NLJ)和Block Nested-Loop Join(BNL)。
我们发现在使用NLJ算法的时候，其实效果还是不错的，比通过应用层拆分成多个语句然后再拼接查询结果更方便，而且性能也不会差。
但是，BNL算法在大表join的时候性能就差多了，比较次数等于两个表参与join的行数的乘积，很消耗CPU资源。
当然了，这两个算法都还有继续优化的空间，我们今天就来聊聊这个话题。
为了便于分析，我还是创建两个表t1、t2来和你展开今天的问题。
create table t1(id int primary key, a int, b int, index(a)); create table t2 like t1; drop procedure idata; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=1000)do insert into t1 values(i, 1001-i, i); set i=i+1; end while; set i=1; while(i&amp;lt;=1000000)do insert into t2 values(i, i, i); set i=i+1; end while;
end;; delimiter ; call idata(); 为了便于后面量化说明，我在表t1里，插入了1000行数据，每一行的a=1001-id的值。也就是说，表t1中字段a是逆序的。同时，我在表t2中插入了100万行数据。</description></item><item><title>35_Trie树：如何实现搜索引擎的搜索关键词提示功能？</title><link>https://artisanbox.github.io/2/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/36/</guid><description>搜索引擎的搜索关键词提示功能，我想你应该不陌生吧？为了方便快速输入，当你在搜索引擎的搜索框中，输入要搜索的文字的某一部分的时候，搜索引擎就会自动弹出下拉框，里面是各种关键词提示。你可以直接从下拉框中选择你要搜索的东西，而不用把所有内容都输入进去，一定程度上节省了我们的搜索时间。
尽管这个功能我们几乎天天在用，作为一名工程师，你是否思考过，它是怎么实现的呢？它底层使用的是哪种数据结构和算法呢？
像Google、百度这样的搜索引擎，它们的关键词提示功能非常全面和精准，肯定做了很多优化，但万变不离其宗，底层最基本的原理就是今天要讲的这种数据结构：Trie树。
什么是“Trie树”？Trie树，也叫“字典树”。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。
当然，这样一个问题可以有多种解决方法，比如散列表、红黑树，或者我们前面几节讲到的一些字符串匹配算法，但是，Trie树在这个问题的解决上，有它特有的优点。不仅如此，Trie树能解决的问题也不限于此，我们一会儿慢慢分析。
现在，我们先来看下，Trie树到底长什么样子。
我举个简单的例子来说明一下。我们有6个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这6个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？
这个时候，我们就可以先对这6个字符串做一下预处理，组织成Trie树的结构，之后每次查找，都是在Trie树中进行匹配查找。Trie树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。最后构造出来的就是下面这个图中的样子。
其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）。
为了让你更容易理解Trie树是怎么构造出来的，我画了一个Trie树构造的分解过程。构造过程的每一步，都相当于往Trie树中插入一个字符串。当所有字符串都插入完成之后，Trie树就构造好了。
当我们在Trie树中查找一个字符串的时候，比如查找字符串“her”，那我们将要查找的字符串分割成单个的字符h，e，r，然后从Trie树的根节点开始匹配。如图所示，绿色的路径就是在Trie树中匹配的路径。
如果我们要查找的是字符串“he”呢？我们还用上面同样的方法，从根节点开始，沿着某条路径来匹配，如图所示，绿色的路径，是字符串“he”匹配的路径。但是，路径的最后一个节点“e”并不是红色的。也就是说，“he”是某个字符串的前缀子串，但并不能完全匹配任何字符串。
如何实现一棵Trie树？知道了Trie树长什么样子，我们现在来看下，如何用代码来实现一个Trie树。
从刚刚Trie树的介绍来看，Trie树主要有两个操作，一个是将字符串集合构造成Trie树。这个过程分解开来的话，就是一个将字符串插入到Trie树的过程。另一个是在Trie树中查询一个字符串。
了解了Trie树的两个主要操作之后，我们再来看下，如何存储一个Trie树？
从前面的图中，我们可以看出，Trie树是一个多叉树。我们知道，二叉树中，一个节点的左右子节点是通过两个指针来存储的，如下所示Java代码。那对于多叉树来说，我们怎么存储一个节点的所有子节点的指针呢？
class BinaryTreeNode { char data; BinaryTreeNode left; BinaryTreeNode right; } 我先介绍其中一种存储方式，也是经典的存储方式，大部分数据结构和算法书籍中都是这么讲的。还记得我们前面讲到的散列表吗？借助散列表的思想，我们通过一个下标与字符一一映射的数组，来存储子节点的指针。这句话稍微有点抽象，不怎么好懂，我画了一张图你可以看看。
假设我们的字符串中只有从a到z这26个小写字母，我们在数组中下标为0的位置，存储指向子节点a的指针，下标为1的位置存储指向子节点b的指针，以此类推，下标为25的位置，存储的是指向的子节点z的指针。如果某个字符的子节点不存在，我们就在对应的下标的位置存储null。
class TrieNode { char data; TrieNode children[26]; } 当我们在Trie树中查找字符串的时候，我们就可以通过字符的ASCII码减去“a”的ASCII码，迅速找到匹配的子节点的指针。比如，d的ASCII码减去a的ASCII码就是3，那子节点d的指针就存储在数组中下标为3的位置中。
描述了这么多，有可能你还是有点懵，我把上面的描述翻译成了代码，你可以结合着一块看下，应该有助于你理解。
public class Trie { private TrieNode root = new TrieNode('/'); // 存储无意义字符 // 往Trie树中插入一个字符串 public void insert(char[] text) { TrieNode p = root; for (int i = 0; i &amp;lt; text.length; ++i) { int index = text[i] - &amp;lsquo;a&amp;rsquo;; if (p.</description></item><item><title>35_存储器层次结构全景：数据存储的大金字塔长什么样？</title><link>https://artisanbox.github.io/4/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/35/</guid><description>今天开始，我们要进入到计算机另一个重要的组成部分，存储器。
如果你自己组装过PC机，你肯定知道，想要CPU，我们只要买一个就好了，但是存储器，却有不同的设备要买。比方说，我们要买内存，还要买硬盘。买硬盘的时候，不少人会买一块SSD硬盘作为系统盘，还会买上一块大容量的HDD机械硬盘作为数据盘。内存和硬盘都是我们的存储设备。而且，像硬盘这样的持久化存储设备，同时也是一个I/O设备。
在实际的软件开发过程中，我们常常会遇到服务端的请求响应时间长，吞吐率不够的情况。在分析对应问题的时候，相信你没少听过类似“主要瓶颈不在CPU，而在I/O”的论断。可见，存储在计算机中扮演着多么重要的角色。那接下来这一整个章节，我会为你梳理和讲解整个存储器系统。
这一讲，我们先从存储器的层次结构说起，让你对各种存储器设备有一个整体的了解。
理解存储器的层次结构在有计算机之前，我们通常把信息和数据存储在书、文件这样的物理介质里面。有了计算机之后，我们通常把数据存储在计算机的存储器里面。而存储器系统是一个通过各种不同的方法和设备，一层一层组合起来的系统。下面，我们把计算机的存储器层次结构和我们日常生活里处理信息、阅读书籍做个对照，好让你更容易理解、记忆存储器的层次结构。
我们常常把CPU比喻成计算机的“大脑”。我们思考的东西，就好比CPU中的寄存器（Register）。寄存器与其说是存储器，其实它更像是CPU本身的一部分，只能存放极其有限的信息，但是速度非常快，和CPU同步。
而我们大脑中的记忆，就好比CPU Cache（CPU高速缓存，我们常常简称为“缓存”）。CPU Cache用的是一种叫作SRAM（Static Random-Access Memory，静态随机存取存储器）的芯片。
SRAMSRAM之所以被称为“静态”存储器，是因为只要处在通电状态，里面的数据就可以保持存在。而一旦断电，里面的数据就会丢失了。在SRAM里面，一个比特的数据，需要6～8个晶体管。所以SRAM的存储密度不高。同样的物理空间下，能够存储的数据有限。不过，因为SRAM的电路简单，所以访问速度非常快。
图片来源 6个晶体管组成SRAM的一个比特在CPU里，通常会有L1、L2、L3这样三层高速缓存。每个CPU核心都有一块属于自己的L1高速缓存，通常分成指令缓存和数据缓存，分开存放CPU使用的指令和数据。
不知道你还记不记得我们在第22讲讲过的哈佛架构，这里的指令缓存和数据缓存，其实就是来自于哈佛架构。L1的Cache往往就嵌在CPU核心的内部。
L2的Cache同样是每个CPU核心都有的，不过它往往不在CPU核心的内部。所以，L2 Cache的访问速度会比L1稍微慢一些。而L3 Cache，则通常是多个CPU核心共用的，尺寸会更大一些，访问速度自然也就更慢一些。
你可以把CPU中的L1 Cache理解为我们的短期记忆，把L2/L3 Cache理解成长期记忆，把内存当成我们拥有的书架或者书桌。 当我们自己记忆中没有资料的时候，可以从书桌或者书架上拿书来翻阅。这个过程中就相当于，数据从内存中加载到CPU的寄存器和Cache中，然后通过“大脑”，也就是CPU，进行处理和运算。
DRAM内存用的芯片和Cache有所不同，它用的是一种叫作DRAM（Dynamic Random Access Memory，动态随机存取存储器）的芯片，比起SRAM来说，它的密度更高，有更大的容量，而且它也比SRAM芯片便宜不少。
DRAM被称为“动态”存储器，是因为DRAM需要靠不断地“刷新”，才能保持数据被存储起来。DRAM的一个比特，只需要一个晶体管和一个电容就能存储。所以，DRAM在同样的物理空间下，能够存储的数据也就更多，也就是存储的“密度”更大。但是，因为数据是存储在电容里的，电容会不断漏电，所以需要定时刷新充电，才能保持数据不丢失。DRAM的数据访问电路和刷新电路都比SRAM更复杂，所以访问延时也就更长。
存储器的层级结构整个存储器的层次结构，其实都类似于SRAM和DRAM在性能和价格上的差异。SRAM更贵，速度更快。DRAM更便宜，容量更大。SRAM好像我们的大脑中的记忆，而DRAM就好像属于我们自己的书桌。
大脑（CPU）中的记忆（L1 Cache），不仅受成本层面的限制，更受物理层面的限制。这就好比L1 Cache不仅昂贵，其访问速度和它到CPU的物理距离有关。芯片造得越大，总有部分离CPU的距离会变远。电信号的传输速度又受物理原理的限制，没法超过光速。所以想要快，并不是靠多花钱就能解决的。
我们自己的书房和书桌（也就是内存）空间一般是有限的，没有办法放下所有书（也就是数据）。如果想要扩大空间的话，就相当于要多买几平方米的房子，成本就会很高。于是，想要放下更多的书，我们就要寻找更加廉价的解决方案。
没错，我们想到了公共图书馆。对于内存来说，SSD（Solid-state drive或Solid-state disk，固态硬盘）、HDD（Hard Disk Drive，硬盘）这些被称为硬盘的外部存储设备，就是公共图书馆。于是，我们就可以去家附近的图书馆借书了。图书馆有更多的空间（存储空间）和更多的书（数据）。
你应该也在自己的个人电脑上用过SSD硬盘。过去几年，SSD这种基于NAND芯片的高速硬盘，价格已经大幅度下降。
而HDD硬盘则是一种完全符合“磁盘”这个名字的传统硬件。“磁盘”的硬件结构，决定了它的访问速度受限于它的物理结构，是最慢的。
这些我们后面都会详细说，你可以对照下面这幅图了解一下，对存储器层次之间的作用和关联有个大致印象就可以了。
存储器的层次关系图从Cache、内存，到SSD和HDD硬盘，一台现代计算机中，就用上了所有这些存储器设备。其中，容量越小的设备速度越快，而且，CPU并不是直接和每一种存储器设备打交道，而是每一种存储器设备，只和它相邻的存储设备打交道。比如，CPU Cache是从内存里加载而来的，或者需要写回内存，并不会直接写回数据到硬盘，也不会直接从硬盘加载数据到CPU Cache中，而是先加载到内存，再从内存加载到Cache中。
这样，各个存储器只和相邻的一层存储器打交道，并且随着一层层向下，存储器的容量逐层增大，访问速度逐层变慢，而单位存储成本也逐层下降，也就构成了我们日常所说的存储器层次结构。
使用存储器的时候，该如何权衡价格和性能？存储器在不同层级之间的性能差异和价格差异，都至少在一个数量级以上。L1 Cache的访问延时是1纳秒（ns），而内存就已经是100纳秒了。在价格上，这两者也差出了400倍。
我这里放了一张各种存储器成本的对比表格，你可以看看。你也可以在点击这个链接，通过拖拉，查看1990～2020年随着硬件设备的进展，访问延时的变化。
因为这个价格和性能的差异，你会看到，我们实际在进行电脑硬件配置的时候，会去组合配置各种存储设备。
我们可以找一台现在主流的笔记本电脑来看看，比如，一款入门级的惠普战66的笔记本电脑。今天在京东上的价格是4999人民币。它的配置是下面这样的。
Intle i5-8265U的CPU（这是一块4核的CPU） 这块CPU每个核有32KB，一共128KB的L1指令Cache。 同样，每个核还有32KB，一共128KB的L1数据Cache，指令Cache和数据Cache都是采用8路组相连的放置策略。 每个核有256KB，一共1MB的L2 Cache。L2 Cache是用4路组相连的放置策略。 最后还有一块多个核心共用的12MB的L3 Cache，采用的是12路组相连的放置策略。 8GB的内存 一块128G的SSD硬盘 一块1T的HDD硬盘 你可以看到，在一台实际的计算机里面，越是速度快的设备，容量就越小。这里一共十多兆的Cache，成本只是几十美元。而8GB的内存、128G的SSD以及1T的HDD，大概零售价格加在一起，也就和我们的高速缓存的价格差不多。
总结延伸这节的内容不知道你掌握了多少呢？为了帮助你记忆，我这里再带你复习一下本节的重点。
我们常常把CPU比喻成高速运转的大脑，那么和大脑同步的寄存器（Register），就存放着我们当下正在思考和处理的数据。而L1-L3的CPU Cache，好比存放在我们大脑中的短期到长期的记忆。我们需要小小花费一点时间，就能调取并进行处理。
我们自己的书桌书架就好比计算机的内存，能放下更多的书也就是数据，但是找起来和看起来就要慢上不少。而图书馆更像硬盘这个外存，能够放下更多的数据，找起来也更费时间。从寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，速度越来越慢，空间越来越大，价格也越来越便宜。</description></item><item><title>35｜内存管理第2关：实现垃圾回收</title><link>https://artisanbox.github.io/3/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/37/</guid><description>你好，我是宫文学。
今天这节课，我们继续上一节课未完成的内容，完成垃圾回收功能。
在上一节课，我们已经实现了一个基于Arena做内存分配的模块。并且，我们还在可执行程序里保存了函数、类和闭包相关的元数据信息。
有了上一节课的基础之后，我们这节课就能正式编写垃圾回收的算法了。算法思路是这样的：
首先，我们要有一个种机制来触发垃圾回收，进入垃圾回收的处理程序； 第二，我们要基于元数据信息来遍历栈帧，找到所有的GC根； 第三，从每个GC根出发，我们需要去标记GC根直接和间接引用的内存对象； 最后，我们再基于对象的标记信息，来回收内存垃圾。 在今天这节课，你不仅仅会掌握标记-清除算法，其中涉及的知识点，也会让你能够更容易地实现其他垃圾回收算法，并且让我们的程序能更好地与运行时功能相配合。
那接下来，我们就顺着算法实现思路，看看如何启动垃圾回收机制。
启动垃圾回收机制在现代的计算机语言中，我们可以有各种策略来启动垃圾回收机制。比如，在申请内存时，如果内存不足，就可以触发垃圾回收。甚至，你也可以每隔一段时间就触发一下垃圾收集。不过不论采取哪种机制，我们首先要有办法从程序的正常执行流程，进入垃圾回收程序才行。
进入垃圾回收程序，其实有一个经常使用的时机，就是在函数返回的时候。这个时候，我们可以不像平常那样，使用retq跳回调用者，而是先去检查是否需要做垃圾回收：如果需要做垃圾回收，那就先回收完垃圾，再返回到原来函数的调用者；如果不需要做垃圾回收，那就直接跳转到函数的调用者。
实现这个功能很简单，只需要在return语句之前调用frame_walker这个内置函数，并把当前%rbp寄存器的值作为参数传进去就好了：
visitReturnStatement(rtnStmt:ReturnStatement):any{ if (rtnStmt.exp!=null){ let ret = this.visit(rtnStmt.exp) as Oprand; //调用一个内置函数，来做垃圾回收 this.callBuiltIns(&amp;quot;frame_walker&amp;quot;,[Register.rbp]);//把当前%rbp的值传进去 //把返回值赋给相应的寄存器 let dataType = getCpuDataType(rtnStmt.exp.theType as Type); this.movIfNotSame(dataType, ret, Register.returnReg(dataType)); &amp;hellip; } } 这样，我们就能获得调用GC程序的时机。
在这段代码中，frame_walker内置函数的功能是遍历整个调用栈。这就是我们启动垃圾回收机制后，要进行的下一个任务。接下来我们就来分析一下具体怎么做。
遍历栈帧和对象遍历栈帧其实很简单，因为我们能够知道每个栈帧的起始地址。从哪里知道呢？就是rbp寄存器。rbp寄存器里保存的是每个栈帧的底部地址。
每次新建立一个栈帧的时候，我们总是把前一个栈帧的rbp值保护起来，这就是你在每个函数开头看到的第一行指令：pushq %rbp。因此，我们从栈帧里的第一个8字节区域，就可以读出前一个栈帧的%rbp值，这就意味着我们得到了前一个栈帧的栈底。然后你可以到这个位置，再继续获取更前一个栈帧的地址。具体你可以看下面这张图：
那这个思路真的有用吗？我们直接动手试一下！
首先，我写了一个简单的测试程序。在这个程序里，main函数调用了foo，foo又调用了bar。这样，在foo和bar返回前，我们都可以启动垃圾回收，但从main函数返回的时候就没有必要启动了，因为这个时候进程结束，进程从操作系统申请的所有内存，都会还给操作系统。
function foo(a:number):string{ let s:string = &amp;ldquo;PlayScript!&amp;rdquo; let b:number = bar(a+5); return s; }
function bar(b:number):number{ let a:number[] = [1,2,b];
let s:string = &amp;ldquo;Hello&amp;rdquo;; println(s); println(a[2]); b = b*10; return b; }</description></item><item><title>36_AC自动机：如何用多模式串匹配实现敏感词过滤功能？</title><link>https://artisanbox.github.io/2/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/37/</guid><description>很多支持用户发表文本内容的网站，比如BBS，大都会有敏感词过滤功能，用来过滤掉用户输入的一些淫秽、反动、谩骂等内容。你有没有想过，这个功能是怎么实现的呢？
实际上，这些功能最基本的原理就是字符串匹配算法，也就是通过维护一个敏感词的字典，当用户输入一段文字内容之后，通过字符串匹配算法，来查找用户输入的这段文字，是否包含敏感词。如果有，就用“***”把它替代掉。
我们前面讲过好几种字符串匹配算法了，它们都可以处理这个问题。但是，对于访问量巨大的网站来说，比如淘宝，用户每天的评论数有几亿、甚至几十亿。这时候，我们对敏感词过滤系统的性能要求就要很高。毕竟，我们也不想，用户输入内容之后，要等几秒才能发送出去吧？我们也不想，为了这个功能耗费过多的机器吧？那如何才能实现一个高性能的敏感词过滤系统呢？这就要用到今天的多模式串匹配算法。
基于单模式串和Trie树实现的敏感词过滤我们前面几节讲了好几种字符串匹配算法，有BF算法、RK算法、BM算法、KMP算法，还有Trie树。前面四种算法都是单模式串匹配算法，只有Trie树是多模式串匹配算法。
我说过，单模式串匹配算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串。多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。
尽管，单模式串匹配算法也能完成多模式串的匹配工作。例如开篇的思考题，我们可以针对每个敏感词，通过单模式串匹配算法（比如KMP算法）与用户输入的文字内容进行匹配。但是，这样做的话，每个匹配过程都需要扫描一遍用户输入的内容。整个过程下来就要扫描很多遍用户输入的内容。如果敏感词很多，比如几千个，并且用户输入的内容很长，假如有上千个字符，那我们就需要扫描几千遍这样的输入内容。很显然，这种处理思路比较低效。
与单模式匹配算法相比，多模式匹配算法在这个问题的处理上就很高效了。它只需要扫描一遍主串，就能在主串中一次性查找多个模式串是否存在，从而大大提高匹配效率。我们知道，Trie树就是一种多模式串匹配算法。那如何用Trie树实现敏感词过滤功能呢？
我们可以对敏感词字典进行预处理，构建成Trie树结构。这个预处理的操作只需要做一次，如果敏感词字典动态更新了，比如删除、添加了一个敏感词，那我们只需要动态更新一下Trie树就可以了。
当用户输入一个文本内容后，我们把用户输入的内容作为主串，从第一个字符（假设是字符C）开始，在Trie树中匹配。当匹配到Trie树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符C的下一个字符开始，重新在Trie树中匹配。
基于Trie树的这种处理方法，有点类似单模式串匹配的BF算法。我们知道，单模式串匹配算法中，KMP算法对BF算法进行改进，引入了next数组，让匹配失败时，尽可能将模式串往后多滑动几位。借鉴单模式串的优化改进方法，能否对多模式串Trie树进行改进，进一步提高Trie树的效率呢？这就要用到AC自动机算法了。
经典的多模式串匹配算法：AC自动机AC自动机算法，全称是Aho-Corasick算法。其实，Trie树跟AC自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟KMP算法之间的关系一样，只不过前者针对的是多模式串而已。所以，AC自动机实际上就是在Trie树之上，加了类似KMP的next数组，只不过此处的next数组是构建在树上罢了。如果代码表示，就是下面这个样子：
public class AcNode { public char data; public AcNode[] children = new AcNode[26]; // 字符集只包含a~z这26个字符 public boolean isEndingChar = false; // 结尾字符为true public int length = -1; // 当isEndingChar=true时，记录模式串长度 public AcNode fail; // 失败指针 public AcNode(char data) { this.data = data; } } 所以，AC自动机的构建，包含两个操作：
将多个模式串构建成Trie树；
在Trie树上构建失败指针（相当于KMP中的失效函数next数组）。
关于如何构建Trie树，我们上一节已经讲过了。所以，这里我们就重点看下，构建好Trie树之后，如何在它之上构建失败指针？
我用一个例子给你讲解。这里有4个模式串，分别是c，bc，bcd，abcd；主串是abcd。
Trie树中的每一个节点都有一个失败指针，它的作用和构建过程，跟KMP算法中的next数组极其相似。所以要想看懂这节内容，你要先理解KMP算法中next数组的构建过程。如果你还有点不清楚，建议你先回头去弄懂KMP算法。
假设我们沿Trie树走到p节点，也就是下图中的紫色节点，那p的失败指针就是从root走到紫色节点形成的字符串abc，跟所有模式串前缀匹配的最长可匹配后缀子串，就是箭头指的bc模式串。
这里的最长可匹配后缀子串，我稍微解释一下。字符串abc的后缀子串有两个bc，c，我们拿它们与其他模式串匹配，如果某个后缀子串可以匹配某个模式串的前缀，那我们就把这个后缀子串叫作可匹配后缀子串。
我们从可匹配后缀子串中，找出最长的一个，就是刚刚讲到的最长可匹配后缀子串。我们将p节点的失败指针指向那个最长匹配后缀子串对应的模式串的前缀的最后一个节点，就是下图中箭头指向的节点。
计算每个节点的失败指针这个过程看起来有些复杂。其实，如果我们把树中相同深度的节点放到同一层，那么某个节点的失败指针只有可能出现在它所在层的上一层。</description></item><item><title>36_为什么临时表可以重名？</title><link>https://artisanbox.github.io/1/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/36/</guid><description>今天是大年三十，在开始我们今天的学习之前，我要先和你道一声春节快乐！
在上一篇文章中，我们在优化join查询的时候使用到了临时表。当时，我们是这么用的：
create temporary table temp_t like t1; alter table temp_t add index(b); insert into temp_t select * from t2 where b&amp;gt;=1 and b&amp;lt;=2000; select * from t1 join temp_t on (t1.b=temp_t.b); 你可能会有疑问，为什么要用临时表呢？直接用普通表是不是也可以呢？
今天我们就从这个问题说起：临时表有哪些特征，为什么它适合这个场景？
这里，我需要先帮你厘清一个容易误解的问题：有的人可能会认为，临时表就是内存表。但是，这两个概念可是完全不同的。
内存表，指的是使用Memory引擎的表，建表语法是create table … engine=memory。这种表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在。除了这两个特性看上去比较“奇怪”外，从其他的特征上看，它就是一个正常的表。
而临时表，可以使用各种引擎类型 。如果是使用InnoDB引擎或者MyISAM引擎的临时表，写数据的时候是写到磁盘上的。当然，临时表也可以使用Memory引擎。
弄清楚了内存表和临时表的区别以后，我们再来看看临时表有哪些特征。
临时表的特性为了便于理解，我们来看下下面这个操作序列：
图1 临时表特性示例可以看到，临时表在使用上有以下几个特点：
建表语法是create temporary table …。
一个临时表只能被创建它的session访问，对其他线程不可见。所以，图中session A创建的临时表t，对于session B就是不可见的。
临时表可以与普通表同名。
session A内有同名的临时表和普通表的时候，show create语句，以及增删改查语句访问的是临时表。</description></item><item><title>36_局部性原理：数据库性能跟不上，加个缓存就好了？</title><link>https://artisanbox.github.io/4/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/36/</guid><description>平时进行服务端软件开发的时候，我们通常会把数据存储在数据库里。而服务端系统遇到的第一个性能瓶颈，往往就发生在访问数据库的时候。这个时候，大部分工程师和架构师会拿出一种叫作“缓存”的武器，通过使用Redis或者Memcache这样的开源软件，在数据库前面提供一层缓存的数据，来缓解数据库面临的压力，提升服务端的程序性能。
在数据库前添加数据缓存是常见的性能优化方式那么，不知道你有没有想过，这种添加缓存的策略一定是有效的吗？或者说，这种策略在什么情况下是有效的呢？如果从理论角度去分析，添加缓存一定是我们的最佳策略么？进一步地，如果我们对于访问性能的要求非常高，希望数据在1毫秒，乃至100微秒内完成处理，我们还能用这个添加缓存的策略么？
理解局部性原理我们先来回顾一下，上一讲的这张不同存储器的性能和价目表。可以看到，不同的存储器设备之间，访问速度、价格和容量都有几十乃至上千倍的差异。
以上一讲的Intel 8265U的CPU为例，它的L1 Cache只有256K，L2 Cache有个1MB，L3 Cache有12MB。一共13MB的存储空间，如果按照7美元/1MB的价格计算，就要91美元。
我们的内存有8GB，容量是CPU Cache的600多倍，按照表上的价格差不多就是120美元。如果按照今天京东上的价格，恐怕不到40美元。128G的SSD和1T的HDD，现在的价格加起来也不会超过100美元。虽然容量是内存的16倍乃至128倍，但是它们的访问速度却不到内存的1/1000。
性能和价格的巨大差异，给我们工程师带来了一个挑战：我们能不能既享受CPU Cache的速度，又享受内存、硬盘巨大的容量和低廉的价格呢？你可以停下来自己思考一下，或者点击文章右上方的“请朋友读”，邀请你的朋友一起来思考这个问题。然后，再一起听我的讲解。
好了，现在我公布答案。想要同时享受到这三点，前辈们已经探索出了答案，那就是，存储器中数据的局部性原理（Principle of Locality）。我们可以利用这个局部性原理，来制定管理和访问数据的策略。这个局部性原理包括时间局部性（temporal locality）和空间局部性（spatial locality）这两种策略。
我们先来看时间局部性。这个策略是说，如果一个数据被访问了，那么它在短时间内还会被再次访问。这么看这个策略有点奇怪是吧？我用一个简单的例子给你解释下，你一下就能明白了。
比如说，《哈利波特与魔法石》这本小说，我今天读了一会儿，没读完，明天还会继续读。同理，在一个电子商务型系统中，如果一个用户打开了App，看到了首屏。我们推断他应该很快还会再次访问网站的其他内容或者页面，我们就将这个用户的个人信息，从存储在硬盘的数据库读取到内存的缓存中来。这利用的就是时间局部性。
同一份数据在短时间内会反复多次被访问我们再来看空间局部性。这个策略是说，如果一个数据被访问了，那么和它相邻的数据也很快会被访问。
我们还拿刚才读《哈利波特与魔法石》的例子来说。我读完了这本书之后，感觉这书不错，所以就会借阅整套“哈利波特”。这就好比我们的程序，在访问了数组的首项之后，多半会循环访问它的下一项。因为，在存储数据的时候，数组内的多项数据会存储在相邻的位置。这就好比图书馆会把“哈利波特”系列放在一个书架上，摆放在一起，加载的时候，也会一并加载。我们去图书馆借书，往往会一次性把7本都借回来。
相邻的数据会被连续访问有了时间局部性和空间局部性，我们不用再把所有数据都放在内存里，也不用都放在HDD硬盘上，而是把访问次数多的数据，放在贵但是快一点的存储器里，把访问次数少的数据，放在慢但是大一点的存储器里。这样组合使用内存、SSD硬盘以及HDD硬盘，使得我们可以用最低的成本提供实际所需要的数据存储、管理和访问的需求。
如何花最少的钱，装下亚马逊的所有商品？了解了局部性原理，下面我用一些真实世界中的数据举个例子，带你做个小小的思维体操，来看一看通过局部性原理，利用不同层次存储器的组合，究竟会有什么样的好处。
我们现在要提供一个亚马逊这样的电商网站。我们假设里面有6亿件商品，如果每件商品需要4MB的存储空间（考虑到商品图片的话，4MB已经是一个相对较小的估计了），那么一共需要2400TB（ = 6亿 × 4MB）的数据存储。
如果我们把数据都放在内存里面，那就需要3600万美元（ = 2400TB/1MB × 0.015美元 = 3600万美元）。但是，这6亿件商品中，不是每一件商品都会被经常访问。比如说，有Kindle电子书这样的热销商品，也一定有基本无人问津的商品，比如偏门的缅甸语词典。
如果我们只在内存里放前1%的热门商品，也就是600万件热门商品，而把剩下的商品，放在机械式的HDD硬盘上，那么，我们需要的存储成本就下降到45.6万美元（ = 3600 万美元 × 1% + 2400TB / 1MB × 0.00004 美元），是原来成本的1.3%左右。
这里我们用的就是时间局部性。我们把有用户访问过的数据，加载到内存中，一旦内存里面放不下了，我们就把最长时间没有在内存中被访问过的数据，从内存中移走，这个其实就是我们常用的LRU（Least Recently Used）缓存算法。热门商品被访问得多，就会始终被保留在内存里，而冷门商品被访问得少，就只存放在HDD硬盘上，数据的读取也都是直接访问硬盘。即使加载到内存中，也会很快被移除。越是热门的商品，越容易在内存中找到，也就更好地利用了内存的随机访问性能。
那么，只放600万件商品真的可以满足我们实际的线上服务请求吗？这个就要看LRU缓存策略的缓存命中率（Hit Rate/Hit Ratio）了，也就是访问的数据中，可以在我们设置的内存缓存中找到的，占有多大比例。
内存的随机访问请求需要100ns。这也就意味着，在极限情况下，内存可以支持1000万次随机访问。我们用了24TB内存，如果8G一条的话，意味着有3000条内存，可以支持每秒300亿次（ = 24TB/8GB × 1s/100ns）访问。以亚马逊2017年3亿的用户数来看，我们估算每天的活跃用户为1亿，这1亿用户每人平均会访问100个商品，那么平均每秒访问的商品数量，就是12万次。
但是如果数据没有命中内存，那么对应的数据请求就要访问到HDD磁盘了。刚才的图表中，我写了，一块HDD硬盘只能支撑每秒100次的随机访问，2400TB的数据，以4TB一块磁盘来计算，有600块磁盘，也就是能支撑每秒 6万次（ = 2400TB/4TB × 1s/10ms ）的随机访问。
这就意味着，所有的商品访问请求，都直接到了HDD磁盘，HDD磁盘支撑不了这样的压力。我们至少要50%的缓存命中率，HDD磁盘才能支撑对应的访问次数。不然的话，我们要么选择添加更多数量的HDD硬盘，做到每秒12万次的随机访问，或者将HDD替换成SSD硬盘，让单个硬盘可以支持更多的随机访问请求。
当然，这里我们只是一个简单的估算。在实际的应用程序中，查看一个商品的数据可能意味着不止一次的随机内存或者随机磁盘的访问。对应的数据存储空间也不止要考虑数据，还需要考虑维护数据结构的空间，而缓存的命中率和访问请求也要考虑均值和峰值的问题。
通过这个估算过程，你需要理解，如何进行存储器的硬件规划。你需要考虑硬件的成本、访问的数据量以及访问的数据分布，然后根据这些数据的估算，来组合不同的存储器，能用尽可能低的成本支撑所需要的服务器压力。而当你用上了数据访问的局部性原理，组合起了多种存储器，你也就理解了怎么基于存储器层次结构，来进行硬件规划了。</description></item><item><title>36｜节点之海：怎么生成基于图的IR？</title><link>https://artisanbox.github.io/3/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/38/</guid><description>你好，我是宫文学。
从今天这节课开始，我们就要学习我们这门课的最后一个主题，也就是优化篇。
在前面的起步篇和进阶篇，我们基本上把编译器前端、后端和运行时的主要技术点都过了一遍。虽然到现在，我们语言支持的特性还不够丰富，但基本上都是工作量的问题了。当然，每个技术点我们还可以继续深挖下去，比如我们可以在类型计算中增加泛型计算的内容，可以把上两节课的垃圾收集算法改成更实用的版本，等等。在这个过程中，你还需要不断克服新冒出来的各种技术挑战。不过，基本上，你已经算入了门了，已经把主要的知识脉络都打通了。
而第三部分的内容，是我们整个知识体系中相对独立、相对完整的一部分，也是我们之前屡次提起过，但一直没有真正深化的内容，这就是优化。
优化是现代语言的编译器最重要的工作之一。像V8和其他JavaScript虚拟机的速度，比早期的JavaScript引擎提升了上百倍，让运行在浏览器中的应用可以具备强大的处理能力，这都是优化技术的功劳。
所以，在这第三部分，我会带你涉猎优化技术中的一些基础话题，让你能够理解优化是怎么回事，并能够上手真正做一些优化。
那在这第一节课，我会带你总体了解优化技术的作用、相关算法和所采用的数据结构。接着，我会介绍本课程所采用的一个行业内前沿的数据结构，基于图的IR，又叫节点之海，从而为后面具体的优化任务奠定一个基础。
那首先，我们先简单介绍一下与优化有关的背景知识。
有关优化的背景知识如果我要把优化的内容和算法都大致介绍一下，可能也需要好几节课的篇幅。不过，我在《编译原理之美》的第27节和28节，对优化算法的场景和分类，做了一些通俗的介绍。对于优化算法，特别是基于数据流分析的优化算法，也做了一些介绍。
而在《编译原理实战课》中，我在第14节、15节、21节、23节、24节分别涉及了Java、JavaScript、Julia和Go语言的编译器中的优化技术。所以，我这里就不重复那些内容了，只提炼几个要点，重点和你说一下优化的目标、分类、算法，以及数据结构，让你做好讨论优化技术的知识准备。
优化的目标优化工作最常见的目标，是提高代码运行的性能。在有些场景下，我们还会关注降低目标代码的大小、优化IO次数等其他方面。
优化工作的分类优化技术的种类非常多，我们很难用一个分类标准把各种优化工作都涵盖进去。但通常，我们会按照几个不同的维度来进行分类。
从优化算法的作用范围（或者空间维度）来说，可以分为局部优化（针对基本块的优化）、全局优化（针对整个函数）和过程间优化（多个函数一起统筹优化）。
从优化的时机（也就是时间维度）来说，我们在编译和运行的各个阶段都可以做优化。所以llvm的主要发起人Chris Lattner曾经发表了一篇论文，主题就是全生命周期优化。在编译期呢，编译器的前端就可以做优化，比如我们已经做过一些常数折叠工作。在后端也可以做一些优化，比如我们前面讲过的尾递归和尾调用优化。
但大部分优化是发生在前端和后端中间的过渡阶段，这个阶段有时候也被叫做中端。除了这些，还有运行时的优化。对于V8这种JIT的引擎，在运行时还可以收集程序运行时的一些统计信息，对程序做进一步的优化编译，在某些场景下，甚至比静态编译的效果还好。
优化的算法优化涉及的算法也有很多。比如，前面我们做常量折叠的时候，基本上遍历一下AST，进行属性计算就行了 ，但在做尾递归和尾调用优化的时候，我们就需要基于栈桢的知识对生成的汇编代码做调整，这里面就涉及到了一些优化的算法。但其中最有用的，则是控制流和数据流分析。
对于数据流分析，我们已经讲过不少了。那控制流分析是怎么回事呢？控制流分析的重点是分析程序跳转的模式，比如识别出来哪些是循环语句、哪些是条件分支语句等等，从而找到可以优化的地方。
比如，如果一个循环内部的变量，是跟循环无关的。那我们就可以把它提到循环外面，避免重复计算该变量的值，这种优化叫做“循环无关变量外提”。比如下面的示例程序中，变量c的值跟循环是无关的，所以我们就没必要每次循环都去计算它了。而要实现这种优化，需要优化算法把程序的控制流分析清楚。
function foo(a:number):number{ let b = 0; for (let i = 0; i&amp;lt; a; i++){ let c = a*a; //变量c的值与循环无关，导致重复计算！ b = i + c; } return b; } 优化算法所依托的数据结构针对中端的优化工作，我们最经常采用的数据结构是控制流图，也就是CFG。在生成汇编代码的时候，我们已经接触过控制流图了。当时我们把代码划分成一个个的基本块，每个基本块都保存一些汇编代码，基本块之间形成控制流的跳转。控制流图的数据结构用得很广泛，比如llvm编译器就是基于CFG的，这也意味着像C、C++、Rust、Julia这些基于llvm的语言都受益于CFG数据结构。另外，虽然Go语言并不是基于llvm编译器的，但也采用了CFG。
控制流图最大的优点，当然是能够非常清楚地显示出控制流来，也就是程序的全局结构。而我们做数据流分析的时候，通常也要基于这样一个控制流的大框架来进行。比如，我们在做变量活跃性分析的时候，就是先分析了在单个的基本块里的变量活跃性，然后再扩展到基于CFG，在多个基本块之间做数据流分析。
不过，虽然CFG的应用很普遍，但它并不是唯一用于优化的数据结构。特别是，像Java编译器Graal和JavaScript的V8引擎，都采用了另一种基于图的IR。不过构成这个图的节点并不是基本块。我在这节课后面会重点介绍这个数据结构，并且说明为什么采用这个数据结构的原因。
刚才我挑重点介绍了与优化有关的背景知识。不过，我用短短的篇幅浓缩了太多的干货，你可能会觉得过于抽象。所以，我还是举几个例子更加直观地说明一下与优化有关的知识点，借此我们也可以继续讨论下面关于IR的话题。
一个优化的例子我们先来看这个代码片段，这段代码中，x和y都被赋值成了a+b。
x = a + b y = a + b z = y - x 你用肉眼就能看出来，第二行代码是可以被优化的，因为x和y的值是一样的，所以在第二行代码中，我们就不需要再计算一遍a+b了，直接把x赋值给y就行。这种优化，叫做“公共子表达式删除（Common Subexpression Elimination）”：</description></item><item><title>37_什么时候会使用内部临时表？</title><link>https://artisanbox.github.io/1/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/37/</guid><description>今天是大年初二，在开始我们今天的学习之前，我要先和你道一声春节快乐！
在第16和第34篇文章中，我分别和你介绍了sort buffer、内存临时表和join buffer。这三个数据结构都是用来存放语句执行过程中的中间数据，以辅助SQL语句的执行的。其中，我们在排序的时候用到了sort buffer，在使用join语句的时候用到了join buffer。
然后，你可能会有这样的疑问，MySQL什么时候会使用内部临时表呢？
今天这篇文章，我就先给你举两个需要用到内部临时表的例子，来看看内部临时表是怎么工作的。然后，我们再来分析，什么情况下会使用内部临时表。
union 执行流程为了便于量化分析，我用下面的表t1来举例。
create table t1(id int primary key, a int, b int, index(a)); delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=1000)do insert into t1 values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 然后，我们执行下面这条语句：
(select 1000 as f) union (select id from t1 order by id desc limit 2); 这条语句用到了union，它的语义是，取这两个子查询结果的并集。并集的意思就是这两个集合加起来，重复的行只保留一行。
下图是这个语句的explain结果。
图1 union语句explain 结果可以看到：</description></item><item><title>37_从AST到IR：体会数据流和控制流思维</title><link>https://artisanbox.github.io/3/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/39/</guid><description>你好，我是宫文学。
在上一节课，我们已经初步认识了基于图的IR。那接下来，我们就直接动手来实现它，这需要我们修改之前的编译程序，基于AST来生成IR，然后再基于IR生成汇编代码。
过去，我们语言的编译器只有前端和后端。加上这种中间的IR来过渡以后，我们就可以基于这个IR添加很多优化算法，形成编译器的中端。这样，我们编译器的结构也就更加完整了。
今天这节课，我先带你熟悉这个IR，让你能够以数据流和控制流的思维模式来理解程序的运行逻辑。之后，我还会带你设计IR的数据结构，并介绍HIR、MIR和LIR的概念。最后，我们再来讨论如何基于AST生成IR，从而为基于IR做优化、生成汇编代码做好铺垫。
首先，我还是以上一节课的示例程序为础，介绍一下程序是如何基于这个IR来运行的，加深你对控制流和数据流的理解。
理解基于图的运行逻辑下面是上节课用到的示例程序，一个带有if语句的函数，它能够比较充分地展示数据流和控制流的特点：
function foo(a:number, b:number):number{ let x:number; if (a&amp;gt;10){ x = a + b; } else{ x = a - b; } return x; } 我们把这个程序转化成图，是这样的：
我们之前说了，这个图能够忠实地反映源代码的逻辑。那如果程序是基于这个图来解释执行的，它应该如何运行呢？我们来分析一下。
第1步，从start节点进入程序。
第2步，程序顺着控制流，遇到if节点，并且要在if节点这里产生分支。但为了确定如何产生分支，if节点需要从数据流中获取一个值，这个值是由“&amp;gt;”运算符节点提供的。所以，“a&amp;gt;10”这个表达式，必须要在if节点之前运行完毕，来产生if节点需要的值。
第3步，我们假设a&amp;gt;10返回的是true，那么控制流就会走最左边的分支，也就是if块，直到这个块运行结束。而如果返回的是false，那么就走右边的分支，也就是else块，直到这个块运行结束。这里，if块和else块都是以Begin节点开始，以End节点结束。如果块中有if或for循环这样导致控制流变化的语句，那么它们对应的控制流就会出现在Begin和End之间，作为子图。
第4步，在if块或else执行结束后，控制流又会汇聚到一起。所以图中这里就出现了一个Merge节点。这个节点把两个分支的End节点作为输入，这样我们就能知道实际程序执行的时候，是从哪个分支过来的。
第5步，控制流到达Return节点。Return节点需要返回x的值，所以这就要求数据流必须在Return之前把x的值提供出来。那到底是x1的值，还是x2的值呢？这需要由Phi节点来确定。而Phi节点会从控制流的Merge节点获取控制流路径的信息，决定到底采用x1还是x2。
最后，return语句会把所获取的x值返回，程序结束。
在我这个叙述过程中，你有没有发现一个重要的特点，就是程序的控制流和数据流是相对独立的，只是在个别地方有交互。这跟我们平常写程序的思维方式是很不一样的。在写程序的时候，我们是把数据流与控制流混合在一起的，不加以区分。
比如，针对当前我们的示例程序，我们的源代码里一个if语句，然后在if块和else块中分别写一些代码。这似乎意味着，只能在进入if块的时候，才运行x1=a+b的代码，而在进入else块的时候，才可以运行x2=a-b的逻辑。
但如果你把数据流和控制流分开来思考，你会发现，其实我们在任何时候都可以去计算x1和x2的值，只要在return语句之前计算完就行。比如说，你可以把x1和x2的计算挪到if语句前面去，相当于把程序改成下面的样子：
function foo(a:number, b:number):number{ x1 = a + b; x2 = a - b; if (a&amp;gt;10){ x = x1; } else{ x = x2; } return x; } 当然，针对我们现在的例子，把x1和x2提前计算并没有什么好处，反倒增加了计算量。我的用意在于说明，其实数据流和控制流之间可以不必耦合得那么紧，可以相对独立。
我们可以用这种思想再来分析下我们上节课提到的几个优化技术。</description></item><item><title>37_贪心算法：如何用贪心算法实现Huffman压缩编码？</title><link>https://artisanbox.github.io/2/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/38/</guid><description>基础的数据结构和算法我们基本上学完了，接下来几节，我会讲几种更加基本的算法。它们分别是贪心算法、分治算法、回溯算法、动态规划。更加确切地说，它们应该是算法思想，并不是具体的算法，常用来指导我们设计具体的算法和编码等。
贪心、分治、回溯、动态规划这4个算法思想，原理解释起来都很简单，但是要真正掌握且灵活应用，并不是件容易的事情。所以，接下来的这4个算法思想的讲解，我依旧不会长篇大论地去讲理论，而是结合具体的问题，让你自己感受这些算法是怎么工作的，是如何解决问题的，带你在问题中体会这些算法的本质。我觉得，这比单纯记忆原理和定义要更有价值。
今天，我们先来学习一下贪心算法（greedy algorithm）。贪心算法有很多经典的应用，比如霍夫曼编码（Huffman Coding）、Prim和Kruskal最小生成树算法、还有Dijkstra单源最短路径算法。最小生成树算法和最短路径算法我们后面会讲到，所以我们今天讲下霍夫曼编码，看看它是如何利用贪心算法来实现对数据压缩编码，有效节省数据存储空间的。
如何理解“贪心算法”？关于贪心算法，我们先看一个例子。
假设我们有一个可以容纳100kg物品的背包，可以装各种物品。我们有以下5种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？
实际上，这个问题很简单，我估计你一下子就能想出来，没错，我们只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆，所以，我们可以往背包里装20kg黑豆、30kg绿豆、50kg红豆。
这个问题的解决思路显而易见，它本质上借助的就是贪心算法。结合这个例子，我总结一下贪心算法解决问题的步骤，我们一起来看看。
第一步，当我们看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。
类比到刚刚的例子，限制值就是重量不能超过100kg，期望值就是物品的总价值。这组数据就是5种豆子。我们从中选出一部分，满足重量不超过100kg，并且总价值最大。
第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。
类比到刚刚的例子，我们每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。
第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。
实际上，用贪心算法解决问题的思路，并不总能给出最优解。
我来举一个例子。在一个有权图中，我们从顶点S开始，找一条到顶点T的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点T。按照这种思路，我们求出的最短路径是S-&amp;gt;A-&amp;gt;E-&amp;gt;T，路径长度是1+4+4=9。
但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径S-&amp;gt;B-&amp;gt;D-&amp;gt;T才是最短路径，因为这条路径的长度是2+2+2=6。为什么贪心算法在这个问题上不工作了呢？
在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果我们第一步从顶点S走到顶点A，那接下来面对的顶点和边，跟第一步从顶点S走到顶点B，是完全不同的。所以，即便我们第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。
贪心算法实战分析对于贪心算法，你是不是还有点懵？如果死抠理论的话，确实很难理解透彻。掌握贪心算法的关键是多练习。只要多练习几道题，自然就有感觉了。所以，我带着你分析几个具体的例子，帮助你深入理解贪心算法。
1.分糖果我们有m个糖果和n个孩子。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m&amp;lt;n），所以糖果只能分配给一部分孩子。
每个糖果的大小不等，这m个糖果的大小分别是s1，s2，s3，……，sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这n个孩子对糖果大小的需求分别是g1，g2，g3，……，gn。
我的问题是，如何分配糖果，能尽可能满足最多数量的孩子？
我们可以把这个问题抽象成，从n个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数（期望值）是最大的。这个问题的限制值就是糖果个数m。
我们现在来看看如何用贪心算法来解决。对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。
我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。
2.钱币找零这个问题在我们的日常生活中更加普遍。假设我们有1元、2元、5元、10元、20元、50元、100元这些面额的纸币，它们的张数分别是c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付K元，最少要用多少张纸币呢？
在生活中，我们肯定是先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用1元来补齐。
在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。直觉告诉我们，这种处理方法就是最好的。实际上，要严谨地证明这种贪心算法的正确性，需要比较复杂的、有技巧的数学推导，我不建议你花太多时间在上面，不过如果感兴趣的话，可以自己去研究下。
3.区间覆盖假设我们有n个区间，区间的起始端点和结束端点分别是[l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。我们从这n个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？
这个问题的处理思路稍微不是那么好懂，不过，我建议你最好能弄懂，因为这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。
这个问题的解决思路是这样的：我们假设这n个区间中最左端点是lmin，最右端点是rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将[lmin, rmax]覆盖上。我们按照起始端点从小到大的顺序对这n个区间排序。
我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。
解答开篇今天的内容就讲完了，我们现在来看开篇的问题，如何用贪心算法实现霍夫曼编码？
假设我有一个包含1000个字符的文件，每个字符占1个byte（1byte=8bits），存储这1000个字符就一共需要8000bits，那有没有更加节省空间的存储方式呢？
假设我们通过统计分析发现，这1000个字符中只包含6种不同字符，假设它们分别是a、b、c、d、e、f。而3个二进制位（bit）就可以表示8个不同的字符，所以，为了尽量减少存储空间，每个字符我们用3个二进制位来表示。那存储这1000个字符只需要3000bits就可以了，比原来的存储方式节省了很多空间。不过，还有没有更加节省空间的存储方式呢？
a(000)、b(001)、c(010)、d(011)、e(100)、f(101) 霍夫曼编码就要登场了。霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在20%～90%之间。
霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。
对于等长的编码来说，我们解压缩起来很简单。比如刚才那个例子中，我们用3个bit表示一个字符。在解压缩的时候，我们每次从文本中读取3位二进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取1位还是2位、3位等等来解压缩呢？这个问题就导致霍夫曼编码解压缩起来比较复杂。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。
假设这6个字符出现的频率从高到低依次是a、b、c、d、e、f。我们把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀，在解压缩的时候，我们每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这1000个字符只需要2100bits就可以了。
尽管霍夫曼编码的思想并不难理解，但是如何根据字符出现频率的不同，给不同的字符进行不同长度的编码呢？这里的处理稍微有些技巧。
我们把每个字符看作一个节点，并且附带着把频率放到优先级队列中。我们从队列中取出频率最小的两个节点A、B，然后新建一个节点C，把频率设置为两个节点的频率之和，并把这个新节点C作为节点A、B的父节点。最后再把C节点放入到优先级队列中。重复这个过程，直到队列中没有数据。
现在，我们给每一条边加上画一个权值，指向左子节点的边我们统统标记为0，指向右子节点的边，我们统统标记为1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。
内容小结今天我们学习了贪心算法。
实际上，贪心算法适用的场景比较有限。这种算法思想更多的是指导设计基础算法。比如最小生成树算法、单源最短路径算法，这些算法都用到了贪心算法。从我个人的学习经验来讲，不要刻意去记忆贪心算法的原理，多练习才是最有效的学习方法。
贪心算法的最难的一块是如何将要解决的问题抽象成贪心算法模型，只要这一步搞定之后，贪心算法的编码一般都很简单。贪心算法解决问题的正确性虽然很多时候都看起来是显而易见的，但是要严谨地证明算法能够得到最优解，并不是件容易的事。所以，很多时候，我们只需要多举几个例子，看一下贪心算法的解决方案是否真的能得到最优解就可以了。
课后思考 在一个非负整数a中，我们希望从中移除k个数字，让剩下的数字值最小，如何选择移除哪k个数字呢？
假设有n个人等待被服务，但是服务窗口只有一个，每个人需要被服务的时间长度是不同的，如何安排被服务的先后顺序，才能让这n个人总的等待时间最短？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>37_高速缓存（上）：“4毫秒”究竟值多少钱？</title><link>https://artisanbox.github.io/4/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/37/</guid><description>在这一节内容开始之前，我们先来看一个3行的小程序。你可以猜一猜，这个程序里的循环1和循环2，运行所花费的时间会差多少？你可以先思考几分钟，然后再看我下面的解释。
int[] arr = new int[64 * 1024 * 1024]; // 循环1 for (int i = 0; i &amp;lt; arr.length; i++) arr[i] *= 3;
// 循环2 for (int i = 0; i &amp;lt; arr.length; i += 16) arr[i] *= 3 在这段Java程序中，我们首先构造了一个64×1024×1024大小的整型数组。在循环1里，我们遍历整个数组，将数组中每一项的值变成了原来的3倍；在循环2里，我们每隔16个索引访问一个数组元素，将这一项的值变成了原来的3倍。
按道理来说，循环2只访问循环1中1/16的数组元素，只进行了循环1中1/16的乘法计算，那循环2花费的时间应该是循环1的1/16左右。但是实际上，循环1在我的电脑上运行需要50毫秒，循环2只需要46毫秒。这两个循环花费时间之差在15%之内。
为什么会有这15%的差异呢？这和我们今天要讲的CPU Cache有关。之前我们看到了内存和硬盘之间存在的巨大性能差异。在CPU眼里，内存也慢得不行。于是，聪明的工程师们就在CPU里面嵌入了CPU Cache（高速缓存），来解决这一问题。
我们为什么需要高速缓存?按照摩尔定律，CPU的访问速度每18个月便会翻一番，相当于每年增长60%。内存的访问速度虽然也在不断增长，却远没有这么快，每年只增长7%左右。而这两个增长速度的差异，使得CPU性能和内存访问性能的差距不断拉大。到今天来看，一次内存的访问，大约需要120个CPU Cycle，这也意味着，在今天，CPU和内存的访问速度已经有了120倍的差距。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;如果拿我们现实生活来打个比方的话，CPU的速度好比风驰电掣的高铁，每小时350公里，然而，它却只能等着旁边腿脚不太灵便的老太太，也就是内存，以每小时3公里的速度缓慢步行。因为CPU需要执行的指令、需要访问的数据，都在这个速度不到自己1%的内存里。
随着时间变迁，CPU和内存之间的性能差距越来越大为了弥补两者之间的性能差异，我们能真实地把CPU的性能提升用起来，而不是让它在那儿空转，我们在现代CPU中引入了高速缓存。
从CPU Cache被加入到现有的CPU里开始，内存中的指令、数据，会被加载到L1-L3 Cache中，而不是直接由CPU访问内存去拿。在95%的情况下，CPU都只需要访问L1-L3 Cache，从里面读取指令和数据，而无需访问内存。要注意的是，这里我们说的CPU Cache或者L1/L3 Cache，不是一个单纯的、概念上的缓存（比如之前我们说的拿内存作为硬盘的缓存），而是指特定的由SRAM组成的物理芯片。
这里是一张Intel CPU的放大照片。这里面大片的长方形芯片，就是这个CPU使用的20MB的L3 Cache。
现代CPU中大量的空间已经被SRAM占据，图中用红色框出的部分就是CPU的L3 Cache芯片在这一讲一开始的程序里，运行程序的时间主要花在了将对应的数据从内存中读取出来，加载到CPU Cache里。CPU从内存中读取数据到CPU Cache的过程中，是一小块一小块来读取数据的，而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在CPU Cache里面，我们把它叫作Cache Line（缓存块）。
在我们日常使用的Intel服务器或者PC里，Cache Line的大小通常是64字节。而在上面的循环2里面，我们每隔16个整型数计算一次，16个整型数正好是64个字节。于是，循环1和循环2，需要把同样数量的Cache Line数据从内存中读取到CPU Cache中，最终两个程序花费的时间就差别不大了。</description></item><item><title>38_分治算法：谈一谈大规模计算框架MapReduce中的分治思想</title><link>https://artisanbox.github.io/2/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/39/</guid><description>MapReduce是Google大数据处理的三驾马车之一，另外两个是GFS和Bigtable。它在倒排索引、PageRank计算、网页分析等搜索引擎相关的技术中都有大量的应用。
尽管开发一个MapReduce看起来很高深，感觉跟我们遥不可及。实际上，万变不离其宗，它的本质就是我们今天要学的这种算法思想，分治算法。
如何理解分治算法？为什么说MapRedue的本质就是分治算法呢？我们先来看，什么是分治算法？
分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成n个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。
这个定义看起来有点类似递归的定义。关于分治和递归的区别，我们在排序（下）的时候讲过，分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：
分解：将原问题分解成一系列子问题；
解决：递归地求解各个子问题，若子问题足够小，则直接求解；
合并：将子问题的结果合并成原问题。
分治算法能解决的问题，一般需要满足下面这几个条件：
原问题与分解成的小问题具有相同的模式；
原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；
具有分解终止条件，也就是说，当问题足够小时，可以直接求解；
可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。
分治算法应用举例分析理解分治算法的原理并不难，但是要想灵活应用并不容易。所以，接下来，我会带你用分治算法解决我们在讲排序的时候涉及的一个问题，加深你对分治算法的理解。
还记得我们在排序算法里讲的数据的有序度、逆序度的概念吗？我当时讲到，我们用有序度来表示一组数据的有序程度，用逆序度表示一组数据的无序程度。
假设我们有n个数据，我们期望数据从小到大排列，那完全有序的数据的有序度就是n(n-1)/2，逆序度等于0；相反，倒序排列的数据的有序度就是0，逆序度是n(n-1)/2。除了这两种极端情况外，我们通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。
我现在的问题是，如何编程求出一组数据的有序对个数或者逆序对个数呢？因为有序对个数和逆序对个数的求解方式是类似的，所以你可以只思考逆序对个数的求解方法。
最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的k值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是O(n^2)。那有没有更加高效的处理方法呢？
我们用分治算法来试试。我们套用分治的思想来求数组A的逆序对个数。我们可以将数组分成前后两半A1和A2，分别计算A1和A2的逆序对个数K1和K2，然后再计算A1与A2之间的逆序对个数K3。那数组A的逆序对个数就等于K1+K2+K3。
我们前面讲过，使用分治算法其中一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了。那回到这个问题，如何快速计算出两个子问题A1与A2之间的逆序对个数呢？
这里就要借助归并排序算法了。你可以先试着想想，如何借助归并排序算法来解决呢？
归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。实际上，在这个合并的过程中，我们就可以计算这两个小数组的逆序对个数了。每次合并操作，我们都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。
尽管我画了张图来解释，但是我个人觉得，对于工程师来说，看代码肯定更好理解一些，所以我们把这个过程翻译成了代码，你可以结合着图和文字描述一起看下。
private int num = 0; // 全局变量或者成员变量 public int count(int[] a, int n) { num = 0; mergeSortCounting(a, 0, n-1); return num; }
private void mergeSortCounting(int[] a, int p, int r) { if (p &amp;gt;= r) return; int q = (p+r)/2; mergeSortCounting(a, p, q); mergeSortCounting(a, q+1, r); merge(a, p, q, r); }</description></item><item><title>38_都说InnoDB好，那还要不要使用Memory引擎？</title><link>https://artisanbox.github.io/1/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/38/</guid><description>我在上一篇文章末尾留给你的问题是：两个group by 语句都用了order by null，为什么使用内存临时表得到的语句结果里，0这个值在最后一行；而使用磁盘临时表得到的结果里，0这个值在第一行？
今天我们就来看看，出现这个问题的原因吧。
内存表的数据组织结构为了便于分析，我来把这个问题简化一下，假设有以下的两张表t1 和 t2，其中表t1使用Memory 引擎， 表t2使用InnoDB引擎。
create table t1(id int primary key, c int) engine=Memory; create table t2(id int primary key, c int) engine=innodb; insert into t1 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); insert into t2 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); 然后，我分别执行select * from t1和select * from t2。
图1 两个查询结果-0的位置可以看到，内存表t1的返回结果里面0在最后一行，而InnoDB表t2的返回结果里0在第一行。
出现这个区别的原因，要从这两个引擎的主键索引的组织方式说起。
表t2用的是InnoDB引擎，它的主键索引id的组织方式，你已经很熟悉了：InnoDB表的数据就放在主键索引树上，主键索引是B+树。所以表t2的数据组织方式如下图所示：
图2 表t2的数据组织主键索引上的值是有序存储的。在执行select *的时候，就会按照叶子节点从左到右扫描，所以得到的结果里，0就出现在第一行。
与InnoDB引擎不同，Memory引擎的数据和索引是分开的。我们来看一下表t1中的数据内容。
图3 表t1 的数据组织可以看到，内存表的数据部分以数组的方式单独存放，而主键id索引里，存的是每个数据的位置。主键id是hash索引，可以看到索引上的key并不是有序的。
在内存表t1中，当我执行select *的时候，走的是全表扫描，也就是顺序扫描这个数组。因此，0就是最后一个被读到，并放入结果集的数据。
可见，InnoDB和Memory引擎的数据组织方式是不同的：
InnoDB引擎把数据放在主键索引上，其他索引上保存的是主键id。这种方式，我们称之为索引组织表（Index Organizied Table）。 而Memory引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为堆组织表（Heap Organizied Table）。 从中我们可以看出，这两个引擎的一些典型不同：
InnoDB表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的；</description></item><item><title>38_高速缓存（下）：你确定你的数据更新了么？</title><link>https://artisanbox.github.io/4/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/38/</guid><description>在我工作的十几年里，写了很多Java的程序。同时，我也面试过大量的Java工程师。对于一些表示自己深入了解和擅长多线程的同学，我经常会问这样一个面试题：“volatile这个关键字有什么作用？”如果你或者你的朋友写过Java程序，不妨来一起试着回答一下这个问题。
就我面试过的工程师而言，即使是工作了多年的Java工程师，也很少有人能准确说出volatile这个关键字的含义。这里面最常见的理解错误有两个，一个是把volatile当成一种锁机制，认为给变量加上了volatile，就好像是给函数加了sychronized关键字一样，不同的线程对于特定变量的访问会去加锁；另一个是把volatile当成一种原子化的操作机制，认为加了volatile之后，对于一个变量的自增的操作就会变成原子性的了。
// 一种错误的理解，是把volatile关键词，当成是一个锁，可以把long/double这样的数的操作自动加锁 private volatile long synchronizedValue = 0; // 另一种错误的理解，是把volatile关键词，当成可以让整数自增的操作也变成原子性的 private volatile int atomicInt = 0; amoticInt++; 事实上，这两种理解都是完全错误的。很多工程师容易把volatile关键字，当成和锁或者数据数据原子性相关的知识点。而实际上，volatile关键字的最核心知识点，要关系到Java内存模型（JMM，Java Memory Model）上。
虽然JMM只是Java虚拟机这个进程级虚拟机里的一个内存模型，但是这个内存模型，和计算机组成里的CPU、高速缓存和主内存组合在一起的硬件体系非常相似。理解了JMM，可以让你很容易理解计算机组成里CPU、高速缓存和主内存之间的关系。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;“隐身”的变量我们先来一起看一段Java程序。这是一段经典的volatile代码，来自知名的Java开发者网站dzone.com，后续我们会修改这段代码来进行各种小实验。
public class VolatileTest { private static volatile int COUNTER = 0;
public static void main(String[] args) { new ChangeListener().start(); new ChangeMaker().start(); } static class ChangeListener extends Thread { @Override public void run() { int threadValue = COUNTER; while ( threadValue &amp;amp;lt; 5){ if( threadValue!</description></item><item><title>38｜中端优化第1关：实现多种本地优化</title><link>https://artisanbox.github.io/3/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/40/</guid><description>你好，我是宫文学。
上一节课，我们设计了IR的数据结构，并且分析了如何从AST生成IR。并且，这些IR还可以生成.dot文件，以直观的图形化的方式显示出来。
不过，我们上一节课只分析了if语句，这还远远不够。这节课，我会先带你分析for循环语句，加深对你控制流和数据流的理解。接着，我们就会开始享受这个IR带来的红利，用它来完成一些基本的本地优化工作，包括公共子表达式删除、拷贝传播和死代码删除，让你初步体会基于IR做优化的感觉。
那么，我们先接着上一节课，继续把for循环从AST转换成IR。
把For循环转换成IR同样地，我们还是借助一个例子来做分析。这个例子是一个实现累加功能的函数，bar函数接受一个参数a，然后返回从1到a的累加值。
function bar(a:number):number{ let sum:number = 0; for(let i = 1; i &amp;lt;= a; i++){ sum = sum + i; } return sum; } 这里，我先直接画出最后生成的IR图的样子：
你一看这个图，肯定会觉得有点眼花缭乱，摸不清头绪。不过没关系，这里面是有着清晰的逻辑的。
第一步，我们先来看控制流的部分。
在程序开头的时候，依然还是一个Start节点。
而下面的LoopBegin节点，则代表了整个for循环语句的开始。开始后，它会根据for循环的条件，确定是否进入循环体。这里，我们引入了一个If节点，来代表循环条件。If节点要依据一个if条件，所以这里有一条黑线指向一个条件表达式节点。
当循环条件为true的时候，程序就进入循环体。循环体以Begin开头，以LoopEnd结尾。而当循环条件为false的时候，程序则要通过LoopExit来退出循环。最后再通过Return语句从函数中返回。
并且，LoopEnd和LoopExit各自都有一条输入边，连接到LoopBegin。这样，循环的开始和结束就能正确地配对，不至于搞混。
不过，你可能注意到了一个现象，Start节点的后序节点并不马上是循环的开始LoopBegin。为什么呢？因为其实有两条控制流能够到达LoopBegin：一条是从程序开始的上方进去，另一条是在每次循环结束以后，又重新开始循环。所以LoopBegin相当于我们上一节见过的Merge节点，两条控制流在这里汇聚。而我们在控制流中，如果用一条蓝线往下连接其他节点，只适用于单一控制流和流程分叉的情况，不包括流程汇聚的情况。我们上节课也说过，每个ControlNode最多只有一个前序节点。
那控制流的部分就说清楚了。第二步，我们就来看一下数据流。
在数据流中，我们需要计算i和sum这两个变量。我们先看i：
function bar(a:number):number{ let sum1:number = 0; for(let i1 = 1; i &amp;lt;= a; i2 = i + 1){ sum2 = sum + i; } return sum; } 这里，变量i被静态赋值了两次。一开始被赋值为1，后来又通过i++来递增。为了符合SSA格式，我们要把它拆分成i1和i2两个变量，然后再用Phi节点把它们聚合起来，用于循环条件的判断。
我们把与i有关的数据流加入到图中，就是下面这样：
我再解释一下这张图。i1=1这个表达式，在刚进入循环时被触发，一次循环结束后，会触发i2 = i + 1。所以，在i&amp;lt;=a这个条件中的i，在刚进入循环的时候，会选择i1；而在循环体中循环过一次以后，会选择i2。因此，我们图中这个phi节点有一条输入边指向LoopBegin，用于判断控制流到底是从上面那条边进入的，还是从LoopEnd返回的。</description></item><item><title>39_MESI协议：如何让多核CPU的高速缓存保持一致？</title><link>https://artisanbox.github.io/4/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/39/</guid><description>你平时用的电脑，应该都是多核的CPU。多核CPU有很多好处，其中最重要的一个就是，它使得我们在不能提升CPU的主频之后，找到了另一种提升CPU吞吐率的办法。
不知道上一讲的内容你还记得多少？上一节，我们讲到，多核CPU里的每一个CPU核，都有独立的属于自己的L1 Cache和L2 Cache。多个CPU之间，只是共用L3 Cache和主内存。
我们说，CPU Cache解决的是内存访问速度和CPU的速度差距太大的问题。而多核CPU提供的是，在主频难以提升的时候，通过增加CPU核心来提升CPU的吞吐率的办法。我们把多核和CPU Cache两者一结合，就给我们带来了一个新的挑战。因为CPU的每个核各有各的缓存，互相之间的操作又是各自独立的，就会带来缓存一致性（Cache Coherence）的问题。
缓存一致性问题那什么是缓存一致性呢？我们拿一个有两个核心的CPU，来看一下。你可以看这里这张图，我们结合图来说。
在这两个CPU核心里，1号核心要写一个数据到内存里。这个怎么理解呢？我拿一个例子来给你解释。
比方说，iPhone降价了，我们要把iPhone最新的价格更新到内存里。为了性能问题，它采用了上一讲我们说的写回策略，先把数据写入到L2 Cache里面，然后把Cache Block标记成脏的。这个时候，数据其实并没有被同步到L3 Cache或者主内存里。1号核心希望在这个Cache Block要被交换出去的时候，数据才写入到主内存里。
如果我们的CPU只有1号核心这一个CPU核，那这其实是没有问题的。不过，我们旁边还有一个2号核心呢！这个时候，2号核心尝试从内存里面去读取iPhone的价格，结果读到的是一个错误的价格。这是因为，iPhone的价格刚刚被1号核心更新过。但是这个更新的信息，只出现在1号核心的L2 Cache里，而没有出现在2号核心的L2 Cache或者主内存里面。这个问题，就是所谓的缓存一致性问题，1号核心和2号核心的缓存，在这个时候是不一致的。
为了解决这个缓存不一致的问题，我们就需要有一种机制，来同步两个不同核心里面的缓存数据。那这样的机制需要满足什么条件呢？我觉得能够做到下面两点就是合理的。
第一点叫写传播（Write Propagation）。写传播是说，在一个CPU核心里，我们的Cache数据更新，必须能够传播到其他的对应节点的Cache Line里。
第二点叫事务的串行化（Transaction Serialization），事务串行化是说，我们在一个CPU核心里面的读取和写入，在其他的节点看起来，顺序是一样的。
第一点写传播很容易理解。既然我们数据写完了，自然要同步到其他CPU核的Cache里。但是第二点事务的串行化，可能没那么好理解，我这里仔细解释一下。
我们还拿刚才修改iPhone的价格来解释。这一次，我们找一个有4个核心的CPU。1号核心呢，先把iPhone的价格改成了5000块。差不多在同一个时间，2号核心把iPhone的价格改成了6000块。这里两个修改，都会传播到3号核心和4号核心。
然而这里有个问题，3号核心先收到了2号核心的写传播，再收到1号核心的写传播。所以3号核心看到的iPhone价格是先变成了6000块，再变成了5000块。而4号核心呢，是反过来的，先看到变成了5000块，再变成6000块。虽然写传播是做到了，但是各个Cache里面的数据，是不一致的。
事实上，我们需要的是，从1号到4号核心，都能看到相同顺序的数据变化。比如说，都是先变成了5000块，再变成了6000块。这样，我们才能称之为实现了事务的串行化。
事务的串行化，不仅仅是缓存一致性中所必须的。比如，我们平时所用到的系统当中，最需要保障事务串行化的就是数据库。多个不同的连接去访问数据库的时候，我们必须保障事务的串行化，做不到事务的串行化的数据库，根本没法作为可靠的商业数据库来使用。
而在CPU Cache里做到事务串行化，需要做到两点，第一点是一个CPU核心对于数据的操作，需要同步通信给到其他CPU核心。第二点是，如果两个CPU核心里有同一个数据的Cache，那么对于这个Cache数据的更新，需要有一个“锁”的概念。只有拿到了对应Cache Block的“锁”之后，才能进行对应的数据更新。接下来，我们就看看实现了这两个机制的MESI协议。
总线嗅探机制和MESI协议要解决缓存一致性问题，首先要解决的是多个CPU核心之间的数据传播问题。最常见的一种解决方案呢，叫作总线嗅探（Bus Snooping）。这个名字听起来，你多半会很陌生，但是其实特很好理解。
这个策略，本质上就是把所有的读写请求都通过总线（Bus）广播给所有的CPU核心，然后让各个核心去“嗅探”这些请求，再根据本地的情况进行响应。
总线本身就是一个特别适合广播进行数据传输的机制，所以总线嗅探这个办法也是我们日常使用的Intel CPU进行缓存一致性处理的解决方案。关于总线这个知识点，我们会放在后面的I/O部分更深入地进行讲解，这里你只需要了解就可以了。
基于总线嗅探机制，其实还可以分成很多种不同的缓存一致性协议。不过其中最常用的，就是今天我们要讲的MESI协议。和很多现代的CPU技术一样，MESI协议也是在Pentium时代，被引入到Intel CPU中的。
MESI协议，是一种叫作写失效（Write Invalidate）的协议。在写失效协议里，只有一个CPU核心负责写入数据，其他的核心，只是同步读取到这个写入。在这个CPU核心写入Cache之后，它会去广播一个“失效”请求告诉所有其他的CPU核心。其他的CPU核心，只是去判断自己是否也有一个“失效”版本的Cache Block，然后把这个也标记成失效的就好了。
相对于写失效协议，还有一种叫作写广播（Write Broadcast）的协议。在那个协议里，一个写入请求广播到所有的CPU核心，同时更新各个核心里的Cache。
写广播在实现上自然很简单，但是写广播需要占用更多的总线带宽。写失效只需要告诉其他的CPU核心，哪一个内存地址的缓存失效了，但是写广播还需要把对应的数据传输给其他CPU核心。
MESI协议的由来呢，来自于我们对Cache Line的四个不同的标记，分别是：
M：代表已修改（Modified） E：代表独占（Exclusive） S：代表共享（Shared） I：代表已失效（Invalidated） 我们先来看看“已修改”和“已失效”，这两个状态比较容易理解。所谓的“已修改”，就是我们上一讲所说的“脏”的Cache Block。Cache Block里面的内容我们已经更新过了，但是还没有写回到主内存里面。而所谓的“已失效“，自然是这个Cache Block里面的数据已经失效了，我们不可以相信这个Cache Block里面的数据。
然后，我们再来看“独占”和“共享”这两个状态。这就是MESI协议的精华所在了。无论是独占状态还是共享状态，缓存里面的数据都是“干净”的。这个“干净”，自然对应的是前面所说的“脏”的，也就是说，这个时候，Cache Block里面的数据和主内存里面的数据是一致的。
那么“独占”和“共享”这两个状态的差别在哪里呢？这个差别就在于，在独占状态下，对应的Cache Line只加载到了当前CPU核所拥有的Cache里。其他的CPU核，并没有加载对应的数据到自己的Cache里。这个时候，如果要向独占的Cache Block写入数据，我们可以自由地写入数据，而不需要告知其他CPU核。
在独占状态下的数据，如果收到了一个来自于总线的读取对应缓存的请求，它就会变成共享状态。这个共享状态是因为，这个时候，另外一个CPU核心，也把对应的Cache Block，从内存里面加载到了自己的Cache里来。
而在共享状态下，因为同样的数据在多个CPU核心的Cache里都有。所以，当我们想要更新Cache里面的数据的时候，不能直接修改，而是要先向所有的其他CPU核心广播一个请求，要求先把其他CPU核心里面的Cache，都变成无效的状态，然后再更新当前Cache里面的数据。这个广播操作，一般叫作RFO（Request For Ownership），也就是获取当前对应Cache Block数据的所有权。</description></item><item><title>39_回溯算法：从电影《蝴蝶效应》中学习回溯算法的核心思想</title><link>https://artisanbox.github.io/2/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/40/</guid><description>我们在第31节提到，深度优先搜索算法利用的是回溯算法思想。这个算法思想非常简单，但是应用却非常广泛。它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。
除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1背包、图的着色、旅行商问题、全排列等等。既然应用如此广泛，我们今天就来学习一下这个算法思想，看看它是如何指导我们解决问题的。
如何理解“回溯算法”？在我们的一生中，会遇到很多重要的岔路口。在岔路口上，每个选择都会影响我们今后的人生。有的人在每个岔路口都能做出最正确的选择，最后生活、事业都达到了一个很高的高度；而有的人一路选错，最后碌碌无为。如果人生可以量化，那如何才能在岔路口做出最正确的选择，让自己的人生“最优”呢？
我们可以借助前面学过的贪心算法，在每次面对岔路口的时候，都做出看起来最优的选择，期望这一组选择可以使得我们的人生达到“最优”。但是，我们前面也讲过，贪心算法并不一定能得到最优解。那有没有什么办法能得到最优解呢？
2004年上映了一部非常著名的电影《蝴蝶效应》，讲的就是主人公为了达到自己的目标，一直通过回溯的方法，回到童年，在关键的岔路口，重新做选择。当然，这只是科幻电影，我们的人生是无法倒退的，但是这其中蕴含的思想其实就是回溯算法。
笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。
回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。
理论的东西还是过于抽象，老规矩，我还是举例说明一下。我举一个经典的回溯例子，我想你可能已经猜到了，那就是八皇后问题。
我们有一个8x8的棋盘，希望往里放8个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。你可以看我画的图，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。
我们把这个问题划分成8个阶段，依次将8个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前放法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种放法，继续尝试。
回溯算法非常适合用递归代码实现，所以，我把八皇后的算法翻译成代码。我在代码里添加了详细的注释，你可以对比着看下。如果你之前没有接触过八皇后问题，建议你自己用熟悉的编程语言实现一遍，这对你理解回溯思想非常有帮助。
int[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列 public void cal8queens(int row) { // 调用方式：cal8queens(0); if (row == 8) { // 8个棋子都放置好了，打印结果 printQueens(result); return; // 8行棋子都放好了，已经没法再往下递归了，所以就return } for (int column = 0; column &amp;lt; 8; ++column) { // 每一行都有8中放法 if (isOk(row, column)) { // 有些放法不满足要求 result[row] = column; // 第row行的棋子放到了column列 cal8queens(row+1); // 考察下一行 } } } private boolean isOk(int row, int column) {//判断row行column列放置是否合适 int leftup = column - 1, rightup = column + 1; for (int i = row-1; i &amp;gt;= 0; &amp;ndash;i) { // 逐行往上考察每一行 if (result[i] == column) return false; // 第i行的column列有棋子吗？ if (leftup &amp;gt;= 0) { // 考察左上对角线：第i行leftup列有棋子吗？ if (result[i] == leftup) return false; } if (rightup &amp;lt; 8) { // 考察右上对角线：第i行rightup列有棋子吗？ if (result[i] == rightup) return false; } &amp;ndash;leftup; ++rightup; } return true; }</description></item><item><title>39_自增主键为什么不是连续的？</title><link>https://artisanbox.github.io/1/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/39/</guid><description>在第4篇文章中，我们提到过自增主键，由于自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑。
之前我见过有的业务设计依赖于自增主键的连续性，也就是说，这个设计假设自增主键是连续的。但实际上，这样的假设是错的，因为自增主键不能保证连续递增。
今天这篇文章，我们就来说说这个问题，看看什么情况下自增主键会出现 “空洞”？
为了便于说明，我们创建一个表t，其中id是自增主键字段、c是唯一索引。
CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `c` (`c`) ) ENGINE=InnoDB; 自增值保存在哪儿？在这个空表t里面执行insert into t values(null, 1, 1);插入一行数据，再执行show create table命令，就可以看到如下图所示的结果：
图1 自动生成的AUTO_INCREMENT值可以看到，表定义里面出现了一个AUTO_INCREMENT=2，表示下一次插入数据时，如果需要自动生成自增值，会生成id=2。
其实，这个输出结果容易引起这样的误解：自增值是保存在表结构定义里的。实际上，表的结构定义存放在后缀名为.frm的文件中，但是并不会保存自增值。
不同的引擎对于自增值的保存策略不同。
MyISAM引擎的自增值保存在数据文件中。 InnoDB引擎的自增值，其实是保存在了内存里，并且到了MySQL 8.0版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为MySQL重启前的值”，具体情况是： 在MySQL 5.7及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值max(id)，然后将max(id)+1作为这个表当前的自增值。﻿
举例来说，如果一个表当前数据行里最大的id是10，AUTO_INCREMENT=11。这时候，我们删除id=10的行，AUTO_INCREMENT还是11。但如果马上重启实例，重启后这个表的AUTO_INCREMENT就会变成10。﻿
也就是说，MySQL重启可能会修改一个表的AUTO_INCREMENT的值。 在MySQL 8.0版本，将自增值的变更记录在了redo log中，重启的时候依靠redo log恢复重启之前的值。 理解了MySQL对自增值的保存策略以后，我们再看看自增值修改机制。
自增值修改机制在MySQL里面，如果字段id被定义为AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下：
如果插入数据时id字段指定为0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT值填到自增字段；
如果插入数据时id字段指定了具体的值，就直接使用语句里指定的值。
根据要插入的值和当前自增值的大小关系，自增值的变更结果也会有所不同。假设，某次要插入的值是X，当前的自增值是Y。</description></item><item><title>39｜中端优化第2关：全局优化要怎么搞？</title><link>https://artisanbox.github.io/3/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/41/</guid><description>你好，我是宫文学。
上一节课，我们用了一些例子，讨论了如何用基于图的IR来实现一些优化，包括公共子表达式删除、拷贝传播和死代码删除。但这些例子，都属于本地优化的场景。也就是说，在未来生成的汇编代码中，这些代码其实都位于同一个基本块。
不过，复杂一点的程序，都会有if语句和循环语句这种流程控制语句，所以程序就会存在多个基本块。那么就会存在跨越多个基本块的优化工作，也就是全局优化。
所以，今天这节课，我们就来讨论一下如何基于当前的IR做全局优化。同时，为了达到优化效果，我们这一节课还需要把浮动的数据节点划分到具体的基本块中去，实现指令的调度。
但在讨论全局优化的场景之前，我还要先给你补充一块知识点，就是变量的版本和控制流的关系，让你能更好地理解全局优化。
变量的版本和控制流的关系通过前几节课我们已经知道，我们的IR生成算法能够对一个变量产生多个版本的定义，从而让IR符合SSA格式。可是，我们是如何来表示不同版本的定义的，又是如何确定程序中到底引用的是变量的哪个版本呢？
在IR的模型中，我引入了一个VarProxy类，来引用变量的一个版本，就像d0、d1和d2，也有的文献把变量的一个定义叫做变量的一个定值。VarProxy里面保存了一个VarSymbol，还包括了一个下标：
//代表了变量的一次定义。每次变量重新定义，都会生成一个新的Proxy，以便让IR符合SSA格式 class VarProxy{ varSym:VarSymbol; index:number; //变量的第几个定义 constructor(varSym:VarSymbol, index:number){ this.varSym = varSym; this.index = index; } get label():string{ return this.varSym.name+this.index; } } 每次遇到变量声明、变量赋值，以及像i++这样能够导致变量值改变的语句时，我们就会产生一个新的变量定义，也就是一个VarProxy。这个VarProxy会被绑定到一个具体的DataNode上。所以，我在IR中显示DataNode节点的时候，也会把绑定在这个节点上的变量定义一并显示出来。
那当我们在程序中遇到一个变量的时候，如何确定它采用的是哪个版本呢？
这就需要我们在生成IR的过程中，把VarProxy与当前的控制流绑定。每个控制流针对每个变量，只有一个确定的版本。
//把每个变量绑定到控制流，从而知道当前代码用到的是变量的哪个定义 //在同一个控制流里，如果有多个定义，则后面的定义会替换掉前面的。 varProxyMap:Map&amp;lt;AbstractBeginNode,Map&amp;lt;VarSymbol,VarProxy&amp;gt;&amp;gt; = new Map(); 在这里，我们还用了一个AbstractBeginNode节点来标识一个控制流。因为每个控制流都是存在一个起点的。而每个控制流节点，透过它的predecessor链，总能找到自己这条控制流的开始节点。
//获取这条控制流的开头节点 get beginNode():AbstractBeginNode{ if (this instanceof AbstractBeginNode){ return this; } else{ return (this.predecessor as UniSuccessorNode).beginNode; } } 但是，如果变量不是在当前控制流中定义的，而是在前面的控制流中定义的，那我们可以递归地往前查找。这里具体的实现，你可以参考一下getVarProxyFromFlow()。
最后，如果控制流的起点是一个merge节点，那这个变量就可能是在分支语句中定义的，那我们就要生成一个Phi节点，并把这个Phi节点也看成是变量定义的一个版本，方便我们在后续程序中引用。
好了，相信现在你已经可以更清晰地理解变量版本与控制流之间的关系了。现在我们基于这些前置知识，就可以开始讨论全局优化的场景了。
全局的死代码删除上一节课，我们实现了基本块中的死代码删除功能。那个时候，我们基本上只需要考虑数据流的特点，把uses属性为空的节点删除掉就行了。因为这些节点对应的变量定义没有被引用，所以它们就是死代码。
那么，现在考虑带有程序分支的情况，会怎么样呢？
我们还是通过一个例子来分析一下。你可以先停下来两分钟，用肉眼看一下，看看哪些代码可以删除：
function deadCode2(b:number,c:number){ let a:number = b+c; let d:number; let y:number; if (b &amp;gt; 0){ b = a+b; d = a+b; } else{ d = a+c; y = b+d; } let x = a+b; y = c + d; return x; } 我也把答案写出来了，看看跟你想的是否一样。在整个代码优化完毕以后，其实只剩下很少的代码了。变量c、d和y的定义都被优化掉了。</description></item><item><title>40_insert语句的锁为什么这么多？</title><link>https://artisanbox.github.io/1/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/40/</guid><description>在上一篇文章中，我提到MySQL对自增主键锁做了优化，尽量在申请到自增id以后，就释放自增锁。
因此，insert语句是一个很轻量的操作。不过，这个结论对于“普通的insert语句”才有效。也就是说，还有些insert语句是属于“特殊情况”的，在执行过程中需要给其他资源加锁，或者无法在申请到自增id以后就立马释放自增锁。
那么，今天这篇文章，我们就一起来聊聊这个话题。
insert … select 语句我们先从昨天的问题说起吧。表t和t2的表结构、初始化数据语句如下，今天的例子我们还是针对这两个表展开。
CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(null, 1,1); insert into t values(null, 2,2); insert into t values(null, 3,3); insert into t values(null, 4,4);
create table t2 like t 现在，我们一起来看看为什么在可重复读隔离级别下，binlog_format=statement时执行：
insert into t2(c,d) select c,d from t; 这个语句时，需要对表t的所有行和间隙加锁呢？
其实，这个问题我们需要考虑的还是日志和数据的一致性。我们看下这个执行序列：
图1 并发insert场景实际的执行效果是，如果session B先执行，由于这个语句对表t主键索引加了(-∞,1]这个next-key lock，会在语句执行完成后，才允许session A的insert语句执行。</description></item><item><title>40_初识动态规划：如何巧妙解决“双十一”购物时的凑单问题？</title><link>https://artisanbox.github.io/2/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/41/</guid><description>淘宝的“双十一”购物节有各种促销活动，比如“满200元减50元”。假设你女朋友的购物车中有n个（n&amp;gt;100）想买的商品，她希望从里面选几个，在凑够满减条件的前提下，让选出来的商品价格总和最大程度地接近满减条件（200元），这样就可以极大限度地“薅羊毛”。作为程序员的你，能不能编个代码来帮她搞定呢？
要想高效地解决这个问题，就要用到我们今天讲的动态规划（Dynamic Programming）。
动态规划学习路线动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。不过，它也是出了名的难学。它的主要学习难点跟递归类似，那就是，求解问题的过程不太符合人类常规的思维方式。对于新手来说，要想入门确实不容易。不过，等你掌握了之后，你会发现，实际上并没有想象中那么难。
为了让你更容易理解动态规划，我分了三节给你讲解。这三节分别是，初识动态规划、动态规划理论、动态规划实战。
第一节，我会通过两个非常经典的动态规划问题模型，向你展示我们为什么需要动态规划，以及动态规划解题方法是如何演化出来的。实际上，你只要掌握了这两个例子的解决思路，对于其他很多动态规划问题，你都可以套用类似的思路来解决。
第二节，我会总结动态规划适合解决的问题的特征，以及动态规划解题思路。除此之外，我还会将贪心、分治、回溯、动态规划这四种算法思想放在一起，对比分析它们各自的特点以及适用的场景。
第三节，我会教你应用第二节讲的动态规划理论知识，实战解决三个非常经典的动态规划问题，加深你对理论的理解。弄懂了这三节中的例子，对于动态规划这个知识点，你就算是入门了。
0-1背包问题我在讲贪心算法、回溯算法的时候，多次讲到背包问题。今天，我们依旧拿这个问题来举例。
对于一组不同重量、不可分割的物品，我们需要选择一些装入背包，在满足背包最大重量限制的前提下，背包中物品总重量的最大值是多少呢？
关于这个问题，我们上一节讲了回溯的解决方法，也就是穷举搜索所有可能的装法，然后找出满足条件的最大值。不过，回溯算法的复杂度比较高，是指数级别的。那有没有什么规律，可以有效降低时间复杂度呢？我们一起来看看。
// 回溯算法实现。注意：我把输入的变量都定义成了成员变量。 private int maxW = Integer.MIN_VALUE; // 结果放到maxW中 private int[] weight = {2，2，4，6，3}; // 物品重量 private int n = 5; // 物品个数 private int w = 9; // 背包承受的最大重量 public void f(int i, int cw) { // 调用f(0, 0) if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了 if (cw &amp;gt; maxW) maxW = cw; return; } f(i+1, cw); // 选择不装第i个物品 if (cw + weight[i] &amp;lt;= w) { f(i+1,cw + weight[i]); // 选择装第i个物品 } } 规律是不是不好找？那我们就举个例子、画个图看看。我们假设背包的最大承载重量是9。我们有5个不同的物品，每个物品的重量分别是2，2，4，6，3。如果我们把这个例子的回溯求解过程，用递归树画出来，就是下面这个样子：</description></item><item><title>40_理解内存（上）：虚拟内存和内存保护是什么？</title><link>https://artisanbox.github.io/4/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/40/</guid><description>我们在专栏一开始说过，计算机有五大组成部分，分别是：运算器、控制器、存储器、输入设备和输出设备。如果说计算机最重要的组件，是承担了运算器和控制器作用的CPU，那内存就是我们第二重要的组件了。内存是五大组成部分里面的存储器，我们的指令和数据，都需要先加载到内存里面，才会被CPU拿去执行。
专栏第9讲，我们讲了程序装载到内存的过程。可以知道，在我们日常使用的Linux或者Windows操作系统下，程序并不能直接访问物理内存。
我们的内存需要被分成固定大小的页（Page），然后再通过虚拟内存地址（Virtual Address）到物理内存地址（Physical Address）的地址转换（Address Translation），才能到达实际存放数据的物理内存位置。而我们的程序看到的内存地址，都是虚拟内存地址。
既然如此，这些虚拟内存地址究竟是怎么转换成物理内存地址的呢？这一讲里，我们就来看一看。
简单页表想要把虚拟内存地址，映射到物理内存地址，最直观的办法，就是来建一张映射表。这个映射表，能够实现虚拟内存里面的页，到物理内存里面的页的一一映射。这个映射表，在计算机里面，就叫作页表（Page Table）。
页表这个地址转换的办法，会把一个内存地址分成页号（Directory）和偏移量（Offset）两个部分。这么说太理论了，我以一个32位的内存地址为例，帮你理解这个概念。
其实，前面的高位，就是内存地址的页号。后面的低位，就是内存地址里面的偏移量。做地址转换的页表，只需要保留虚拟内存地址的页号和物理内存地址的页号之间的映射关系就可以了。同一个页里面的内存，在物理层面是连续的。以一个页的大小是4K字节（4KB）为例，我们需要20位的高位，12位的低位。
总结一下，对于一个内存地址转换，其实就是这样三个步骤：
把虚拟内存地址，切分成页号和偏移量的组合； 从页表里面，查询出虚拟页号，对应的物理页号； 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。 看起来这个逻辑似乎很简单，很容易理解，不过问题马上就来了。你能算一算，这样一个页表需要多大的空间吗？我们以32位的内存地址空间为例，你可以暂停一下，拿出纸笔算一算。
不知道你算出的数字是多少？32位的内存地址空间，页表一共需要记录2^20个到物理页号的映射关系。这个存储关系，就好比一个2^20大小的数组。一个页号是完整的32位的4字节（Byte），这样一个页表就需要4MB的空间。听起来4MB的空间好像还不大啊，毕竟我们现在的内存至少也有4GB，服务器上有个几十GB的内存和很正常。
不过，这个空间可不是只占用一份哦。我们每一个进程，都有属于自己独立的虚拟内存地址空间。这也就意味着，每一个进程都需要这样一个页表。不管我们这个进程，是个本身只有几KB大小的程序，还是需要几GB的内存空间，都需要这样一个页表。如果你用的是Windows，你可以打开你自己电脑上的任务管理器看看，现在你的计算机里同时在跑多少个进程，用这样的方式，页表需要占用多大的内存。
这还只是32位的内存地址空间，现在大家用的内存，多半已经超过了4GB，也已经用上了64位的计算机和操作系统。这样的话，用上面这个数组的数据结构来保存页面，内存占用就更大了。那么，我们有没有什么更好的解决办法呢？你可以先仔细思考一下。
多级页表仔细想一想，我们其实没有必要存下这2^20个物理页表啊。大部分进程所占用的内存是有限的，需要的页也自然是很有限的。我们只需要去存那些用到的页之间的映射关系就好了。如果你对数据结构比较熟悉，你可能要说了，那我们是不是应该用哈希表（Hash Map）这样的数据结构呢？
很可惜你猜错了：）。在实践中，我们其实采用的是一种叫作多级页表（Multi-Level Page Table）的解决方案。这是为什么呢？为什么我们不用哈希表而用多级页表呢？别着急，听我慢慢跟你讲。
我们先来看一看，一个进程的内存地址空间是怎么分配的。在整个进程的内存地址空间，通常是“两头实、中间空”。在程序运行的时候，内存地址从顶部往下，不断分配占用的栈的空间。而堆的空间，内存地址则是从底部往上，是不断分配占用的。
所以，在一个实际的程序进程里面，虚拟内存占用的地址空间，通常是两段连续的空间。而不是完全散落的随机的内存地址。而多级页表，就特别适合这样的内存地址分布。
我们以一个4级的多级页表为例，来看一下。同样一个虚拟内存地址，偏移量的部分和上面简单页表一样不变，但是原先的页号部分，我们把它拆成四段，从高到低，分成4级到1级这样4个页表索引。
对应的，一个进程会有一个4级页表。我们先通过4级页表索引，找到4级页表里面对应的条目（Entry）。这个条目里存放的是一张3级页表所在的位置。4级页面里面的每一个条目，都对应着一张3级页表，所以我们可能有多张3级页表。
找到对应这张3级页表之后，我们用3级索引去找到对应的3级索引的条目。3级索引的条目再会指向一个2级页表。同样的，2级页表里我们可以用2级索引指向一个1级页表。
而最后一层的1级页表里面的条目，对应的数据内容就是物理页号了。在拿到了物理页号之后，我们同样可以用“页号+偏移量”的方式，来获取最终的物理内存地址。
我们可能有很多张1级页表、2级页表，乃至3级页表。但是，因为实际的虚拟内存空间通常是连续的，我们很可能只需要很少的2级页表，甚至只需要1张3级页表就够了。
事实上，多级页表就像一个多叉树的数据结构，所以我们常常称它为页表树（Page Table Tree）。因为虚拟内存地址分布的连续性，树的第一层节点的指针，很多就是空的，也就不需要有对应的子树了。所谓不需要子树，其实就是不需要对应的2级、3级的页表。找到最终的物理页号，就好像通过一个特定的访问路径，走到树最底层的叶子节点。
以这样的分成4级的多级页表来看，每一级如果都用5个比特表示。那么每一张某1级的页表，只需要2^5=32个条目。如果每个条目还是4个字节，那么一共需要128个字节。而一个1级索引表，对应32个4KB的也就是128KB的大小。一个填满的2级索引表，对应的就是32个1级索引表，也就是4MB的大小。
我们可以一起来测算一下，一个进程如果占用了8MB的内存空间，分成了2个4MB的连续空间。那么，它一共需要2个独立的、填满的2级索引表，也就意味着64个1级索引表，2个独立的3级索引表，1个4级索引表。一共需要69个索引表，每个128字节，大概就是9KB的空间。比起4MB来说，只有差不多1/500。
不过，多级页表虽然节约了我们的存储空间，却带来了时间上的开销，所以它其实是一个“以时间换空间”的策略。原本我们进行一次地址转换，只需要访问一次内存就能找到物理页号，算出物理内存地址。但是，用了4级页表，我们就需要访问4次内存，才能找到物理页号了。
我们在前面两讲讲过，内存访问其实比Cache要慢很多。我们本来只是要做一个简单的地址转换，反而是一下子要多访问好多次内存。对于这个时间层面的性能损失，我们有没有什么更好的解决办法呢？那请你一定要关注下一讲的内容哦！
总结延伸好了，这一讲的内容差不多了，我们来总结一下。
我们从最简单的进行虚拟页号一一映射的简单页表说起，仔细讲解了现在实际应用的多级页表。多级页表就像是一颗树。因为一个进程的内存地址相对集中和连续，所以采用这种页表树的方式，可以大大节省页表所需要的空间。而因为每个进程都需要一个独立的页表，这个空间的节省是非常可观的。
在优化页表的过程中，我们可以观察到，数组这样的紧凑的数据结构，以及树这样稀疏的数据结构，在时间复杂度和空间复杂度的差异。另外，纯粹理论软件的数据结构和硬件的设计也是高度相关的。
推荐阅读对于虚拟内存的知识点，你可以再深入读一读《计算机组成与设计：硬件/软件接口》的第5.7章节。如果你觉得还不过瘾，可以进一步去读一读《What Every Programmer Should Know About Memory》的第4部分，也就是Virtual Memory。
课后思考在实际的虚拟内存地址到物理内存地址的地址转换的过程里，我们没有采用哈希表，而是采用了多级页表的解决方案。你能想一想，使用多级页表，对于哈希表有哪些优点，又有哪些缺点吗？
欢迎留言和我分享你的想法，如果觉得有收获，你也可以把这篇文章分享给你的朋友，和他一起学习和进步。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>40｜中端优化第3关：一起来挑战过程间优化</title><link>https://artisanbox.github.io/3/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/42/</guid><description>你好，我是宫文学。
在前面两节课，我们分析了本地优化和全局优化的场景。我们发现，由于基于图IR的优点，也就是控制流和数据流之间耦合度比较低的这个特点，我们很多优化算法的实现都变得更简单了。
那么，对于过程间优化的场景，我们这个基于图IR是否也会带来类似的便利呢？
过程间优化（Inter-procedural Optimization）指的是跨越多个函数（或者叫做过程），对程序进行多方面的分析，包括过程间的控制流分析和数据流分析，从而找出可以优化的机会。
今天这节课，我们就来分析两种常用的过程间优化技术，也就是内联优化和全局的逃逸分析，让你能了解过程间优化的思路，也能明白如何基于我们的IR来实现这些优化。之后，我还会给你补充另一个优化技术方面的知识点，也就是规范化。
内联优化内联优化是最常见到的一个过程间优化场景，说的就是当一个函数调用一个子函数时，干脆把子函数的代码拷贝到调用者中，从而减少由于函数调用导致的开销。
特别是，如果调用者是在一个循环中调用子函数，那么由很多次循环累积而导致的性能开销是很大的。内联优化的优势在这时就会得到体现。
而在面向对象编程中，我们通常会写很多很简短的setter和getter方法，并且在程序里频繁调用。如果编译器能自动把这些短方法做内联优化，我们就可以放心大胆地写这些短方法，而不用担心由此导致的性能开销了。
现在我们就举一个非常简单的、可以做内联的例子看看。在这个示例中，inline函数是调用者，它调用了add函数。
//内联 function inline(x:number):number{ return add(x, x+1); } function add(x:number, y:number):number{ return x + y; } 显然，在编译inline函数的时候，我们没必要额外多产生一次对add函数的调用，而是把add函数内联进来就行了，形成下面这些优化后的代码：
//内联 function inline(x:number):number{ return x + (x+1); } 那要如何基于我们的IR实现内联优化呢？
首先，我们还是看看在没有优化以前，inline和add两个函数的IR：
在inline函数的IR里，你能发现两个新的节点：一个是Invoke节点，代表函数调用的控制流；另一个是CallTarget节点，代表函数调用的数据流。
而内联优化就是要把这两个IR图合并，形成一个大的IR。如下图所示：
具体来说，要实现上面的合并，我们需要完成两个任务：
首先，把inline函数中的函数调用的节点替换成add函数中的加法节点； 第二，将加法节点中的x和y两个形式参数，替换成inline函数里的两个实际参数。 总的来说，整个算法都是去做节点的替换和重新连接，思路还是很清晰的。
我们之前说过，编译器在做了一种优化以后，经常可以给其他优化制造机会。在这里，内联优化不仅仅减少了函数调用导致的开销，它还会导致一些其他优化。比如说，我们在Inline函数里调用add函数的时候，传入两个参数x和-x，如下面的示例代码：
//内联 function inline2(x:number):number{ return add(x, -x); } function add(x:number, y:number):number{ return x + y; } 那么内联之后，这里就相当于计算x+(-x)的值，那也能计算出一个常量0。至于如何把x+(-x)化简成0，我先留个悬念，你先自己思考一下，我们这节课后面会介绍到。
//内联 function inline(x:number):number{ return x + (-x); //常量0 } 再比如，我们在主函数里调用add的时候，传的参数是常量。那么内联以后，我们就可以进行常量传播和常量折叠的优化，在编译期就能计算出结果为5：</description></item><item><title>41_动态规划理论：一篇文章带你彻底搞懂最优子结构、无后效性和重复子问题</title><link>https://artisanbox.github.io/2/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/42/</guid><description>上一节，我通过两个非常经典的问题，向你展示了用动态规划解决问题的过程。现在你对动态规划应该有了一个初步的认识。
今天，我主要讲动态规划的一些理论知识。学完这节内容，可以帮你解决这样几个问题：什么样的问题可以用动态规划解决？解决动态规划问题的一般思考过程是什么样的？贪心、分治、回溯、动态规划这四种算法思想又有什么区别和联系？
理论的东西都比较抽象，不过你不用担心，我会结合具体的例子来讲解，争取让你这次就能真正理解这些知识点，也为后面的应用和实战做好准备。
“一个模型三个特征”理论讲解什么样的问题适合用动态规划来解决呢？换句话说，动态规划能解决的问题有什么规律可循呢？实际上，动态规划作为一个非常成熟的算法思想，很多人对此已经做了非常全面的总结。我把这部分理论总结为“一个模型三个特征”。
首先，我们来看，什么是“一个模型”？它指的是动态规划适合解决的问题的模型。我把这个模型定义为“多阶段决策最优解模型”。下面我具体来给你讲讲。
我们一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。
现在，我们再来看，什么是“三个特征”？它们分别是最优子结构、无后效性和重复子问题。这三个概念比较抽象，我来逐一详细解释一下。
1.最优子结构最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，后面阶段的状态可以通过前面阶段的状态推导出来。
2.无后效性无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。
3.重复子问题这个概念比较好理解。前面一节，我已经多次提过。如果用一句话概括一下，那就是，不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。
“一个模型三个特征”实例剖析“一个模型三个特征”这部分是理论知识，比较抽象，你看了之后可能还是有点懵，有种似懂非懂的感觉，没关系，这个很正常。接下来，我结合一个具体的动态规划问题，来给你详细解释。
假设我们有一个n乘以n的矩阵w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？
我们先看看，这个问题是否符合“一个模型”？
从(0, 0)走到(n-1, n-1)，总共要走2*(n-1)步，也就对应着2*(n-1)个阶段。每个阶段都有向右走或者向下走两种决策，并且每个阶段都会对应一个状态集合。
我们把状态定义为min_dist(i, j)，其中i表示行，j表示列。min_dist表达式的值表示从(0, 0)到达(i, j)的最短路径长度。所以，这个问题是一个多阶段决策最优解问题，符合动态规划的模型。
我们再来看，这个问题是否符合“三个特征”？
我们可以用回溯算法来解决这个问题。如果你自己写一下代码，画一下递归树，就会发现，递归树中有重复的节点。重复的节点表示，从左上角到节点对应的位置，有多种路线，这也能说明这个问题中存在重复子问题。
如果我们走到(i, j)这个位置，我们只能通过(i-1, j)，(i, j-1)这两个位置移动过来，也就是说，我们想要计算(i, j)位置对应的状态，只需要关心(i-1, j)，(i, j-1)两个位置对应的状态，并不关心棋子是通过什么样的路线到达这两个位置的。而且，我们仅仅允许往下和往右移动，不允许后退，所以，前面阶段的状态确定之后，不会被后面阶段的决策所改变，所以，这个问题符合“无后效性”这一特征。
刚刚定义状态的时候，我们把从起始位置(0, 0)到(i, j)的最小路径，记作min_dist(i, j)。因为我们只能往右或往下移动，所以，我们只有可能从(i, j-1)或者(i-1, j)两个位置到达(i, j)。也就是说，到达(i, j)的最短路径要么经过(i, j-1)，要么经过(i-1, j)，而且到达(i, j)的最短路径肯定包含到达这两个位置的最短路径之一。换句话说就是，min_dist(i, j)可以通过min_dist(i, j-1)和min_dist(i-1, j)两个状态推导出来。这就说明，这个问题符合“最优子结构”。
min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j)) 两种动态规划解题思路总结刚刚我讲了，如何鉴别一个问题是否可以用动态规划来解决。现在，我再总结一下，动态规划解题的一般思路，让你面对动态规划问题的时候，能够有章可循，不至于束手无策。
我个人觉得，解决动态规划问题，一般有两种思路。我把它们分别叫作，状态转移表法和状态转移方程法。
1.状态转移表法一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。所以，当我们拿到问题的时候，我们可以先用简单的回溯算法解决，然后定义状态，每个状态表示一个节点，然后对应画出递归树。从递归树中，我们很容易可以看出来，是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。
找到重复子问题之后，接下来，我们有两种处理思路，第一种是直接用回溯加“备忘录”的方法，来避免重复子问题。从执行效率上来讲，这跟动态规划的解决思路没有差别。第二种是使用动态规划的解决方法，状态转移表法。第一种思路，我就不讲了，你可以看看上一节的两个例子。我们重点来看状态转移表法是如何工作的。
我们先画出一个状态表。状态表一般都是二维的，所以你可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。
尽管大部分状态表都是二维的，但是如果问题的状态比较复杂，需要很多变量来表示，那对应的状态表可能就是高维的，比如三维、四维。那这个时候，我们就不适合用状态转移表法来解决了。一方面是因为高维状态转移表不好画图表示，另一方面是因为人脑确实很不擅长思考高维的东西。
现在，我们来看一下，如何套用这个状态转移表法，来解决之前那个矩阵最短路径的问题？
从起点到终点，我们有很多种不同的走法。我们可以穷举所有走法，然后对比找出一个最短走法。不过如何才能无重复又不遗漏地穷举出所有走法呢？我们可以用回溯算法这个比较有规律的穷举算法。
回溯算法的代码实现如下所示。代码很短，而且我前面也分析过很多回溯算法的例题，这里我就不多做解释了，你自己来看看。
private int minDist = Integer.MAX_VALUE; // 全局变量或者成员变量 // 调用方式：minDistBacktracing(0, 0, 0, w, n); public void minDistBT(int i, int j, int dist, int[][] w, int n) { // 到达了n-1, n-1这个位置了，这里看着有点奇怪哈，你自己举个例子看下 if (i == n &amp;amp;&amp;amp; j == n) { if (dist &amp;lt; minDist) minDist = dist; return; } if (i &amp;lt; n) { // 往下走，更新i=i+1, j=j minDistBT(i + 1, j, dist+w[i][j], w, n); } if (j &amp;lt; n) { // 往右走，更新i=i, j=j+1 minDistBT(i, j+1, dist+w[i][j], w, n); } } 有了回溯代码之后，接下来，我们要画出递归树，以此来寻找重复子问题。在递归树中，一个状态（也就是一个节点）包含三个变量(i, j, dist)，其中i，j分别表示行和列，dist表示从起点到达(i, j)的路径长度。从图中，我们看出，尽管(i, j, dist)不存在重复的，但是(i, j)重复的有很多。对于(i, j)重复的节点，我们只需要选择dist最小的节点，继续递归求解，其他节点就可以舍弃了。</description></item><item><title>41_怎么最快地复制一张表？</title><link>https://artisanbox.github.io/1/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/41/</guid><description>我在上一篇文章最后，给你留下的问题是怎么在两张表中拷贝数据。如果可以控制对源表的扫描行数和加锁范围很小的话，我们简单地使用insert … select 语句即可实现。
当然，为了避免对源表加读锁，更稳妥的方案是先将数据写到外部文本文件，然后再写回目标表。这时，有两种常用的方法。接下来的内容，我会和你详细展开一下这两种方法。
为了便于说明，我还是先创建一个表db1.t，并插入1000行数据，同时创建一个相同结构的表db2.t。
create database db1; use db1; create table t(id int primary key, a int, b int, index(a))engine=innodb; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&amp;lt;=1000)do insert into t values(i,i,i); set i=i+1; end while; end;; delimiter ; call idata();
create database db2; create table db2.t like db1.t 假设，我们要把db1.t里面a&amp;gt;900的数据行导出来，插入到db2.t中。
mysqldump方法一种方法是，使用mysqldump命令将数据导出成一组INSERT语句。你可以使用下面的命令：
mysqldump -h$host -P$port -u$user &amp;ndash;add-locks=0 &amp;ndash;no-create-info &amp;ndash;single-transaction &amp;ndash;set-gtid-purged=OFF db1 t &amp;ndash;where=&amp;quot;a&amp;gt;900&amp;quot; &amp;ndash;result-file=/client_tmp/t.sql 把结果输出到临时文件。</description></item><item><title>41_理解内存（下）：解析TLB和内存保护</title><link>https://artisanbox.github.io/4/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/41/</guid><description>机器指令里面的内存地址都是虚拟内存地址。程序里面的每一个进程，都有一个属于自己的虚拟内存地址空间。我们可以通过地址转换来获得最终的实际物理地址。我们每一个指令都存放在内存里面，每一条数据都存放在内存里面。因此，“地址转换”是一个非常高频的动作，“地址转换”的性能就变得至关重要了。这就是我们今天要讲的第一个问题，也就是性能问题。
因为我们的指令、数据都存放在内存里面，这里就会遇到我们今天要谈的第二个问题，也就是内存安全问题。如果被人修改了内存里面的内容，我们的CPU就可能会去执行我们计划之外的指令。这个指令可能是破坏我们服务器里面的数据，也可能是被人获取到服务器里面的敏感信息。
现代的CPU和操作系统，会通过什么样的方式来解决这两个问题呢？别着急，等讲完今天的内容，你就知道答案了。
加速地址转换：TLB上一节我们说了，从虚拟内存地址到物理内存地址的转换，我们通过页表这个数据结构来处理。为了节约页表的内存存储空间，我们会使用多级页表数据结构。
不过，多级页表虽然节约了我们的存储空间，但是却带来了时间上的开销，变成了一个“以时间换空间”的策略。原本我们进行一次地址转换，只需要访问一次内存就能找到物理页号，算出物理内存地址。但是用了4级页表，我们就需要访问4次内存，才能找到物理页号。
我们知道，内存访问其实比Cache要慢很多。我们本来只是要做一个简单的地址转换，现在反而要一下子多访问好多次内存。这种情况该怎么处理呢？你是否还记得之前讲过的“加个缓存”的办法呢？我们来试一试。
程序所需要使用的指令，都顺序存放在虚拟内存里面。我们执行的指令，也是一条条顺序执行下去的。也就是说，我们对于指令地址的访问，存在前面几讲所说的“空间局部性”和“时间局部性”，而需要访问的数据也是一样的。我们连续执行了5条指令。因为内存地址都是连续的，所以这5条指令通常都在同一个“虚拟页”里。
因此，这连续5次的内存地址转换，其实都来自于同一个虚拟页号，转换的结果自然也就是同一个物理页号。那我们就可以用前面几讲说过的，用一个“加个缓存”的办法。把之前的内存转换地址缓存下来，使得我们不需要反复去访问内存来进行内存地址转换。
于是，计算机工程师们专门在CPU里放了一块缓存芯片。这块缓存芯片我们称之为TLB，全称是地址变换高速缓冲（Translation-Lookaside Buffer）。这块缓存存放了之前已经进行过地址转换的查询结果。这样，当同样的虚拟地址需要进行地址转换的时候，我们可以直接在TLB里面查询结果，而不需要多次访问内存来完成一次转换。
TLB和我们前面讲的CPU的高速缓存类似，可以分成指令的TLB和数据的TLB，也就是ITLB和DTLB。同样的，我们也可以根据大小对它进行分级，变成L1、L2这样多层的TLB。
除此之外，还有一点和CPU里的高速缓存也是一样的，我们需要用脏标记这样的标记位，来实现“写回”这样缓存管理策略。
为了性能，我们整个内存转换过程也要由硬件来执行。在CPU芯片里面，我们封装了内存管理单元（MMU，Memory Management Unit）芯片，用来完成地址转换。和TLB的访问和交互，都是由这个MMU控制的。
安全性与内存保护讲完了虚拟内存和物理内存的转换，我们来看看内存保护和安全性的问题。
进程的程序也好，数据也好，都要存放在内存里面。实际程序指令的执行，也是通过程序计数器里面的地址，去读取内存内的内容，然后运行对应的指令，使用相应的数据。
虽然我们现代的操作系统和CPU，已经做了各种权限的管控。正常情况下，我们已经通过虚拟内存地址和物理内存地址的区分，隔离了各个进程。但是，无论是CPU这样的硬件，还是操作系统这样的软件，都太复杂了，难免还是会被黑客们找到各种各样的漏洞。
就像我们在软件开发过程中，常常会有一个“兜底”的错误处理方案一样，在对于内存的管理里面，计算机也有一些最底层的安全保护机制。这些机制统称为内存保护（Memory Protection）。我这里就为你简单介绍两个。
可执行空间保护第一个常见的安全机制，叫可执行空间保护（Executable Space Protection）。
这个机制是说，我们对于一个进程使用的内存，只把其中的指令部分设置成“可执行”的，对于其他部分，比如数据部分，不给予“可执行”的权限。因为无论是指令，还是数据，在我们的CPU看来，都是二进制的数据。我们直接把数据部分拿给CPU，如果这些数据解码后，也能变成一条合理的指令，其实就是可执行的。
这个时候，黑客们想到了一些搞破坏的办法。我们在程序的数据区里，放入一些要执行的指令编码后的数据，然后找到一个办法，让CPU去把它们当成指令去加载，那CPU就能执行我们想要执行的指令了。对于进程里内存空间的执行权限进行控制，可以使得CPU只能执行指令区域的代码。对于数据区域的内容，即使找到了其他漏洞想要加载成指令来执行，也会因为没有权限而被阻挡掉。
其实，在实际的应用开发中，类似的策略也很常见。我下面给你举两个例子。
比如说，在用PHP进行Web开发的时候，我们通常会禁止PHP有eval函数的执行权限。这个其实就是害怕外部的用户，所以没有把数据提交到服务器，而是把一段想要执行的脚本提交到服务器。服务器里在拼装字符串执行命令的时候，可能就会执行到预计之外被“注入”的破坏性脚本。这里我放了一个例子，用这个办法可以去删除服务器上的数据。
script.php?param1=xxx //我们的PHP接受一个传入的参数，这个参数我们希望提供计算功能 $code = eval($_GET[&amp;quot;param1&amp;quot;]); // 我们直接通过 eval 计算出来对应的参数公式的计算结果 script.php?param1=&amp;quot;;%20echo%20exec(&amp;lsquo;rm -rf ~/&amp;rsquo;);%20// // 用户传入的参数里面藏了一个命令 $code = &amp;quot;&amp;quot;; echo exec(&amp;lsquo;rm -rf ~/&amp;rsquo;); //&amp;quot;; // 执行的结果就变成了删除服务器上的数据 还有一个例子就是SQL注入攻击。如果服务端执行的SQL脚本是通过字符串拼装出来的，那么在Web请求里面传输的参数就可以藏下一些我们想要执行的SQL，让服务器执行一些我们没有想到过的SQL语句。这样的结果就是，或者破坏了数据库里的数据，或者被人拖库泄露了数据。
地址空间布局随机化第二个常见的安全机制，叫地址空间布局随机化（Address Space Layout Randomization）。
内存层面的安全保护核心策略，是在可能有漏洞的情况下进行安全预防。上面的可执行空间保护就是一个很好的例子。但是，内存层面的漏洞还有其他的可能性。
这里的核心问题是，其他的人、进程、程序，会去修改掉特定进程的指令、数据，然后，让当前进程去执行这些指令和数据，造成破坏。要想修改这些指令和数据，我们需要知道这些指令和数据所在的位置才行。
原先我们一个进程的内存布局空间是固定的，所以任何第三方很容易就能知道指令在哪里，程序栈在哪里，数据在哪里，堆又在哪里。这个其实为想要搞破坏的人创造了很大的便利。而地址空间布局随机化这个机制，就是让这些区域的位置不再固定，在内存空间随机去分配这些进程里不同部分所在的内存空间地址，让破坏者猜不出来。猜不出来呢，自然就没法找到想要修改的内容的位置。如果只是随便做点修改，程序只会crash掉，而不会去执行计划之外的代码。
这样的“随机化”策略，其实也是我们日常应用开发中一个常见的策略。一个大家都应该接触过的例子就是密码登陆功能。网站和App都会需要你设置用户名和密码，之后用来登陆自己的账号。然后，在服务器端，我们会把用户名和密码保存下来，在下一次用户登陆的时候，使用这个用户名和密码验证。
我们的密码当然不能明文存储在数据库里，不然就会有安全问题。如果明文存储在数据库里，意味着能拿到数据库访问权限的人，都能看到用户的明文密码。这个可能是因为安全漏洞导致被人拖库，而且网站的管理员也能直接看到所有的用户名和密码信息。
比如，前几年CSDN就发生过被人拖库的事件。虽然用户名和密码都是明文保存的，别人如果只是拿到了CSDN网站的用户名密码，用户的损失也不会太大。但是很多用户可能会在不同的网站使用相同的密码，如果拿到这些用户名和密码的人，能够成功登录用户的银行、支付、社交等等其他网站的话，用户损失就大了去了。
于是，大家会在数据库里存储密码的哈希值，比如用现在常用的SHA256，生成一一个验证的密码哈希值。但是这个往往还是不够的。因为同样的密码，对应的哈希值都是相同的，大部分用户的密码又常常比较简单。于是，拖库成功的黑客可以通过彩虹表的方式，来推测出用户的密码。
这个时候，我们的“随机化策略”就可以用上了。我们可以在数据库里，给每一个用户名生成一个随机的、使用了各种特殊字符的盐值（Salt）。这样，我们的哈希值就不再是仅仅使用密码来生成的了，而是密码和盐值放在一起生成的对应的哈希值。哈希值的生成中，包括了一些类似于“乱码”的随机字符串，所以通过彩虹表碰撞来猜出密码的办法就用不了了。
$password = &amp;quot;goodmorning12345&amp;quot;; // 我们的密码是明文存储的</description></item><item><title>41｜后端优化：生成LIR和指令选择</title><link>https://artisanbox.github.io/3/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/43/</guid><description>你好，我是宫文学。
前面几节课中，我们讨论的主要是中端的优化。中端优化是跟具体硬件关系并不大，但由于我们还要生成针对具体CPU的汇编代码或机器码，所以做完中端优化之后，我们还要针对具体CPU的特性来做一些优化，也就是后端优化。
其实，我们已经接触过一些后端优化技术了。比如，之前我们已经讲过寄存器分配算法、尾调用和尾递归的优化，这些基本上都属于后端优化。不过，那个时候，我们是从AST直接生成汇编代码，然后在这个过程中做一些后端优化的。
在第三部分优化篇中，我们引入了新的、基于图的IR，进行了很多与硬件无关的优化。用这个基于图的IR来做后端优化，效果又怎样呢？接下来，我们就要修改以前的生成汇编代码的逻辑，改成从这个IR来生成汇编代码，并在这个过程中做一些优化。
今天这一节课，我就带你从中端过渡到后端，看看如何实现后端优化，并生成目标代码。这其中包括IR的Lower、生成LIR和指令选择，以及寄存器分配、指令重排序、窥孔优化和汇编代码的生成，等等工作。不过有些知识点我之前已经讲过了，还有一些知识点不是我们这门课的重点，我就不再展开讲了，重点帮你贯穿一下整个过程。
首先，我们先了解一下给IR做Lower的过程。
Lower过程HIR要经过一系列Lower过程，最后变成LIR。我先举一个例子，让你理解一下在Lower过程中会发生什么事情。这是一个简单的例子，它只实现了给mammal对象weight字段赋值的功能：
function accessField(mammal:Mammal, weight:number){ &amp;nbsp; &amp;nbsp; mammal.weight = weight; } 针对对象属性的赋值，通常编译器要生成写内存的指令。这是因为，对象通常使用的是在堆里申请的内存。而且，由于一个对象可能会由多个线程访问，所以只有把对象属性写到内存里，另一个线程才能访问到更新后的属性。
当然，我们前一节课也说过，如果这个对象并没有逃逸，那就是另一种情况了。我们先假设该对象是逃逸的，那么我们要给对象属性赋值，首先就需要进行写内存的操作。
对于这个简单的场景，一开始这个程序的IR是下面这样：
这里，我们使用了一个抽象度比较高的节点，叫做StoreField。它接受两个输入：一个输入是对象的引用，也就是对象地址；第二个输入是weight属性的值。
在编译的过程中，这个IR会被做Lower处理。StoreField节点会被一个Write节点代替，Write节点是一个写内存的操作。而内存地址呢，用OffsetAddress表示，也就是一个基地址加上一定的偏移量。基地址就是对象的地址，偏移量是对象头的大小。在PlayScript的设计中，它是16个字节。Lower过一次的IR图是这样的：
到目前为止，这个IR还是跟具体CPU无关的。因为按照这张IR图，无论针对什么CPU，你都可以通过在某个地址的基础上加上一个偏移量，来获得新的地址。
然后这个IR还会进一步被Lower，让地址的表示方式更贴近x86-64（或AMD64）架构的具体寻址方式。我们曾经学过x86-64的寻址方式，它的完整形式包括基地址、偏移量、下标值、元素字节数等多个参数。但这里我们只需要它的简化方式，也就是基地址加上一个偏移量就行。进一步Lower的IR变成了这样：
到了这一步，我们的IR已经变得跟具体CPU架构相关了。接下来，我们就把它彻底转化成LIR的格式。
生成LIR和指令选择那LIR又是什么样子的呢？你可以思考一下，如果你来设计编译器，应该如何设计LIR呢？
LIR的目的，是进行机器相关的优化，并最后生成汇编代码。所以，大部分LIR的设计，都是跟汇编代码是同构的。也就是说，LIR是由一条条指令构成的，指令是放在基本块中的，而基本块之间存在跳转关系。
从这个意义上说，我们之前生成汇编代码的时候，已经设计过这样的LIR。而Graal、LLVM、Go语言的gc编译器，在生成汇编代码或机器码之前也都有类似的LIR设计。这个数据结构看上去仍然是基于CFG的，但我们目前已经不需要分析它的控制流和数据流，并调整里面的代码了。这些工作，我们在中端优化的时候都已经完成了。现在，基于这个LIR，我们关心的主要是指令选择、寄存器分配和指令重排序（或者叫做指令调度）这样的话题。
由于我们的IR设计借鉴了Graal编译器，那么同样的，我们继续跟着Graal看看它是怎么处理LIR的。图中是Graal编译器中生成的LIR的例子，你可以通过它建立对LIR的直观感觉：
这张图中一行行的文本，是为了显示LIR中内容，便于调试，实际上的LIR都是内存里的一条条的指令对象。在这个图中，你还能看到Graal编译器的后端处理过程，包括生成LIR、寄存器分配，一直到生成目标代码。我们自己实现的编译器，也需要完成类似的功能。
好了，我们现在已经理解了LIR是什么样子了。那我们现在就从HIR生成LIR，并在这个过程中进行指令的选择，这又可以分成几项子任务。
首先，我们要把HIR中的不同节点分配到不同的基本块中，这被叫做调度算法（Schedule）。
我们在39节已经介绍过，由于很多数据流节点是浮动的，我们可以自由地选择在什么时候进行计算。但我们在生成汇编代码之前，还是要把确定这些数据节点的计算时机，因此要把它们分配到具体的基本块中。
而且，我们需要基于一些规则来完成这个分配工作，比如对于循环无关的代码，我们会提到循环外边；而基于控制流来求值的代码，比如if语句的两个不同分支的代码，我们尽量分配到这两个分支对应的基本块中。制定这些规则的出发点，是尽可能地提升程序的性能，但其实并不能完全保证。在比较AOT和JIT时，我们已经讲过这点了。
在划分好基本块以后，我们再做第二项工作，把IR图转化成LIR的指令，并在这个过程中进行指令的选择。
如果细讲起来，指令选择有两层含义，而这两层含义的工作经常是一起实现的。指令选择的第一层含义，是把抽象的运算，准确地Lower到硬件的具体指令上。比如说，我们可以从比较抽象的层次，对整数和浮点数都执行加法运算。但到了CPU层面，整数的加法指令和浮点数的加法指令就不一样了。其他指令，比如比较运算、数据拷贝的指令，也是跟数据类型和所采用的指令集相关的，编译器要确定出正确的指令。
指令选择的第二层含义，指的是相同的功能，可以用不同的指令组合来实现，而我们要尽量选择让整体性能最优的那组指令。这实际上是一个最优化问题。
关于指令选择的算法，我在《编译原理之美》的29节做过一些理论性的介绍，在《编译原理实战课》的16节，我也介绍过Graal编译器的具体实现。这门课，我也会参考Graal的思路，做一个比较简化的实现。
在我看来，要理解指令选择，除了学习算法，更重要的是要了解很多具体的指令选择场景。下面，我们就以x86-64架构的指令来举几个例子，帮助你建立直观理解。经过这些讲解后，你就能理解那些抽象的算法到底在说些什么了。
第一个例子，是经常出现在if语句中的条件跳转指令：
if(a&amp;gt;b){ //somecode } else{ //some other code } 回忆一下，我们在这门课的第9节、为字节码虚拟机生成字节码的时候，if条件和跳转相关的字节码是分两步来生成的：第一步，处理if条件，计算条件表达式"a&amp;lt;b"，并生成1或0两个值，代表true和false；第二步，处理if节点，根据&amp;lt;节点的值来生成跳转指令。跳转指令使用JE或JNE就行了，也就是比较if条件是不是1。
用这个方式生成指令比较简单。算法上说，就是对每个AST节点依次进行处理。像字面量、变量这样的节点，我们会返回一个Operand。而对于计算性节点，我们就要生成指令，并把指令运行的结果作为Operand来返回。
不过，大部分CPU或虚拟机都提供了更丰富的条件跳转指令，比如JL指令就可以用于在a&amp;lt;b的时候做跳转，而JG指令就可以用于在a&amp;gt;b的时候跳转。这个时候，我们需要同时处理if和&amp;lt;号两个节点，确定采用什么指令，你可以看一下这张示意图：
这就是指令选择算法的特点，我们需要一次性地考虑AST或IR中的多个节点，并生成合适的指令，只要最后算法确实覆盖了所有节点就行。
第二类经常需要做指令选择的情况，是对内存的访问。我还是用这节课开头这个、给mammal对象的weight属性赋值的例子来做说明：
function accessField(mammal:Mammal, weight:number){ &amp;nbsp; &amp;nbsp; mammal.weight = weight; } 在生成指令的时候，我们需要在对象的基地址的基础上，添加一个偏移量，获得weight属性的地址，然后再给这个地址赋值。
要完成这个操作，我们有两个办法。第一个办法是分成两步来生成指令，第一步是先计算出weight属性的地址，第二步是往内存地址写weight的值：
这两步对应的LIR相当于下面两条代码：
add $16, p0 #把p0,也就是对象的地址加上偏移量 mov p1, (p0) #把p1赋给p0指向的内存地址 不过，我们还有第二个方法来生成指令，这就是直接使用x86-64的寻址方式，用一条指令就能完成地址计算和写内存这两个操作。我们的指令选择算法需要一次性处理Write和AMD64Address两个节点：</description></item><item><title>42_grant之后要跟着flushprivileges吗？</title><link>https://artisanbox.github.io/1/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/42/</guid><description>在MySQL里面，grant语句是用来给用户赋权的。不知道你有没有见过一些操作文档里面提到，grant之后要马上跟着执行一个flush privileges命令，才能使赋权语句生效。我最开始使用MySQL的时候，就是照着一个操作文档的说明按照这个顺序操作的。
那么，grant之后真的需要执行flush privileges吗？如果没有执行这个flush命令的话，赋权语句真的不能生效吗？
接下来，我就先和你介绍一下grant语句和flush privileges语句分别做了什么事情，然后再一起来分析这个问题。
为了便于说明，我先创建一个用户：
create user 'ua'@'%' identified by 'pa'; 这条语句的逻辑是创建一个用户’ua’@’%’，密码是pa。注意，在MySQL里面，用户名(user)+地址(host)才表示一个用户，因此 ua@ip1 和 ua@ip2代表的是两个不同的用户。
这条命令做了两个动作：
磁盘上，往mysql.user表里插入一行，由于没有指定权限，所以这行数据上所有表示权限的字段的值都是N；
内存里，往数组acl_users里插入一个acl_user对象，这个对象的access字段值为0。
图1就是这个时刻用户ua在user表中的状态。
图1 mysql.user 数据行在MySQL中，用户权限是有不同的范围的。接下来，我就按照用户权限范围从大到小的顺序依次和你说明。
全局权限全局权限，作用于整个MySQL实例，这些权限信息保存在mysql库的user表里。如果我要给用户ua赋一个最高权限的话，语句是这么写的：
grant all privileges on *.* to 'ua'@'%' with grant option; 这个grant命令做了两个动作：
磁盘上，将mysql.user表里，用户’ua’@’%'这一行的所有表示权限的字段的值都修改为‘Y’；
内存里，从数组acl_users中找到这个用户对应的对象，将access值（权限位）修改为二进制的“全1”。
在这个grant命令执行完成后，如果有新的客户端使用用户名ua登录成功，MySQL会为新连接维护一个线程对象，然后从acl_users数组里查到这个用户的权限，并将权限值拷贝到这个线程对象中。之后在这个连接中执行的语句，所有关于全局权限的判断，都直接使用线程对象内部保存的权限位。
基于上面的分析我们可以知道：
grant 命令对于全局权限，同时更新了磁盘和内存。命令完成后即时生效，接下来新创建的连接会使用新的权限。
对于一个已经存在的连接，它的全局权限不受grant命令的影响。
需要说明的是，一般在生产环境上要合理控制用户权限的范围。我们上面用到的这个grant语句就是一个典型的错误示范。如果一个用户有所有权限，一般就不应该设置为所有IP地址都可以访问。
如果要回收上面的grant语句赋予的权限，你可以使用下面这条命令：
revoke all privileges on *.* from 'ua'@'%'; 这条revoke命令的用法与grant类似，做了如下两个动作：
磁盘上，将mysql.</description></item><item><title>42_动态规划实战：如何实现搜索引擎中的拼写纠错功能？</title><link>https://artisanbox.github.io/2/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/43/</guid><description>在Trie树那节我们讲过，利用Trie树，可以实现搜索引擎的关键词提示功能，这样可以节省用户输入搜索关键词的时间。实际上，搜索引擎在用户体验方面的优化还有很多，比如你可能经常会用的拼写纠错功能。
当你在搜索框中，一不小心输错单词时，搜索引擎会非常智能地检测出你的拼写错误，并且用对应的正确单词来进行搜索。作为一名软件开发工程师，你是否想过，这个功能是怎么实现的呢？
如何量化两个字符串的相似度？计算机只认识数字，所以要解答开篇的问题，我们就要先来看，如何量化两个字符串之间的相似程度呢？有一个非常著名的量化方法，那就是编辑距离（Edit Distance）。
顾名思义，编辑距离指的就是，将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。编辑距离越大，说明两个字符串的相似程度越小；相反，编辑距离就越小，说明两个字符串的相似程度越大。对于两个完全相同的字符串来说，编辑距离就是0。
根据所包含的编辑操作种类的不同，编辑距离有多种不同的计算方式，比较著名的有莱文斯坦距离（Levenshtein distance）和最长公共子串长度（Longest common substring length）。其中，莱文斯坦距离允许增加、删除、替换字符这三个编辑操作，最长公共子串长度只允许增加、删除字符这两个编辑操作。
而且，莱文斯坦距离和最长公共子串长度，从两个截然相反的角度，分析字符串的相似程度。莱文斯坦距离的大小，表示两个字符串差异的大小；而最长公共子串的大小，表示两个字符串相似程度的大小。
关于这两个计算方法，我举个例子给你说明一下。这里面，两个字符串mitcmu和mtacnu的莱文斯坦距离是3，最长公共子串长度是4。
了解了编辑距离的概念之后，我们来看，如何快速计算两个字符串之间的编辑距离？
如何编程计算莱文斯坦距离？之前我反复强调过，思考过程比结论更重要，所以，我现在就给你展示一下，解决这个问题，我的完整的思考过程。
这个问题是求把一个字符串变成另一个字符串，需要的最少编辑次数。整个求解过程，涉及多个决策阶段，我们需要依次考察一个字符串中的每个字符，跟另一个字符串中的字符是否匹配，匹配的话如何处理，不匹配的话又如何处理。所以，这个问题符合多阶段决策最优解模型。
我们前面讲了，贪心、回溯、动态规划可以解决的问题，都可以抽象成这样一个模型。要解决这个问题，我们可以先看一看，用最简单的回溯算法，该如何来解决。
回溯是一个递归处理的过程。如果a[i]与b[j]匹配，我们递归考察a[i+1]和b[j+1]。如果a[i]与b[j]不匹配，那我们有多种处理方式可选：
可以删除a[i]，然后递归考察a[i+1]和b[j]；
可以删除b[j]，然后递归考察a[i]和b[j+1]；
可以在a[i]前面添加一个跟b[j]相同的字符，然后递归考察a[i]和b[j+1];
可以在b[j]前面添加一个跟a[i]相同的字符，然后递归考察a[i+1]和b[j]；
可以将a[i]替换成b[j]，或者将b[j]替换成a[i]，然后递归考察a[i+1]和b[j+1]。
我们将上面的回溯算法的处理思路，翻译成代码，就是下面这个样子：
private char[] a = &amp;quot;mitcmu&amp;quot;.toCharArray(); private char[] b = &amp;quot;mtacnu&amp;quot;.toCharArray(); private int n = 6; private int m = 6; private int minDist = Integer.MAX_VALUE; // 存储结果 // 调用方式 lwstBT(0, 0, 0); public lwstBT(int i, int j, int edist) { if (i == n || j == m) { if (i &amp;lt; n) edist += (n-i); if (j &amp;lt; m) edist += (m - j); if (edist &amp;lt; minDist) minDist = edist; return; } if (a[i] == b[j]) { // 两个字符匹配 lwstBT(i+1, j+1, edist); } else { // 两个字符不匹配 lwstBT(i + 1, j, edist + 1); // 删除a[i]或者b[j]前添加一个字符 lwstBT(i, j + 1, edist + 1); // 删除b[j]或者a[i]前添加一个字符 lwstBT(i + 1, j + 1, edist + 1); // 将a[i]和b[j]替换为相同字符 } } 根据回溯算法的代码实现，我们可以画出递归树，看是否存在重复子问题。如果存在重复子问题，那我们就可以考虑能否用动态规划来解决；如果不存在重复子问题，那回溯就是最好的解决方法。</description></item><item><title>42_总线：计算机内部的高速公路</title><link>https://artisanbox.github.io/4/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/42/</guid><description>专栏讲到现在，如果我再问你，计算机五大组成部分是什么，应该没有人不知道了吧？我们这一节要讲的内容，依然要围绕这五大部分，控制器、运算器、存储器、输入设备和输出设备。
CPU所代表的控制器和运算器，要和存储器，也就是我们的主内存，以及输入和输出设备进行通信。那问题来了，CPU从我们的键盘、鼠标接收输入信号，向显示器输出信号，这之间究竟是怎么通信的呢？换句话说，计算机是用什么样的方式来完成，CPU和内存、以及外部输入输出设备的通信呢？
这个问题就是我们今天要讲的主题，也就是总线。之前很多同学留言问，我什么时候会讲一讲总线。那这一讲，你就要听仔细了。
降低复杂性：总线的设计思路来源计算机里其实有很多不同的硬件设备，除了CPU和内存之外，我们还有大量的输入输出设备。可以说，你计算机上的每一个接口，键盘、鼠标、显示器、硬盘，乃至通过USB接口连接的各种外部设备，都对应了一个设备或者模块。
如果各个设备间的通信，都是互相之间单独进行的。如果我们有$N$个不同的设备，他们之间需要各自单独连接，那么系统复杂度就会变成$N^2$。每一个设备或者功能电路模块，都要和其他$N-1$个设备去通信。为了简化系统的复杂度，我们就引入了总线，把这个$N^2$的复杂度，变成一个$N$的复杂度。
那怎么降低复杂度呢？与其让各个设备之间互相单独通信，不如我们去设计一个公用的线路。CPU想要和什么设备通信，通信的指令是什么，对应的数据是什么，都发送到这个线路上；设备要向CPU发送什么信息呢，也发送到这个线路上。这个线路就好像一个高速公路，各个设备和其他设备之间，不需要单独建公路，只建一条小路通向这条高速公路就好了。
这个设计思路，就是我们今天要说的总线（Bus）。
总线，其实就是一组线路。我们的CPU、内存以及输入和输出设备，都是通过这组线路，进行相互间通信的。总线的英文叫作Bus，就是一辆公交车。这个名字很好地描述了总线的含义。我们的“公交车”的各个站点，就是各个接入设备。要想向一个设备传输数据，我们只要把数据放上公交车，在对应的车站下车就可以了。
其实，对应的设计思路，在软件开发中也是非常常见的。我们在做大型系统开发的过程中，经常会用到一种叫作事件总线（Event Bus）的设计模式。
进行大规模应用系统开发的时候，系统中的各个组件之间也需要相互通信。模块之间如果是两两之间单独去定义协议，这个软件系统一样会遇到一个复杂度变成了$N^2$的问题。所以常见的一个解决方案，就是事件总线这个设计模式。
在事件总线这个设计模式里，各个模块触发对应的事件，并把事件对象发送到总线上。也就是说，每个模块都是一个发布者（Publisher）。而各个模块也会把自己注册到总线上，去监听总线上的事件，并根据事件的对象类型或者是对象内容，来决定自己是否要进行特定的处理或者响应。
这样的设计下，注册在总线上的各个模块就是松耦合的。模块互相之间并没有依赖关系。无论代码的维护，还是未来的扩展，都会很方便。
理解总线：三种线路和多总线架构理解了总线的设计概念，我们来看看，总线在实际的计算机硬件里面，到底是什么样。
现代的Intel CPU的体系结构里面，通常有好几条总线。
首先，CPU和内存以及高速缓存通信的总线，这里面通常有两种总线。这种方式，我们称之为双独立总线（Dual Independent Bus，缩写为DIB）。CPU里，有一个快速的本地总线（Local Bus），以及一个速度相对较慢的前端总线（Front-side Bus）。
我们在前面几讲刚刚讲过，现代的CPU里，通常有专门的高速缓存芯片。这里的高速本地总线，就是用来和高速缓存通信的。而前端总线，则是用来和主内存以及输入输出设备通信的。有时候，我们会把本地总线也叫作后端总线（Back-side Bus），和前面的前端总线对应起来。而前端总线也有很多其他名字，比如处理器总线（Processor Bus）、内存总线（Memory Bus）。
除了前端总线呢，我们常常还会听到PCI总线、I/O总线或者系统总线（System Bus）。看到这么多总线的名字，你是不是已经有点晕了。这些名词确实容易混为一谈。其实各种总线的命名一直都很混乱，我们不如直接来看一看CPU的硬件架构图。对照图来看，一切问题就都清楚了。
CPU里面的北桥芯片，把我们上面说的前端总线，一分为二，变成了三个总线。
我们的前端总线，其实就是系统总线。CPU里面的内存接口，直接和系统总线通信，然后系统总线再接入一个I/O桥接器（I/O Bridge）。这个I/O桥接器，一边接入了我们的内存总线，使得我们的CPU和内存通信；另一边呢，又接入了一个I/O总线，用来连接I/O设备。
事实上，真实的计算机里，这个总线层面拆分得更细。根据不同的设备，还会分成独立的PCI总线、ISA总线等等。
在物理层面，其实我们完全可以把总线看作一组“电线”。不过呢，这些电线之间也是有分工的，我们通常有三类线路。
数据线（Data Bus），用来传输实际的数据信息，也就是实际上了公交车的“人”。 地址线（Address Bus），用来确定到底把数据传输到哪里去，是内存的某个位置，还是某一个I/O设备。这个其实就相当于拿了个纸条，写下了上面的人要下车的站点。 控制线（Control Bus），用来控制对于总线的访问。虽然我们把总线比喻成了一辆公交车。那么有人想要做公交车的时候，需要告诉公交车司机，这个就是我们的控制信号。 尽管总线减少了设备之间的耦合，也降低了系统设计的复杂度，但同时也带来了一个新问题，那就是总线不能同时给多个设备提供通信功能。
我们的总线是很多个设备公用的，那多个设备都想要用总线，我们就需要有一个机制，去决定这种情况下，到底把总线给哪一个设备用。这个机制，就叫作总线裁决（Bus Arbitraction）。总线裁决的机制有很多种不同的实现，如果你对这个实现的细节感兴趣，可以去看一看Wiki里面关于裁决器的对应条目，这里我们就不多说了。
总结延伸好了，你现在明白计算机里的总线、各种不同的总线到底是什么意思了吧？希望这一讲能够帮你厘清计算机总线的知识点。现在我们一起来总结梳理一下这节的内容。
这一讲，我为你讲解了计算机里各个不同的组件之间用来通信的渠道，也就是总线。总线的设计思路，核心是为了减少多个模块之间交互的复杂性和耦合度。实际上，总线这个设计思路在我们的软件开发过程中也经常会被用到。事件总线就是我们常见的一个设计模式，通常事件总线也会和订阅者发布者模式结合起来，成为大型系统的各个松耦合的模块之间交互的一种主要模式。
在实际的硬件层面，总线其实就是一组连接电路的线路。因为不同设备之间的速度有差异，所以一台计算机里面往往会有多个总线。常见的就有在CPU内部和高速缓存通信的本地总线，以及和外部I/O设备以及内存通信的前端总线。
前端总线通常也被叫作系统总线。它可以通过一个I/O桥接器，拆分成两个总线，分别来和I/O设备以及内存通信。自然，这样拆开的两个总线，就叫作I/O总线和内存总线。总线本身的电路功能，又可以拆分成用来传输数据的数据线、用来传输地址的地址线，以及用来传输控制信号的控制线。
总线是一个各个接入的设备公用的线路，所以自然会在各个设备之间争夺总线所有权的情况。于是，我们需要一个机制来决定让谁来使用总线，这个决策机制就是总线裁决。
推荐阅读总线是一个抽象的设计模式，它不仅在我们计算机的硬件设计里出现。在日常的软件开发中，也是一个常见的设计模式，你可以去读一读Google开源的Java的一个常用的工具库Guava的相关资料和代码，进一步理解事件总线的设计模式，看看在软件层面怎么实现它。
对于计算机硬件层面的总线，很多教科书里讲得都比较少，你可以去读一读Wiki里面总线和系统总线的相关条目。
课后思考2008年之后，我们的Intel CPU其实已经没有前端总线了。Intel发明了快速通道互联（Intel Quick Path Interconnect，简称为QPI）技术，替代了传统的前端总线。这个QPI技术，你可以搜索和翻阅一下相关资料，了解一下它引入了什么新的设计理念。
欢迎在留言区分享你查阅到的资料，以及阅读之后的思考总结，和大家一起交流。如果有收获，你也可以把这篇文章分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>42｜到这里，我们的收获和未尽的工作有哪些？</title><link>https://artisanbox.github.io/3/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/44/</guid><description>你好，我是宫文学。
到今天这节课为止，我们已经把这门课程的主要内容都学完了。感谢你一路的坚持！
所以，在今天这节课，我想做一个简单的总结。我想先带你回顾一下我们一起闯过的那些技术关卡，以及取得的成果。接下来，我还想梳理一下我们尚未完成的工作，也对我们后续作为开源项目的PlayScript语言做一下规划。
在这个过程中，你可以暂时从技术细节中解脱出来，站在一个语言的架构师的角度，一起做一些高层面的思考，锻炼一下架构思维。
首先，我们简单总结一下当前已经完成的工作。
当前的收获到目前为止，我们在40多节课的内容里，塞进了大量的知识点。我们按课程顺序来梳理一下。
基础篇：三大关卡在第一部分基础篇中，我带你连续闯了三个关卡。
第一个关卡，是编译器前端技术，包括词法分析、语法分析和语义分析技术。
在词法和语法分析方面，我们这门课没有带你进入相关算法的迷魂阵，而是带你去掌握最佳实践。一方面，这些算法我在《编译原理之美》课程中已经讲过了。另一方面，如果你只是写个编译器，而不是写个像Yacc、Antlr这样的编译器生成工具，其实不需要深究那些算法，只要大概明白原理就行了。
即使是这样，对于递归下降中的左递归问题这样偏理论性的知识点，很多同学免不了还是有疑惑。比如，有同学会问我，我在课程里用到的有些文法，为什么仍然有一些是左递归的呢？这里其实涉及到PEG文法的一个知识点，我会在后面的加餐里讲一下PEG。其实，并不是所有的左递归都没有办法处理。关于左递归，直到现在仍然是做算法研究的人感兴趣的一个领域。
在语义分析方面，我们体会了如何建立符号表、如何做引用消解、如何检查一些语义错误的过程，这样，你会对课本中讲到的一些抽象概念建立具象的理解。
为了编译TypeScript语言，编译器在语义分析阶段最重要的功能是进行类型的处理，其中的关键点又是类型的联合和窄化。在这门课里，我们主要采用了集合运算的手法来处理类型。在PlayScript开源项目中，我计划把这部分进一步优化，让类型计算更简洁、更精准。
对很多同学来说，闯过编译器前端的这个关卡，其实已经收获满满，可以在自己的项目里大展身手了。不过，如果你喜欢钻研底层实现，显然还不会满足止步于此，那你就可以继续闯第二关。
在第二关，我们实现了两个版本的字节码虚拟机。一个版本是用TypeScript实现的，另一个版本使用C语言实现的。
通过这个实现过程，你会了解到像Java这样的成熟语言的字节码是如何设计的，又是如何实际运行的。这样，在需要的时候，我希望你能够敢于自己生成Java或.NET的字节码，实现自己想要的软件编程功能。
并且，通过实现C语言版本的虚拟机，我们也能初步了解运行时的功能。特别是，你要知道，如果我们不能做好内存的管理，系统运行的性能就会大受影响。而且，通过我们多次的性能测试，你应该已经对一个解释器中影响性能的因素产生了直观的理解，这样你自己写程序的时候，也能够进行更明智的决策。
在初步实现了字节码解释器以后，我们又进入了第三关，挑战把源代码编译成二进制的可执行程序。
为了完成第三关的任务，我们必须对程序的运行机制有深入的了解，包括程序运行跟硬件是什么关系，跟操作系统和ABI又是什么关系。再进一步，我们还需要了解CPU架构和它支持的指令集，学会阅读甚至手写汇编代码。
生成汇编代码有两个关键点：第一，要熟悉ABI，正确地维护栈桢和寄存器的状态，否则程序运行时就会报segment错误；第二，就是要实现寄存器分配算法。你要知道，不同的指令集会使用不同的寄存器，并且我们在函数调用前后要保护某些寄存器。做好这些以后，程序就可以充分利用寄存器，飞一般地运行了。
在闯完这三关以后，你已经从前端到后端打通了技术路线。接下来在第二部分进阶篇中，我们把这条路线做了拓宽。
进阶篇：拓宽路径怎么去做拓宽呢？我们的主线是支持更多的数据类型，包括浮点数、字符串和数组这几个基础类型，还包括自定义对象、高阶函数这种高级的类型。
为了支持这些类型，我们必须增强运行时的功能，需要设计对象的内存布局。在访问对象属性、数组成员的时候，我们也要能够正确计算出内存地址来。
进阶篇最难的部分，是自动内存管理功能，包括基于Arena的内存申请机制，以及垃圾回收机制。而实现垃圾回收的关键点，在于找到GC根，并顺着GC根去找到直接引用和间接引用的对象。因此，我们需要保存栈桢的布局信息、对象的元数据信息、闭包的元数据信息等各种静态信息。
如果你充分掌握了内存管理涉及的技术点，那么你在后面实现很多高级功能的时候都能用上。比如对程序做调试、支持运行时的类型判断和元编程功能，都需要用到我们提前保存的元数据信息。
学完第二部分以后，你对实现像面向对象、函数式编程等各种语言特性基本上心里有数了。在第三部分优化篇，我们就专注于解决一个问题，就是优化问题。
优化篇：基于图的IR优化是编译器最重要的功能之一。优化可以发生在全生命周期，包括前端、中端、后端和运行过程中。第三部分的核心，就是一个基于图的IR。这个IR被Java和JavaScript的编译器采用，你可以想象一定有它的优势。
在使用这个IR的过程中，我们确实发现，我们很多优化的实现都变得很简单了。像公共子表达式删除，我们在生成IR的过程中顺带着就能完成。还有，像拷贝传播、死代码删除、值编号等这些优化，实现起来也很简单。最重要的是，这个IR更容易打通本地优化、全局优化和过程间优化三者的边界，让代码更容易在不同的基本块中移动，获取我们想要的优化效果。
并且，我们还了解了基于这个IR不断地做lower，直到生成LIR，然后基于LIR做指令选择、寄存器分配等后端优化的完整过程。通过这一部分的学习，你对于前端、中端、后端优化要做的工作就都比较清晰了。
好了，以上就是这门课中我们领略的各种技术风光，我希望你能够充分掌握，一方面这能开阔你的技术思路，另一方面这些技术也能用到你的实际项目中。
不过，受限于时间，我还没有把一门完整的语言完全实现完。所以，我后面会把这门课的示例代码，作为一个开源项目继续迭代下去，并形成完整的、实用的版本。至于当前已有的基础，我们就把它作为0.1版本吧！
那如果我们要实现一个实用的版本，还有哪些工作要做呢？
后续工作第一，我们要对编译器前端做比较大的增强和重构。
首先，当前我们已有的词法分析、语法分析和语义分析功能，都要支持更多的特性，比如，除了我们已经支持的for循环和if分支语句外，还有while循环、switch语句，等等。
而且，我们对编译错误的处理要更加友好。你应该也感受到了，目前我们的编译器，在遇到某些语法错误的时候，会持续不停地尝试，不断打印错误信息。这显然太不友好了，要做优化。
然后还有一个比较大的工作，就是对类型系统进行升级。我们要重构一下我们之前类型计算的算法，让它变得更加简洁和准确。我们目前使用的Nominal的类型系统，也要修改成支持structural的类型系统，并且我们还要让我们的类型计算支持泛型。
另外，我们在升级编译器前端的时候，对AST和符号表这两个重要的数据结构，也需要重构和优化一下。
第二，我们要把面向对象和函数式编程的特性实现完整。
比如，面向对象方面，我们需要实现严格的对象的初始化流程，需要支持访问权限，还要支持接口。在函数式编程方面，怎么着也要把Lambda表达式这些基础功能实现。这主要是工作量的问题，但需要前端、中端、后端和运行时各方面的配合。
第三，是升级编译器的中端优化功能。
基于目前的IR，我们只实现了少量的优化，并且还没有支持面向对象等复杂的语言特性，这些都需要进行扩展和支持。在这个实现的过程中，我们IR的数据结构也会得到丰富和完善。
第四，是升级编译器的后端功能。
我们之前的编译器后端主要是基于AST来生成汇编代码。所以，在引入IR以后，我们编译器的后端也需要重构一下。从AST生成IR后，再基于HIR生成LIR，然后在LIR的基础上重新实现指令选择和寄存器分配。
另外，我们目前只支持x86-64架构，并且也没有在多个操作系统上做测试。在后面，我们要支持至少两种CPU架构，我计划先支持的是x86-64和Aarch64。前者被广泛用于PC和服务器中，后者被广泛用于智能手机和苹果新一代Mac电脑中。而且，我们还要兼容多种操作系统。
第五，是内存管理方面的升级。
在垃圾收集方面，我们的GC还是很基础的，达不到实用级别。那么，接下来我们首先要完善基本的标记-清除算法。之后，我计划实现一个自动引用计数的（ARC）的机制。
ARC的原理是记录每个对象的引用数，当引用数为零的时候，就自动作为垃圾清理掉。ARC的好处就是垃圾回收不会引起大的停顿，能让系统的响应比较平缓。苹果的Objective-C和Swift都采用了ARC，这也是苹果的系统很少卡顿的原因之一。
第六，实现并发机制。
你应该也注意到了，目前我们这门课中并没有涉及并发机制。但如果不实现并发机制，显然会是一个遗憾。所以后面，我会给出协程功能的一个参考实现。至于另外的并发功能的设想，我接下来还会介绍到。
好了，上面就是要实现一个完整的、实用的、静态编译的语言会涉及到的工作量，细细看一下，还真是挺大的。不过，你也不用怵，这里并没有太多技术点是我们这门课没有涵盖到的，更多的是工作量和工程化的问题。
花费这么多的工作量，我并不完全是为了兴趣爱好，或者是做技术验证，还是想未来有一天，能把它用于一些实际的应用场景中。那接下来，我就谈谈对开源的PlayScript语言项目的一些规划。
对PlayScript的规划为什么我会产生自己动手实现一门语言的想法呢？这其实是出于一些实际的需求。现有的语言，或者已有语言的现有实现，有时会让我很不满意。所以我就想，与其等着别人来满足我的需求，不如自己动手试试看。
我就分享一下我的这几个需求，看看你是否也遇到过类似的问题。
首先，我对后端编程语言不满意。
你知道吗？要实现像微信这样的应用的后端，你只能使用C++这样的语言。并且，微信团队还开发了自己的协程库，才能应对海量并发的需求。我们每天用微信，觉得它总是会实时响应，其实后端的挑战是巨大的。你想想看就知道，成亿的用户，加上成亿的并发，绝对是顶级的技术挑战。
Java、Go等典型的后端语言，都不能满足这种场景的需求。Java的内存占用太大，自带的并发机制只有线程。虽然Go语言好一点，但它和Java都有一个致命的弱点，就是垃圾回收导致的停顿是不可控的。这对于微信这种大型的、高并发的平台，会带来灾难性的后果。但是，让普通的技术团队用C++开发应用，门槛有点高。
而且，我对现有后端语言提供的可靠性也不满意。在这方面，我比较喜欢Erlang，它的并发机制和其他特性结合起来，能提供9个9的可靠性。我觉得，如果每个应用都能实现这么高的可靠性就好了。但是，我对Erlang的性能又不满意，而且它的语法对于大多数程序员来说，也不是太友好。
实现一门高可靠性的语言，其实有隐含一个需求，就是语言中的功能是能够在运行时被动态替换的，因为你没有办法停下整个系统。所以，我们不能仅仅实现AOT的功能，还要有JIT、动态优化、动态部署、动态Dispatch的功能。
所以，我理想的后端语言，是能够用比较低的成本，开发出高并发、高可靠性、资源消耗低的应用。不知道你是不是也有类似的需求呢？
第二，我对前端编程环境也不太满意。
我感觉，现在的前端编程环境太碎片化了，包括浏览器、Android、IOS、Windows、macOS、Linux等不同的平台，而且国内还有好几个不同的小程序平台。
所以，我想要是有一个语言或者工具，能够开发一次，部署到很多客户端，那就好了。
第三，我对企业应用的编程语言也不满意。
我曾经参与过很多企业应用的开发工作。在企业应用的开发中，很多时候我们要更关注业务逻辑。但是，现在很多应用的业务逻辑和技术逻辑都是混杂的，企业应用的开发成本太高了。
我一直认为，如果你是做企业软件的厂商，那应该有相应的开发语言才好，比如，德国的SAP就有自己的ABAP语言。不过，这个语言在现代的应用架构下已经过时了，用ABAP开发不出很容易横向扩展的应用。再有一点，这个语言是企业私有的，不是公共的。
当然，现在我们已经迎来了低代码开发的一波浪潮。但是，如果低代码工具的开发者不是像微软这么有实力的厂商，很难维护一个完全私有的生态。这样的情况下，客户在你私有的平台上开发应用，就是比较有风险的，所以还是应该有一个公共开放的语言。
而且，就算是低代码开发，我也希望是基于某个语言的，而不是仅仅提供一些图形化的定制工具。最好呢，是语言可以转化为图形，图形也可以转化为代码。这种代码和图形化表达双向转化的能力，我在华为的HarmonyOS开发工具上看到了，感觉很喜欢。其实我觉得，低代码开发工具也应该实现类似的功能才算合格，这种能把应用表达为代码的能力，是保证应用的可移植性、保护企业投资的关键。
第四，如有可能，我希望能让物联网应用的开发变得更简单一点。
我在之前的一篇加餐里，介绍过工控领域软件开发的情况。工控领域的技术原来叫做OT，它们的技术跟IT是不一样的。但在我看来，现在很多IT技术可以进入OT领域。比如，现在儿童编程都可以用一个图形化开发工具控制机器人，这跟控制发电机、控制高铁，其实没有本质的区别。所以，我们应该也可以把这两个领域的开发工具打通才对。就算是OT强调高可靠性，但其实，IT里现在就有更高可靠性的技术，比如我前面提到过的Erlang的9个9的可靠性。
另外，OT的技术生态，原来都把控在少数外国的企业手里。我觉得，如有可能，我们最好可以搞搞破坏，把它搞成一个开放的生态。</description></item><item><title>43_拓扑排序：如何确定代码源文件的编译依赖关系？</title><link>https://artisanbox.github.io/2/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/44/</guid><description>从今天开始，我们就进入了专栏的高级篇。相对基础篇，高级篇涉及的知识点，都比较零散，不是太系统。所以，我会围绕一个实际软件开发的问题，在阐述具体解决方法的过程中，将涉及的知识点给你详细讲解出来。
所以，相较于基础篇“开篇问题-知识讲解-回答开篇-总结-课后思考”这样的文章结构，高级篇我稍作了些改变，大致分为这样几个部分：“问题阐述-算法解析-总结引申-课后思考”。
好了，现在，我们就进入高级篇的第一节，如何确定代码源文件的编译依赖关系？
我们知道，一个完整的项目往往会包含很多代码源文件。编译器在编译整个项目的时候，需要按照依赖关系，依次编译每个源文件。比如，A.cpp依赖B.cpp，那在编译的时候，编译器需要先编译B.cpp，才能编译A.cpp。
编译器通过分析源文件或者程序员事先写好的编译配置文件（比如Makefile文件），来获取这种局部的依赖关系。那编译器又该如何通过源文件两两之间的局部依赖关系，确定一个全局的编译顺序呢？
算法解析这个问题的解决思路与“图”这种数据结构的一个经典算法“拓扑排序算法”有关。那什么是拓扑排序呢？这个概念很好理解，我们先来看一个生活中的拓扑排序的例子。
我们在穿衣服的时候都有一定的顺序，我们可以把这种顺序想成，衣服与衣服之间有一定的依赖关系。比如说，你必须先穿袜子才能穿鞋，先穿内裤才能穿秋裤。假设我们现在有八件衣服要穿，它们之间的两两依赖关系我们已经很清楚了，那如何安排一个穿衣序列，能够满足所有的两两之间的依赖关系？
这就是个拓扑排序问题。从这个例子中，你应该能想到，在很多时候，拓扑排序的序列并不是唯一的。你可以看我画的这幅图，我找到了好几种满足这些局部先后关系的穿衣序列。
弄懂了这个生活中的例子，开篇的关于编译顺序的问题，你应该也有思路了。开篇问题跟这个问题的模型是一样的，也可以抽象成一个拓扑排序问题。
拓扑排序的原理非常简单，我们的重点应该放到拓扑排序的实现上面。
我前面多次讲过，算法是构建在具体的数据结构之上的。针对这个问题，我们先来看下，如何将问题背景抽象成具体的数据结构？
我们可以把源文件与源文件之间的依赖关系，抽象成一个有向图。每个源文件对应图中的一个顶点，源文件之间的依赖关系就是顶点之间的边。
如果a先于b执行，也就是说b依赖于a，那么就在顶点a和顶点b之间，构建一条从a指向b的边。而且，这个图不仅要是有向图，还要是一个有向无环图，也就是不能存在像a-&amp;gt;b-&amp;gt;c-&amp;gt;a这样的循环依赖关系。因为图中一旦出现环，拓扑排序就无法工作了。实际上，拓扑排序本身就是基于有向无环图的一个算法。
public class Graph { private int v; // 顶点的个数 private LinkedList&amp;lt;Integer&amp;gt; adj[]; // 邻接表 public Graph(int v) { this.v = v; adj = new LinkedList[v]; for (int i=0; i&amp;lt;v; ++i) { adj[i] = new LinkedList&amp;lt;&amp;gt;(); } }
public void addEdge(int s, int t) { // s先于t，边s-&amp;gt;t adj[s].add(t); } } 数据结构定义好了，现在，我们来看，如何在这个有向无环图上，实现拓扑排序？
拓扑排序有两种实现方法，都不难理解。它们分别是Kahn算法和DFS深度优先搜索算法。我们依次来看下它们都是怎么工作的。
1.Kahn算法Kahn算法实际上用的是贪心算法思想，思路非常简单、好懂。
定义数据结构的时候，如果s需要先于t执行，那就添加一条s指向t的边。所以，如果某个顶点入度为0， 也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。
我们先从图中，找出一个入度为0的顶点，将其输出到拓扑排序的结果序列中（对应代码中就是把它打印出来），并且把这个顶点从图中删除（也就是把这个顶点可达的顶点的入度都减1）。我们循环执行上面的过程，直到所有的顶点都被输出。最后输出的序列，就是满足局部依赖关系的拓扑排序。
我把Kahn算法用代码实现了一下，你可以结合着文字描述一块看下。不过，你应该能发现，这段代码实现更有技巧一些，并没有真正删除顶点的操作。代码中有详细的注释，你自己来看，我就不多解释了。</description></item><item><title>43_要不要使用分区表？</title><link>https://artisanbox.github.io/1/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/43/</guid><description>我经常被问到这样一个问题：分区表有什么问题，为什么公司规范不让使用分区表呢？今天，我们就来聊聊分区表的使用行为，然后再一起回答这个问题。
分区表是什么？为了说明分区表的组织形式，我先创建一个表t：
CREATE TABLE `t` ( `ftime` datetime NOT NULL, `c` int(11) DEFAULT NULL, KEY (`ftime`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 PARTITION BY RANGE (YEAR(ftime)) (PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB, PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB, PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB, PARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB); insert into t values('2017-4-1',1),('2018-4-1',1); 图1 表t的磁盘文件我在表t中初始化插入了两行记录，按照定义的分区规则，这两行记录分别落在p_2018和p_2019这两个分区上。
可以看到，这个表包含了一个.frm文件和4个.ibd文件，每个分区对应一个.ibd文件。也就是说：
对于引擎层来说，这是4个表； 对于Server层来说，这是1个表。 你可能会觉得这两句都是废话。其实不然，这两句话非常重要，可以帮我们理解分区表的执行逻辑。</description></item><item><title>43_输入输出设备：我们并不是只能用灯泡显示“0”和“1”</title><link>https://artisanbox.github.io/4/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/43/</guid><description>我们在前面的章节搭建了最简单的电路，在这里面，计算机的输入设备就是一个一个开关，输出设备呢，是一个一个灯泡。的确，早期发展的时候，计算机的核心是做“计算”。我们从“计算机”这个名字上也能看出这一点。不管是中文名字“计算机”，还是英文名字“Computer”，核心都是在”计算“这两个字上。不过，到了今天，这些“计算”的工作，更多的是一个幕后工作。
我们无论是使用自己的PC，还是智能手机，大部分时间都是在和计算机进行各种“交互操作”。换句话说，就是在和输入输出设备打交道。这些输入输出设备也不再是一个一个开关，或者一个一个灯泡。你在键盘上直接敲击的都是字符，而不是“0”和“1”，你在显示器上看到的，也是直接的图形或者文字的画面，而不是一个一个闪亮或者关闭的灯泡。想要了解这其中的关窍，那就请你和我一起来看一看，计算机里面的输入输出设备。
接口和设备：经典的适配器模式我们在前面讲解计算机的五大组成部分的时候，我看到这样几个留言。
一个同学问，像蓝牙、WiFi无线网卡这样的设备也是输入输出设备吗？还有一个同学问，我们的输入输出设备的寄存器在哪里？到底是在主板上，还是在硬件设备上？
这两个问题问得很好。其实你只要理解了这两个问题，也就理解输入输出设备是怎么回事儿了。
实际上，输入输出设备，并不只是一个设备。大部分的输入输出设备，都有两个组成部分。第一个是它的接口（Interface），第二个才是实际的I/O设备（Actual I/O Device）。我们的硬件设备并不是直接接入到总线上和CPU通信的，而是通过接口，用接口连接到总线上，再通过总线和CPU通信。
图片来源SATA硬盘，上面的整个绿色电路板和黄色的齿状部分就是接口电路，黄色齿状的就是和主板对接的接口，绿色的电路板就是控制电路你平时听说的并行接口（Parallel Interface）、串行接口（Serial Interface）、USB接口，都是计算机主板上内置的各个接口。我们的实际硬件设备，比如，使用并口的打印机、使用串口的老式鼠标或者使用USB接口的U盘，都要插入到这些接口上，才能和CPU工作以及通信的。
接口本身就是一块电路板。CPU其实不是和实际的硬件设备打交道，而是和这个接口电路板打交道。我们平时说的，设备里面有三类寄存器，其实都在这个设备的接口电路上，而不在实际的设备上。
那这三类寄存器是哪三类寄存器呢？它们分别是状态寄存器（Status Register）、 命令寄存器（Command Register）以及数据寄存器（Data Register），
除了内置在主板上的接口之外，有些接口可以集成在设备上。你可能都没有见过老一点儿的硬盘，我来简单给你介绍一下。
上世纪90年代的时候，大家用的硬盘都叫作IDE硬盘。这个IDE不是像IntelliJ或者WebStorm这样的软件开发集成环境（Integrated Development Environment）的IDE，而是代表着集成设备电路（Integrated Device Electronics）。也就是说，设备的接口电路直接在设备上，而不在主板上。我们需要通过一个线缆，把集成了接口的设备连接到主板上去。
我自己使用的PC的设备管理器把接口和实际设备分离，这个做法实际上来自于计算机走向开放架构（Open Architecture）的时代。
当我们要对计算机升级，我们不会扔掉旧的计算机，直接买一台全新的计算机，而是可以单独升级硬盘这样的设备。我们把老硬盘从接口上拿下来，换一个新的上去就好了。各种输入输出设备的制造商，也可以根据接口的控制协议，来设计和制造硬盘、鼠标、键盘、打印机乃至其他种种外设。正是这样的分工协作，带来了PC时代的繁荣。
其实，在软件的设计模式里也有这样的思路。面向对象里的面向接口编程的接口，就是Interface。如果你做iOS的开发，Objective-C里面的Protocol其实也是这个意思。而Adaptor设计模式，更是一个常见的、用来解决不同外部应用和系统“适配”问题的方案。可以看到，计算机的软件和硬件，在逻辑抽象上，其实是相通的。
如果你用的是Windows操作系统，你可以打开设备管理器，里面有各种各种的Devices（设备）、Controllers（控制器）、Adaptors（适配器）。这些，其实都是对于输入输出设备不同角度的描述。被叫作Devices，看重的是实际的I/O设备本身。被叫作Controllers，看重的是输入输出设备接口里面的控制电路。而被叫作Adaptors，则是看重接口作为一个适配器后面可以插上不同的实际设备。
CPU是如何控制I/O设备的？无论是内置在主板上的接口，还是集成在设备上的接口，除了三类寄存器之外，还有对应的控制电路。正是通过这个控制电路，CPU才能通过向这个接口电路板传输信号，来控制实际的硬件。
我们先来看一看，硬件设备上的这些寄存器有什么用。这里，我拿我们平时用的打印机作为例子。
首先是数据寄存器（Data Register）。CPU向I/O设备写入需要传输的数据，比如要打印的内容是“GeekTime”，我们就要先发送一个“G”给到对应的I/O设备。 然后是命令寄存器（Command Register）。CPU发送一个命令，告诉打印机，要进行打印工作。这个时候，打印机里面的控制电路会做两个动作。第一个，是去设置我们的状态寄存器里面的状态，把状态设置成not-ready。第二个，就是实际操作打印机进行打印。 而状态寄存器（Status Register），就是告诉了我们的CPU，现在设备已经在工作了，所以这个时候，CPU你再发送数据或者命令过来，都是没有用的。直到前面的动作已经完成，状态寄存器重新变成了ready状态，我们的CPU才能发送下一个字符和命令。 当然，在实际情况中，打印机里通常不只有数据寄存器，还会有数据缓冲区。我们的CPU也不是真的一个字符一个字符这样交给打印机去打印的，而是一次性把整个文档传输到打印机的内存或者数据缓冲区里面一起打印的。不过，通过上面这个例子，相信你对CPU是怎么操作I/O设备的，应该有所了解了。
信号和地址：发挥总线的价值搞清楚了实际的I/O设备和接口之间的关系，一个新的问题就来了。那就是，我们的CPU到底要往总线上发送一个什么样的命令，才能和I/O接口上的设备通信呢？
CPU和I/O设备的通信，一样是通过CPU支持的机器指令来执行的。
如果你回头去看一看第5讲，MIPS的机器指令的分类，你会发现，我们并没有一种专门的和I/O设备通信的指令类型。那么，MIPS的CPU到底是通过什么样的指令来和I/O设备来通信呢？
答案就是，和访问我们的主内存一样，使用“内存地址”。为了让已经足够复杂的CPU尽可能简单，计算机会把I/O设备的各个寄存器，以及I/O设备内部的内存地址，都映射到主内存地址空间里来。主内存的地址空间里，会给不同的I/O设备预留一段一段的内存地址。CPU想要和这些I/O设备通信的时候呢，就往这些地址发送数据。这些地址信息，就是通过上一讲的地址线来发送的，而对应的数据信息呢，自然就是通过数据线来发送的了。
而我们的I/O设备呢，就会监控地址线，并且在CPU往自己地址发送数据的时候，把对应的数据线里面传输过来的数据，接入到对应的设备里面的寄存器和内存里面来。CPU无论是向I/O设备发送命令、查询状态还是传输数据，都可以通过这样的方式。这种方式呢，叫作内存映射IO（Memory-Mapped I/O，简称MMIO）。
那么，MMIO是不是唯一一种CPU和设备通信的方式呢？答案是否定的。精简指令集MIPS的CPU特别简单，所以这里只有MMIO。而我们有2000多个指令的Intel X86架构的计算机，自然可以设计专门的和I/O设备通信的指令，也就是 in 和 out 指令。
Intel CPU虽然也支持MMIO，不过它还可以通过特定的指令，来支持端口映射I/O（Port-Mapped I/O，简称PMIO）或者也可以叫独立输入输出（Isolated I/O）。
其实PMIO的通信方式和MMIO差不多，核心的区别在于，PMIO里面访问的设备地址，不再是在内存地址空间里面，而是一个专门的端口（Port）。这个端口并不是指一个硬件上的插口，而是和CPU通信的一个抽象概念。
无论是PMIO还是MMIO，CPU都会传送一条二进制的数据，给到I/O设备的对应地址。设备自己本身的接口电路，再去解码这个数据。解码之后的数据呢，就会变成设备支持的一条指令，再去通过控制电路去操作实际的硬件设备。对于CPU来说，它并不需要关心设备本身能够支持哪些操作。它要做的，只是在总线上传输一条条数据就好了。
这个，其实也有点像我们在设计模式里面的Command模式。我们在总线上传输的，是一个个数据对象，然后各个接受这些对象的设备，再去根据对象内容，进行实际的解码和命令执行。
这是我计算机上，设备管理器里显卡设备的资源信息这是一张我自己的显卡，在设备管理器里面的资源（Resource）信息。你可以看到，里面既有Memory Range，这个就是设备对应映射到的内存地址，也就是我们上面所说的MMIO的访问方式。同样的，里面还有I/O Range，这个就是我们上面所说的PMIO，也就是通过端口来访问I/O设备的地址。最后，里面还有一个IRQ，也就是会来自于这个设备的中断信号了。
总结延伸好了，讲到这里，不知道，现在你是不是可以把CPU的指令、总线和I/O设备之间的关系彻底串联起来了呢？我来带你回顾一下。
CPU并不是发送一个特定的操作指令来操作不同的I/O设备。因为如果是那样的话，随着新的I/O设备的发明，我们就要去扩展CPU的指令集了。
在计算机系统里面，CPU和I/O设备之间的通信，是这么来解决的。
首先，在I/O设备这一侧，我们把I/O设备拆分成，能和CPU通信的接口电路，以及实际的I/O设备本身。接口电路里面有对应的状态寄存器、命令寄存器、数据寄存器、数据缓冲区和设备内存等等。接口电路通过总线和CPU通信，接收来自CPU的指令和数据。而接口电路中的控制电路，再解码接收到的指令，实际去操作对应的硬件设备。
而在CPU这一侧，对CPU来说，它看到的并不是一个个特定的设备，而是一个个内存地址或者端口地址。CPU只是向这些地址传输数据或者读取数据。所需要的指令和操作内存地址的指令其实没有什么本质差别。通过软件层面对于传输的命令数据的定义，而不是提供特殊的新的指令，来实际操作对应的I/O硬件。
推荐阅读想要进一步了解CPU和I/O设备交互的技术细节，我推荐你去看一看北京大学在Coursera上的视频课程，《计算机组成》第10周的内容。这个课程在Coursera上是中文的，而且可以免费观看。相信这一个小时的视频课程，对于你深入理解输入输出设备，会很有帮助。
课后思考我们还是回到，这节开始的时候同学留言的问题。如果你买的是一个带无线接收器的蓝牙鼠标，你需要把蓝牙接收器插在电脑的USB接口上，然后你的鼠标会和这个蓝牙接收器进行通信。那么，你能想一下，我们的CPU和蓝牙鼠标这个输入设备之间的通信是怎样的吗？
你可以好好思考一下，然后在留言区写下你的想法。当然，你也可以把这个问题分享给你的朋友，拉上他一起学习。</description></item><item><title>44_最短路径：地图软件是如何计算出最优出行路径的？</title><link>https://artisanbox.github.io/2/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/45/</guid><description>基础篇的时候，我们学习了图的两种搜索算法，深度优先搜索和广度优先搜索。这两种算法主要是针对无权图的搜索算法。针对有权图，也就是图中的每条边都有一个权重，我们该如何计算两点之间的最短路径（经过的边的权重和最小）呢？今天，我就从地图软件的路线规划问题讲起，带你看看常用的最短路径算法（Shortest Path Algorithm）。
像Google地图、百度地图、高德地图这样的地图软件，我想你应该经常使用吧？如果想从家开车到公司，你只需要输入起始、结束地址，地图就会给你规划一条最优出行路线。这里的最优，有很多种定义，比如最短路线、最少用时路线、最少红绿灯路线等等。作为一名软件开发工程师，你是否思考过，地图软件的最优路线是如何计算出来的吗？底层依赖了什么算法呢？
算法解析我们刚提到的最优问题包含三个：最短路线、最少用时和最少红绿灯。我们先解决最简单的，最短路线。
解决软件开发中的实际问题，最重要的一点就是建模，也就是将复杂的场景抽象成具体的数据结构。针对这个问题，我们该如何抽象成数据结构呢？
我们之前也提到过，图这种数据结构的表达能力很强，显然，把地图抽象成图最合适不过了。我们把每个岔路口看作一个顶点，岔路口与岔路口之间的路看作一条边，路的长度就是边的权重。如果路是单行道，我们就在两个顶点之间画一条有向边；如果路是双行道，我们就在两个顶点之间画两条方向不同的边。这样，整个地图就被抽象成一个有向有权图。
具体的代码实现，我放在下面了。于是，我们要求解的问题就转化为，在一个有向有权图中，求两个顶点间的最短路径。
public class Graph { // 有向有权图的邻接表表示 private LinkedList&amp;lt;Edge&amp;gt; adj[]; // 邻接表 private int v; // 顶点个数 public Graph(int v) { this.v = v; this.adj = new LinkedList[v]; for (int i = 0; i &amp;lt; v; ++i) { this.adj[i] = new LinkedList&amp;lt;&amp;gt;(); } }
public void addEdge(int s, int t, int w) { // 添加一条边 this.adj[s].add(new Edge(s, t, w)); }
private class Edge { public int sid; // 边的起始顶点编号 public int tid; // 边的终止顶点编号 public int w; // 权重 public Edge(int sid, int tid, int w) { this.</description></item><item><title>44_理解IO_WAIT：IO性能到底是怎么回事儿？</title><link>https://artisanbox.github.io/4/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/44/</guid><description>在专栏一开始的时候，我和你说过，在计算机组成原理这门课里面，很多设计的核心思路，都来源于性能。在前面讲解CPU的时候，相信你已经有了切身的感受了。
大部分程序员开发的都是应用系统。在开发应用系统的时候，我们遇到的性能瓶颈大部分都在I/O上。在第36讲讲解局部性原理的时候，我们一起看了通过把内存当作是缓存，来提升系统的整体性能。在第37讲讲解CPU Cache的时候，我们一起看了CPU Cache和主内存之间性能的巨大差异。
然而，我们知道，并不是所有问题都能靠利用内存或者CPU Cache做一层缓存来解决。特别是在这个“大数据”的时代。我们在硬盘上存储了越来越多的数据，一个MySQL数据库的单表有个几千万条记录，早已经不算是什么罕见现象了。这也就意味着，用内存当缓存，存储空间是不够用的。大部分时间，我们的请求还是要打到硬盘上。那么，这一讲我们就来看看硬盘I/O性能的事儿。
IO性能、顺序访问和随机访问如果去看硬盘厂商的性能报告，通常你会看到两个指标。一个是响应时间（Response Time），另一个叫作数据传输率（Data Transfer Rate）。没错，这个和我们在专栏的一开始讲的CPU的性能一样，前面那个就是响应时间，后面那个就是吞吐率了。
我们先来看一看后面这个指标，数据传输率。
我们现在常用的硬盘有两种。一种是HDD硬盘，也就是我们常说的机械硬盘。另一种是SSD硬盘，一般也被叫作固态硬盘。现在的HDD硬盘，用的是SATA 3.0的接口。而SSD硬盘呢，通常会用两种接口，一部分用的也是SATA 3.0的接口；另一部分呢，用的是PCI Express的接口。
现在我们常用的SATA 3.0的接口，带宽是6Gb/s。这里的“b”是比特。这个带宽相当于每秒可以传输768MB的数据。而我们日常用的HDD硬盘的数据传输率，差不多在200MB/s左右。
这是在我自己的电脑上，运行AS SSD测算SATA接口SSD硬盘性能的结果，第一行的Seq就是顺序读写硬盘得到的数据传输率的实际结果当我们换成SSD的硬盘，性能自然会好上不少。比如，我最近刚把自己电脑的HDD硬盘，换成了一块Crucial MX500的SSD硬盘。它的数据传输速率能到差不多500MB/s，比HDD的硬盘快了一倍不止。不过SATA接口的硬盘，差不多到这个速度，性能也就到顶了。因为SATA接口的速度也就这么快。
不过，实际SSD硬盘能够更快，所以我们可以换用PCI Express的接口。我自己电脑的系统盘就是一块使用了PCI Express的三星SSD硬盘。它的数据传输率，在读取的时候就能做到2GB/s左右，差不多是HDD硬盘的10倍，而在写入的时候也能有1.2GB/s。
除了数据传输率这个吞吐率指标，另一个我们关心的指标响应时间，其实也可以在AS SSD的测试结果里面看到，就是这里面的Acc.Time指标。
这个指标，其实就是程序发起一个硬盘的写入请求，直到这个请求返回的时间。可以看到，在上面的两块SSD硬盘上，大概时间都是在几十微秒这个级别。如果你去测试一块HDD的硬盘，通常会在几毫秒到十几毫秒这个级别。这个性能的差异，就不是10倍了，而是在几十倍，乃至几百倍。
光看响应时间和吞吐率这两个指标，似乎我们的硬盘性能很不错。即使是廉价的HDD硬盘，接收一个来自CPU的请求，也能够在几毫秒时间返回。一秒钟能够传输的数据，也有200MB左右。你想一想，我们平时往数据库里写入一条记录，也就是1KB左右的大小。我们拿200MB去除以1KB，那差不多每秒钟可以插入20万条数据呢。但是这个计算出来的数字，似乎和我们日常的经验不符合啊？这又是为什么呢？
答案就来自于硬盘的读写。在顺序读写和随机读写的情况下，硬盘的性能是完全不同的。
我们回头看一下上面的AS SSD的性能指标。你会看到，里面有一个“4K”的指标。这个指标是什么意思呢？它其实就是我们的程序，去随机读取磁盘上某一个4KB大小的数据，一秒之内可以读取到多少数据。
你会发现，在这个指标上，我们使用SATA 3.0接口的硬盘和PCI Express接口的硬盘，性能差异变得很小。这是因为，在这个时候，接口本身的速度已经不是我们硬盘访问速度的瓶颈了。更重要的是，你会发现，即使我们用PCI Express的接口，在随机读写的时候，数据传输率也只能到40MB/s左右，是顺序读写情况下的几十分之一。
我们拿这个40MB/s和一次读取4KB的数据算一下。
40MB / 4KB = 10,000也就是说，一秒之内，这块SSD硬盘可以随机读取1万次的4KB的数据。如果是写入的话呢，会更多一些，90MB /4KB 差不多是2万多次。
这个每秒读写的次数，我们称之为IOPS，也就是每秒输入输出操作的次数。事实上，比起响应时间，我们更关注IOPS这个性能指标。IOPS和DTR（Data Transfer Rate，数据传输率）才是输入输出性能的核心指标。
这是因为，我们在实际的应用开发当中，对于数据的访问，更多的是随机读写，而不是顺序读写。我们平时所说的服务器承受的“并发”，其实是在说，会有很多个不同的进程和请求来访问服务器。自然，它们在硬盘上访问的数据，是很难顺序放在一起的。这种情况下，随机读写的IOPS才是服务器性能的核心指标。
好了，回到我们引出IOPS这个问题的HDD硬盘。我现在要问你了，那一块HDD硬盘能够承受的IOPS是多少呢？其实我们应该已经在第36讲说过答案了。
HDD硬盘的IOPS通常也就在100左右，而不是在20万次。在后面讲解机械硬盘的原理和性能优化的时候，我们还会再来一起看一看，这个100是怎么来的，以及我们可以有哪些优化的手段。
如何定位IO_WAIT？我们看到，即使是用上了PCI Express接口的SSD硬盘，IOPS也就是在2万左右。而我们的CPU的主频通常在2GHz以上，也就是每秒可以做20亿次操作。
即使CPU向硬盘发起一条读写指令，需要很多个时钟周期，一秒钟CPU能够执行的指令数，和我们硬盘能够进行的操作数，也有好几个数量级的差异。这也是为什么，我们在应用开发的时候往往会说“性能瓶颈在I/O上”。因为很多时候，CPU指令发出去之后，不得不去“等”我们的I/O操作完成，才能进行下一步的操作。
那么，在实际遇到服务端程序的性能问题的时候，我们怎么知道这个问题是不是来自于CPU等I/O来完成操作呢？别着急，我们接下来，就通过top和iostat这些命令，一起来看看CPU到底有没有在等待io操作。
# top 你一定在Linux下用过 top 命令。对于很多刚刚入门Linux的同学，会用top去看服务的负载，也就是load average。不过，在top命令里面，我们一样可以看到CPU是否在等待IO操作完成。
top - 06:26:30 up 4 days, 53 min, 1 user, load average: 0.</description></item><item><title>44_答疑文章（三）：说一说这些好问题</title><link>https://artisanbox.github.io/1/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/44/</guid><description>这是我们专栏的最后一篇答疑文章，今天我们来说说一些好问题。
在我看来，能够帮我们扩展一个逻辑的边界的问题，就是好问题。因为通过解决这样的问题，能够加深我们对这个逻辑的理解，或者帮我们关联到另外一个知识点，进而可以帮助我们建立起自己的知识网络。
在工作中会问好问题，是一个很重要的能力。
经过这段时间的学习，从评论区的问题我可以感觉出来，紧跟课程学习的同学，对SQL语句执行性能的感觉越来越好了，提出的问题也越来越细致和精准了。
接下来，我们就一起看看同学们在评论区提到的这些好问题。在和你一起分析这些问题的时候，我会指出它们具体是在哪篇文章出现的。同时，在回答这些问题的过程中，我会假设你已经掌握了这篇文章涉及的知识。当然，如果你印象模糊了，也可以跳回文章再复习一次。
join的写法在第35篇文章《join语句怎么优化？》中，我在介绍join执行顺序的时候，用的都是straight_join。@郭健 同学在文后提出了两个问题：
如果用left join的话，左边的表一定是驱动表吗？
如果两个表的join包含多个条件的等值匹配，是都要写到on里面呢，还是只把一个条件写到on里面，其他条件写到where部分？
为了同时回答这两个问题，我来构造两个表a和b：
create table a(f1 int, f2 int, index(f1))engine=innodb; create table b(f1 int, f2 int)engine=innodb; insert into a values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6); insert into b values(3,3),(4,4),(5,5),(6,6),(7,7),(8,8); 表a和b都有两个字段f1和f2，不同的是表a的字段f1上有索引。然后，我往两个表中都插入了6条记录，其中在表a和b中同时存在的数据有4行。
@郭健 同学提到的第二个问题，其实就是下面这两种写法的区别：
select * from a left join b on(a.f1=b.f1) and (a.f2=b.f2); /*Q1*/ select * from a left join b on(a.f1=b.f1) where (a.f2=b.f2);/*Q2*/ 我把这两条语句分别记为Q1和Q2。
首先，需要说明的是，这两个left join语句的语义逻辑并不相同。我们先来看一下它们的执行结果。
图1 两个join的查询结果可以看到：
语句Q1返回的数据集是6行，表a中即使没有满足匹配条件的记录，查询结果中也会返回一行，并将表b的各个字段值填成NULL。 语句Q2返回的是4行。从逻辑上可以这么理解，最后的两行，由于表b中没有匹配的字段，结果集里面b.f2的值是空，不满足where 部分的条件判断，因此不能作为结果集的一部分。 接下来，我们看看实际执行这两条语句时，MySQL是怎么做的。</description></item><item><title>45_位图：如何实现网页爬虫中的URL去重功能？</title><link>https://artisanbox.github.io/2/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/46/</guid><description>网页爬虫是搜索引擎中的非常重要的系统，负责爬取几十亿、上百亿的网页。爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。而同一个网页链接有可能被包含在多个页面中，这就会导致爬虫在爬取的过程中，重复爬取相同的网页。如果你是一名负责爬虫的工程师，你会如何避免这些重复的爬取呢？
最容易想到的方法就是，我们记录已经爬取的网页链接（也就是URL），在爬取一个新的网页之前，我们拿它的链接，在已经爬取的网页链接列表中搜索。如果存在，那就说明这个网页已经被爬取过了；如果不存在，那就说明这个网页还没有被爬取过，可以继续去爬取。等爬取到这个网页之后，我们将这个网页的链接添加到已经爬取的网页链接列表了。
思路非常简单，我想你应该很容易就能想到。不过，我们该如何记录已经爬取的网页链接呢？需要用什么样的数据结构呢？
算法解析关于这个问题，我们可以先回想下，是否可以用我们之前学过的数据结构来解决呢？
这个问题要处理的对象是网页链接，也就是URL，需要支持的操作有两个，添加一个URL和查询一个URL。除了这两个功能性的要求之外，在非功能性方面，我们还要求这两个操作的执行效率要尽可能高。除此之外，因为我们处理的是上亿的网页链接，内存消耗会非常大，所以在存储效率上，我们要尽可能地高效。
我们回想一下，满足这些条件的数据结构有哪些呢？显然，散列表、红黑树、跳表这些动态数据结构，都能支持快速地插入、查找数据，但是在内存消耗方面，是否可以接受呢？
我们拿散列表来举例。假设我们要爬取10亿个网页（像Google、百度这样的通用搜索引擎，爬取的网页可能会更多），为了判重，我们把这10亿网页链接存储在散列表中。你来估算下，大约需要多少内存？
假设一个URL的平均长度是64字节，那单纯存储这10亿个URL，需要大约60GB的内存空间。因为散列表必须维持较小的装载因子，才能保证不会出现过多的散列冲突，导致操作的性能下降。而且，用链表法解决冲突的散列表，还会存储链表指针。所以，如果将这10亿个URL构建成散列表，那需要的内存空间会远大于60GB，有可能会超过100GB。
当然，对于一个大型的搜索引擎来说，即便是100GB的内存要求，其实也不算太高，我们可以采用分治的思想，用多台机器（比如20台内存是8GB的机器）来存储这10亿网页链接。这种分治的处理思路，我们讲过很多次了，这里就不详细说了。
对于爬虫的URL去重这个问题，刚刚讲到的分治加散列表的思路，已经是可以实实在在工作的了。不过，作为一个有追求的工程师，我们应该考虑，在添加、查询数据的效率以及内存消耗方面，是否还有进一步的优化空间呢？
你可能会说，散列表中添加、查找数据的时间复杂度已经是O(1)，还能有进一步优化的空间吗？实际上，我们前面也讲过，时间复杂度并不能完全代表代码的执行时间。大O时间复杂度表示法，会忽略掉常数、系数和低阶，并且统计的对象是语句的频度。不同的语句，执行时间也是不同的。时间复杂度只是表示执行时间随数据规模的变化趋势，并不能度量在特定的数据规模下，代码执行时间的多少。
如果时间复杂度中原来的系数是10，我们现在能够通过优化，将系数降为1，那在时间复杂度没有变化的情况下，执行效率就提高了10倍。对于实际的软件开发来说，10倍效率的提升，显然是一个非常值得的优化。
如果我们用基于链表的方法解决冲突问题，散列表中存储的是URL，那当查询的时候，通过哈希函数定位到某个链表之后，我们还需要依次比对每个链表中的URL。这个操作是比较耗时的，主要有两点原因。
一方面，链表中的结点在内存中不是连续存储的，所以不能一下子加载到CPU缓存中，没法很好地利用到CPU高速缓存，所以数据访问性能方面会打折扣。
另一方面，链表中的每个数据都是URL，而URL不是简单的数字，是平均长度为64字节的字符串。也就是说，我们要让待判重的URL，跟链表中的每个URL，做字符串匹配。显然，这样一个字符串匹配操作，比起单纯的数字比对，要慢很多。所以，基于这两点，执行效率方面肯定是有优化空间的。
对于内存消耗方面的优化，除了刚刚这种基于散列表的解决方案，貌似没有更好的法子了。实际上，如果要想内存方面有明显的节省，那就得换一种解决方案，也就是我们今天要着重讲的这种存储结构，布隆过滤器（Bloom Filter）。
在讲布隆过滤器前，我要先讲一下另一种存储结构，位图（BitMap）。因为，布隆过滤器本身就是基于位图的，是对位图的一种改进。
我们先来看一个跟开篇问题非常类似、但比那个稍微简单的问题。我们有1千万个整数，整数的范围在1到1亿之间。如何快速查找某个整数是否在这1千万个整数中呢？
当然，这个问题还是可以用散列表来解决。不过，我们可以使用一种比较“特殊”的散列表，那就是位图。我们申请一个大小为1亿、数据类型为布尔类型（true或者false）的数组。我们将这1千万个整数作为数组下标，将对应的数组值设置成true。比如，整数5对应下标为5的数组值设置为true，也就是array[5]=true。
当我们查询某个整数K是否在这1千万个整数中的时候，我们只需要将对应的数组值array[K]取出来，看是否等于true。如果等于true，那说明1千万整数中包含这个整数K；相反，就表示不包含这个整数K。
不过，很多语言中提供的布尔类型，大小是1个字节的，并不能节省太多内存空间。实际上，表示true和false两个值，我们只需要用一个二进制位（bit）就可以了。那如何通过编程语言，来表示一个二进制位呢？
这里就要用到位运算了。我们可以借助编程语言中提供的数据类型，比如int、long、char等类型，通过位运算，用其中的某个位表示某个数字。文字描述起来有点儿不好理解，我把位图的代码实现写了出来，你可以对照着代码看下，应该就能看懂了。
public class BitMap { // Java中char类型占16bit，也即是2个字节 private char[] bytes; private int nbits; public BitMap(int nbits) { this.nbits = nbits; this.bytes = new char[nbits/16+1]; }
public void set(int k) { if (k &amp;gt; nbits) return; int byteIndex = k / 16; int bitIndex = k % 16; bytes[byteIndex] |= (1 &amp;lt;&amp;lt; bitIndex); }</description></item><item><title>45_机械硬盘：Google早期用过的“黑科技”</title><link>https://artisanbox.github.io/4/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/45/</guid><description>在1991年，我刚接触计算机的时候，很多计算机还没有硬盘。整个操作系统都安装在5寸或者3.5寸的软盘里。不过，很快大部分计算机都开始用上了直接安装在主板上的机械硬盘。到了今天，更早的软盘早已经被淘汰了。在个人电脑和服务器里，更晚出现的光盘也已经很少用了。
机械硬盘的生命力仍然非常顽强。无论是作为个人电脑的数据盘，还是在数据中心里面用作海量数据的存储，机械硬盘仍然在被大量使用。不仅如此，随着成本的不断下降，机械硬盘还替代掉了很多传统的存储设备，比如，以前常常用来备份冷数据的磁带。
那这一讲里，我们就从机械硬盘的物理构造开始，从原理到应用剖析一下，看看我们可以怎么样用好机械硬盘。
拆解机械硬盘上一讲里，我们提到过机械硬盘的IOPS。我们说，机械硬盘的IOPS，大概只能做到每秒100次左右。那么，这个100次究竟是怎么来的呢？
我们把机械硬盘拆开来看一看，看看它的物理构造是怎么样的，你就自然知道为什么它的IOPS是100左右了。
我们之前看过整个硬盘的构造，里面有接口，有对应的控制电路版，以及实际的I/O设备（也就是我们的机械硬盘）。这里，我们就拆开机械硬盘部分来看一看。
图片来源一块机械硬盘是由盘面、磁头和悬臂三个部件组成的。下面我们一一来看每一个部件。
首先，自然是盘面（Disk Platter）。盘面其实就是我们实际存储数据的盘片。如果你剪开过软盘的外壳，或者看过光盘DVD，那你看到盘面应该很熟悉。盘面其实和它们长得差不多。
盘面本身通常是用的铝、玻璃或者陶瓷这样的材质做成的光滑盘片。然后，盘面上有一层磁性的涂层。我们的数据就存储在这个磁性的涂层上。盘面中间有一个受电机控制的转轴。这个转轴会控制我们的盘面去旋转。
我们平时买硬盘的时候经常会听到一个指标，叫作这个硬盘的转速。我们的硬盘有5400转的、7200转的，乃至10000转的。这个多少多少转，指的就是盘面中间电机控制的转轴的旋转速度，英文单位叫RPM，也就是每分钟的旋转圈数（Rotations Per Minute）。所谓7200转，其实更准确地说是7200RPM，指的就是一旦电脑开机供电之后，我们的硬盘就可以一直做到每分钟转上7200圈。如果折算到每一秒钟，就是120圈。
说完了盘面，我们来看磁头（Drive Head）。我们的数据并不能直接从盘面传输到总线上，而是通过磁头，从盘面上读取到，然后再通过电路信号传输给控制电路、接口，再到总线上的。
通常，我们的一个盘面上会有两个磁头，分别在盘面的正反面。盘面在正反两面都有对应的磁性涂层来存储数据，而且一块硬盘也不是只有一个盘面，而是上下堆叠了很多个盘面，各个盘面之间是平行的。每个盘面的正反两面都有对应的磁头。
最后我们来看悬臂（Actutor Arm）。悬臂链接在磁头上，并且在一定范围内会去把磁头定位到盘面的某个特定的磁道（Track）上。这个磁道是怎么来呢？想要了解这个问题，我们要先看一看我们的数据是怎么存放在盘面上的。
一个盘面通常是圆形的，由很多个同心圆组成，就好像是一个个大小不一样的“甜甜圈”嵌套在一起。每一个“甜甜圈”都是一个磁道。每个磁道都有自己的一个编号。悬臂其实只是控制，到底是读最里面那个“甜甜圈”的数据，还是最外面“甜甜圈”的数据。
图片来源知道了我们硬盘的物理构成，现在我们就可以看一看，这样的物理结构，到底是怎么来读取数据的。
我们刚才说的一个磁道，会分成一个一个扇区（Sector）。上下平行的一个一个盘面的相同扇区呢，我们叫作一个柱面（Cylinder）。
读取数据，其实就是两个步骤。一个步骤，就是把盘面旋转到某一个位置。在这个位置上，我们的悬臂可以定位到整个盘面的某一个子区间。这个子区间的形状有点儿像一块披萨饼，我们一般把这个区间叫作几何扇区（Geometrical Sector），意思是，在“几何位置上”，所有这些扇区都可以被悬臂访问到。另一个步骤，就是把我们的悬臂移动到特定磁道的特定扇区，也就在这个“几何扇区”里面，找到我们实际的扇区。找到之后，我们的磁头会落下，就可以读取到正对着扇区的数据。
所以，我们进行一次硬盘上的随机访问，需要的时间由两个部分组成。
第一个部分，叫作平均延时（Average Latency）。这个时间，其实就是把我们的盘面旋转，把几何扇区对准悬臂位置的时间。这个时间很容易计算，它其实就和我们机械硬盘的转速相关。随机情况下，平均找到一个几何扇区，我们需要旋转半圈盘面。上面7200转的硬盘，那么一秒里面，就可以旋转240个半圈。那么，这个平均延时就是
1s / 240 = 4.17ms第二个部分，叫作平均寻道时间（Average Seek Time），也就是在盘面选转之后，我们的悬臂定位到扇区的的时间。我们现在用的HDD硬盘的平均寻道时间一般在4-10ms。
这样，我们就能够算出来，如果随机在整个硬盘上找一个数据，需要 8-14 ms。我们的硬盘是机械结构的，只有一个电机转轴，也只有一个悬臂，所以我们没有办法并行地去定位或者读取数据。那一块7200转的硬盘，我们一秒钟随机的IO访问次数，也就是
1s / 8 ms = 125 IOPS 或者 1s / 14ms = 70 IOPS现在，你明白我们上一讲所说的，HDD硬盘的IOPS每秒100次左右是怎么来的吧？好了，现在你再思考一个问题。如果我们不是去进行随机的数据访问，而是进行顺序的数据读写，我们应该怎么最大化读取效率呢？
我们可以选择把顺序存放的数据，尽可能地存放在同一个柱面上。这样，我们只需要旋转一次盘面，进行一次寻道，就可以去写入或者读取，同一个垂直空间上的多个盘面的数据。如果一个柱面上的数据不够，我们也不要去动悬臂，而是通过电机转动盘面，这样就可以顺序读完一个磁道上的所有数据。所以，其实对于HDD硬盘的顺序数据读写，吞吐率还是很不错的，可以达到200MB/s左右。
Partial Stroking：根据场景提升性能只有100的IOPS，其实很难满足现在互联网海量高并发的请求。所以，今天的数据库，都会把数据存储在SSD硬盘上。不过，如果我们把时钟倒播20年，那个时候，我们可没有现在这么便宜的SSD硬盘。数据库里面的数据，只能存放在HDD硬盘上。
今天，即便是数据中心用的HDD硬盘，一般也是7200转的，因为如果要更快的随机访问速度，我们会选择用SSD硬盘。但是在当时，SSD硬盘价格非常昂贵，还没有能够商业化。硬盘厂商们在不断地研发转得更快的硬盘。在数据中心里，往往我们会用上10000转，乃至15000转的硬盘。甚至直到2010年，SSD硬盘已经开始逐步进入市场了，西数还在尝试研发20000转的硬盘。转速更高、寻道时间更短的机械硬盘，才能满足实际的数据库需求。
不过，10000转，乃至15000转的硬盘也更昂贵。如果你想要节约成本，提高性价比，那就得想点别的办法。你应该听说过，Google早年用家用PC乃至二手的硬件，通过软件层面的设计来解决可靠性和性能的问题。那么，我们是不是也有什么办法，能提高机械硬盘的IOPS呢？
还真的有。这个方法，就叫作Partial Stroking或者Short Stroking。我没有看到过有中文资料给这个方法命名。在这里，我就暂时把它翻译成“缩短行程”技术。
其实这个方法的思路很容易理解，我一说你就明白了。既然我们访问一次数据的时间，是“平均延时+寻道时间”，那么只要能缩短这两个之一，不就可以提升IOPS了吗？
一般情况下，硬盘的寻道时间都比平均延时要长。那么我们自然就可以想一下，有什么办法可以缩短平均的寻道时间。最极端的办法就是我们不需要寻道，也就是说，我们把所有数据都放在一个磁道上。比如，我们始终把磁头放在最外道的磁道上。这样，我们的寻道时间就基本为0，访问时间就只有平均延时了。那样，我们的IOPS，就变成了
1s / 4ms = 250 IOPS不过呢，只用一个磁道，我们能存的数据就比较有限了。这个时候，可能我们还不如把这些数据直接都放到内存里面呢。所以，实践当中，我们可以只用1/2或者1/4的磁道，也就是最外面1/4或者1/2的磁道。这样，我们硬盘可以使用的容量可能变成了1/2或者1/4。但是呢，我们的寻道时间，也变成了1/4或者1/2，因为悬臂需要移动的“行程”也变成了原来的1/2或者1/4，我们的IOPS就能够大幅度提升了。
比如说，我们一块7200转的硬盘，正常情况下，平均延时是4.17ms，而寻道时间是9ms。那么，它原本的IOPS就是
1s / (4.</description></item><item><title>45_自增id用完怎么办？</title><link>https://artisanbox.github.io/1/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/45/</guid><description>MySQL里有很多自增的id，每个自增id都是定义了初始值，然后不停地往上加步长。虽然自然数是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无符号整型(unsigned int)是4个字节，上限就是232-1。
既然自增id有上限，就有可能被用完。但是，自增id用完了会怎么样呢？
今天这篇文章，我们就来看看MySQL里面的几种自增id，一起分析一下它们的值达到上限以后，会出现什么情况。
表定义自增值id说到自增id，你第一个想到的应该就是表结构定义里的自增字段，也就是我在第39篇文章《自增主键为什么不是连续的？》中和你介绍过的自增主键id。
表定义的自增值达到上限后的逻辑是：再申请下一个id时，得到的值保持不变。
我们可以通过下面这个语句序列验证一下：
create table t(id int unsigned auto_increment primary key) auto_increment=4294967295; insert into t values(null); //成功插入一行 4294967295 show create table t; /* CREATE TABLE `t` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4294967295; */ insert into t values(null); //Duplicate entry &amp;lsquo;4294967295&amp;rsquo; for key &amp;lsquo;PRIMARY&amp;rsquo; 可以看到，第一个insert语句插入数据成功后，这个表的AUTO_INCREMENT没有改变（还是4294967295），就导致了第二个insert语句又拿到相同的自增id值，再试图执行插入语句，报主键冲突错误。
232-1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创建成8个字节的bigint unsigned。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;InnoDB系统自增row_id如果你创建的InnoDB表没有指定主键，那么InnoDB会给你创建一个不可见的，长度为6个字节的row_id。InnoDB维护了一个全局的dict_sys.row_id值，所有无主键的InnoDB表，每插入一行数据，都将当前的dict_sys.row_id值作为要插入数据的row_id，然后把dict_sys.row_id的值加1。
实际上，在代码实现时row_id是一个长度为8字节的无符号长整型(bigint unsigned)。但是，InnoDB在设计时，给row_id留的只是6个字节的长度，这样写到数据表中时只放了最后6个字节，所以row_id能写到数据表中的值，就有两个特征：
row_id写入表中的值范围，是从0到248-1；
当dict_sys.row_id=248时，如果再有插入数据的行为要来申请row_id，拿到以后再取最后6个字节的话就是0。
也就是说，写入表的row_id是从0开始到248-1。达到上限后，下一个值就是0，然后继续循环。</description></item><item><title>46_SSD硬盘（上）：如何完成性能优化的KPI？</title><link>https://artisanbox.github.io/4/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/46/</guid><description>随着3D垂直封装技术和QLC技术的出现，今年的“618”，SSD硬盘的价格进一步大跳水，趁着这个机会，我把自己电脑上的仓库盘，从HDD换成了SSD硬盘。我的个人电脑彻底摆脱了机械硬盘。
随着智能手机的出现，互联网用户在2008年之后开始爆发性增长，大家在网上花的时间也越来越多。这也就意味着，隐藏在精美App和网页之后的服务端数据请求量，呈数量级的上升。
无论是用10000转的企业级机械硬盘，还是用Short Stroking这样的方式进一步提升IOPS，HDD硬盘已经满足不了我们的需求了。上面这些优化措施，无非就是，把IOPS从100提升到300、500也就到头了。
于是，SSD硬盘在2010年前后，进入了主流的商业应用。我们在第44讲看过，一块普通的SSD硬盘，可以轻松支撑10000乃至20000的IOPS。那个时候，不少互联网公司想要完成性能优化的KPI，最后的解决方案都变成了换SSD的硬盘。如果这还不够，那就换上使用PCI Express接口的SSD。
不过，只是简单地换一下SSD硬盘，真的最大限度地用好了SSD硬盘吗？另外，即便现在SSD硬盘很便宜了，大部分公司的批量数据处理系统，仍然在用传统的机械硬盘，这又是为什么呢？
那么接下来这两讲，就请你和我一起来看一看，SSD硬盘的工作原理，以及怎么最大化利用SSD的工作原理，使得访问的速度最快，硬盘的使用寿命最长。
SSD的读写原理SSD没有像机械硬盘那样的寻道过程，所以它的随机读写都更快。我在下面列了一个表格，对比了一下SSD和机械硬盘的优缺点。
你会发现，不管是机械硬盘不擅长的随机读写，还是它本身已经表现不错的顺序写入，SSD在这些方面都要比HDD强。不过，有一点，机械硬盘要远强于SSD，那就是耐用性。如果我们需要频繁地重复写入删除数据，那么机械硬盘要比SSD性价比高很多。
要想知道为什么SSD的耐用性不太好，我们先要理解SSD硬盘的存储和读写原理。我们之前说过，CPU Cache用的SRAM是用一个电容来存放一个比特的数据。对于SSD硬盘，我们也可以先简单地认为，它是由一个电容加上一个电压计组合在一起，记录了一个或者多个比特。
SLC、MLC、TLC和QLC能够记录一个比特很容易理解。给电容里面充上电有电压的时候就是1，给电容放电里面没有电就是0。采用这样方式存储数据的SSD硬盘，我们一般称之为使用了SLC的颗粒，全称是Single-Level Cell，也就是一个存储单元中只有一位数据。
但是，这样的方式会遇到和CPU Cache类似的问题，那就是，同样的面积下，能够存放下的元器件是有限的。如果只用SLC，我们就会遇到，存储容量上不去，并且价格下不来的问题。于是呢，硬件工程师们就陆续发明了MLC（Multi-Level Cell）、TLC（Triple-Level Cell）以及QLC（Quad-Level Cell），也就是能在一个电容里面存下2个、3个乃至4个比特。
只有一个电容，我们怎么能够表示更多的比特呢？别忘了，这里我们还有一个电压计。4个比特一共可以从0000-1111表示16个不同的数。那么，如果我们能往电容里面充电的时候，充上15个不同的电压，并且我们电压计能够区分出这15个不同的电压。加上电容被放空代表的0，就能够代表从0000-1111这样4个比特了。
不过，要想表示15个不同的电压，充电和读取的时候，对于精度的要求就会更高。这会导致充电和读取的时候都更慢，所以QLC的SSD的读写速度，要比SLC的慢上好几倍。如果你想要知道是什么样的物理原理导致这个QLC更慢，可以去读一读这篇文章。
P/E擦写问题如果我们去看一看SSD硬盘的硬件构造，可以看到，它大概是自顶向下是这么构成的。
首先，自然和其他的I/O设备一样，它有对应的接口和控制电路。现在的SSD硬盘用的是SATA或者PCI Express接口。在控制电路里，有一个很重要的模块，叫作FTL（Flash-Translation Layer），也就是闪存转换层。这个可以说是SSD硬盘的一个核心模块，SSD硬盘性能的好坏，很大程度上也取决于FTL的算法好不好。现在容我卖个关子，我们晚一会儿仔细讲FTL的功能。
接下来是实际I/O设备，它其实和机械硬盘很像。现在新的大容量SSD硬盘都是3D封装的了，也就是说，是由很多个裸片（Die）叠在一起的，就好像我们的机械硬盘把很多个盘面（Platter）叠放再一起一样，这样可以在同样的空间下放下更多的容量。
接下来，一张裸片上可以放多个平面（Plane），一般一个平面上的存储容量大概在GB级别。一个平面上面，会划分成很多个块（Block），一般一个块（Block）的存储大小， 通常几百KB到几MB大小。一个块里面，还会区分很多个页（Page），就和我们内存里面的页一样，一个页的大小通常是4KB。
在这一层一层的结构里面，处在最下面的两层块和页非常重要。
对于SSD硬盘来说，数据的写入叫作Program。写入不能像机械硬盘一样，通过覆写（Overwrite）来进行的，而是要先去擦除（Erase），然后再写入。
SSD的读取和写入的基本单位，不是一个比特（bit）或者一个字节（byte），而是一个页（Page）。SSD的擦除单位就更夸张了，我们不仅不能按照比特或者字节来擦除，连按照页来擦除都不行，我们必须按照块来擦除。
而且，你必须记住的一点是，SSD的使用寿命，其实是每一个块（Block）的擦除的次数。你可以把SSD硬盘的一个平面看成是一张白纸。我们在上面写入数据，就好像用铅笔在白纸上写字。如果想要把已经写过字的地方写入新的数据，我们先要用橡皮把已经写好的字擦掉。但是，如果频繁擦同一个地方，那这个地方就会破掉，之后就没有办法再写字了。
我们上面说的SLC的芯片，可以擦除的次数大概在10万次，MLC就在1万次左右，而TLC和QLC就只在几千次了。这也是为什么，你去购买SSD硬盘，会看到同样的容量的价格差别很大，因为它们的芯片颗粒和寿命完全不一样。
SSD读写的生命周期下面我们来实际看一看，一块SSD硬盘在日常是怎么被用起来的。
我用三种颜色分别来表示SSD硬盘里面的页的不同状态，白色代表这个页从来没有写入过数据，绿色代表里面写入的是有效的数据，红色代表里面的数据，在我们的操作系统看来已经是删除的了。
一开始，所有块的每一个页都是白色的。随着我们开始往里面写数据，里面的有些页就变成了绿色。
然后，因为我们删除了硬盘上的一些文件，所以有些页变成了红色。但是这些红色的页，并不能再次写入数据。因为SSD硬盘不能单独擦除一个页，必须一次性擦除整个块，所以新的数据，我们只能往后面的白色的页里面写。这些散落在各个绿色空间里面的红色空洞，就好像硬盘碎片。
如果有哪一个块的数据一次性全部被标红了，那我们就可以把整个块进行擦除。它就又会变成白色，可以重新一页一页往里面写数据。这种情况其实也会经常发生。毕竟一个块不大，也就在几百KB到几MB。你删除一个几MB的文件，数据又是连续存储的，自然会导致整个块可以被擦除。
随着硬盘里面的数据越来越多，红色空洞占的地方也会越来越多。于是，你会发现，我们就要没有白色的空页去写入数据了。这个时候，我们要做一次类似于Windows里面“磁盘碎片整理”或者Java里面的“内存垃圾回收”工作。找一个红色空洞最多的块，把里面的绿色数据，挪到另一个块里面去，然后把整个块擦除，变成白色，可以重新写入数据。
不过，这个“磁盘碎片整理”或者“内存垃圾回收”的工作，我们不能太主动、太频繁地去做。因为SSD的擦除次数是有限的。如果动不动就搞个磁盘碎片整理，那么我们的SSD硬盘很快就会报废了。
说到这里，你可能要问了，这是不是说，我们的SSD硬盘的容量是用不满的？因为我们总会遇到一些红色空洞？
没错，一块SSD的硬盘容量，是没办法完全用满的。不过，为了不得罪消费者，生产SSD硬盘的厂商，其实是预留了一部分空间，专门用来做这个“磁盘碎片整理”工作的。一块标成240G的SSD硬盘，往往实际有256G的硬盘空间。SSD硬盘通过我们的控制芯片电路，把多出来的硬盘空间，用来进行各种数据的闪转腾挪，让你能够写满那240G的空间。这个多出来的16G空间，叫作预留空间（Over Provisioning），一般SSD的硬盘的预留空间都在7%-15%左右。
总结延伸到这里，相信你对SSD硬盘的写入和擦除的原理已经清楚了，也明白了SSD硬盘的使用寿命受限于可以擦除的次数。
仔细想一想，你会发现SSD硬盘，特别适合读多写少的应用。在日常应用里面，我们的系统盘适合用SSD。但是，如果我们用SSD做专门的下载盘，一直下载各种影音数据，然后刻盘备份就不太好了，特别是现在QLC颗粒的SSD，它只有几千次可擦写的寿命啊。
在数据中心里面，SSD的应用场景也是适合读多写少的场景。我们拿SSD硬盘用来做数据库，存放电商网站的商品信息很合适。但是，用来作为Hadoop这样的Map-Reduce应用的数据盘就不行了。因为Map-Reduce任务会大量在任务中间向硬盘写入中间数据再删除掉，这样用不了多久，SSD硬盘的寿命就会到了。
好了，最后让我们总结一下。
这一讲，我们从SSD的物理原理，也就是“电容+电压计”的组合，向你介绍了SSD硬盘存储数据的原理，以及从SLC、MLC、TLC，直到今天的QLC颗粒是怎么回事儿。
然后，我们一起看了SSD硬盘的物理构造，也就是裸片、平面、块、页的层次结构。我们对于数据的写入，只能是一页一页的，不能对页进行覆写。对于数据的擦除，只能整块进行。所以，我们需要用一个，类似“磁盘碎片整理”或者“内存垃圾回收”这样的机制，来清理块当中的数据空洞。而SSD硬盘也会保留一定的预留空间，避免出现硬盘无法写满的情况。
到了这里，我们SSD硬盘在硬件层面的写入机制就介绍完了。不过，更有挑战的一个问题是，在这样的机制下，我们怎么尽可能延长SSD的使用寿命呢？如果要开发一个跑在SSD硬盘上的数据库，我们可以利用SSD的哪些特性呢？想要知道这些，请你一定要记得回来听下一讲。
推荐阅读想要对于SSD的硬件实现原理有所了解，我推荐你去读一读这一篇Understand TLC NAND。
课后思考现在大家使用的数据系统里，往往会有日志系统。你觉得日志系统适合存放在SSD硬盘上吗？
欢迎在留言区写下你的思考。如果有收获，你也可以把这篇文章分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>46_概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？</title><link>https://artisanbox.github.io/2/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/47/</guid><description>上一节我们讲到，如何用位图、布隆过滤器，来过滤重复的数据。今天，我们再讲一个跟过滤相关的问题，如何过滤垃圾短信？
垃圾短信和骚扰电话，我想每个人都收到过吧？买房、贷款、投资理财、开发票，各种垃圾短信和骚扰电话，不胜其扰。如果你是一名手机应用开发工程师，让你实现一个简单的垃圾短信过滤功能以及骚扰电话拦截功能，该用什么样的数据结构和算法实现呢？
算法解析实际上，解决这个问题并不会涉及很高深的算法。今天，我就带你一块看下，如何利用简单的数据结构和算法，解决这种看似非常复杂的问题。
1.基于黑名单的过滤器我们可以维护一个骚扰电话号码和垃圾短信发送号码的黑名单。这个黑名单的收集，有很多途径，比如，我们可以从一些公开的网站上下载，也可以通过类似“360骚扰电话拦截”的功能，通过用户自主标记骚扰电话来收集。对于被多个用户标记，并且标记个数超过一定阈值的号码，我们就可以定义为骚扰电话，并将它加入到我们的黑名单中。
如果黑名单中的电话号码不多的话，我们可以使用散列表、二叉树等动态数据结构来存储，对内存的消耗并不会很大。如果我们把每个号码看作一个字符串，并且假设平均长度是16个字节，那存储50万个电话号码，大约需要10MB的内存空间。即便是对于手机这样的内存有限的设备来说，这点内存的消耗也是可以接受的。
但是，如果黑名单中的电话号码很多呢？比如有500万个。这个时候，如果再用散列表存储，就需要大约100MB的存储空间。为了实现一个拦截功能，耗费用户如此多的手机内存，这显然有点儿不合理。
上一节我们讲了，布隆过滤器最大的特点就是比较省存储空间，所以，用它来解决这个问题再合适不过了。如果我们要存储500万个手机号码，我们把位图大小设置为10倍数据大小，也就是5000万，那也只需要使用5000万个二进制位（5000万bits），换算成字节，也就是不到7MB的存储空间。比起散列表的解决方案，内存的消耗减少了很多。
实际上，我们还有一种时间换空间的方法，可以将内存的消耗优化到极致。
我们可以把黑名单存储在服务器端上，把过滤和拦截的核心工作，交给服务器端来做。手机端只负责将要检查的号码发送给服务器端，服务器端通过查黑名单，判断这个号码是否应该被拦截，并将结果返回给手机端。
用这个解决思路完全不需要占用手机内存。不过，有利就有弊。我们知道，网络通信是比较慢的，所以，网络延迟就会导致处理速度降低。而且，这个方案还有个硬性要求，那就是只有在联网的情况下，才能正常工作。
基于黑名单的过滤器我就讲完了，不过，你可能还会说，布隆过滤器会有判错的概率呀！如果它把一个重要的电话或者短信，当成垃圾短信或者骚扰电话拦截了，对于用户来说，这是无法接受的。你说得没错，这是一个很大的问题。不过，我们现在先放一放，等三种过滤器都讲完之后，我再来解答。
2.基于规则的过滤器刚刚讲了一种基于黑名单的垃圾短信过滤方法，但是，如果某个垃圾短信发送者的号码并不在黑名单中，那这种方法就没办法拦截了。所以，基于黑名单的过滤方式，还不够完善，我们再继续看一种基于规则的过滤方式。
对于垃圾短信来说，我们还可以通过短信的内容，来判断某条短信是否是垃圾短信。我们预先设定一些规则，如果某条短信符合这些规则，我们就可以判定它是垃圾短信。实际上，规则可以有很多，比如下面这几个：
短信中包含特殊单词（或词语），比如一些非法、淫秽、反动词语等；
短信发送号码是群发号码，非我们正常的手机号码，比如+60389585；
短信中包含回拨的联系方式，比如手机号码、微信、QQ、网页链接等，因为群发短信的号码一般都是无法回拨的；
短信格式花哨、内容很长，比如包含各种表情、图片、网页链接等；
符合已知垃圾短信的模板。垃圾短信一般都是重复群发，对于已经判定为垃圾短信的短信，我们可以抽象成模板，将获取到的短信与模板匹配，一旦匹配，我们就可以判定为垃圾短信。
当然，如果短信只是满足其中一条规则，如果就判定为垃圾短信，那会存在比较大的误判的情况。我们可以综合多条规则进行判断。比如，满足2条以上才会被判定为垃圾短信；或者每条规则对应一个不同的得分，满足哪条规则，我们就累加对应的分数，某条短信的总得分超过某个阈值，才会被判定为垃圾短信。
不过，我只是给出了一些制定规则的思路，具体落实到执行层面，其实还有很大的距离，还有很多细节需要处理。比如，第一条规则中，我们该如何定义特殊单词；第二条规则中，我们该如何定义什么样的号码是群发号码等等。限于篇幅，我就不一一详细展开来讲了。我这里只讲一下，如何定义特殊单词？
如果我们只是自己拍脑袋想，哪些单词属于特殊单词，那势必有比较大的主观性，也很容易漏掉某些单词。实际上，我们可以基于概率统计的方法，借助计算机强大的计算能力，找出哪些单词最常出现在垃圾短信中，将这些最常出现的单词，作为特殊单词，用来过滤短信。
不过这种方法的前提是，我们有大量的样本数据，也就是说，要有大量的短信（比如1000万条短信），并且我们还要求，每条短信都做好了标记，它是垃圾短信还是非垃圾短信。
我们对这1000万条短信，进行分词处理（借助中文或者英文分词算法），去掉“的、和、是”等没有意义的停用词（Stop words），得到n个不同的单词。针对每个单词，我们统计有多少个垃圾短信出现了这个单词，有多少个非垃圾短信会出现这个单词，进而求出每个单词出现在垃圾短信中的概率，以及出现在非垃圾短信中的概率。如果某个单词出现在垃圾短信中的概率，远大于出现在非垃圾短信中的概率，那我们就把这个单词作为特殊单词，用来过滤垃圾短信。
文字描述不好理解，我举个例子来解释一下。
3.基于概率统计的过滤器基于规则的过滤器，看起来很直观，也很好理解，但是它也有一定的局限性。一方面，这些规则受人的思维方式局限，规则未免太过简单；另一方面，垃圾短信发送者可能会针对规则，精心设计短信，绕过这些规则的拦截。对此，我们再来看一种更加高级的过滤方式，基于概率统计的过滤方式。
这种基于概率统计的过滤方式，基础理论是基于朴素贝叶斯算法。为了让你更好地理解下面的内容，我们先通过一个非常简单的例子来看下，什么是朴素贝叶斯算法？
假设事件A是“小明不去上学”，事件B是“下雨了”。我们现在统计了一下过去10天的下雨情况和小明上学的情况，作为样本数据。
我们来分析一下，这组样本有什么规律。在这10天中，有4天下雨，所以下雨的概率P(B)=4/10。10天中有3天，小明没有去上学，所以小明不去上学的概率P(A)=3/10。在4个下雨天中，小明有2天没去上学，所以下雨天不去上学的概率P(A|B)=2/4。在小明没有去上学的3天中，有2天下雨了，所以小明因为下雨而不上学的概率是P(B|A)=2/3。实际上，这4个概率值之间，有一定的关系，这个关系就是朴素贝叶斯算法，我们用公式表示出来，就是下面这个样子。
朴素贝叶斯算法是不是非常简单？我们用一个公式就可以将它概括。弄懂了朴素贝叶斯算法，我们再回到垃圾短信过滤这个问题上，看看如何利用朴素贝叶斯算法，来做垃圾短信的过滤。
基于概率统计的过滤器，是基于短信内容来判定是否是垃圾短信。而计算机没办法像人一样理解短信的含义。所以，我们需要把短信抽象成一组计算机可以理解并且方便计算的特征项，用这一组特征项代替短信本身，来做垃圾短信过滤。
我们可以通过分词算法，把一个短信分割成n个单词。这n个单词就是一组特征项，全权代表这个短信。因此，判定一个短信是否是垃圾短信这样一个问题，就变成了，判定同时包含这几个单词的短信是否是垃圾短信。
不过，这里我们并不像基于规则的过滤器那样，非黑即白，一个短信要么被判定为垃圾短信、要么被判定为非垃圾短息。我们使用概率，来表征一个短信是垃圾短信的可信程度。如果我们用公式将这个概率表示出来，就是下面这个样子：
尽管我们有大量的短信样本，但是我们没法通过样本数据统计得到这个概率。为什么不可以呢？你可能会说，我只需要统计同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信有多少个（我们假设有x个），然后看这里面属于垃圾短信的有几个（我们假设有y个），那包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信是垃圾短信的概率就是y/x。
理想很丰满，但现实往往很骨感。你忽视了非常重要的一点，那就是样本的数量再大，毕竟也是有限的，样本中不会有太多同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$的短信的，甚至很多时候，样本中根本不存在这样的短信。没有样本，也就无法计算概率。所以这样的推理方式虽然正确，但是实践中并不好用。
这个时候，朴素贝叶斯公式就可以派上用场了。我们通过朴素贝叶斯公式，将这个概率的求解，分解为其他三个概率的求解。你可以看我画的图。那转化之后的三个概率是否可以通过样本统计得到呢？
P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中 | 短信是垃圾短信）这个概率照样无法通过样本来统计得到。但是我们可以基于下面这条著名的概率规则来计算。
独立事件发生的概率计算公式：P(A*B) = P(A)*P(B)
如果事件A和事件B是独立事件，两者的发生没有相关性，事件A发生的概率P(A)等于p1，事件B发生的概率P(B)等于p2，那两个同时发生的概率P(A*B)就等于P(A)*P(B)。
基于这条独立事件发生概率的计算公式，我们可以把P（W1，W2，W3，…，Wn同时出现在一条短信中 | 短信是垃圾短信）分解为下面这个公式：
其中，P（$W_{i}$出现在短信中 | 短信是垃圾短信）表示垃圾短信中包含$W_{i}$这个单词的概率有多大。这个概率值通过统计样本很容易就能获得。我们假设垃圾短信有y个，其中包含$W_{i}$的有x个，那这个概率值就等于x/y。
P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中 | 短信是垃圾短信）这个概率值，我们就计算出来了，我们再来看下剩下两个。
P（短信是垃圾短信）表示短信是垃圾短信的概率，这个很容易得到。我们把样本中垃圾短信的个数除以总样本短信个数，就是短信是垃圾短信的概率。
不过，P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中）这个概率还是不好通过样本统计得到，原因我们前面说过了，样本空间有限。不过，我们没必要非得计算这一部分的概率值。为什么这么说呢？
实际上，我们可以分别计算同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信，是垃圾短信和非垃圾短信的概率。假设它们分别是p1和p2。我们并不需要单纯地基于p1值的大小来判断是否是垃圾短信，而是通过对比p1和p2值的大小，来判断一条短信是否是垃圾短信。更细化一点讲，那就是，如果p1是p2的很多倍（比如10倍），我们才确信这条短信是垃圾短信。
基于这两个概率的倍数来判断是否是垃圾短信的方法，我们就可以不用计算P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中）这一部分的值了，因为计算p1与p2的时候，都会包含这个概率值的计算，所以在求解p1和p2倍数（p1/p2）的时候，我们也就不需要这个值。</description></item><item><title>47_SSD硬盘（下）：如何完成性能优化的KPI？</title><link>https://artisanbox.github.io/4/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/47/</guid><description>如果你平时用的是Windows电脑，你会发现，用了SSD的系统盘，就不能用磁盘碎片整理功能。这是因为，一旦主动去运行磁盘碎片整理功能，就会发生一次块的擦除，对应块的寿命就少了一点点。这个SSD的擦除寿命的问题，不仅会影响像磁盘碎片整理这样的功能，其实也很影响我们的日常使用。
我们的操作系统上，并没有SSD硬盘上各个块目前已经擦写的情况和寿命，所以它对待SSD硬盘和普通的机械硬盘没有什么区别。
我们日常使用PC进行软件开发的时候，会先在硬盘上装上操作系统和常用软件，比如Office，或者工程师们会装上VS Code、WebStorm这样的集成开发环境。这些软件所在的块，写入一次之后，就不太会擦除了，所以就只有读的需求。
一旦开始开发，我们就会不断添加新的代码文件，还会不断修改已经有的代码文件。因为SSD硬盘没有覆写（Override）的功能，所以，这个过程中，其实我们是在反复地写入新的文件，然后再把原来的文件标记成逻辑上删除的状态。等SSD里面空的块少了，我们会用“垃圾回收”的方式，进行擦除。这样，我们的擦除会反复发生在这些用来存放数据的地方。
有一天，这些块的擦除次数到了，变成了坏块。但是，我们安装操作系统和软件的地方还没有坏，而这块硬盘的可以用的容量却变小了。
磨损均衡、TRIM和写入放大效应FTL和磨损均衡那么，我们有没有什么办法，不让这些坏块那么早就出现呢？我们能不能，匀出一些存放操作系统的块的擦写次数，给到这些存放数据的地方呢？
相信你一定想到了，其实我们要的就是想一个办法，让SSD硬盘各个块的擦除次数，均匀分摊到各个块上。这个策略呢，就叫作磨损均衡（Wear-Leveling）。实现这个技术的核心办法，和我们前面讲过的虚拟内存一样，就是添加一个间接层。这个间接层，就是我们上一讲给你卖的那个关子，就是FTL这个闪存转换层。
就像在管理内存的时候，我们通过一个页表映射虚拟内存页和物理页一样，在FTL里面，存放了逻辑块地址（Logical Block Address，简称LBA）到物理块地址（Physical Block Address，简称PBA）的映射。
操作系统访问的硬盘地址，其实都是逻辑地址。只有通过FTL转换之后，才会变成实际的物理地址，找到对应的块进行访问。操作系统本身，不需要去考虑块的磨损程度，只要和操作机械硬盘一样来读写数据就好了。
操作系统所有对于SSD硬盘的读写请求，都要经过FTL。FTL里面又有逻辑块对应的物理块，所以FTL能够记录下来，每个物理块被擦写的次数。如果一个物理块被擦写的次数多了，FTL就可以将这个物理块，挪到一个擦写次数少的物理块上。但是，逻辑块不用变，操作系统也不需要知道这个变化。
这也是我们在设计大型系统中的一个典型思路，也就是各层之间是隔离的，操作系统不需要考虑底层的硬件是什么，完全交由硬件的控制电路里面的FTL，来管理对于实际物理硬件的写入。
TRIM指令的支持不过，操作系统不去关心实际底层的硬件是什么，在SSD硬盘的使用上，也会带来一个问题。这个问题就是，操作系统的逻辑层和SSD的逻辑层里的块状态，是不匹配的。
我们在操作系统里面去删除一个文件，其实并没有真的在物理层面去删除这个文件，只是在文件系统里面，把对应的inode里面的元信息清理掉，这代表这个inode还可以继续使用，可以写入新的数据。这个时候，实际物理层面的对应的存储空间，在操作系统里面被标记成可以写入了。
所以，其实我们日常的文件删除，都只是一个操作系统层面的逻辑删除。这也是为什么，很多时候我们不小心删除了对应的文件，我们可以通过各种恢复软件，把数据找回来。同样的，这也是为什么，如果我们想要删除干净数据，需要用各种“文件粉碎”的功能才行。
这个删除的逻辑在机械硬盘层面没有问题，因为文件被标记成可以写入，后续的写入可以直接覆写这个位置。但是，在SSD硬盘上就不一样了。我在这里放了一张详细的示意图。我们下面一起来看看具体是怎么回事儿。
一开始，操作系统里面有好几个文件，不同的文件我用不同的颜色标记出来了。下面的SSD的逻辑块里面占用的页，我们也用同样的颜色标记出来文件占用的对应页。
当我们在操作系统里面，删除掉一个刚刚下载的文件，比如标记成黄色 openjdk.exe 这样一个jdk的安装文件，在操作系统里面，对应的inode里面，就没有文件的元信息。
但是，这个时候，我们的SSD的逻辑块层面，其实并不知道这个事情。所以在，逻辑块层面，openjdk.exe 仍然是占用了对应的空间。对应的物理页，也仍然被认为是被占用了的。
这个时候，如果我们需要对SSD进行垃圾回收操作，openjdk.exe 对应的物理页，仍然要在这个过程中，被搬运到其他的Block里面去。只有当操作系统，再在刚才的inode里面写入数据的时候，我们才会知道原来的些黄色的页，其实都已经没有用了，我们才会把它标记成废弃掉。
所以，在使用SSD的硬盘情况下，你会发现，操作系统对于文件的删除，SSD硬盘其实并不知道。这就导致，我们为了磨损均衡，很多时候在都在搬运很多已经删除了的数据。这就会产生很多不必要的数据读写和擦除，既消耗了SSD的性能，也缩短了SSD的使用寿命。
为了解决这个问题，现在的操作系统和SSD的主控芯片，都支持TRIM命令。这个命令可以在文件被删除的时候，让操作系统去通知SSD硬盘，对应的逻辑块已经标记成已删除了。现在的SSD硬盘都已经支持了TRIM命令。无论是Linux、Windows还是MacOS，这些操作系统也都已经支持了TRIM命令了。
写入放大其实，TRIM命令的发明，也反应了一个使用SSD硬盘的问题，那就是，SSD硬盘容易越用越慢。
当SSD硬盘的存储空间被占用得越来越多，每一次写入新数据，我们都可能没有足够的空白。我们可能不得不去进行垃圾回收，合并一些块里面的页，然后再擦除掉一些页，才能匀出一些空间来。
这个时候，从应用层或者操作系统层面来看，我们可能只是写入了一个4KB或者4MB的数据。但是，实际通过FTL之后，我们可能要去搬运8MB、16MB甚至更多的数据。
我们通过“实际的闪存写入的数据量 / 系统通过FTL写入的数据量 = 写入放大”，可以得到，写入放大的倍数越多，意味着实际的SSD性能也就越差，会远远比不上实际SSD硬盘标称的指标。
而解决写入放大，需要我们在后台定时进行垃圾回收，在硬盘比较空闲的时候，就把搬运数据、擦除数据、留出空白的块的工作做完，而不是等实际数据写入的时候，再进行这样的操作。
AeroSpike：如何最大化SSD的使用效率？讲到这里，相信你也发现了，想要把SSD硬盘用好，其实没有那么简单。如果我们只是简单地拿一块SSD硬盘替换掉原来的HDD硬盘，而不是从应用层面考虑任何SSD硬盘特性的话，我们多半还是没法获得想要的性能提升。
不过，既然清楚了SSD硬盘的各种特性，我们就可以依据这些特性，来设计我们的应用。接下来，我就带你一起看一看，AeroSpike这个专门针对SSD硬盘特性设计的Key-Value数据库（键值对数据库），是怎么利用这些物理特性的。
首先，AeroSpike操作SSD硬盘，并没有通过操作系统的文件系统。而是直接操作SSD里面的块和页。因为操作系统里面的文件系统，对于KV数据库来说，只是让我们多了一层间接层，只会降低性能，对我们没有什么实际的作用。
其次，AeroSpike在读写数据的时候，做了两个优化。在写入数据的时候，AeroSpike尽可能去写一个较大的数据块，而不是频繁地去写很多小的数据块。这样，硬盘就不太容易频繁出现磁盘碎片。并且，一次性写入一个大的数据块，也更容易利用好顺序写入的性能优势。AeroSpike写入的一个数据块，是128KB，远比一个页的4KB要大得多。
另外，在读取数据的时候，AeroSpike倒是可以读取512字节（Bytes）这样的小数据。因为SSD的随机读取性能很好，也不像写入数据那样有擦除寿命问题。而且，很多时候我们读取的数据是键值对里面的值的数据，这些数据要在网络上传输。如果一次性必须读出比较大的数据，就会导致我们的网络带宽不够用。
因为AeroSpike是一个对于响应时间要求很高的实时KV数据库，如果出现了严重的写放大效应，会导致写入数据的响应时间大幅度变长。所以AeroSpike做了这样几个动作：
第一个是持续地进行磁盘碎片整理。AeroSpike用了所谓的高水位（High Watermark）算法。其实这个算法很简单，就是一旦一个物理块里面的数据碎片超过50%，就把这个物理块搬运压缩，然后进行数据擦除，确保磁盘始终有足够的空间可以写入。
第二个是在AeroSpike给出的最佳实践中，为了保障数据库的性能，建议你只用到SSD硬盘标定容量的一半。也就是说，我们人为地给SSD硬盘预留了50%的预留空间，以确保SSD硬盘的写放大效应尽可能小，不会影响数据库的访问性能。
正是因为做了这种种的优化，在NoSQL数据库刚刚兴起的时候，AeroSpike的性能把Cassandra、MongoDB这些数据库远远甩在身后，和这些数据库之间的性能差距，有时候会到达一个数量级。这也让AeroSpike成为了当时高性能KV数据库的标杆。你可以看一看InfoQ出的这个Benchmark，里面有2013年的时候，这几个NoSQL数据库巨大的性能差异。
总结延伸好了，现在让我们一起来总结一下今天的内容。
因为SSD硬盘的使用寿命，受限于块的擦除次数，所以我们需要通过一个磨损均衡的策略，来管理SSD硬盘的各个块的擦除次数。我们通过在逻辑块地址和物理块地址之间，引入FTL这个映射层，使得操作系统无需关心物理块的擦写次数，而是由FTL里的软件算法，来协调到底每一次写入应该磨损哪一块。
除了磨损均衡之外，操作系统和SSD硬件的特性还有一个不匹配的地方。那就是，操作系统在删除数据的时候，并没有真的删除物理层面的数据，而只是修改了inode里面的数据。这个“伪删除”，使得SSD硬盘在逻辑和物理层面，都没有意识到有些块其实已经被删除了。这就导致在垃圾回收的时候，会浪费很多不必要的读写资源。
SSD这个需要进行垃圾回收的特性，使得我们在写入数据的时候，会遇到写入放大。明明我们只是写入了4MB的数据，可能在SSD的硬件层面，实际写入了8MB、16MB乃至更多的数据。
针对这些特性，AeroSpike，这个专门针对SSD硬盘特性的KV数据库，设计了很多的优化点，包括跳过文件系统直写硬盘、写大块读小块、用高水位算法持续进行磁盘碎片整理，以及只使用SSD硬盘的一半空间。这些策略，使得AeroSpike的性能，在早年间远远超过了Cassandra等其他NoSQL数据库。
可以看到，针对硬件特性设计的软件，才能最大化发挥我们的硬件性能。
推荐阅读如果你想要基于SSD硬盘本身的特性来设计开发你的系统，我推荐你去读一读AeroSpike的这个PPT。AeroSpike是市面上最优秀的KV数据库之一，通过深入地利用了SSD本身的硬件特性，最大化提升了作为一个KV数据库的性能。真正在进行系统软件开发的时候，了解硬件是必不可少的一个环节。
课后思考在SSD硬盘的价格大幅度下降了之后，LFS，也就是Log-Structured File System，在业界出现了第二春。你可以去了解一下什么是LFS，以及为什么LFS特别适合SSD硬盘。
欢迎在留言区分享你了解到的信息，和大家一起交流。如果有收获，你可以把这篇文章分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>47_向量空间：如何实现一个简单的音乐推荐系统？</title><link>https://artisanbox.github.io/2/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/48/</guid><description>很多人都喜爱听歌，以前我们用MP3听歌，现在直接通过音乐App在线就能听歌。而且，各种音乐App的功能越来越强大，不仅可以自己选歌听，还可以根据你听歌的口味偏好，给你推荐可能会喜爱的音乐，而且有时候，推荐的音乐还非常适合你的口味，甚至会惊艳到你！如此智能的一个功能，你知道它是怎么实现的吗？
算法解析实际上，要解决这个问题，并不需要特别高深的理论。解决思路的核心思想非常简单、直白，用两句话就能总结出来。
找到跟你口味偏好相似的用户，把他们爱听的歌曲推荐给你；
找出跟你喜爱的歌曲特征相似的歌曲，把这些歌曲推荐给你。
接下来，我就分别讲解一下这两种思路的具体实现方法。
1.基于相似用户做推荐如何找到跟你口味偏好相似的用户呢？或者说如何定义口味偏好相似呢？实际上，思路也很简单，我们把跟你听类似歌曲的人，看作口味相似的用户。你可以看我下面画的这个图。我用“1”表示“喜爱”，用“0”笼统地表示“不发表意见”。从图中我们可以看出，你跟小明共同喜爱的歌曲最多，有5首。于是，我们就可以说，小明跟你的口味非常相似。
我们只需要遍历所有的用户，对比每个用户跟你共同喜爱的歌曲个数，并且设置一个阈值，如果你和某个用户共同喜爱的歌曲个数超过这个阈值，我们就把这个用户看作跟你口味相似的用户，把这个用户喜爱但你还没听过的歌曲，推荐给你。
不过，刚刚的这个解决方案中有一个问题，我们如何知道用户喜爱哪首歌曲呢？也就是说，如何定义用户对某首歌曲的喜爱程度呢？
实际上，我们可以通过用户的行为，来定义这个喜爱程度。我们给每个行为定义一个得分，得分越高表示喜爱程度越高。
还是刚刚那个例子，我们如果把每个人对每首歌曲的喜爱程度表示出来，就是下面这个样子。图中，某个人对某首歌曲是否喜爱，我们不再用“1”或者“0”来表示，而是对应一个具体的分值。
有了这样一个用户对歌曲的喜爱程度的对应表之后，如何来判断两个用户是否口味相似呢？
显然，我们不能再像之前那样，采用简单的计数来统计两个用户之间的相似度。还记得我们之前讲字符串相似度度量时，提到的编辑距离吗？这里的相似度度量，我们可以使用另外一个距离，那就是欧几里得距离（Euclidean distance）。欧几里得距离是用来计算两个向量之间的距离的。这个概念中有两个关键词，向量和距离，我来给你解释一下。
一维空间是一条线，我们用1，2，3……这样单个的数，来表示一维空间中的某个位置；二维空间是一个面，我们用（1，3）（4，2）（2，2）……这样的两个数，来表示二维空间中的某个位置；三维空间是一个立体空间，我们用（1，3，5）（3，1，7）（2，4，3）……这样的三个数，来表示三维空间中的某个位置。一维、二维、三维应该都不难理解，那更高维中的某个位置该如何表示呢？
类比一维、二维、三维的表示方法，K维空间中的某个位置，我们可以写作（$X_{1}$，$X_{2}$，$X_{3}$，…，$X_{K}$）。这种表示方法就是向量（vector）。我们知道，二维、三维空间中，两个位置之间有距离的概念，类比到高纬空间，同样也有距离的概念，这就是我们说的两个向量之间的距离。
那如何计算两个向量之间的距离呢？我们还是可以类比到二维、三维空间中距离的计算方法。通过类比，我们就可以得到两个向量之间距离的计算公式。这个计算公式就是欧几里得距离的计算公式：
我们把每个用户对所有歌曲的喜爱程度，都用一个向量表示。我们计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量。从图中的计算可以看出，小明与你的欧几里得距离距离最小，也就是说，你俩在高维空间中靠得最近，所以，我们就断定，小明跟你的口味最相似。
2.基于相似歌曲做推荐刚刚我们讲了基于相似用户的歌曲推荐方法，但是，如果用户是一个新用户，我们还没有收集到足够多的行为数据，这个时候该如何推荐呢？我们现在再来看另外一种推荐方法，基于相似歌曲的推荐方法，也就是说，如果某首歌曲跟你喜爱的歌曲相似，我们就把它推荐给你。
如何判断两首歌曲是否相似呢？对于人来说，这个事情可能会比较简单，但是对于计算机来说，判断两首歌曲是否相似，那就需要通过量化的数据来表示了。我们应该通过什么数据来量化两个歌曲之间的相似程度呢？
最容易想到的是，我们对歌曲定义一些特征项，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等。类似基于相似用户的推荐方法，我们给每个歌曲的每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量。我们可以基于这个特征项向量，来计算两个歌曲之间的欧几里得距离。欧几里得距离越小，表示两个歌曲的相似程度越大。
但是，要实现这个方案，需要有一个前提，那就是我们能够找到足够多，并且能够全面代表歌曲特点的特征项，除此之外，我们还要人工给每首歌标注每个特征项的得分。对于收录了海量歌曲的音乐App来说，这显然是一个非常大的工程。此外，人工标注有很大的主观性，也会影响到推荐的准确性。
既然基于歌曲特征项计算相似度不可行，那我们就换一种思路。对于两首歌，如果喜欢听的人群都是差不多的，那侧面就可以反映出，这两首歌比较相似。如图所示，每个用户对歌曲有不同的喜爱程度，我们依旧通过上一个解决方案中定义得分的标准，来定义喜爱程度。
你有没有发现，这个图跟基于相似用户推荐中的图几乎一样。只不过这里把歌曲和用户主次颠倒了一下。基于相似用户的推荐方法中，针对每个用户，我们将对各个歌曲的喜爱程度作为向量。基于相似歌曲的推荐思路中，针对每个歌曲，我们将每个用户的打分作为向量。
有了每个歌曲的向量表示，我们通过计算向量之间的欧几里得距离，来表示歌曲之间的相似度。欧几里得距离越小，表示两个歌曲越相似。然后，我们就在用户已经听过的歌曲中，找出他喜爱程度较高的歌曲。然后，我们找出跟这些歌曲相似度很高的其他歌曲，推荐给他。
总结引申实际上，这个问题是推荐系统（Recommendation System）里最典型的一类问题。之所以讲这部分内容，主要还是想给你展示，算法的强大之处，利用简单的向量空间的欧几里得距离，就能解决如此复杂的问题。不过，今天，我只给你讲解了基本的理论，实践中遇到的问题还有很多，比如冷启动问题，产品初期积累的数据不多，不足以做推荐等等。这些更加深奥的内容，你可以之后自己在实践中慢慢探索。
课后思考关于今天讲的推荐算法，你还能想到其他应用场景吗？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>48_B+树：MySQL数据库索引是如何实现的？</title><link>https://artisanbox.github.io/2/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/49/</guid><description>作为一个软件开发工程师，你对数据库肯定再熟悉不过了。作为主流的数据存储系统，它在我们的业务开发中，有着举足轻重的地位。在工作中，为了加速数据库中数据的查找速度，我们常用的处理思路是，对表中数据创建索引。那你是否思考过，数据库索引是如何实现的呢？底层使用的是什么数据结构和算法呢？
算法解析思考的过程比结论更重要。跟着我学习了这么多节课，很多同学已经意识到这一点，比如Jerry银银同学。我感到很开心。所以，今天的讲解，我会尽量还原这个解决方案的思考过程，让你知其然，并且知其所以然。
1.解决问题的前提是定义清楚问题如何定义清楚问题呢？除了对问题进行详细的调研，还有一个办法，那就是，通过对一些模糊的需求进行假设，来限定要解决的问题的范围。
如果你对数据库的操作非常了解，针对我们现在这个问题，你就能把索引的需求定义得非常清楚。但是，对于大部分软件工程师来说，我们可能只了解一小部分常用的SQL语句，所以，这里我们假设要解决的问题，只包含这样两个常用的需求：
根据某个值查找数据，比如select * from user where id=1234；
根据区间值来查找某些数据，比如select * from user where id &amp;gt; 1234 and id &amp;lt; 2345。
除了这些功能性需求之外，这种问题往往还会涉及一些非功能性需求，比如安全、性能、用户体验等等。限于专栏要讨论的主要是数据结构和算法，对于非功能性需求，我们着重考虑性能方面的需求。性能方面的需求，我们主要考察时间和空间两方面，也就是执行效率和存储空间。
在执行效率方面，我们希望通过索引，查询数据的效率尽可能地高；在存储空间方面，我们希望索引不要消耗太多的内存空间。
2.尝试用学过的数据结构解决这个问题问题的需求大致定义清楚了，我们现在回想一下，能否利用已经学习过的数据结构解决这个问题呢？支持快速查询、插入等操作的动态数据结构，我们已经学习过散列表、平衡二叉查找树、跳表。
我们先来看散列表。散列表的查询性能很好，时间复杂度是O(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。
我们再来看平衡二叉查找树。尽管平衡二叉查找树查询的性能也很高，时间复杂度是O(logn)。而且，对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但这仍然不足以支持按照区间快速查找数据。
我们再来看跳表。跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是O(logn)。并且，跳表也支持按照区间快速地查找数据。我们只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止，这期间遍历得到的数据就是满足区间值的数据。
这样看来，跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫作B+树。不过，它是通过二叉查找树演化过来的，而非跳表。为了给你还原发明B+树的整个思考过程，所以，接下来，我还要从二叉查找树讲起，看它是如何一步一步被改造成B+树的。
3.改造二叉查找树来解决这个问题为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。经过改造之后的二叉树，就像图中这样，看起来是不是很像跳表呢？
改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。
但是，我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。
比如，我们给一亿个数据构建二叉查找树索引，那索引中会包含大约1亿个节点，每个节点假设占用16个字节，那就需要大约1GB的内存空间。给一张表建立索引，我们需要1GB的内存空间。如果我们要给10张表建立索引，那对内存的需求是无法满足的。如何解决这个索引占用太多内存的问题呢？
我们可以借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。
这种将索引存储在硬盘中的方案，尽管减少了内存消耗，但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。
二叉查找树，经过改造之后，支持区间查找的功能就实现了。不过，为了节省内存，如果把树存储在硬盘中，那么每个节点的读取（或者访问），都对应一次磁盘IO操作。树的高度就等于每次查询数据时磁盘IO操作的次数。
我们前面讲到，比起内存读写操作，磁盘IO操作非常耗时，所以我们优化的重点就是尽量减少磁盘IO操作，也就是，尽量降低树的高度。那如何降低树的高度呢？
我们来看下，如果我们把索引构建成m叉树，高度是不是比二叉树要小呢？如图所示，给16个数据构建二叉树索引，树的高度是4，查找一个数据，就需要4个磁盘IO操作（如果根节点存储在内存中，其他节点存储在磁盘中），如果对16个数据构建五叉树索引，那高度只有2，查找一个数据，对应只需要2次磁盘操作。如果m叉树中的m是100，那对一亿个数据构建索引，树的高度也只是3，最多只要3次磁盘IO就能获取到数据。磁盘IO变少了，查找数据的效率也就提高了。
如果我们将m叉树实现B+树索引，用代码实现出来，就是下面这个样子（假设我们给int类型的数据库字段添加索引，所以代码中的keywords是int类型的）：
/** * 这是B+树非叶子节点的定义。 * * 假设keywords=[3, 5, 8, 10] * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF) * 5个区间分别对应：children[0]...children[4] * * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = (m-1)*4[keywordss大小]+m*8[children大小] */ public class BPlusTreeNode { public static int m = 5; // 5叉树 public int[] keywords = new int[m-1]; // 键值，用来划分数据区间 public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针 } /**</description></item><item><title>48_DMA：为什么Kafka这么快？</title><link>https://artisanbox.github.io/4/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/48/</guid><description>过去几年里，整个计算机产业界，都在尝试不停地提升I/O设备的速度。把HDD硬盘换成SSD硬盘，我们仍然觉得不够快；用PCI Express接口的SSD硬盘替代SATA接口的SSD硬盘，我们还是觉得不够快，所以，现在就有了傲腾（Optane）这样的技术。
但是，无论I/O速度如何提升，比起CPU，总还是太慢。SSD硬盘的IOPS可以到2万、4万，但是我们CPU的主频有2GHz以上，也就意味着每秒会有20亿次的操作。
如果我们对于I/O的操作，都是由CPU发出对应的指令，然后等待I/O设备完成操作之后返回，那CPU有大量的时间其实都是在等待I/O设备完成操作。
但是，这个CPU的等待，在很多时候，其实并没有太多的实际意义。我们对于I/O设备的大量操作，其实都只是把内存里面的数据，传输到I/O设备而已。在这种情况下，其实CPU只是在傻等而已。特别是当传输的数据量比较大的时候，比如进行大文件复制，如果所有数据都要经过CPU，实在是有点儿太浪费时间了。
因此，计算机工程师们，就发明了DMA技术，也就是直接内存访问（Direct Memory Access）技术，来减少CPU等待的时间。
理解DMA，一个协处理器其实DMA技术很容易理解，本质上，DMA技术就是我们在主板上放一块独立的芯片。在进行内存和I/O设备的数据传输的时候，我们不再通过CPU来控制数据传输，而直接通过DMA控制器（DMA Controller，简称DMAC）。这块芯片，我们可以认为它其实就是一个协处理器（Co-Processor）。
DMAC最有价值的地方体现在，当我们要传输的数据特别大、速度特别快，或者传输的数据特别小、速度特别慢的时候。
比如说，我们用千兆网卡或者硬盘传输大量数据的时候，如果都用CPU来搬运的话，肯定忙不过来，所以可以选择DMAC。而当数据传输很慢的时候，DMAC可以等数据到齐了，再发送信号，给到CPU去处理，而不是让CPU在那里忙等待。
好了，现在你应该明白DMAC的价值，知道了它适合用在什么情况下。那我们现在回过头来看。我们上面说，DMAC是一块“协处理器芯片”，这是为什么呢？
注意，这里面的“协”字。DMAC是在“协助”CPU，完成对应的数据传输工作。在DMAC控制数据传输的过程中，我们还是需要CPU的。
除此之外，DMAC其实也是一个特殊的I/O设备，它和CPU以及其他I/O设备一样，通过连接到总线来进行实际的数据传输。总线上的设备呢，其实有两种类型。一种我们称之为主设备（Master），另外一种，我们称之为从设备（Slave）。
想要主动发起数据传输，必须要是一个主设备才可以，CPU就是主设备。而我们从设备（比如硬盘）只能接受数据传输。所以，如果通过CPU来传输数据，要么是CPU从I/O设备读数据，要么是CPU向I/O设备写数据。
这个时候你可能要问了，那我们的I/O设备不能向主设备发起请求么？可以是可以，不过这个发送的不是数据内容，而是控制信号。I/O设备可以告诉CPU，我这里有数据要传输给你，但是实际数据是CPU拉走的，而不是I/O设备推给CPU的。
不过，DMAC就很有意思了，它既是一个主设备，又是一个从设备。对于CPU来说，它是一个从设备；对于硬盘这样的IO设备来说呢，它又变成了一个主设备。那使用DMAC进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。
1.首先，CPU还是作为一个主设备，向DMAC设备发起请求。这个请求，其实就是在DMAC里面修改配置寄存器。
2.CPU修改DMAC的配置的时候，会告诉DMAC这样几个信息：
首先是源地址的初始值以及传输时候的地址增减方式。
所谓源地址，就是数据要从哪里传输过来。如果我们要从内存里面写入数据到硬盘上，那么就是要读取的数据在内存里面的地址。如果是从硬盘读取数据到内存里，那就是硬盘的I/O接口的地址。
我们讲过总线的时候说过，I/O的地址可以是一个内存地址，也可以是一个端口地址。而地址的增减方式就是说，数据是从大的地址向小的地址传输，还是从小的地址往大的地址传输。 其次是目标地址初始值和传输时候的地址增减方式。目标地址自然就是和源地址对应的设备，也就是我们数据传输的目的地。 第三个自然是要传输的数据长度，也就是我们一共要传输多少数据。 3.设置完这些信息之后，DMAC就会变成一个空闲的状态（Idle）。
4.如果我们要从硬盘上往内存里面加载数据，这个时候，硬盘就会向DMAC发起一个数据传输请求。这个请求并不是通过总线，而是通过一个额外的连线。
5.然后，我们的DMAC需要再通过一个额外的连线响应这个申请。
6.于是，DMAC这个芯片，就向硬盘的接口发起要总线读的传输请求。数据就从硬盘里面，读到了DMAC的控制器里面。
7.然后，DMAC再向我们的内存发起总线写的数据传输请求，把数据写入到内存里面。
8.DMAC会反复进行上面第6、7步的操作，直到DMAC的寄存器里面设置的数据长度传输完成。
9.数据传输完成之后，DMAC重新回到第3步的空闲状态。
所以，整个数据传输的过程中，我们不是通过CPU来搬运数据，而是由DMAC这个芯片来搬运数据。但是CPU在这个过程中也是必不可少的。因为传输什么数据，从哪里传输到哪里，其实还是由CPU来设置的。这也是为什么，DMAC被叫作“协处理器”。
现在的外设里面，很多都内置了DMAC最早，计算机里是没有DMAC的，所有数据都是由CPU来搬运的。随着人们对于数据传输的需求越来越多，先是出现了主板上独立的DMAC控制器。到了今天，各种I/O设备越来越多，数据传输的需求越来越复杂，使用的场景各不相同。加之显示器、网卡、硬盘对于数据传输的需求都不一样，所以各个设备里面都有自己的DMAC芯片了。
为什么那么快？一起来看Kafka的实现原理了解了DMAC是怎么回事儿，那你可能要问了，这和我们实际进行程序开发有什么关系呢？有什么API，我们直接调用一下，就能加速数据传输，减少CPU占用吗？
你还别说，过去几年的大数据浪潮里面，还真有一个开源项目很好地利用了DMA的数据传输方式，通过DMA的方式实现了非常大的性能提升。这个项目就是Kafka。下面我们就一起来看看它究竟是怎么利用DMA的。
Kafka是一个用来处理实时数据的管道，我们常常用它来做一个消息队列，或者用来收集和落地海量的日志。作为一个处理实时数据和日志的管道，瓶颈自然也在I/O层面。
Kafka里面会有两种常见的海量数据传输的情况。一种是从网络中接收上游的数据，然后需要落地到本地的磁盘上，确保数据不丢失。另一种情况呢，则是从本地磁盘上读取出来，通过网络发送出去。
我们来看一看后一种情况，从磁盘读数据发送到网络上去。如果我们自己写一个简单的程序，最直观的办法，自然是用一个文件读操作，从磁盘上把数据读到内存里面来，然后再用一个Socket，把这些数据发送到网络上去。
File.read(fileDesc, buf, len); Socket.send(socket, buf, len); 代码来源这段伪代码，来自IBM Developer Works上关于Zero Copy的文章在这个过程中，数据一共发生了四次传输的过程。其中两次是DMA的传输，另外两次，则是通过CPU控制的传输。下面我们来具体看看这个过程。
第一次传输，是从硬盘上，读到操作系统内核的缓冲区里。这个传输是通过DMA搬运的。
第二次传输，需要从内核缓冲区里面的数据，复制到我们应用分配的内存里面。这个传输是通过CPU搬运的。
第三次传输，要从我们应用的内存里面，再写到操作系统的Socket的缓冲区里面去。这个传输，还是由CPU搬运的。
最后一次传输，需要再从Socket的缓冲区里面，写到网卡的缓冲区里面去。这个传输又是通过DMA搬运的。
这个时候，你可以回过头看看这个过程。我们只是要“搬运”一份数据，结果却整整搬运了四次。而且这里面，从内核的读缓冲区传输到应用的内存里，再从应用的内存里传输到Socket的缓冲区里，其实都是把同一份数据在内存里面搬运来搬运去，特别没有效率。
像Kafka这样的应用场景，其实大部分最终利用到的硬件资源，其实又都是在干这个搬运数据的事儿。所以，我们就需要尽可能地减少数据搬运的需求。
事实上，Kafka做的事情就是，把这个数据搬运的次数，从上面的四次，变成了两次，并且只有DMA来进行数据搬运，而不需要CPU。
@Override public long transferFrom(FileChannel fileChannel, long position, long count) throws IOException { return fileChannel.</description></item><item><title>49_搜索：如何用Ax搜索算法实现游戏中的寻路功能？</title><link>https://artisanbox.github.io/2/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/50/</guid><description>魔兽世界、仙剑奇侠传这类MMRPG游戏，不知道你有没有玩过？在这些游戏中，有一个非常重要的功能，那就是人物角色自动寻路。当人物处于游戏地图中的某个位置的时候，我们用鼠标点击另外一个相对较远的位置，人物就会自动地绕过障碍物走过去。玩过这么多游戏，不知你是否思考过，这个功能是怎么实现的呢？
算法解析实际上，这是一个非常典型的搜索问题。人物的起点就是他当下所在的位置，终点就是鼠标点击的位置。我们需要在地图中，找一条从起点到终点的路径。这条路径要绕过地图中所有障碍物，并且看起来要是一种非常聪明的走法。所谓“聪明”，笼统地解释就是，走的路不能太绕。理论上讲，最短路径显然是最聪明的走法，是这个问题的最优解。
不过，在第44节最优出行路线规划问题中，我们也讲过，如果图非常大，那Dijkstra最短路径算法的执行耗时会很多。在真实的软件开发中，我们面对的是超级大的地图和海量的寻路请求，算法的执行效率太低，这显然是无法接受的。
实际上，像出行路线规划、游戏寻路，这些真实软件开发中的问题，一般情况下，我们都不需要非得求最优解（也就是最短路径）。在权衡路线规划质量和执行效率的情况下，我们只需要寻求一个次优解就足够了。那如何快速找出一条接近于最短路线的次优路线呢？
这个快速的路径规划算法，就是我们今天要学习的A*算法。实际上，A*算法是对Dijkstra算法的优化和改造。如何将Dijkstra算法改造成A*算法呢？为了更好地理解接下来要讲的内容，我建议你先温习下第44节中的Dijkstra算法的实现原理。
Dijkstra算法有点儿类似BFS算法，它每次找到跟起点最近的顶点，往外扩展。这种往外扩展的思路，其实有些盲目。为什么这么说呢？我举一个例子来给你解释一下。下面这个图对应一个真实的地图，每个顶点在地图中的位置，我们用一个二维坐标（x，y）来表示，其中，x表示横坐标，y表示纵坐标。
在Dijkstra算法的实现思路中，我们用一个优先级队列，来记录已经遍历到的顶点以及这个顶点与起点的路径长度。顶点与起点路径长度越小，就越先被从优先级队列中取出来扩展，从图中举的例子可以看出，尽管我们找的是从s到t的路线，但是最先被搜索到的顶点依次是1，2，3。通过肉眼来观察，这个搜索方向跟我们期望的路线方向（s到t是从西向东）是反着的，路线搜索的方向明显“跑偏”了。
之所以会“跑偏”，那是因为我们是按照顶点与起点的路径长度的大小，来安排出队列顺序的。与起点越近的顶点，就会越早出队列。我们并没有考虑到这个顶点到终点的距离，所以，在地图中，尽管1，2，3三个顶点离起始顶点最近，但离终点却越来越远。
如果我们综合更多的因素，把这个顶点到终点可能还要走多远，也考虑进去，综合来判断哪个顶点该先出队列，那是不是就可以避免“跑偏”呢？
当我们遍历到某个顶点的时候，从起点走到这个顶点的路径长度是确定的，我们记作g(i)（i表示顶点编号）。但是，从这个顶点到终点的路径长度，我们是未知的。虽然确切的值无法提前知道，但是我们可以用其他估计值来代替。
这里我们可以通过这个顶点跟终点之间的直线距离，也就是欧几里得距离，来近似地估计这个顶点跟终点的路径长度（注意：路径长度跟直线距离是两个概念）。我们把这个距离记作h(i)（i表示这个顶点的编号），专业的叫法是启发函数（heuristic function）。因为欧几里得距离的计算公式，会涉及比较耗时的开根号计算，所以，我们一般通过另外一个更加简单的距离计算公式，那就是曼哈顿距离（Manhattan distance）。曼哈顿距离是两点之间横纵坐标的距离之和。计算的过程只涉及加减法、符号位反转，所以比欧几里得距离更加高效。
int hManhattan(Vertex v1, Vertex v2) { // Vertex表示顶点，后面有定义 return Math.abs(v1.x - v2.x) + Math.abs(v1.y - v2.y); } 原来只是单纯地通过顶点与起点之间的路径长度g(i)，来判断谁先出队列，现在有了顶点到终点的路径长度估计值，我们通过两者之和f(i)=g(i)+h(i)，来判断哪个顶点该最先出队列。综合两部分，我们就能有效避免刚刚讲的“跑偏”。这里f(i)的专业叫法是估价函数（evaluation function）。
从刚刚的描述，我们可以发现，A*算法就是对Dijkstra算法的简单改造。实际上，代码实现方面，我们也只需要稍微改动几行代码，就能把Dijkstra算法的代码实现，改成A*算法的代码实现。
在A*算法的代码实现中，顶点Vertex类的定义，跟Dijkstra算法中的定义，稍微有点儿区别，多了x，y坐标，以及刚刚提到的f(i)值。图Graph类的定义跟Dijkstra算法中的定义一样。为了避免重复，我这里就没有再贴出来了。
private class Vertex { public int id; // 顶点编号ID public int dist; // 从起始顶点，到这个顶点的距离，也就是g(i) public int f; // 新增：f(i)=g(i)+h(i) public int x, y; // 新增：顶点在地图中的坐标（x, y） public Vertex(int id, int x, int y) { this.id = id; this.</description></item><item><title>49_数据完整性（上）：硬件坏了怎么办？</title><link>https://artisanbox.github.io/4/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/49/</guid><description>2012年的时候，我第一次在工作中，遇到一个因为硬件的不可靠性引发的Bug。正是因为这个Bug，让我开始逐步花很多的时间，去复习回顾整个计算机系统里面的底层知识。
当时，我正在MediaV带领一个20多人的团队，负责公司的广告数据和机器学习算法。其中有一部分工作，就是用Hadoop集群处理所有的数据和报表业务。当时我们的业务增长很快，所以会频繁地往Hadoop集群里面添置机器。2012年的时候，国内的云计算平台还不太成熟，所以我们都是自己采购硬件，放在托管的数据中心里面。
那个时候，我们的Hadoop集群服务器，在从100台服务器往1000台服务器走。我们觉得，像Dell这样品牌厂商的服务器太贵了，而且能够提供的硬件配置和我们的期望也有差异。于是，运维的同学开始和OEM厂商合作，自己定制服务器，批量采购硬盘、内存。
那个时候，大家都听过Google早期发展时，为了降低成本买了很多二手的硬件来降低成本，通过分布式的方式来保障系统的可靠性的办法。虽然我们还没有抠门到去买二手硬件，不过当时，我们选择购买了普通的机械硬盘，而不是企业级的、用在数据中心的机械硬盘；采购了普通的内存条，而不是带ECC纠错的服务器内存条，想着能省一点儿是一点儿。
单比特翻转：软件解决不了的硬件错误忽然有一天，我们最大的、每小时执行一次的数据处理报表应用，完成时间变得比平时晚了不少。一开始，我们并没有太在意，毕竟当时数据量每天都在增长，慢一点就慢一点了。但是，接着糟糕的事情开始发生了。
一方面，我们发现，报表任务有时候在一个小时之内执行不完，接着，偶尔整个报表任务会执行失败。于是，我们不得不停下手头开发的工作，开始排查这个问题。
用过Hadoop的话，你可能知道，作为一个分布式的应用，考虑到硬件的故障，Hadoop本身会在特定节点计算出错的情况下，重试整个计算过程。之前的报表跑得慢，就是因为有些节点的计算任务失败过，只是在重试之后又成功了。进一步分析，我们发现，程序的错误非常奇怪。有些数据计算的结果，比如“34+23”，结果应该是“57”，但是却变成了一个美元符号“$”。
前前后后折腾了一周，我们发现，从日志上看，大部分出错的任务都在几个固定的硬件节点上。
另一方面，我们发现，问题出现在我们新的一批自己定制的硬件上架之后。于是，和运维团队的同事沟通近期的硬件变更，并且翻阅大量Hadoop社区的邮件组列表之后，我们有了一个大胆的推测。
我们推测，这个错误，来自我们自己定制的硬件。定制的硬件没有使用ECC内存，在大量的数据中，内存中出现了单比特翻转（Single-Bit Flip）这个传说中的硬件错误。
那这个符号是怎么来的呢？是由于内存中的一个整数字符，遇到了一次单比特翻转转化而来的。 它的ASCII码二进制表示是0010 0100，所以它完全可能来自 0011 0100 遇到一次在第4个比特的单比特翻转，也就是从整数“4”变过来的。但是我们也只能推测是这个错误，而不能确信是这个错误。因为单比特翻转是一个随机现象，我们没法稳定复现这个问题。
ECC内存的全称是Error-Correcting Code memory，中文名字叫作纠错内存。顾名思义，就是在内存里面出现错误的时候，能够自己纠正过来。
在和运维同学沟通之后，我们把所有自己定制的服务器的内存替换成了ECC内存，之后这个问题就消失了。这也使得我们基本确信，问题的来源就是因为没有使用ECC内存。我们所有工程师的开发用机在2012年，也换成了32G内存。是的，换下来的内存没有别的去处，都安装到了研发团队的开发机上。
奇偶校验和校验位：捕捉错误的好办法其实，内存里面的单比特翻转或者错误，并不是一个特别罕见的现象。无论是因为内存的制造质量造成的漏电，还是外部的射线，都有一定的概率，会造成单比特错误。而内存层面的数据出错，软件工程师并不知道，而且这个出错很有可能是随机的。遇上随机出现难以重现的错误，大家肯定受不了。我们必须要有一个办法，避免这个问题。
其实，在ECC内存发明之前，工程师们已经开始通过奇偶校验的方式，来发现这些错误。
奇偶校验的思路很简单。我们把内存里面的N位比特当成是一组。常见的，比如8位就是一个字节。然后，用额外的一位去记录，这8个比特里面有奇数个1还是偶数个1。如果是奇数个1，那额外的一位就记录为1；如果是偶数个1，那额外的一位就记录成0。那额外的一位，我们就称之为校验码位。
如果在这个字节里面，我们不幸发生了单比特翻转，那么数据位计算得到的校验码，就和实际校验位里面的数据不一样。我们的内存就知道出错了。
除此之外，校验位有一个很大的优点，就是计算非常快，往往只需要遍历一遍需要校验的数据，通过一个O(N)的时间复杂度的算法，就能把校验结果计算出来。
校验码的思路，在很多地方都会用到。
比方说，我们下载一些软件的时候，你会看到，除了下载的包文件，还会有对应的MD5这样的哈希值或者循环冗余编码（CRC）的校验文件。这样，当我们把对应的软件下载下来之后，我们可以计算一下对应软件的校验码，和官方提供的校验码去做个比对，看看是不是一样。
如果不一样，你就不能轻易去安装这个软件了。因为有可能，这个软件包是坏的。但是，还有一种更危险的情况，就是你下载的这个软件包，可能是被人植入了后门的。安装上了之后，你的计算机的安全性就没有保障了。
不过，使用奇偶校验，还是有两个比较大的缺陷。
第一个缺陷，就是奇偶校验只能解决遇到单个位的错误，或者说奇数个位的错误。如果出现2个位进行了翻转，那么这个字节的校验位计算结果其实没有变，我们的校验位自然也就不能发现这个错误。
第二个缺陷，是它只能发现错误，但是不能纠正错误。所以，即使在内存里面发现数据错误了，我们也只能中止程序，而不能让程序继续正常地运行下去。如果这个只是我们的个人电脑，做一些无关紧要的应用，这倒是无所谓了。
但是，你想一下，如果你在服务器上进行某个复杂的计算任务，这个计算已经跑了一周乃至一个月了，还有两三天就跑完了。这个时候，出现内存里面的错误，要再从头跑起，估计你内心是崩溃的。
所以，我们需要一个比简单的校验码更好的解决方案，一个能够发现更多位的错误，并且能够把这些错误纠正过来的解决方案，也就是工程师们发明的ECC内存所使用的解决方案。
我们不仅能捕捉到错误，还要能够纠正发生的错误。这个策略，我们通常叫作纠错码（Error Correcting Code）。它还有一个升级版本，叫作纠删码（Erasure Code），不仅能够纠正错误，还能够在错误不能纠正的时候，直接把数据删除。无论是我们的ECC内存，还是网络传输，乃至硬盘的RAID，其实都利用了纠错码和纠删码的相关技术。
想要看看我们怎么通过算法，怎么配置硬件，使得我们不仅能够发现单个位的错误，而能发现更多位的错误，你一定要记得跟上下一讲的内容。
总结延伸好了，让我们一起来总结一下今天的内容。
我给你介绍了我自己亲身经历的一个硬件错误带来的Bug。由于没有采用ECC内存，导致我们的数据处理中，出现了大量的单比特数据翻转的错误。这些硬件带来的错误，其实我们没有办法在软件层面解决。
如果对于硬件以及硬件本身的原理不够熟悉，恐怕这个问题的解决方案还是遥遥无期。如果你对计算机组成原理有所了解，并能够意识到，在硬件的存储层有着数据验证和纠错的需求，那你就能在有限的时间内定位到问题所在。
进一步地，我为你简单介绍了奇偶校验，也就是如何通过冗余的一位数据，发现在硬件层面出现的位错误。但是，奇偶校验以及其他的校验码，只能发现错误，没有办法纠正错误。所以，下一讲，我们一起来看看，怎么利用纠错码这样的方式，来解决问题。
推荐阅读我推荐你去深入阅读一下Wikipedia里面，关于CRC的内容，了解一下，这样的校验码的详细算法。
课后思考有人说，奇偶校验只是循环冗余编码（CRC）的一种特殊情况。在读完推荐阅读里面的CRC算法的实现之后，你能分析一下为什么奇偶校验只是CRC的一种特殊情况吗？
欢迎把你阅读和分析的内容写在留言区，和大家一起分享。如果觉得有帮助，你也可以把今天的内容分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>50_数据完整性（下）：如何还原犯罪现场？</title><link>https://artisanbox.github.io/4/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/50/</guid><description>讲完校验码之后，你现在应该知道，无论是奇偶校验码，还是CRC这样的循环校验码，都只能告诉我们一个事情，就是你的数据出错了。所以，校验码也被称为检错码（Error Detecting Code）。
不管是校验码，还是检错码，在硬件出错的时候，只能告诉你“我错了”。但是，下一个问题，“错哪儿了”，它是回答不了的。这就导致，我们的处理方式只有一种，那就是当成“哪儿都错了”。如果是下载一个文件，发现校验码不匹配，我们只能重新去下载；如果是程序计算后放到内存里面的数据，我们只能再重新算一遍。
这样的效率实在是太低了，所以我们需要有一个办法，不仅告诉我们“我错了”，还能告诉我们“错哪儿了”。于是，计算机科学家们就发明了纠错码。纠错码需要更多的冗余信息，通过这些冗余信息，我们不仅可以知道哪里的数据错了，还能直接把数据给改对。这个是不是听起来很神奇？接下来就让我们一起来看一看。
海明码：我们需要多少信息冗余？最知名的纠错码就是海明码。海明码（Hamming Code）是以他的发明人Richard Hamming（理查德·海明）的名字命名的。这个编码方式早在上世纪四十年代就被发明出来了。而直到今天，我们上一讲所说到的ECC内存，也还在使用海明码来纠错。
最基础的海明码叫7-4海明码。这里的“7”指的是实际有效的数据，一共是7位（Bit）。而这里的“4”，指的是我们额外存储了4位数据，用来纠错。
首先，你要明白一点，纠错码的纠错能力是有限的。不是说不管错了多少位，我们都能给纠正过来。不然我们就不需要那7个数据位，只需要那4个校验位就好了，这意味着我们可以不用数据位就能传输信息了。这就不科学了。事实上，在7-4海明码里面，我们只能纠正某1位的错误。这是怎么做到的呢？我们一起来看看。
4位的校验码，一共可以表示 2^4 = 16 个不同的数。根据数据位计算出来的校验值，一定是确定的。所以，如果数据位出错了，计算出来的校验码，一定和确定的那个校验码不同。那可能的值，就是在 2^4 - 1 = 15 那剩下的15个可能的校验值当中。
15个可能的校验值，其实可以对应15个可能出错的位。这个时候你可能就会问了，既然我们的数据位只有7位，那为什么我们要用4位的校验码呢？用3位不就够了吗？2^3 - 1 = 7，正好能够对上7个不同的数据位啊！
你别忘了，单比特翻转的错误，不仅可能出现在数据位，也有可能出现在校验位。校验位本身也是可能出错的。所以，7位数据位和3位校验位，如果只有单比特出错，可能出错的位数就是10位，2^3 - 1 = 7 种情况是不能帮我们找到具体是哪一位出错的。
事实上，如果我们的数据位有K位，校验位有N位。那么我们需要满足下面这个不等式，才能确保我们能够对单比特翻转的数据纠错。这个不等式就是：
K + N + 1 &amp;lt;= 2^N在有7位数据位，也就是K=7的情况下，N的最小值就是4。4位校验位，其实最多可以支持到11位数据位。我在下面列了一个简单的数据位数和校验位数的对照表，你可以自己算一算，理解一下上面的公式。
海明码的纠错原理现在你应该搞清楚了，在数据位数确定的情况下，怎么计算需要的校验位。那接下来，我们就一起看看海明码的编码方式是怎么样的。
为了算起来简单一点，我们少用一些位数，来算一个4-3海明码（也就是4位数据位，3位校验位）。我们把4位数据位，分别记作d1、d2、d3、d4。这里的d，取的是数据位data bits的首字母。我们把3位校验位，分别记作p1、p2、p3。这里的p，取的是校验位parity bits的首字母。
从4位的数据位里面，我们拿走1位，然后计算出一个对应的校验位。这个校验位的计算用之前讲过的奇偶校验就可以了。比如，我们用d1、d2、d4来计算出一个校验位p1；用d1、d3、d4计算出一个校验位p2；用d2、d3、d4计算出一个校验位p3。就像下面这个对应的表格一样：
这个时候，你去想一想，如果d1这一位的数据出错了，会发生什么情况？我们会发现，p1和p2和校验的计算结果不一样。d2出错了，是因为p1和p3的校验的计算结果不一样；d3出错了，则是因为p2和p3；如果d4出错了，则是p1、p2、p3都不一样。你会发现，当数据码出错的时候，至少会有2位校验码的计算是不一致的。
那我们倒过来，如果是p1的校验码出错了，会发生什么情况呢？这个时候，只有p1的校验结果出错。p2和p3的出错的结果也是一样的，只有一个校验码的计算是不一致的。
所以校验码不一致，一共有 2^3-1=7种情况，正好对应了7个不同的位数的错误。我把这个对应表格也放在下面了，你可以理解一下。
可以看到，海明码这样的纠错过程，有点儿像电影里面看到的推理探案的过程。通过出错现场的额外信息，一步一步条分缕析地找出，到底是哪一位的数据出错，还原出错时候的“犯罪现场”。
看到这里，相信你一方面会觉得海明码特别神奇，但是同时也会冒出一个新的疑问，我们怎么才能用一套程序或者规则来生成海明码呢？其实这个步骤并不复杂，接下来我们就一起来看一下。
首先，我们先确定编码后，要传输的数据是多少位。比如说，我们这里的7-4海明码，就是一共11位。
然后，我们给这11位数据从左到右进行编号，并且也把它们的二进制表示写出来。
接着，我们先把这11个数据中的二进制的整数次幂找出来。在这个7-4海明码里面，就是1、2、4、8。这些数，就是我们的校验码位，我们把他们记录做p1～p4。如果从二进制的角度看，它们是这11个数当中，唯四的，在4个比特里面只有一个比特是1的数值。
那么剩下的7个数，就是我们d1-d7的数据码位了。
然后，对于我们的校验码位，我们还是用奇偶校验码。但是每一个校验码位，不是用所有的7位数据来计算校验码。而是p1用3、5、7、9、11来计算。也就是，在二进制表示下，从右往左数的第一位比特是1的情况下，用p1作为校验码。
剩下的p2，我们用3、6、10、11来计算校验码，也就是在二进制表示下，从右往左数的第二位比特是1的情况下，用p2。那么，p3自然是从右往左数，第三位比特是1的情况下的数字校验码。而p4则是第四位比特是1的情况下的校验码。
这个时候，你会发现，任何一个数据码出错了，就至少会有对应的两个或者三个校验码对不上，这样我们就能反过来找到是哪一个数据码出错了。如果校验码出错了，那么只有校验码这一位对不上，我们就知道是这个校验码出错了。
上面这个方法，我们可以用一段确定的程序表示出来，意味着无论是几位的海明码，我们都不再需要人工去精巧地设计编码方案了。
海明距离：形象理解海明码的作用其实，我们还可以换一个角度来理解海明码的作用。对于两个二进制表示的数据，他们之间有差异的位数，我们称之为海明距离。比如 1001 和 0001 的海明距离是1，因为他们只有最左侧的第一位是不同的。而1001 和 0000 的海明距离是2，因为他们最左侧和最右侧有两位是不同的。
于是，你很容易可以想到，所谓的进行一位纠错，也就是所有和我们要传输的数据的海明距离为1的数，都能被纠正回来。
而任何两个实际我们想要传输的数据，海明距离都至少要是3。你可能会问了，为什么不能是2呢？因为如果是2的话，那么就会有一个出错的数，到两个正确的数据的海明距离都是1。当我们看到这个出错的数的时候，我们就不知道究竟应该纠正到那一个数了。</description></item><item><title>50_索引：如何在海量数据中快速查找某个数据？</title><link>https://artisanbox.github.io/2/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/51/</guid><description>在第48节中，我们讲了MySQL数据库索引的实现原理。MySQL底层依赖的是B+树这种数据结构。留言里有同学问我，那类似Redis这样的Key-Value数据库中的索引，又是怎么实现的呢？底层依赖的又是什么数据结构呢？
今天，我就来讲一下索引这种常用的技术解决思路，底层往往会依赖哪些数据结构。同时，通过索引这个应用场景，我也带你回顾一下，之前我们学过的几种支持动态集合的数据结构。
为什么需要索引？在实际的软件开发中，业务纷繁复杂，功能千变万化，但是，万变不离其宗。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为“对数据的存储和计算”。对应到数据结构和算法中，那“存储”需要的就是数据结构，“计算”需要的就是算法。
对于存储的需求，功能上无外乎增删改查。这其实并不复杂。但是，一旦存储的数据很多，那性能就成了这些系统要关注的重点，特别是在一些跟存储相关的基础系统（比如MySQL数据库、分布式文件系统等）、中间件（比如消息中间件RocketMQ等）中。
“如何节省存储空间、如何提高数据增删改查的执行效率”，这样的问题就成了设计的重点。而这些系统的实现，都离不开一个东西，那就是索引。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。
索引这个概念，非常好理解。你可以类比书籍的目录来理解。如果没有目录，我们想要查找某个知识点的时候，就要一页一页翻。通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。
索引的需求定义索引的概念不难理解，我想你应该已经搞明白。接下来，我们就分析一下，在设计索引的过程中，需要考虑到的一些因素，换句话说就是，我们该如何定义清楚需求呢？
对于系统设计需求，我们一般可以从功能性需求和非功能性需求两方面来分析，这个我们之前也说过。因此，这个问题也不例外。
1.功能性需求对于功能性需求需要考虑的点，我把它们大致概括成下面这几点。
数据是格式化数据还是非格式化数据？要构建索引的原始数据，类型有很多。我把它分为两类，一类是结构化数据，比如，MySQL中的数据；另一类是非结构化数据，比如搜索引擎中网页。对于非结构化数据，我们一般需要做预处理，提取出查询关键词，对关键词构建索引。
数据是静态数据还是动态数据？如果原始数据是一组静态数据，也就是说，不会有数据的增加、删除、更新操作，所以，我们在构建索引的时候，只需要考虑查询效率就可以了。这样，索引的构建就相对简单些。不过，大部分情况下，我们都是对动态数据构建索引，也就是说，我们不仅要考虑到索引的查询效率，在原始数据更新的同时，我们还需要动态地更新索引。支持动态数据集合的索引，设计起来相对也要更加复杂些。
索引存储在内存还是硬盘？如果索引存储在内存中，那查询的速度肯定要比存储在磁盘中的高。但是，如果原始数据量很大的情况下，对应的索引可能也会很大。这个时候，因为内存有限，我们可能就不得不将索引存储在磁盘中了。实际上，还有第三种情况，那就是一部分存储在内存，一部分存储在磁盘，这样就可以兼顾内存消耗和查询效率。
单值查找还是区间查找？所谓单值查找，也就是根据查询关键词等于某个值的数据。这种查询需求最常见。所谓区间查找，就是查找关键词处于某个区间值的所有数据。你可以类比MySQL数据库的查询需求，自己想象一下。实际上，不同的应用场景，查询的需求会多种多样。
单关键词查找还是多关键词组合查找？比如，搜索引擎中构建的索引，既要支持一个关键词的查找，比如“数据结构”，也要支持组合关键词查找，比如“数据结构 AND 算法”。对于单关键词的查找，索引构建起来相对简单些。对于多关键词查询来说，要分多种情况。像MySQL这种结构化数据的查询需求，我们可以实现针对多个关键词的组合，建立索引；对于像搜索引擎这样的非结构数据的查询需求，我们可以针对单个关键词构建索引，然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。
实际上，不同的场景，不同的原始数据，对于索引的需求也会千差万别。我这里只列举了一些比较有共性的需求。
2.非功能性需求讲完了功能性需求，我们再来看，索引设计的非功能性需求。
不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非常有限，一个中间件启动后就占用几个GB的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会超过原始数据。
在考虑索引查询效率的同时，我们还要考虑索引的维护成本。索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，我们还要考虑到，索引的维护成本。因为在原始数据动态增删改的同时，我们也需要动态地更新索引。而索引的更新势必会影响到增删改操作的性能。
构建索引常用的数据结构有哪些？我刚刚从很宏观的角度，总结了在索引设计的过程中，需要考虑的一些共性因素。现在，我们就来看，对于不同需求的索引结构，底层一般使用哪种数据结构。
实际上，常用来构建索引的数据结构，就是我们之前讲过的几种支持动态数据集合的数据结构。比如，散列表、红黑树、跳表、B+树。除此之外，位图、布隆过滤器可以作为辅助索引，有序数组可以用来对静态数据构建索引。
我们知道，散列表增删改查操作的性能非常好，时间复杂度是O(1)。一些键值数据库，比如Redis、Memcache，就是使用散列表来构建索引的。这类索引，一般都构建在内存中。
红黑树作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是O(logn)，也非常适合用来构建内存索引。Ext文件系统中，对磁盘块的索引，用的就是红黑树。
B+树比起红黑树来说，更加适合构建存储在磁盘中的索引。B+树是一个多叉树，所以，对相同个数的数据构建索引，B+树的高度要低于红黑树。当借助索引查询数据的时候，读取B+树索引，需要的磁盘IO次数会更少。所以，大部分关系型数据库的索引，比如MySQL、Oracle，都是用B+树来实现的。
跳表也支持快速添加、删除、查找数据。而且，我们通过灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率。Redis中的有序集合，就是用跳表来构建的。
除了散列表、红黑树、B+树、跳表之外，位图和布隆过滤器这两个数据结构，也可以用于索引中，辅助存储在磁盘中的索引，加速数据查找的效率。我们来看下，具体是怎么做的？
我们知道，布隆过滤器有一定的判错率。但是，我们可以规避它的短处，发挥它的长处。尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在。而且，布隆过滤器还有一个更大的特点，那就是内存占用非常少。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。
实际上，有序数组也可以被作为索引。如果数据是静态的，也就是不会有插入、删除、更新操作，那我们可以把数据的关键词（查询用的）抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。
总结引申今天这节算是一节总结课。我从索引这个非常常用的技术方案，给你展示了散列表、红黑树、跳表、位图、布隆过滤器、有序数组这些数据结构的应用场景。学习完这节课之后，不知道你对这些数据结构以及索引，有没有更加清晰的认识呢？
从这一节内容中，你应该可以看出，架构设计离不开数据结构和算法。要想成长为一个优秀的业务架构师、基础架构师，数据结构和算法的根基一定要打稳。因为，那些看似很惊艳的架构设计思路，实际上，都是来自最常用的数据结构和算法。
课后思考你知道基础系统、中间件、开源软件等系统中，有哪些用到了索引吗？这些系统的索引是如何实现的呢？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>51_分布式计算：如果所有人的大脑都联网会怎样？</title><link>https://artisanbox.github.io/4/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/51/</guid><description>今天是原理篇的最后一篇。过去50讲，我们一起看了抽象概念上的计算机指令，看了这些指令怎么拆解成一个个简单的电路，以及CPU是怎么通过一个一个的电路组成的。我们还一起看了高速缓存、内存、SSD硬盘和机械硬盘，以及这些组件又是怎么通过总线和CPU连在一起相互通信的。
把计算机这一系列组件组合起来，我们就拿到了一台完整的计算机。现在我们每天在用的个人PC、智能手机，乃至云上的服务器，都是这样一台计算机。
但是，一台计算机在数据中心里是不够的。因为如果只有一台计算机，我们会遇到三个核心问题。第一个核心问题，叫作垂直扩展和水平扩展的选择问题，第二问题叫作如何保持高可用性（High Availability），第三个问题叫作一致性问题（Consistency）。
围绕这三个问题，其实就是我们今天要讲的主题，分布式计算。当然，短短的一讲肯定讲不完这么大一个主题。分布式计算拿出来单开一门专栏也绰绰有余。我们今天这一讲的目标，是让你能理解水平扩展、高可用性这两个核心问题。对于分布式系统带来的一致性问题，我们会留在我们的实战篇里面，再用案例来为大家分析。
从硬件升级到水平扩展从技术开发的角度来讲，想要在2019年创业真的很幸福。只要在AWS或者阿里云这样的云服务上注册一个账号，一个月花上一两百块钱，你就可以有一台在数据中心里面的服务器了。而且这台服务器，可以直接提供给世界各国人民访问。如果你想要做海外市场，你可以把这个服务器放在美国、欧洲、东南亚，任何一个你想要去的市场的数据中心里，然后把自己的网站部署在这台服务器里面就可以了。
现在在云服务商购买服务器的成本和方便程度都已经很高了当然，这台服务器就是我们在第34讲里说的虚拟机。不过因为只是个业余时间的小项目，一开始这台服务器的配置也不会太高。我以我现在公司所用的Google Cloud为例。最低的配置差不多是1个CPU核心、3.75G内存以及一块10G的SSD系统盘。这样一台服务器每个月的价格差不多是28美元。
幸运的是，你的网站很受大家欢迎，访问量也上来了。这个时候，这台单核心的服务器的性能有点不够用了。这个时候，你需要升级你的服务器。于是，你就会面临两个选择。
第一个选择是升级现在这台服务器的硬件，变成2个CPU核心、7.5G内存。这样的选择我们称之为垂直扩展（Scale Up）。第二个选择则是我们再租用一台和之前一样的服务器。于是，我们有了2台1个CPU核心、3.75G内存的服务器。这样的选择我们称之为水平扩展（Scale Out）。
在这个阶段，这两个选择，从成本上看起来没有什么差异。2核心、7.5G内存的服务器，成本是56.61美元，而2台1核心、3.75G内存的服务器价格，成本是57美元，这之间的价格差异不到1%。
不过，垂直扩展和水平扩展看似是两个不同的选择，但是随着流量不断增长。到最后，只会变成一个选择。那就是既会垂直扩展，又会水平扩展，并且最终依靠水平扩展，来支撑Google、Facebook、阿里、腾讯这样体量的互联网服务。
垂直扩展背后的逻辑和优势都很简单。一般来说，垂直扩展通常不需要我们去改造程序，也就是说，我们没有研发成本。那为什么我们最终还是要用水平扩展呢？你可以先自己想一想。
原因其实很简单，因为我们没有办法不停地去做垂直扩展。我们在Google Cloud上现在能够买到的性能最好的服务器，是96个CPU核心、1.4TB的内存。如果我们的访问量逐渐增大，一台96核心的服务器也支撑不了了，那么我们就没有办法再去做垂直扩展了。这个时候，我们就不得不采用水平扩展的方案了。
96个CPU核心看起来是个很强大的服务器，但是你算一算就知道，其实它的计算资源并没有多大。你现在多半在用一台4核心，或者至少也是2核心的CPU。96个CPU也就是30～50台日常使用的开发机的计算性能。而我们今天在互联网上遇到的问题，是每天数亿的访问量，靠30～50台个人电脑的计算能力想要支撑这样的计算需求，可谓是天方夜谭了。
然而，一旦开始采用水平扩展，我们就会面临在软件层面改造的问题了。也就是我们需要开始进行分布式计算了。我们需要引入负载均衡（Load Balancer）这样的组件，来进行流量分配。我们需要拆分应用服务器和数据库服务器，来进行垂直功能的切分。我们也需要不同的应用之间通过消息队列，来进行异步任务的执行。
所有这些软件层面的改造，其实都是在做分布式计算的一个核心工作，就是通过消息传递（Message Passing）而不是共享内存（Shared Memory）的方式，让多台不同的计算机协作起来共同完成任务。
而因为我们最终必然要进行水平扩展，我们需要在系统设计的早期就基于消息传递而非共享内存来设计系统。即使这些消息只是在同一台服务器上进行传递。
事实上，有不少增长迅猛的公司，早期没有准备好通过水平扩展来支撑访问量的情况，而一味通过提升硬件配置Scale Up，来支撑更大的访问量，最终影响了公司的存亡。最典型的例子，就是败在Facebook手下的MySpace。
理解高可用性和单点故障尽管在1个CPU核心的服务器支撑不了我们的访问量的时候，选择垂直扩展是一个最简单的办法。不过如果是我的话，第一次扩展我会选择水平扩展。
选择水平扩展的一个很好的理由，自然是可以“强迫”从开发的角度，尽早地让系统能够支持水平扩展，避免在真的流量快速增长的时候，垂直扩展的解决方案跟不上趟。不过，其实还有一个更重要的理由，那就是系统的可用性问题。
上面的1核变2核的垂直扩展的方式，扩展完之后，我们还是只有1台服务器。如果这台服务器出现了一点硬件故障，比如，CPU坏了，那我们的整个系统就坏了，就不可用了。
如果采用了水平扩展，即便有一台服务器的CPU坏了，我们还有另外一台服务器仍然能够提供服务。负载均衡能够通过健康检测（Health Check）发现坏掉的服务器没有响应了，就可以自动把所有的流量切换到第2台服务器上，这个操作就叫作故障转移（Failover），我们的系统仍然是可用的。
系统的可用性（Avaiability）指的就是，我们的系统可以正常服务的时间占比。无论是因为软硬件故障，还是需要对系统进行停机升级，都会让我们损失系统的可用性。可用性通常是用一个百分比的数字来表示，比如99.99%。我们说，系统每个月的可用性要保障在99.99%，也就是意味着一个月里，你的服务宕机的时间不能超过4.32分钟。
有些系统可用性的损失，是在我们计划内的。比如上面说的停机升级，这个就是所谓的计划内停机时间（Scheduled Downtime）。有些系统可用性的损失，是在我们计划外的，比如一台服务器的硬盘忽然坏了，这个就是所谓的计划外停机时间（Unscheduled Downtime）。
我们的系统是一定不可能做到100%可用的，特别是计划外的停机时间。从简单的硬件损坏，到机房停电、光缆被挖断，乃至于各种自然灾害，比如地震、洪水、海啸，都有可能使得我们的系统不可用。作为一个工程师和架构师，我们要做的就是尽可能低成本地提高系统的可用性。
咱们的专栏是要讲计算机组成原理，那我们先来看一看硬件服务器的可用性。
现在的服务器的可用性都已经很不错了，通常都能保障99.99%的可用性了。如果我们有一个小小的三台服务器组成的小系统，一台部署了Nginx来作为负载均衡和反向代理，一台跑了PHP-FPM作为Web应用服务器，一台用来作为MySQL数据库服务器。每台服务器的可用性都是99.99%。那么我们整个系统的可用性是多少呢？你可以先想一想。
答案是99.99% × 99.99% × 99.99% = 99.97%。在这个系统当中，这个数字看起来似乎没有那么大区别。不过反过来看，我们是从损失了0.01%的可用性，变成了损失0.03%的可用性，不可用的时间变成了原来的3倍。
如果我们有1000台服务器，那么整个的可用性，就会变成 99.99% ^ 1000 = 90.5%。也就是说，我们的服务一年里有超过一个月是不可用的。这可怎么办呀？
我们先来分析一下原因。之所以会出现这个问题，是因为在这个场景下，任何一台服务器出错了，整个系统就没法用了。这个问题就叫作单点故障问题（Single Point of Failure，SPOF）。我们这里的这个假设特别糟糕。我们假设这1000台服务器，每一个都存在单点故障问题。所以，我们的服务也就特别脆弱，随便哪台出现点风吹草动，整个服务就挂了。
要解决单点故障问题，第一点就是要移除单点。其实移除单点最典型的场景，在我们水平扩展应用服务器的时候就已经看到了，那就是让两台服务器提供相同的功能，然后通过负载均衡把流量分发到两台不同的服务器去。即使一台服务器挂了，还有一台服务器可以正常提供服务。
不过光用两台服务器是不够的，单点故障其实在数据中心里面无处不在。我们现在用的是云上的两台虚拟机。如果这两台虚拟机是托管在同一台物理机上的，那这台物理机本身又成为了一个单点。那我们就需要把这两台虚拟机分到两台不同的物理机上。
不过这个还是不够。如果这两台物理机在同一个机架（Rack）上，那机架上的交换机（Switch）就成了一个单点。即使放到不同的机架上，还是有可能出现整个数据中心遭遇意外故障的情况。
去年我自己就遇到过，部署在Azure上的服务所在的数据中心，因为散热问题触发了整个数据中心所有服务器被关闭的问题。面对这种情况，我们就需要设计进行异地多活的系统设计和部署。所以，在现代的云服务，你在买服务器的时候可以选择服务器的area（地区）和zone（区域），而要不要把服务器放在不同的地区或者区域里，也是避免单点故障的一个重要因素。
只是能够去除单点，其实我们的可用性问题还没有解决。比如，上面我们用负载均衡把流量均匀地分发到2台服务器上，当一台应用服务器挂掉的时候，我们的确还有一台服务器在提供服务。但是负载均衡会把一半的流量发到已经挂掉的服务器上，所以这个时候只能算作一半可用。
想要让整个服务完全可用，我们就需要有一套故障转移（Failover）机制。想要进行故障转移，就首先要能发现故障。
以我们这里的PHP-FPM的Web应用为例，负载均衡通常会定时去请求一个Web应用提供的健康检测（Health Check）的地址。这个时间间隔可能是5秒钟，如果连续2～3次发现健康检测失败，负载均衡就会自动将这台服务器的流量切换到其他服务器上。于是，我们就自动地产生了一次故障转移。故障转移的自动化在大型系统里是很重要的，因为服务器越多，出现故障基本就是个必然发生的事情。而自动化的故障转移既能够减少运维的人手需求，也能够缩短从故障发现到问题解决的时间周期，提高可用性。
我们在Web应用上设置了一个Heartbeat接口，每20秒检查一次，出现问题的时候可以进行故障转移切换那么，让我们算一算，通过水平扩展相同功能的服务器来去掉单点故障，并且通过健康检查机制来触发自动的故障转移，这样的可用性会变成多少呢？你可以拿出纸和笔来试一下。
不知道你想明白应该怎么算了没有，在这种情况下，我们其实只要有任何一台服务器能够正常运转，就能正常提供服务。那么，我们的可用性就是：
100% - (100% - 99.</description></item><item><title>51_并行算法：如何利用并行处理提高算法的执行效率？</title><link>https://artisanbox.github.io/2/52/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/52/</guid><description>时间复杂度是衡量算法执行效率的一种标准。但是，时间复杂度并不能跟性能划等号。在真实的软件开发中，即便在不降低时间复杂度的情况下，也可以通过一些优化手段，提升代码的执行效率。毕竟，对于实际的软件开发来说，即便是像10%、20%这样微小的性能提升，也是非常可观的。
算法的目的就是为了提高代码执行的效率。那当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢？我们今天就讲一种非常简单但又非常好用的优化方法，那就是并行计算。今天，我就通过几个例子，给你展示一下，如何借助并行计算的处理思想对算法进行改造？
并行排序假设我们要给大小为8GB的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据。对于排序来说，最常用的就是时间复杂度为O(nlogn)的三种排序算法，归并排序、快速排序、堆排序。从理论上讲，这个排序问题，已经很难再从算法层面优化了。而利用并行的处理思想，我们可以很轻松地将这个给8GB数据排序问题的执行效率提高很多倍。具体的实现思路有下面两种。
第一种是对归并排序并行化处理。我们可以将这8GB的数据划分成16个小的数据集合，每个集合包含500MB的数据。我们用16个线程，并行地对这16个500MB的数据集合进行排序。这16个小集合分别排序完成之后，我们再将这16个有序集合合并。
第二种是对快速排序并行化处理。我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成16个小区间。我们将8GB的数据划分到对应的区间中。针对这16个小区间的数据，我们启动16个线程，并行地进行排序。等到16个线程都执行结束之后，得到的数据就是有序数据了。
对比这两种处理思路，它们利用的都是分治的思想，对数据进行分片，然后并行处理。它们的区别在于，第一种处理思路是，先随意地对数据分片，排序之后再合并。第二种处理思路是，先对数据按照大小划分区间，然后再排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。
这里我还要多说几句，如果要排序的数据规模不是8GB，而是1TB，那问题的重点就不是算法的执行效率了，而是数据的读取效率。因为1TB的数据肯定是存在硬盘中，无法一次性读取到内存中，这样在排序的过程中，就会有频繁地磁盘数据的读取和写入。如何减少磁盘的IO操作，减少磁盘数据读取和写入的总量，就变成了优化的重点。不过这个不是我们这节要讨论的重点，你可以自己思考下。
并行查找我们知道，散列表是一种非常适合快速查找的数据结构。
如果我们是给动态数据构建索引，在数据不断加入的时候，散列表的装载因子就会越来越大。为了保证散列表性能不下降，我们就需要对散列表进行动态扩容。对如此大的散列表进行动态扩容，一方面比较耗时，另一方面比较消耗内存。比如，我们给一个2GB大小的散列表进行扩容，扩展到原来的1.5倍，也就是3GB大小。这个时候，实际存储在散列表中的数据只有不到2GB，所以内存的利用率只有60%，有1GB的内存是空闲的。
实际上，我们可以将数据随机分割成k份（比如16份），每份中的数据只有原来的1/k，然后我们针对这k个小数据集合分别构建散列表。这样，散列表的维护成本就变低了。当某个小散列表的装载因子过大的时候，我们可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。
还是刚才那个例子，假设现在有2GB的数据，我们放到16个散列表中，每个散列表中的数据大约是150MB。当某个散列表需要扩容的时候，我们只需要额外增加150*0.5=75MB的内存（假设还是扩容到原来的1.5倍）。无论从扩容的执行效率还是内存的利用率上，这种多个小散列表的处理方法，都要比大散列表高效。
当我们要查找某个数据的时候，我们只需要通过16个线程，并行地在这16个散列表中查找数据。这样的查找性能，比起一个大散列表的做法，也并不会下降，反倒有可能提高。
当往散列表中添加数据的时候，我们可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。
并行字符串匹配我们前面学过，在文本中查找某个关键词这样一个功能，可以通过字符串匹配算法来实现。我们之前学过的字符串匹配算法有KMP、BM、RK、BF等。当在一个不是很长的文本中查找关键词的时候，这些字符串匹配算法中的任何一个，都可以表现得非常高效。但是，如果我们处理的是超级大的文本，那处理的时间可能就会变得很长，那有没有办法加快匹配速度呢？
我们可以把大的文本，分割成k个小文本。假设k是16，我们就启动16个线程，并行地在这16个小文本中查找关键词，这样整个查找的性能就提高了16倍。16倍效率的提升，从理论的角度来说并不多。但是，对于真实的软件开发来说，这显然是一个非常可观的优化。
不过，这里还有一个细节要处理，那就是原本包含在大文本中的关键词，被一分为二，分割到两个小文本中，这就会导致尽管大文本中包含这个关键词，但在这16个小文本中查找不到它。实际上，这个问题也不难解决，我们只需要针对这种特殊情况，做一些特殊处理就可以了。
我们假设关键词的长度是m。我们在每个小文本的结尾和开始各取m个字符串。前一个小文本的末尾m个字符和后一个小文本的开头m个字符，组成一个长度是2m的字符串。我们再拿关键词，在这个长度为2m的字符串中再重新查找一遍，就可以补上刚才的漏洞了。
并行搜索前面我们学习过好几种搜索算法，它们分别是广度优先搜索、深度优先搜索、Dijkstra最短路径算法、A*启发式搜索算法。对于广度优先搜索算法，我们也可以将其改造成并行算法。
广度优先搜索是一种逐层搜索的搜索策略。基于当前这一层顶点，我们可以启动多个线程，并行地搜索下一层的顶点。在代码实现方面，原来广度优先搜索的代码实现，是通过一个队列来记录已经遍历到但还没有扩展的顶点。现在，经过改造之后的并行广度优先搜索算法，我们需要利用两个队列来完成扩展顶点的工作。
假设这两个队列分别是队列A和队列B。多线程并行处理队列A中的顶点，并将扩展得到的顶点存储在队列B中。等队列A中的顶点都扩展完成之后，队列A被清空，我们再并行地扩展队列B中的顶点，并将扩展出来的顶点存储在队列A。这样两个队列循环使用，就可以实现并行广度优先搜索算法。
总结引申上一节，我们通过实际软件开发中的“索引”这一技术点，回顾了之前学过的一些支持动态数据集合的数据结构。今天，我们又通过“并行算法”这个话题，回顾了之前学过的一些算法。
今天的内容比较简单，没有太复杂的知识点。我通过一些例子，比如并行排序、查找、搜索、字符串匹配，给你展示了并行处理的实现思路，也就是对数据进行分片，对没有依赖关系的任务，并行地执行。
并行计算是一个工程上的实现思路，尽管跟算法关系不大，但是，在实际的软件开发中，它确实可以非常巧妙地提高程序的运行效率，是一种非常好用的性能优化手段。
特别是，当要处理的数据规模达到一定程度之后，我们无法通过继续优化算法，来提高执行效率 的时候，我们就需要在实现的思路上做文章，利用更多的硬件资源，来加快执行的效率。所以，在很多超大规模数据处理中，并行处理的思想，应用非常广泛，比如MapReduce实际上就是一种并行计算框架。
课后思考假设我们有n个任务，为了提高执行的效率，我们希望能并行执行任务，但是各个任务之间又有一定的依赖关系，如何根据依赖关系找出可以并行执行的任务？
欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>52_算法实战（一）：剖析Redis常用数据类型对应的数据结构</title><link>https://artisanbox.github.io/2/53/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/53/</guid><description>到此为止，专栏前三部分我们全部讲完了。从今天开始，我们就正式进入实战篇的部分。这部分我主要通过一些开源项目、经典系统，真枪实弹地教你，如何将数据结构和算法应用到项目中。所以这部分的内容，更多的是知识点的回顾，相对于基础篇、高级篇的内容，其实这部分会更加容易看懂。
不过，我希望你不要只是看懂就完了。你要多举一反三地思考，自己接触过的开源项目、基础框架、中间件中，都用过哪些数据结构和算法。你也可以想一想，在自己做的项目中，有哪些可以用学过的数据结构和算法进一步优化。这样的学习效果才会更好。
好了，今天我就带你一块儿看下，经典数据库Redis中的常用数据类型，底层都是用哪种数据结构实现的？
Redis数据库介绍Redis是一种键值（Key-Value）数据库。相对于关系型数据库（比如MySQL），Redis也被叫作非关系型数据库。
像MySQL这样的关系型数据库，表的结构比较复杂，会包含很多字段，可以通过SQL语句，来实现非常复杂的查询需求。而Redis中只包含“键”和“值”两部分，只能通过“键”来查询“值”。正是因为这样简单的存储结构，也让Redis的读写效率非常高。
除此之外，Redis主要是作为内存数据库来使用，也就是说，数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中。这一点，我们后面会介绍。
Redis中，键的数据类型是字符串，但是为了丰富数据存储的方式，方便开发者使用，值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。
“字符串（string）”这种数据类型非常简单，对应到数据结构里，就是字符串。你应该非常熟悉，这里我就不多介绍了。我们着重看下，其他四种比较复杂点的数据类型，看看它们底层都依赖了哪些数据结构。
列表（list）我们先来看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是压缩列表（ziplist），另一种是双向循环链表。
当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件：
列表中保存的单个数据（有可能是字符串类型的）小于64字节；
列表中数据个数少于512个。
关于压缩列表，我这里稍微解释一下。它并不是基础数据结构，而是Redis自己设计的一种数据存储结构。它有点儿类似数组，通过一片连续的内存空间，来存储数据。不过，它跟数组不同的一点是，它允许存储的数据大小不同。具体的存储结构也非常简单，你可以看我下面画的这幅图。
现在，我们来看看，压缩列表中的“压缩”两个字该如何理解？
听到“压缩”两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存，是相较于数组的存储思路而言的。我们知道，数组要求每个元素的大小相同，如果我们要存储不同长度的字符串，那我们就需要用最大长度的字符串大小作为元素的大小（假设是20个字节）。那当我们存储小于20个字节长度的字符串的时候，便会浪费部分存储空间。听起来有点儿拗口，我画个图解释一下。
压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。而且，因为数据存储在一片连续的内存空间，通过键来获取值为列表类型的数据，读取的效率也非常高。
当列表中存储的数据量比较大的时候，也就是不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。
在链表里，我们已经讲过双向循环链表这种数据结构了，如果不记得了，你可以先回去复习一下。这里我们着重看一下Redis中双向链表的编码实现方式。
Redis的这种双向链表的实现方式，非常值得借鉴。它额外定义一个list结构体，来组织链表的首、尾指针，还有长度等信息。这样，在使用的时候就会非常方便。
// 以下是C语言代码，因为Redis是用C语言实现的。 typedef struct listnode { struct listNode *prev; struct listNode *next; void *value; } listNode; typedef struct list { listNode *head; listNode *tail; unsigned long len; // &amp;hellip;.省略其他定义 } list; 字典（hash）字典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的压缩列表，另一种是散列表。
同样，只有当存储的数据量比较小的情况下，Redis才使用压缩列表来实现字典类型。具体需要满足两个条件：
字典中保存的键和值的大小都要小于64字节；
字典中键值对的个数要小于512个。
当不能同时满足上面两个条件的时候，Redis就使用散列表来实现字典类型。Redis使用MurmurHash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，Redis使用链表法来解决。除此之外，Redis还支持散列表的动态扩容、缩容。
当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于1的时候，Redis会触发扩容，将散列表扩大为原来大小的2倍左右（具体值需要计算才能得到，如果感兴趣，你可以去阅读源码）。
当数据动态减少之后，为了节省内存，当装载因子小于0.1的时候，Redis就会触发缩容，缩小为字典中数据个数的大约2倍大小（这个值也是计算得到的，如果感兴趣，你也可以去阅读源码）。
我们前面讲过，扩容缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。针对这个问题，Redis使用我们在散列表（中）讲的渐进式扩容缩容策略，将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。
集合（set）集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。</description></item><item><title>52_设计大型DMP系统（上）：MongoDB并不是什么灵丹妙药</title><link>https://artisanbox.github.io/4/52/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/52/</guid><description>如果你一讲一讲跟到现在，那首先要恭喜你，马上就看到胜利的曙光了。过去的50多讲里，我把计算机组成原理中的各个知识点，一点一点和你拆解了。对于其中的很多知识点，我也给了相应的代码示例和实际的应用案例。
不过呢，相信你和我一样，觉得只了解这样一个个零散的知识点和案例还不过瘾。那么从今天开始，我们就进入应用篇。我会通过两个应用系统的案例，串联起计算机组成原理的两大块知识点，一个是我们的整个存储器系统，另一个自然是我们的CPU和指令系统了。
我们今天就先从搭建一个大型的DMP系统开始，利用组成原理里面学到的存储器知识，来做选型判断，从而更深入地理解计算机组成原理。
DMP：数据管理平台我们先来看一下什么是DMP系统。DMP系统的全称叫作数据管理平台（Data Management Platform），目前广泛应用在互联网的广告定向（Ad Targeting）、个性化推荐（Recommendation）这些领域。
通常来说，DMP系统会通过处理海量的互联网访问数据以及机器学习算法，给一个用户标注上各种各样的标签。然后，在我们做个性化推荐和广告投放的时候，再利用这些这些标签，去做实际的广告排序、推荐等工作。无论是Google的搜索广告、淘宝里千人千面的商品信息，还是抖音里面的信息流推荐，背后都会有一个DMP系统。
那么，一个DMP系统应该怎么搭建呢？对于外部使用DMP的系统或者用户来说，可以简单地把DMP看成是一个键-值对（Key-Value）数据库。我们的广告系统或者推荐系统，可以通过一个客户端输入用户的唯一标识（ID），然后拿到这个用户的各种信息。
这些信息中，有些是用户的人口属性信息（Demographic），比如性别、年龄；有些是非常具体的行为（Behavior），比如用户最近看过的商品是什么，用户的手机型号是什么；有一些是我们通过算法系统计算出来的兴趣（Interests），比如用户喜欢健身、听音乐；还有一些则是完全通过机器学习算法得出的用户向量，给后面的推荐算法或者广告算法作为数据输入。
基于此，对于这个KV数据库，我们的期望也很清楚，那就是：低响应时间（Low Response Time）、高可用性（High Availability）、高并发（High Concurrency）、海量数据（Big Data），同时我们需要付得起对应的成本（Affordable Cost）。如果用数字来衡量这些指标，那么我们的期望就会具体化成下面这样。
低响应时间：一般的广告系统留给整个广告投放决策的时间也就是10ms左右，所以对于访问DMP获取用户数据，预期的响应时间都在1ms之内。 高可用性：DMP常常用在广告系统里面。DMP系统出问题，往往就意味着我们整个的广告收入在不可用的时间就没了，所以我们对于可用性的追求可谓是没有上限的。Google 2018年的广告收入是1160亿美元，折合到每一分钟的收入是22万美元。即使我们做到 99.99% 的可用性，也意味着每个月我们都会损失100万美元。 高并发：还是以广告系统为例，如果每天我们需要响应100亿次的广告请求，那么我们每秒的并发请求数就在 100亿 / (86400) ~= 12K 次左右，所以我们的DMP需要支持高并发。 数据量：如果我们的产品针对中国市场，那么我们需要有10亿个Key，对应的假设每个用户有500个标签，标签有对应的分数。标签和分数都用一个4字节（Bytes）的整数来表示，那么一共我们需要 10亿 x 500 x (4 + 4) Bytes = 4 TB 的数据了。 低成本：我们还是从广告系统的角度来考虑。广告系统的收入通常用CPM（Cost Per Mille），也就是千次曝光来统计。如果千次曝光的利润是 0.10美元，那么每天100亿次的曝光就是100万美元的利润。这个利润听起来非常高了。但是反过来算一下，你会发现，DMP每1000次的请求的成本不能超过 0.10美元。最好只有0.01美元，甚至更低，我们才能尽可能多赚到一点广告利润。 这五个因素一结合，听起来是不是就不那么简单了？不过，更复杂的还在后面呢。
虽然从外部看起来，DMP特别简单，就是一个KV数据库，但是生成这个数据库需要做的事情更多。我们下面一起来看一看。
在这个系统中，我们关心的是蓝色的数据管道、绿色的数据仓库和KV数据库为了能够生成这个KV数据库，我们需要有一个在客户端或者Web端的数据采集模块，不断采集用户的行为，向后端的服务器发送数据。服务器端接收到数据，就要把这份数据放到一个数据管道（Data Pipeline）里面。数据管道的下游，需要实际将数据落地到数据仓库（Data Warehouse），把所有的这些数据结构化地存储起来。后续，我们就可以通过程序去分析这部分日志，生成报表或者或者利用数据运行各种机器学习算法。
除了这个数据仓库之外，我们还会有一个实时数据处理模块（Realtime Data Processing），也放在数据管道的下游。它同样会读取数据管道里面的数据，去进行各种实时计算，然后把需要的结果写入到DMP的KV数据库里面去。
MongoDB真的万能吗？面对这里的KV数据库、数据管道以及数据仓库，这三个不同的数据存储的需求，最合理的技术方案是什么呢？你可以先自己思考一下，我这里先卖个关子。
我共事过的不少不错的Web程序员，面对这个问题的时候，常常会说：“这有什么难的，用MongoDB就好了呀！”如果你也选择了MongoDB，那最终的结果一定是一场灾难。我为什么这么说呢？
MongoDB的设计听起来特别厉害，不需要预先数据Schema，访问速度很快，还能够无限水平扩展。作为KV数据库，我们可以把MongoDB当作DMP里面的KV数据库；除此之外，MongoDB还能水平扩展、跑MQL，我们可以把它当作数据仓库来用。至于数据管道，只要我们能够不断往MongoDB里面，插入新的数据就好了。从运维的角度来说，我们只需要维护一种数据库，技术栈也变得简单了。看起来，MongoDB这个选择真是相当完美！
但是，作为一个老程序员，第一次听到MongoDB这样“万能”的解决方案，我的第一反应是，“天底下哪有这样的好事”。所有的软件系统，都有它的适用场景，想通过一种解决方案适用三个差异非常大的应用场景，显然既不合理，又不现实。接下来，我们就来仔细看一下，这个“不合理”“不现实”在什么地方。
上面我们已经讲过DMP的KV数据库期望的应用场景和性能要求了，这里我们就来看一下数据管道和数据仓库的性能取舍。
对于数据管道来说，我们需要的是高吞吐量，它的并发量虽然和KV数据库差不多，但是在响应时间上，要求就没有那么严格了，1-2秒甚至再多几秒的延时都是可以接受的。而且，和KV数据库不太一样，数据管道的数据读写都是顺序读写，没有大量的随机读写的需求。
数据仓库就更不一样了，数据仓库的数据读取的量要比管道大得多。管道的数据读取就是我们当时写入的数据，一天有10TB日志数据，管道只会写入10TB。下游的数据仓库存放数据和实时数据模块读取的数据，再加上个2倍的10TB，也就是20TB也就够了。
但是，数据仓库的数据分析任务要读取的数据量就大多了。一方面，我们可能要分析一周、一个月乃至一个季度的数据。这一次分析要读取的数据可不是10TB，而是100TB乃至1PB。我们一天在数据仓库上跑的分析任务也不是1个，而是成千上万个，所以数据的读取量是巨大的。另一方面，我们存储在数据仓库里面的数据，也不像数据管道一样，存放几个小时、最多一天的数据，而是往往要存上3个月甚至是1年的数据。所以，我们需要的是1PB乃至5PB这样的存储空间。
我把KV数据库、数据管道和数据仓库的应用场景，总结成了一个表格，放在这里。你可以对照着看一下，想想为什么MongoDB在这三个应用场景都不合适。
在KV数据库的场景下，需要支持高并发。那么MongoDB需要把更多的数据放在内存里面，但是这样我们的存储成本就会特别高了。
在数据管道的场景下，我们需要的是大量的顺序读写，而MongoDB则是一个文档数据库系统，并没有为顺序写入和吞吐量做过优化，看起来也不太适用。
而在数据仓库的场景下，主要的数据读取时顺序读取，并且需要海量的存储。MongoDB这样的文档式数据库也没有为海量的顺序读做过优化，仍然不是一个最佳的解决方案。而且文档数据库里总是会有很多冗余的字段的元数据，还会浪费更多的存储空间。
那我们该选择什么样的解决方案呢？</description></item><item><title>53_算法实战（二）：剖析搜索引擎背后的经典数据结构和算法</title><link>https://artisanbox.github.io/2/54/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/54/</guid><description>像百度、Google这样的搜索引擎，在我们平时的工作、生活中，几乎天天都会用到。如果我们把搜索引擎也当作一个互联网产品的话，那它跟社交、电商这些类型的产品相比，有一个非常大的区别，那就是，它是一个技术驱动的产品。所谓技术驱动是指，搜索引擎实现起来，技术难度非常大，技术的好坏直接决定了这个产品的核心竞争力。
在搜索引擎的设计与实现中，会用到大量的算法。有很多针对特定问题的算法，也有很多我们专栏中讲到的基础算法。所以，百度、Google这样的搜索引擎公司，在面试的时候，会格外重视考察候选人的算法能力。
今天我就借助搜索引擎，这样一个非常有技术含量的产品，来给你展示一下，数据结构和算法是如何应用在其中的。
整体系统介绍像Google这样的大型商用搜索引擎，有成千上万的工程师，十年如一日地对它进行优化改进，所以，它所包含的技术细节非常多。我很难、也没有这个能力，通过一篇文章把所有细节都讲清楚，当然这也不是我们专栏所专注的内容。
所以，接下来的讲解，我主要给你展示，如何在一台机器上（假设这台机器的内存是8GB， 硬盘是100多GB），通过少量的代码，实现一个小型搜索引擎。不过，麻雀虽小，五脏俱全。跟大型搜索引擎相比，实现这样一个小型搜索引擎所用到的理论基础是相通的。
搜索引擎大致可以分为四个部分：搜集、分析、索引、查询。其中，搜集，就是我们常说的利用爬虫爬取网页。分析，主要负责网页内容抽取、分词，构建临时索引，计算PageRank值这几部分工作。索引，主要负责通过分析阶段得到的临时索引，构建倒排索引。查询，主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户。
接下来，我就按照网页处理的生命周期，从这四个阶段，依次来给你讲解，一个网页从被爬取到最终展示给用户，这样一个完整的过程。与此同时，我会穿插讲解，这个过程中需要用到哪些数据结构和算法。
搜集现在，互联网越来越发达，网站越来越多，对应的网页也就越来越多。对于搜索引擎来说，它事先并不知道网页都在哪里。打个比方来说就是，我们只知道海里面有很多鱼，但却并不知道鱼在哪里。那搜索引擎是如何爬取网页的呢？
搜索引擎把整个互联网看作数据结构中的有向图，把每个页面看作一个顶点。如果某个页面中包含另外一个页面的链接，那我们就在两个顶点之间连一条有向边。我们可以利用图的遍历搜索算法，来遍历整个互联网中的网页。
我们前面介绍过两种图的遍历方法，深度优先和广度优先。搜索引擎采用的是广度优先搜索策略。具体点讲的话，那就是，我们先找一些比较知名的网页（专业的叫法是权重比较高）的链接（比如新浪主页网址、腾讯主页网址等），作为种子网页链接，放入到队列中。爬虫按照广度优先的策略，不停地从队列中取出链接，然后去爬取对应的网页，解析出网页里包含的其他网页链接，再将解析出来的链接添加到队列中。
基本的原理就是这么简单。但落实到实现层面，还有很多技术细节。我下面借助搜集阶段涉及的几个重要文件，来给你解释一下搜集工程都有哪些关键技术细节。
1.待爬取网页链接文件：links.bin在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。于是，队列中的链接就会越来越多，可能会多到内存放不下。所以，我们用一个存储在磁盘中的文件（links.bin）来作为广度优先搜索中的队列。爬虫从links.bin文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到links.bin文件中。
这样用文件来存储网页链接的方式，还有其他好处。比如，支持断点续爬。也就是说，当机器断电之后，网页链接不会丢失；当机器重启之后，还可以从之前爬取到的位置继续爬取。
关于如何解析页面获取链接，我额外多说几句。我们可以把整个页面看作一个大的字符串，然后利用字符串匹配算法，在这个大字符串中，搜索&amp;lt;link&amp;gt;这样一个网页标签，然后顺序读取&amp;lt;link&amp;gt;&amp;lt;/link&amp;gt;之间的字符串。这其实就是网页链接。
2.网页判重文件：bloom_filter.bin如何避免重复爬取相同的网页呢？这个问题我们在位图那一节已经讲过了。使用布隆过滤器，我们就可以快速并且非常节省内存地实现网页的判重。
不过，还是刚刚那个问题，如果我们把布隆过滤器存储在内存中，那机器宕机重启之后，布隆过滤器就被清空了。这样就可能导致大量已经爬取的网页会被重复爬取。
这个问题该怎么解决呢？我们可以定期地（比如每隔半小时）将布隆过滤器持久化到磁盘中，存储在bloom_filter.bin文件中。这样，即便出现机器宕机，也只会丢失布隆过滤器中的部分数据。当机器重启之后，我们就可以重新读取磁盘中的bloom_filter.bin文件，将其恢复到内存中。
3.原始网页存储文件：doc_raw.bin爬取到网页之后，我们需要将其存储下来，以备后面离线分析、索引之用。那如何存储海量的原始网页数据呢？
如果我们把每个网页都存储为一个独立的文件，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。常用的文件系统显然不适合存储如此多的文件。所以，我们可以把多个网页存储在一个文件中。每个网页之间，通过一定的标识进行分隔，方便后续读取。具体的存储格式，如下图所示。其中，doc_id这个字段是网页的编号，我们待会儿再解释。
当然，这样的一个文件也不能太大，因为文件系统对文件的大小也有一定的限制。所以，我们可以设置每个文件的大小不能超过一定的值（比如1GB）。随着越来越多的网页被添加到文件中，文件的大小就会越来越大，当超过1GB的时候，我们就创建一个新的文件，用来存储新爬取的网页。
假设一台机器的硬盘大小是100GB左右，一个网页的平均大小是64KB。那在一台机器上，我们可以存储100万到200万左右的网页。假设我们的机器的带宽是10MB，那下载100GB的网页，大约需要10000秒。也就是说，爬取100多万的网页，也就是只需要花费几小时的时间。
4.网页链接及其编号的对应文件：doc_id.bin刚刚我们提到了网页编号这个概念，我现在解释一下。网页编号实际上就是给每个网页分配一个唯一的ID，方便我们后续对网页进行分析、索引。那如何给网页编号呢？
我们可以按照网页被爬取的先后顺序，从小到大依次编号。具体是这样做的：我们维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，我们将网页链接跟编号之间的对应关系，存储在另一个doc_id.bin文件中。
爬虫在爬取网页的过程中，涉及的四个重要的文件，我就介绍完了。其中，links.bin和bloom_filter.bin这两个文件是爬虫自身所用的。另外的两个（doc_raw.bin、doc_id.bin）是作为搜集阶段的成果，供后面的分析、索引、查询用的。
分析网页爬取下来之后，我们需要对网页进行离线分析。分析阶段主要包括两个步骤，第一个是抽取网页文本信息，第二个是分词并创建临时索引。我们逐一来讲解。
1.抽取网页文本信息网页是半结构化数据，里面夹杂着各种标签、JavaScript代码、CSS样式。对于搜索引擎来说，它只关心网页中的文本信息，也就是，网页显示在浏览器中时，能被用户肉眼看到的那部分信息。我们如何从半结构化的网页中，抽取出搜索引擎关系的文本信息呢？
我们之所以把网页叫作半结构化数据，是因为它本身是按照一定的规则来书写的。这个规则就是HTML语法规范。我们依靠HTML标签来抽取网页中的文本信息。这个抽取的过程，大体可以分为两步。
第一步是去掉JavaScript代码、CSS格式以及下拉框中的内容（因为下拉框在用户不操作的情况下，也是看不到的）。也就是&amp;lt;style&amp;gt;&amp;lt;/style&amp;gt;，&amp;lt;script&amp;gt;&amp;lt;/script&amp;gt;，&amp;lt;option&amp;gt;&amp;lt;/option&amp;gt;这三组标签之间的内容。我们可以利用AC自动机这种多模式串匹配算法，在网页这个大字符串中，一次性查找&amp;lt;style&amp;gt;, &amp;lt;script&amp;gt;, &amp;lt;option&amp;gt;这三个关键词。当找到某个关键词出现的位置之后，我们只需要依次往后遍历，直到对应结束标签（&amp;lt;/style&amp;gt;, &amp;lt;/script&amp;gt;, &amp;lt;/option）为止。而这期间遍历到的字符串连带着标签就应该从网页中删除。
第二步是去掉所有HTML标签。这一步也是通过字符串匹配算法来实现的。过程跟第一步类似，我就不重复讲了。
2.分词并创建临时索引经过上面的处理之后，我们就从网页中抽取出了我们关心的文本信息。接下来，我们要对文本信息进行分词，并且创建临时索引。
对于英文网页来说，分词非常简单。我们只需要通过空格、标点符号等分隔符，将每个单词分割开来就可以了。但是，对于中文来说，分词就复杂太多了。我这里介绍一种比较简单的思路，基于字典和规则的分词方法。
其中，字典也叫词库，里面包含大量常用的词语（我们可以直接从网上下载别人整理好的）。我们借助词库并采用最长匹配规则，来对文本进行分词。所谓最长匹配，也就是匹配尽可能长的词语。我举个例子解释一下。
比如要分词的文本是“中国人民解放了”，我们词库中有“中国”“中国人”“中国人民”“中国人民解放军”这几个词，那我们就取最长匹配，也就是“中国人民”划为一个词，而不是把“中国”、“中国人”划为一个词。具体到实现层面，我们可以将词库中的单词，构建成Trie树结构，然后拿网页文本在Trie树中匹配。
每个网页的文本信息在分词完成之后，我们都得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件。临时索引文件的格式如下：
在临时索引文件中，我们存储的是单词编号，也就是图中的term_id，而非单词本身。这样做的目的主要是为了节省存储的空间。那这些单词的编号是怎么来的呢？
给单词编号的方式，跟给网页编号类似。我们维护一个计数器，每当从网页文本信息中分割出一个新的单词的时候，我们就从计数器中取一个编号，分配给它，然后计数器加一。
在这个过程中，我们还需要使用散列表，记录已经编过号的单词。在对网页文本信息分词的过程中，我们拿分割出来的单词，先到散列表中查找，如果找到，那就直接使用已有的编号；如果没有找到，我们再去计数器中拿号码，并且将这个新单词以及编号添加到散列表中。
当所有的网页处理（分词及写入临时索引）完成之后，我们再将这个单词跟编号之间的对应关系，写入到磁盘文件中，并命名为term_id.bin。
经过分析阶段，我们得到了两个重要的文件。它们分别是临时索引文件（tmp_index.bin）和单词编号文件（term_id.bin）。
索引索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引。倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表。文字描述比较难理解，我画了一张倒排索引的结构图，你一看就明白。
我们刚刚讲到，在临时索引文件中，记录的是单词跟每个包含它的文档之间的对应关系。那如何通过临时索引文件，构建出倒排索引文件呢？这是一个非常典型的算法问题，你可以先自己思考一下，再看我下面的讲解。
解决这个问题的方法有很多。考虑到临时索引文件很大，无法一次性加载到内存中，搜索引擎一般会选择使用多路归并排序的方法来实现。
我们先对临时索引文件，按照单词编号的大小进行排序。因为临时索引很大，所以一般基于内存的排序算法就没法处理这个问题了。我们可以用之前讲到的归并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起。当然，实际的软件开发中，我们其实可以直接利用MapReduce来处理。
临时索引文件排序完成之后，相同的单词就被排列到了一起。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，然后把它们存储在倒排索引文件中。具体的处理过程，我画成了一张图。通过图，你应该更容易理解。
除了倒排文件之外，我们还需要一个文件，来记录每个单词编号在倒排索引文件中的偏移位置。我们把这个文件命名为term_offset.bin。这个文件的作用是，帮助我们快速地查找某个单词编号在倒排索引中存储的位置，进而快速地从倒排索引中读取单词编号对应的网页编号列表。
经过索引阶段的处理，我们得到了两个有价值的文件，它们分别是倒排索引文件（index.bin）和记录单词编号在索引文件中的偏移位置的文件（term_offset.bin）。
查询前面三个阶段的处理，只是为了最后的查询做铺垫。因此，现在我们就要利用之前产生的几个文件，来实现最终的用户搜索功能。
doc_id.bin：记录网页链接和编号之间的对应关系。
term_id.bin：记录单词和编号之间的对应关系。
index.bin：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。
term_offsert.bin：记录每个单词编号在倒排索引文件中的偏移位置。
这四个文件中，除了倒排索引文件（index.bin）比较大之外，其他的都比较小。为了方便快速查找数据，我们将其他三个文件都加载到内存中，并且组织成散列表这种数据结构。
当用户在搜索框中，输入某个查询文本的时候，我们先对用户输入的文本进行分词处理。假设分词之后，我们得到k个单词。</description></item><item><title>53_设计大型DMP系统（下）：SSD拯救了所有的DBA</title><link>https://artisanbox.github.io/4/53/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/53/</guid><description>上一讲里，根据DMP系统的各个应用场景，我们从抽象的原理层面，选择了AeroSpike作为KV数据库，Kafka作为数据管道，Hadoop/Hive来作为数据仓库。
不过呢，肯定有不信邪的工程师会问，为什么MongoDB，甚至是MySQL这样的文档数据库或者传统的关系型数据库不适用呢？为什么不能通过优化SQL、添加缓存这样的调优手段，解决这个问题呢？
今天DMP的下半场，我们就从数据库实现的原理，一起来看一看，这背后的原因。如果你能弄明白今天的这些更深入、更细节的原理，对于什么场景使用什么数据库，就会更加胸有成竹，而不是只有跑了大量的性能测试才知道。下次做数据库选型的时候，你就可以“以理服人”了。
关系型数据库：不得不做的随机读写我们先来想一想，如果现在让你自己写一个最简单的关系型数据库，你的数据要怎么存放在硬盘上？
最简单最直观的想法是，用一个CSV文件格式。一个文件就是一个数据表。文件里面的每一行就是这个表里面的一条记录。如果要修改数据库里面的某一条记录，那么我们要先找到这一行，然后直接去修改这一行的数据。读取数据也是一样的。
要找到这样数据，最笨的办法自然是一行一行读，也就是遍历整个CSV文件。不过这样的话，相当于随便读取任何一条数据都要扫描全表，太浪费硬盘的吞吐量了。那怎么办呢？我们可以试试给这个CSV文件加一个索引。比如，给数据的行号加一个索引。如果你学过数据库原理或者算法和数据结构，那你应该知道，通过B+树多半是可以来建立这样一个索引的。
索引里面没有一整行的数据，只有一个映射关系，这个映射关系可以让行号直接从硬盘的某个位置去读。所以，索引比起数据小很多。我们可以把索引加载到内存里面。即使不在内存里面，要找数据的时候快速遍历一下整个索引，也不需要读太多的数据。
加了索引之后，我们要读取特定的数据，就不用去扫描整个数据表文件了。直接从特定的硬盘位置，就可以读到想要的行。索引不仅可以索引行号，还可以索引某个字段。我们可以创建很多个不同的独立的索引。写SQL的时候，where子句后面的查询条件可以用到这些索引。
不过，这样的话，写入数据的时候就会麻烦一些。我们不仅要在数据表里面写入数据，对于所有的索引也都需要进行更新。这个时候，写入一条数据就要触发好几个随机写入的更新。
在这样一个数据模型下，查询操作很灵活。无论是根据哪个字段查询，只要有索引，我们就可以通过一次随机读，很快地读到对应的数据。但是，这个灵活性也带来了一个很大的问题，那就是无论干点什么，都有大量的随机读写请求。而随机读写请求，如果请求最终是要落到硬盘上，特别是HDD硬盘的话，我们就很难做到高并发了。毕竟HDD硬盘只有100左右的QPS。
而这个随时添加索引，可以根据任意字段进行查询，这样表现出的灵活性，又是我们的DMP系统里面不太需要的。DMP的KV数据库主要的应用场景，是根据主键的随机查询，不需要根据其他字段进行筛选查询。数据管道的需求，则只需要不断追加写入和顺序读取就好了。即使进行数据分析的数据仓库，通常也不是根据字段进行数据筛选，而是全量扫描数据进行分析汇总。
后面的两个场景还好说，大不了我们让程序去扫描全表或者追加写入。但是，在KV数据库这个需求上，刚才这个最简单的关系型数据库的设计，就会面临大量的随机写入和随机读取的挑战。
所以，在实际的大型系统中，大家都会使用专门的分布式KV数据库，来满足这个需求。那么下面，我们就一起来看一看，Facebook开源的Cassandra的数据存储和读写是怎么做的，这些设计是怎么解决高并发的随机读写问题的。
Cassandra：顺序写和随机读Cassandra的数据模型作为一个分布式的KV数据库，Cassandra的键一般被称为Row Key。其实就是一个16到36个字节的字符串。每一个Row Key对应的值其实是一个哈希表，里面可以用键值对，再存入很多你需要的数据。
Cassandra本身不像关系型数据库那样，有严格的Schema，在数据库创建的一开始就定义好了有哪些列（Column）。但是，它设计了一个叫作列族（Column Family）的概念，我们需要把经常放在一起使用的字段，放在同一个列族里面。比如，DMP里面的人口属性信息，我们可以把它当成是一个列族。用户的兴趣信息，可以是另外一个列族。这样，既保持了不需要严格的Schema这样的灵活性，也保留了可以把常常一起使用的数据存放在一起的空间局部性。
往Cassandra的里面读写数据，其实特别简单，就好像是在一个巨大的分布式的哈希表里面写数据。我们指定一个Row Key，然后插入或者更新这个Row Key的数据就好了。
Cassandra的写操作Cassandra只有顺序写入，没有随机写入Cassandra解决随机写入数据的解决方案，简单来说，就叫作“不随机写，只顺序写”。对于Cassandra数据库的写操作，通常包含两个动作。第一个是往磁盘上写入一条提交日志（Commit Log）。另一个操作，则是直接在内存的数据结构上去更新数据。后面这个往内存的数据结构里面的数据更新，只有在提交日志写成功之后才会进行。每台机器上，都有一个可靠的硬盘可以让我们去写入提交日志。写入提交日志都是顺序写（Sequential Write），而不是随机写（Random Write），这使得我们最大化了写入的吞吐量。
如果你不明白这是为什么，可以回到第47讲，看看硬盘的性能评测。无论是HDD硬盘还是SSD硬盘，顺序写入都比随机写入要快得多。
内存的空间比较有限，一旦内存里面的数据量或者条目超过一定的限额，Cassandra就会把内存里面的数据结构dump到硬盘上。这个Dump的操作，也是顺序写而不是随机写，所以性能也不会是一个问题。除了Dump的数据结构文件，Cassandra还会根据row key来生成一个索引文件，方便后续基于索引来进行快速查询。
随着硬盘上的Dump出来的文件越来越多，Cassandra会在后台进行文件的对比合并。在很多别的KV数据库系统里面，也有类似这种的合并动作，比如AeroSpike或者Google的BigTable。这些操作我们一般称之为Compaction。合并动作同样是顺序读取多个文件，在内存里面合并完成，再Dump出来一个新的文件。整个操作过程中，在硬盘层面仍然是顺序读写。
Cassandra的读操作Cassandra的读请求，会通过缓存、BloomFilter进行两道过滤，尽可能避免数据请求命中硬盘当我们要从Cassandra读数据的时候，会从内存里面找数据，再从硬盘读数据，然后把两部分的数据合并成最终结果。这些硬盘上的文件，在内存里面会有对应的Cache，只有在Cache里面找不到，我们才会去请求硬盘里面的数据。
如果不得不访问硬盘，因为硬盘里面可能Dump了很多个不同时间点的内存数据的快照。所以，找数据的时候，我们也是按照时间从新的往旧的里面找。
这也就带来另外一个问题，我们可能要查询很多个Dump文件，才能找到我们想要的数据。所以，Cassandra在这一点上又做了一个优化。那就是，它会为每一个Dump的文件里面所有Row Key生成一个BloomFilter，然后把这个BloomFilter放在内存里面。这样，如果想要查询的Row Key在数据文件里面不存在，那么99%以上的情况下，它会被BloomFilter过滤掉，而不需要访问硬盘。
这样，只有当数据在内存里面没有，并且在硬盘的某个特定文件上的时候，才会触发一次对于硬盘的读请求。
SSD：DBA们的大救星Cassandra是Facebook在2008年开源的。那个时候，SSD硬盘还没有那么普及。可以看到，它的读写设计充分考虑了硬件本身的特性。在写入数据进行持久化上，Cassandra没有任何的随机写请求，无论是Commit Log还是Dump，全部都是顺序写。
在数据读的请求上，最新写入的数据都会更新到内存。如果要读取这些数据，会优先从内存读到。这相当于是一个使用了LRU的缓存机制。只有在万般无奈的情况下，才会有对于硬盘的随机读请求。即使在这样的情况下，Cassandra也在文件之前加了一层BloomFilter，把本来因为Dump文件带来的需要多次读硬盘的问题，简化成多次内存读和一次硬盘读。
这些设计，使得Cassandra即使是在HDD硬盘上，也能有不错的访问性能。因为所有的写入都是顺序写或者写入到内存，所以，写入可以做到高并发。HDD硬盘的吞吐率还是很不错的，每秒可以写入100MB以上的数据，如果一条数据只有1KB，那么10万的WPS（Writes per seconds）也是能够做到的。这足够支撑我们DMP期望的写入压力了。
而对于数据的读，就有一些挑战了。如果数据读请求有很强的局部性，那我们的内存就能搞定DMP需要的访问量。
但是，问题就出在这个局部性上。DMP的数据访问分布，其实是缺少局部性的。你仔细想一想DMP的应用场景就明白了。DMP里面的Row Key都是用户的唯一标识符。普通用户的上网时长怎么会有局部性呢？每个人上网的时间和访问网页的次数就那么多。上网多的人，一天最多也就24小时。大部分用户一天也要上网2～3小时。我们没办法说，把这些用户的数据放在内存里面，那些用户不放。
DMP系统，只有根据国家和时区不同有比较明显的局部性，是局部性不强的系统那么，我们可不可能有一定的时间局部性呢？如果是Facebook那样的全球社交网络，那可能还有一定的时间局部性。毕竟不同国家的人的时区不一样。我们可以说，在印度人民的白天，把印度人民的数据加载到内存里面，美国人民的数据就放在硬盘上。到了印度人民的晚上，再把美国人民的数据换到内存里面来。
如果你的主要业务是在国内，那这个时间局部性就没有了。大家的上网高峰时段，都是在早上上班路上、中午休息的时候以及晚上下班之后的时间，没有什么区分度。
面临这个情况，如果你们的CEO或者CTO问你，是不是可以通过优化程序来解决这个问题？如果你没有仔细从数据分布和原理的层面思考这个问题，而直接一口答应下来，那你可能之后要头疼了，因为这个问题很有可能是搞不定的。
因为缺少了时间局部性，我们内存的缓存能够起到的作用就很小了，大部分请求最终还是要落到HDD硬盘的随机读上。但是，HDD硬盘的随机读的性能太差了，我们在第45讲看过，也就是100QPS左右。而如果全都放内存，那就太贵了，成本在HDD硬盘100倍以上。
不过，幸运的是，从2010年开始，SSD硬盘的大规模商用帮助我们解决了这个问题。它的价格在HDD硬盘的10倍，但是随机读的访问能力在HDD硬盘的百倍以上。也就是说，用上了SSD硬盘，我们可以用1/10的成本获得和内存同样的QPS。同样的价格的SSD硬盘，容量则是内存的几十倍，也能够满足我们的需求，用较低的成本存下整个互联网用户信息。
不夸张地说，过去十年的“大数据”“高并发”“千人千面”，有一半的功劳应该归在让SSD容量不断上升、价格不断下降的硬盘产业上。
回到我们看到的Cassandra的读写设计，你会发现，Cassandra的写入机制完美匹配了我们在第46和47讲所说的SSD硬盘的优缺点。
在数据写入层面，Cassandra的数据写入都是Commit Log的顺序写入，也就是不断地在硬盘上往后追加内容，而不是去修改现有的文件内容。一旦内存里面的数据超过一定的阈值，Cassandra又会完整地Dump一个新文件到文件系统上。这同样是一个追加写入。
数据的对比和紧凑化（Compaction），同样是读取现有的多个文件，然后写一个新的文件出来。写入操作只追加不修改的特性，正好天然地符合SSD硬盘只能按块进行擦除写入的操作。在这样的写入模式下，Cassandra用到的SSD硬盘，不需要频繁地进行后台的Compaction，能够最大化SSD硬盘的使用寿命。这也是为什么，Cassandra在SSD硬盘普及之后，能够获得进一步快速发展。
总结延伸好了，关于DMP和存储器的内容，讲到这里就差不多了。希望今天的这一讲，能够让你从Cassandra的数据库实现的细节层面，彻底理解怎么运用好存储器的性能特性和原理。
传统的关系型数据库，我们把一条条数据存放在一个地方，同时再把索引存放在另外一个地方。这样的存储方式，其实很方便我们进行单次的随机读和随机写，数据的存储也可以很紧凑。但是问题也在于此，大部分的SQL请求，都会带来大量的随机读写的请求。这使得传统的关系型数据库，其实并不适合用在真的高并发的场景下。
我们的DMP需要的访问场景，其实没有复杂的索引需求，但是会有比较高的并发性。我带你一看了Facebook开源的Cassandra这个分布式KV数据库的读写设计。通过在追加写入Commit Log和更新内存，Cassandra避开了随机写的问题。内存数据的Dump和后台的对比合并，同样也都避开了随机写的问题，使得Cassandra的并发写入性能极高。
在数据读取层面，通过内存缓存和BloomFilter，Cassandra已经尽可能地减少了需要随机读取硬盘里面数据的情况。不过挑战在于，DMP系统的局部性不强，使得我们最终的随机读的请求还是要到硬盘上。幸运的是，SSD硬盘在数据海量增长的那几年里价格不断下降，使得我们最终通过SSD硬盘解决了这个问题。
而SSD硬盘本身的擦除后才能写入的机制，正好非常适合Cassandra的数据读写模式，最终使得Cassandra在SSD硬盘普及之后得到了更大的发展。
推荐阅读今天的推荐阅读，是一篇相关的论文。我推荐你去读一读Cassandra - A Decentralized Structured Storage System。读完这篇论文，一方面你会对分布式KV数据库的设计原则有所了解，了解怎么去做好数据分片、故障转移、数据复制这些机制；另一方面，你可以看到基于内存和硬盘的不同存储设备的特性，Cassandra是怎么有针对性地设计数据读写和持久化的方式的。
课后思考除了MySQL这样的关系型数据库，还有Cassandra这样的分布式KV数据库。实际上，在海量数据分析的过程中，还有一种常见的数据库，叫作列式存储的OLAP的数据库，比如Clickhouse。你可以研究一下，Clickhouse这样的数据库里面的数据是怎么存储在硬盘上的。
欢迎把你研究的结果写在留言区，和大家一起分享、交流。如果觉得有帮助，你也可以把这篇文章分享给你的朋友，和他一起讨论、学习。</description></item><item><title>54_理解Disruptor（上）：带你体会CPU高速缓存的风驰电掣</title><link>https://artisanbox.github.io/4/54/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/54/</guid><description>坚持到底就是胜利，终于我们一起来到了专栏的最后一个主题。让我一起带你来看一看，CPU到底能有多快。在接下来的两讲里，我会带你一起来看一个开源项目Disruptor。看看我们怎么利用CPU和高速缓存的硬件特性，来设计一个对于性能有极限追求的系统。
不知道你还记不记得，在第37讲里，为了优化4毫秒专门铺设光纤的故事。实际上，最在意极限性能的并不是互联网公司，而是高频交易公司。我们今天讲解的Disruptor就是由一家专门做高频交易的公司LMAX开源出来的。
有意思的是，Disruptor的开发语言，并不是很多人心目中最容易做到性能极限的C/C++，而是性能受限于JVM的Java。这到底是怎么一回事呢？那通过这一讲，你就能体会到，其实只要通晓硬件层面的原理，即使是像Java这样的高级语言，也能够把CPU的性能发挥到极限。
Padding Cache Line，体验高速缓存的威力我们先来看看Disruptor里面一段神奇的代码。这段代码里，Disruptor在RingBufferPad这个类里面定义了p1，p2一直到p7 这样7个long类型的变量。
abstract class RingBufferPad { protected long p1, p2, p3, p4, p5, p6, p7; } 我在看到这段代码的第一反应是，变量名取得不规范，p1-p7这样的变量名没有明确的意义啊。不过，当我深入了解了Disruptor的设计和源代码，才发现这些变量名取得恰如其分。因为这些变量就是没有实际意义，只是帮助我们进行缓存行填充（Padding Cache Line），使得我们能够尽可能地用上CPU高速缓存（CPU Cache）。那么缓存行填充这个黑科技到底是什么样的呢？我们接着往下看。
不知道你还记不记得，我们在35讲里面的这个表格。如果访问内置在CPU里的L1 Cache或者L2 Cache，访问延时是内存的1/15乃至1/100。而内存的访问速度，其实是远远慢于CPU的。想要追求极限性能，需要我们尽可能地多从CPU Cache里面拿数据，而不是从内存里面拿数据。
CPU Cache装载内存里面的数据，不是一个一个字段加载的，而是加载一整个缓存行。举个例子，如果我们定义了一个长度为64的long类型的数组。那么数据从内存加载到CPU Cache里面的时候，不是一个一个数组元素加载的，而是一次性加载固定长度的一个缓存行。
我们现在的64位Intel CPU的计算机，缓存行通常是64个字节（Bytes）。一个long类型的数据需要8个字节，所以我们一下子会加载8个long类型的数据。也就是说，一次加载数组里面连续的8个数值。这样的加载方式使得我们遍历数组元素的时候会很快。因为后面连续7次的数据访问都会命中缓存，不需要重新从内存里面去读取数据。这个性能层面的好处，我在第37讲的第一个例子里面为你演示过，印象不深的话，可以返回去看看。
但是，在我们不使用数组，而是使用单独的变量的时候，这里就会出现问题了。在Disruptor的RingBuffer（环形缓冲区）的代码里面，定义了一个RingBufferFields类，里面有indexMask和其他几个变量，用来存放RingBuffer的内部状态信息。
CPU在加载数据的时候，自然也会把这个数据从内存加载到高速缓存里面来。不过，这个时候，高速缓存里面除了这个数据，还会加载这个数据前后定义的其他变量。这个时候，问题就来了。Disruptor是一个多线程的服务器框架，在这个数据前后定义的其他变量，可能会被多个不同的线程去更新数据、读取数据。这些写入以及读取的请求，会来自于不同的 CPU Core。于是，为了保证数据的同步更新，我们不得不把CPU Cache里面的数据，重新写回到内存里面去或者重新从内存里面加载数据。
而我们刚刚说过，这些CPU Cache的写回和加载，都不是以一个变量作为单位的。这些动作都是以整个Cache Line作为单位的。所以，当INITIAL_CURSOR_VALUE 前后的那些变量被写回到内存的时候，这个字段自己也写回到了内存，这个常量的缓存也就失效了。当我们要再次读取这个值的时候，要再重新从内存读取。这也就意味着，读取速度大大变慢了。
...... abstract class RingBufferPad { protected long p1, p2, p3, p4, p5, p6, p7; }
abstract class RingBufferFields&amp;lt;E&amp;gt; extends RingBufferPad { &amp;hellip;&amp;hellip; private final long indexMask; private final Object[] entries; protected final int bufferSize; protected final Sequencer sequencer; &amp;hellip;&amp;hellip; }</description></item><item><title>54_算法实战（三）：剖析高性能队列Disruptor背后的数据结构和算法</title><link>https://artisanbox.github.io/2/55/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/55/</guid><description>Disruptor你是否听说过呢？它是一种内存消息队列。从功能上讲，它其实有点儿类似Kafka。不过，和Kafka不同的是，Disruptor是线程之间用于消息传递的队列。它在Apache Storm、Camel、Log4j 2等很多知名项目中都有广泛应用。
之所以如此受青睐，主要还是因为它的性能表现非常优秀。它比Java中另外一个非常常用的内存消息队列ArrayBlockingQueue（ABS）的性能，要高一个数量级，可以算得上是最快的内存消息队列了。它还因此获得过Oracle官方的Duke大奖。
如此高性能的内存消息队列，在设计和实现上，必然有它独到的地方。今天，我们就来一块儿看下，Disruptor是如何做到如此高性能的？其底层依赖了哪些数据结构和算法？
基于循环队列的“生产者-消费者模型”什么是内存消息队列？对很多业务工程师或者前端工程师来说，可能会比较陌生。不过，如果我说“生产者-消费者模型”，估计大部分人都知道。在这个模型中，“生产者”生产数据，并且将数据放到一个中心存储容器中。之后，“消费者”从中心存储容器中，取出数据消费。
这个模型非常简单、好理解，那你有没有思考过，这里面存储数据的中心存储容器，是用什么样的数据结构来实现的呢？
实际上，实现中心存储容器最常用的一种数据结构，就是我们在第9节讲的队列。队列支持数据的先进先出。正是这个特性，使得数据被消费的顺序性可以得到保证，也就是说，早被生产的数据就会早被消费。
我们在第9节讲过，队列有两种实现思路。一种是基于链表实现的链式队列，另一种是基于数组实现的顺序队列。不同的需求背景下，我们会选择不同的实现方式。
如果我们要实现一个无界队列，也就是说，队列的大小事先不确定，理论上可以支持无限大。这种情况下，我们适合选用链表来实现队列。因为链表支持快速地动态扩容。如果我们要实现一个有界队列，也就是说，队列的大小事先确定，当队列中数据满了之后，生产者就需要等待。直到消费者消费了数据，队列有空闲位置的时候，生产者才能将数据放入。
实际上，相较于无界队列，有界队列的应用场景更加广泛。毕竟，我们的机器内存是有限的。而无界队列占用的内存数量是不可控的。对于实际的软件开发来说，这种不可控的因素，就会有潜在的风险。在某些极端情况下，无界队列就有可能因为内存持续增长，而导致OOM（Out of Memory）错误。
在第9节中，我们还讲过一种特殊的顺序队列，循环队列。我们讲过，非循环的顺序队列在添加、删除数据的工程中，会涉及数据的搬移操作，导致性能变差。而循环队列正好可以解决这个数据搬移的问题，所以，性能更加好。所以，大部分用到顺序队列的场景中，我们都选择用顺序队列中的循环队列。
实际上，循环队列这种数据结构，就是我们今天要讲的内存消息队列的雏形。我借助循环队列，实现了一个最简单的“生产者-消费者模型”。对应的代码我贴到这里，你可以看看。
为了方便你理解，对于生产者和消费者之间操作的同步，我并没有用到线程相关的操作。而是采用了“当队列满了之后，生产者就轮训等待；当队列空了之后，消费者就轮训等待”这样的措施。
public class Queue { private Long[] data; private int size = 0, head = 0, tail = 0; public Queue(int size) { this.data = new Long[size]; this.size = size; } public boolean add(Long element) { if ((tail + 1) % size == head) return false; data[tail] = element; tail = (tail + 1) % size; return true; }</description></item><item><title>55_理解Disruptor（下）：不需要换挡和踩刹车的CPU，有多快？</title><link>https://artisanbox.github.io/4/55/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/55/</guid><description>上一讲，我们学习了一个精妙的想法，Disruptor通过缓存行填充，来利用好CPU的高速缓存。不知道你做完课后思考题之后，有没有体会到高速缓存在实践中带来的速度提升呢？
不过，利用CPU高速缓存，只是Disruptor“快”的一个因素，那今天我们就来看一看Disruptor快的另一个因素，也就是“无锁”，而尽可能发挥CPU本身的高速处理性能。
缓慢的锁Disruptor作为一个高性能的生产者-消费者队列系统，一个核心的设计就是通过RingBuffer实现一个无锁队列。
上一讲里我们讲过，Java里面的基础库里，就有像LinkedBlockingQueue这样的队列库。但是，这个队列库比起Disruptor里用的RingBuffer要慢上很多。慢的第一个原因我们说过，因为链表的数据在内存里面的布局对于高速缓存并不友好，而RingBuffer所使用的数组则不然。
LinkedBlockingQueue慢，有另外一个重要的因素，那就是它对于锁的依赖。在生产者-消费者模式里，我们可能有多个消费者，同样也可能有多个生产者。多个生产者都要往队列的尾指针里面添加新的任务，就会产生多个线程的竞争。于是，在做这个事情的时候，生产者就需要拿到对于队列尾部的锁。同样地，在多个消费者去消费队列头的时候，也就产生竞争。同样消费者也要拿到锁。
那只有一个生产者，或者一个消费者，我们是不是就没有这个锁竞争的问题了呢？很遗憾，答案还是否定的。一般来说，在生产者-消费者模式下，消费者要比生产者快。不然的话，队列会产生积压，队列里面的任务会越堆越多。
一方面，你会发现越来越多的任务没有能够及时完成；另一方面，我们的内存也会放不下。虽然生产者-消费者模型下，我们都有一个队列来作为缓冲区，但是大部分情况下，这个缓冲区里面是空的。也就是说，即使只有一个生产者和一个消费者者，这个生产者指向的队列尾和消费者指向的队列头是同一个节点。于是，这两个生产者和消费者之间一样会产生锁竞争。
在LinkedBlockingQueue上，这个锁机制是通过ReentrantLock这个Java 基础库来实现的。这个锁是一个用Java在JVM上直接实现的加锁机制，这个锁机制需要由JVM来进行裁决。这个锁的争夺，会把没有拿到锁的线程挂起等待，也就需要经过一次上下文切换（Context Switch）。
不知道你还记不记得，我们在第28讲讲过的异常和中断，这里的上下文切换要做的和异常和中断里的是一样的。上下文切换的过程，需要把当前执行线程的寄存器等等的信息，保存到线程栈里面。而这个过程也必然意味着，已经加载到高速缓存里面的指令或者数据，又回到了主内存里面，会进一步拖慢我们的性能。
我们可以按照Disruptor介绍资料里提到的Benchmark，写一段代码来看看，是不是真是这样的。这里我放了一段Java代码，代码的逻辑很简单，就是把一个long类型的counter，从0自增到5亿。一种方式是没有任何锁，另外一个方式是每次自增的时候都要去取一个锁。
你可以在自己的电脑上试试跑一下这个程序。在我这里，两个方式执行所需要的时间分别是207毫秒和9603毫秒，性能差出了将近50倍。
package com.xuwenhao.perf.jmm; import java.util.concurrent.atomic.AtomicLong; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock;
public class LockBenchmark{
public static void runIncrement() { long counter = 0; long max = 500000000L; long start = System.currentTimeMillis(); while (counter &amp;amp;lt; max) { counter++; } long end = System.currentTimeMillis(); System.out.println(&amp;amp;quot;Time spent is &amp;amp;quot; + (end-start) + &amp;amp;quot;ms without lock&amp;amp;quot;); } public static void runIncrementWithLock() { Lock lock = new ReentrantLock(); long counter = 0; long max = 500000000L; long start = System.</description></item><item><title>55_算法实战（四）：剖析微服务接口鉴权限流背后的数据结构和算法</title><link>https://artisanbox.github.io/2/56/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/56/</guid><description>微服务是最近几年才兴起的概念。简单点讲，就是把复杂的大应用，解耦拆分成几个小的应用。这样做的好处有很多。比如，这样有利于团队组织架构的拆分，毕竟团队越大协作的难度越大；再比如，每个应用都可以独立运维，独立扩容，独立上线，各个应用之间互不影响。不用像原来那样，一个小功能上线，整个大应用都要重新发布。
不过，有利就有弊。大应用拆分成微服务之后，服务之间的调用关系变得更复杂，平台的整体复杂熵升高，出错的概率、debug问题的难度都高了好几个数量级。所以，为了解决这些问题，服务治理便成了微服务的一个技术重点。
所谓服务治理，简单点讲，就是管理微服务，保证平台整体正常、平稳地运行。服务治理涉及的内容比较多，比如鉴权、限流、降级、熔断、监控告警等等。这些服务治理功能的实现，底层依赖大量的数据结构和算法。今天，我就拿其中的鉴权和限流这两个功能，来带你看看，它们的实现过程中都要用到哪些数据结构和算法。
鉴权背景介绍以防你之前可能对微服务没有太多了解，所以我对鉴权的背景做了简化。
假设我们有一个微服务叫用户服务（User Service）。它提供很多用户相关的接口，比如获取用户信息、注册、登录等，给公司内部的其他应用使用。但是，并不是公司内部所有应用，都可以访问这个用户服务，也并不是每个有访问权限的应用，都可以访问用户服务的所有接口。
我举了一个例子给你讲解一下，你可以看我画的这幅图。这里面，只有A、B、C、D四个应用可以访问用户服务，并且，每个应用只能访问用户服务的部分接口。
要实现接口鉴权功能，我们需要事先将应用对接口的访问权限规则设置好。当某个应用访问其中一个接口的时候，我们就可以拿应用的请求URL，在规则中进行匹配。如果匹配成功，就说明允许访问；如果没有可以匹配的规则，那就说明这个应用没有这个接口的访问权限，我们就拒绝服务。
如何实现快速鉴权？接口的格式有很多，有类似Dubbo这样的RPC接口，也有类似Spring Cloud这样的HTTP接口。不同接口的鉴权实现方式是类似的，我这里主要拿HTTP接口给你讲解。
鉴权的原理比较简单、好理解。那具体到实现层面，我们该用什么数据结构来存储规则呢？用户请求URL在规则中快速匹配，又该用什么样的算法呢？
实际上，不同的规则和匹配模式，对应的数据结构和匹配算法也是不一样的。所以，关于这个问题，我继续细化为三个更加详细的需求给你讲解。
1.如何实现精确匹配规则？我们先来看最简单的一种匹配模式。只有当请求URL跟规则中配置的某个接口精确匹配时，这个请求才会被接受、处理。为了方便你理解，我举了一个例子，你可以看一下。
不同的应用对应不同的规则集合。我们可以采用散列表来存储这种对应关系。我这里着重讲下，每个应用对应的规则集合，该如何存储和匹配。
针对这种匹配模式，我们可以将每个应用对应的权限规则，存储在一个字符串数组中。当用户请求到来时，我们拿用户的请求URL，在这个字符串数组中逐一匹配，匹配的算法就是我们之前学过的字符串匹配算法（比如KMP、BM、BF等）。
规则不会经常变动，所以，为了加快匹配速度，我们可以按照字符串的大小给规则排序，把它组织成有序数组这种数据结构。当要查找某个URL能否匹配其中某条规则的时候，我们可以采用二分查找算法，在有序数组中进行匹配。
而二分查找算法的时间复杂度是O(logn)（n表示规则的个数），这比起时间复杂度是O(n)的顺序遍历快了很多。对于规则中接口长度比较长，并且鉴权功能调用量非常大的情况，这种优化方法带来的性能提升还是非常可观的 。
2.如何实现前缀匹配规则？我们再来看一种稍微复杂的匹配模式。只要某条规则可以匹配请求URL的前缀，我们就说这条规则能够跟这个请求URL匹配。同样，为了方便你理解这种匹配模式，我还是举一个例子说明一下。
不同的应用对应不同的规则集合。我们采用散列表来存储这种对应关系。我着重讲一下，每个应用的规则集合，最适合用什么样的数据结构来存储。
在Trie树那节，我们讲到，Trie树非常适合用来做前缀匹配。所以，针对这个需求，我们可以将每个用户的规则集合，组织成Trie树这种数据结构。
不过，Trie树中的每个节点不是存储单个字符，而是存储接口被“/”分割之后的子目录（比如“/user/name”被分割为“user”“name”两个子目录）。因为规则并不会经常变动，所以，在Trie树中，我们可以把每个节点的子节点们，组织成有序数组这种数据结构。在匹配的过程中，我们可以利用二分查找算法，决定从一个节点应该跳到哪一个子节点。
3.如何实现模糊匹配规则？如果我们的规则更加复杂，规则中包含通配符，比如“**”表示匹配任意多个子目录，“*”表示匹配任意一个子目录。只要用户请求URL可以跟某条规则模糊匹配，我们就说这条规则适用于这个请求。为了方便你理解，我举一个例子来解释一下。
不同的应用对应不同的规则集合。我们还是采用散列表来存储这种对应关系。这点我们刚才讲过了，这里不再重复说了。我们着重看下，每个用户对应的规则集合，该用什么数据结构来存储？针对这种包含通配符的模糊匹配，我们又该使用什么算法来实现呢？
还记得我们在回溯算法那节讲的正则表达式的例子吗？我们可以借助正则表达式那个例子的解决思路，来解决这个问题。我们采用回溯算法，拿请求URL跟每条规则逐一进行模糊匹配。如何用回溯算法进行模糊匹配，这部分我就不重复讲了。你如果忘记了，可以回到相应章节复习一下。
不过，这个解决思路的时间复杂度是非常高的。我们需要拿每一个规则，跟请求URL匹配一遍。那有没有办法可以继续优化一下呢？
实际上，我们可以结合实际情况，挖掘出这样一个隐形的条件，那就是，并不是每条规则都包含通配符，包含通配符的只是少数。于是，我们可以把不包含通配符的规则和包含通配符的规则分开处理。
我们把不包含通配符的规则，组织成有序数组或者Trie树（具体组织成什么结构，视具体的需求而定，是精确匹配，就组织成有序数组，是前缀匹配，就组织成Trie树），而这一部分匹配就会非常高效。剩下的是少数包含通配符的规则，我们只要把它们简单存储在一个数组中就可以了。尽管匹配起来会比较慢，但是毕竟这种规则比较少，所以这种方法也是可以接受的。
当接收到一个请求URL之后，我们可以先在不包含通配符的有序数组或者Trie树中查找。如果能够匹配，就不需要继续在通配符规则中匹配了；如果不能匹配，就继续在通配符规则中查找匹配。
限流背景介绍讲完了鉴权的实现思路，我们再来看一下限流。
所谓限流，顾名思义，就是对接口调用的频率进行限制。比如每秒钟不能超过100次调用，超过之后，我们就拒绝服务。限流的原理听起来非常简单，但它在很多场景中，发挥着重要的作用。比如在秒杀、大促、双11、618等场景中，限流已经成为了保证系统平稳运行的一种标配的技术解决方案。
按照不同的限流粒度，限流可以分为很多种类型。比如给每个接口限制不同的访问频率，或者给所有接口限制总的访问频率，又或者更细粒度地限制某个应用对某个接口的访问频率等等。
不同粒度的限流功能的实现思路都差不多，所以，我今天主要针对限制所有接口总的访问频率这样一个限流需求来讲解。其他粒度限流需求的实现思路，你可以自己思考。
如何实现精准限流？最简单的限流算法叫固定时间窗口限流算法。这种算法是如何工作的呢？首先我们需要选定一个时间起点，之后每当有接口请求到来，我们就将计数器加一。如果在当前时间窗口内，根据限流规则（比如每秒钟最大允许100次访问请求），出现累加访问次数超过限流值的情况时，我们就拒绝后续的访问请求。当进入下一个时间窗口之后，计数器就清零重新计数。
这种基于固定时间窗口的限流算法的缺点是，限流策略过于粗略，无法应对两个时间窗口临界时间内的突发流量。这是怎么回事呢？我举一个例子给你解释一下。
假设我们的限流规则是，每秒钟不能超过100次接口请求。第一个1s时间窗口内，100次接口请求都集中在最后10ms内。在第二个1s的时间窗口内，100次接口请求都集中在最开始的10ms内。虽然两个时间窗口内流量都符合限流要求（≤100个请求），但在两个时间窗口临界的20ms内，会集中有200次接口请求。固定时间窗口限流算法并不能对这种情况做限制，所以，集中在这20ms内的200次请求就有可能压垮系统。
为了解决这个问题，我们可以对固定时间窗口限流算法稍加改造。我们可以限制任意时间窗口（比如1s）内，接口请求数都不能超过某个阈值（ 比如100次）。因此，相对于固定时间窗口限流算法，这个算法叫滑动时间窗口限流算法。
流量经过滑动时间窗口限流算法整形之后，可以保证任意一个1s的时间窗口内，都不会超过最大允许的限流值，从流量曲线上来看会更加平滑。那具体到实现层面，我们该如何来做呢？
我们假设限流的规则是，在任意1s内，接口的请求次数都不能大于K次。我们就维护一个大小为K+1的循环队列，用来记录1s内到来的请求。注意，这里循环队列的大小等于限流次数加一，因为循环队列存储数据时会浪费一个存储单元。
当有新的请求到来时，我们将与这个新请求的时间间隔超过1s的请求，从队列中删除。然后，我们再来看循环队列中是否有空闲位置。如果有，则把新请求存储在队列尾部（tail指针所指的位置）；如果没有，则说明这1秒内的请求次数已经超过了限流值K，所以这个请求被拒绝服务。
为了方便你理解，我举一个例子，给你解释一下。在这个例子中，我们假设限流的规则是，任意1s内，接口的请求次数都不能大于6次。
即便滑动时间窗口限流算法可以保证任意时间窗口内，接口请求次数都不会超过最大限流值，但是仍然不能防止，在细时间粒度上访问过于集中的问题。
比如我刚刚举的那个例子，第一个1s的时间窗口内，100次请求都集中在最后10ms中，也就是说，基于时间窗口的限流算法，不管是固定时间窗口还是滑动时间窗口，只能在选定的时间粒度上限流，对选定时间粒度内的更加细粒度的访问频率不做限制。
实际上，针对这个问题，还有很多更加平滑的限流算法，比如令牌桶算法、漏桶算法等。如果感兴趣，你可以自己去研究一下。
总结引申今天，我们讲解了跟微服务相关的接口鉴权和限流功能的实现思路。现在，我稍微总结一下。
关于鉴权，我们讲了三种不同的规则匹配模式。不管是哪种匹配模式，我们都可以用散列表来存储不同应用对应的不同规则集合。对于每个应用的规则集合的存储，三种匹配模式使用不同的数据结构。
对于第一种精确匹配模式，我们利用有序数组来存储每个应用的规则集合，并且通过二分查找和字符串匹配算法，来匹配请求URL与规则。对于第二种前缀匹配模式，我们利用Trie树来存储每个应用的规则集合。对于第三种模糊匹配模式，我们采用普通的数组来存储包含通配符的规则，通过回溯算法，来进行请求URL与规则的匹配。
关于限流，我们讲了两种限流算法，第一种是固定时间窗口限流算法，第二种是滑动时间窗口限流算法。对于滑动时间窗口限流算法，我们用了之前学习过的循环队列来实现。比起固定时间窗口限流算法，它对流量的整形效果更好，流量更加平滑。
从今天的学习中，我们也可以看出，对于基础架构工程师来说，如果不精通数据结构和算法，我们就很难开发出性能卓越的基础架构、中间件。这其实就体现了数据结构和算法的重要性。
课后思考 除了用循环队列来实现滑动时间窗口限流算法之外，我们是否还可以用其他数据结构来实现呢？请对比一下这些数据结构跟循环队列在解决这个问题时的优劣之处。
分析一下鉴权那部分内容中，前缀匹配算法的时间复杂度和空间复杂度。
最后，有个消息提前通知你一下。本节是专栏的倒数第二节课了，不知道学到现在，你掌握得怎么样呢？为了帮你复习巩固，做到真正掌握这些知识，我针对专栏涉及的数据结构和算法，精心编制了一套练习题。从正月初一到初七，每天发布一篇。你要做好准备哦！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>56_算法实战（五）：如何用学过的数据结构和算法实现一个短网址系统？</title><link>https://artisanbox.github.io/2/57/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/57/</guid><description>短网址服务你用过吗？如果我们在微博里发布一条带网址的信息，微博会把里面的网址转化成一个更短的网址。我们只要访问这个短网址，就相当于访问原始的网址。比如下面这两个网址，尽管长度不同，但是都可以跳转到我的一个GitHub开源项目里。其中，第二个网址就是通过新浪提供的短网址服务生成的。
原始网址：https://github.com/wangzheng0822/ratelimiter4j 短网址：http://t.cn/EtR9QEG 从功能上讲，短网址服务其实非常简单，就是把一个长的网址转化成一个短的网址。作为一名软件工程师，你是否思考过，这样一个简单的功能，是如何实现的呢？底层都依赖了哪些数据结构和算法呢？
短网址服务整体介绍刚刚我们讲了，短网址服务的一个核心功能，就是把原始的长网址转化成短网址。除了这个功能之外，短网址服务还有另外一个必不可少的功能。那就是，当用户点击短网址的时候，短网址服务会将浏览器重定向为原始网址。这个过程是如何实现的呢？
为了方便你理解，我画了一张对比图，你可以看下。
从图中我们可以看出，浏览器会先访问短网址服务，通过短网址获取到原始网址，再通过原始网址访问到页面。不过这部分功能并不是我们今天要讲的重点。我们重点来看，如何将长网址转化成短网址？
如何通过哈希算法生成短网址？我们前面学过哈希算法。哈希算法可以将一个不管多长的字符串，转化成一个长度固定的哈希值。我们可以利用哈希算法，来生成短网址。
前面我们已经提过一些哈希算法了，比如MD5、SHA等。但是，实际上，我们并不需要这些复杂的哈希算法。在生成短网址这个问题上，毕竟，我们不需要考虑反向解密的难度，所以我们只需要关心哈希算法的计算速度和冲突概率。
能够满足这样要求的哈希算法有很多，其中比较著名并且应用广泛的一个哈希算法，那就是MurmurHash算法。尽管这个哈希算法在2008年才被发明出来，但现在它已经广泛应用到Redis、MemCache、Cassandra、HBase、Lucene等众多著名的软件中。
MurmurHash算法提供了两种长度的哈希值，一种是32bits，一种是128bits。为了让最终生成的短网址尽可能短，我们可以选择32bits的哈希值。对于开头那个GitHub网址，经过MurmurHash计算后，得到的哈希值就是181338494。我们再拼上短网址服务的域名，就变成了最终的短网址http://t.cn/181338494（其中，http://t.cn 是短网址服务的域名）。
1.如何让短网址更短？不过，你可能已经看出来了，通过MurmurHash算法得到的短网址还是很长啊，而且跟我们开头那个网址的格式好像也不一样。别着急，我们只需要稍微改变一个哈希值的表示方法，就可以轻松把短网址变得更短些。
我们可以将10进制的哈希值，转化成更高进制的哈希值，这样哈希值就变短了。我们知道，16进制中，我们用A～F，来表示10～15。在网址URL中，常用的合法字符有0～9、a～z、A～Z这样62个字符。为了让哈希值表示起来尽可能短，我们可以将10进制的哈希值转化成62进制。具体的计算过程，我写在这里了。最终用62进制表示的短网址就是http://t.cn/cgSqq。
2.如何解决哈希冲突问题？不过，我们前面讲过，哈希算法无法避免的一个问题，就是哈希冲突。尽管MurmurHash算法，冲突的概率非常低。但是，一旦冲突，就会导致两个原始网址被转化成同一个短网址。当用户访问短网址的时候，我们就无从判断，用户想要访问的是哪一个原始网址了。这个问题该如何解决呢？
一般情况下，我们会保存短网址跟原始网址之间的对应关系，以便后续用户在访问短网址的时候，可以根据对应关系，查找到原始网址。存储这种对应关系的方式有很多，比如我们自己设计存储系统或者利用现成的数据库。前面我们讲到的数据库有MySQL、Redis。我们就拿MySQL来举例。假设短网址与原始网址之间的对应关系，就存储在MySQL数据库中。
当有一个新的原始网址需要生成短网址的时候，我们先利用MurmurHash算法，生成短网址。然后，我们拿这个新生成的短网址，在MySQL数据库中查找。
如果没有找到相同的短网址，这也就表明，这个新生成的短网址没有冲突。于是我们就将这个短网址返回给用户（请求生成短网址的用户），然后将这个短网址与原始网址之间的对应关系，存储到MySQL数据库中。
如果我们在数据库中，找到了相同的短网址，那也并不一定说明就冲突了。我们从数据库中，将这个短网址对应的原始网址也取出来。如果数据库中的原始网址，跟我们现在正在处理的原始网址是一样的，这就说明已经有人请求过这个原始网址的短网址了。我们就可以拿这个短网址直接用。如果数据库中记录的原始网址，跟我们正在处理的原始网址不一样，那就说明哈希算法发生了冲突。不同的原始网址，经过计算，得到的短网址重复了。这个时候，我们该怎么办呢？
我们可以给原始网址拼接一串特殊字符，比如“[DUPLICATED]”，然后再重新计算哈希值，两次哈希计算都冲突的概率，显然是非常低的。假设出现非常极端的情况，又发生冲突了，我们可以再换一个拼接字符串，比如“[OHMYGOD]”，再计算哈希值。然后把计算得到的哈希值，跟原始网址拼接了特殊字符串之后的文本，一并存储在MySQL数据库中。
当用户访问短网址的时候，短网址服务先通过短网址，在数据库中查找到对应的原始网址。如果原始网址有拼接特殊字符（这个很容易通过字符串匹配算法找到），我们就先将特殊字符去掉，然后再将不包含特殊字符的原始网址返回给浏览器。
3.如何优化哈希算法生成短网址的性能？为了判断生成的短网址是否冲突，我们需要拿生成的短网址，在数据库中查找。如果数据库中存储的数据非常多，那查找起来就会非常慢，势必影响短网址服务的性能。那有没有什么优化的手段呢？
还记得我们之前讲的MySQL数据库索引吗？我们可以给短网址字段添加B+树索引。这样通过短网址查询原始网址的速度就提高了很多。实际上，在真实的软件开发中，我们还可以通过一个小技巧，来进一步提高速度。
在短网址生成的过程中，我们会跟数据库打两次交道，也就是会执行两条SQL语句。第一个SQL语句是通过短网址查询短网址与原始网址的对应关系，第二个SQL语句是将新生成的短网址和原始网址之间的对应关系存储到数据库。
我们知道，一般情况下，数据库和应用服务（只做计算不存储数据的业务逻辑部分）会部署在两个独立的服务器或者虚拟服务器上。那两条SQL语句的执行就需要两次网络通信。这种IO通信耗时以及SQL语句的执行，才是整个短网址服务的性能瓶颈所在。所以，为了提高性能，我们需要尽量减少SQL语句。那又该如何减少SQL语句呢？
我们可以给数据库中的短网址字段，添加一个唯一索引（不只是索引，还要求表中不能有重复的数据）。当有新的原始网址需要生成短网址的时候，我们并不会先拿生成的短网址，在数据库中查找判重，而是直接将生成的短网址与对应的原始网址，尝试存储到数据库中。如果数据库能够将数据正常写入，那说明并没有违反唯一索引，也就是说，这个新生成的短网址并没有冲突。
当然，如果数据库反馈违反唯一性索引异常，那我们还得重新执行刚刚讲过的“查询、写入”过程，SQL语句执行的次数不减反增。但是，在大部分情况下，我们把新生成的短网址和对应的原始网址，插入到数据库的时候，并不会出现冲突。所以，大部分情况下，我们只需要执行一条写入的SQL语句就可以了。所以，从整体上看，总的SQL语句执行次数会大大减少。
实际上，我们还有另外一个优化SQL语句次数的方法，那就是借助布隆过滤器。
我们把已经生成的短网址，构建成布隆过滤器。我们知道，布隆过滤器是比较节省内存的一种存储结构，长度是10亿的布隆过滤器，也只需要125MB左右的内存空间。
当有新的短网址生成的时候，我们先拿这个新生成的短网址，在布隆过滤器中查找。如果查找的结果是不存在，那就说明这个新生成的短网址并没有冲突。这个时候，我们只需要再执行写入短网址和对应原始网页的SQL语句就可以了。通过先查询布隆过滤器，总的SQL语句的执行次数减少了。
到此，利用哈希算法来生成短网址的思路，我就讲完了。实际上，这种解决思路已经完全满足需求了，我们已经可以直接用到真实的软件开发中。不过，我们还有另外一种短网址的生成算法，那就是利用自增的ID生成器来生成短网址。我们接下来就看一下，这种算法是如何工作的？对于哈希算法生成短网址来说，它又有什么优势和劣势？
如何通过ID生成器生成短网址？我们可以维护一个ID自增生成器。它可以生成1、2、3…这样自增的整数ID。当短网址服务接收到一个原始网址转化成短网址的请求之后，它先从ID生成器中取一个号码，然后将其转化成62进制表示法，拼接到短网址服务的域名（比如http://t.cn/）后面，就形成了最终的短网址。最后，我们还是会把生成的短网址和对应的原始网址存储到数据库中。
理论非常简单好理解。不过，这里有几个细节问题需要处理。
1.相同的原始网址可能会对应不同的短网址每次新来一个原始网址，我们就生成一个新的短网址，这种做法就会导致两个相同的原始网址生成了不同的短网址。这个该如何处理呢？实际上，我们有两种处理思路。
第一种处理思路是不做处理。听起来有点无厘头，我稍微解释下你就明白了。实际上，相同的原始网址对应不同的短网址，这个用户是可以接受的。在大部分短网址的应用场景里，用户只关心短网址能否正确地跳转到原始网址。至于短网址长什么样子，他其实根本就不关心。所以，即便是同一个原始网址，两次生成的短网址不一样，也并不会影响到用户的使用。
第二种处理思路是借助哈希算法生成短网址的处理思想，当要给一个原始网址生成短网址的时候，我们要先拿原始网址在数据库中查找，看数据库中是否已经存在相同的原始网址了。如果数据库中存在，那我们就取出对应的短网址，直接返回给用户。
不过，这种处理思路有个问题，我们需要给数据库中的短网址和原始网址这两个字段，都添加索引。短网址上加索引是为了提高用户查询短网址对应的原始网页的速度，原始网址上加索引是为了加快刚刚讲的通过原始网址查询短网址的速度。这种解决思路虽然能满足“相同原始网址对应相同短网址”这样一个需求，但是是有代价的：一方面两个索引会占用更多的存储空间，另一方面索引还会导致插入、删除等操作性能的下降。
2.如何实现高性能的ID生成器？实现ID生成器的方法有很多，比如利用数据库自增字段。当然我们也可以自己维护一个计数器，不停地加一加一。但是，一个计数器来应对频繁的短网址生成请求，显然是有点吃力的（因为计数器必须保证生成的ID不重复，笼统概念上讲，就是需要加锁）。如何提高ID生成器的性能呢？关于这个问题，实际上，有很多解决思路。我这里给出两种思路。
第一种思路是借助第54节中讲的方法。我们可以给ID生成器装多个前置发号器。我们批量地给每个前置发号器发送ID号码。当我们接受到短网址生成请求的时候，就选择一个前置发号器来取号码。这样通过多个前置发号器，明显提高了并发发号的能力。
第二种思路跟第一种差不多。不过，我们不再使用一个ID生成器和多个前置发号器这样的架构，而是，直接实现多个ID生成器同时服务。为了保证每个ID生成器生成的ID不重复。我们要求每个ID生成器按照一定的规则，来生成ID号码。比如，第一个ID生成器只能生成尾号为0的，第二个只能生成尾号为1的，以此类推。这样通过多个ID生成器同时工作，也提高了ID生成的效率。
总结引申今天，我们讲了短网址服务的两种实现方法。我现在来稍微总结一下。
第一种实现思路是通过哈希算法生成短网址。我们采用计算速度快、冲突概率小的MurmurHash算法，并将计算得到的10进制数，转化成62进制表示法，进一步缩短短网址的长度。对于哈希算法的哈希冲突问题，我们通过给原始网址添加特殊前缀字符，重新计算哈希值的方法来解决。
第二种实现思路是通过ID生成器来生成短网址。我们维护一个ID自增的ID生成器，给每个原始网址分配一个ID号码，并且同样转成62进制表示法，拼接到短网址服务的域名之后，形成最终的短网址。
课后思考 如果我们还要额外支持用户自定义短网址功能（http//t.cn/{用户自定部分}），我们又该如何改造刚刚的算法呢?
我们在讲通过ID生成器生成短网址这种实现思路的时候，讲到相同的原始网址可能会对应不同的短网址。针对这个问题，其中一个解决思路就是，不做处理。但是，如果每个请求都生成一个短网址，并且存储在数据库中，那这样会不会撑爆数据库呢？我们又该如何解决呢？
今天是农历的大年三十，我们专栏的正文到这里也就全部结束了。从明天开始，我会每天发布一篇练习题，内容针对专栏涉及的数据结构和算法。从初一到初七，帮你复习巩固所学知识，拿下数据结构和算法，打响新年进步的第一枪！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>FAQ第一期_学与不学，知识就在那里，不如就先学好了</title><link>https://artisanbox.github.io/4/57/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/57/</guid><description>你好，我是徐文浩。专栏上线三个多月，我们已经进入后半段。
首先，恭喜跟到这里的同学，很快你就可以看到胜利的曙光了。如果你已经掉队了，不要紧，现在继续依然来得及。
其次，非常感谢同学们的积极留言，看到这么多人因为我的文章受到启发、产生思考，我也感到非常开心。因此，我特意把留言区中非常棒的、值得反复阅读和思考的内容，摘录出来，供你反复阅读学习。
有些内容你可能已经非常熟悉了，但是随着工作、学习经验的不同，相信你的理解也会不一样；有些内容可能刚好也是你的疑问，但是你还没发现，这里说不定就帮你解决了。
今天第一期，我们先来聊聊，“学习”这件事。我准备了五个问题，话不多说，一起来看看吧！
Q1：“要不要学”和“学不会怎么办”系列专栏已经更新三个多月了，但是我估计很多人还是停留在前面3篇的学习上（我相信，一定有的）。
我观察了一下，其实很多人并不是真的学不会，而是“不敢学”，往往还没开始就被自己给吓到了。很多优秀的人，并非真的智商有多么高，而是他们敢于尝试，敢于突破自己的舒适区。
所以说，学习底层知识或者新知识的第一点，就是要“克服恐惧”，其实大部分东西上手了都不难，都很有意思。就像《冰与火之歌》里面，水舞者教导艾莉亚时的情况一样，“恐惧比利剑”更伤人。破除对于基础知识“难”的迷信，是迈向更高水平必经的一步。
“组成原理可以算是理解计算机运作机制的第一门入门课，这门课的交付目标就是让科班的同学们能够温故而知新，为非科班的同学们打开深入学习计算机核心课程的大门。”这是我在专栏刚上线的时候给一个同学的留言回复，现在拿过来再给你说一遍。
另外，大家在学校里学这些课程的时候，都会遇到一个问题，那就是理论和我们的编程应用实践离得比较远。在这个专栏里，我的目标是让大家能够更“实践”地去学习计算机组成原理。
所以，这门课我的目标就是尽量讲得“理论和实践相结合”，能和你的日常代码工作结合起来。让非科班的同学们也能学习到计算机组成原理的知识，所以在深入讲解知识点之外，我会尽量和你在开发过程中可能遇到的问题放到一块儿，只要跟着课程的节奏走，不会跟不上哦。（跟到这里的同学可以在留言区冒个泡，给跟不上的同学招个手，让他们放心大胆看过来。）
我自己在大学的时候也不是个“好学生”。现在回头看，我自己常常觉得大学的时候没有好好读书，浪费了很多时间。常常想，当时要是做了就好了。当时，不是就是现在么？学或者不学，知识就在那里，不如就先学好了啊。
Q2：“计算机组成原理”和“操作系统”到底有啥不一样？其实操作系统也是一个“软件”，而开发操作系统，只需要关注到“组成原理”或者“体系结构”就好了，不需要真的了解硬件。操作系统，其实是在“组成原理”所讲的“指令集”上做一层封装。
体系结构、操作系统、编译原理以及计算机网络，都可以认为是组成原理的后继课程。体系结构不是一个系统软件，它更多地是讲，如何量化地设计和研究体系结构和指令集。操作系统、编译原理和计算机网络都是基于体系结构之上的系统软件。
其实这几门基础学科，都是环环相扣，相互渗透的，每一门课都不可能独立存在。不知道你现在是否明白这几门基础学科的价值呢？
Q3：“图灵机”和“冯·诺依曼机”的区别首先，先回答一下这道题本身。有些同学已经回答的不错。我把他们的答案贴在这里。你可以看看跟你想的是不是一样。
Amanda 同学：
两者有交叉但是不同，根据了解整理如下：
图灵机是一种思想模型（计算机的基本理论基础），是一种有穷的、构造性的问题的求解思路，图灵认为凡是能用算法解决的问题也一定能用图灵机解决；
冯·诺依曼提出了“存储程序”的计算机设计思想，并“参照”图灵模型设计了历史上第一台电子计算机，即冯·诺依曼机。
图灵机其实是一个很有意思的话题。我上大学的时候，对应着图灵机也有一门课程，叫作“可计算性理论”，其实就是告诉我们什么样的问题是计算机解决得了的，什么样的问题是它解决不了的。
在我看来，图灵机就是一个抽象的“思维实验”，而冯·诺依曼机就是对应着这个“思维实验”的“物理实现”。如果我们把“图灵机”当成“灵魂”，代表计算机最抽象的本质，那么“冯诺伊曼机”就是“肉体”，代表了计算机最具体的本质。这两者之间颇有理论物理学家和实验物理学家的合作关系的意思，可谓是一个问题的两面。
冯·诺依曼体系结构距今已经几十年了，目前，我们还没有看到真正颠覆性的新的体系结构出现，更多地是针对硬件变化和应用场景变化的优化。但是过去几年随着深度学习、IoT等的发展，体系结构又有了一波新的大发展，也许未来会有新的变化呢，我们可以拭目以待。
Q4：工作多年，如何保持对知识清晰、准确的认识？我之前跟很多人聊过，发现工作很多年之后的工程师，在计算机科学的基础知识上，反而比不上很多应届的同学。我总结下来，大概有这么几个因素。
首先，很多工程师只是满足于工作的需求被满足了，没有真的深入去搞清楚一个问题的原理。从网络上搜索一段代码，复制粘贴到自己的程序里，只要能跑就认为问题解决了，并没有深入一行行看明白每行代码到底是做了什么，为什么要这么做。
比如说，我们现在要提升RPC和序列化的性能，很多人的做法是，找一个教程用一下Thrift这样的开源框架，解决眼下的问题就完事儿。至于，Thrift是怎么序列化的，每一种里面支持的RPC协议是怎么回事儿，完全不清楚。其实这些开源代码并不复杂，稍微花点时间，搞清楚里面的实现细节和原理，你对二进制存储、程序性能、网络性能，就会有一个更深刻的认识，之后遇到类似的问题你就不会再一问三不知，久而久之你的能力就会得到提升。
其次，读书的时候我们认为一个东西掌握扎实了，有时候其实未必。很多人估计都有感受，像计算机这类实践性比较强的专业，书上所学和真正实践中所用完全是两码事。背出计算机的五大组成部分，似乎和我们的实际应用没有联系，但是在实际的系统开发过程中，无论是内存地址转换使用的页表树这样的数据结构，还是各个系统组件间通过总线进行通信的模式，其实都可以和我们自己的应用系统开发里的模式和思路联系起来。
至于究竟该怎么去掌握知识，其实没有什么特别好的方法。我就说说我一般会怎么做，一方面，遇到疑难问题、复杂的系统时，必须要用更底层更本质的理解计算机运作的方式，去处理问题，自然会去回头把这些基础知识捡起来；另一方面，时不时抽点时间回头看看一些“大部头”的教科书，对我自己而言，本身就很有自我满足感，而这种自我满足感也会促使我不断去读它们，从而形成一个良性循环。
Q5：六个最实用的、督促自己学习的办法看到很多同学在留言里分享了自己学习方法，我看了也非常受益，我把这些方法筛选总结了一下，又结合我自己的学习经验，放在这里分享给你。
好奇心是一个优秀程序员必然要有的特质。多去想想“为什么是这样的”，有助于你更深入地掌握这些知识点。 先了解知识面，再寻找自己有兴趣的点深入，学习也是个反复迭代的过程。 带着问题去学习是最快的成长方式之一。彻底搞清楚实际在开发过程中遇到的困难的问题，而不是只满足于功能问题被实现和解决，是提升自己的必经之路。 “教别人”是一种非常高效的学习方式，自己有没有弄清楚，在教别人的过程中，会体会得明明白白。 每个月给自己投资100-200块在专业学习上面，这样花了钱，通过外部约束，也是一个让自己坚持下去的好办法。 坚持到底就是胜利✌️。把学习和成长变成一种习惯，这个习惯带来的惯性会让你更快地成长。 好了，到这里，我们第一期答疑就要结束了。这次我主要和你谈了谈“学习”这个话题，不知道你有什么感受呢？你还想听我和你聊什么专栏之外的话题呢？
欢迎积极留言给我。如果觉得这篇文章对你有帮助，也欢迎你收藏并分享给你的朋友。对了，看到这里的同学，记得在留言区给后面的同学招个手啊：）
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>FAQ第二期_世界上第一个编程语言是怎么来的？</title><link>https://artisanbox.github.io/4/56/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/56/</guid><description>你好，我是徐文浩，今天是第二期FAQ，我搜集了第3讲到第6讲，大家在留言区问的比较多的问题，来做一次集中解答。
有些问题，可能你已经知道了答案，不妨看看和我的理解是否一样；如果这些问题刚好你也有，那可要认真看啦！
希望今天的你，也同样有收获！
Q1：为什么user + sys运行出来会比real time多呢？我们知道，实际的计算机运行的过程中，CPU会在多个不同的进程里面切换，分配不同的时间片去执行任务。所以，运行一个程序，在现实中走过的时间，并不是实际CPU运行这个程序所花费的时间。前者在现实中走过的时间，我们叫作real time。有时候叫作wall clock time，也就是墙上挂着的钟走过的时间。
而实际CPU上所花费的时间，又可以分成在操作系统的系统调用里面花的sys time和用户态的程序所花的user time。如果我们只有一个CPU的话，那real time &amp;gt;= sys time + user time 。所以，我当时在文章里给大家看了对应的示例。
不过，有不少同学运行出来的结果不是这样的。这是因为现在大家都已经用上多核的CPU了。也就是同一时间，有两个CPU可以同时运行任务。
你在一台多核或者多CPU的机器上运行，seq和wc命令会分配到两个CPU上。虽然seq和wc这两个命令都是单线程运行的，但是这两个命令在多核CPU运行的情况下，会分别分配到两个不同的CPU。
于是，user和sys的时间是两个CPU上运行的时间之和，这就可能超过real的时间。而real只是现实时钟里走过的时间，极端情况下user+sys可以到达real的两倍。
你可以运行下面这个命令，快速验证。让这个命令多跑一会儿，并且在后台运行。
time seq 100000000 | wc -l &amp;amp; 然后，我们利用top命令，查看不同进程的CPU占用情况。你会在top的前几行里看到，seq和wc的CPU占用都接近100，实际上，它们各被分配到了一个不同的CPU执行。
我写这篇文章的时候，测试时只开了一个1u的最小的虚拟机，只有一个CPU，所以不会遇到这个问题。
Q2：时钟周期时间和指令执行耗时有直接关系吗？这个问题提的得非常好，@易儿易 同学的学习和思考都很仔细、深入。
“晶振时间与CPU执行固定指令耗时成正比”，这个说法更准确一点。我们为了理解，可以暂且认为，是晶振在触发一条一条电路变化指令。这就好比你拨算盘的节奏一样。算盘拨得快，珠算就算得快。结果就是，一条简单的指令需要的时间就和一个时钟周期一样。
当然，实际上，这个问题要比这样一句话复杂很多。你可以仔细去读一读专栏关于CPU的章节呢。
从最简单的单指令周期CPU来说，其实时钟周期应该是放下最复杂的一条指令的时间长度。但是，我们现在实际用的都没有单指令周期CPU了，而是采用了流水线技术。采用了流水线技术之后，单个时钟周期里面，能够执行的就不是一个指令了。我们会把一条机器指令，拆分成很多个小步骤。不同的指令的步骤数量可能还不一样。不同的步骤的执行时间，也不一样。所以，一个时钟周期里面，能够放下的是最耗时间的某一个指令步骤。
这样的话，单看一条指令，其实一定需要很多个时钟周期。也就是说，从响应时间的角度来看，一个时钟周期一定是不够执行一条指令的。但是呢，因为有流水线，我们同时又会去执行很多个指令的不同步骤。再加上后面讲的像超线程技术等等，从吞吐量的角度来看，我们又能够做到，平均一个时钟周期里面，完成指令数可以超过1。
想要准确理解CPU的性能问题，请你一定去仔细读一读专栏的整个CPU的部分啊。
Q3：为什么低压主频只有标压的2/3？计算向量点积的时候，怎么提高性能？低压和低主频都是为了减少能耗。比如Surface Go的电池很小，机器的尺寸也很小。如果用上高主频，性能更好了，但是耗电并没有下来。
另外，低电压对于CPU的工艺有更高的要求，因为太低的电压可能导致电路都不能导通，要高主频一样对工艺有更高的要求。所以一般低压CPU都是通过和低主频配合，用在对于移动性和续航要求比较高的机器上。
向量计算是可以通过让加法也并行来优化的，不过真实的CPU里面其实是通过SIMD指令来优化向量计算的，我在后面也会讲到SIMD指令。
Q4：世界上第一个编程语言是怎么来的？如果你去计算机历史博物馆看一下真机，就会明白，第一台通用计算机ENIAC，它的各种输入都是一些旋钮，可以认为是类似用机器码在编程，后来才有了汇编语言、C语言这样越来越高级的语言。
编程语言是自举的，指的是说，我们能用自己写出来的程序编译自己。但是自举，并不要求这门语言的第一个编译器就是用自己写的。
比如，这里说到的Go，先是有了Go语言，我们通过C++写了编译器A。然后呢，我们就可以用这个编译器A，来编译Go语言的程序。接着，我们再用Go语言写一个编译器程序B，然后用A去编译B，就得到了Go语言写好的编译器的可执行文件了。
这个之后，我们就可以一直用B来编译未来的Go语言程序，这也就实现了所谓的自举了。所以，即使是自举，也通常是先有了别的语言写好的编译器，然后再用自己来写自己语言的编译器。
更详细的关于鸡蛋问题，可以直接看Wikipedia上这个链接，里面讲了多种这个问题的解决方案。
Q5：不同指令集中，汇编语言和机器码的关系怎么对应的？不同指令集里，对应的汇编代码会对应这个指令集的机器码呀。大家不要把“汇编语言”当成是像C一样的一门统一编程语言。
“汇编语言”其实可以理解成“机器码”的一种别名或者书写方式，不同的指令集和体系结构的机器会有不同的“机器码”。
高级语言在转换成为机器码的时候，是通过编译器进行的，需要编译器指定编译成哪种汇编/机器码。
物理机自己执行的时候只有机器码，并不认识汇编代码。
编译器如果支持编译成不同的体系结构的汇编/机器码，就要维护很多不同的对应关系表，但是这个表并不会太大。以最复杂的Intel X86的指令集为例，也只有2000条不同的指令而已。
Q6：某篇文章大段大段读不懂怎么办？@胖胖胖 同学说得很好。在专栏最开始几篇，或者到后面比较深入的文章，很多非科班的或者基础不太好的同学，会觉得读不下去，甚至很多地方看不懂。这些其实都是正常现象。
即便我在写的时候，已经尽可能考虑得比较完善，照顾大家的情况，但是肯定无法面面俱到。在我平时学习过程遇到拦路虎的时候，我一般有两种方法，这里跟你分享一下。
第一种，硬读。
你可能说了，这也叫方法吗？没错，事实就是这样。如果这个知识点，我必须要攻克，就想要搞明白，那我就会尽我所能，去看每一个字眼，把每个不理解的地方，都一点一点搞明白。不吝啬花费时间和精力。
当然这种情况适合我对这个内容完全不了解，或者已经基本了解，现在需要进一步提升的情况下。因为，在完全不了解一个知识的时候，这个壁垒是很高的。如果不想办法突破的话，那可能就没办法了解这个新的领域。而在已经基本了解某个领域或者某块知识的情况下，我去攻克一些更高难度的知识，很多时候也需要同样的方法，我会建立在兴趣的基础上去硬读，但是之后会非常非常有成就感。
第二种，先抓主要矛盾，再抓细节问题。
很多时候，大家在对一个知识不了解的时候，会感觉很“恐慌”。其实完全没必要，大家学任何东西都是从不会到会这么一个过程。就像@胖胖胖 同学说的那样，先找出这篇文章的主干，先对这些东西有个大致的概念。如果有需要，在之后的过程中，你还会碰到，你可以再重读，加深印象。
有时候，学习知识可以尝试“短期多次”。也就是说，看完一遍之后，如果不明白，先放下，过一段时间再看一遍，如果还不明白，再过一段时间再看。这样循环几次，在大脑中发酵几次，说不定就明白了，要给大脑一个缓冲的时间。
好了，今天的答疑到这里就结束了。不知道能否帮你解决了一些疑惑和问题呢？
我会持续不断地回复留言，并把比较好的问题精选出来，作为答疑。欢迎你继续在留言区留言，和大家一起交流学习。</description></item><item><title>“他山之石”｜Sugar：这门课你可以试试这么学</title><link>https://artisanbox.github.io/3/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/1/</guid><description>你好，我是这门课程的助教 Sugar，曾供职于百度，现就职于某大型互联网公司，是一名软件工程师。和你一样，我也是一名编译技术的爱好者。
我们的课程更新到今天，已经过半了，不知道你学习得怎么样呢？有没有卡在哪个知识点的实现上？不要担心，如果你有任何的问题，除了在留言区留言外，还可以添加我们的微信交流群，直接 @ 我或者是宫老师，或者是其他志同道合的同学们，我们都会来帮你解决。
回归正题，今天，刚好是国庆假期，我们第一部分的起步篇也更新完了。我们就先停下来休息休息，夯实基础。在这篇分享里，我想跟你聊聊我对编译技术的看法。我也会从我个人的角度，给你总结一下已更新完的起步篇都讲了什么，以及在日常工作中，我们又可以从哪些方向把这门课学到的知识落到实践中来。最开始，我们聊聊为什么要学习编译技术这个话题。
我为什么推荐你学习编译技术？我大概是在2014年入行互联网行业的，一晃就是7年。这7年间，很多公司的口号和观点都从“从PC业务向移动端转型”，变成了“国内移动端用户饱和，期待其他新兴领域带来业务增量”。科技行业发展之快，令人欣喜也叫人唏嘘。
不得不说，作为一名计算机专业科班出身的同学，我很惭愧。我其实是在走上工作岗位后，才对作为本科必修课的编译原理有了真正的了解，也真正感受到了这个技术领域独特的魅力。所以现在，我也推荐你关注这项技术。
为什么我会推荐你学习编译技术呢？如果你一定要问我理由，我想给你分享两点。
第一点，网红热点的“花开花落”，远不如底层技术的“静水流深”有力量。
在我还是一名“产品工程师”（Product Engineer）的时候，我从事过客户端、前端、服务端，以及一些面向业务应用的算法方面的工作。
近些年，每一个技术领域都涌现过各种新概念、新趋势，这可能会让你陷入“学习了这门新技术，就能分享这种技术形态的成长红利，提升自我价值”的错觉。我也曾追赶过这些“技术时尚”：做后端时一会儿学学这个语言，一会儿瞧瞧那个中间件；做前端时，一会儿用用这个框架，一会儿看看那个三方库。
但冷静下来，你就会发现，这些“网红型”技术热点，能够沉淀下来、为业务长期赋能的寥寥无几。真正值得长期学习和不断实践的，反倒是计算机专业的那些基础课，包括但不限于数据结构与算法、计算机网络、操作系统、编译原理、计算机体系结构，还有软件工程等等。这些课程你在学校里学习的时候，可能感觉枯燥乏味、过于抽象，甚至毫无成就感，然而在工作以后你才会发现，这可能是一名 coder 应对“行业内卷”最坚实有力的后盾。
第二点，技术上卡的“脖子”也正意味着更多的机会和可能。
其实，近两年来，我们国内IT行业很多龙头企业都遭遇了“技术卡脖子”的情况。一时间，芯片、操作系统以及其他基础软件国产化的呼声，都起来了。但是，在我看来，这些“low-level-stuff”需要的不止是时间与金钱的投入，更需要技术人才的“梯队化”。
国外的许多大公司里，你都能见到50多岁，甚至年龄更大的资深程序员。他们往往不是从事一些面向用户的业务逻辑开发，而是做一些被称为 Infrastructure 的基础架构工作。我们国内这样的趋势还不够明朗，毕竟我们起步太晚，做这些基础技术的工作，又需要公司雄厚且稳定的营收去支撑。这个差距是显而易见的，甚至短期内都没法快速弥补上。
我曾经就遇到过一个Google V8引擎方面的技术难题。我当时是在一个C++的程序中集成了 V8，通过 V8 的 ObjectTemplate 和 HandleScope 等优化尽可能快速生成 jsObject，并传递到 V8-Isolate 内部的 jsContext 里。但我用尽了所有方法，依然达不到原生 JavaScript 中字面量 Object 的性能。我跟身边许多架构师同事进行了探讨，也通过e-mail和V8项目的一些参与者进行了交流。但是我发现，相比起国外的技术社区，国内工程师们能给出的一些建议确实非常有限。
当然，我不认为这是技术水平、或者是智力这些因素造成的，事实上，国内有大量非常聪明，而且比国外更加勤劳（内卷所致）的软件工程师。在我看来，造成这个现象的原因，主要是 V8 项目的历史实在太久远了，而且早期的核心开发人员又有许多来自于历史更久远的 Java 虚拟机。整个过程具有很强的“技术继承性”，国内的工程师很难有机会，真正深入地了解这些系统的技术内幕。
或许，这就是我们在很多核心技术领域被“卡脖子”现象的主要原因之一。不过，困境也就意味者突破和机会。在底层软硬件国产化的浪潮下，编译器和操作系统是两座绕不过、躲不开的大山，国产芯片也会创造出大量让编译技术大放异彩的机会。在可预见的未来5～10年内，国内的这个技术趋势都是存在的。所以，我看好编译技术、操作系统等这些技术领域的发展，也推荐你深入学习这些底层技术。
起步篇讲了什么？前面聊完了“为什么”，现在我们就来解析一下“是什么”，聊聊我们这门课已更新完的起步篇里，都讲了什么。
不过，在这里我希望你能理解一点，当前这门课程是宫老师讲编译技术的第三季课程了，所以不可避免地存在着一定的知识继承性和延续性。对于编译器的前端部分，这次的课程中讲得相对没有那么深入。如果你想深入了解前端的知识，我建议你去看看第一季《编译原理之美》和第二季《编译原理实战课》。第三季的重点是放在了编译器后端部分，和物理机、操作系统打交道。
如果你看到我梳理的概念中，有很多是你无法理解的，你也可以带着疑问，试着把整个流程串起来。有了一个整体的“大局观”之后，再回头去第一季和第二季中找寻答案，当然也可以在我们的微信交流群里提问。
编译器是一个工业级的基础软件，因此从理论体系上我们就将编译器分成了前端、中端和后端三部分（有些文献上也把中端算为后端）。
你在学习中也会发现，宫老师起步篇的安排，也是按照这个顺序：02是讲词法分析，03和04的前半部分是讲语法分析，后面的04到06的部分则是循序渐进地把编译器前端的语义分析和语法分析的功能，拆成一个个具体的 feature 一点一点放到我们的示范程序中来完成、实现。
07到11节部分呢，是有关虚拟机的话题。严格意义上，其实虚拟机相关的技术并不算是传统的编译原理范畴。在编译技术的三大圣经（《龙书》、《虎书》和《鲸书》）里有关虚拟机、垃圾回收等方面的篇幅少之又少。不过这也是因为历史的局限性，毕竟 Java 这样的语言在1995年才诞生。不过我们的课程却是与时俱进的，在读到宫老师的这部分内容时候，我眼前一亮。
接下来的11-12两节课呢，是一些基础知识的铺垫。这里涉及到编译器与操作系统、和计算机硬件之间“打交道”时的一些“责任边界”。后面的14-18节，则是对编译器后端技术的实践。由于我们课程的受众大部分是软件工程师，所以在14和15两节课，宫老师又花了不少篇幅为大家科普芯片指令集的一些基础知识。
在我看来，理解芯片和汇编语言有一个很好的方法，就是把芯片看成我们中学时期用过的“科学计算器”，甚至是更简单的“日常使用的普通计算器”。唯一的不同之处就是，芯片没有给人类手指去触摸的按钮，取而代之的是需要用程序通过一组组汇编代码去操纵这个“超级微型却功能强大的计算器”。希望我这样的描述，能减少你对芯片指令集的陌生感、缓解你对“超纲知识”的恐惧。
起步篇最后的19-21这三节课呢，是对一些难点知识的精讲。如果你的基础不牢固，我建议你优先学习前面的知识内容。除此之外，我还为你整理了一张脑图，帮你“高亮”出了一些学习这门课有必要弄懂的关键概念：
动手实践才是目的理清了我们起步篇的内容，最后我们聊聊在日常工作中，我们可以从哪些方向把这门课学到的知识落到实践中来。我想从我从事过的前端、客户端、服务端和算法这四个软件工程师岗位，给你讲讲我是如何在日常工作中实践编译技术的，希望能对你有一些参考价值。
平心而论，我非常建议你，把课程作为自己学习的一个起点而不是终点。只有你真正实践过，你才能真正明白为什么大部分语言的前端都在依靠手写递归下降+算符优先级算法的组合去实现，而鲜有教科书上那样设计精巧的LL算法实现（因为first和follow集的维护成本太高了！）。
领域一：前端你可能会问，前端领域真的有必要，学习编译原理这样的技术吗？我理解，毕竟很多前端的同学，每天的工作就是机械地进行设计稿（PS、Sketch等生成的文件）到 HTML+ CSS 代码的转换。但你可能忽略了一些我们前端每天都在使用的构建工具，比如webpack、Rollup，或者是近两年涌现出的ESbuild、swc等等，这些恰恰是我在从事前端工作期间，认为最有意思的一些infra类的工作。
而且，我们这门课也使用了 JavaScript/TypeScript 语言作为教学工具。如果你就从事前端，那我非常建议你把上面这些构建工具作为自己的研究目标，像电影《速度与激情》里的剧情一样，把自己每天开的“车子”拿过来拆开看看，动手改装“魔改”一番，这会是一件非常有乐趣和成就感的事。
另外，从 Typescript 到 Wasm 这些新工具、新技术的出现也能看出，前端是最有可能在近几年内，因为编译技术而出现新变革的技术领域。如何设计出一种在开发阶段可以使用JavaScript技术栈、而在运行时又能提供尽可能像C++一样高性能的编程语言工具链，将成为业界的一个关键课题。</description></item><item><title>“屠龙之秘”｜实现计算机语言这样的技术能用在哪里？（一）</title><link>https://artisanbox.github.io/3/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/2/</guid><description>你好，我是宫文学。
在学习这么多硬知识的间隙里，我给你准备了一些相对轻松一点的内容，想让你转换一下，让大脑休息休息。
不知道你在学习这门课的时候，有没有这样的困惑：实现计算机语言这样的“屠龙技”，到底在哪里能够发挥作用呢？毕竟，不是每个人都有机会成为像Java、JavaScript这样的通用语言的发明者的。
我的答案是：其实在任何软件领域，只要你做得足够深，其实都能用上这门课程的知识。我也一直在给你传达一个理念：任何好的软件，其实到最后都会变成一个开发平台，所以也就需要用到实现计算机语言的这些相关技术。
所以，在这里，我想分成两节课或者更多节课，给你介绍一些有意思的应用场景，希望你能从中得到启发，在自己的工作中也能更好地运用我们这节课的知识，做出一些耀眼的成绩。
今天这节课，我会跟你分享自动化控制领域（或者简称自控领域），对软件编程的需求，以及如何设计一个开发平台来满足这样的需求。在学习这节课的过程里，你可以不断地做一些印证，想想你可以用在门课学到的知识，怎么来满足这个领域的需求。
但是，我需要补充一句，我自己并不是自控领域的从业者。只不过，我恰好有一些朋友，是国内这个领域顶级的、资深的专家，所以我就有机会学习到了这个领域的一点知识。并且，我跟他们参与了一个工业领域的操作系统项目的策划工作，这也激发了我对这个领域的编程模式的兴趣。但是呢，我对这个领域了解得并不那么深，也不那么专业，我主要关注其中与编程技术有关的部分。
那么首先，我们来了解一下自控系统的作用和应用场景，方便我们理解它对编程技术的需求。
自控系统的应用场景计算机技术在工业领域有着丰富的应用，其中一个应用领域，就是自动化控制。
其实自动化控制离我们特别近，你肯定不陌生。比如，当你乘坐地铁或高铁的时候，它们总是能够准确地停在站台边，停靠的位置甚至可以精准到厘米级。这里面就有自动控制的程序在起作用。
自动化控制也有在工厂里的应用，你可能没那么熟悉，但也跟你的生活密切相关。比如在发电厂里，我们需要对发电机组机组进行精确地控制。如果转子的旋转速度出了问题，那就会酿成巨大的灾难。在核电厂里，我们也需要对核燃料进行精确地控制，以便正常发电，不要引起事故。
你知道，制造业是一个现代国家的根本。而现代工厂的生产过程，几乎全部都会用到自动控制系统，包括离散制造型企业，就是给我们制造手机、电脑、汽车的企业；还有流程型企业，比如各种化工厂，还有前面提到的电厂。
最后，其实我们身边应用得非常广泛的物联网，也属于广义上的自控领域。比如，你家里的智能电表、智能门锁，还有小区停车场的自动道闸等等。
所有这些应用场景，基本上都会采用类似下面的这种技术架构：
你看这张图，最底下是各种生产设备。在这层之上呢，是一些自动化的控制器。控制器最重要的作用就是控制设备的运行。它的原理是这样的：设备在运行过程中，会不断产生信号，信号会被实时传输给控制器。控制器接收到信息以后，在内部进行实时计算，然后根据需要输出一些控制信号，让设备的运行做出一些改变。
通常来讲，控制器对计算能力的要求不会太高，控制的逻辑通常也并不是特别复杂，因为控制程序的时延必须很低。这也很容易理解，比如你开车的时候，肯定不能容忍刹车的响应很慢，那会出事故的。也正是因为实时性的要求，自动系统一般都运行在实时操作系统上，甚至运行在裸设备上。
而且，控制器会跟一个上位机连接，并通过某种工业协议进行通信。上位机的计算能力和存储能力都会比控制器更强。我们通过上位机，就可以了解每个控制器的运行情况，可以对控制器做一些操作，比如升级控制器上的程序等等。
好了，这就是自控领域最基本的计算架构，以及系统的运行场景。那么为了满足自控系统的需求，需要什么样的开发工具呢？是不是用普通的编程语言和开发工具就能满足自动的需求呢？
自控系统的技术特征自控程序的开发和运行有特殊的要求。我们平常用到的开发语言和开发工具，很难满足这些需求，因此我们必须设计专门的工具。我来给你分析一下。
首先，自控系统运行的硬件环境是不同的。
从硬件架构来看，这些控制器的设计跟我们平常的手机和桌面电脑都不一样。。
从硬件的电气特性来看，这些硬件在环境适应上的要求要比普通民用的更加严苛。这些硬件在设计上能够更好适应比较恶劣的工业环境，比如能承受比较大的震动、电磁、灰尘等的影响。普通民用的硬件设计，在这种环境下可能会从硬件产生出错误的信号，或者存储的数据发生错误，甚至无法正常工作。
从编程者的角度看，这些控制器的CPU的架构也跟我们手机和电脑不大相同。CPU架构的概念我们前面已经学过了。不同的CPU架构，意味着不同的指令集、不同的寄存器、不同的内存寻址方式，以及不同的异常处理机制等等。
不同的CPU架构，自然会导致程序开发方式的不同。普通的编译器，有可能都不支持这些芯片，或者没有针对这些硬件进行专门的优化。这个时候，如果你掌握了一些编译技术的后端知识，就知道如何以最优的方式生成机器码，并充分发挥硬件的能力。
第二，自控系统运行的软件环境也是不同的。
我们前面提到过，对于实时性要求比较高的场景，自控程序通常运行在实时操作系统里，或者是在裸设备上运行。所以，我们的开发工具也要能够生成在实时操作系统或裸设备上运行的程序。
不知道你还记不记得，我们前面讲过，在操作系统上运行，需要遵守相应的ABI，让程序能够跟操作系统相互配合起来。所以，这就需要开发工具支持实时操作系统才可以。当然，如果运行在裸设备上，那么就需要我们的开发平台本身来提供一些运行时功能，有支持IO、任务调度等能力，那这个运行时在某种意义上也是一个简化版的实时操作系统。
第三，自控系统的软件更新机制是不同的。
自控系统跟我们其他软件一样，在开发过程上也需要进行版本迭代。不过它的版本迭代有时候要求更高。比如，像发电机组等设备一旦投入运行，是不可以随意停机的。每次停机，都会造成很大的成本和影响。还有像卫星、太空舱上面的关键设备，大概也是这样的。这就需要自控系统能够在不重启的情况下实现更新。在这种需求下，自控软件的更新可能会有下面几个特点：
第一个特点：通常这些系统的更新都不是整体更新，而是支持一个个小模块的动态更新。
如果我们拿Java语言来打个比方，这相当于程序在运行的时候每次只更新一个类。从这个角度看，我们在课程里讲过的动态编译并形成可运行的模块的技术，就会派上用场了。但是静态编译并不能满足这个场景的要求，因为静态编译后的程序，在代码区里的内容通常是固定的、不能修改的。
第二个特点：在更新的时候，要满足实时性的需求。
在自控系统上，通常会有这样的场景：在10毫秒前的一次调用，可能使用的是前一个版本，在下一次调用，就已经无缝切换成了新版本。在这种情况下，程序之间不是静态链接在一起的。当我们要执行某个函数的时候，需要动态查询这个函数的地址，然后再跳转。在采用JIT技术的系统中，通常也会采用类似的技术。
第三个特点：在模块更新的时候，要能够保留程序中原来的状态信息。
自控软件在控制设备运行的时候，需要正确地掌握设备过去的状态。拿我们熟悉的场景来举个例子，如果你要升级红绿灯的控制软件，那么程序中的状态信息就要包括每个灯的状态是什么、已经持续了多久。
如果按照原来的规则，红灯要在5秒后转绿，那么软件升级后，必须仍然是在5秒后转绿，系统整体的运行逻辑才不受影响。否则的话，可能会出现路口两个方向都是绿灯的错误状态，从而引发交通事故。这种状态的错误，如果出现在电梯控制上、发电机组控制上，都会导致致命的后果。
第四，自控系统对可靠性和安全性有着很高的要求。
通过前面的描述，你应该已经体会到了，自控系统对于可靠性和安全性，有着很高的要求。我们可以想象一下，在未来的战争中，可能我们受到的第一波攻击并不是从天而降的导弹，而是来自网络的攻击。如果网络攻击就瘫痪掉来一个国家的高铁、核电、各种基础设施和工厂，那么战争也就根本不用打了。
可靠性和安全性要通过多个方面的工作来保证，包括管理角度、物理防护角度等等。落实到IT技术上，也会有多个层面的工作。比如，在操作系统层面，我们会进行可靠性方面的增强，尽量消除由于硬件的原因而产生的数据错误，比如网络传输中的数据错误、由于存储设备的原因导致的数据错误，等等。
从软件开发和运行的角度，在可靠性和安全性方面也有很多可以提升的地方。比如，传统的软件运行方式，代码段的地址都是固定的，所以就会比较容易导致攻击，比如内存溢出攻击。如果你的运行机制，让每个函数的代码地址都是随机的，那么就可能避开这种类型的攻击。
通过我们前面这些对自控程序的运行特征的分析，你会发现它确实跟我们普通的桌面软件、服务端软件、移动APP软件都不太相同。所以，针对这个领域，我们就需要专门的开发工具。那我们就看看这个领域的开发工具都有哪些特点。
自控系统的开发平台由于自控领域自身的独特性，所以多年来这个领域也发展出了一系列独特的技术。这些技术被统称为OT，也就是Operational Technology。而你熟悉的互联网系统等，则属于IT领域。在OT领域，也形成了相应的国际组织和技术标准。在过去，这些组合和标准的话语权主要在国外一些企业的手中。
在OT领域，软件开发被叫做“组态”开发。组态是英文Configuration的意思。从字面上你可以这样理解，我们给控制器开发的软件，相当于是对控制器在做配置。
而这些控制器呢，在OT领域有个广为人知的名称，叫做PLC，也就是可编程控制器（Program Logic Controller）。那么我们可以怎么为可编程控制器编程呢？
根据相关国际标准（IEC 61131-3），如果我们要给PLC编程，可以使用5中不同的编程语言，如下图：
首先是文本化的编程语言。我们平常用的编程语言其实都是文本化的，这里面又细分为两种。一种是指令表，它相当于一种语法特别简单的计算机语言。虽然简单，但是对某些编程需求来说却足够。第二种是结构化的文本，这个跟普通的高级语言差不多，语法上有点像Pascal语言。
除了这两种文本化的编程语言以外，还有三种图形化编程语言，包括梯形图、功能块图和顺序功能流图。我相信，这几个图对大多数IT领域的同学来说，应该都不太熟悉。但对于弱电领域或OT领域的很多工程师来说，阅读这些图是他们的基本功。
通过这些图，他们就能很容易地理解程序的逻辑。其实在IT领域，一直也有图形化编程的方式，比如少儿编程领域，就可以通过拖拉图形块的方式来编程。而最近越来越为人所知的低代码开发平台，也大量采用了图形化编程的方式。
我这里给出了几张图，是对某个组态开发平台的截屏，可以帮助你更直观地理解这些编程语言。
那么，问题来了，你可以怎么来利用我们这门课学到的知识点，来实现上面这样一个开发平台呢？
首先，针对这5种语言，无论是文本化的语言，还是图形化的语言，你都可以利用词法分析、语法分析和语义分析技术，形成统一的、正确的AST。
第二，基于这个统一的AST，你再可以继续编译成机器码。我们在课程里讲过了如何针对x86-64架构的CPU生成汇编代码。不过呢，你要根据PLC所采用的芯片，来为它生成针对该芯片的汇编代码。一般芯片厂商都要提供工具链，能够把相应的汇编代码生成机器码。
第三，我们要设计一个专门的运行时。在我们的课程中，我们已经实现过虚拟机，这就是一种运行时。不过，针对OT的需求，PLC的运行时要复杂一些，比如要能够实现软件模块的动态加载和更新的管理、代码地址的转换、状态信息的维护，还有与通讯模块和底层操作系统的衔接等工作。
第四，我们要对程序进行优化。对于比较复杂的控制逻辑，我们要运用优化算法，提高程序的性能。程序的性能越高，可以满足的实时性要求就越高。像控制机器人臂这样的场景，对实时性要求就是很高的。如果我们不把代码进行充分优化，那就很难满足这些高实时性场景的要求。
所以，我们在课程的第三部分，会专门花时间学习优化技术。如果你能把优化技术也掌握透彻，那么你就有可能成为这个领域的顶级技术专家了。
通过上面的这些分析，你会发现，其实很多知识点，我们在前面的课程中都已经涉及了，还有一些知识点我们会在后面继续学习。所以，只要你认真掌握了我们这门课的内容，你基本上就可以胜任这些技术工作了！
课程小结今天的加餐，我分享了自动控制领域的一些背景信息，也讨论了如何针对这个领域的需求来研发相应的组态软件平台。我写这篇加餐其实有几个目的：
第一，是开阔你的视野。你可能并不是OT领域的技术人员，以后也不会做OT有关的事情。但是，它山之石可以攻玉。OT和IT在发展过程中，一直在互相影响。比如，OT处理高可靠性的一些思路，就有可能用于高可靠的IT应用中。
第二，我希望你能理解我们这节课的知识点，是怎么用于解决具体领域的问题的。比如，你可能发现，我在这门课里特别重视让你理解程序的运行机制，包括程序跟CPU架构的关系、跟操作系统的关系、理解ABI等等。从今天的分享中你会看到，要解决一个特定领域的问题，特别是当你需要自己研发相关工具的时候，这些知识都很重要。
最后，我也希望通过今天这节课，能让工业领域之外的人也了解一点工业软件。毕竟，工业是我们国家的立身之本。而工业领域的基础软件，还有很多工作，需要有志之士参与进来，提升我们国家在这个领域的创新能力和话语权。
我在后面的加餐里，还准备给你分析一些其他的应用领域和开发工具。我们总说“学以致用”，了解更多的应用场景，也会有助于你理解和掌握我们这门课程的知识点。
思考题如果你是来自于自控领域的，我想请你帮我补充一些信息，包括自控领域的其他应用场景、编程技术等等。
如果你来自其他领域，那么我想问问，你们的领域有没有跟自控领域类似的技术问题，以至于需要研发专门的开发平台呢？欢迎在留言区和我分享。
我是宫文学，我们下节课见。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>《数据结构与算法之美》学习指导手册</title><link>https://artisanbox.github.io/2/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/1/</guid><description>你好，我是王争。
在设计专栏内容的时候，为了兼顾不同基础的同学，我在内容上做到了难易结合，既有简单的数组、链表、栈、队列这些基础内容，也有红黑树、BM、KMP这些难度较大的算法。但是，对于初学者来说，一下子面对这么多知识，可能还是比较懵。
我觉得，对于初学者来说，先把最简单、最基础、最重要的知识点掌握好，再去研究难度较高、更加高级的知识点，这样由易到难、循序渐进的学习路径，无疑是最合理的。
基于这个路径，我对专栏内容，重新做了一次梳理，希望给你一份具体、明确、有效的学习指导。我会写清楚每个知识点的难易程度、需要你掌握到什么程度、具体如何来学习。
如果你是数据结构和算法的初学者，或者你觉得自己的基础比较薄弱，希望这份学习指导，能够让你学起来能更加有的放矢，能把精力、时间花在刀刃上，获得更好的学习效果。
下面，我先给出一个大致的学习路线。
（建议保存后查看大图）现在，针对每个知识点，我再给你逐一解释一下。我这里先说明一下，下面标记的难易程度、是否重点、掌握程度，都只是针对初学者来说的，如果你已经有一定基础，可以根据自己的情况，安排自己的学习。
1.复杂度分析尽管在专栏中，我只用了两节课的内容，来讲复杂度分析这个知识点。但是，我想说的是，它真的非常重要。你必须要牢牢掌握这两节，基本上要做到，简单代码能很快分析出时间、空间复杂度；对于复杂点的代码，比如递归代码，你也要掌握专栏中讲到的两种分析方法：递推公式和递归树。
对于初学者来说，光看入门篇的两节复杂度分析文章，可能还不足以完全掌握复杂度分析。不过，在后续讲解每种数据结构和算法的时候，我都有详细分析它们的时间、空间复杂度。所以，你可以在学习专栏中其他章节的时候，再不停地、有意识地去训练自己的复杂度分析能力。
难易程度：Medium
是否重点：10分
掌握程度：在不看我的分析的情况下，能自行分析专栏中大部分数据结构和算法的时间、空间复杂度
2.数组、栈、队列这一部分内容非常简单，初学者学起来也不会很难。但是，作为基础的数据结构，数组、栈、队列，是后续很多复杂数据结构和算法的基础，所以，这些内容你一定要掌握。
难易程度：Easy
是否重点：8分
掌握程度：能自己实现动态数组、栈、队列
3.链表链表非常重要！虽然理论内容不多，但链表上的操作却很复杂。所以，面试中经常会考察，你一定要掌握。而且，我这里说“掌握”不只是能看懂专栏中的内容，还能将专栏中提到的经典链表题目，比如链表反转、求中间结点等，轻松无bug地实现出来。
难易程度：Medium
是否重点：9分
掌握程度：能轻松写出经典链表题目代码
4.递归对于初学者来说，递归代码非常难掌握，不管是读起来，还是写起来。但是，这道坎你必须要跨过，跨不过就不能算是入门数据结构和算法。我们后面讲到的很多数据结构和算法的代码实现，都要用到递归。
递归相关的理论知识也不多，所以还是要多练。你可以先在网上找些简单的题目练手，比如斐波那契数列、求阶乘等，然后再慢慢过渡到更加有难度的，比如归并排序、快速排序、二叉树的遍历、求高度，最后是回溯八皇后、背包问题等。
难易程度：Hard
是否重点：10分
掌握程度：轻松写出二叉树遍历、八皇后、背包问题、DFS的递归代码
5.排序、二分查找这一部分并不难，你只需要能看懂我专栏里的内容即可。
难易程度：Easy
是否重点：7分
掌握程度：能自己把各种排序算法、二分查找及其变体代码写一遍就可以了
6.跳表对于初学者来说，并不需要非得掌握跳表，所以，如果没有精力，这一章节可以先跳过。
难易程度：Medium
是否重点：6分
掌握程度：初学者可以先跳过。如果感兴趣，看懂专栏内容即可，不需要掌握代码实现
7.散列表尽管散列表的内容我讲了很多，有三节课。但是，总体上来讲，这块内容理解起来并不难。但是，作为一种应用非常广泛的数据结构，你还是要掌握牢固散列表。
难易程度：Medium
是否重点：8分
掌握程度：对于初学者来说，自己能代码实现一个拉链法解决冲突的散列表即可
8.哈希算法这部分纯粹是为了开拓思路，初学者可以略过。
难易程度：Easy
是否重点：3分
掌握程度：可以暂时不看
9.二叉树这一部分非常重要！二叉树在面试中经常会被考到，所以要重点掌握。但是我这里说的二叉树，并不包含专栏中红黑树的内容。红黑树我们待会再讲。
难易程度：Medium
是否重点：9分
掌握程度：能代码实现二叉树的三种遍历算法、按层遍历、求高度等经典二叉树题目
10.红黑树对于初学者来说，这一节课完全可以不看。
难易程度：Hard
是否重点：3分
掌握程度：初学者不用把时间浪费在上面
11. B+树虽然B+树也算是比较高级的一种数据结构了，但是对初学者来说，也不是重点。有时候面试的时候还是会问的，所以这一部分内容，你能看懂专栏里的讲解就可以了。
难易程度：Medium
是否重点：5分
掌握程度：可看可不看
12.堆与堆排序这一部分内容不是很难，初学者也是要掌握的。
难易程度：Medium
是否重点：8分
掌握程度：能代码实现堆、堆排序，并且掌握堆的三种应用（优先级队列、Top k、中位数）
13.图的表示图的内容很多，但是初学者不需要掌握那么多。一般BAT等大厂面试，不怎么会面试有关图的内容，因为面试官可能也对这块不会很熟悉哈：）。但是，最基本图的概念、表示方法还是要掌握的。
难易程度：Easy
是否重点：8分
掌握程度：理解图的三种表示方法（邻接矩阵、邻接表、逆邻接表），能自己代码实现
14.深度广度优先搜索这算是图上最基础的遍历或者说是搜索算法了，所以还是要掌握一下。这两种算法的原理都不难哈，但是代码实现并不简单，一个用到了队列，另一个用到了递归。对于初学者来说，看懂这两个代码实现就是一个挑战！可以等到其他更重要的内容都掌握之后，再来挑战，也是可以的。
难易程度：Hard
是否重点：8分
掌握程度：能代码实现广度优先、深度优先搜索算法
15.拓扑排序、最短路径、A*算法这几个算法稍微高级点。如果你能轻松实现深度、广度优先搜索，那看懂这三个算法不成问题。不过，这三种算法不是重点。面试不会考的。
难易程度：Hard</description></item><item><title>不定期福利第一期_数据结构与算法学习书单</title><link>https://artisanbox.github.io/2/61/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/61/</guid><description>你好，我是王争。欢迎来到不定期更新的周末福利时间。
专栏已经上线两周了，看到这么多人在留言区写下自己的疑惑或者观点，我特别开心。在留言里，很多同学让我推荐一些学习数据结构与算法的书籍。因此我特意跟编辑商量了，给你一个周末福利。所以这一期呢，我们就来聊一聊数据结构和算法学习过程中有哪些必读书籍。
有的同学还在读大学，代码还没写过几行；有的同学已经工作数十年，这之间的差别还是挺大的。而不同基础的人，适宜看的书是完全不一样的。因此，针对不同层次、不同语言的同学，我分别推荐了不同的书。希望每个同学，都能找到适合自己的学习资料，都能在现有水平上有所提高。
针对入门的趣味书入门的同学，我建议你不要过度追求上去就看经典书。像《算法导论》《算法》这些书，虽然比较经典、比较权威，但是非常厚。初学就去啃这些书肯定会很费劲。而一旦啃不下来，挫败感就会很强。所以，入门的同学，我建议你找一些比较容易看的书来看，比如《大话数据结构》和《算法图解》。不要太在意书写得深浅，重要的是能不能坚持看完。
《大话数据结构》 这本书最大的特点是，它把理论讲得很有趣，不枯燥。而且每个数据结构和算法，作者都结合生活中的例子进行了讲解，能让你有非常直观的感受。虽然这本书有400多页，但是花两天时间读完，应该是没问题的。如果你之前完全不懂数据结构和算法，可以先从这本书看起。
《算法图解》 跟《大话数据结构》走的是同样的路线，就像这本书副标题写的那样，“像小说一样有趣的算法入门书”，主打“图解”，通俗易懂。它只有不到200页，所以内容比较少。作为入门，看看这本书，能让你对数据结构和算法有个大概的认识。
这些入门书共同的问题是，缺少细节，不够系统，也不够严谨。所以，如果你想要系统地学数据结构和算法，看这两本书肯定是不够的。
针对特定编程语言的教科书讲数据结构和算法，肯定会跟代码实现挂钩。所以，很多人就很关心，某某书籍是用什么语言实现的，是不是自己熟悉的语言。市面大部分数据结构和算法书籍都是用C、C++、Java语言实现的，还有些是用伪代码。而使用Python、Go、PHP、JavaScript、Objective-C这些编程语言实现的就更少了。
我这里推荐《数据结构和算法分析》。国内外很多大学都拿这本书当作教材。这本书非常系统、全面、严谨，而且又不是特别难，适合对数据结构和算法有些了解，并且掌握了至少一门编程语言的同学。而且，这个作者也很用心。他用了三种语言，写了三个版本，分别是：《数据结构与算法分析 ：C语言描述》《数据结构与算法分析：C++描述》《数据结构与算法分析：Java语言描述》。
如果你熟悉的是Python或者JavaScript，可以参考《数据结构与算法JavaScript描述》《数据结构与算法：Python语言描述》 。至于其他语言的算法书籍，确实比较少。如果你有推荐，可以在留言区补充一下。
面试必刷的宝典算法对面试很重要，很多人也很关心。我这里推荐几本有益于面试的书籍，分别是：《剑指offer》《编程珠玑》《编程之美》。
从《剑指offer》这本书的名字就可以看出，作者的写作目的非常明确，就是为了面试。这本书几乎包含了所有常见的、经典的面试题。如果能搞懂这本书里的内容，应付一般公司的面试应该不成问题。
《编程珠玑》这本书的豆瓣评分非常高，有9分。这本书最大的特色就是讲了很多针对海量数据的处理技巧。这个可能是其他算法书籍很少涉及的。面试的时候，海量数据处理的问题也是经常会问的，特别是校招面试。不管是开拓眼界，还是应付面试，这本书都很值得一看。
《编程之美》这本书有多位作者，其中绝大部分是微软的工程师，所以书的质量很有保证。不过，这里面的算法题目稍微有点难，也不是很系统，这也是我把它归到面试这一部分的原因。如果你有一定基础，也喜欢钻研些算法问题，或者要面试Google、Facebook这样的公司，可以拿这本书里的题，先来自测一下。
经典大部头很多人一提到算法书就会搬出《算法导论》和《算法》。这两本确实非常经典，但是都太厚了，看起来比较费劲，我估计很少有人能坚持全部看下来。如果你想更加深入地学一学数据结构和算法，我还是强烈建议你看看。
我个人觉得，《算法导论》这本书的章节安排不是循序渐进的，里面充斥着各种算法的正确性、复杂度的证明、推导，数学公式比较多，一般人看起来会比较吃力。所以，作为入门书籍，并不是很推荐。
《算法》这本书也是一本经典大部头，不过它比起《算法导论》来要友好很多，更容易看懂，更适合初学者入门。但是这本书的缺点也很明显，就是内容不够全面，比如动态规划这么重要的知识点，这本书就没有讲。对于数据结构的东西，它讲的也不多，基本就是偏重讲算法。
殿堂级经典说到殿堂级经典书，如果《计算机程序设计艺术》称第二，我想没人敢称第一。这本书包括很多卷。说实话，我也只看过比较简单的几卷，比如《基本算法》《排序和查找》。
这套书的深度、广度、系统性、全面性是其他所有数据结构和算法书籍都无法相比的。但是，如果你对算法和数据结构不是特别感兴趣，没有很好的数学、算法、计算机基础，想要把这套书读完、读懂是比较难的。你可以把它当作你算法学习的终极挑战。
闲暇阅读算法无处不在。我这里再推荐几本适合闲暇时间阅读的书：《算法帝国》《数学之美》《算法之美》。
这些书共同的特点是，都列举了大量的例子，非常通俗易懂。夸张点说，像《算法帝国》，文科生都能读懂。当你看这些书的时候，你常常会深深感受到算法的力量，被算法的优美之处折服。即便不是从事IT工作的，看完这几本书也可以开拓眼界。
书籍差不多就是这些。除此之外，留言区很多人问到算法的实现语言。我这里也解释一下。因为我现在比较常用的编程语言是Java。所以，在专栏里，特别简单的、不涉及高级语法的，我会用Java或者C、C++来实现。稍微复杂的，为了让你能看懂，我会用伪代码。所以你完全不用担心语言的问题。
每节课中有需要代码实现的数据结构和算法，我都另外用Java语言实现一遍，然后放到Github上，供你参考。Github的地址我放在这里，你可以收藏一下：https://github.com/wangzheng0822/algo。
至于其他语言的同学，比如C、C++、Python、Go、PHP、JavaScript、Objective-C等，我想了一个crowd sourcing的方法。
我希望基础较好的同学，参照我的Java实现，用你熟悉的编程语言再实现一遍，并且将代码留言给我。如果你写得正确，我会将你的代码上传到Github上，分享给更多人。
还有人问，我学完这个专栏，就可以拿下数据结构和算法吗？我想说的是，每个人的基础、学习能力都不一样，掌握程度取决于你的努力程度。除了你之外，没有人能百分之百保证你能掌握什么知识。
有的同学只是把每一节课听下来、看下来，就束之高阁，也不求甚解，那效果肯定会很差。而有些同学除了听、看之外，遇到不懂的会自己去查资料、看参考书籍，还会把我讲的数据结构和算法都认真地实现一遍，这样的学习效果自然就比只听一遍、看一遍要好很多。即便我已经尽我所能把这些知识讲得深入浅出，通俗易懂，但是学习依然还是要靠你自己啊。
这种答疑的方式也会成为我们之后的固定动作，我会把留言里有价值的问题和反馈沉淀下来，希望对你的日常学习起到补充作用。如果你有什么看不懂、听不懂的地方，或者工作中有遇到算法问题、技术难题，欢迎写在留言区。（我发现留言区里卧虎藏龙啊，没事儿可以多扫扫留言区。）
这次的周末福利时间就到这啦，我们下次见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>不定期福利第三期_测一测你的算法阶段学习成果</title><link>https://artisanbox.github.io/2/59/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/59/</guid><description>专栏最重要的基础篇马上就要讲完了，不知道你掌握了多少？我从前面的文章中挑选了一些案例，稍加修改，组成了一套测试题。
你先不要着急看答案，自己先想一想怎么解决，测一测自己对之前的知识掌握的程度。如果有哪里卡壳或者不怎么清楚的，可以回过头再复习一下。
正所谓温故知新，这种通过实际问题查缺补漏的学习方法，非常利于你巩固前面讲的知识点，你可要好好珍惜这次机会哦！
实战测试题（一）假设猎聘网有10万名猎头顾问，每个猎头顾问都可以通过做任务（比如发布职位），来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这10万个猎头ID和积分信息，让它能够支持这样几个操作：
根据猎头的ID快速查找、删除、更新这个猎头的积分信息；
查找积分在某个区间的猎头ID列表；
查询积分从小到大排在第x位的猎头ID信息；
查找按照积分从小到大排名在第x位到第y位之间的猎头ID列表。
相关章节17 | 跳表：为什么Redis一定要用跳表来实现有序集合？
20 | 散列表（下）：为什么散列表和链表经常会一起使用？
25 | 红黑树：为什么工程中都用红黑树这种二叉树？
题目解析这个问题既要通过ID来查询，又要通过积分来查询，所以，对于猎头这样一个对象，我们需要将其组织成两种数据结构，才能支持这两类操作。
我们按照ID，将猎头信息组织成散列表。这样，就可以根据ID信息快速地查找、删除、更新猎头的信息。我们按照积分，将猎头信息组织成跳表这种数据结构，按照积分来查找猎头信息，就非常高效，时间复杂度是O(logn)。
我刚刚讲的是针对第一个、第二个操作的解决方案。第三个、第四个操作是类似的，按照排名来查询，这两个操作该如何实现呢？
我们可以对刚刚的跳表进行改造，每个索引结点中加入一个span字段，记录这个索引结点到下一个索引结点的包含的链表结点的个数。这样就可以利用跳表索引，快速计算出排名在某一位的猎头或者排名在某个区间的猎头列表。
实际上，这些就是Redis中有序集合这种数据类型的实现原理。在开发中，我们并不需要从零开始代码实现一个散列表和跳表，我们可以直接利用Redis的有序集合来完成。
实战测试题（二）电商交易系统中，订单数据一般都会很大，我们一般都分库分表来存储。假设我们分了10个库并存储在不同的机器上，在不引入复杂的分库分表中间件的情况下，我们希望开发一个小的功能，能够快速地查询金额最大的前K个订单（K是输入参数，可能是1、10、1000、10000，假设最大不会超过10万）。如果你是这个功能的设计开发负责人，你会如何设计一个比较详细的、可以落地执行的设计方案呢？
为了方便你设计，我先交代一些必要的背景，在设计过程中，如果有其他需要明确的背景，你可以自行假设。
数据库中，订单表的金额字段上建有索引，我们可以通过select order by limit语句来获取数据库中的数据；
我们的机器的可用内存有限，比如只有几百M剩余可用内存。希望你的设计尽量节省内存，不要发生Out of Memory Error。
相关章节12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？
28 | 堆和堆排序：为什么说堆排序没有快速排序快？
29 | 堆的应用：如何快速获取到Top 10最热门的搜索关键词？
题目解析解决这个题目的基本思路我想你应该能想到，就是借助归并排序中的合并函数，这个我们在排序（下）以及堆的应用那一节中讲过。
我们从每个数据库中，通过select order by limit语句，各取局部金额最大的订单，把取出来的10个订单放到优先级队列中，取出最大值（也就是大顶堆堆顶数据），就是全局金额最大的订单。然后再从这个全局金额最大订单对应的数据库中，取出下一条订单（按照订单金额从大到小排列的），然后放到优先级队列中。一直重复上面的过程，直到找到金额前K（K是用户输入的）大订单。
从算法的角度看起来，这个方案非常完美，但是，从实战的角度来说，这个方案并不高效，甚至很低效。因为我们忽略了，数据库读取数据的性能才是这个问题的性能瓶颈。所以，我们要尽量减少SQL请求，每次多取一些数据出来，那一次性取出多少才合适呢？这就比较灵活、比较有技巧了。一次性取太多，会导致数据量太大，SQL执行很慢，还有可能触发超时，而且，我们题目中也说了，内存有限，太多的数据加载到内存中，还有可能导致Out of Memory Error。
所以，一次性不能取太多数据，也不能取太少数据，到底是多少，还要根据实际的硬件环境做benchmark测试去找最合适的。
实战测试题（三）我们知道，CPU资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致CPU频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。
当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？
相关章节09 | 队列：队列在线程池等有限资源池中的应用</description></item><item><title>不定期福利第二期_王争：羁绊前行的，不是肆虐的狂风，而是内心的迷茫</title><link>https://artisanbox.github.io/2/58/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/58/</guid><description>你好，我是王争。
专栏更新过半，我发现有些小伙伴已经掉队，虽然有人掉队也挺正常，但是我还是想尽量拉一把。于是，周末的时间，我就在想，究竟是什么原因让有些小伙伴掉队了？是内容本身太难了吗？是我讲得不够清楚吗？还是小伙伴本身基础太差、不够努力、没有掌握学习方法？
我觉得都不是，让你掉队的原因，从根儿上讲，是你内心的迷茫。如果我们不那么确信能不能看懂、能不能学会的时候，当面对困难的时候，很容易就会否定自己，也就很容易半途而废。
这就好比你迷失在沙漠中，对你来说，肆虐的狂风并不可怕，可怕的是，你不知道该努力多久才能走出沙漠，不知道到底能不能走出沙漠。这种对结果的未知、不确定，导致了你内心的恐惧，最后就差那么一点点就可以走出沙漠的时候，你放弃了。
学习也是同样的道理。所以，我今天不打算讲学习方法，也不打算给你灌输心灵鸡汤，我就讲讲，对这个专栏的学习，或者对于任何学习来说，我觉得你应该建立的一些正确认知。有了这些认知，希望你能在后面的专栏学习中，少一点迷茫，多一份坚持。
没有捷径，没有杀手锏，更没有一招致胜的“葵花宝典”有小伙伴给我留言说：“看书五分钟，笔记两小时，急求学霸的学习方法”，还有人问，“数据结构和算法好难，到底该怎么学？是我的学习方法不对？还是我太笨？”
我想说，并没有什么杀手锏的学习方法，更没有一招致胜的“葵花宝典”。不知道这么说有没有让你失望。如果你真要“求”一个学习方法，那就再看看我在专栏开始写的“如何抓住重点，系统高效地学习数据结构与算法”那篇文章吧。
说实话，我也挺想知道学霸的学习方法的，所以，在求学路上，每当有学霸来分享学习方法，我都要去听一听。但是，听多了之后，我发现其实并没有太多用。因为那些所谓学霸的学习方法，其实都很简单，比如“认认真真听讲”“认认真真做每一道题”等等。
也不是他们说的不对，但是这种大实话，我总有一种领会不了的感觉，更别说真正指导我的学习了。而且，我觉得，很多时候，这些方法论的难点并不在于能不能听懂，而是在于能不能执行到位。比如很多人都听过“一万小时定律”，坚持一万个小时，你就能成为大牛，但有多少人能坚持一万个小时呢？
所以，这里我要纠正一个认知，那就是，学习没有“杀手锏”似的方法论。不要怀疑是不是自己的学习方法不对，不要在开始就否定自己。因为否定得越多，你就越迷茫，越不能坚持。
不要浮躁，不要丧失思考能力，不要丧失学习能力有小伙伴给我留言说：“老师，这个地方看不懂，你能不能再解释一下”，还有小伙伴留言说：“《红黑树（上）》里的图为什么跟你的定义不相符？”
对于留言的问题，我都挺重视的，但是当仔细看这些问题的时候，我发现，实际上文章里已经有答案了，他根本没有认真看、认真思考，更别说去自己搜搜资料，再研究下，就来提问了。
一般情况下，我都会回复“你自己再认真看一遍”或者“你自己先去网上搜一下，研究研究，如果还不懂再给我留言”。告诉你答案，并不会花费我太长时间，但是，这样会让你丢失最宝贵的东西，那就是，你自己的思考能力、学习能力，能自己沉下心来研究的能力。这个是很可怕的。
现在，互联网如此发达，我们每天都会面对各种各样的信息轰炸，人也变得越来越浮躁。很多人习惯看些不动脑子就能看懂的东西，看到稍微复杂的东西，就感觉脑子转不动了。
上学的时候还好，要考试，有老师督促，还能坚持学习。但是工作之后，没有人监督，很多人陷入各种手机App中不能自拔，学一会儿就想玩会儿手机，想静下心来学上半个小时都无比困难。无法自律，沉不下心来，那你就基本可以跟学习说拜拜了。
只有做好打硬仗的心理准备，遇到困难才能心态平和还有小伙伴给我留言说：“看不懂，一个4000多字的文章、10分钟的音频，反复看了、听了2个小时都没怎么看懂”。我给他的回复是：“如果之前没有基础或者基础不好的话，看2个小时还不懂，很正常，看一个礼拜试试。”
“一个礼拜”的说法，我一点都不是夸张。虽然专栏的每篇文章都只有三四千字，10分钟左右的音频，但是知识点的密度还是很高的。如果你潜意识里觉得应该一下子就能看懂，就会出现这样的情况：看了一遍不懂，又看了一遍还是不怎么懂，然后就放弃了。
数据结构和算法就是一个非常难啃的硬骨头，可以说是计算机学科中最难学的学科之一了。我当时学习也费了老大的劲，能做到讲给你听，我靠的也是十年如一的积累和坚持。如果没有基础、或者基础不好，你怎能期望看2个小时就能完全掌握呢？
面对这种硬骨头，我觉得我们要有打硬仗、打持久战的心理准备。只有这样，在学习的过程中遇到困难的时候，心态才能更加平和，才能沉下心来有条不紊地去解决一个个的疑难问题。这样，碰到问题，你可能还会“窃喜”，我又遇到了一个之前不怎么懂的知识点了，看懂它我又进步了一点。甚至你还会“坏坏地”想，又多了一个拉开我跟其他人距离的地方了。跨过这些点，我就能比别人更厉害。
一口吃不成胖子，如果你基础不好，那就从长计议吧，给自己定一个长一点的“死磕”计划，比如一年。面对不懂的知识点，沉下心来逐个突破，这样你的信心慢慢也就建立了。
“放弃”的念头像是一个心魔，它会一直围绕着你还有小伙伴给我留言说：“开始没怎么看懂，看了一下午，终于看懂了”。看到这样的留言，我其实挺为他感到庆幸的，庆幸他没有中途放弃。因为，放弃的念头就像一个心魔，在我们的学习过程中，它会一直围绕着我们，一旦被它打败一次，你就会被它打败很多次，掉队就不可避免了。
我分享一个我最近思考比较多的事情。前一段时间，我在研究多线程方面的东西，它涉及一块比较复杂的内容，“Java内存模型”。虽然看懂并不难，但是要透彻、无盲点地理解并不容易。本来以为半天就能看懂的东西，结果我从周一一直看到周五下午，断断续续花了5天的时间才把它彻底搞懂。回忆起这5天，我有不下10次都想放弃，每次心里都在想：“算了，先放一放，以后再说吧”“太难了，啃不下来，算了。”“就这样吧，反正也用不到，没必要浪费时间”等等。这种放弃的念头就像一个邪恶的魔鬼一样，一直围绕着我这5天的研究中。
现在回想起来，我很庆幸我当时没有放弃，多坚持了几天。如果当时我放弃了，那之后再遇到技术难题时，“放弃”的心魔还会再来拜访我，潜意识里我还是会认输。
之所以没有放弃，我自己总结了两点原因。
第一，我对学习这件事情认识得比较清楚，我一直觉得，没有学不会的东西，没有攻克不了的技术难题，如果有，那就说明时间花得还不够多。
第二，我之前遇到卡壳的时候，几乎从来没有放弃过，即便短暂地停歇，我也会继续拎起来再死磕，而且每次都能搞定，正是这种正向的激励，给了我信心，让我再遇到困难的时候，都能坚信自己能搞定它。
入门是一个非常漫长和煎熬的过程，谁都逃不过还有小伙伴留言说：“看到有小伙伴有很多疑问，我来帮作者说句话，文章写得很好，通俗易懂，如果有一定基础，看懂还是不成问题的。”
我觉得，有些小伙伴的觉悟还是挺高的：）。我文章写得再通俗易懂，对于之前没有任何基础的人来说，看起来还是挺费劲的。
第一，数据结构和算法这门课程本身的难度摆在那里，想要轻松看懂，本身就不太现实。第二，对于任何新知识的学习，入门都是一个非常漫长和煎熬的过程。但是这个过程都是要经历的，谁都逃不过。只要你挺过去，入了门，再学习更深的知识就简单多了。
我大学里的第一堂课是C语言，现在回想起来，当时对我来说，简直就是听天书。因为之前没有接触过计算机，更别说编程语言，对我来说，C语言就像另一个世界的东西。从完全看不懂，到慢慢有点看懂，再到完全看懂，不夸张地讲，我花了好几年的时间，但是当掌握了之后，我发现这个东西其实也不难。但是如果没有度过漫长和煎熬的入门的过程，如果没有一点韧性，没有一点点信念，那可能也没有现在的我了。
其实我一直觉得情商比智商更重要。对于很多学科的学习，智商并不是瓶颈，最终能够决定你能达到的高度的，还是情商，而情商中最重要的，我觉得就是逆商（逆境商数，Adversity Quotient），也就是，当你遇到困难时，你会如何去面对，这将会决定你的人生最终能够走多远。
好了，今天我想分享的关于学习的几个认知就讲完了。现在，你有没有对学习这件事有更加清晰的认识呢？能不能让你少一点迷茫，多一份坚持呢？
最后，我有一句送给你：吃得苦中苦，方为人上人。耐得住寂寞，才能守得住繁华。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>不定期福利第四期_刘超：我是怎么学习《数据结构与算法之美》的？</title><link>https://artisanbox.github.io/2/60/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/60/</guid><description>你好，我是刘超，是隔壁《趣谈网络协议》专栏的作者。今天来“串个门儿”，讲讲我学习《数据结构与算法之美》这个专栏的一些体会和感受。
《数据结构与算法之美》是目前“极客时间”订阅量最多的专栏，我也是其中最早购买的一员。我之所以一看就心动了，源于王争老师在开篇词里面说的那段话：
基础知识就像是一座大楼的地基，它决定了我们的技术高度。那技术人究竟都需要修炼哪些“内功”呢？我觉得，无外乎就是大学里的那些基础课程，操作系统、计算机网络、编译原理等等，当然还有数据结构和算法。
这个也是我写《趣谈网络协议》的时候，在开篇词里反复强调的观点。我为什么这么说呢？因为，我们作为面试官，在招人的时候，往往发现，使用框架速成的人很多，基础知识扎实的人少见，而基础不扎实会影响你以后学习新技术的速度和职业发展的广度。
和“极客时间”编辑聊的时候，我也多次表达，希望我们讲的东西和一般的培训机构有所区别，希望“极客时间”能做真正对程序员的技能提升和职业发展有价值的内容，希望“极客时间”能够成为真正帮助程序员成长的助手。
所以，当“极客时间”相继推出《Java核心技术36讲》《零基础学Python》《从0开始学架构》《MySQL实战45讲》这些课程的时候，我非常开心。我希望将来能够继续覆盖到编译原理、操作系统、计算机组成原理等等。在这些课程里，算法是基础的基础，也是我本人很想精进的部分。
当然，除了长远的职业发展需要，搞定算法还有一个看得见、摸得着的好处，面试。
我经常讲，越是薪资低的企业，面试的时候，它们往往越注重你会不会做网站，甚至会要求你现场做出个东西来。你要注意了，这其实是在找代码熟练工。相反，越是薪资高的企业，越是重视考察基础知识。基础好，说明可塑性强，培养起来也比较快。而最牛的公司，考的往往是算法和思路。
相信很多购买《数据结构与算法之美》专栏的同学，下单的时候，已经想象自己面试的时候，在白板上挥洒代码，面试官频频点头的场景，想着自己马上就能“进驻牛公司，迎娶白富美”了。
然而，事实却是，武功套路容易学，扎马步基本功难练，编程也是一样。框架容易学，基本功难。你没办法讨巧，你要像郭靖学习降龙十八掌那样，一掌一掌劈下去才行。
于是，咱们这个专栏就开始了，你见到的仍然是困难的复杂度计算，指针指来指去，烧脑的逻辑，小心翼翼的边界条件判断。你发现，数据结构和算法好像并不是你上下班时间顺便听一听就能攻克的问题。你需要静下心来仔细想，拿个笔画一画，甚至要写一写代码，Debug一下，才能够理解。是的，的确不轻松，那你坚持下来了吗？
我在这里分享一下我的学习思路，我将这个看起来困难的过程分成了几部分来完成。
第一部分，数据结构和算法的基础知识部分。如果在大学学过这门课，在专栏里，你会看到很多熟悉的描述。有些基础比较好的同学会质疑写这些知识的必要性。这大可不必，因为每个人的基础不一样，为了专栏内容的系统性和完整性，老师肯定要把这些基础知识重新讲述一遍的。对于这一部分内容，如果你的基础比较好，可以像学其他课程一样，在上下班或者午休的时候进行学习，主要是起到温习的作用。
第二部分，需要代码练习的部分。由于王争老师面试过很多人，所以在专栏里，他会列举一些他在面试中常常会问的题目。很多情况下，这些题目需要当场就能在白板上写出来。这些问题对于想要提升自己面试能力的同学来说，应该是很有帮助的。
我这里列举几个，你可以看看，是不是都能回答出来呢？
在链表这一节：单链表反转，链表中环的检测，两个有序的链表合并，删除链表倒数第n个结点，求链表的中间结点等。
在栈这一节，在函数调用中的应用，在表达式求值中的应用，在括号匹配中的应用。
在排序这一节，如何在O(n)的时间复杂度内查找一个无序数组中的第 K大元素？
在二分查找这一节，二分查找的四个变体。
这些问题你都应该上手写写代码，或者在面试之前拿来练练手，而且，不仅仅只是实现主要功能。大公司的面试很多情况下都会考虑边界条件。只要被面试官抓住漏洞，就会被扣分，所以你最好事先写写。
第三部分，对于海量数据的处理思路问题。现在排名靠前的大公司，大都存在海量数据的处理问题。对于这一类问题，在面试的时候，也是经常会问到的。由于这类问题复杂度比较高，很少让当场就写代码，但是基本上会让你说一个思路，或者写写伪代码。想要解决海量数据的问题，你会的就不能只是基础的数据结构和算法了，你需要综合应用。如果平时没有想过这部分问题，临时被问，肯定会懵。
在专栏里，王争老师列举了大量这类问题，你要重点思考这类问题背后的思路，然后平时自己处理问题的时候，也多想想，如果这个问题数据量大的话，应该怎么办。这样多思考，面试的时候，思路很容易就来了。
比如，我这里随便列了几个，都是很经典的问题。你要是想不起来，就赶紧去复习吧！
比如说，我们有10GB的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百MB，没办法一次性把10GB的数据都加载到内存中。这个时候该怎么办呢？
如果你所在的省有50万考生，如何通过成绩快速排序得出名次呢？
假设我们有10万个手机号码，希望将这10万个手机号码从小到大排序，你有什么比较快速的排序方法呢？
假设我们有1000万个整型数据，每个数据占8个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这1000万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过100MB，你会怎么做呢？
第四部分，工业实践部分。在每种数据结构的讲解中，老师会重点分析一些这些数据结构在工业上的实践，封装在库里面的，一般人不注意的。
我看王争老师也是个代码分析控。一般同学可能遇到问题，查一查有没有开源软件或者现成的库，可以用就完了。而王争老师会研究底层代码的实现，解析为什么这些在工业中大量使用的库，应该这样实现。这部分不但对于面试有帮助，对于实际开发也有很大的帮助。普通程序员和高手的差距，就是一个用完了就完了，一个用完了要看看为啥这样用。
例如，老师解析了Glibc中的qsort() 函数，Java中的HashMap如何实现工业级的散列表，Redis中的有序集合（Sorted Set）的实现，工程上使用的红黑树等等。
尤其是对于哈希算法，老师解析了安全加密、数据校验、唯一标识、散列函数，负载均衡、数据分片、分布式存储等应用。如果你同时订阅了架构、微服务的课程，你会发现这些算法在目前最火的架构设计中，都有使用。
师傅领进门，修行在个人。尽管老师只是解析了其中一部分，但是咱们在平时使用开源软件和库的时候，也要多问个为什么。写完了程序，看看官方文档，看看原理解析的书，看看源代码，然后映射到算法与数据结构中，你会发现，这些知识和思路到处都在使用。
最后，我还想说一句，坚持，别放弃，啃下来。基础越扎实，路走得越远，走得越宽。加油！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>加餐1_创作故事：我是如何创作“趣谈网络协议”专栏的？</title><link>https://artisanbox.github.io/5/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/42/</guid><description>我用将近半年的时间在“极客时间”写了一个专栏“趣谈网络协议”。对于我自己来讲，这真的是个非常特殊而又难忘的经历。
很多人都很好奇，这个专栏究竟是怎么一步步创作出来的，每一篇文章是怎么写出来的？自己录音频又是什么样的感受？写完整个专栏之后，我终于有时间回顾、整理一下这半年的所感所想。对我来说，这是一次难得的体验，也是一次与“极客时间”的深度沟通。
专栏是写给谁的？和极客时间的编辑谈妥主题之后，他们首先要求我基于约定的主题，写一个36节至50节的大纲，之后会以每周三篇的频率，文字加音频的方式发布。每篇文章的体量要求在3000字左右，录成音频大约就是10分钟。
我本来觉得写这么一个专栏根本就不是个事儿。毕竟咱也是在IT圈摸爬滚打了许多年的“老司机”，干货积累得也不少。只要是熟悉的领域，不用准备，聊个把小时都没啥问题。况且我原来还写过书、写过博客、写过公众号。所以，我对自己文字方面的能力很有自信。
至于语言方面，咱常年出入各大技术论坛，什么场子没趟过。一个两天的线下培训，咱都能扛过来。每篇10分钟，总共36篇，那不才是6个小时嘛，肯定没问题。
但是，写了之后我发现，自己会是一回事儿，能讲给别人是另一回事儿，而能讲给“看不见的陌生人”听，是这世上最难的事儿。
我知道，很多技术人都有这样一个“毛病”，就是觉得掌握技术本身是最重要的，其他什么产品、市场、销售，都没技术含量。这种思维导致很多技术比较牛的人会以自我为中心，仅站在自己的角度思考问题。所以，常常是自己讲得很爽，完全不管听的人是不是真的接受了。写专栏的时候，这绝对是个大忌。
除此之外，这种思维对职业发展的影响也是很大的。单打独斗，一个人搞定一个软件的时代已经过去了。学会和别人合作，才是现代社会的生存法则，而良好的合作源于沟通。
但沟通不易，高质量的沟通更难。面对的人越多，沟通的难度就越大。因为每个人的背景、知识、基础都不同，想听的内容肯定更是千差万别。况且不是每个人都能准确地表达出自己的需求，加之需求的表达、转述都会因表达方式和传递媒介而发生变形，这样一来，接收信息的一方自然很难把握真实的需求。
写专栏的时候，“极客时间”的编辑不断地告诉我，我的受众只有一个人，就是“你”。我心想，这个简单啊，因为面对的人最少嘛！可是，事实上证明，我又“错”了。
这个抽象的“你”，看起来只有一个，其实却是看不到、摸不着的许许多多的人。所以，这个其实是最难的。协议专栏上线10天，就有10000多人订阅，而订阅专栏的用户里，只有少数人会留言。所以，对于很多读者的真实情况，我都无从得知，你可能每天都听但是没有留言的习惯，也可能买了之后觉得我讲得不好，骂一句“这钱白花了”，然后再也不听。
所以，如何把控内容，写给广大未知受众，是我写这个专栏面临的最大挑战。而这里面，文章的深度、广度，音频的语调、语气，每一个细节都非常重要。
专栏文章是怎么写的？经过大纲和前几篇文稿的打磨，我对“极客时间”和专栏创作也有了更深的了解。我私下和很多人交流过一个问题，那就是，咱们平时聊一个话题的时候，有很多话可以说。但是真正去写一篇文章的时候，好像又没有什么可讲的，尤其是那些看起来很基础的内容。
我在写专栏的过程中，仔细思考过这样一个问题：很多人对某一领域或者行业研究得很深入，也有自己长期的实践，但是有多少人可以从感性认识上升到理性认知的高度呢？
现在技术变化这么快，我们每个人的精力都是有限的，不少人学习新知识的方式就是看看书，看看博客、技术文章，或者听同事讲一下，了解个大概就觉得可以直接上手去做了。我也是这样的。可是一旦到写专栏的时候，基础掌握不扎实的问题一下子全都“暴露”出来了。
落到文字上的东西一定要是严谨的。所以，在写到很多细节的时候，我查了大量的资料，找到权威的书籍、官方文档、RFC里面的具体描述，有时候我甚至要做个实验，或者打开代码再看一下，才放心下笔。
尽管我对自己写文章有很多“完美倾向”的要求，但是这其实依旧是站在我自己的角度去看的。读者究竟想要看什么内容呢？
太深入了，看不懂；太浅显了，也不行。太长了，负担太重；太短了，没有干货；同时，每篇文字还要自成一体，所有文章要是一个完整的知识体系。我发现，原来我不仅是对知识的了解没那么全面、具体，对用户阅读和倾听场景也没有过多的考虑。
除了写文字，专栏还要录音频，所以为了方便“听”，文章内不能放大量代码、实验。如果很多人在通勤路上听，而我把一张图片讲得天花乱坠，听的人却根本看不到，那肯定是不行的，所以写文章的时候，我还要把故事性、画面感都考虑进去，尽量详尽而不啰嗦。
把这些限制条件加起来之后，我发现，写专栏这件事儿，真的太不容易了。每篇文章看起来内容不多，但是都是费了很多心思的，这也是为什么很多老师说，写完专栏就像是过了火焰山。
专栏音频是怎么录的？说完写文章，我来说说录音频。我平时听播音员说话，感觉非常轻松，所以当时我毫不犹豫地就说，“我要自己录”。但是在录开篇词的时候，我就觉得这完全不是我想的那么回事啊！
专栏的文章在录音的时候一定会有个“音频稿”，我一开始很不理解，我对着发布的稿件直接讲就好了啊，为什么还要特意准备一个供录音频的稿件啊？
我在没有音频稿的情况下，自己试着“发挥”了几次，结果，我发现我的嘴会“吃”字，会反复讲一个内容而且表达不清，但是自己却经常毫无察觉，还会自己讲着讲着就收不住等等。
咱们平时说话的时候，会有很多口头语和重复的词语。面对面交流的时候，我们为什么没有注意这个问题呢？因为我们会更注重对方的表情、手势，但是一旦录成音频，这些“啰嗦”的地方就特别明显。
而有了音频稿之后，整个过程就严谨很多。如果哪句话说错了，看着稿件再说一遍就好了。而且，你会发现录音的时间大大缩短了，原来需要用十分钟，现在五分钟就可以很精炼地讲出来了。
有了稿子，那我是不是对着念就好了？这不是很容易吗？不，我又遇到了新的难题。
录音频的时候，我常常一个人关在密闭的房间里，对着显示器“读”，这和公共演讲肯定是不一样的。加上因为有写好的音频稿，我常常感觉束手束脚，找不到演讲那种有激情的感觉，很容易就变成了念课文。
为了同时满足自然和严谨，一方面我会先熟记“台词”；另一方面，每次录的时候，我都假想对面有个人，我在对着他缓缓地讲出来。讲到某些地方，我还会假想他对这个知识点是不是有疑问，这样就更加有互动感。
录音频这件事对我的改变非常大。我说话、演讲的时候变得更加严谨了。我会下意识地不去重复已经说过的话。一旦想重复，也闭嘴不发音，等想好了下一句再说。后面，我的录音也越来越顺利，一开始要录五六遍才能成功，后面基本一遍就过了。
创作专栏的过程还有许多事情，都是我很难得的记忆。我很佩服“极客时间”的编辑做专栏时的专业和认真。我也很庆幸，我没有固执地按照自己认为正确的方向和方式来做，而是尊重了他们的专业。很显然，他们没有我懂技术，但是他们比我更懂“你”。
专栏结束后，我回看这半年的准备和努力，我发现，无论对自己的领域多么熟悉，写这个专栏都让我又上升了一个新高度。
我知道很多技术人都喜欢分享，而写文章又是最容易实现的方式。写文章的时候，可以检验你对基础知识的掌握是否扎实，是不是有换位思考能力，能不能从感性认识上升到理性认知。
除此之外，我觉得最重要的一点是，在创作专栏文章的过程中，我学到了很多技术之外的东西，比如换位思考能力和细节把控的能力。
我在这里记下与“极客时间”的相识和相知。希望看到更多人在极客时间，分享自己的知识和见解。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>加餐2_“趣谈网络协议”专栏食用指南</title><link>https://artisanbox.github.io/5/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/43/</guid><description>你好，我是刘超。
“趣谈网络协议”专栏现在已经全部更新完毕。这里有一份「食用指南」，可以帮你找到学习本专栏的最佳姿势。
在这份指南中，我为你整理了专栏的所有学习资料，并告诉你如何更高效地使用这些资料，从而帮助你消化吸收，以期获得更好的学习效果。
不管你是刚刚打开这个专栏，还是进入温故的阶段，我的这份指南，都可以帮你更上一个台阶。一起加油吧！
1. 能力测试我从常用的网络协议中，精心筛选了核心知识点，编成了10道题。这里面的题目和答案都是我精心设计的。希望你一定要先拿出纸笔，认真思考，记录下自己的答案，之后再和文末的详细解析进行对照。
刚刚打开这个专栏的你，可以据此寻找自己的薄弱点，对症下药；已经学习了一段时间的你，可以检测学习成果，查漏补缺。
点击查看：网络协议能力测试题
2. 答疑解惑每篇文章后，我都会留两个思考题，其中第一个问题意在启发你的思考，是对本节内容的延伸学习；第二个问题是为引出下一节，下一节的内容其实就是答案（所以我就不单独解答啦）。
我希望你能够好好地利用这些思考题，毕竟所有的“知”，只有经过了自己的思考之后，才能成为“识”。
如果你是刚刚加入学习，你可以继续在思考题后的“留言区”写下你的答案，学习过程中遇到的问题和思考也欢迎多多分享，我依然会在这里回复你的留言，和你一起讨论。
我知道你肯定也很好奇我对这些问题的思考是怎样的，因此，我针对每一节课后的第一道思考题及留言区比较有代表性的、有深度的问题，特意写了一系列答疑文章。
我再强调一遍，对于这一系列的答疑文章，你一定要在自己进行深度思考之后，再来看文章对比答案，这样可以更有效地拓展你的知识边界。
点击查看：
第一期：第1讲至第2讲答疑解惑合辑
第二期：第3讲至第6讲答疑解惑合辑
第三期：第7讲至第13讲答疑解惑合辑
第四期：第14讲至第21讲答疑解惑合辑
第五期：第22讲至第36讲答疑解惑合辑
3. 知识串讲在学习完前面36讲的内容之后，我详细讲解了一个“下单”的过程。我把这个过程分为十个阶段，从云平台中搭建一个电商开始，到BGP路由广播，再到DNS域名解析；从客户看商品图片，到最终下单的整个过程，每个步骤我都画了详细的分解图。
你可以用这个过程，串起我们讲过的所有网络协议，还原真实的使用场景，学以致用。我相信，学完前面的详细内容之后，再来看这个串讲内容，你对网络协议一定会有一个全面、深入的把握。
点击查看：
知识串讲（上篇）
知识串讲（中篇）
知识串讲（下篇）
4. 知识图谱专栏中最精华的内容，我都整理在这张图上了。
点击查看：网络协议知识图谱
5. 实验环境纸上得来终觉浅。网络是一门实验性很强的学科，我在写专栏的过程中也深深体会到了。有时候，遇到疑问，我常常会拿一个现实的环境，上手操作一下，抓个包看看，这样心里就会有定论。
网络方面最权威的书籍《TCP/IP详解》（TCP/IP illustrated）的作者斯蒂文森（W. Richard Stevens），也是经过无数次实验，才完成这本巨著。
因此，我在这本书中的实验基础上，带你搭建一个实验环境，希望你能够上手操作一下学过的知识。毕竟，只有经过你自己动手和思考产生的内容，才是真正属于你的知识。
点击查看我搭建实验环境时候的具体操作，希望给你的思维晋升指路：《搭建一个网络实验环境：授人以鱼不如授人以渔》
6. 专栏音频我在这里想特别提一下专栏音频。我的每篇专栏文章都包含了很多图片，为了帮助你更好地理解文章内容，我在录音的时候，常常会对图片做一些补充解释和说明，所以音频和文字稿并非完全一一对应。不知道你具体的学习习惯是怎样的，我建议你除了阅读文字以外，一定要听一下音频，可以利用“倍速播放”的功能，还可以自由把控播放速度，更高效地学习。
7. 记录，高效；分享，快乐我们专栏还有不少功能，提醒你好好利用起来，成为高效的学习者。
比如，在学习的过程中，遇到自己不懂的地方，或者是有深刻感受的地方，一定要及时利用“划线笔记”的功能，记录下自己当时的想法。这样在过程中点滴积累，等学完后，还可以回过头来再过一遍。如果有可能，你可以把自己的这些思考梳理成文。相信我，这样做，你的提升速度会快到让自己意外。
再比如“请朋友读”功能。如果你觉得某篇内容对自己很有帮助，不妨把它推荐给身边有同样需求的朋友，这一个动作或许就能帮他解决一个手边的问题。最重要的是，通过这些分享，你会找到那些和你一样热爱学习的伙伴，一起学习更快乐。
最后，还有一个小小的彩蛋。我把自己这半年写专栏的经历，写成了一篇文章。我是如何写专栏中每一篇文章的？每一篇音频又是如何录出来的？创作专栏给我带来了哪些改变？带你走进“极客时间万人专栏”背后的创作故事。
点击查看：我是如何创作“趣谈网络协议”专栏的？
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>协议专栏特别福利_答疑解惑第一期</title><link>https://artisanbox.github.io/5/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/51/</guid><description>你好，我是刘超。
首先，感谢大家关注并在留言区写下近3000条留言。留言太多，没有及时回复，一是每周写三篇文章压力真的挺大的。为了保质保量地产出，晚上和周末的时间基本上都搭进去了。二是很多人的留言非常有深度，水平很高，提的问题一两句话解释不清楚。
每一节结尾我基本都会留两个思考题，其中第一个问题是启发思考的，是对本节内容的延伸学习；第二个问题是为了引出下一节，下一节的内容其实就是答案。
所以我会回答一下每一节的第一个问题，并列出第一个同我的思路最相近的同学，并对留言中比较有代表性的问题，做一个统一的回答，顺便也实现之前要送知识图谱和奖励礼券的承诺。
当然，这并不能说明我的回答就是一定是正确的或者全面的，有很多同学的留言有非常大的信息量，甚至更广的思路，也对这些同学表示感谢。还有些同学指出了我的错误，也感谢你们。
《第1讲 | 为什么要学习网络协议？》课后思考题当网络包到达一个城关的时候，可以通过路由表得到下一个城关的 IP 地址，直接通过 IP地址找就可以了，为什么还要通过本地的MAC地址呢？
徐良红同学说的比较接近。在网络包里，有源IP地址和目标IP地址、源MAC地址和目标MAC地址。从路由表中取得下一跳的IP地址后，应该把这个地址放在哪里呢？如果放在目标IP地址里面，到了城关，谁知道最终的目标在哪里呢？所以要用MAC地址。
所谓的下一跳，看起来是IP地址，其实是要通过ARP得到MAC地址，将下一跳的MAC地址放在目标MAC地址里面。
留言问题1.MAC地址可以修改吗？
我查了一下，MAC（Media Access Control，介质访问控制）地址，也叫硬件地址，长度是48比特（6字节），由16进制的数字组成，分为前24位和后24位。
前24位叫作组织唯一标志符（Organizationally Unique Identifier，OUI），是由IEEE的注册管理机构给不同厂家分配的代码，用于区分不同的厂家。后24位是厂家自己分配的，称为扩展标识符。同一个厂家生产的网卡中MAC地址后24位是不同的。
也就是说，MAC本来设计为唯一性的，但是后来设备越来越多，而且还有虚拟化的设备和网卡，有很多工具可以修改，就很难保证不冲突了。但是至少应该保持一个局域网内是唯一的。
MAC的设计，使得即便不能保证绝对唯一，但是能保证一个局域网内出现冲突的概率很小。这样，一台机器启动的时候，就能够在没有IP地址的情况下，先用MAC地址进行通信，获得IP地址。
好在MAC地址是工作在一个局域网中的，因而即便出现了冲突，网络工程师也能够在自己的范围内很快定位并解决这个问题。这就像我们生成UUID或者哈希值，大部分情况下是不会冲突的，但是如果碰巧出现冲突了，采取一定的机制解决冲突就好。
2.TCP重试有没有可能导致重复下单？
答案是不会的。这个在TCP那一节有详细的讲解。因为TCP层收到了重复包之后，TCP层自己会进行去重，发给应用层、HTTP层。还是一个唯一的下单请求，所以不会重复下单。
那什么时候会导致重复下单呢？因为网络原因或者服务端错误，导致TCP连接断了，这样会重新发送应用层的请求，也即HTTP的请求会重新发送一遍。
如果服务端设计的是无状态的，它记不住上一次已经发送了一次请求。如果处理不好，就会导致重复下单，这就需要服务端除了实现无状态，还需要根据传过来的订单号实现幂等，同一个订单只处理一次。
还会有的现象是请求被黑客拦截，发送多次，这在HTTPS层可以有很多种机制，例如通过 Timestamp和Nonce随机数联合起来，然后做一个不可逆的签名来保证。
3.TCP报平安的包是原路返回吗？
谢谢语鬼同学的指正。这里的比喻不够严谨，容易让读者产生误会，这里的原路返回的意思是原样返回，也就是返回也是这个过程，不一定是完全一样的路径。
4.IP地址和MAC地址的关系？
芒果同学的理解非常准确，讲IP和MAC的关系的时候说了这个问题。IP是有远程定位功能的，MAC是没有远程定位功能的，只能通过本地ARP的方式找到。
我个人认为，即便有了IPv6，也不会改变当前的网络分层模式，还是IP层解决远程定位问题，只不过改成IPv6了，到了本地，还是通过MAC。
5.如果最后一跳的时候，IP改变了怎么办？
对于IP层来讲，当包到达最后一跳的时候，原来的IP不存在了。比如网线拔掉了，或者服务器直接宕机了，则ARP就找不到了，所以这个包就会发送失败了。对于IP层的工作就结束了。
但是IP层之上还有TCP层，TCP会重试的，包还是会重新发送，但是如果服务器没有启动起来，超过一定的次数，最终放弃。
如果服务器重启了，IP还是原来的IP地址，这个时候TCP重新发送的一个包的时候，ARP是能够得到这个地址的，因而会发到这台机器上来，但是机器上面没有启动服务端监听那个端口，于是会发送ICMP端口不可达。
如果服务器重启了，服务端也重新启动了，也在监听那个端口了，这个时候TCP的服务端由于是新的，Sequence Number根本对不上，说明不是原来的连接，会发送RST。
那有没有可能有特殊的场景Sequence Number也能对的上呢？按照Sequence Number的生成算法，是不可能的。
但是有一个非常特殊的方式，就是虚拟机的热迁移，从一台物理机迁移到另外一台物理机，IP不变，MAC不变，内存也拷贝过去，Sequence Number在内存里面也保持住了，在迁移的过程中会丢失一两个包，但是从TCP来看，最终还是能够连接成功的。
6.TCP层报平安，怎么确认浏览器收到呢？
TCP报平安，只能保证TCP层能够收到，不保证浏览器能够收到。但是可以想象，如果浏览器是你写的一个程序，你也是通过socket编程写的，你是通过socket，建立一个TCP的连接，然后从这个连接里面读取数据，读取的数据就是TCP层确认收到的。
这个读取的动作是本地系统调用，大部分情况下不会失败的。如果读取失败呢，当然本地会报错，你的socket读取函数会返回错误，如果你是浏览器程序的实现者，你有两种选择，一个是将错误报告给用户，另一个是重新发送一次请求，获取结果显示给用户。
7.ARP协议属于哪一层？
ARP属于哪个层，一直是有争议的。比如《TCP/IP详解》把它放在了二层和三层之间，但是既然是协议，只要大家都遵守相同的格式、流程就可以了，在实际应用的时候，不会有歧义的，唯一有歧义的是参加各种考试，让你做选择题，ARP属于哪一层？平时工作中咱不用纠结这个。
《第2讲 | 网络分层的真实含义是什么？》课后思考题如果你也觉得总经理和员工的比喻不恰当，你有更恰当的比喻吗？
我觉得，寄快递和寄信这两个比喻都挺好的。关键是有了封装和解封装的过程。有的同学举了爬楼，或者公司各层之间的沟通，都无法体现封装和解封装的过程。
留言问题1.为什么要分层？
是的，仅仅用复杂性来解释分层，太过牵强了。
其实这是一个架构设计的通用问题，不仅仅是网络协议的问题。一旦涉及到复杂的逻辑，或者软件需求需要经常变动，一般都会通过分层来解决问题。
假如我们将所有的代码都写在一起，但是产品经理突然想调整一下界面，这背后的业务逻辑变不变，那要不要一起修改呢？所以会拆成两层，把UI层从业务逻辑中分离出来，调用API来进行组合。API不变，仅仅界面变，是不是就不影响后台的代码了？
为什么要把一些原子的API放在基础服务层呢？将数据库、缓存、搜索引擎等，屏蔽到基础服务层以下，基础服务层之上的组合逻辑层、API层都只能调用基础服务层的API，不能直接访问数据库。
比如我们要将Oracle切换成MySQL。MySQL有一个库，分库分表成为4个库。难道所有的代码都要修改吗？当然只要把基础服务层屏蔽，提供一致的接口就可以了。
网络协议也是这样的。有的想基于TCP，自己不操心就能够保证到达；有的想自己实现可靠通信，不基于TCP，而使用UDP。一旦分了层就好办了，定制化后要依赖于下一层的接口，只要实现自己的逻辑就可以了。如果TCP的实现将所有的逻辑耦合在了整个七层，不用TCP的可靠传输机制都没有办法。
2.层级之间真实的调用方式是什么样的？
如果文中是一个逻辑图，这个问题其实已经到实现层面上来了，需要看TCP/IP的协议栈代码了。这里首先推荐一本书《深入理解Linux网络技术内幕》。
其实下层的协议知道上层协议的，因为在每一层的包头里面，都会有上一层是哪个协议的标识，所以不是一个回调函数，每一层的处理函数都会在操作系统启动的时候，注册到内核的一个数据结构里面，但是到某一层的时候，是通过判断到底是哪一层的哪一个协议，然后去找相应的处理函数去调用。
调用的大致过程我这里再讲一下。由于TCP比较复杂，我们以UDP为例子，其实发送的包就是一个sk_buff结构。这个在Socket那一节讲过。
int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4) 接着，UDP层会调用IP层的函数。</description></item><item><title>协议专栏特别福利_答疑解惑第三期</title><link>https://artisanbox.github.io/5/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/48/</guid><description>你好，我是刘超。
第三期答疑涵盖第7讲至第13讲的内容。我依旧对课后思考题和留言中比较有代表性的问题作出回答。你可以点击文章名，回到对应的章节复习，也可以继续在留言区写下你的疑问，我会持续不断地解答。希望对你有帮助。
《第7讲 | ICMP与ping：投石问路的侦察兵》课后思考题当发送的报文出问题的时候，会发送一个ICMP的差错报文来报告错误，但是如果 ICMP 的差错报文也出问题了呢？
我总结了一下，不会导致产生ICMP差错报文的有：
ICMP差错报文（ICMP查询报文可能会产生ICMP差错报文）；
目的地址是广播地址或多播地址的IP数据报；
作为链路层广播的数据报；
不是IP分片的第一片；
源地址不是单个主机的数据报。这就是说，源地址不能为零地址、环回地址、广播地址或多播地址。
留言问题1.ping使用的是什么网络编程接口？
咱们使用的网络编程接口是Socket，对于ping来讲，使用的是ICMP，创建Socket如下：
socket(AF_INET, SOCK_RAW, IPPROTO_ICMP) SOCK_RAW就是基于IP层协议建立通信机制。
如果是TCP，则建立下面的Socket：
socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) 如果是UDP，则建立下面的Socket：
socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP) 2.ICMP差错报文是谁发送的呢？
我看留言里有很多人对这个问题有疑惑。ICMP包是由内核返回的，在内核中，有一个函数用于发送ICMP的包。
void icmp_send(struct sk_buff *skb_in, int type, int code, __be32 info); 例如，目标不可达，会调用下面的函数。
icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PROT_UNREACH, 0); 当IP大小超过MTU的时候，发送需要分片的ICMP。
if (ip_exceeds_mtu(skb, mtu)) { icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu)); goto drop; } 《第8讲 | 世界这么大，我想出网关：欧洲十国游与玄奘西行》课后思考题当在你家里要访问 163 网站的时候，你的包需要 NAT 成为公网 IP，返回的包又要 NAT 成你的私有 IP，返回包怎么知道这是你的请求呢？它怎么能这么智能地 NAT 成了你的 IP 而非别人的 IP 呢？</description></item><item><title>协议专栏特别福利_答疑解惑第二期</title><link>https://artisanbox.github.io/5/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/47/</guid><description>你好，我是刘超。
第二期答疑涵盖第3讲至第6讲的内容。我依旧对课后思考题和留言中比较有代表性的问题作出回答。你可以点击文章名，回到对应的章节复习，也可以继续在留言区写下你的疑问，我会持续不断地解答。希望对你有帮助。
《第3讲 | ifconfig：最熟悉又陌生的命令行》课后思考题你知道 net-tools 和 iproute2 的“历史”故事吗？
这个问题的答案，盖同学已经写的比较全面了。具体的对比，我这里推荐一篇文章https://linoxide.com/linux-command/use-ip-command-linux/，感兴趣的话可以看看。
留言问题1.A、B、C类地址的有效地址范围是多少？
我在写的时候，没有考虑这么严谨，平时使用地址的时候，也是看个大概的范围。所以这里再回答一下。
A类IP的地址第一个字段范围是0～127，但是由于全0和全1的地址用作特殊用途，实际可指派的范围是1～126。所以我仔细查了一下，如果较真的话，你在答考试题的时候可以说，A类地址范围和A类有效地址范围。
2.网络号、IP地址、子网掩码和广播地址的先后关系是什么？
当在一个数据中心或者一个办公室规划一个网络的时候，首先是网络管理员规划网段，一般是根据将来要容纳的机器数量来规划，一旦定了，以后就不好变了。
假如你在一个小公司里，总共就没几台机器，对于私有地址，一般选择192.168.0.0/24就可以了。
这个时候先有的是网络号。192.168.0就是网络号。有了网络号，子网掩码同时也就有了，就是前面都是网络号的是1，其他的是0，广播地址也有了，除了网络号之外都是1。
当规划完网络的时候，一般这个网络里面的第一个、第二个地址被默认网关DHCP服务器占用，你自己创建的机器，只要和其他的不冲突就可以了，当然你也可以让DHCP服务自动配置。
规划网络原来都是网络管理员的事情。有了公有云之后，一般有个概念虚拟网络（VPC），鼠标一点就能创建一个网络，网络完全软件化了，任何人都可以做网络规划。
3.组播和广播的意义和原理是什么？
C类地址的主机号8位，去掉0和255，就只有254个了。
在《TCP/IP详解》这本书里面，有两章讲了广播、多播以及IGMP。广播和组播分为两个层面，其中MAC层有广播和组播对应的地址，IP层也有自己的广播地址和组播地址。
广播相对比较简单，MAC层的广播为ff:ff:ff:ff:ff:ff，IP层指向子网的广播地址为主机号为全1且有特定子网号的地址。
组播复杂一些，MAC层中，当地址中最高字节的最低位设置为1时，表示该地址是一个组播地址，用十六进制可表示为01:00:00:00:00:00。IP层中，组播地址为D类IP地址，当IP地址为组播地址的时候，有一个算法可以计算出对应的MAC层地址。
多播进程将目的IP地址指明为多播地址，设备驱动程序将它转换为相应的以太网地址，然后把数据发送出去。这些接收进程必须通知它们的IP层，它们想接收的发给定多播地址的数据报，并且设备驱动程序必须能够接收这些多播帧。这个过程就是“加入一个多播组”。
当多播跨越路由器的时候，需要通过IGMP协议告诉多播路由器，多播数据包应该如何转发。
4.MTU 1500的具体含义是什么？
MTU（Maximum Transmission Unit，最大传输单元）是二层的一个定义。以以太网为例，MTU为1500个Byte，前面有6个Byte的目标MAC地址，6个Byte的源MAC地址，2个Byte的类型，后面有4个Byte的CRC校验，共1518个Byte。
在IP层，一个IP数据报在以太网中传输，如果它的长度大于该MTU值，就要进行分片传输。如果不允许分片DF，就会发送ICMP包，这个在ICMP那一节讲过。
在TCP层有个MSS（Maximum Segment Size，最大分段大小），它等于MTU减去IP头，再减去TCP头。即在不分片的情况下，TCP里面放的最大内容。
在HTTP层看来，它的body没有限制，而且在应用层看来，下层的TCP是一个流，可以一直发送，但其实是会被分成一个个段的。
《第4讲 | DHCP与PXE：IP是怎么来的，又是怎么没的》课后思考题PXE 协议可以用来安装操作系统，但是如果每次重启都安装操作系统，就会很麻烦。你知道如何使得第一次安装操作系统，后面就正常启动吗？
一般如果咱们手动安装一台电脑的时候，都是有启动顺序的，如果改为硬盘启动，就没有问题了。
好在服务器一般都提供IPMI接口，可以通过这个接口启动、重启、设置启动模式等等远程访问，这样就可以批量管理一大批机器。
这里提到Cobbler，这是一个批量安装操作系统的工具。在OpenStack里面，还有一个Ironic，也是用来管理裸机的。有兴趣的话可以研究一下。
留言问题1.在DHCP网络里面，手动配置IP地址会冲突吗?
在一个DHCP网络里面，如果某一台机器手动配置了一个IP地址，并且在DHCP管理的网段里的话，DHCP服务器是会将这个地址分配给其他机器的。一旦分配了，ARP的时候，就会收到两个应答，IP地址就冲突了。
当发生这种情况的时候，应该怎么办呢？DHCP的过程虽然没有明确如何处理，但是DHCP的客户端和服务器都可以添加相应的机制来检测冲突。
如果由客户端来检测冲突，一般情况是，客户端在接受分配的IP之前，先发送一个ARP，看是否有应答，有就说明冲突了，于是发送一个DHCPDECLINE，放弃这个IP地址。
如果由服务器来检测冲突，DHCP服务器会发送ping，来看某个IP是否已经被使用。如果被使用了，它就不再将这个IP分配给其他的客户端了。
2.DHCP的Offer和ACK应该是单播还是广播呢？
没心没肺 回答得很正确。
这个我们来看DHCP的RFC，我截了个图放在这儿：
这里面说了几个问题。
正常情况下，一旦有了IP地址，DHCP Server还是希望通过单播的方式发送OFFER和ACK。但是不幸的是，有的客户端协议栈的实现，如果还没有配置IP地址，就使用单播。协议栈是不接收这个包的，因为OFFER和ACK的时候，IP地址还没有配置到网卡上。
所以，一切取决于客户端的协议栈的能力，如果没配置好IP，就不能接收单播的包，那就将BROADCAST设为1，以广播的形式进行交互。
如果客户端的协议栈实现很厉害，即便是没有配置好IP，仍然能够接受单播的包，那就将BROADCAST位设置为0，就以单播的形式交互。
3.DHCP如何解决内网安全问题?
其实DHCP协议的设计是基于内网互信的基础来设计的，而且是基于UDP协议。但是这里面的确是有风险的。例如一个普通用户无意地或者恶意地安装一台DHCP服务器，发放一些错误或者冲突的配置；再如，有恶意的用户发出很多的DHCP请求，让DHCP服务器给他分配大量的IP。
对于第一种情况，DHCP服务器和二层网络都是由网管管理的，可以在交换机配置只有来自某个DHCP服务器的包才是可信的，其他全部丢弃。如果有SDN，或者在云中，非法的DHCP包根本就拦截到虚拟机或者物理机的出口。
对于第二种情况，一方面进行监控，对DHCP报文进行限速，并且异常的端口可以关闭，一方面还是SDN或者在云中，除了被SDN管控端登记过的IP和MAC地址，其他的地址是不允许出现在虚拟机和物理机出口的，也就无法模拟大量的客户端。
《第5讲 | 从物理层到MAC层：如何在宿舍里自己组网玩联机游戏？》课后思考题1.在二层中我们讲了 ARP 协议，即已知 IP 地址求 MAC；还有一种 RARP 协议，即已知 MAC 求 IP 的，你知道它可以用来干什么吗？</description></item><item><title>协议专栏特别福利_答疑解惑第五期</title><link>https://artisanbox.github.io/5/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/50/</guid><description>你好，我是刘超。
第五期答疑涵盖第22讲至第36讲的内容。我依旧对课后思考题和留言中比较有代表性的问题作出回答。你可以点击文章名，回到对应的章节复习，也可以继续在留言区写下你的疑问，我会持续不断地解答。希望对你有帮助。
《第22讲 | VPN：朝中有人好做官》 课后思考题 当前业务的高可用性和弹性伸缩很重要，所以很多机构都会在自建私有云之外，采购公有云，你知道私有云和公有云应该如何打通吗？
留言问题 DH算法会因为传输随机数被破解吗？
这位同学的笔记特别认真，让人感动。DH算法的交换材料要分公钥部分和私钥部分，公钥部分和其他非对称加密一样，都是可以传输的，所以对于安全性是没有影响的，而且传输材料远比传输原始的公钥更加安全。私钥部分是谁都不能给的，因此也是不会截获到的。
《第23讲 | 移动网络：去巴塞罗那，手机也上不了脸书》 课后思考题 咱们上网都有套餐，有交钱多的，有交钱少的，你知道移动网络是如何控制不同优先级的用户的上网流量的吗？
这个其实是PCRF协议进行控制的，它可以下发命令给PGW来控制上网的行为和特性。
《第24讲 | 云中网络：自己拿地成本高，购买公寓更灵活》 课后思考题 为了直观，这一节的内容我们以桌面虚拟化系统举例。在数据中心里面，有一款著名的开源软件OpenStack，这一节讲的网络连通方式对应OpenStack中的哪些模型呢？
OpenStack的早期网络模式有Flat、Flat DHCP、VLAN，后来才有了VPC，用VXLAN和GRE进行隔离。
《第25讲 | 软件定义网络：共享基础设施的小区物业管理办法》 课后思考题 在这一节中，提到了通过VIP可以通过流表在不同的机器之间实现负载均衡，你知道怎样才能做到吗？
可以通过ovs-ofctl下发流表规则，创建group，并把端口加入group中，所有发现某个地址的包在两个端口之间进行负载均衡。
sudo ovs-ofctl -O openflow11 add-group br-lb &amp;quot;group_id=100 type=select selection_method=dp_hash bucket=output:1 bucket=output:2&amp;quot; sudo ovs-ofctl -O openflow11 add-flow br-lb &amp;quot;table=0,ip,nw_dst=192.168.2.0/24,actions=group:100&amp;quot; 留言问题 SDN控制器是什么东西？
SDN控制器是一个独立的集群，主要是在管控面，因为要实现一定的高可用性。
主流的开源控制器有OpenContrail、OpenDaylight等。当然每个网络硬件厂商都有自己的控制器，而且可以实现自己的私有协议，进行更加细粒度的控制，所以江湖一直没有办法统一。
流表是在每一台宿主机上保存的，大小限制取决于内存，而集中存放的缺点就是下发会很慢。
《第26讲 | 云中的网络安全：虽然不是土豪，也需要基本安全和保障》 课后思考题 这一节中重点讲了iptables的filter和nat功能，iptables还可以通过QUEUE实现负载均衡，你知道怎么做吗？
我们可以在iptables里面添加下面的规则：
-A PREROUTING -p tcp -m set --match-set minuteman dst,dst -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j NFQUEUE --queue-balance 50:58 -A OUTPUT -p tcp -m set --match-set minuteman dst,dst -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j NFQUEUE --queue-balance 50:58 NFQUEUE的规则表示将把包的处理权交给用户态的一个进程。–queue-balance表示会将包发给几个queue。</description></item><item><title>协议专栏特别福利_答疑解惑第四期</title><link>https://artisanbox.github.io/5/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/49/</guid><description>你好，我是刘超。
第四期答疑涵盖第14讲至第21讲的内容。我依旧对课后思考题和留言中比较有代表性的问题作出回答。你可以点击文章名，回到对应的章节复习，也可以继续在留言区写下你的疑问，我会持续不断地解答。希望对你有帮助。
《第14讲 | HTTP协议：看个新闻原来这么麻烦》课后思考题QUIC是一个精巧的协议，所以它肯定不止今天我提到的四种机制，你知道还有哪些吗？
云学讲了一个QUIC的特性。
QUIC还有其他特性，一个是快速建立连接。这个我放在下面HTTPS的时候一起说。另一个是拥塞控制，QUIC协议当前默认使用了TCP协议的CUBIC（拥塞控制算法）。
你还记得TCP的拥塞控制算法吗？每当收到一个ACK的时候，就需要调整拥塞窗口的大小。但是这也造成了一个后果，那就是RTT比较小的，窗口增长快。
然而这并不符合当前网络的真实状况，因为当前的网络带宽比较大，但是由于遍布全球，RTT也比较长，因而基于RTT的窗口调整策略，不仅不公平，而且由于窗口增加慢，有时候带宽没满，数据就发送完了，因而巨大的带宽都浪费掉了。
CUBIC进行了不同的设计，它的窗口增长函数仅仅取决于连续两次拥塞事件的时间间隔值，窗口增长完全独立于网络的时延RTT。
CUBIC的窗口大小以及变化过程如图所示。
当出现丢包事件时，CUBIC会记录这时的拥塞窗口大小，把它作为Wmax。接着，CUBIC会通过某个因子执行拥塞窗口的乘法减小，然后，沿着立方函数进行窗口的恢复。
从图中可以看出，一开始恢复的速度是比较快的，后来便从快速恢复阶段进入拥塞避免阶段，也即当窗口接近Wmax的时候，增加速度变慢；立方函数在Wmax处达到稳定点，增长速度为零，之后，在平稳期慢慢增长，沿着立方函数的开始探索新的最大窗口。
留言问题HTTP的keepalive模式是什么样？
在没有keepalive模式下，每个HTTP请求都要建立一个TCP连接，并且使用一次之后就断开这个TCP连接。
使用keepalive之后，在一次TCP连接中可以持续发送多份数据而不会断开连接，可以减少TCP连接建立次数，减少TIME_WAIT状态连接。
然而，长时间的TCP连接容易导致系统资源无效占用，因而需要设置正确的keepalive timeout时间。当一个HTTP产生的TCP连接在传送完最后一个响应后，还需要等待keepalive timeout秒后，才能关闭这个连接。如果这个期间又有新的请求过来，可以复用TCP连接。
《第15讲 | HTTPS协议：点外卖的过程原来这么复杂》课后思考题HTTPS 协议比较复杂，沟通过程太繁复，这样会导致效率问题，那你知道有哪些手段可以解决这些问题吗？
通过HTTPS访问的确复杂，至少经历四个阶段：DNS查询、TCP连接建立、TLS连接建立，最后才是HTTP发送数据。我们可以一项一项来优化这个过程。
首先如果使用基于UDP的QUIC，可以省略掉TCP的三次握手。至于TLS的建立，如果按文章中基于TLS 1.2的，双方要交换key，经过两个来回，也即两个RTT，才能完成握手。但是咱们讲IPSec的时候，讲过通过共享密钥、DH算法进行握手的场景。
在TLS 1.3中，握手过程中移除了ServerKeyExchange和ClientKeyExchange，DH参数可以通过key_share进行传输。这样只要一个来回，就可以搞定RTT了。
对于QUIC来讲，也可以这样做。当客户端首次发起QUIC连接时，会发送一个client hello消息，服务器会回复一个消息，里面包括server config，类似于TLS1.3中的key_share交换。当客户端获取到server config以后，就可以直接计算出密钥，发送应用数据了。
留言问题1.HTTPS的双向认证流程是什么样的？
2.随机数和premaster的含义是什么？
《第16讲 | 流媒体协议：如何在直播里看到美女帅哥？》课后思考题你觉得基于 RTMP 的视频流传输的机制存在什么问题？如何进行优化？
Jason的回答很对。
Jealone的回答更加具体。
当前有基于自研UDP协议传输的，也有基于QUIC协议传输的。
留言问题RTMP建立连接的序列是什么样的？
的确，这个图我画错了，我重新画了一个。
不过文章中这部分的文字描述是没问题的。
客户端发送C0、C1、 C2，服务器发送S0、 S1、 S2。
首先，客户端发送C0表明自己的版本号，不必等对方的回复，然后发送C1表明自己的时间戳。
服务器只有在收到C0的时候，才能返回S0，表明自己的版本号。如果版本不匹配，可以断开连接。
服务器发送完S0后，也不用等什么，就直接发送自己的时间戳S1。客户端收到S1的时候，发一个知道了对方时间戳的ACK C2。同理服务器收到C1的时候，发一个知道了对方时间戳的ACK S2。
于是，握手完成。
《第17讲 | P2P协议：我下小电影，99%急死你》课后思考题除了这种去中心化分布式哈希的算法，你还能想到其他的应用场景吗？
留言问题99%卡住的原因是什么？
《第18讲 | DNS协议：网络世界的地址簿》课后思考题全局负载均衡使用过程中，常常遇到失灵的情况，你知道具体有哪些情况吗？对应应该怎么来解决呢？
留言问题如果权威DNS连不上，怎么办？
一般情况下，DNS是基于UDP协议的。在应用层设置一个超时器，如果UDP发出没有回应，则会进行重试。
DNS服务器一般也是高可用的，很少情况下会挂。即便挂了，也会很快切换，重试一般就会成功。
对于客户端来讲，为了DNS解析能够成功，也会配置多个DNS服务器，当一个不成功的时候，可以选择另一个来尝试。</description></item><item><title>开篇词_为什么你需要学习计算机组成原理？</title><link>https://artisanbox.github.io/4/59/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/59/</guid><description>你好，我是徐文浩，一个正在创业的工程师。目前主要是通过自然语言处理技术，为走向海外的中国企业提供英语的智能客服和社交网络营销服务。
2005年从上海交通大学计算机系毕业之后，我一直以写代码为生。如果从7岁第一次在少年宫写程序开始算起，到今天，我的码龄快有30岁了。这些年里，我在Trilogy Software写过各种大型企业软件；在MediaV这样的广告科技公司，从零开始搭建过支撑每天百亿流量的广告算法系统；2015年，我又加入了拼多多，参与重写拼多多的交易系统。
这么多年一直在开发软件，我深感软件这个行业变化太快了。语言上，十年前流行Java，这两年流行Go；框架上，前两年流行TensorFlow，最近又流行PyTorch。我逐渐发现，学习应用层的各种语言、框架，好比在练拳法招式，可以短期给予你回报，而深入学习“底层知识”，就是在练扎马步、核心肌肉力量，是在提升你自己的“根骨”和“资质”。
正所谓“练拳不练功，到老一场空”。如果越早去弄清楚计算机的底层原理，在你的知识体系中“储蓄”起这些知识，也就意味着你有越长的时间来收获学习知识的“利息”。虽然一开始可能不起眼，但是随着时间带来的复利效应，你的长线投资项目，就能让你在成长的过程中越走越快。
计算机底层知识的“第一课”如果找出各大学计算机系的培养计划，你会发现，它们都有差不多十来门核心课程。其中，“计算机组成原理”是入门和底层层面的第一课。
这是为什么呢？我们直接用肉眼来看，计算机是由CPU、内存、显示器这些设备组成的硬件，但是，计算机系的学生毕业之后，大部分却都是从事各种软件开发工作。显然，在硬件和软件之间需要一座桥梁，而“计算机组成原理”就扮演了这样一个角色，它既隔离了软件和硬件，也提供了让软件无需关心硬件，就能直接操作硬件的接口。
也就是说，你只需要对硬件有原理性的理解，就可以信赖硬件的可靠性，安安心心用高级语言来写程序。无论是写操作系统和编译器这样的硬核代码，还是写Web应用和手机App这样的应用层代码，你都可以做到心里有底。
除此之外，组成原理是计算机其他核心课程的一个“导引”。学习组成原理之后，向下，你可以学习数字电路相关的课程，向上，你可以学习编译原理、操作系统这些核心课程。如果想要深入理解，甚至设计一台自己的计算机，体系结构是必不可少的一门课，而组成原理是计算机体系结构的一个入门版本。
所以说，无论你想要学习计算机的哪一门核心课程，之前你都应该先学习一下“计算机组成原理”，这样无论是对计算机的硬件原理，还是软件架构，你对计算机方方面面的知识都会有一个全局的了解。
学习这门“第一课”的过程，会为你在整个软件开发领域中打开一扇扇窗和门，让你看到更加广阔的天地。比如说，明白了高级语言是如何对应着CPU能够处理的一条条指令，能为你打开编译原理这扇门；搞清楚程序是如何加载运行的，能够让你对操作系统有更深入的理解。
因此，学好计算机组成原理，会让你对整个软件开发领域的全貌有一个系统了解，也会给你带来更多的职业发展机会。像我自己的团队里，有个小伙伴开始是做算法应用开发的，因为有扎实的计算机基础知识，后来就转去开发TVM这样的深度学习编译器了，是不是很厉害？
理论和实践相结合说了这么多计算机组成原理的重要性，但到底该怎么学呢？接下来跟你分享我的心得。
我自己对计算机硬件的发展历史一直很感兴趣，所以，我读了市面上很多组成原理相关的资料。
互联网时代，我们从来不缺少资料。无论是Coursera上北京大学的《计算机组成》开放课程，还是图灵奖作者写的《计算机组成与设计：硬件/软件接口》，都珠玉在前，是非常优秀的学习资料。不过“买书如山倒，读书如抽丝”。从业这么多年，周围想要好好学一学组成原理的工程师不少，但是真的坚持下来学完、学好的却不多。大部分买来的书，都是前面100页已经发黄了，后面500页从来没有打开过；更有不少非科班出身的程序员，直接说“这些书根本看不懂”。
对这些问题，我都深有感触。从自己学习和工作的经验看，我找到了三个主要原因。
第一，广。组成原理中的概念非常多，每个概念的信息量也非常大。比如想要理解CPU中的算术逻辑单元（也就是ALU）是怎么实现加法的，需要牵涉到如何把整数表示成二进制，还需要了解这些表示背后的电路、逻辑门、CPU时钟、触发器等知识。
第二，深。组成原理中的很多概念，阐述开来就是计算机学科的另外一门核心课程。比如，计算机的指令是怎么从你写的C、Java这样的高级语言，变成计算机可以执行的机器码的？如果我们展开并深入讲解这个问题，就会变成《编译原理》这样一门核心课程。
第三，学不能致用。学东西是要拿来用的，但因为这门课本身的属性，很多人在学习时，常常沉溺于概念和理论中，无法和自己日常的开发工作联系起来，以此来解决工作中遇到的问题，所以，学习往往没有成就感，就很难有动力坚持下去。
考虑到这些，在这个专栏构思之初，我就给自己定了一个交付目标：我要把这些知识点和日常工作、生活以及整个计算机行业的发展史联系起来，教你真正看懂、学会、记住组成原理的核心内容，教你更多地从“为什么”这个角度，去理解这些知识点，而不是只是去记忆“是什么”。
对于这个专栏，具体我是这样设计的。
第一，我把组成原理里面的知识点，和我在应用开发和架构设计中遇到的实际案例，放到一起进行印证，通过代码和案例，让你消化理解。
比如，为什么Disruptor这个高性能队列框架里，要定义很多没有用的占位变量呢？其实这是为了确保我们唯一关心的参数，能够始终保留在CPU的高速缓存里面，而高速缓存比我们的内存要快百倍以上。
第二，我会尽可能地多举一些我们日常生活里面的例子，让你理解计算机的各个组件是怎么运作的。在真实的开发中，我们会遇到什么问题，这些问题产生的根源是什么。让你从知识到应用，最终又回到知识，让学习和实践之间形成一道闭环。
计算机组成中很多组件的设计，都不是凭空发明出来，它们中的很多都来自现实生活中的想法和比喻。而底层很多硬件设计和开发的思路，其实也和你进行软件架构的开发设计和思路是一样的。
比如说，在硬件上，我们是通过最基本的与、或、非、异或门这些最基础的门电路组合形成了强大的CPU。而在面向对象和设计模式里，我们也常常是通过定义基本的Command，然后组合来完成更复杂的功能；再比如说，CPU里面的冒险和分支预测的策略，就好像在接力赛跑里面后面几棒的选手早点起跑，如果交接棒没有问题，自然占了便宜，但是如果没能交接上，就会吃个大亏。
第三，在知识点和应用之外，我会多讲一些计算机硬件发展史上的成功和失败，让你明白很多设计的历史渊源，让你更容易记住“为什么”，更容易记住这些知识点。
比如说，奔腾4的失败，就是受限于超长流水线带来的散热和功耗问题，而移动时代ARM的崛起，则是因为Intel的芯片功耗太大，不足以在小小的手机里放下足够支撑1天的电池。计算机芯片的兴盛和衰亡，往往都是因为我们的计算机遇到了“功耗墙”这个散热和能耗上的挑战。而现代的云计算数据中心的设计到选址，也是围绕功耗和散热的。理解了这些成功和失败背后的原因，你自然记住了这些背后的知识点。
最后，在这三种帮助你理解“为什么”的方法之上，我会把整个的计算机组成原理通过指令、计算、CPU、存储系统和I/O串起来。通过一个程序的执行过程进行逐层分解，让你能对整个系统有一个全貌的了解。
我希望这个专栏，不仅能够让你学好计算机组成原理的知识，更能够成为引领你进入更多底层知识的大门，让你有动力、有方法、更深入地去进一步学习体系结构、操作系统、编译原理这样的课程，成为真正的“内家高手”。
“人生如逆旅，我亦是行人”。学习总不会是一件太轻松的事情，希望在这个专栏里，你能和我多交流，坚持练完这一手内功。
下面，你可以讲一讲，你对于计算机组成原理的认识是怎样的？在之前工作中，哪些地方用到了计算机组成原理相关的知识呢？欢迎写在留言区，我们一起交流。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>开篇词_从今天起，跨过“数据结构与算法”这道坎</title><link>https://artisanbox.github.io/2/77/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/77/</guid><description>你好，我是王争，毕业于西安交通大学计算机专业。现在回想起来，本科毕业的时候，我的编程水平其实是很差的。直到读研究生的时候，一个师兄给了我一本《算法导论》，说你可以看看，对你的编程会很有帮助。
没想到，从此我对算法的“迷恋”便一发不可收拾。之后，我如饥似渴地把图书馆里几乎所有数据结构和算法书籍都读了一遍。
我常常边读边练。没多久，我就发现，写代码的时候，我会不由自主考虑很多性能方面的问题。我写出时间复杂度高、空间复杂度高的垃圾代码越来越少了，算法能力提升了很多，编程能力也有了质的飞跃。得益于此，研究生毕业后，我直接进入Google，从事Google翻译相关的开发工作。
这是我自己学习数据结构与算法的经历，现在，你可以想想你的情况。
是不是从学校开始，你就觉得数据结构难学，然后一直没认真学？
工作中，一遇到数据结构这个坑，你又发自本能地迅速避让，因为你觉得自己不懂，所以也不想深究，反正看起来无关大局？
当你想换工作面试，或者研究某个开源项目源码，亦或者和团队讨论某个非框架层面的高可用难题的时候，你又发现，自己的基础跟不上别人的节奏？
如果你是这种情况，其实你并不孤独，这不是你一个人遇到的问题。工作十年间，我见过许多程序员。他们有着各种各样的背景，有很多既有潜力又非常努力，但始终无法在自己现有水平上更进一步。
在技术圈里，我们经常喜欢谈论高大上的架构，比如高可用、微服务、服务治理等等。鲜有人关注代码层面的编程能力，而愿意沉下心来，花几个月时间啃一啃计算机基础知识、认认真真夯实基础的人，简直就是凤毛麟角。
我认识一位原来腾讯T4的技术大牛。在区块链大潮之前，他在腾讯工作了10多年，长期负责手机QQ后台整体建设。他经历了手机QQ从诞生到亿级用户在线的整个过程。后来他去了微众银行，有一天老板让他去做区块链。他用了不到半年时间，就把区块链的整个技术脉络摸清楚了。 现在，他是微众银行的区块链负责人，微众科技创新产品部的老总。你说厉害不？你可以花半年时间就能精通一个新的领域吗？为什么他就可以做到？
我觉得这其中最重要的就是基础足够扎实。他曾经跟我说，像区块链、人工智能这些看似很新的技术，其实一点儿都不“新”。最初学编程的时候，他就把那些基础的知识都学透了。当面临行业变动、新技术更迭的时候，他不断发现，那些所谓的新技术，核心和本质的东西其实就是当初学的那些知识。掌握了这个“规律”之后，他学任何东西都很快，任何新技术都能快速迎头赶上。这就是他快速学习并且获得成功的秘诀。
所以说，基础知识就像是一座大楼的地基，它决定了我们的技术高度。而要想快速做出点事情，前提条件一定是基础能力过硬，“内功”要到位。
那技术人究竟都需要修炼哪些“内功”呢？我觉得，无外乎就是大学里的那些基础课程，操作系统、计算机网络、编译原理等等，当然还有数据结构和算法。
可是，我们都知道，像《算法导论》这些经典书籍，虽然很全面，但是过于理论，学起来非常枯燥；而市面很多课程大多缺失真实的开发场景，费劲学完感觉好像还是用不上，过不了几天就忘了。
所以，我尝试做一个让你能真正受用的数据结构与算法课程，希望给你指明一个简洁、高效的学习路径，教你一个学习基础知识的通用方法 。那么，关于专栏内容，我是怎样设计的呢？
我根据自己研读数十本算法书籍和多年项目开发的经验，在众多的数据结构和算法中，精选了最实用的内容进行讲解。
我不只会教你怎么用，还会告诉你，我们为什么需要这种数据结构和算法，一点点帮你捋清它们背后的设计思想，培养你举一反三的能力。
对于每种数据结构和算法，我都会结合真实的软件开发案例来讲解，让你知道，数据结构和算法，究竟应该如何应用到实际的编码中。
为了由浅入深地带你学习，我把专栏分成四个递进的模块。
入门篇 时间、空间复杂度分析是数据结构和算法中非常重要的知识点，贯穿整个专栏的学习过程。但同时也是比较难掌握的，所以我用了2节课来讲这部分内容，而且还举了大量的实例，让你一边学一边练，真正能掌握复杂度分析，为后面的学习铺路。
我希望通过这一模块，你能掌握时间、空间复杂度的概念，大O表示法的由来，各种复杂度分析技巧，以及最好、最坏、平均、均摊复杂度分析方法。之后，面对任何代码的复杂度分析，你都能游刃有余、毫不畏惧！
基础篇 这部分是专栏中篇幅最大的内容，也是我们学习的重点，共有26节内容，涵盖了最基础、最常用的数据结构和算法。针对每种数据结构和算法，我都会结合具体的软件开发实例，由浅入深进行讲解，并适时总结一些实用“宝典”，保证你印象深刻、学有所用。
比如递归这一节，我会讲到，为什么递归代码比较难写？如何避免堆栈溢出？如何避免递归冗余计算？如何将递归代码转化为非递归代码？
高级篇 这部分我会讲一些不是那么常用的数据结构和算法。虽然不常用，但是这些内容你也需要知道。设置这一部分的目的，是为了让你开拓视野，强化训练算法思维、逻辑思维。如果说学完基础部分可以考80分，那掌握这一部分就能让你成为尖子生！
实战篇 我们整个专栏都是围绕数据结构和算法在具体软件实践中的应用来讲的，所以最后我会通过实战部分串讲一下前面讲到的数据结构和算法。我会拿一些开源项目、框架或者系统设计问题，剖析它们背后的数据结构和算法，让你有一个更加直观的感受。
人生路上，我们会遇到很多的坎。跨过去，你就可以成长，跨不过去就是困难和停滞。而在后面很长的一段时间里，你都需要为这个困难买单。对于我们技术人来说，更是这样。既然数据结构和算法这个坎，我们总归是要跨过去，为什么不是现在呢？
我很感激师兄当年给我的那本《算法导论》，这是我人生中为数不多的转折点之一。没有那本书，也可能就没有今天的我。我希望这个专栏也能成为你的一个人生转折点。
我希望，通过这个专栏，不仅能帮你跨过数据结构与算法这个坎，还能帮你掌握一种学习知识和技能的方法，帮你度过职场甚至人生的重要时刻！一起加油吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>开篇词_想成为技术牛人？先搞定网络协议！</title><link>https://artisanbox.github.io/5/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/46/</guid><description>你好，我是刘超，网易研究院云计算技术部的首席架构师。我主要负责两部分工作，对内支撑网易核心业务上云，对外帮助客户搞定容器化与微服务化架构。
当极客时间约我做“趣谈网络协议”专栏的时候，我非常开心，因为网络协议也是我长期研究和关注的点。摸爬滚打15年，有了一些收获也溅了一身血，我才能在这里和你分享。
为什么网络协议这么重要呢？为什么“计算机组成与系统结构”“数据结构与算法”“操作系统”“计算机网络”“编译原理”，会成为大学计算机的核心课程呢？至少看起来，这些内容没有“多少天搞定MFC、Structs”这样的内容更容易帮你找到工作。我毕业的时候，也感到很困惑。
不过当时我抱着一个理想，也可能是大多数程序员的理想：我要做技术牛人，我要搞定大系统。
工作15年，我在EMC做过类似GFS的分布式存储开发，做过基于Lucene的搜索引擎，做过Hadoop的运维；在HP和华为做过OpenStack的开发、实施和解决方案；还创业倒腾过Mesos容器平台，后来在网易做Kubernetes。
随着见过的世面越来越多，我渐渐发现，无论是对于大规模系统的架构，还是对于程序员的个人职业生涯，网络和网络协议都是绕不过去的坎儿。
集群规模一大，我们首先想到的就是网络互通的问题；应用吞吐量压不上去，我们首先想到的也是网络互通的问题。不客气地讲，很多情况下，只要搞定了网络，一个大型系统也就搞定了一半。所以，要成为技术牛人，搞定大系统，一定要过网络这一关，而网络协议在网络中占有举足轻重的地位。
相信大部分人都思考过“技术变化太快，容易过时”的问题。毕竟，技术浪潮一浪接一浪，新技术层出不穷。从搜索引擎、大数据、云计算，到人工智能、区块链，简直就是“你方唱罢我登场”。这里面究竟有没有最本质的东西，使得你掌握了它，就能在新技术的滚滚浪潮中，保持快速学习的能力？
通过对大量开源技术的代码进行分析，我发现很多技术看起来轰轰烈烈，扒下外衣，本质的东西其实就是基础知识和核心概念。想要不被滚滚而来的新技术淘汰，就要掌握这些可以长久使用的知识，而网络协议就是值得你学习，而且是到40岁之后依然有价值的知识。
但是，要想真正学习和掌握网络协议，也并非易事。下面这些场景，你是不是也感同身受呢？
网络协议知识点太多，学完记不住。我们都学过计算机网络课程，学的时候感觉并不难。尤其这门课没有公式，更像是文科。学了一大堆，也背了一大堆，应付完考试之后，最终都“还给老师”了。
看上去懂了，但是经不住问。没关系，网上有很多的文章嘛。于是，你会搜索很多文章去看。看的时候，你感觉别人说的很有道理，好像理解了，但是经不住问，一问就发现，你只是了解了大概的流程，很多细节还是不知道。所以说，从能看懂到能给别人讲明白，中间还有很长一段距离。
知识学会了，实际应用依旧不会。细节都摸索得差不多了，但是当你自己去应用和调试的时候，发现还是没有思路。比如，当创建出来的虚拟机不能上网的时候，该怎么办呢？学过的东西，怎么还是不会用？
我把这样的网络协议学习过程总结为：一看觉得懂，一问就打鼓，一用就糊涂。 那网络协议究竟该怎么学？基于这个问题，我决定从以下三个角度和你分享我所理解的网络协议。
第一，我会从身边经常见到的事情出发，用故事来讲解各种网络协议，然后慢慢扩展到不熟悉的领域。
例如，每个人都会查看IP地址，那我们就从这个命令开始，展开一些概念；很多人都在大学宿舍组过简单的网络来打游戏，我就从宿舍里最简单的网络概念开始讲；然后说到办公室，说到日常上网、购物、视频下载等过程涉及的协议；最后说到最陌生的数据中心。
第二，我会用贴近场景的方式来讲解网络协议，将各个层次的关系串起来，而非孤立地讲解某个概念。
常见的计算机网络课程往往会按照网络分层，一层一层地讲，却很少讲层与层之间的关系。例如，我们学习路由协议的时候，在真实场景中，这么多的算法和二层是什么关系呢？和四层又是什么关系呢？例如，在真实的网络通信中，我们访问一个网站，做一个支付，在TCP进行三次握手的时候，IP层在干嘛？MAC层又在干嘛？这些你是不是都清楚？
第三，我会在讲解完各个层次的网络协议之后，着重剖析如何在当下热门领域使用这些协议，比如云计算、容器和微服务。
一方面你可以知道网络协议真实应用的地方，另一方面你也可以通过上手使用云计算、容器、微服务来进一步加深对于协议的理解。
千里之行，始于足下。不管何时，我相信，扎实的功底和过硬的技术，都会是你职业发展的助力器。
希望这个专栏，不仅可以帮你理清繁杂的网络协议概念，帮你构建一个精准的网络协议知识框架，帮你在热门领域应用这些底层知识，更重要的是给你一种学习知识的方法和态度：看似最枯燥、最基础的东西往往具有最长久的生命力。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>开篇词_这一次，让我们一起来搞懂MySQL</title><link>https://artisanbox.github.io/1/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/47/</guid><description>你好，我是林晓斌，网名“丁奇”，欢迎加入我的专栏，和我一起开始MySQL学习之旅。我曾先后在百度和阿里任职，从事MySQL数据库方面的工作，一步步地从一个数据库小白成为MySQL内核开发人员。回想起来，从我第一次带着疑问翻MySQL的源码查到答案至今，已经有十个年头了。在这个过程中，走了不少弯路，但同时也收获了很多的知识和思考，希望能在这个专栏里分享给你。
记得刚开始接触MySQL，是我在百度贴吧做权限系统的时候。我们遇到了一个奇怪的问题，一个正常10毫秒就能完成的SQL查询请求偶尔要执行100多毫秒才结束。当时主管问我是什么原因，我其实也搞不清楚，就上网查答案，但怎么找都找不到，又脸皮薄不想说自己不知道，只好硬着头皮翻源码。后来遇到了越来越多的问题，也是类似的情景，所以我逐步养成了通过分析源码理解原理的习惯。
当时，我自己的感觉是，即使我只是一个开发工程师，只是MySQL的用户，在了解了一个个系统模块的原理后，再来使用它，感觉是完全不一样的。当在代码里写下一行数据库命令的时候，我就能想到它在数据库端将怎么执行，它的性能是怎么样的，怎样写能让我的应用程序访问数据库的性能最高。进一步，哪些数据处理让数据库系统来做性能会更好，哪些数据处理在缓存里做性能会更好，我心里也会更清楚。在建表和建索引的时候，我也会更有意识地为将来的查询优化做综合考虑，比如确定是否使用递增主键、主键的列怎样选择，等等。
但随后我又有了一个新的困惑，我觉得自己了解的MySQL知识点是零散的，没有形成网络。于是解决完一个问题后，很容易忘记。再碰到类似的问题，我又得再翻一次代码。
所幸在阿里工作的时候，我参与了阿里云关系型数据库服务内核的开发，并且负责开发开源分支AliSQL，让我对MySQL内核和源码有了更深层次的研究和理解。在服务内部客户和公有云客户的过程中，我有机会面对和解决足够多的问题，再通过手册进行系统的学习，算是比较坎坷地将MySQL的知识网络补了起来。
所以，在回顾这个过程的时候，我的第一个感受是，如果一开始就有一些从理论到实战的系统性指导，那该多好啊，也许我可以学习得更快些。
在极客时间团队跟我联系策划这个专栏的时候，我还是持怀疑态度的。为什么呢？现在不比当年了，犹记得十余年前，你使用MySQL的过程中碰到问题的话，基本上都只能到代码里去找答案，因为那时网上的资料太少了。
而近十年来，MySQL在中国广泛普及，技术分享文章可以说是浩如烟海。所以，现在要系统地介绍一遍MySQL的话，恐怕里面提及的大多数知识点，都可以在社区文章中找到。那么我们做这个专栏的意义在哪里，而它又凭什么可以收费呢？
直到收到极客时间团队的答复，我才开始对这个专栏“想做和可以做”的事情感觉清晰起来。数据库是一个综合系统，其背后是发展了几十年的数据库理论。同时，数据库系统也是一个应用系统，可能一个业务开发人员用了两三年MySQL，还未必清楚那些自己一直在用的“最佳实践”为什么是最佳的。
于是，我希望这个专栏能够帮助这样的一些开发者：他们正在使用MySQL，知道如何写出逻辑正确的SQL语句来实现业务目标，却不确定这个语句是不是最优的；他们听说了一些使用数据库的最佳实践，但是更想了解为什么这么做；他们使用的数据库偶尔会出问题，亟需了解如何更快速、更准确地定位问题，甚至自己解决问题……
在过去的七年里，我带过十几个应届毕业生，看着他们成长，要求他们原理先行，再实践验证。几年下来，他们的成长速度都很快，其中好几个毕业没两年就成为团队的骨干力量了。我也在社招的时候面试过很多有着不错的运维实践经验和能力的候选人，但都因为对数据库原理仅有一知半解的了解，而最终遗憾地没有通过面试。
因此，我希望这个专栏能够激发开发者对数据库原理的探索欲，从而更好地理解工作中遇到的问题，更能知道背后的为什么。所以我会选那些平时使用数据库时高频出现的知识，如事务、索引、锁等内容构成专栏的主线。这些主线上是一个个的知识点。每个点就是一个概念、一个机制或者一个原理说明。在每个说明之后，我会和你讨论一个实践相关的问题。
希望能以这样的方式，让你对MySQL的几条主线有一个整体的认识，并且了解基本概念。在之后的实践篇中，我会引用到这些主线的知识背景，并着力说明它们是怎样指导实践的。这样，你可以从点到线，再到面，形成自己的MySQL知识网络。
在这里，有一份目录，你也可以先了解下整个专栏的知识结构。
如前面说的，这几条主线上的每个知识点几乎都不是最新的，有些甚至十年前就这样，并没有改过。但我希望针对这些点的说明，可以让你在使用MySQL时心里更有底，知道怎么做选择，并且明白为什么。了解了原理，才能在实践中不断创新，提升个人的价值和工作输出。
从这里开始，跟我一起搞懂MySQL!
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } .</description></item><item><title>开篇词｜让我们来写一门计算机语言吧</title><link>https://artisanbox.github.io/3/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/46/</guid><description>你好，我是宫文学，一名技术创业者。
20多年前，我从北大毕业，搞过一些基础软件，也创过业，最近也一直在研究编译器、操作系统这样的底层技术。
我其实是极客时间的老面孔了，我曾在极客时间开过两门课，《编译原理之美》和《编译原理实战》。这两门课都聚焦于编译技术，一个是“读万卷书”，带你掌握编译原理；另一个是“行万里路”，教你怎么用编译技术。
今天我又设计了这门新课，带你来实现一门计算机语言。但是，你要知道，对于实现一门计算机语言而言，编译技术只是构成要素之一。它还有另外两块大的要素：一个是计算机语言的特性，包括类型系统、面向对象、函数式编程等；另一个是运行时技术，如虚拟机技术、内存管理等。
通过这门课你更好地把握计算机语言中涉及的各类技术的全貌，体会一下实现一门计算机语言的过程。
可能你还有点懵：我为啥要经历实现一门计算机语言的过程呢？这件事能帮我们提升哪些方面的能力或者成就感呢？接下来，我慢慢告诉你。
为什么要自己折腾一门编程语言？现在，每个程序员都熟悉一门或几门计算机语言，但是，我们很少有人想过自己去动手实现一门语言。又或者，虽然豪情万丈地计划过，却又因为种种原因不曾真正付诸实践。
当然，我们也会为自己找很多理由。
最常见的想法是，计算机语言已经很多了，我们会用就行，干嘛要自己去实现呢？另一个常见的想法是，计算机语言，我学习起来都挺不容易，要想去实现它，那更是难以逾越的吧？
这些顾虑看起来都很有说服力，可是，尽管如此，仍然有不少人还是会摆脱这些顾虑，去动手亲自实现一下计算机语言。为什么要做这样看上去很不理性的事情呢？我从我自身的体会来谈一下。
第一，实现一门计算机语言所带来的能力，是真的有用。
我们有时候觉得实现一门计算机语言这样的事情，纯属闲着没事干，因为要让一门计算机语言成功的机会太渺茫了，条件太苛刻了。
不过，在实现一门计算机语言的时候，你能接触到编译技术、运行时技术、汇编语言、硬件架构和各种算法，基本上是从顶层到底层把技术做穿。有了这些硬功夫，其实你已经能够胜任大多数高层次的软件开发工作了。比如，据我了解，Python语言的异步IO模块的开源贡献者，前一阵创业就去做一种创新的异步数据库产品了。
你一样也可以。从前你只会用人家写的东西，现在你要自己来实现了，肯定能更好地理解这套东西的核心逻辑，也能获得更多技术上的高维优势。最现实的就是你能拿下一个个难啃的技术难题，获得更多的晋升机会。
第二，不仅有用，而且这个过程真的很爽。
在我的前两门课里，有同学在实现了一个简单的脚本解释器之后，留言说“激动得浑身发抖”。是的，钻研一些比较深入的技术，会给人带来极大的成就感。特别是对于天天使用计算机语言的程序员来说，如果你有机会把计算机语言实现一遍，洞悉其中的技术秘密，那带来的成就感更是难以比拟的！
第三，像计算机语言这样的领域，更是大有前景。
如果你在关注中国技术发展，那你肯定知道我们目前正奋力在补基础技术方面的课，希望有朝一日也能拥有我们中国自己的优秀基础软件，比如HarmonyOS就在做这种尝试。而且，我们会看到中国涌现出越来越多的编程平台，很多产品会具备二次编程能力，甚至我们自己的计算机语言也会出现并逐步成熟。
要实现这样的突破，需要有更多具备底层编程能力的人才加入进来，要能够深刻理解程序在计算机硬件和操作系统之上运行的基础机制，以及计算机语言编译和运行所需要的技术。而学习如何实现一门计算机语言的过程，就能够带给你这些方面的提高。
现在，我相信你已经了解了，为什么你有必要掌握实现一门计算机语言所涉及的各种技术。不管仅仅是兴趣爱好的原因，还是为了自己的发展，甚至是为了在科技创新的趋势中弄潮，我都邀请你参与进来，一起来玩一玩这些技术。
而要开始实现一门计算机语言，我们首先就需要做一个决策：去实现一门什么样的语言？
我要带你实现什么样的语言？首先，我否决了去设计一门全新的语言。
因为设计一门语言真的很有难度，也是最容易引起争议的话题。而且，这项工作不仅是一项技术工作，还是一个产品设计工作，需要兼顾艺术性和用户体验，是个见仁见智的话题。
其次，我也不想像一些教科书那样，去实现一个玩具语言，这些语言往往不具备最后真正意义上的实用性。
经过几番思考，最终我选择去实现一门已经存在的语言：TypeScript。一门计算机语言其实可以有多个具体实现，像JavaScript就有V8（用于Chrome和Node.js）、TraceMonkey（用于FireFox）、QuickJS等多个不同的实现，每个实现都有不同的适用场景。而我要带你做的是，TypeScript的一个全新的实现。
TypeScript（以及JavaScript）的程序员群体相当庞大，并且它还具备编译成原生应用的潜力，所以HarmonyOS选择了TypeScript作为主力开发语言。
我前一阵参与发起了一个开源工业操作系统的项目。为顺应HarmonyOS的趋势，我也准备用TypeScript实现工业控制软件的开发。过去这些领域都是用C++做开发，其实用TS也完全可以。我甚至也跟JavaScript（ECMAScript）标准组的一名专家热烈讨论过，我们可以把HarmonyOS的基于TypeScript的前端开发工具扩展开来，用于支持安卓、IOS、桌面应用乃至小程序的开发，变成一个跨平台的开发工具，这也完全可以。
所以说，我们这门课选择TypeScript，是看好它未来有更大的发展空间。HarmonyOS已经开了个头，我们还可以做更多的探索。
而且，这门课的大部分内容，比如编译功能等，我们也是采用TypeScript来实现的。对于前端工程师来说，他们本身就很熟悉TypeScript。对于众多的后端工程师而言，由于TypeScript是静态类型的语言，所以他们上手起来也会很快。对于移动端的开发者而言，未来肯定需要了解在HarmonyOS上如何开发应用，所以熟悉一下TypeScript也是有必要的。
那接下来，我们看看在这门课里，我会带你完成哪些工作和挑战。
我会带着你完成什么挑战？总的来说，实现一门计算机语言，我们需要实现编译器、运行时，还要实现面向对象等各种语言特性。
具体一点，首先，我会带着你实现一个纯手写的编译器前端。
编译器前端指的是词法分析、语法分析和语义分析功能。我们目前使用的大多数计算机语言，比如Java、Go、JavaScript等，其编译器前端功能都是纯手写的，而不是采用工具生成的。我会带你了解那些被这些语言所广泛采用的最佳实践，比如LL算法、运算符优先级算法等等。
这种纯手写的实现方式，能让你最大程度体会相关算法的原理。另外，也非常有利于你根据自己的需要进行修改，来满足特定领域的需求。比如，我同学的公司有一个产品，支持在浏览器里编写代码，处理遥感数据。这样的需求，完全可以用TypeScript实现，再编译成JavaScript在浏览器里运行即可。
第二，我还会带你实现纯手写的编译器后端。
编译器后端指的是生成目标代码的功能，而目标代码呢，指的是字节码或汇编代码。编译器后端不仅要生成代码，还要对代码进行优化，尽量提升它的性能。
编译器后端的工作量通常更大，所以像Rust、Julia等新兴起的语言，往往采用一个后端工具，比如LLVM，而不是自己编写后端，这样可以节省大量的工作。不过这个方式也有缺陷，比如针对移动应用或者浏览器运行环境，在资源占用、即时编译速度等方面就不够理想。所以，像JVM、Android的运行时、V8等，都会自己去实现后端。
而且，如果你对语言的运行机制有特殊的要求，并且跟C/C++这些不同，那么你最好自己实现一个后端，比如Go语言就是这样。
编译器后端通常还包含大量的优化算法（有时候，我们把这些优化功能归为中端），这些优化算法具有比较强的工程性，所以教科书里的描述往往不够具体，也不能体现业界的一些最佳实践。在这门课里呢，我们可以自己动手去体会这些最佳实践，包括基于图的IR，以及一些优化算法，从而对优化算法的理解更加具象化。
在编译器后端里，因为我们还要生成汇编代码，所以能带你掌握汇编语言的精髓。在实现一些系统级软件的时候，我们有时候必须能够想象出来，这些软件的逻辑落实到汇编代码层面是什么样子的，这样才能确定最佳的技术策略。而破除对汇编代码的陌生感，是打通技术人员奇经八脉的重要一环。
第三，我还会带你实现多个运行时机制。
要让编译后的程序运行起来，我们必须要设计程序的运行机制。当然了，让一个程序跑起来的方法很多。在这门课里，我将带你实现多种运行时机制，让你能体会它们各自的设计思想，并能进行相互间的比较。
首先，我会带你实现一个AST解释器，也就是通过遍历AST的方式来运行程序。这种方式虽然简单，但很实用，对很多应用需求来说都够用了。
接着，我会把AST编译成字节码，在虚拟机上运行。像Java、JavaScript、Python等语言，都支持这种运行方式。在这个环节，我们会讲解栈机和寄存器机的差别，设计字节码，并实现一个栈机。
并且，我还会带你实现两个不同版本的虚拟机，一个是基于TypeScript实现的，一个是基于C语言实现的。当你采用C语言时，你对于运行时的一些实现细节拥有更多的掌控能力。你会看到，只有掌控了像内存分配这些技术细节，才能让基于C语言的虚拟机在性能上胜出。
当然，最后，我还会带你把程序编译成本地可执行文件来运行。在这个过程中，我最希望你能够彻底搞清楚，当一个编译成本地代码的程序在运行的时候，到底CPU、操作系统和计算机语言本身各自都扮演了什么角色。这是打通技术上的奇经八脉来说，是非常重要的一环。
你会发现，作为计算机语言的实现者，你其实拥有比自己想象中大得多的发挥空间。所以，当你实现像协程、JIT机制等高级特性的时候，就能够更好地设计或理解相应的技术方案。
对于一些技术细节，比如通过汇编代码做栈桢的管理，我们也会上手获得细致的理解。基于这些透彻的理解，你会有能力基于栈桢的机制来实现尾递归和尾调用的优化，从而让你增强对于物理机的运行机制的掌控感。
最重要的是，每实现一个运行时机制，我们都会进行性能的测试和比拼。这些真实的测试和数据，会让你对于运行时机制产生非常具象的感受。下面这张图就是在课程的某一讲中，我们集齐了5个版本的运行时进行对比测试的结果。更重要的是，这几个版本的运行时，你都可以自己动手做出来。
第四，我会带你理解一些高级语言的特性是如何实现的。
在实现了计算机语言的一些基本特性以后，我们会去讨论一些高级一点的话题，比如类型体系的实现；在支持面向对象时，如何用最小的代价实现运行时的多态特性；在支持函数编程特性时，又是如何实现高阶函数功能、闭包功能等。
而对象、闭包等特性，又不可避免地会引出运行时的内存管理问题，因此，我们也会实现一个自己的垃圾收集器。
我会怎么带你一步步实现？看着我前面大段大段的介绍，你觉得这些东西难吗？有编译，有运行时，还有一些更高级的语言特性，看上去还挺难吧？内容也很多，你可能心里已经开始打“退堂鼓”了：这么多内容，难度又不小，我能跟下来吗？
请打住！其实你根本不用担心。我在课程内容的设计上是逐步递进的，你会自然而然地跟着走下来，不会感觉有很大的学习困难。我会从原理出发，带你走完整个语言的实现过程，一方面能避免各种繁琐的编程工作，对你理解原理带来干扰；另一方面又能保留足够多的技术细节，让我们的教学语言具备足够的实用性。
哪怕你只学了几节课，你也能够掌握编译器前端的基础技能，实现一个AST解释器。再学几节课呢，就能搞出一个基于TypeScript的虚拟机出来。然后再加两节呢，又搞出一个C语言版本的虚拟机出来。不知不觉间，你就走出很远，爬得很高了。
在第一部分起步篇中，我会主要选取少量的语言特性，带你迅速实现从前到后的技术贯穿，这样你就能对计算机语言涉及的各项技术有一个全局性的了解。
这一部分又分成了三个阶段。在第一个阶段，我会带你用AST解释器把TypeScript跑起来，并在这个过程中带你掌握业界最常用的词法分析技术、语法分析技术和语义分析技术。在第二个阶段，我会升级解释运行的机制，带你掌握字节码技术和栈机。而在第三个阶段，我们就已经能够让程序编译成本地代码运行了！
紧接着在第二部分进阶篇呢，我会把这条路拓宽，也就是增加更丰富的语言特性，比如支持更多的数据类型、支持面向对象和函数式编程特性，等等。在这一部分，你能够丰富知识面，从而有能力解决更多的基础技术问题，其中就有内存管理这个关键技术。
学完进阶篇以后，你对实现一门计算机语言中所涉及的知识点，掌握得就比较全面了。剩下的知识点，通常只有专门从事这个领域工作的人或研究人员才会去涉足，这里面就包含编译优化技术。
每一门语言都会特别重视性能，而优化技术就是提升语言性能的关键。我还看到现实中一些做开发平台的项目中，真正的硬骨头往往就是优化技术。所以在最后在第三部分优化篇里，我就主要介绍一下优化技术。我会用比较浅显和直观的方式，让你了解Java、JavaScript等语言所采用的前沿优化技术，洞悉它们最深处的奥秘，让你有能力去承担那些攻坚性的任务。
更具体的详细目录你可以看看这个：
如果把实现一门计算机语言看成是一场冒险，那么现在我已经给你规划好了目标和路线，也会在路途中给你不断充实“武器”和“弹药”。
但俗语也有说，“兵马未动，粮草先行。”贴心的我还给你储备好了“衣物”和“粮草”，我给你备好了有着上万行的实验代码的代码库。而且，我们课中采用的技术，是基于我手头正在做的一门实用级语言为素材的，而且会作为开源项目一直进行版本迭代，所以你甚至可以拿这个开源项目作为自己工作的基础。当然了，我也无比欢迎你加入其中和我一起共建，为我们的“后勤保障”添砖加瓦。
好了，现在万事俱备，只欠东风。加入我吧，我已经迫不及待和你开启这一场计算机语言的冒险了！
欢迎点击链接加入交流群 ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>总结课_在实际开发中，如何权衡选择使用哪种数据结构和算法？</title><link>https://artisanbox.github.io/2/81/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/81/</guid><description>你好，我是王争，今天是一篇总结课。我们学了这么多数据结构和算法，在实际开发中，究竟该如何权衡选择使用哪种数据结构和算法呢？今天我们就来聊一聊这个问题，希望能帮你把学习带回实践中。
我一直强调，学习数据结构和算法，不要停留在学院派的思维中，只把算法当作应付面试、考试或者竞赛的花拳绣腿。作为软件开发工程师，我们要把数据结构和算法，应用到软件开发中，解决实际的开发问题。
不过，要想在实际的开发中，灵活、恰到好处地应用数据结构和算法，需要非常深厚的实战经验积累。尽管我在课程中，一直都结合实际的开发场景来讲解，希望带你真枪实弹地演练算法如何解决实际的问题。但是，在今后的软件开发中，你要面对的问题远比我讲的场景要复杂、多变、不确定。
要想游刃有余地解决今后你要面对的问题，光是熟知每种数据结构和算法的功能、特点、时间空间复杂度，还是不够的。毕竟工程上的问题不是算法题。算法题的背景、条件、限制都非常明确，我们只需要在规定的输入、输出下，找最优解就可以了。
而工程上的问题往往都比较开放，在选择数据结构和算法的时候，我们往往需要综合各种因素，比如编码难度、维护成本、数据特征、数据规模等，最终选择一个工程的最合适解，而非理论上的最优解。
为了让你能做到活学活用，在实际的软件开发中，不生搬硬套数据结构和算法，今天，我们就聊一聊，在实际的软件开发中，如何权衡各种因素，合理地选择使用哪种数据结构和算法？关于这个问题，我总结了六条经验。
1.时间、空间复杂度不能跟性能划等号我们在学习每种数据结构和算法的时候，都详细分析了算法的时间复杂度、空间复杂度，但是，在实际的软件开发中，复杂度不能与性能简单划等号，不能表示执行时间和内存消耗的确切数据量。为什么这么说呢？原因有下面几点。
复杂度不是执行时间和内存消耗的精确值
在用大O表示法表示复杂度的时候，我们会忽略掉低阶、常数、系数，只保留高阶，并且它的度量单位是语句的执行频度。每条语句的执行时间，并非是相同、确定的。所以，复杂度给出的只能是一个非精确量值的趋势。
代码的执行时间有时不跟时间复杂度成正比
我们常说，时间复杂度是O(nlogn)的算法，比时间复杂度是O(n^2)的算法，执行效率要高。这样说的一个前提是，算法处理的是大规模数据的情况。对于小规模数据的处理，算法的执行效率并不一定跟时间复杂度成正比，有时还会跟复杂度成反比。
对于处理不同问题的不同算法，其复杂度大小没有可比性
复杂度只能用来表征不同算法，在处理同样的问题，以及同样数据类型的情况下的性能表现。但是，对于不同的问题、不同的数据类型，不同算法之间的复杂度大小并没有可比性。
2.抛开数据规模谈数据结构和算法都是“耍流氓”在平时的开发中，在数据规模很小的情况下，普通算法和高级算法之间的性能差距会非常小。如果代码执行频率不高、又不是核心代码，这个时候，我们选择数据结构和算法的主要依据是，其是否简单、容易维护、容易实现。大部分情况下，我们直接用最简单的存储结构和最暴力的算法就可以了。
比如，对于长度在一百以内的字符串匹配，我们直接使用朴素的字符串匹配算法就够了。如果用KMP、BM这些更加高效的字符串匹配算法，实际上就大材小用了。因为这对于处理时间是毫秒量级敏感的系统来说，性能的提升并不大。相反，这些高级算法会徒增编码的难度，还容易产生bug。
3.结合数据特征和访问方式来选择数据结构面对实际的软件开发场景，当我们掌握了基础数据结构和算法之后，最考验能力的并不是数据结构和算法本身，而是对问题需求的挖掘、抽象、建模。如何将一个背景复杂、开放的问题，通过细致的观察、调研、假设，理清楚要处理数据的特征与访问方式，这才是解决问题的重点。只有理清楚了这些东西，我们才能将问题转化成合理的数据结构模型，进而找到满足需求的算法。
比如我们前面讲过，Trie树这种数据结构是一种非常高效的字符串匹配算法。但是，如果你要处理的数据，并没有太多的前缀重合，并且字符集很大，显然就不适合利用Trie树了。所以，在用Trie树之前，我们需要详细地分析数据的特点，甚至还要写些分析代码、测试代码，明确要处理的数据是否适合使用Trie树这种数据结构。
再比如，图的表示方式有很多种，邻接矩阵、邻接表、逆邻接表、二元组等等。你面对的场景应该用哪种方式来表示，具体还要看你的数据特征和访问方式。如果每个数据之间联系很少，对应到图中，就是一个稀疏图，就比较适合用邻接表来存储。相反，如果是稠密图，那就比较适合采用邻接矩阵来存储。
4.区别对待IO密集、内存密集和计算密集如果你要处理的数据存储在磁盘，比如数据库中。那代码的性能瓶颈有可能在磁盘IO，而并非算法本身。这个时候，你需要合理地选择数据存储格式和存取方式，减少磁盘IO的次数。
比如我们在递归那一节讲过最终推荐人的例子。你应该注意到了，当时我给出的代码尽管正确，但其实并不高效。如果某个用户是经过层层推荐才来注册的，那我们获取他的最终推荐人的时候，就需要多次访问数据库，性能显然就不高了。
不过，这个问题解决起来不难。我们知道，某个用户的最终推荐人一旦确定，就不会变动。所以，我们可以离线计算每个用户的最终推荐人，并且保存在表中的某个字段里。当我们要查看某个用户的最终推荐人的时候，访问一次数据库就可以获取到。
刚刚我们讲了数据存储在磁盘的情况，现在我们再来看下，数据存储在内存中的情况。如果你的数据是存储在内存中，那我们还需要考虑，代码是内存密集型的还是CPU密集型的。
所谓CPU密集型，简单点理解就是，代码执行效率的瓶颈主要在CPU执行的效率。我们从内存中读取一次数据，到CPU缓存或者寄存器之后，会进行多次频繁的CPU计算（比如加减乘除），CPU计算耗时占大部分。所以，在选择数据结构和算法的时候，要尽量减少逻辑计算的复杂度。比如，用位运算代替加减乘除运算等。
所谓内存密集型，简单点理解就是，代码执行效率的瓶颈在内存数据的存取。对于内存密集型的代码，计算操作都比较简单，比如，字符串比较操作，实际上就是内存密集型的。每次从内存中读取数据之后，我们只需要进行一次简单的比较操作。所以，内存数据的读取速度，是字符串比较操作的瓶颈。因此，在选择数据结构和算法的时候，需要考虑是否能减少数据的读取量，数据是否在内存中连续存储，是否能利用CPU缓存预读。
5.善用语言提供的类，避免重复造轮子实际上，对于大部分常用的数据结构和算法，编程语言都提供了现成的类和函数实现。比如，Java中的HashMap就是散列表的实现，TreeMap就是红黑树的实现等。在实际的软件开发中，除非有特殊的要求，我们都可以直接使用编程语言中提供的这些类或函数。
这些编程语言提供的类和函数，都是经过无数验证过的，不管是正确性、鲁棒性，都要超过你自己造的轮子。而且，你要知道，重复造轮子，并没有那么简单。你需要写大量的测试用例，并且考虑各种异常情况，还要团队能看懂、能维护。这显然是一个出力不讨好的事情。这也是很多高级的数据结构和算法，比如Trie树、跳表等，在工程中，并不经常被应用的原因。
但这并不代表，学习数据结构和算法是没用的。深入理解原理，有助于你能更好地应用这些编程语言提供的类和函数。能否深入理解所用工具、类的原理，这也是普通程序员跟技术专家的区别。
6.千万不要漫无目的地过度优化掌握了数据结构和算法这把锤子，不要看哪里都是钉子。比如，一段代码执行只需要0.01秒，你非得用一个非常复杂的算法或者数据结构，将其优化成0.005秒。即便你的算法再优秀，这种微小优化的意义也并不大。相反，对应的代码维护成本可能要高很多。
不过度优化并不代表，我们在软件开发的时候，可以不加思考地随意选择数据结构和算法。我们要学会估算。估算能力实际上也是一个非常重要的能力。我们不仅要对普通情况下的数据规模和性能压力做估算，还需要对异常以及将来一段时间内，可能达到的数据规模和性能压力做估算。这样，我们才能做到未雨绸缪，写出来的代码才能经久可用。
还有，当你真的要优化代码的时候，一定要先做Benchmark基准测试。这样才能避免你想当然地换了一个更高效的算法，但真实情况下，性能反倒下降了。
总结工程上的问题，远比课本上的要复杂。所以，我今天总结了六条经验，希望你能把数据结构和算法用在刀刃上，恰当地解决实际问题。
我们在利用数据结构和算法解决问题的时候，一定要先分析清楚问题的需求、限制、隐藏的特点等。只有搞清楚了这些，才能有针对性地选择恰当的数据结构和算法。这种灵活应用的实战能力，需要长期的刻意锻炼和积累。这是一个有经验的工程师和一个学院派的工程师的区别。
好了，今天的内容就到这里了。最后，我想听你谈一谈，你在实际开发选择数据结构和算法时，有什么感受和方法呢？
欢迎在留言区写下你的想法，也欢迎你把今天的文章分享给你的朋友，帮助他在数据结构和算法的实际运用中走得更远。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>打卡召集令_60天攻克数据结构与算法</title><link>https://artisanbox.github.io/2/69/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/69/</guid><description>你好，我是王争。
今年4月，专栏更新结束之后，我在专栏发布了一篇《数据结构与算法之美》学习指导手册》，在这篇文章里，我对专栏内容重新做了一次梳理，将整个专栏拆分成四个阶段，列出了每个阶段的核心知识点、标注了每个知识点的难易程度（E-Easy，M-Medium，H-Hard），并用 1-10 分说明其重要性。
但是，我发现，很多同学还是没能坚持下来，久而久之对学习算法失去了信心。
回想起来，写专栏之初，我就立下 Flag，要做一个跟国内外经典书籍不一样、可以长期影响一些人的专栏。所以，在专栏完结 9 个月后，我想再做一些事情。
为了带你彻底拿下“数据结构与算法”这座大山，我发起了“60 天攻克数据结构与算法”打卡行动，一起登顶！
下面是我为你精心规划的学习计划表：
活动时间：2019.11.25-2020.1.19
你将获得：
1.坚持 60 天，与 2000 位优秀的工程师一起，彼此激励，相互学习；
2.整个学习周期内，我会进行2次高质量的社群分享；
3.我会精心整理 4 张知识脑图，为你梳理每个阶段的学习重点，发布在专栏里；
4.我和极客时间准备了 20 万奖学金，给坚持下来的同学。
活动规则：
在下方申请进入活动打卡群，根据课表打卡，完成学习。
打卡要求：
1.每个阶段持续 2 周，每周仅需打卡 3 次，即视为完成该阶段的学习。
2.4个阶段（8 周）的学习，打卡总数仅需 30 次，即视为完成“60 天攻克数据结构与算法行动”。
3.为了让大家养成习惯，每日只计 1 次打卡，单日内多次打卡视为 1 次。
进入打卡群后，完成学习还有如下奖励：
第一阶段（第1-2周）：¥15 奖励金 第二阶段（第3-4周）：¥25 奖励金 第三阶段（第5-6周）：¥35 奖励金 四个阶段（第7-8周）：¥50 奖励金 （注：奖励金会以无门槛优惠券形式、分阶段进行发放，发放时间为每阶段结束后的 7 个工作日内。）
当然，优惠券只是对你的小小奖励。坚持 60 天，与 2000 位优秀的工程师一起，互相学习，彼此激励，彻底拿下数据结构与算法，我奉陪到底。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>打卡召集令_第一阶段知识总结</title><link>https://artisanbox.github.io/2/73/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/73/</guid><description>ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } ._2sjJGcOH_0 ._36ChpWj4_0 { margin-left: 0.</description></item><item><title>打卡召集令_第三阶段知识总结</title><link>https://artisanbox.github.io/2/71/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/71/</guid><description>ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } ._2sjJGcOH_0 ._36ChpWj4_0 { margin-left: 0.</description></item><item><title>打卡召集令_第二阶段知识总结</title><link>https://artisanbox.github.io/2/70/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/70/</guid><description>ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } ._2sjJGcOH_0 ._36ChpWj4_0 { margin-left: 0.</description></item><item><title>打卡召集令_第四阶段知识总结</title><link>https://artisanbox.github.io/2/72/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/72/</guid><description>ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } ._2sjJGcOH_0 ._36ChpWj4_0 { margin-left: 0.</description></item><item><title>春节7天练_Day1：数组和链表</title><link>https://artisanbox.github.io/2/62/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/62/</guid><description>你好，我是王争。首先祝你新年快乐！
专栏的正文部分已经结束，相信这半年的时间，你学到了很多，究竟学习成果怎样呢？
我整理了数据结构和算法中必知必会的30个代码实现，从今天开始，分7天发布出来，供你复习巩固所用。你可以每天花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。
除此之外，@Smallfly 同学还整理了一份配套的LeetCode练习题，你也可以一起练习一下。在此，我谨代表我本人对@Smallfly 表示感谢！
另外，我还为假期坚持学习的同学准备了丰厚的春节加油礼包。
2月5日-2月14日，只要在专栏文章下的留言区写下你的答案，参与答题，并且留言被精选，即可获得极客时间10元无门槛优惠券。
7篇中的所有题目，只要回答正确3道及以上，即可获得极客时间99元专栏通用阅码。
如果7天连续参与答题，并且每天的留言均被精选，还可额外获得极客时间价值365元的每日一课年度会员。
关于数组和链表的几个必知必会的代码实现数组 实现一个支持动态扩容的数组
实现一个大小固定的有序数组，支持动态增删改操作
实现两个有序数组合并为一个有序数组
链表 实现单链表、循环链表、双向链表，支持增删操作
实现单链表反转
实现两个有序的链表合并为一个有序链表
实现求链表的中间结点
对应的LeetCode练习题（@Smallfly 整理）数组 Three Sum（求三数之和） 英文版：https://leetcode.com/problems/3sum/
中文版：https://leetcode-cn.com/problems/3sum/
Majority Element（求众数） 英文版：https://leetcode.com/problems/majority-element/
中文版：https://leetcode-cn.com/problems/majority-element/
Missing Positive（求缺失的第一个正数） 英文版：https://leetcode.com/problems/first-missing-positive/
中文版：https://leetcode-cn.com/problems/first-missing-positive/
链表 Linked List Cycle I（环形链表） 英文版：https://leetcode.com/problems/linked-list-cycle/
中文版：https://leetcode-cn.com/problems/linked-list-cycle/
Merge k Sorted Lists（合并k个排序链表） 英文版：https://leetcode.com/problems/merge-k-sorted-lists/</description></item><item><title>春节7天练_Day2：栈、队列和递归</title><link>https://artisanbox.github.io/2/63/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/63/</guid><description>你好，我是王争。初二好！
为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的30个代码实现，分7天发布出来，供你复习巩固所用。今天是第二篇。
和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。
关于栈、队列和递归的几个必知必会的代码实现栈 用数组实现一个顺序栈
用链表实现一个链式栈
编程模拟实现一个浏览器的前进、后退功能
队列 用数组实现一个顺序队列
用链表实现一个链式队列
实现一个循环队列
递归 编程实现斐波那契数列求值f(n)=f(n-1)+f(n-2)
编程实现求阶乘n!
编程实现一组数据集合的全排列
对应的LeetCode练习题（@Smallfly 整理）栈 Valid Parentheses（有效的括号） 英文版：https://leetcode.com/problems/valid-parentheses/
中文版：https://leetcode-cn.com/problems/valid-parentheses/
Longest Valid Parentheses（最长有效的括号） 英文版：https://leetcode.com/problems/longest-valid-parentheses/
中文版：https://leetcode-cn.com/problems/longest-valid-parentheses/
Evaluate Reverse Polish Notatio（逆波兰表达式求值） 英文版：https://leetcode.com/problems/evaluate-reverse-polish-notation/
中文版：https://leetcode-cn.com/problems/evaluate-reverse-polish-notation/
队列 Design Circular Deque（设计一个双端队列） 英文版：https://leetcode.com/problems/design-circular-deque/
中文版：https://leetcode-cn.com/problems/design-circular-deque/
Sliding Window Maximum（滑动窗口最大值） 英文版：https://leetcode.com/problems/sliding-window-maximum/
中文版：https://leetcode-cn.com/problems/sliding-window-maximum/
递归 Climbing Stairs（爬楼梯） 英文版：https://leetcode.com/problems/climbing-stairs/
中文版：https://leetcode-cn.com/problems/climbing-stairs/</description></item><item><title>春节7天练_Day3：排序和二分查找</title><link>https://artisanbox.github.io/2/64/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/64/</guid><description>你好，我是王争。初三好！
为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的30个代码实现，分7天发布出来，供你复习巩固所用。今天是第三篇。
和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。
前两天的内容，是关于数组和链表、排序和二分查找的。如果你错过了，点击文末的“上一篇”，即可进入测试。
关于排序和二分查找的几个必知必会的代码实现排序 实现归并排序、快速排序、插入排序、冒泡排序、选择排序
编程实现O(n)时间复杂度内找到一组数据的第K大元素
二分查找 实现一个有序数组的二分查找算法
实现模糊二分查找算法（比如大于等于给定值的第一个元素）
对应的LeetCode练习题（@Smallfly 整理） Sqrt(x) （x 的平方根） 英文版：https://leetcode.com/problems/sqrtx/
中文版：https://leetcode-cn.com/problems/sqrtx/
做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。
祝你取得好成绩！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>春节7天练_Day4：散列表和字符串</title><link>https://artisanbox.github.io/2/65/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/65/</guid><description>你好，我是王争。初四好！
为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的30个代码实现，分7天发布出来，供你复习巩固所用。今天是第四篇。
和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。
前几天的内容。如果你错过了，点击文末的“上一篇”，即可进入测试。
关于散列表和字符串的4个必知必会的代码实现散列表 实现一个基于链表法解决冲突问题的散列表
实现一个LRU缓存淘汰算法
字符串 实现一个字符集，只包含a～z这26个英文字母的Trie树
实现朴素的字符串匹配算法
对应的LeetCode练习题（@Smallfly 整理）字符串 Reverse String （反转字符串） 英文版：https://leetcode.com/problems/reverse-string/
中文版：https://leetcode-cn.com/problems/reverse-string/
Reverse Words in a String（翻转字符串里的单词） 英文版：https://leetcode.com/problems/reverse-words-in-a-string/
中文版：https://leetcode-cn.com/problems/reverse-words-in-a-string/
String to Integer (atoi)（字符串转换整数 (atoi)） 英文版：https://leetcode.com/problems/string-to-integer-atoi/
中文版：https://leetcode-cn.com/problems/string-to-integer-atoi/
做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。
祝你取得好成绩！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>春节7天练_Day5：二叉树和堆</title><link>https://artisanbox.github.io/2/66/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/66/</guid><description>你好，我是王争。春节假期进入尾声了。你现在是否已经准备返回工作岗位了呢？今天更新的是测试题的第五篇，我们继续来复习。
关于二叉树和堆的7个必知必会的代码实现二叉树 实现一个二叉查找树，并且支持插入、删除、查找操作
实现查找二叉查找树中某个节点的后继、前驱节点
实现二叉树前、中、后序以及按层遍历
堆 实现一个小顶堆、大顶堆、优先级队列
实现堆排序
利用优先级队列合并K个有序数组
求一组动态数据集合的最大Top K
对应的LeetCode练习题（@Smallfly 整理） Invert Binary Tree（翻转二叉树） 英文版：https://leetcode.com/problems/invert-binary-tree/
中文版：https://leetcode-cn.com/problems/invert-binary-tree/
Maximum Depth of Binary Tree（二叉树的最大深度） 英文版：https://leetcode.com/problems/maximum-depth-of-binary-tree/
中文版：https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/
Validate Binary Search Tree（验证二叉查找树） 英文版：https://leetcode.com/problems/validate-binary-search-tree/
中文版：https://leetcode-cn.com/problems/validate-binary-search-tree/
Path Sum（路径总和） 英文版：https://leetcode.com/problems/path-sum/
中文版：https://leetcode-cn.com/problems/path-sum/
做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友。
祝你取得好成绩！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>春节7天练_Day6：图</title><link>https://artisanbox.github.io/2/67/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/67/</guid><description>你好，我是王争。初六好！
为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的30个代码实现，分7天发布出来，供你复习巩固所用。今天是第六篇。
和之前一样，你可以花一点时间，来手写这些必知必会的代码。写完之后，你可以根据结果，回到相应章节，有针对性地进行复习。做到这些，相信你会有不一样的收获。
关于图的几个必知必会的代码实现图 实现有向图、无向图、有权图、无权图的邻接矩阵和邻接表表示方法
实现图的深度优先搜索、广度优先搜索
实现Dijkstra算法、A*算法
实现拓扑排序的Kahn算法、DFS算法
对应的LeetCode练习题（@Smallfly 整理） Number of Islands（岛屿的个数） 英文版：https://leetcode.com/problems/number-of-islands/description/
中文版：https://leetcode-cn.com/problems/number-of-islands/description/
Valid Sudoku（有效的数独） 英文版：https://leetcode.com/problems/valid-sudoku/
中文版：https://leetcode-cn.com/problems/valid-sudoku/
做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。
祝你取得好成绩！明天见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>春节7天练_Day7：贪心、分治、回溯和动态规划</title><link>https://artisanbox.github.io/2/68/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/68/</guid><description>你好，我是王争。今天是节后的第一个工作日，也是我们“春节七天练”的最后一篇。
几种算法思想必知必会的代码实现回溯 利用回溯算法求解八皇后问题
利用回溯算法求解0-1背包问题
分治 利用分治算法求一组数据的逆序对个数 动态规划 0-1背包问题
最小路径和（详细可看@Smallfly整理的 Minimum Path Sum）
编程实现莱文斯坦最短编辑距离
编程实现查找两个字符串的最长公共子序列
编程实现一个数据序列的最长递增子序列
对应的LeetCode练习题（@Smallfly 整理） Regular Expression Matching（正则表达式匹配） 英文版：https://leetcode.com/problems/regular-expression-matching/
中文版：https://leetcode-cn.com/problems/regular-expression-matching/
Minimum Path Sum（最小路径和） 英文版：https://leetcode.com/problems/minimum-path-sum/
中文版：https://leetcode-cn.com/problems/minimum-path-sum/
Coin Change （零钱兑换） 英文版：https://leetcode.com/problems/coin-change/
中文版：https://leetcode-cn.com/problems/coin-change/
Best Time to Buy and Sell Stock（买卖股票的最佳时机） 英文版：https://leetcode.com/problems/best-time-to-buy-and-sell-stock/
中文版：https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/
Maximum Product Subarray（乘积最大子序列） 英文版：https://leetcode.com/problems/maximum-product-subarray/
中文版：https://leetcode-cn.com/problems/maximum-product-subarray/
Triangle（三角形最小路径和） 英文版：https://leetcode.com/problems/triangle/
中文版：https://leetcode-cn.com/problems/triangle/
到此为止，七天的练习就结束了。这些题目都是我精选出来的，是基础数据结构和算法中最核心的内容。建议你一定要全部手写练习。如果一遍搞不定，你可以结合前面的章节，多看几遍，反复练习，直到能够全部搞定为止。</description></item><item><title>期中测试｜快来检验你在起步篇的学习成果吧</title><link>https://artisanbox.github.io/3/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/49/</guid><description>你好，我是宫文学。
不知不觉间，我们的课程已经更新过半了。前几天，我们也更新完了第一部分，也就是起步篇的内容。到这里，我其实已经带你完整地跑完了，实现一门计算机语言需要的全部流程了。不知道你学习得怎么样呀？不如做套题来检验一下吧？
趁着国庆假期，我根据我们第一部分起步篇里讲过的知识，给你出了20道选择题，你可以检验一下自己的学习成果。如果你有什么不理解的地方，欢迎在留言区留言，也可以直接来我们的微信交流群找我。
快点击下面的按钮开始测试吧，我期待着你满分的好消息。</description></item><item><title>期末考试｜实现编程语言这些核心知识点，你掌握得咋样了？</title><link>https://artisanbox.github.io/3/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/48/</guid><description>你好，我是宫文学。
我们的课程已接近尾声，要学的内容我们都已经全部学完了。不知道你掌握得怎么样了呢？今天，我给你准备了20道选择题，满分100分，范围囊括我们这门课的众多核心知识，一起来挑战一下吧！
如果有什么不明白的，欢迎直接在留言区提问，也可以在交流群找我，期待你满分的好消息！
另外，我还给你准备了一份调查问卷，想听一下你对我这门课的看法和建议。题目不多，两分钟就可以填完，非常希望能看到你的反馈。</description></item><item><title>特别加餐_我在2019年F8大会的两日见闻录</title><link>https://artisanbox.github.io/4/61/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/61/</guid><description>你好，我是徐文浩。4月30日，我在美国圣何塞参加了F8大会，趁此机会和你分享一下，我在大会上的一些见闻。下面是我参会这两天写的见闻录，分享给你。希望可以看到更多技术人走出去，抬头看看世界，丰富自己的见识和经历。
Day 1：“The Future is Private”今年是我连续第三年来F8了。如果说第一年是带着一点好奇和忐忑，作为一个开发者来看看世界上最大的社交网络的开发者大会是怎么回事儿，到了第二年，作为一个Developer Partner，看到自己公司的logo出现在首日的Keynote里，就觉得格外兴奋；那么今年第三年就有些轻车熟路了，没有什么压力，反而很想看一看，每年一次的开发者大会，还能办出什么新花样。
作为开发者，连续两年看到自己公司的logo出现在会场里还是很高兴的从旧金山国际机场出来，一路Uber到了圣何塞住下，不禁感慨，互联网和智能手机的确改变了世界。一个中国人到美国，拿着手机也能在这里生存下来了。
为了倒时差，我硬是熬到半夜睡了一觉。早早赶到圣何塞市中心的会场，发现已经有不少人在排队入场了。
去年的F8，因为Facebook面临“剑桥门事件”，主题的Keynote颇有些疲于应对的感觉。然而在过去的一年里，Facebook推出的种种隐私保护的功能，似乎并没有解决“隐私泄露”的问题，反而给人一种此起彼伏、应接不暇的感觉。
于是，今年的F8，Facebook颇有些破釜沉舟、不破不立之感。扎克伯格的开场Keynote，就表示，Facebook要开始在整个公司的运营策略上做出重大改变，打造一个“Privacy Focused Social Platform”，接着更是亮出了“The Future is Private”的slogan。
紧接着，扎克伯格介绍了Facebook这两年力推的产品Messenger。Messenger团队重写了整个手机客户端，让整个客户端小于30MB，冷启动时间少于1.3秒，默认端到端加密，并给它起了一个代号叫LightSpeed。
这几项指标都可以直接拿来，和自家被认为简单易用的WhatsApp做对比，而且明显胜出。为了服务更多Messenger的发达国家用户，Facebook更是干脆开发了一个桌面版的客户端。要知道，在这个移动端主宰一切的年代，还会投入精力开发桌面客户端的公司可不多了。可以看出，Facebook推动Messenger产品的决心。
WhatsApp产品更新介绍的核心也还是在隐私上。他们能够通过Messenger直接和WhatsApp联系人通信，这更是可以看出，Facebook迈出了打通旗下所有产品的第一步。
然而更重磅的还在后面。在介绍完Messenger和WhatsApp的产品更新之后，会场的大屏幕上打出了白底蓝字的“FB5”的logo。
作为Facebook最核心的产品，也是自己公司名字的Facebook，迎来了多年以来的第一次大型改版：App和Web端界面完全重写，产品中心从原先的信息流转向以Group为核心。“Groups at the heart”，传统蓝底白字的“f”字logo，也变成了有背景动图的“f”字logo了。
如果说其他App上的改动，还可以认为是Facebook的尝试或者探索，作为其主要收入来源的Facebook改版，恐怕是动真格的了。从一个开放信息流式的产品，变成一个以Group为核心的、有着私密性的产品，怕是多年以来Facebook这个App的另一次重大转变了。
之后的Instagram、Portal以及Spark VR的产品更新，都没有引起太多关注。Keynote的下一个爆点自然是Oculus。
Facebook是目前市场上唯一还在大力投入VR的大型厂商。这一次让人尖叫的就是Oculus Quest。这第一个“无线”的Oculus的确引人注目。当现场宣布所有参加F8的人将人手派发一个Oculus Quest，更是引来全场的掌声。大屏幕上，看着卡马克头戴Oculus挥舞光剑，更是让老程序员们回忆起，在DOS上玩“Wolfenstein 3D”的旧时光。
早上的Keynote结束之后，就是自由活动了。参会的工程师们可以选择去不同的会议室，听各种开发和产品相关的小讲座，也可以直接在主会场的各个“摊位”前，和Facebook的工程师沟通交流。通常如果提问的话，还会拿到背包、T恤、帽子这样的小奖品。
当然，排长队去体验Oculus是每年最热门的项目。你也可以在会场里面转悠，和其他开发者认识一下。免费的零食和饮料到处都是。与其说这是一个开发者大会，其实更像是一个Facebook生态圈的嘉年华。
F8的第一天，仍然是以Facebook自己的四大产品为核心的一个主题会议，并没有介绍太多AI和VR的黑科技。按照惯例，这些黑科技会在明天的Keynote呈现，值得期待。我印象比较深的是，今天在讲解Oculus Rift S的时候，介绍了Oculus Insight Position Tracking，不知道明天又会有什么新科技出现。
Day 2：科技改变世界第二天的Keynote仍然是在圣何塞市中心的McEnery会议中心举办。虽然Keynote要到10点开始，但是我住的公寓没有早餐，我和同事们还是8点刚过就跑到会场去“蹭饭”吃。
女性在科技界始终是“少数派”，所以Facebook特地在F8的第二天，在会场旁边的万豪酒店，举办了一个Women Breakfast的活动，邀请所有参与F8的女性，一起吃早餐，相互交流。
我们的一位产品经理也早早地去了会场参与这个活动。我想起，前一天的Keynote里，介绍4个核心产品的演讲者中，有3位都是女性，这让这个充满“科技感”的活动平添了一分人文的色彩。
免费早餐之后，大家的焦点又转移到了主会场。第二天的主题Keynote，不同于第一天以Facebook的产品为核心，而是集中在“技术”这个词上。
一般来说，第一天的Keynote关注的是最近这三五年来，Facebook的产品发展方向，那么第二天的Keynote的目标则放得更加长远，关注的是Facebook未来十年会关注和投入的技术。今年也毫不例外。Connectivity、机器学习、AR和VR，把整个会场带入了一个更有科技感的主题里去。
不过，今年的Keynote和往年的还是有点不一样。过去几年里，F8第二天的Keynote都显得更有“梦想”一些，比如通过无人机为经济不发达区域提供网络接入，研究怎么通过Reinforcement Learning让机器打《星际争霸》。
过去两年里，我们常常能看到一些或许挑战很大，但是却又容易让人憧憬的项目出现。而今年第二天的Keynote主题却和前一天环环相扣，专注在了“Responsible Innovation”这样一个主题上。
如何通过机器学习找出虚假账号，如何过滤仇恨言论，乃至如何解决网络霸凌，变成了一个个的机器学习案例，反复出现在今天的Keynote里。似乎Facebook是想更坚定地传达这样一个信息，“The Future is Private”，这件事情我们是认真的。
在整个Keynote的过程里面，也让大家看到了对于有害内容的过滤，从简单的关键词匹配进化到应用计算机视觉，直到今天使用的Nearest Neighbor Manifold Expansion &amp;amp; Multi-modal Understanding这样更加复杂的机器学习技术，一步一步是如何发展的。
Keynote结束之后，第二天的其他内容都安排得更加紧凑了一些。大部分的小会场都在午餐时间同时进行，内容也更加“硬核”。各个小会场里看到的不是产品更新，而是各种机器学习问题在Facebook的实际解决方法和应用。更有不少小会场里面，Facebook的工程师直接给大家展示了代码。有心想要了解一些特定问题的工程师，可以从这里面学到不少有用的东西。
除了“学习”之外，参加F8很重要的一个方面是社交。第二天的有些小会场是以座谈会的形式，邀请外部的开发者合作伙伴，来分享他们的成功案例。开发者之间的相互交流也更多了起来。
在所有内容结束之后，Facebook新加坡办公室的Partner Manager，带着我们移师会议中心附近的餐馆，开始了一个小小的After Party。我们一群来自五湖四海的华人，就在美国一边吃着墨西哥菜一边交流。“微信”是Facebook的开发者大会上始终绕不开的话题，Facebook自己的各类消息类产品，其实也一直在从微信里面汲取养分。
晚餐过后，今年的F8就算正式结束了。认识的新朋友，重新见面的老朋友，之后就又要各奔东西了。而我自己，打算在回程之前，跑一趟心心念念的计算机历史博物馆，去看看里面收藏的从ENIAC到现代计算机的经典型号，为今年的旅程画上一个完满的句号。
推荐阅读Facebook：全球最大社交网络，向未知转型</description></item><item><title>特别加餐_我的一天怎么过？</title><link>https://artisanbox.github.io/4/60/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/60/</guid><description>你好，我是徐文浩。专栏更新到50多篇，快要结束了。在进入实战篇之前，我想先和你分享一个专栏之外的话题，那就是我的一天是怎么过的。
为什么想写这篇文章呢？主要目的是“破除神话”。周围一些朋友说，你在创业很厉害；也有朋友说，你能写专栏很厉害。其实我觉得自己和大家一样，就是一个普普通通的工程师，每一天都是普通且忙碌的。同时，我也希望通过这篇文章，能够拉近和你的距离，在专栏快要完结之际，可以在未来和你有更多的交流。
作为一个工程师出身的创业者，很多人会好奇，我是不是还常常写代码？也有朋友看我一直出差，会问我现在主要精力是不是都在产品上了？还有，我究竟要花多少时间在写这个专栏上？
事实上，作为一个创业者，我很难给自己的工作划定个小小的范围，然后说，“看，这个就是我做的事情”。在公司里，我每天在做的，其实主要就是两件事情。一件事情，我称之为“让事情按次发生”，主要是规划和推动公司里想要做的事情，推动产品结合业务往前走。另一件事情，我称之为“面对问题，解决问题”，主要是给各种突发的、意料之外的问题找解决办法。
规划和推动产品的工作，往往时间安排上主动一些，我会尽可能找完整大块的时间来做。而解决问题的事情，往往就比较碎片化，只能时时响应处理。
很多学习专栏的同学，工作时间应该都不是非常久，还有不少属于自己的业余时间。对我来说，想有属于自己的时间，基本上是奢望了。特别是最近半年多时间，每天都要抽出时间来写专栏，睡眠时间都牺牲了不少。
当然，我和大部分同学以及其他专栏作者，在时间安排上，差异最大的一点是，我会比较频繁地去海外出差了。在国内的时候，我的时间安排通常还比较有规律，比如，下面是我最近在国内的一个周一。
1.周一一早9点刚到公司，我先会看看我们用作视频会议设备是否都连上了。虽然其实公司人还不多，但是因为主要是针对海外的业务，所以有马尼拉、曼谷、杭州、深圳四个办公地点，异地沟通成了一个很大的问题。通过发消息或者视频会议的方式，沟通效率仍然很低，所以我们干脆通过Facebook Portal群组聊天的方式，8小时“直播”各个办公室的情况。需要找另外一个办公室的同事的时候，对着视频会议的屏幕吼一声就是了。
2.9:45开始，我连续参加了两个小团队的站会。站会有对应负责的同学来主持推动，我主要是多听一听，大家是否遇到什么问题，以及需要什么样的支持。这里面的问题，可能来自内部的其他团队，也可能是需要问外部的客户、Facebook、合作方的各种问题。这一天很顺利，事情团队自运转就继续正常推进了我们的产品进度。
3.因为是周一，所以10:00开始，我会和各个团队的负责人开一个非业务内容的周会。因为最近在推动公司内部做好跨团队职责的协同，所以最近的重点是在做两件事情。一个是从后端的研发团队开始推进强流程的代码审核，目标是提升代码质量和长期的迭代速度。第二个是培养整个系统里各个非功能模块的首要负责人，主要是要把从云服务器管理、CDN、网络、监测等等非功能性的需求和职责划分给到更多不同的工程师，让他们各自负责之后，再做学习分享。这样可以让大家对整个系统的全貌有个了解，而不是只是把这些问题放在一两个资深的技术同学身上。
这一天里，我发现代码审核进展很慢，主要是大家都还是觉得这样会影响进度，但是我内心深处知道不是这么回事儿，因为从开始要做这个事情已经两三周过去了。所以，我就不再是“建议”，而是“强迫”团队开始做代码审核了。各种非功能性的“负责人”的分配倒是相对比较顺利。
4.我们通常开会都很短，三个会开完，也就是10:30这样子。不过因为是周一，所以接下来的主要时间还是在清理邮件。这里面既有来自外部客户和合作伙伴的问题，也有系统自动生成的各种报告。能直接回复的都会直接回复掉，不能直接回复的我会加到Microsoft TO-DO里面，作为待办事项列表。
5.基本上把邮件清理完了，也就到了中午。我一般不叫外卖，而是和同事们一起出门觅食。因为大部分时间都是在办公室里坐着，运动也少，所以除非是暴雨天气，我一定是要出去走动走动的。和不同的同事吃饭，聊两句生活，互相之间的距离也能拉近不少。
6.吃完午饭，我自己的常备节目是去买杯瑞幸或者全家的咖啡。通常也有不少同事会一起过去，不管买还是不买，都要溜个弯儿。我自己最近有点睡得少，不靠咖啡下午就会犯困。
7.之后回到办公室，想要开始写点代码。因为团队越来越大，所以现在我已经不写任何“必须要写”的代码了，避免自己的时间安排成为发布计划的瓶颈。不过，我还是尽可能会抽一些时间来写一点效率提升的代码。这天要写的，是答应了团队，把自动化滚动部署（Auto Rolling Update）的脚本给写了。不过，还没写多少，我们的产品经理YC就来找我一起和团队过新的OMS（订单管理系统）的产品评审。虽然作为程序员被打断总是会觉得很头疼，不过该过的事情还是要过。
8.等到产品评审走完，终于又有了点儿时间，重新开始写滚动部署的脚本。脚本写起来方便，测试起来却是非常麻烦，要频繁地开关虚拟机去做检查，也没有什么太好的办法做单元测试或者自动化测试。前前后后几个小时下去，终于把整个脚本调通。不过，我又在JIRA里面记了一串新的想法，主要是想要进一步把目前手动在云平台上创建负载均衡，后端服务的手工工作都自动化掉。
9.抬头一看，已经快晚上9点了，其实已经过了饭点儿了。办公室里也空了大半，于是干脆收拾好包出门吃饭回家。
10.回家刷了一会儿抖音，重新打开电脑，开始写专栏。专栏的工作量比想象中大不少，基本上写到12点、1点，除非已经是死线了。不然即使进度比想象中慢一点，我也会先去睡了，不然第二天效率更差。毕竟，明天我们又要开始创造明天么。
这就是我上周的一天，不知道和你想象中差别大吗？下次有机会，我会再写写我在海外出差的一天是什么过的。
最后，我想听你讲讲，你的一天是怎么过的呢？欢迎在留言区和同学们一起分享。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>用户故事_Jerry银银：这一年我的脑海里只有算法</title><link>https://artisanbox.github.io/2/79/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/79/</guid><description>比尔·盖茨曾说过：“如果你自以为是一个很好的程序员，请去读读Donald E. Knuth的《计算机程序设计艺术》吧……要是你真把它读下来了，就毫无疑问可以给我递简历了。”虽然比尔·盖茨推荐的是《计算机程序设计艺术》这本书，但是本质却折射出了算法的重要性。
大家好，我是Jerry银银，购买过算法专栏的同学应该时不时会看到我的留言！目前我是一名Android应用开发工程师，主要从事移动互联网教育软件的研发，坐标上海。
我为何要学算法？细细想来，从毕业到现在，7年多的时间，我的脑海里一直没有停止过思考这样一个问题：技术人究竟能够走多远，技术人的路究竟该如何走下去？相信很多技术人应该有同样的感受，因为技术的更新迭代实在是太快了，但是我心里明白：我得为长远做打算，否则，就算换公司、换工作，可能本质也不会有什么改变。
但是，我其实不太清楚自己到底应该往什么地方努力。于是，我翻阅了好多书籍，搜寻IT领域各种牛人的观点。多方比较之后，我终于决定，从基础开始，从计算机领域最基础、最重要的一门课开始。毫无疑问，这门课就是数据结构和算法。
我是如何遇见极客时间的？既然找到了方向，那就开始吧。可是问题来了，从哪儿开始呢？大方向虽然有了，可是具体的实现细节还是得慢慢摸索。大学没怎么学，工作这么多年也没有刻意练习，起初我还真不知道从哪儿开始，只是买了本书，慢慢地啃，也找了一些简单的题目开始做。有过自学经历的同学，应该有同感吧？刚开始连单链表翻转这样简单的题都要折腾半天，真心觉得“痛苦”。
之前我在极客时间上订阅过“Java核心技术36讲”，体会到了专栏和书本的不同。极客时间的专栏作者都是有着丰富的一线开发经验，能很好地把知识和实战结合在一起的大牛。这些课听起来非常爽。估计你应该经常跟我一样感叹：“哦！原来这些知识还可以这么使用！”当时我就在想，极客时间啥时候有一门算法课就好了。
说来真是巧，没多久，极客时间就推出了“数据结构与算法之美”。我试读了《为什么要学习数据结构和算法》和《数组：为什么很多编程语言中数组都从0开始编号？》这两篇之后，立即购买了。
到现在，专栏学完了，但是我依然记得，王争老师在《为什么要学习数据结构和算法》这篇文章里面提到的三句话，因为这每一句话都刺痛了我的小心脏！
第一句：业务开发工程师，你真的愿意做一辈子CRUD Boy吗？
第二句：基础架构研发工程师，写出达到开源水平的框架才是你的目标！
第三句：对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！
我每天是怎么学专栏的？于是，每天早上醒来，我的第一件事就是听专栏！专栏在每周的一、三、五更新，每周的这三天早上，我会听更新的文章。其它时间，我就听老的文章，当作复习。
听的过程，我一般会分这么几种情况。
第一种情况，更新的内容是我之前就已经学过的，基本已经掌握了的。这种情况下，听起来相对轻松点，基本上听一遍就够了。起床之后，再做一下老师给的思考题。这种情况在专栏的基础部分出现得比较多，像数组、链表、栈、队列、哈希表这些章节，我基本上都是这么过来的。
第二种情况，更新的内容是我学过的，但是还不太精通的。这种情况下，王争老师讲的内容都会将我的认知往前“推进”一步。顺利的话，我会在上班之前就搞懂今天更新的内容。这种情况是曾经没有接触过的内容，但是整体来说不难的理解的，比如跳表、递归等。
还有一种情况，就是听一遍不够，听完再看一遍也不行，上午上班之前也搞不定的。不过，我也不会急躁。我心里知道，我可能需要换换脑子，说不定，在上午工作期间，灵感会突然冒出来。这种情况一般出现在红黑树、字符串查找算法、动态规划这些章节。
到了中午休息时间，我会一个人在公司楼下转一圈，同样，还是听专栏、看专栏。
如果今天的文章，早上已经搞定了，我会重新看下其他同学的留言，看看其他同学是如何思考文章的课后思考题的，还有就是，我会看看其他同学学习过程中，会有哪些疑问，这些疑问自己曾经是否遇到过，现在是否已经完全解决了。
如果今天的文章，早上没有彻底搞懂，这种情况下，我会极力利用中午的时间去思考。
晚上的时间通常无法确定，我有时候会加班到很晚，回到家，再去啃算法，效率也不高。所以，我一般会在晚上“看”算法。为什么我会用双引号呢？是因为我真得只是“看”，目的就是加深印象。
以上基本是我工作日学专栏的“套路”。
等到了周末或者其它节假日，就是“打攻坚战”的时候了。估计很多上班族和我一样，只有周末才有大量集中思考的时间。这时候，我一般会通过做题来反向推动自己的算法学习。
像红黑树、Trie树、递归、动态规划这些内容，我都是在周末和节假日搞懂的。虽然到现在对其中一些知识还不能达到游刃有余的地步，但是对一般的问题，大体上我都知道该如何抽象、如何拆解了。
我在学习算法时记的笔记通过学习专栏，我有什么不一样的收获？首先，专栏学习拓宽了我的知识面。例如，很多书本不讲的跳表，王争老师用了一篇文章来讲解。犹记得当我看完跳表时，心想，这么简单、易懂、高效的数据结构，为什么很多书籍都没有呢？这个专栏真的买值了！
其次，专栏的理论和实践结合很强。书籍是通用性很强的教材，一般很少会涉及软件系统是如何使用具体的数据结构和算法的。在专栏中，老师把对应的知识和实践相互结合，听起来特别过瘾！比如堆这种数据结构，理解起来不难，但是要用好它，还得下点功夫，经过老师一讲解，搭配音频，我的理解也变得更加深入了。
最后，专栏留言这个功能真的太好了，为自学带来了诸多便利，也让我获得了很多正向反馈。很多时候，经过相当长的一段时间思考，还是不能打通任督二脉，其实后来回想，当时就差那一层窗户纸了。于是，我在文末留下了自己的疑问，结果王争老师轻描淡写一句话我就明白了。
留言功能还有个非常大的好处。如果你用心学习，用心思考，用心留言，你的留言很大概率会被同伴点赞，很多时候还能被置顶。这本身就是一种正向反馈，也会更加促进自己的学习动力。还有一种更爽的体验，突然有一天早上，我照例醒来听专栏，突然听到了自己的名字。这个专栏4万多人订阅，老师居然记得我！可见王争老师真的认真看了每一条留言。
最后，我总结下自己学这个专栏的收获。尽管很多，但是我想用三句话来概括。
第一，写代码的时候，我会很自然地从时间和空间角度去衡量代码的优劣，时间、空间意识被加强了很多。
第二，学习算法的过程，有很多的“痛苦”，也正是因为这些“痛苦”，我学到了很多知识以外的东西。
第三，过程可能比知识更重要。要从过程中体会成长和精进的乐趣，而知识是附加产品！
专栏虽然结束，但是学习并没有结束。同学们，我们开头见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>用户故事_zixuan：站在思维的高处，才有足够的视野和能力欣赏“美”</title><link>https://artisanbox.github.io/2/80/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/80/</guid><description>大家好，我是zixuan，在一家国内大型互联网公司做后端开发，坐标深圳，工作5年多了。今天和大家分享一下，我学习专栏的一些心得体会。
随着年龄的增长，我经历了不少业务、技术平台、中间件等多种环境和编程工具的迭代变更。与此同时，我越来越意识到，要做一名优秀的程序员，或者说，能够抵御年龄增长并且增值的程序员，有两样内功是必须持续积累的，那就是软件工程经验方法和算法应用能力。
通俗地讲，就是不论在什么系统或业务环境下、用什么编程工具，都能写出高质量、可维护、接口化代码的能力，以及分解并给出一个实际问题有效解决方案的能力。
我为什么会订阅这个专栏？这也是为什么我在极客时间上看到王争老师的“数据结构与算法之美”的开篇词之后，果断地加入学习行列。同时，我也抱有以下两点期望。
第一，这个专栏是从工程应用，也就是解决实际问题的角度出发来讲算法的，原理和实践相辅相成，现学现用，并且重视思考过程。从我个人经验来看，这的确是比较科学的学习方法。我相信很多人和我一样，以前在学校里都学过算法，不过一旦不碰书了，又没有了应用场景后，很快就把学过的东西丢了，重新拾起来非常困难。
第二，从专栏的标题看出，王争老师试图带我们感受算法的“美”，那必将要先引导我们站在思维的高处，这样才有足够的视野和能力去欣赏这种“美”。我很好奇他会怎么做，也好奇我能否真正地改变以前的认知，切身地感受到“美”。
我是如何学习这个专栏的？就这样，同时带着笃定和疑问，我上路了。经过几个月的认真学习，“数据结构与算法之美”成了我在极客时间打开次数最多，花费时间最多，完成度也最高的一门课。尽管如此，我觉得今后我很可能还会再二刷、多刷这门课，把它作为一个深入学习的索引入口。 接下来，我就从几个方面，跟你分享下，这半年我学习这个专栏的一些感受和收获。
1.原理和实用并重：从实践中总结，应用到实践中去学习的最终目的是为了解决实际问题，专栏里讲的很多方法甚至代码，都能够直接应用到大型项目中去，而不仅仅是简单的原理示例。
比如王争老师在讲散列表的时候，讲了实现一个工业级强度的散列表有哪些需要注意的点。基本上面面俱到，我在很多标准库里都找到了印证。再比如，老师讲的LRU Cache、Bloom Filter、范围索引上的二分查找等等，也基本和我之前阅读LevelDB源代码时，看到的实现细节如出一辙，无非是编程语言的差别。
所以，看这几部分的时候，我觉得十分惊喜，因为我经历过相关的实际应用场景。反过来，专栏这种原理和实用并重的风格，也能帮助我今后在阅读开源代码时提升效率、增进理解。
另外，我觉察到，文章的组织结构，应该也是老师试图传达给我们的“他自己的学习方法”：从开篇介绍一个经典的实际问题开始（需求），到一步步思考引导（分析），再到正式引出相关的数据结构和算法（有效解决方案），再将其应用于开篇问题的解决（实现、测试），最后提出一个课后思考题（泛化、抽象、交流、提升）。
这个形式其实和解决实际工程问题的过程非常类似。我想，大部分工程师就是在一个个这样的过程中不断积累和提升自己的，所以我觉得这个专栏，不论是内容还是形式真的都很赞。
2. 学习新知识的角度：体系、全面、严谨、精炼，可视化配图易于理解“全面”并不是指所有细节面面俱到。事实上，由于算法这门学科本身庞大的体量，这类专栏一般只能看作一个丰富的综述目录，或者深入学习的入口。尽管如此，王争老师依然用简洁精炼的语言Cover到了几乎所有最主要的数据结构和算法，以及它们背后的本质思想、原理和应用场景，知识体系结构全面完整并自成一体。
我发现只要能紧跟老师的思路，把每一节的内容理解透彻，到了语言实现部分，往往变成了一种自然的总结描述，所以代码本身并不是重点，重点是背后的思路。
例如，KMP单模式串匹配和AC自动机多模式串匹配算法是我的知识盲区。以前读过几次KMP的代码，都没完全搞懂，于是就放弃了。至于AC自动机，惭愧地说，我压根儿就没怎么听说过。
但是，在专栏里，王争老师从BruteForce方法讲起，经过系统的优化思路铺垫，通俗的举例，再结合恰到好处的配图，最后给出精简的代码。我跟随着老师一路坚持下来，当我看到第二遍时突然就豁然开朗了。而当我真正理解了AC自动机的构建和工作原理之后，在某一瞬间，我的内心的确生出了一种美的感觉（或者更多的是“妙”吧？）。
AC自动机构建的代码，让我不自觉地想到“编织”这个词。之前还觉得凌乱的、四处喷洒的指针，在这里一下子变成了一张有意义的网，编织的过程和成品都体现出了算法的巧妙。这类联想无疑加深了我对这类算法的理解，也许这也意味着，我可以把它正式加入到自己的算法工具箱里了。
另外一个例子是动态规划。以前应用DP的时候，我常常比较盲目，不知道怎么确定状态的表示，甚至需要几维的状态都不清楚，可以说是在瞎猜碰运气。经过老师从原理到实例的系统讲解后，我现在明白，原来DP本质上就是在压缩重复子问题，状态的定义可以通过最直接的回溯搜索来启发确定。明白这些之后，动态规划也被我轻松拿下了。
3. 已有知识加深的角度：促进思考，连点成线之前看目录的时候，我发现专栏里包含了不少我已经知道的知识。但真正学习了之后，我发现，以前头脑中的不少概念知识点，是相对独立存在的，基本上一个点就对应固定的那几个场景，而在专栏里，王争老师比较注重概念之间的相互关联。对于这些知识，经过王争老师的讲解，基本可以达到交叉强化理解，甚至温故知新的效果。
比如老师会问你，在链表上怎么做二分查找？哈希和链表为什么经常在一起出现？这些问题我之前很少会考虑到，但是当我看到的时候，却启发出很多新的要点和场景（比如SkipList、LRUCache）。
更重要的是，跟着专栏学习一段时间之后，我脑中原本的一些旧概念，也开始自发地建立起新的连接，连点成线，最后产生了一些我之前从未注意到的想法。
举个感触最深的例子。在跟随专栏做了大量递归状态跟进推演，以及递归树分析后，我现在深刻地认识到，递归这种编程技巧背后，其实是树和堆栈这两种看似关联不大的数据结构。为什么这么说呢？
堆栈和树在某个层面上，其实有着强烈的对应关系。我刚接触递归的时候，和大多数初学者一样，脑子很容易跟着机器执行的顺序往深里绕，就像Debug一个很深的函数调用链一样，每遇到一个函数就step into，也就是递归函数展开-&amp;gt;下一层-&amp;gt;递归函数展开-&amp;gt;下一层-&amp;gt;…，结果就是只有“递”，没有“归”，大脑连一次完整调用的一半都跑不完（或者跑完一次很辛苦），自然就会觉得无法分析。如下图，每个圈代表在某一层执行的递归函数，向下的箭头代表调用并进入下一层。
我初学递归时遇到的问题：有去无回，陷得太深随着我处理了越来越多的递归，我慢慢意识到，为什么人的思考一定要follow机器的执行呢？在递归函数体中，我完全可以不用每遇到递归调用都展开并进入下一层（step into），而是可以直接假定下一层调用能够正确返回，然后我该干嘛就继续干嘛（step over），这样的话，我只需要保证最深一层的逻辑，也就是递归的终止条件正确即可。
原因也很简单，不管在哪一层，都是在执行递归函数这同一份代码，不同的层只有一些状态数据不同而已，所以我只需要保证递归函数代码逻辑的正确性，就确保了运行时任意一层的结果正确性。像这样说服自己可以随时step over后，我的大脑终于有“递”也有“归”了，后续事务也就能够推动了。
有一定经验后我如何思考递归：有去有回，自由把握最近在学习这门课程的过程中，我进一步认识到，其实上面两个理解递归的方式，分别对应递归树的深度遍历和广度遍历。尽管机器只能按照深度优先的方式执行递归代码，但人写递归代码的时候更适合用广度的思考方式。当我在实现一个递归函数的时候，其实就是在确定这棵树的整体形状：什么时候终止，什么条件下生出子树，也就是说我实际上是在编程实现一棵树。
那递归树和堆栈又有什么关系呢？递归树中从根节点到树中任意节点的路径，都对应着某个时刻的函数调用链组成的堆栈。递归越深的节点越靠近栈顶，也越早返回。因而我们可以说，递归的背后是一棵树，递归的执行过程，就是在这棵树上做深度遍历的过程，每次进入下一层（“递”）就是压栈，每次退出当前层（“归”）就是出栈。所有的入栈、出栈形成的脉络就组成了递归树的形态。递归树是静态逻辑背景，而当前活跃堆栈是当前动态运行前景。
学完专栏后我怎么看待递归：胸有成“树”，化动为静这样理解之后，编写或阅读递归代码的时候，我真的能够站得更高，看得更全面，也更不容易掉入一些细节陷阱里去了。
说到这里，我想起之前在不同时间做过的两道题，一道是计算某个长度为n的入栈序列可以有多少种出栈序列，另一道是计算包含n个节点的二叉树有多少种形状。我惊讶地发现，这两个量竟然是相等的（其实就是卡特兰数）。当时我并不理解为什么栈和树会存在这种关联，现在通过类似递归树的思路我觉得我能够理解了，那就是每种二叉树形状的中序遍历都能够对应上一种出栈顺序。
类似这样“旧知识新理解”还有很多，尽管专栏里并没有直接提到，但是这都是我跟随专栏，坚持边学边思考，逐步感受和收获的。
总结基于以上谈的几点收获和感受，我再总结下我认为比较有用的、学习这个专栏的方法。
1.紧跟老师思路走，尽量理解每一句话、每一幅配图，亲手推演每一个例子。
王争老师语言精炼。有些文字段落虽短，但背后的信息量却很大。为了方便我们理解，老师用了大量的例子和配图来讲解。即便是非常复杂、枯燥的理论知识，我们理解起来也不会太吃力。
当然有些地方确实有点儿难，这时我们可以退而求其次，“先保接口，再求实现”。例如，红黑树保持平衡的具体策略实现，我跟不下来，就暂时跳过去了，但是我只要知道，它是一种动态数据的高效平衡树，就不妨碍我先使用这个工具，之后再慢慢理解。
2.在学的过程中回顾和刷新老知识点，并往工程实践上靠。学以致用是最高效的方法。
3.多思考，思考比结果重要；多交流，亲身感受和其他同学一起交流帮助很大。
最后，感谢王争老师和极客时间，让我在这个专栏里有了不少新收获。祝王争老师事业蒸蒸日上，继续开创新品，也希望极客时间能够联合更多的大牛老师，开发出更多严谨又实用的精品课程！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>用户故事_赵文海：怕什么真理无穷，进一寸有一寸的欢喜</title><link>https://artisanbox.github.io/4/62/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/62/</guid><description>大家好，我是赵文海，一名Android开发仔，坐标北京，目前工作刚满一年，在这里分享一下自己学习“深入浅出计算机组成原理”专栏的心得。
为什么要学计算机组成原理？一直以来我心里都有一个念想，就是好好把计算机基础知识补一补，原因有两个。
第一，我不是计算机专业的，如果连基础知识都不熟悉，那怎么与科班出身的同事交流呢？虽然我目前的工作主要是在业务层进行开发，涉及基础知识的场景其实并不多，但是，既然我要在程序员这个行业长久地走下去，我觉得自己还是有必要补一下基础知识。
第二，虽然现在各种新框架、新技术层出不穷，但它们的根基其实还是那些基础知识。我们每个人的精力有限，整天追随这些“新”的东西，在我看来并不是一个很明智的选择。相反，正所谓“磨刀不误砍柴工”，如果我把先基础知识掌握好，那学习和了解那些应用层的框架应该会更容易一点。
所以，我给自己设定了两个学习方向，一是深入学习移动端开发相关技术，比如，学习Android 系统知识、深入了解一些框架、接触 Flutter 这类跨平台技术；二是学习计算机基础知识，然后再随着工作慢慢深入去学习移动端开发技术。
正好那时候极客时间出了很多基础课程，比如王争老师的“数据结构与算法之美”、刘超老师的“趣谈网络协议”等等。我是先从数据结构与算法开始学的，后面又学了一些网络协议的知识，然后才开始学习徐文浩老师的“深入浅出计算机组成原理”。
我记得徐老师在开篇词里写过这么一段话：
正所谓“练拳不练功，到老一场空”。如果越早去弄清楚计算机的底层原理，在你的知识体系中储备这些知识，也就意味着你有越长的时间来收获学习的“利息”。虽然一开始可能不起眼，但是随着时间带来的复利效应，你的长线投资项目，就能让你在成长的过程中越走越快。
这段话和我的想法不谋而合，也给了我极大的鼓舞，我学习基础知识的决心也更加坚定了 。
我是怎么学习专栏的？刚开始看的时候，专栏已经更新了二十多讲，但我没有想要赶快跟上老师的步伐，就希望自己每天都能坚持看一讲。
我一般都是在晚上下班回家后看专栏，也有时候是早上到公司后，因为住的地方离公司比较近，还有时候一个人出去坐地铁也会拿出来看一会儿。
最开始由于我过于自信了，我想着只是把文章看了一遍就可以了。后来过了一周，我发现看完的东西，很快就没有印象了。我当时就想，这样可不行啊！不知道你是不是经常有这种感受，费很大劲搞懂的东西，结果因为只看了一遍，没有及时复习，很快就又忘记了。于是，后面每篇文章，我至少都会看两遍。第一遍认真阅读思考，第二遍、第三遍作为复习巩固。
另外，学习这个专栏时我没有做笔记，因为我觉得老师的文章不长，而且言语足够简练，没有必要自己再提炼一次。毕竟我每天都会打开极客时间，如果碰到哪里想不起来了，就直接再看一遍文章就好了，也花不了多长时间。
当然，记不记笔记是个人喜好，如果时间充裕，你也可以选择通过做笔记来加深印象。极客时间的划线笔记功能也是极好的，看到有问题或者非常好的地方，直接记录下来，方便以后查阅、学习。
就这样，保持一天一节的速度，慢慢我就赶上了老师更新的步伐，后面的文章基本就是更新当天就看完。
虽然进度跟上了，但是老师文章后面附的书籍我目前并没有去读。作为一个非科班出身的工作党，平时时间并不宽裕，掌握老师每一节的主要知识，已经挺不容易了。
这里特别说一下，徐老师每篇文章下的推荐阅读，是我个人最喜欢这个专栏的地方。徐老师每次都会在文章结尾列出相关书籍的对应章节、相关的博客或论文，这为我后面深入学习相关知识提供了很大的便利。
因此，关于这一块内容我是这么打算的。我准备学完第一遍之后，仔细去读一读老师推荐的书籍。这样有了第一遍的铺垫，读起老师推荐的书和论文，不至于那么困难和恐惧。读书的时候还可以结合书中内容再复习一遍专栏，到时候肯定会有新的收获。毕竟，基础知识的学习是一个长期积累、慢慢参悟、螺旋上升的过程，我已经做好了打持久战的准备。
学习专栏有什么收获？徐老师的文章长度适中、图文并茂、言简意赅。在解释一些名词和概念的时候，徐老师经常拿生活中我们熟悉的事物来举例。我觉得这一点非常好。
比如，他把电路组装看成“搭乐高积木”、把动态链接比喻成程序内部的“共享单车”、把总线比喻成计算机里的“高速公路”，这很容易让我这种非科班的同学，对陌生概念迅速建立起一个初步印象。当然，能把概念解释地如此清晰和“接地气”，也反映了老师的深厚功底。
通过专栏的学习，我对计算机的CPU、内存、I/O设备以及它们之间的通信有了初步的了解。
另外，像 GPU、TPU相关的章节也让我开拓了眼界，比如 GPU 那一节老师就讲到了计算机图形渲染的流程，这些知识是我之前从未接触过的。
同时，专栏还有很多很实用的章节，比如讲 CPU 的高速缓存时，讲到了 Java 中的 volatile 关键字的作用，这些都可以直接运用到实际的工作或面试中。
不过，除了计算机组成原理的知识外，我还有其他的收获，在我看来这些收获甚至比那些知识还重要。
首先，就是克服了对于基础知识的恐惧。
我之前觉得基础知识是晦涩难懂的，像计算机组成原理、网络协议、操作系统，这些课听起来就觉得很难。开始学习之前，心里总是怕自己理解不了或者坚持不下来，但是通过学习专栏，我发现它们并没有想象中那么可怕，很多技术灵感其实就是源于我们的生活实践。先去学，然后慢慢就能发现其中有趣的地方。
比如，CPU分支预测就和我们天气预测有相似之处。文章里穿插的历史知识也让我意识到，这些知识虽然看似高深，但也是无数前辈经历很长时间、很多次失败才慢慢积累下来的，学习这些知识，就是站在巨人的肩膀上，体会他们思考和实践的过程。
还有就是，我意识到了持续学习的重要性。
徐老师在第2讲时写过，“我工作之后一直在持续学习，在这个过程中，我发现最有效的方法，不是短时间冲刺，而是有节奏的坚持。”我对这句话真是深有感触，所以一直记到现在，估计你也是吧？
总结通过专栏的学习，我确实收获了很多，真的非常感谢徐文浩老师的付出，也感谢极客时间推出了这么多实用的基础课程。
其实，尽管毕业之后工作才半年左右，但是我的心里其实挺焦虑的，主要是担心自己在如此快速的技术变革中，跟不上变化，慢慢被淘汰。在这个过程中，我也思考了很多。通过学习专栏，我的焦虑情绪也化解了很多。在这里我也想说说我对于焦虑的看法。
我觉得人之所以会感到焦虑，是因为有上进心，说白了就是觉得自己不够好。这其实是一件好事，正是因为觉得自己不够好，我们才能产生变得更好的想法，进而找到变得更好的方法，所以我们要正确看待焦虑这种情绪。而缓解焦虑的方式很简单，就是行动。担心自己长胖，那就去锻炼；担心自己被淘汰，那就去学习。
我特别喜欢胡适说的一句话：“怕什么真理无穷，进一寸有一寸的欢喜”。
用这句话与各位共勉吧，希望我们都能抱着长线投资的心态，坚持下去，和时间做朋友，不要急躁。虽然学完这些基础知识，老板也看不到，短时间内也不会加薪升职，但我相信，它们会在未来的某个时间回馈你，让你知道现在的决定是正确的，现在的付出是值得的！
好了，我想要分享的内容就是这些，不知道你学习这个专栏的过程是怎样的呢？有没有什么独特的学习方法和心路历程呢？欢迎你写在留言区，我们一起分享，相互鼓励，共同进步！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>直播回顾_林晓斌：我的MySQL心路历程</title><link>https://artisanbox.github.io/1/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/48/</guid><description>在专栏上线后的11月21日，我来到极客时间做了一场直播，主题就是“我的MySQL心路历程”。今天，我特意将这个直播的回顾文章，放在了专栏下面，希望你可以从我这些年和MySQL打交道的经历中，找到对你有所帮助的点。
这里，我先和你说一下，在这个直播中，我主要分享的内容：
我和MySQL打交道的经历；
你为什么要了解数据库原理；
我建议的MySQL学习路径；
DBA的修炼之道。
我的经历以丰富的经历进入百度我是福州大学毕业的，据我了解，那时候我们学校的应届生很难直接进入百度，都要考到浙江大学读个研究生才行。没想到的是，我投递了简历后居然进了面试。
入职以后，我跑去问当时的面试官，为什么我的简历可以通过筛选？他们说：“因为你的简历厚啊”。我在读书的时候，确实做了很多项目，也实习过不少公司，所以简历里面的经历就显得很丰富了。
在面试的时候，有个让我印象很深刻的事儿。面试官问我说，你有这么多实习经历，有没有什么比较好玩儿的事？我想了想答道，跟你说个数据量很大的事儿 ，在跟移动做日志分析的时候我碰到了几千万行的数据。他听完以后就笑了。
后来，我进了百度才知道，几千万行那都是小数据。
开始尝试看源码解决问题加入百度后，我是在贴吧做后端程序，比如权限系统等等。其实很简单，就是写一个C语言程序，响应客户端请求，然后返回结果。
那个时候，我还仅仅是个MySQL的普通用户，使用了一段时间后就出现问题了：一个跑得很快的请求，偶尔会又跑得非常慢。老板问这是什么原因，而我又不好意思说不知道，于是就自己上网查资料。
但是，2008年那会儿，网上资料很少，花了挺长时间也没查出个所以然。最终，我只好去看源码。翻到源码，我当时就觉得它还蛮有意思的。而且，源码真的可以帮我解决一些问题。
于是一发不可收拾，我从那时候就入了源码的“坑”。
混社区分享经验2010年的时候，阿里正好在招数据库的开发人员。虽然那时我还只是看得懂源码，没有什么开发经验，但还是抱着试试看的态度投了简历。然后顺利通过了面试，成功进入了阿里。之后，我就跟着褚霸（霸爷）干了7年多才离开了阿里。
在百度的时候，我基本上没有参加过社区活动。因为那时候百度可能更提倡内部分享，解决问题的经验基本上都是在内网分享。所以，去了阿里以后，我才建了博客、开了微博。我在阿里的花名叫丁奇，博客、微博、社区也因此都是用的这个名字。
为什么要了解数据库原理？这里，我讲几个亲身经历的事情，和你聊聊为什么要了解数据库原理。
了解原理能帮你更好地定位问题一次同学聚会，大家谈起了技术问题。一个在政府里的同学说，他们的系统很奇怪，每天早上都得重启一下应用程序，否则就提示连接数据库失败，他们都不知道该怎么办。
我分析说，按照这个错误提示，应该就是连接时间过长了，断开了连接。数据库默认的超时时间是8小时，而你们平时六点下班，下班之后系统就没有人用了，等到第二天早上九点甚至十点才上班，这中间的时间已经超过10个小时了，数据库的连接肯定就会断开了。
我当时说，估计这个系统程序写得比较差，连接失败也不会重连，仍然用原来断掉的连接，所以就报错了。然后，我让他回去把超时时间改得长一点。后来他跟我说，按照这个方法，问题已经解决了。
由此，我也更深刻地体会到，作为开发人员，即使我们只知道每个参数的意思，可能就可以给出一些问题的正确应对方法。
了解原理能让你更巧妙地解决问题我在做贴吧系统的时候，每次访问页面都要请求一次权限。所以，这个请求权限的请求，访问概率会非常高，不可能每次都去数据库里查，怎么办呢？
我想了个简单的方案：在应用程序里面开了个很大的内存，启动的时候就把整张表全部load到内存里去。这样再有权限请求的时候，直接从内存里取就行了。
数据库重启时，我的进程也会跟着重启，接下来就会到数据表里面做全表扫描，把整个用户相关信息全部塞到内存里面去。
但是，后来我遇到了一个很郁闷的情况。有时候MySQL 崩溃了，我的程序重新加载权限到内存里，结果这个select语句要执行30分钟左右。本来MySQL正常重启一下是很快的，进程重启也很快，正常加载权限的过程只需要两分钟就跑完了。但是，为什么异常重启的时候就要30分钟呢？
我没辙了，只好去看源码。然后，我发现MySQL有个机制，当它觉得系统空闲时会尽量去刷脏页。
具体到我们的例子里，MySQL重启以后，会执行我的进程做全表扫描，但是因为这个时候权限数据还没有初始化完成，我的Server层不可能提供服务，于是MySQL里面就只有我那一个select全表扫描的请求，MySQL就认为现在很闲，开始拼命地刷脏页，结果就吃掉了大量的磁盘资源，导致我的全表扫描也跑得很慢。
知道了这个机制以后，我就写了个脚本，每隔0.5秒发一个请求，执行一个简单的SQL查询，告诉数据库其实我现在很忙，脏页刷得慢一点。
脚本一发布使用，脏页果然刷得慢了，加载权限的扫描也跑得很快了。据说我离职两年多以后，这个脚本还在用。
你看，如果我们懂得一些参数，并可以理解这些参数，就可以做正确的设置了。而如果我们进一步地懂得一些原理，就可以更巧妙地解决问题了。
看得懂源码让你有更多的方法2012年的时候，阿里双十一业务的压力比较大。当时还没有这么多的SSD，是机械硬盘的时代。
为了应对压力我们开始引入SSD，但是不敢把SSD直接当存储用，而是作为二级缓存。当时，我们用了一个叫作Flashcache的开源系统（现在已经是老古董级别了，不知道你有没有听过这个系统）。
Flashcache实现，把SSD当作物理盘的二级缓存，可以提升性能。但是，我们自己部署后发现性能提升的效果没有预想的那么好，甚至还不如纯机械盘。
于是，我跟霸爷就开始研究。霸爷负责分析Flashcache的源码，我负责分析MySQL源码。后来我们发现Flashcache是有脏页比例的，当脏页比例到了80%就会停下来强行刷盘。
一开始我们以为这个脏页比例是全部的20%，看了源码才知道，原来它分了很多个桶，比如说一个桶20M，这个桶如果用完80%，它就认为脏页满了，就开始刷脏页。这也就意味着，如果你是顺序写的话，很容易就会把一个桶写满。
知道了这个原理以后，我就把日志之类顺序写的数据全都放到了机械硬盘，把随机写的数据放到了Flashcache上。这样修改以后，效果就好了。
你看，如果能看得懂源码，你的操作行为就会不一样。
MySQL学习路径说到MySQL的学习路径，其实我上面分享的这些内容，都可以归结为学习路径。
首先你要会用，要去了解每个参数的意义，这样你的运维行为（使用行为）就会不一样。千万不要从网上拿了一些使用建议，别人怎么用，你就怎么用，而不去想为什么。再往后，就要去了解每个参数的实现原理。一旦你了解了这些原理，你的操作行为就会不一样。 再进一步，如果看得懂源码，那么你对数据库的理解也会不一样。
再来讲讲我是怎么带应届生的。实践是很好的学习方式，所以我会让新人来了以后先搭主备，然后你就会发现每个人的自学能力都不一样。比如遇到有延迟，或者我们故意构造一个主备数据不一致的场景，让新人了解怎么分析问题，解决问题。
如果一定要总结出一条学习路径的话，那首先要会用，然后可以发现问题。
在专栏里面，我在每篇文章末尾，都会提出一个常见问题，作为思考题。这些问题都不会很难，是跟专栏文章挂钩、又是会经常遇到的，但又无法直接从文章里拿到答案。
我的建议是，你可以尝试先不看答案自己去思考，或者去数据库里面翻一翻，这将会是一个不错的过程。
再下一步就是实践。之后当你觉得开始有一些“线”的概念了，再去看MySQL的官方手册。在我的专栏里，有人曾问我要不要直接去看手册？
我的建议是，一开始千万不要着急看手册，这里面有100多万个英文单词，你就算再厉害，也是看了后面忘了前面。所以，你一定要自己先有脉络，然后有一个知识网络，再看手册去查漏补缺。
我自己就是这么一路走过来的。
另外，在专栏的留言区，很多用户都希望我能推荐一本书搭配专栏学习。如果只推荐一本的话，我建议你读一下《高性能MySQL》这本书，它是MySQL这个领域的经典图书，已经出到第三版了，你可以想象一下它的流行度。
这本书的其中两位译者（彭立勋、翟卫祥）是我原团队的小伙伴，有着非常丰富的MySQL源码开发经验，他们对MySQL的深刻理解，让这本书保持了跟原作英文版同样高的质量。
极客时间的编辑说，他们已经和出版社沟通，为我们专栏的用户争取到了全网最低价，仅限3天，你可以直接点击链接购买。
DBA的修炼DBA和开发工程师有什么相同点？我带过开发团队，也带过DBA团队，所以可以分享一下这两个岗位的交集。
其实，DBA本身要有些开发底子，比如说做运维系统的开发。另外，自动化程度越高，DBA的日常运维工作量就越少，DBA得去了解开发业务逻辑，往业务架构师这个方向去做。
开发工程师也是一样，不能所有的问题都指望DBA来解决。因为，DBA在每个公司都是很少的几个人。所以，开发也需要对数据库原理有一定的了解，这样向DBA请教问题时才能更专业，更高效地解决问题。
所以说，这两个岗位应该有一定程度的融合，即：开发要了解数据库原理，DBA要了解业务和开发。
DBA有前途吗？这里我要强调的是，每个岗位都有前途，只需要根据时代变迁稍微调整一下方向。
像原来开玩笑说DBA要体力好，因为得搬服务器。后来DBA的核心技能成了会搭库、会主备切换，但是现在这些也不够用了，因为已经有了自动化系统。
所以，DBA接下来一方面是要了解业务，做业务的架构师；另一方面，是要有前瞻性，做主动诊断系统，把每个业务的问题挑出来做成月报，让业务开发去优化，有不清楚的地方，开发同学会来找你咨询。你帮助他们做好了优化之后，可以把优化的指标呈现出来。这将很好地体现出你对于公司的价值。
有哪些比较好的习惯和提高SQL效率的方法？这个方法，总结起来就是：要多写SQL，培养自己对SQL语句执行效率的感觉。以后再写或者建索引的时候，知道这个语句执行下去大概的时间复杂度，是全表扫描还是索引扫描、是不是需要回表，在心里都有一个大概的概念。
这样每次写出来的SQL都会快一点，而且不容易犯低级错误。这也正式我开设这个专栏的目标。</description></item><item><title>第10讲_UDP协议：因性善而简单，难免碰到“城会玩”</title><link>https://artisanbox.github.io/5/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/1/</guid><description>讲完了IP层以后，接下来我们开始讲传输层。传输层里比较重要的两个协议，一个是TCP，一个是UDP。对于不从事底层开发的人员来讲，或者对于开发应用的人来讲，最常用的就是这两个协议。由于面试的时候，这两个协议经常会被放在一起问，因而我在讲的时候，也会结合着来讲。
TCP和UDP有哪些区别？ 一般面试的时候我问这两个协议的区别，大部分人会回答，TCP是面向连接的，UDP是面向无连接的。
什么叫面向连接，什么叫无连接呢？在互通之前，面向连接的协议会先建立连接。例如，TCP会三次握手，而UDP不会。为什么要建立连接呢？你TCP三次握手，我UDP也可以发三个包玩玩，有什么区别吗？
所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。
例如，TCP提供可靠交付。通过TCP连接传输的数据，无差错、不丢失、不重复、并且按序到达。我们都知道IP包是没有任何可靠性保证的，一旦发出去，就像西天取经，走丢了、被妖怪吃了，都只能随它去。但是TCP号称能做到那个连接维护的程序做的事情，这个下两节我会详细描述。而UDP继承了IP包的特性，不保证不丢失，不保证按顺序到达。
再如，TCP是面向字节流的。发送的时候发的是一个流，没头没尾。IP包可不是一个流，而是一个个的IP包。之所以变成了流，这也是TCP自己的状态维护做的事情。而UDP继承了IP的特性，基于数据报的，一个一个地发，一个一个地收。
还有TCP是可以有拥塞控制的。它意识到包丢弃了或者网络的环境不好了，就会根据情况调整自己的行为，看看是不是发快了，要不要发慢点。UDP就不会，应用让我发，我就发，管它洪水滔天。
因而TCP其实是一个有状态服务，通俗地讲就是有脑子的，里面精确地记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点儿都不行。而UDP则是无状态服务。通俗地说是没脑子的，天真无邪的，发出去就发出去了。
我们可以这样比喻，如果MAC层定义了本地局域网的传输行为，IP层定义了整个网络端到端的传输行为，这两层基本定义了这样的基因：网络传输是以包为单位的，二层叫帧，网络层叫包，传输层叫段。我们笼统地称为包。包单独传输，自行选路，在不同的设备封装解封装，不保证到达。基于这个基因，生下来的孩子UDP完全继承了这些特性，几乎没有自己的思想。
UDP包头是什么样的？ 我们来看一下UDP包头。
前面章节我已经讲过包的传输过程，这里不再赘述。当我发送的UDP包到达目标机器后，发现MAC地址匹配，于是就取下来，将剩下的包传给处理IP层的代码。把IP头取下来，发现目标IP匹配，接下来呢？这里面的数据包是给谁呢？
发送的时候，我知道我发的是一个UDP的包，收到的那台机器咋知道的呢？所以在IP头里面有个8位协议，这里会存放，数据里面到底是TCP还是UDP，当然这里是UDP。于是，如果我们知道UDP头的格式，就能从数据里面，将它解析出来。解析出来以后呢？数据给谁处理呢？
处理完传输层的事情，内核的事情基本就干完了，里面的数据应该交给应用程序自己去处理，可是一台机器上跑着这么多的应用程序，应该给谁呢？
无论应用程序写的使用TCP传数据，还是UDP传数据，都要监听一个端口。正是这个端口，用来区分应用程序，要不说端口不能冲突呢。两个应用监听一个端口，到时候包给谁呀？所以，按理说，无论是TCP还是UDP包头里面应该有端口号，根据端口号，将数据交给相应的应用程序。
当我们看到UDP包头的时候，发现的确有端口号，有源端口号和目标端口号。因为是两端通信嘛，这很好理解。但是你还会发现，UDP除了端口号，再没有其他的了。和下两节要讲的TCP头比起来，这个简直简单得一塌糊涂啊！
UDP的三大特点 UDP就像小孩子一样，有以下这些特点：
第一，沟通简单，不需要一肚子花花肠子（大量的数据结构、处理逻辑、包头字段）。前提是它相信网络世界是美好的，秉承性善论，相信网络通路默认就是很容易送达的，不容易被丢弃的。
第二，轻信他人。它不会建立连接，虽然有端口号，但是监听在这个地方，谁都可以传给他数据，他也可以传给任何人数据，甚至可以同时传给多个人数据。
第三，愣头青，做事不懂权变。不知道什么时候该坚持，什么时候该退让。它不会根据网络的情况进行发包的拥塞控制，无论网络丢包丢成啥样了，它该怎么发还怎么发。
UDP的三大使用场景 基于UDP这种“小孩子”的特点，我们可以考虑在以下的场景中使用。
第一，需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用。这很好理解，就像如果你是领导，你会让你们组刚毕业的小朋友去做一些没有那么难的项目，打一些没有那么难的客户，或者做一些失败了也能忍受的实验性项目。
我们在第四节讲的DHCP就是基于UDP协议的。一般的获取IP地址都是内网请求，而且一次获取不到IP又没事，过一会儿还有机会。我们讲过PXE可以在启动的时候自动安装操作系统，操作系统镜像的下载使用的TFTP，这个也是基于UDP协议的。在还没有操作系统的时候，客户端拥有的资源很少，不适合维护一个复杂的状态机，而且因为是内网，一般也没啥问题。
第二，不需要一对一沟通，建立连接，而是可以广播的应用。咱们小时候人都很简单，大家在班级里面，谁成绩好，谁写作好，应该表扬谁惩罚谁，谁得几个小红花都是当着全班的面讲的，公平公正公开。长大了人心复杂了，薪水、奖金要背靠背，和员工一对一沟通。
UDP的不面向连接的功能，可以使得可以承载广播或者多播的协议。DHCP就是一种广播的形式，就是基于UDP协议的，而广播包的格式前面说过了。
对于多播，我们在讲IP地址的时候，讲过一个D类地址，也即组播地址，使用这个地址，可以将包组播给一批机器。当一台机器上的某个进程想监听某个组播地址的时候，需要发送IGMP包，所在网络的路由器就能收到这个包，知道有个机器上有个进程在监听这个组播地址。当路由器收到这个组播地址的时候，会将包转发给这台机器，这样就实现了跨路由器的组播。
在后面云中网络部分，有一个协议VXLAN，也是需要用到组播，也是基于UDP协议的。
第三，需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候。记得曾国藩建立湘军的时候，专门招出生牛犊不怕虎的新兵，而不用那些“老油条”的八旗兵，就是因为八旗兵经历的事情多，遇到敌军不敢舍死忘生。
同理，UDP简单、处理速度快，不像TCP那样，操这么多的心，各种重传啊，保证顺序啊，前面的不收到，后面的没法处理啊。不然等这些事情做完了，时延早就上去了。而TCP在网络不好出现丢包的时候，拥塞控制策略会主动的退缩，降低发送速度，这就相当于本来环境就差，还自断臂膀，用户本来就卡，这下更卡了。
当前很多应用都是要求低时延的，它们可不想用TCP如此复杂的机制，而是想根据自己的场景，实现自己的可靠和连接保证。例如，如果应用自己觉得，有的包丢了就丢了，没必要重传了，就可以算了，有的比较重要，则应用自己重传，而不依赖于TCP。有的前面的包没到，后面的包到了，那就先给客户展示后面的嘛，干嘛非得等到齐了呢？如果网络不好，丢了包，那不能退缩啊，要尽快传啊，速度不能降下来啊，要挤占带宽，抢在客户失去耐心之前到达。
由于UDP十分简单，基本啥都没做，也就给了应用“城会玩”的机会。就像在和平年代，每个人应该有独立的思考和行为，应该可靠并且礼让；但是如果在战争年代，往往不太需要过于独立的思考，而需要士兵简单服从命令就可以了。
曾国藩说哪支部队需要诱敌牺牲，也就牺牲了，相当于包丢了就丢了。两军狭路相逢的时候，曾国藩说上，没有带宽也要上，这才给了曾国藩运筹帷幄，城会玩的机会。同理如果你实现的应用需要有自己的连接策略，可靠保证，时延要求，使用UDP，然后再应用层实现这些是再好不过了。
基于UDP的“城会玩”的五个例子 我列举几种“城会玩”的例子。
“城会玩”一：网页或者APP的访问 原来访问网页和手机APP都是基于HTTP协议的。HTTP协议是基于TCP的，建立连接都需要多次交互，对于时延比较大的目前主流的移动互联网来讲，建立一次连接需要的时间会比较长，然而既然是移动中，TCP可能还会断了重连，也是很耗时的。而且目前的HTTP协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是TCP的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。
而QUIC（全称Quick UDP Internet Connections，快速UDP互联网连接）是Google提出的一种基于UDP改进的通信协议，其目的是降低网络通信的延迟，提供更好的用户互动体验。
QUIC在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制，是应用层“城会玩”的代表。这一节主要是讲UDP，QUIC我们放到应用层去讲。
“城会玩”二：流媒体的协议 现在直播比较火，直播协议多使用RTMP，这个协议我们后面的章节也会讲，而这个RTMP协议也是基于TCP的。TCP的严格顺序传输要保证前一个收到了，下一个才能确认，如果前一个收不到，下一个就算包已经收到了，在缓存里面，也需要等着。对于直播来讲，这显然是不合适的，因为老的视频帧丢了其实也就丢了，就算再传过来用户也不在意了，他们要看新的了，如果老是没来就等着，卡顿了，新的也看不了，那就会丢失客户，所以直播，实时性比较比较重要，宁可丢包，也不要卡顿的。
另外，对于丢包，其实对于视频播放来讲，有的包可以丢，有的包不能丢，因为视频的连续帧里面，有的帧重要，有的不重要，如果必须要丢包，隔几个帧丢一个，其实看视频的人不会感知，但是如果连续丢帧，就会感知了，因而在网络不好的情况下，应用希望选择性的丢帧。
还有就是当网络不好的时候，TCP协议会主动降低发送速度，这对本来当时就卡的看视频来讲是要命的，应该应用层马上重传，而不是主动让步。因而，很多直播应用，都基于UDP实现了自己的视频传输协议。
“城会玩”三：实时游戏 游戏有一个特点，就是实时性比较高。快一秒你干掉别人，慢一秒你被别人爆头，所以很多职业玩家会买非常专业的鼠标和键盘，争分夺秒。
因而，实时游戏中客户端和服务端要建立长连接，来保证实时传输。但是游戏玩家很多，服务器却不多。由于维护TCP连接需要在内核维护一些数据结构，因而一台机器能够支撑的TCP连接数目是有限的，然后UDP由于是没有连接的，在异步IO机制引入之前，常常是应对海量客户端连接的策略。
另外还是TCP的强顺序问题，对战的游戏，对网络的要求很简单，玩家通过客户端发送给服务器鼠标和键盘行走的位置，服务器会处理每个用户发送过来的所有场景，处理完再返回给客户端，客户端解析响应，渲染最新的场景展示给玩家。
如果出现一个数据包丢失，所有事情都需要停下来等待这个数据包重发。客户端会出现等待接收数据，然而玩家并不关心过期的数据，激战中卡1秒，等能动了都已经死了。
游戏对实时要求较为严格的情况下，采用自定义的可靠UDP协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。
“城会玩”四：IoT物联网 一方面，物联网领域终端资源少，很可能只是个内存非常小的嵌入式系统，而维护TCP协议代价太大；另一方面，物联网对实时性要求也很高，而TCP还是因为上面的那些原因导致时延大。Google旗下的Nest建立Thread Group，推出了物联网通信协议Thread，就是基于UDP协议的。
“城会玩”五：移动通信领域 在4G网络里，移动流量上网的数据面对的协议GTP-U是基于UDP的。因为移动网络协议比较复杂，而GTP协议本身就包含复杂的手机上线下线的通信协议。如果基于TCP，TCP的机制就显得非常多余，这部分协议我会在后面的章节单独讲解。
小结 好了，这节就到这里了，我们来总结一下：
如果将TCP比作成熟的社会人，UDP则是头脑简单的小朋友。TCP复杂，UDP简单；TCP维护连接，UDP谁都相信；TCP会坚持知进退；UDP愣头青一个，勇往直前；
UDP虽然简单，但它有简单的用法。它可以用在环境简单、需要多播、应用层自己控制传输的地方。例如DHCP、VXLAN、QUIC等。
最后，给你留两个思考题吧。</description></item><item><title>第11讲_TCP协议（上）：因性恶而复杂，先恶后善反轻松</title><link>https://artisanbox.github.io/5/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/2/</guid><description>上一节，我们讲的UDP，基本上包括了传输层所必须的端口字段。它就像我们小时候一样简单，相信“网之初，性本善，不丢包，不乱序”。
后来呢，我们都慢慢长大，了解了社会的残酷，变得复杂而成熟，就像TCP协议一样。它之所以这么复杂，那是因为它秉承的是“性恶论”。它天然认为网络环境是恶劣的，丢包、乱序、重传，拥塞都是常有的事情，一言不合就可能送达不了，因而要从算法层面来保证可靠性。
TCP包头格式我们先来看TCP头的格式。从这个图上可以看出，它比UDP复杂得多。
首先，源端口号和目标端口号是不可少的，这一点和UDP是一样的。如果没有这两个端口号。数据就不知道应该发给哪个应用。
接下来是包的序号。为什么要给包编号呢？当然是为了解决乱序的问题。不编好号怎么确认哪个应该先来，哪个应该后到呢。编号是为了解决乱序问题。既然是社会老司机，做事当然要稳重，一件件来，面临再复杂的情况，也临危不乱。
还应该有的就是确认序号。发出去的包应该有确认，要不然我怎么知道对方有没有收到呢？如果没有收到就应该重新发送，直到送达。这个可以解决不丢包的问题。作为老司机，做事当然要靠谱，答应了就要做到，暂时做不到也要有个回复。
TCP是靠谱的协议，但是这不能说明它面临的网络环境好。从IP层面来讲，如果网络状况的确那么差，是没有任何可靠性保证的，而作为IP的上一层TCP也无能为力，唯一能做的就是更加努力，不断重传，通过各种算法保证。也就是说，对于TCP来讲，IP层你丢不丢包，我管不着，但是我在我的层面上，会努力保证可靠性。
这有点像如果你在北京，和客户约十点见面，那么你应该清楚堵车是常态，你干预不了，也控制不了，你唯一能做的就是早走。打车不行就改乘地铁，尽力不失约。
接下来有一些状态位。例如SYN是发起一个连接，ACK是回复，RST是重新连接，FIN是结束连接等。TCP是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。
不像小时候，随便一个不认识的小朋友都能玩在一起，人大了，就变得礼貌，优雅而警觉，人与人遇到会互相热情的寒暄，离开会不舍地道别，但是人与人之间的信任会经过多次交互才能建立。
还有一个重要的就是窗口大小。TCP要做流量控制，通信双方各声明一个窗口，标识自己当前能够的处理能力，别发送的太快，撑死我，也别发的太慢，饿死我。
作为老司机，做事情要有分寸，待人要把握尺度，既能适当提出自己的要求，又不强人所难。除了做流量控制以外，TCP还会做拥塞控制，对于真正的通路堵车不堵车，它无能为力，唯一能做的就是控制自己，也即控制发送的速度。不能改变世界，就改变自己嘛。
作为老司机，要会自我控制，知进退，知道什么时候应该坚持，什么时候应该让步。
通过对TCP头的解析，我们知道要掌握TCP协议，重点应该关注以下几个问题：
顺序问题 ，稳重不乱；
丢包问题，承诺靠谱；
连接维护，有始有终；
流量控制，把握分寸；
拥塞控制，知进知退。
TCP的三次握手所有的问题，首先都要先建立一个连接，所以我们先来看连接维护问题。
TCP的连接建立，我们常常称为三次握手。
A：您好，我是A。
B：您好A，我是B。
A：您好B。
我们也常称为“请求-&amp;gt;应答-&amp;gt;应答之应答”的三个回合。这个看起来简单，其实里面还是有很多的学问，很多的细节。
首先，为什么要三次，而不是两次？按说两个人打招呼，一来一回就可以了啊？为了可靠，为什么不是四次？
我们还是假设这个通路是非常不可靠的，A要发起一个连接，当发了第一个请求杳无音信的时候，会有很多的可能性，比如第一个请求包丢了，再如没有丢，但是绕了弯路，超时了，还有B没有响应，不想和我连接。
A不能确认结果，于是再发，再发。终于，有一个请求包到了B，但是请求包到了B的这个事情，目前A还是不知道的，A还有可能再发。
B收到了请求包，就知道了A的存在，并且知道A要和它建立连接。如果B不乐意建立连接，则A会重试一阵后放弃，连接建立失败，没有问题；如果B是乐意建立连接的，则会发送应答包给A。
当然对于B来说，这个应答包也是一入网络深似海，不知道能不能到达A。这个时候B自然不能认为连接是建立好了，因为应答包仍然会丢，会绕弯路，或者A已经挂了都有可能。
而且这个时候B还能碰到一个诡异的现象就是，A和B原来建立了连接，做了简单通信后，结束了连接。还记得吗？A建立连接的时候，请求包重复发了几次，有的请求包绕了一大圈又回来了，B会认为这也是一个正常的的请求的话，因此建立了连接，可以想象，这个连接不会进行下去，也没有个终结的时候，纯属单相思了。因而两次握手肯定不行。
B发送的应答可能会发送多次，但是只要一次到达A，A就认为连接已经建立了，因为对于A来讲，他的消息有去有回。A会给B发送应答之应答，而B也在等这个消息，才能确认连接的建立，只有等到了这个消息，对于B来讲，才算它的消息有去有回。
当然A发给B的应答之应答也会丢，也会绕路，甚至B挂了。按理来说，还应该有个应答之应答之应答，这样下去就没底了。所以四次握手是可以的，四十次都可以，关键四百次也不能保证就真的可靠了。只要双方的消息都有去有回，就基本可以了。
好在大部分情况下，A和B建立了连接之后，A会马上发送数据的，一旦A发送数据，则很多问题都得到了解决。例如A发给B的应答丢了，当A后续发送的数据到达的时候，B可以认为这个连接已经建立，或者B压根就挂了，A发送的数据，会报错，说B不可达，A就知道B出事情了。
当然你可以说A比较坏，就是不发数据，建立连接后空着。我们在程序设计的时候，可以要求开启keepalive机制，即使没有真实的数据包，也有探活包。
另外，你作为服务端B的程序设计者，对于A这种长时间不发包的客户端，可以主动关闭，从而空出资源来给其他客户端使用。
三次握手除了双方建立连接外，主要还是为了沟通一件事情，就是TCP包的序号的问题。
A要告诉B，我这面发起的包的序号起始是从哪个号开始的，B同样也要告诉A，B发起的包的序号起始是从哪个号开始的。为什么序号不能都从1开始呢？因为这样往往会出现冲突。
例如，A连上B之后，发送了1、2、3三个包，但是发送3的时候，中间丢了，或者绕路了，于是重新发送，后来A掉线了，重新连上B后，序号又从1开始，然后发送2，但是压根没想发送3，但是上次绕路的那个3又回来了，发给了B，B自然认为，这就是下一个包，于是发生了错误。
因而，每个连接都要有不同的序号。这个序号的起始序号是随着时间变化的，可以看成一个32位的计数器，每4微秒加一，如果计算一下，如果到重复，需要4个多小时，那个绕路的包早就死翘翘了，因为我们都知道IP包头里面有个TTL，也即生存时间。
好了，双方终于建立了信任，建立了连接。前面也说过，为了维护这个连接，双方都要维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。
一开始，客户端和服务端都处于CLOSED状态。先是服务端主动监听某个端口，处于LISTEN状态。然后客户端主动发起连接SYN，之后处于SYN-SENT状态。服务端收到发起的连接，返回SYN，并且ACK客户端的SYN，之后处于SYN-RCVD状态。客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态，因为它一发一收成功了。服务端收到ACK的ACK之后，处于ESTABLISHED状态，因为它也一发一收了。
TCP四次挥手好了，说完了连接，接下来说一说“拜拜”，好说好散。这常被称为四次挥手。
A：B啊，我不想玩了。
B：哦，你不想玩了啊，我知道了。
这个时候，还只是A不想玩了，也即A不会再发送数据，但是B能不能在ACK的时候，直接关闭呢？当然不可以了，很有可能A是发完了最后的数据就准备不玩了，但是B还没做完自己的事情，还是可以发送数据的，所以称为半关闭的状态。
这个时候A可以选择不再接收数据了，也可以选择最后再接收一段数据，等待B也主动关闭。
B：A啊，好吧，我也不玩了，拜拜。
A：好的，拜拜。
这样整个连接就关闭了。但是这个过程有没有异常情况呢？当然有，上面是和平分手的场面。
A开始说“不玩了”，B说“知道了”，这个回合，是没什么问题的，因为在此之前，双方还处于合作的状态，如果A说“不玩了”，没有收到回复，则A会重新发送“不玩了”。但是这个回合结束之后，就有可能出现异常情况了，因为已经有一方率先撕破脸。
一种情况是，A说完“不玩了”之后，直接跑路，是会有问题的，因为B还没有发起结束，而如果A跑路，B就算发起结束，也得不到回答，B就不知道该怎么办了。另一种情况是，A说完“不玩了”，B直接跑路，也是有问题的，因为A不知道B是还有事情要处理，还是过一会儿会发送结束。
那怎么解决这些问题呢？TCP协议专门设计了几个状态来处理这些问题。我们来看断开连接的时候的状态时序图。
断开的时候，我们可以看到，当A说“不玩了”，就进入FIN_WAIT_1的状态，B收到“A不玩”的消息后，发送知道了，就进入CLOSE_WAIT的状态。
A收到“B说知道了”，就进入FIN_WAIT_2的状态，如果这个时候B直接跑路，则A将永远在这个状态。TCP协议里面并没有对这个状态的处理，但是Linux有，可以调整tcp_fin_timeout这个参数，设置一个超时时间。
如果B没有跑路，发送了“B也不玩了”的请求到达A时，A发送“知道B也不玩了”的ACK后，从FIN_WAIT_2状态结束，按说A可以跑路了，但是最后的这个ACK万一B收不到呢？则B会重新发一个“B不玩了”，这个时候A已经跑路了的话，B就再也收不到ACK了，因而TCP协议要求A最后等待一段时间TIME_WAIT，这个时间要足够长，长到如果B没收到ACK的话，“B说不玩了”会重发的，A会重新发一个ACK并且足够时间到达B。
A直接跑路还有一个问题是，A的端口就直接空出来了，但是B不知道，B原来发过的很多包很可能还在路上，如果A的端口被一个新的应用占用了，这个新的应用会收到上个连接中B发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而也需要等足够长的时间，等到原来B发送的所有的包都死翘翘，再空出端口来。</description></item><item><title>第12讲_TCP协议（下）：西行必定多妖孽，恒心智慧消磨难</title><link>https://artisanbox.github.io/5/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/3/</guid><description>我们前面说到玄奘西行，要出网关。既然出了网关，那就是在公网上传输数据，公网往往是不可靠的，因而需要很多的机制去保证传输的可靠性，这里面需要恒心，也即各种重传的策略，还需要有智慧，也就是说，这里面包含着大量的算法。
如何做个靠谱的人？TCP想成为一个成熟稳重的人，成为一个靠谱的人。那一个人怎么样才算靠谱呢？咱们工作中经常就有这样的场景，比如你交代给下属一个事情以后，下属到底能不能做到，做到什么程度，什么时候能够交付，往往就会有应答，有回复。这样，处理事情的过程中，一旦有异常，你也可以尽快知道，而不是交代完之后就石沉大海，过了一个月再问，他说，啊我不记得了。
对应到网络协议上，就是客户端每发送的一个包，服务器端都应该有个回复，如果服务器端超过一定的时间没有回复，客户端就会重新发送这个包，直到有回复。
这个发送应答的过程是什么样呢？可以是上一个收到了应答，再发送下一个。这种模式有点像两个人直接打电话，你一句，我一句。但是这种方式的缺点是效率比较低。如果一方在电话那头处理的时间比较长，这一头就要干等着，双方都没办法干其他事情。咱们在日常工作中也不是这样的，不能你交代你的下属办一件事情，就一直打着电话看着他做，而是应该他按照你的安排，先将事情记录下来，办完一件回复一件。在他办事情的过程中，你还可以同时交代新的事情，这样双方就并行了。
如果使⽤这种模式，其实需要你和你的下属就不能靠脑⼦了，⽽是要都准备⼀个本⼦，你每交代下属⼀个事情，双方的本子都要记录⼀下。
当你的下属做完⼀件事情，就回复你，做完了，你就在你的本⼦上将这个事情划去。同时你的本⼦上每件事情都有时限，如果超过了时限下属还没有回复，你就要主动重新交代⼀下：上次那件事情，你还没回复我，咋样啦？
既然多件事情可以一起处理，那就需要给每个事情编个号，防止弄错了。例如，程序员平时看任务的时候，都会看JIRA的ID，而不是每次都要描述一下具体的事情。在大部分情况下，对于事情的处理是按照顺序来的，先来的先处理，这就给应答和汇报工作带来了方便。等开周会的时候，每个程序员都可以将JIRA ID的列表拉出来，说以上的都做完了，⽽不⽤⼀个个说。
如何实现一个靠谱的协议？TCP协议使用的也是同样的模式。为了保证顺序性，每一个包都有一个ID。在建立连接的时候，会商定起始的ID是什么，然后按照ID一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的ID，表示都收到了，这种模式称为累计确认或者累计应答（cumulative acknowledgment）。
为了记录所有发送的包和接收的包，TCP也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的ID一个个排列，根据处理的情况分成四个部分。
第一部分：发送了并且已经确认的。这部分就是你交代下属的，并且也做完了的，应该划掉的。
第二部分：发送了并且尚未确认的。这部分是你交代下属的，但是还没做完的，需要等待做完的回复之后，才能划掉。
第三部分：没有发送，但是已经等待发送的。这部分是你还没有交代给下属，但是马上就要交代的。
第四部分：没有发送，并且暂时还不会发送的。这部分是你还没有交代给下属，而且暂时还不会交代给下属的。
这里面为什么要区分第三部分和第四部分呢？没交代的，一下子全交代了不就完了吗？
这就是我们上一节提到的十个词口诀里的“流量控制，把握分寸”。作为项目管理人员，你应该根据以往的工作情况和这个员工反馈的能力、抗压力等，先在心中估测一下，这个人一天能做多少工作。如果工作布置少了，就会不饱和；如果工作布置多了，他就会做不完；如果你使劲逼迫，人家可能就要辞职了。
到底一个员工能够同时处理多少事情呢？在TCP里，接收端会给发送端报一个窗口的大小，叫Advertised window。这个窗口的大小应该等于上面的第二部分加上第三部分，就是已经交代了没做完的加上马上要交代的。超过这个窗口的，接收端做不过来，就不能发送了。
于是，发送端需要保持下面的数据结构。
LastByteAcked：第一部分和第二部分的分界线
LastByteSent：第二部分和第三部分的分界线
LastByteAcked + AdvertisedWindow：第三部分和第四部分的分界线
对于接收端来讲，它的缓存里记录的内容要简单一些。
第一部分：接受并且确认过的。也就是我领导交代给我，并且我做完的。
第二部分：还没接收，但是马上就能接收的。也即是我自己的能够接受的最大工作量。
第三部分：还没接收，也没法接收的。也即超过工作量的部分，实在做不完。
对应的数据结构就像这样。
﻿﻿
MaxRcvBuffer：最大缓存的量；
LastByteRead之后是已经接收了，但是还没被应用层读取的；
NextByteExpected是第一部分和第二部分的分界线。
第二部分的窗口有多大呢？
NextByteExpected和LastByteRead的差其实是还没被应用层读取的部分占用掉的MaxRcvBuffer的量，我们定义为A。
AdvertisedWindow其实是MaxRcvBuffer减去A。
也就是：AdvertisedWindow=MaxRcvBuffer-((NextByteExpected-1)-LastByteRead)。
那第二部分和第三部分的分界线在哪里呢？NextByteExpected加AdvertisedWindow就是第二部分和第三部分的分界线，其实也就是LastByteRead加上MaxRcvBuffer。
其中第二部分里面，由于受到的包可能不是顺序的，会出现空档，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。
顺序问题与丢包问题接下来我们结合一个例子来看。
还是刚才的图，在发送端来看，1、2、3已经发送并确认；4、5、6、7、8、9都是发送了还没确认；10、11、12是还没发出的；13、14、15是接收方没有空间，不准备发的。
在接收端来看，1、2、3、4、5是已经完成ACK，但是没读取的；6、7是等待接收的；8、9是已经接收，但是没有ACK的。
发送端和接收端当前的状态如下：
1、2、3没有问题，双方达成了一致。
4、5接收方说ACK了，但是发送方还没收到，有可能丢了，有可能在路上。
6、7、8、9肯定都发了，但是8、9已经到了，但是6、7没到，出现了乱序，缓存着但是没办法ACK。
根据这个例子，我们可以知道，顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。</description></item><item><title>第13讲_套接字Socket：Talkischeap,showmethecode</title><link>https://artisanbox.github.io/5/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/4/</guid><description>前面讲完了TCP和UDP协议，还没有上手过，这一节咱们讲讲基于TCP和UDP协议的Socket编程。
在讲TCP和UDP协议的时候，我们分客户端和服务端，在写程序的时候，我们也同样这样分。
Socket这个名字很有意思，可以作插口或者插槽讲。虽然我们是写软件程序，但是你可以想象为弄一根网线，一头插在客户端，一头插在服务端，然后进行通信。所以在通信之前，双方都要建立一个Socket。
在建立Socket的时候，应该设置什么参数呢？Socket编程进行的是端到端的通信，往往意识不到中间经过多少局域网，多少路由器，因而能够设置的参数，也只能是端到端协议之上网络层和传输层的。
在网络层，Socket函数需要指定到底是IPv4还是IPv6，分别对应设置为AF_INET和AF_INET6。另外，还要指定到底是TCP还是UDP。还记得咱们前面讲过的，TCP协议是基于数据流的，所以设置为SOCK_STREAM，而UDP是基于数据报的，因而设置为SOCK_DGRAM。
基于TCP协议的Socket程序函数调用过程两端创建了Socket之后，接下来的过程中，TCP和UDP稍有不同，我们先来看TCP。
TCP的服务端要先监听一个端口，一般是先调用bind函数，给这个Socket赋予一个IP地址和端口。为什么需要端口呢？要知道，你写的是一个应用程序，当一个网络包来的时候，内核要通过TCP头里面的这个端口，来找到你这个应用程序，把包给你。为什么要IP地址呢？有时候，一台机器会有多个网卡，也就会有多个IP地址，你可以选择监听所有的网卡，也可以选择监听一个网卡，这样，只有发给这个网卡的包，才会给你。
当服务端有了IP和端口号，就可以调用listen函数进行监听。在TCP的状态图里面，有一个listen状态，当调用这个函数之后，服务端就进入了这个状态，这个时候客户端就可以发起连接了。
在内核中，为每个Socket维护两个队列。一个是已经建立了连接的队列，这时候连接三次握手已经完毕，处于established状态；一个是还没有完全建立连接的队列，这个时候三次握手还没完成，处于syn_rcvd的状态。
接下来，服务端调用accept函数，拿出一个已经完成的连接进行处理。如果还没有完成，就要等着。
在服务端等待的时候，客户端可以通过connect函数发起连接。先在参数中指明要连接的IP地址和端口号，然后开始发起三次握手。内核会给客户端分配一个临时的端口。一旦握手成功，服务端的accept就会返回另一个Socket。
这是一个经常考的知识点，就是监听的Socket和真正用来传数据的Socket是两个，一个叫作监听Socket，一个叫作已连接Socket。
连接建立成功之后，双方开始通过read和write函数来读写数据，就像往一个文件流里面写东西一样。
这个图就是基于TCP协议的Socket程序函数调用过程。
说TCP的Socket就是一个文件流，是非常准确的。因为，Socket在Linux中就是以文件的形式存在的。除此之外，还存在文件描述符。写入和读出，也是通过文件描述符。
在内核中，Socket是一个文件，那对应就有文件描述符。每一个进程都有一个数据结构task_struct，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是一个整数，是这个数组的下标。
这个数组中的内容是一个指针，指向内核中所有打开的文件的列表。既然是一个文件，就会有一个inode，只不过Socket对应的inode不像真正的文件系统一样，保存在硬盘上的，而是在内存中的。在这个inode中，指向了Socket在内核中的Socket结构。
在这个结构里面，主要的是两个队列，一个是发送队列，一个是接收队列。在这两个队列里面保存的是一个缓存sk_buff。这个缓存里面能够看到完整的包的结构。看到这个，是不是能和前面讲过的收发包的场景联系起来了？
整个数据结构我也画了一张图。
基于UDP协议的Socket程序函数调用过程对于UDP来讲，过程有些不一样。UDP是没有连接的，所以不需要三次握手，也就不需要调用listen和connect，但是，UDP的交互仍然需要IP和端口号，因而也需要bind。UDP是没有维护连接状态的，因而不需要每对连接建立一组Socket，而是只要有一个Socket，就能够和多个客户端通信。也正是因为没有连接状态，每次通信的时候，都调用sendto和recvfrom，都可以传入IP地址和端口。
这个图的内容就是基于UDP协议的Socket程序函数调用过程。
服务器如何接更多的项目？会了这几个基本的Socket函数之后，你就可以轻松地写一个网络交互的程序了。就像上面的过程一样，在建立连接后，进行一个while循环。客户端发了收，服务端收了发。
当然这只是万里长征的第一步，因为如果使用这种方法，基本上只能一对一沟通。如果你是一个服务器，同时只能服务一个客户，肯定是不行的。这就相当于老板成立一个公司，只有自己一个人，自己亲自上来服务客户，只能干完了一家再干下一家，这样赚不来多少钱。
那作为老板你就要想了，我最多能接多少项目呢？当然是越多越好。
我们先来算一下理论值，也就是最大连接数，系统会用一个四元组来标识一个TCP连接。
{本机IP, 本机端口, 对端IP, 对端端口} 服务器通常固定在某个本地端口上监听，等待客户端的连接请求。因此，服务端端TCP连接四元组中只有对端IP, 也就是客户端的IP和对端的端口，也即客户端的端口是可变的，因此，最大TCP连接数=客户端IP数×客户端端口数。对IPv4，客户端的IP数最多为2的32次方，客户端的端口数最多为2的16次方，也就是服务端单机最大TCP连接数，约为2的48次方。
当然，服务端最大并发TCP连接数远不能达到理论上限。首先主要是文件描述符限制，按照上面的原理，Socket都是文件，所以首先要通过ulimit配置文件描述符的数目；另一个限制是内存，按上面的数据结构，每个TCP连接都要占用一定内存，操作系统是有限的。
所以，作为老板，在资源有限的情况下，要想接更多的项目，就需要降低每个项目消耗的资源数目。
方式一：将项目外包给其他公司（多进程方式）这就相当于你是一个代理，在那里监听来的请求。一旦建立了一个连接，就会有一个已连接Socket，这时候你可以创建一个子进程，然后将基于已连接Socket的交互交给这个新的子进程来做。就像来了一个新的项目，但是项目不一定是你自己做，可以再注册一家子公司，招点人，然后把项目转包给这家子公司做，以后对接就交给这家子公司了，你又可以去接新的项目了。
这里有一个问题是，如何创建子公司，并如何将项目移交给子公司呢？
在Linux下，创建子进程使用fork函数。通过名字可以看出，这是在父进程的基础上完全拷贝一个子进程。在Linux内核中，会复制文件描述符的列表，也会复制内存空间，还会复制一条记录当前执行到了哪一行程序的进程。显然，复制的时候在调用fork，复制完毕之后，父进程和子进程都会记录当前刚刚执行完fork。这两个进程刚复制完的时候，几乎一模一样，只是根据fork的返回值来区分到底是父进程，还是子进程。如果返回值是0，则是子进程；如果返回值是其他的整数，就是父进程。
进程复制过程我画在这里。
因为复制了文件描述符列表，而文件描述符都是指向整个内核统一的打开文件列表的，因而父进程刚才因为accept创建的已连接Socket也是一个文件描述符，同样也会被子进程获得。
接下来，子进程就可以通过这个已连接Socket和客户端进行互通了，当通信完毕之后，就可以退出进程，那父进程如何知道子进程干完了项目，要退出呢？还记得fork返回的时候，如果是整数就是父进程吗？这个整数就是子进程的ID，父进程可以通过这个ID查看子进程是否完成项目，是否需要退出。
方式二：将项目转包给独立的项目组（多线程方式）上面这种方式你应该也能发现问题，如果每次接一个项目，都申请一个新公司，然后干完了，就注销掉这个公司，实在是太麻烦了。毕竟一个新公司要有新公司的资产，有新的办公家具，每次都买了再卖，不划算。
于是你应该想到了，我们可以使用线程。相比于进程来讲，这样要轻量级的多。如果创建进程相当于成立新公司，购买新办公家具，而创建线程，就相当于在同一个公司成立项目组。一个项目做完了，那这个项目组就可以解散，组成另外的项目组，办公家具可以共用。
在Linux下，通过pthread_create创建一个线程，也是调用do_fork。不同的是，虽然新的线程在task列表会新创建一项，但是很多资源，例如文件描述符列表、进程空间，还是共享的，只不过多了一个引用而已。
新的线程也可以通过已连接Socket处理请求，从而达到并发处理的目的。
上面基于进程或者线程模型的，其实还是有问题的。新到来一个TCP连接，就需要分配一个进程或者线程。一台机器无法创建很多进程或者线程。有个C10K，它的意思是一台机器要维护1万个连接，就要创建1万个进程或者线程，那么操作系统是无法承受的。如果维持1亿用户在线需要10万台服务器，成本也太高了。
其实C10K问题就是，你接项目接的太多了，如果每个项目都成立单独的项目组，就要招聘10万人，你肯定养不起，那怎么办呢？
方式三：一个项目组支撑多个项目（IO多路复用，一个线程维护多个Socket）当然，一个项目组可以看多个项目了。这个时候，每个项目组都应该有个项目进度墙，将自己组看的项目列在那里，然后每天通过项目墙看每个项目的进度，一旦某个项目有了进展，就派人去盯一下。
由于Socket是文件描述符，因而某个线程盯的所有的Socket，都放在一个文件描述符集合fd_set中，这就是项目进度墙，然后调用select函数来监听文件描述符集合是否有变化。一旦有变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在fd_set对应的位都设为1，表示Socket可读或者可写，从而可以进行读写操作，然后再调用select，接着盯着下一轮的变化。
方式四：一个项目组支撑多个项目（IO多路复用，从“派人盯着”到“有事通知”）上面select函数还是有问题的，因为每次Socket所在的文件描述符集合中有Socket发生变化的时候，都需要通过轮询的方式，也就是需要将全部项目都过一遍的方式来查看进度，这大大影响了一个项目组能够支撑的最大的项目数量。因而使用select，能够同时盯的项目数量由FD_SETSIZE限制。
如果改成事件通知的方式，情况就会好很多，项目组不需要通过轮询挨个盯着这些项目，而是当项目进度发生变化的时候，主动通知项目组，然后项目组再根据项目进展情况做相应的操作。
能完成这件事情的函数叫epoll，它在内核中的实现不是通过轮询的方式，而是通过注册callback函数的方式，当某个文件描述符发送变化的时候，就会主动通知。
如图所示，假设进程打开了Socket m, n, x等多个文件描述符，现在需要通过epoll来监听是否这些Socket都有事件发生。其中epoll_create创建一个epoll对象，也是一个文件，也对应一个文件描述符，同样也对应着打开文件列表中的一项。在这项里面有一个红黑树，在红黑树里，要保存这个epoll要监听的所有Socket。
当epoll_ctl添加一个Socket的时候，其实是加入这个红黑树，同时红黑树里面的节点指向一个结构，将这个结构挂在被监听的Socket的事件列表中。当一个Socket来了一个事件的时候，可以从这个列表中得到epoll对象，并调用call back通知它。
这种通知方式使得监听的Socket数据增加的时候，效率不会大幅度降低，能够同时监听的Socket的数目也非常的多了。上限就为系统定义的、进程打开的最大文件描述符个数。因而，epoll被称为解决C10K问题的利器。
小结好了，这一节就到这里了，我们来总结一下：
你需要记住TCP和UDP的Socket的编程中，客户端和服务端都需要调用哪些函数；
写一个能够支撑大量连接的高并发的服务端不容易，需要多进程、多线程，而epoll机制能解决C10K问题。
最后，给你留两个思考题：
epoll是Linux上的函数，那你知道Windows上对应的机制是什么吗？如果想实现一个跨平台的程序，你知道应该怎么办吗？</description></item><item><title>第14讲_HTTP协议：看个新闻原来这么麻烦</title><link>https://artisanbox.github.io/5/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/5/</guid><description>前面讲述完传输层，接下来开始讲应用层的协议。从哪里开始讲呢，就从咱们最常用的HTTP协议开始。
HTTP协议，几乎是每个人上网用的第一个协议，同时也是很容易被人忽略的协议。
既然说看新闻，咱们就先登录 http://www.163.com 。
http://www.163.com 是个URL，叫作统一资源定位符。之所以叫统一，是因为它是有格式的。HTTP称为协议，www.163.com是一个域名，表示互联网上的一个位置。有的URL会有更详细的位置标识，例如 http://www.163.com/index.html 。正是因为这个东西是统一的，所以当你把这样一个字符串输入到浏览器的框里的时候，浏览器才知道如何进行统一处理。
HTTP请求的准备浏览器会将www.163.com这个域名发送给DNS服务器，让它解析为IP地址。有关DNS的过程，其实非常复杂，这个在后面专门介绍DNS的时候，我会详细描述，这里我们先不管，反正它会被解析成为IP地址。那接下来是发送HTTP请求吗？
不是的，HTTP是基于TCP协议的，当然是要先建立TCP连接了，怎么建立呢？还记得第11节讲过的三次握手吗？
目前使用的HTTP协议大部分都是1.1。在1.1的协议里面，默认是开启了Keep-Alive的，这样建立的TCP连接，就可以在多次请求中复用。
学习了TCP之后，你应该知道，TCP的三次握手和四次挥手，还是挺费劲的。如果好不容易建立了连接，然后就做了一点儿事情就结束了，有点儿浪费人力和物力。
HTTP请求的构建建立了连接以后，浏览器就要发送HTTP的请求。
请求的格式就像这样。
HTTP的报文大概分为三大部分。第一部分是请求行，第二部分是请求的首部，第三部分才是请求的正文实体。
第一部分：请求行在请求行中，URL就是 http://www.163.com ，版本为HTTP 1.1。这里要说一下的，就是方法。方法有几种类型。
对于访问网页来讲，最常用的类型就是GET。顾名思义，GET就是去服务器获取一些资源。对于访问网页来讲，要获取的资源往往是一个页面。其实也有很多其他的格式，比如说返回一个JSON字符串，到底要返回什么，是由服务器端的实现决定的。
例如，在云计算中，如果我们的服务器端要提供一个基于HTTP协议的API，获取所有云主机的列表，这就会使用GET方法得到，返回的可能是一个JSON字符串。字符串里面是一个列表，列表里面是一项的云主机的信息。
另外一种类型叫做POST。它需要主动告诉服务端一些信息，而非获取。要告诉服务端什么呢？一般会放在正文里面。正文可以有各种各样的格式。常见的格式也是JSON。
例如，我们下一节要讲的支付场景，客户端就需要把“我是谁？我要支付多少？我要买啥？”告诉服务器，这就需要通过POST方法。
再如，在云计算里，如果我们的服务器端，要提供一个基于HTTP协议的创建云主机的API，也会用到POST方法。这个时候往往需要将“我要创建多大的云主机？多少CPU多少内存？多大硬盘？”这些信息放在JSON字符串里面，通过POST的方法告诉服务器端。
还有一种类型叫PUT，就是向指定资源位置上传最新内容。但是，HTTP的服务器往往是不允许上传文件的，所以PUT和POST就都变成了要传给服务器东西的方法。
在实际使用过程中，这两者还会有稍许的区别。POST往往是用来创建一个资源的，而PUT往往是用来修改一个资源的。
例如，云主机已经创建好了，我想对这个云主机打一个标签，说明这个云主机是生产环境的，另外一个云主机是测试环境的。那怎么修改这个标签呢？往往就是用PUT方法。
再有一种常见的就是DELETE。这个顾名思义就是用来删除资源的。例如，我们要删除一个云主机，就会调用DELETE方法。
第二部分：首部字段请求行下面就是我们的首部字段。首部是key value，通过冒号分隔。这里面，往往保存了一些非常重要的字段。
例如，Accept-Charset，表示客户端可以接受的字符集。防止传过来的是另外的字符集，从而导致出现乱码。
再如，Content-Type是指正文的格式。例如，我们进行POST的请求，如果正文是JSON，那么我们就应该将这个值设置为JSON。
这里需要重点说一下的就是缓存。为啥要使用缓存呢？那是因为一个非常大的页面有很多东西。
例如，我浏览一个商品的详情，里面有这个商品的价格、库存、展示图片、使用手册等等。商品的展示图片会保持较长时间不变，而库存会根据用户购买的情况经常改变。如果图片非常大，而库存数非常小，如果我们每次要更新数据的时候都要刷新整个页面，对于服务器的压力就会很大。
对于这种高并发场景下的系统，在真正的业务逻辑之前，都需要有个接入层，将这些静态资源的请求拦在最外面。
这个架构的图就像这样。
其中DNS、CDN我在后面的章节会讲。和这一节关系比较大的就是Nginx这一层，它如何处理HTTP协议呢？对于静态资源，有Vanish缓存层。当缓存过期的时候，才会访问真正的Tomcat应用集群。
在HTTP头里面，Cache-control是用来控制缓存的。当客户端发送的请求中包含max-age指令时，如果判定缓存层中，资源的缓存时间数值比指定时间的数值小，那么客户端可以接受缓存的资源；当指定max-age值为0，那么缓存层通常需要将请求转发给应用集群。
另外，If-Modified-Since也是一个关于缓存的。也就是说，如果服务器的资源在某个时间之后更新了，那么客户端就应该下载最新的资源；如果没有更新，服务端会返回“304 Not Modified”的响应，那客户端就不用下载了，也会节省带宽。
到此为止，我们仅仅是拼凑起了HTTP请求的报文格式，接下来，浏览器会把它交给下一层传输层。怎么交给传输层呢？其实也无非是用Socket这些东西，只不过用的浏览器里，这些程序不需要你自己写，有人已经帮你写好了。
HTTP请求的发送HTTP协议是基于TCP协议的，所以它使用面向连接的方式发送请求，通过stream二进制流的方式传给对方。当然，到了TCP层，它会把二进制流变成一个个报文段发送给服务器。
在发送给每个报文段的时候，都需要对方有一个回应ACK，来保证报文可靠地到达了对方。如果没有回应，那么TCP这一层会进行重新传输，直到可以到达。同一个包有可能被传了好多次，但是HTTP这一层不需要知道这一点，因为是TCP这一层在埋头苦干。
TCP层发送每一个报文的时候，都需要加上自己的地址（即源地址）和它想要去的地方（即目标地址），将这两个信息放到IP头里面，交给IP层进行传输。
IP层需要查看目标地址和自己是否是在同一个局域网。如果是，就发送ARP协议来请求这个目标地址对应的MAC地址，然后将源MAC和目标MAC放入MAC头，发送出去即可；如果不在同一个局域网，就需要发送到网关，还要需要发送ARP协议，来获取网关的MAC地址，然后将源MAC和网关MAC放入MAC头，发送出去。
网关收到包发现MAC符合，取出目标IP地址，根据路由协议找到下一跳的路由器，获取下一跳路由器的MAC地址，将包发给下一跳路由器。
这样路由器一跳一跳终于到达目标的局域网。这个时候，最后一跳的路由器能够发现，目标地址就在自己的某一个出口的局域网上。于是，在这个局域网上发送ARP，获得这个目标地址的MAC地址，将包发出去。
目标的机器发现MAC地址符合，就将包收起来；发现IP地址符合，根据IP头中协议项，知道自己上一层是TCP协议，于是解析TCP的头，里面有序列号，需要看一看这个序列包是不是我要的，如果是就放入缓存中然后返回一个ACK，如果不是就丢弃。
TCP头里面还有端口号，HTTP的服务器正在监听这个端口号。于是，目标机器自然知道是HTTP服务器这个进程想要这个包，于是将包发给HTTP服务器。HTTP服务器的进程看到，原来这个请求是要访问一个网页，于是就把这个网页发给客户端。
HTTP返回的构建HTTP的返回报文也是有一定格式的。这也是基于HTTP 1.1的。
状态码会反映HTTP请求的结果。“200”意味着大吉大利；而我们最不想见的，就是“404”，也就是“服务端无法响应这个请求”。然后，短语会大概说一下原因。
接下来是返回首部的key value。
这里面，Retry-After表示，告诉客户端应该在多长时间以后再次尝试一下。“503错误”是说“服务暂时不再和这个值配合使用”。
在返回的头部里面也会有Content-Type，表示返回的是HTML，还是JSON。
构造好了返回的HTTP报文，接下来就是把这个报文发送出去。还是交给Socket去发送，还是交给TCP层，让TCP层将返回的HTML，也分成一个个小的段，并且保证每个段都可靠到达。
这些段加上TCP头后会交给IP层，然后把刚才的发送过程反向走一遍。虽然两次不一定走相同的路径，但是逻辑过程是一样的，一直到达客户端。
客户端发现MAC地址符合、IP地址符合，于是就会交给TCP层。根据序列号看是不是自己要的报文段，如果是，则会根据TCP头中的端口号，发给相应的进程。这个进程就是浏览器，浏览器作为客户端也在监听某个端口。
当浏览器拿到了HTTP的报文。发现返回“200”，一切正常，于是就从正文中将HTML拿出来。HTML是一个标准的网页格式。浏览器只要根据这个格式，展示出一个绚丽多彩的网页。
这就是一个正常的HTTP请求和返回的完整过程。
HTTP 2.0当然HTTP协议也在不断的进化过程中，在HTTP1.1基础上便有了HTTP 2.0。
HTTP 1.1在应用层以纯文本的形式进行通信。每次通信都要带完整的HTTP的头，而且不考虑pipeline模式的话，每次的过程总是像上面描述的那样一去一回。这样在实时性、并发性上都存在问题。
为了解决这些问题，HTTP 2.</description></item><item><title>第15讲_HTTPS协议：点外卖的过程原来这么复杂</title><link>https://artisanbox.github.io/5/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/6/</guid><description>用HTTP协议，看个新闻还没有问题，但是换到更加严肃的场景中，就存在很多的安全风险。例如，你要下单做一次支付，如果还是使用普通的HTTP协议，那你很可能会被黑客盯上。
你发送一个请求，说我要点个外卖，但是这个网络包被截获了，于是在服务器回复你之前，黑客先假装自己就是外卖网站，然后给你回复一个假的消息说：“好啊好啊，来来来，银行卡号、密码拿来。”如果这时候你真把银行卡密码发给它，那你就真的上套了。
那怎么解决这个问题呢？当然一般的思路就是加密。加密分为两种方式一种是对称加密，一种是非对称加密。
在对称加密算法中，加密和解密使用的密钥是相同的。也就是说，加密和解密使用的是同一个密钥。因此，对称加密算法要保证安全性的话，密钥要做好保密。只能让使用的人知道，不能对外公开。
在非对称加密算法中，加密使用的密钥和解密使用的密钥是不相同的。一把是作为公开的公钥，另一把是作为谁都不能给的私钥。公钥加密的信息，只有私钥才能解密。私钥加密的信息，只有公钥才能解密。
因为对称加密算法相比非对称加密算法来说，效率要高得多，性能也好，所以交互的场景下多用对称加密。
对称加密假设你和外卖网站约定了一个密钥，你发送请求的时候用这个密钥进行加密，外卖网站用同样的密钥进行解密。这样就算中间的黑客截获了你的请求，但是它没有密钥，还是破解不了。
这看起来很完美，但是中间有个问题，你们两个怎么来约定这个密钥呢？如果这个密钥在互联网上传输，也是很有可能让黑客截获的。黑客一旦截获这个秘钥，它可以佯作不知，静静地等着你们两个交互。这时候你们之间互通的任何消息，它都能截获并且查看，就等你把银行卡账号和密码发出来。
我们在谍战剧里面经常看到这样的场景，就是特工破译的密码会有个密码本，截获无线电台，通过密码本就能将原文破解出来。怎么把密码本给对方呢？只能通过线下传输。
比如，你和外卖网站偷偷约定时间地点，它给你一个纸条，上面写着你们两个的密钥，然后说以后就用这个密钥在互联网上定外卖了。当然你们接头的时候，也会先约定一个口号，什么“天王盖地虎”之类的，口号对上了，才能把纸条给它。但是，“天王盖地虎”同样也是对称加密密钥，同样存在如何把“天王盖地虎”约定成口号的问题。而且在谍战剧中一对一接头可能还可以，在互联网应用中，客户太多，这样是不行的。
非对称加密所以，只要是对称加密，就会永远在这个死循环里出不来，这个时候，就需要非对称加密介入进来。
非对称加密的私钥放在外卖网站这里，不会在互联网上传输，这样就能保证这个密钥的私密性。但是，对应私钥的公钥，是可以在互联网上随意传播的，只要外卖网站把这个公钥给你，你们就可以愉快地互通了。
比如说你用公钥加密，说“我要定外卖”，黑客在中间就算截获了这个报文，因为它没有私钥也是解不开的，所以这个报文可以顺利到达外卖网站，外卖网站用私钥把这个报文解出来，然后回复，“那给我银行卡和支付密码吧”。
先别太乐观，这里还是有问题的。回复的这句话，是外卖网站拿私钥加密的，互联网上人人都可以把它打开，当然包括黑客。那外卖网站可以拿公钥加密吗？当然不能，因为它自己的私钥只有它自己知道，谁也解不开。
另外，这个过程还有一个问题，黑客也可以模拟发送“我要定外卖”这个过程的，因为它也有外卖网站的公钥。
为了解决这个问题，看来一对公钥私钥是不够的，客户端也需要有自己的公钥和私钥，并且客户端要把自己的公钥，给外卖网站。
这样，客户端给外卖网站发送的时候，用外卖网站的公钥加密。而外卖网站给客户端发送消息的时候，使用客户端的公钥。这样就算有黑客企图模拟客户端获取一些信息，或者半路截获回复信息，但是由于它没有私钥，这些信息它还是打不开。
数字证书不对称加密也会有同样的问题，如何将不对称加密的公钥给对方呢？一种是放在一个公网的地址上，让对方下载；另一种就是在建立连接的时候，传给对方。
这两种方法有相同的问题，那就是，作为一个普通网民，你怎么鉴别别人给你的公钥是对的。会不会有人冒充外卖网站，发给你一个它的公钥。接下来，你和它所有的互通，看起来都是没有任何问题的。毕竟每个人都可以创建自己的公钥和私钥。
例如，我自己搭建了一个网站cliu8site，可以通过这个命令先创建私钥。
openssl genrsa -out cliu8siteprivate.key 1024 然后，再根据这个私钥，创建对应的公钥。
openssl rsa -in cliu8siteprivate.key -pubout -outcliu8sitepublic.pem 这个时候就需要权威部门的介入了，就像每个人都可以打印自己的简历，说自己是谁，但是有公安局盖章的，就只有户口本，这个才能证明你是你。这个由权威部门颁发的称为证书（Certificate）。
证书里面有什么呢？当然应该有公钥，这是最重要的；还有证书的所有者，就像户口本上有你的姓名和身份证号，说明这个户口本是你的；另外还有证书的发布机构和证书的有效期，这个有点像身份证上的机构是哪个区公安局，有效期到多少年。
这个证书是怎么生成的呢？会不会有人假冒权威机构颁发证书呢？就像有假身份证、假户口本一样。生成证书需要发起一个证书请求，然后将这个请求发给一个权威机构去认证，这个权威机构我们称为CA（ Certificate Authority）。
证书请求可以通过这个命令生成。
openssl req -key cliu8siteprivate.key -new -out cliu8sitecertificate.req 将这个请求发给权威机构，权威机构会给这个证书卡一个章，我们称为签名算法。问题又来了，那怎么签名才能保证是真的权威机构签名的呢？当然只有用只掌握在权威机构手里的东西签名了才行，这就是CA的私钥。
签名算法大概是这样工作的：一般是对信息做一个Hash计算，得到一个Hash值，这个过程是不可逆的，也就是说无法通过Hash值得出原来的信息内容。在把信息发送出去时，把这个Hash值加密后，作为一个签名和信息一起发出去。
权威机构给证书签名的命令是这样的。
openssl x509 -req -in cliu8sitecertificate.req -CA cacertificate.pem -CAkey caprivate.key -out cliu8sitecertificate.pem 这个命令会返回Signature ok，而cliu8sitecertificate.pem就是签过名的证书。CA用自己的私钥给外卖网站的公钥签名，就相当于给外卖网站背书，形成了外卖网站的证书。
我们来查看这个证书的内容。
openssl x509 -in cliu8sitecertificate.pem -noout -text 这里面有个Issuer，也即证书是谁颁发的；Subject，就是证书颁发给谁；Validity是证书期限；Public-key是公钥内容；Signature Algorithm是签名算法。</description></item><item><title>第16讲_流媒体协议：如何在直播里看到美女帅哥？</title><link>https://artisanbox.github.io/5/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/7/</guid><description>最近直播比较火，很多人都喜欢看直播，那一个直播系统里面都有哪些组成部分，都使用了什么协议呢？
无论是直播还是点播，其实都是对于视频数据的传输。一提到视频，大家都爱看，但是一提到视频技术，大家都头疼，因为名词实在是太多了。
三个名词系列我这里列三个名词系列，你先大致有个印象。
名词系列一：AVI、MPEG、RMVB、MP4、MOV、FLV、WebM、WMV、ASF、MKV。例如RMVB和MP4，看着是不是很熟悉？
名词系列二：H.261、 H.262、H.263、H.264、H.265。这个是不是就没怎么听过了？别着急，你先记住，要重点关注H.264。
名词系列三：MPEG-1、MPEG-2、MPEG-4、MPEG-7。MPEG好像听说过，但是后面的数字是怎么回事？是不是又熟悉又陌生？
这里，我想问你个问题，视频是什么？我说，其实就是快速播放一连串连续的图片。
每一张图片，我们称为一帧。只要每秒钟帧的数据足够多，也即播放得足够快。比如每秒30帧，以人的眼睛的敏感程度，是看不出这是一张张独立的图片的，这就是我们常说的帧率（FPS）。
每一张图片，都是由像素组成的，假设为1024*768（这个像素数不算多）。每个像素由RGB组成，每个8位，共24位。
我们来算一下，每秒钟的视频有多大？
30帧 × 1024 × 768 × 24 = 566,231,040Bits = 70,778,880Bytes
如果一分钟呢？4,246,732,800Bytes，已经是4个G了。
是不是不算不知道，一算吓一跳？这个数据量实在是太大，根本没办法存储和传输。如果这样存储，你的硬盘很快就满了；如果这样传输，那多少带宽也不够用啊！
怎么办呢？人们想到了编码，就是看如何用尽量少的Bit数保存视频，使播放的时候画面看起来仍然很精美。编码是一个压缩的过程。
视频和图片的压缩过程有什么特点？之所以能够对视频流中的图片进行压缩，因为视频和图片有这样一些特点。
空间冗余：图像的相邻像素之间有较强的相关性，一张图片相邻像素往往是渐变的，不是突变的，没必要每个像素都完整地保存，可以隔几个保存一个，中间的用算法计算出来。
时间冗余：视频序列的相邻图像之间内容相似。一个视频中连续出现的图片也不是突变的，可以根据已有的图片进行预测和推断。
视觉冗余：人的视觉系统对某些细节不敏感，因此不会每一个细节都注意到，可以允许丢失一些数据。
编码冗余：不同像素值出现的概率不同，概率高的用的字节少，概率低的用的字节多，类似霍夫曼编码（Huffman Coding）的思路。
总之，用于编码的算法非常复杂，而且多种多样，但是编码过程其实都是类似的。
视频编码的两大流派能不能形成一定的标准呢？要不然开发视频播放的人得累死了。当然能，我这里就给你介绍，视频编码的两大流派。
流派一：ITU（International Telecommunications Union）的VCEG（Video Coding Experts Group），这个称为国际电联下的VCEG。既然是电信，可想而知，他们最初做视频编码，主要侧重传输。名词系列二，就是这个组织制定的标准。
流派二：ISO（International Standards Organization）的MPEG（Moving Picture Experts Group），这个是ISO旗下的MPEG，本来是做视频存储的。例如，编码后保存在VCD和DVD中。当然后来也慢慢侧重视频传输了。名词系列三，就是这个组织制定的标准。
后来，ITU-T（国际电信联盟电信标准化部门，ITU Telecommunication Standardization Sector）与MPEG联合制定了H.</description></item><item><title>第17讲_P2P协议：我下小电影，99%急死你</title><link>https://artisanbox.github.io/5/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/8/</guid><description>如果你想下载一个电影，一般会通过什么方式呢？
当然，最简单的方式就是通过HTTP进行下载。但是相信你有过这样的体验，通过浏览器下载的时候，只要文件稍微大点，下载的速度就奇慢无比。
还有种下载文件的方式，就是通过FTP，也即文件传输协议。FTP采用两个TCP连接来传输一个文件。
控制连接：服务器以被动的方式，打开众所周知用于FTP的端口21，客户端则主动发起连接。该连接将命令从客户端传给服务器，并传回服务器的应答。常用的命令有：list——获取文件目录；reter——取一个文件；store——存一个文件。
数据连接：每当一个文件在客户端与服务器之间传输时，就创建一个数据连接。
FTP的两种工作模式每传输一个文件，都要建立一个全新的数据连接。FTP有两种工作模式，分别是主动模式（PORT）和被动模式（PASV），这些都是站在FTP服务器的角度来说的。
主动模式下，客户端随机打开一个大于1024的端口N，向服务器的命令端口21发起连接，同时开放N+1端口监听，并向服务器发出 “port N+1” 命令，由服务器从自己的数据端口20，主动连接到客户端指定的数据端口N+1。
被动模式下，当开启一个FTP连接时，客户端打开两个任意的本地端口N（大于1024）和N+1。第一个端口连接服务器的21端口，提交PASV命令。然后，服务器会开启一个任意的端口P（大于1024），返回“227 entering passive mode”消息，里面有FTP服务器开放的用来进行数据传输的端口。客户端收到消息取得端口号之后，会通过N+1号端口连接服务器的端口P，然后在两个端口之间进行数据传输。
P2P是什么？但是无论是HTTP的方式，还是FTP的方式，都有一个比较大的缺点，就是难以解决单一服务器的带宽压力， 因为它们使用的都是传统的客户端服务器的方式。
后来，一种创新的、称为P2P的方式流行起来。P2P就是peer-to-peer。资源开始并不集中地存储在某些设备上，而是分散地存储在多台设备上。这些设备我们姑且称为peer。
想要下载一个文件的时候，你只要得到那些已经存在了文件的peer，并和这些peer之间，建立点对点的连接，而不需要到中心服务器上，就可以就近下载文件。一旦下载了文件，你也就成为peer中的一员，你旁边的那些机器，也可能会选择从你这里下载文件，所以当你使用P2P软件的时候，例如BitTorrent，往往能够看到，既有下载流量，也有上传的流量，也即你自己也加入了这个P2P的网络，自己从别人那里下载，同时也提供给其他人下载。可以想象，这种方式，参与的人越多，下载速度越快，一切完美。
种子（.torrent）文件但是有一个问题，当你想下载一个文件的时候，怎么知道哪些peer有这个文件呢？
这就用到种子啦，也即咱们比较熟悉的.torrent文件。.torrent文件由两部分组成，分别是：announce（tracker URL）和文件信息。
文件信息里面有这些内容。
info区：这里指定的是该种子有几个文件、文件有多长、目录结构，以及目录和文件的名字。
Name字段：指定顶层目录名字。
每个段的大小：BitTorrent（简称BT）协议把一个文件分成很多个小段，然后分段下载。
段哈希值：将整个种子中，每个段的SHA-1哈希值拼在一起。
下载时，BT客户端首先解析.torrent文件，得到tracker地址，然后连接tracker服务器。tracker服务器回应下载者的请求，将其他下载者（包括发布者）的IP提供给下载者。下载者再连接其他下载者，根据.torrent文件，两者分别对方告知自己已经有的块，然后交换对方没有的数据。此时不需要其他服务器参与，并分散了单个线路上的数据流量，因此减轻了服务器的负担。
下载者每得到一个块，需要算出下载块的Hash验证码，并与.torrent文件中的对比。如果一样，则说明块正确，不一样则需要重新下载这个块。这种规定是为了解决下载内容的准确性问题。
从这个过程也可以看出，这种方式特别依赖tracker。tracker需要收集下载者信息的服务器，并将此信息提供给其他下载者，使下载者们相互连接起来，传输数据。虽然下载的过程是非中心化的，但是加入这个P2P网络的时候，都需要借助tracker中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源。
所以，这种工作方式有一个弊端，一旦tracker服务器出现故障或者线路遭到屏蔽，BT工具就无法正常工作了。
去中心化网络（DHT）那能不能彻底非中心化呢？
于是，后来就有了一种叫作DHT（Distributed Hash Table）的去中心化网络。每个加入这个DHT网络的人，都要负责存储这个网络里的资源信息和其他成员的联系信息，相当于所有人一起构成了一个庞大的分布式存储数据库。
有一种著名的DHT协议，叫Kademlia协议。这个和区块链的概念一样，很抽象，我来详细讲一下这个协议。
任何一个BitTorrent启动之后，它都有两个角色。一个是peer，监听一个TCP端口，用来上传和下载文件，这个角色表明，我这里有某个文件。另一个角色DHT node，监听一个UDP的端口，通过这个角色，这个节点加入了一个DHT的网络。
在DHT网络里面，每一个DHT node都有一个ID。这个ID是一个很长的串。每个DHT node都有责任掌握一些知识，也就是文件索引，也即它应该知道某些文件是保存在哪些节点上。它只需要有这些知识就可以了，而它自己本身不一定就是保存这个文件的节点。
哈希值当然，每个DHT node不会有全局的知识，也即不知道所有的文件保存在哪里，它只需要知道一部分。那应该知道哪一部分呢？这就需要用哈希算法计算出来。
每个文件可以计算出一个哈希值，而DHT node的ID是和哈希值相同长度的串。
DHT算法是这样规定的：如果一个文件计算出一个哈希值，则和这个哈希值一样的那个DHT node，就有责任知道从哪里下载这个文件，即便它自己没保存这个文件。
当然不一定这么巧，总能找到和哈希值一模一样的，有可能一模一样的DHT node也下线了，所以DHT算法还规定：除了一模一样的那个DHT node应该知道，ID和这个哈希值非常接近的N个DHT node也应该知道。
什么叫和哈希值接近呢？例如只修改了最后一位，就很接近；修改了倒数2位，也不远；修改了倒数3位，也可以接受。总之，凑齐了规定的N这个数就行。
刚才那个图里，文件1通过哈希运算，得到匹配ID的DHT node为node C，当然还会有其他的，我这里没有画出来。所以，node C有责任知道文件1的存放地址，虽然node C本身没有存放文件1。</description></item><item><title>第18讲_DNS协议：网络世界的地址簿</title><link>https://artisanbox.github.io/5/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/9/</guid><description>前面我们讲了平时常见的看新闻、支付、直播、下载等场景，现在网站的数目非常多，常用的网站就有二三十个，如果全部用IP地址进行访问，恐怕很难记住。于是，就需要一个地址簿，根据名称，就可以查看具体的地址。
例如，我要去西湖边的“外婆家”，这就是名称，然后通过地址簿，查看到底是哪条路多少号。
DNS服务器在网络世界，也是这样的。你肯定记得住网站的名称，但是很难记住网站的IP地址，因而也需要一个地址簿，就是DNS服务器。
由此可见，DNS在日常生活中多么重要。每个人上网，都需要访问它，但是同时，这对它来讲也是非常大的挑战。一旦它出了故障，整个互联网都将瘫痪。另外，上网的人分布在全世界各地，如果大家都去同一个地方访问某一台服务器，时延将会非常大。因而，DNS服务器，一定要设置成高可用、高并发和分布式的。
于是，就有了这样树状的层次结构。
- 根DNS服务器 ：返回顶级域DNS服务器的IP地址
顶级域DNS服务器：返回权威DNS服务器的IP地址
权威DNS服务器 ：返回相应主机的IP地址
DNS解析流程为了提高DNS的解析性能，很多网络都会就近部署DNS缓存服务器。于是，就有了以下的DNS解析流程。
电脑客户端会发出一个DNS请求，问www.163.com的IP是啥啊，并发给本地域名服务器 (本地DNS)。那本地域名服务器 (本地DNS) 是什么呢？如果是通过DHCP配置，本地DNS由你的网络服务商（ISP），如电信、移动等自动分配，它通常就在你网络服务商的某个机房。
本地DNS收到来自客户端的请求。你可以想象这台服务器上缓存了一张域名与之对应IP地址的大表格。如果能找到 www.163.com，它就直接返回IP地址。如果没有，本地DNS会去问它的根域名服务器：“老大，能告诉我www.163.com的IP地址吗？”根域名服务器是最高层次的，全球共有13套。它不直接用于域名解析，但能指明一条道路。
根DNS收到来自本地DNS的请求，发现后缀是 .com，说：“哦，www.163.com啊，这个域名是由.com区域管理，我给你它的顶级域名服务器的地址，你去问问它吧。”
本地DNS转向问顶级域名服务器：“老二，你能告诉我www.163.com的IP地址吗？”顶级域名服务器就是大名鼎鼎的比如 .com、.net、 .org这些一级域名，它负责管理二级域名，比如 163.com，所以它能提供一条更清晰的方向。
顶级域名服务器说：“我给你负责 www.163.com 区域的权威DNS服务器的地址，你去问它应该能问到。”
本地DNS转向问权威DNS服务器：“您好，www.163.com 对应的IP是啥呀？”163.com的权威DNS服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。
权威DNS服务器查询后将对应的IP地址X.X.X.X告诉本地DNS。
本地DNS再将IP地址返回客户端，客户端和目标建立连接。
至此，我们完成了DNS的解析过程。现在总结一下，整个过程我画成了一个图。
负载均衡站在客户端角度，这是一次DNS递归查询过程。因为本地DNS全权为它效劳，它只要坐等结果即可。在这个过程中，DNS除了可以通过名称映射为IP地址，它还可以做另外一件事，就是负载均衡。
还是以访问“外婆家”为例，还是我们开头的“外婆家”，但是，它可能有很多地址，因为它在杭州可以有很多家。所以，如果一个人想去吃“外婆家”，他可以就近找一家店，而不用大家都去同一家，这就是负载均衡。
DNS首先可以做内部负载均衡。
例如，一个应用要访问数据库，在这个应用里面应该配置这个数据库的IP地址，还是应该配置这个数据库的域名呢？显然应该配置域名，因为一旦这个数据库，因为某种原因，换到了另外一台机器上，而如果有多个应用都配置了这台数据库的话，一换IP地址，就需要将这些应用全部修改一遍。但是如果配置了域名，则只要在DNS服务器里，将域名映射为新的IP地址，这个工作就完成了，大大简化了运维。
在这个基础上，我们可以再进一步。例如，某个应用要访问另外一个应用，如果配置另外一个应用的IP地址，那么这个访问就是一对一的。但是当被访问的应用撑不住的时候，我们其实可以部署多个。但是，访问它的应用，如何在多个之间进行负载均衡？只要配置成为域名就可以了。在域名解析的时候，我们只要配置策略，这次返回第一个IP，下次返回第二个IP，就可以实现负载均衡了。
另外一个更加重要的是，DNS还可以做全局负载均衡。
为了保证我们的应用高可用，往往会部署在多个机房，每个地方都会有自己的IP地址。当用户访问某个域名的时候，这个IP地址可以轮询访问多个数据中心。如果一个数据中心因为某种原因挂了，只要在DNS服务器里面，将这个数据中心对应的IP地址删除，就可以实现一定的高可用。
另外，我们肯定希望北京的用户访问北京的数据中心，上海的用户访问上海的数据中心，这样，客户体验就会非常好，访问速度就会超快。这就是全局负载均衡的概念。
示例：DNS访问数据中心中对象存储上的静态资源我们通过DNS访问数据中心中对象存储上的静态资源为例，看一看整个过程。
假设全国有多个数据中心，托管在多个运营商，每个数据中心三个可用区（Available Zone）。对象存储通过跨可用区部署，实现高可用性。在每个数据中心中，都至少部署两个内部负载均衡器，内部负载均衡器后面对接多个对象存储的前置服务器（Proxy-server）。
当一个客户端要访问object.yourcompany.com的时候，需要将域名转换为IP地址进行访问，所以它要请求本地DNS解析器。
本地DNS解析器先查看看本地的缓存是否有这个记录。如果有则直接使用，因为上面的过程太复杂了，如果每次都要递归解析，就太麻烦了。</description></item><item><title>第19讲_HttpDNS：网络世界的地址簿也会指错路</title><link>https://artisanbox.github.io/5/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/10/</guid><description>上一节我们知道了DNS的两项功能，第一是根据名称查到具体的地址，另外一个是可以针对多个地址做负载均衡，而且可以在多个地址中选择一个距离你近的地方访问。
然而有时候这个地址簿也经常给你指错路，明明距离你500米就有个吃饭的地方，非要把你推荐到5公里外。为什么会出现这样的情况呢？
还记得吗？当我们发出请求解析DNS的时候，首先，会先连接到运营商本地的DNS服务器，由这个服务器帮我们去整棵DNS树上进行解析，然后将解析的结果返回给客户端。但是本地的DNS服务器，作为一个本地导游，往往有自己的“小心思”。
传统DNS存在哪些问题？1.域名缓存问题它可以在本地做一个缓存，也就是说，不是每一个请求，它都会去访问权威DNS服务器，而是访问过一次就把结果缓存到自己本地，当其他人来问的时候，直接就返回这个缓存数据。
这就相当于导游去过一个饭店，自己脑子记住了地址，当有一个游客问的时候，他就凭记忆回答了，不用再去查地址簿。这样经常存在的一个问题是，人家那个饭店明明都已经搬了，结果作为导游，他并没有刷新这个缓存，结果你辛辛苦苦到了这个地点，发现饭店已经变成了服装店，你是不是会非常失望？
另外，有的运营商会把一些静态页面，缓存到本运营商的服务器内，这样用户请求的时候，就不用跨运营商进行访问，这样既加快了速度，也减少了运营商之间流量计算的成本。在域名解析的时候，不会将用户导向真正的网站，而是指向这个缓存的服务器。
很多情况下是看不出问题的，但是当页面更新，用户会访问到老的页面，问题就出来了。例如，你听说一个餐馆推出了一个新菜，你想去尝一下。结果导游告诉你，在这里吃也是一样的。有的游客会觉得没问题，但是对于想尝试新菜的人来说，如果导游说带你去，但其实并没有吃到新菜，你是不是也会非常失望呢？
再就是本地的缓存，往往使得全局负载均衡失败，因为上次进行缓存的时候，缓存中的地址不一定是这次访问离客户最近的地方，如果把这个地址返回给客户，那肯定就会绕远路。
就像上一次客户要吃西湖醋鱼的事，导游知道西湖边有一家，因为当时游客就在西湖边，可是，下一次客户在灵隐寺，想吃西湖醋鱼的时候，导游还指向西湖边的那一家，那这就绕得太远了。
2.域名转发问题缓存问题还是说本地域名解析服务，还是会去权威DNS服务器中查找，只不过不是每次都要查找。可以说这还是大导游、大中介。还有一些小导游、小中介，有了请求之后，直接转发给其他运营商去做解析，自己只是外包了出去。
这样的问题是，如果是A运营商的客户，访问自己运营商的DNS服务器，如果A运营商去权威DNS服务器查询的话，权威DNS服务器知道你是A运营商的，就返回给一个部署在A运营商的网站地址，这样针对相同运营商的访问，速度就会快很多。
但是A运营商偷懒，将解析的请求转发给B运营商，B运营商去权威DNS服务器查询的话，权威服务器会误认为，你是B运营商的，那就返回给你一个在B运营商的网站地址吧，结果客户的每次访问都要跨运营商，速度就会很慢。
3.出口NAT问题前面讲述网关的时候，我们知道，出口的时候，很多机房都会配置NAT，也即网络地址转换，使得从这个网关出去的包，都换成新的IP地址，当然请求返回的时候，在这个网关，再将IP地址转换回去，所以对于访问来说是没有任何问题。
但是一旦做了网络地址的转换，权威的DNS服务器，就没办法通过这个地址，来判断客户到底是来自哪个运营商，而且极有可能因为转换过后的地址，误判运营商，导致跨运营商的访问。
4.域名更新问题本地DNS服务器是由不同地区、不同运营商独立部署的。对域名解析缓存的处理上，实现策略也有区别，有的会偷懒，忽略域名解析结果的TTL时间限制，在权威DNS服务器解析变更的时候，解析结果在全网生效的周期非常漫长。但是有的时候，在DNS的切换中，场景对生效时间要求比较高。
例如双机房部署的时候，跨机房的负载均衡和容灾多使用DNS来做。当一个机房出问题之后，需要修改权威DNS，将域名指向新的IP地址，但是如果更新太慢，那很多用户都会出现访问异常。
这就像，有的导游比较勤快、敬业，时时刻刻关注酒店、餐馆、交通的变化，问他的时候，往往会得到最新情况。有的导游懒一些，8年前背的导游词就没换过，问他的时候，指的路往往就是错的。
5.解析延迟问题从上一节的DNS查询过程来看，DNS的查询过程需要递归遍历多个DNS服务器，才能获得最终的解析结果，这会带来一定的时延，甚至会解析超时。
HttpDNS的工作模式既然DNS解析中有这么多问题，那怎么办呢？难不成退回到直接用IP地址？这样显然不合适，所以就有了 HttpDNS。
HttpDNS其实就是，不走传统的DNS解析，而是自己搭建基于HTTP协议的DNS服务器集群，分布在多个地点和多个运营商。当客户端需要DNS解析的时候，直接通过HTTP协议进行请求这个服务器集群，得到就近的地址。
这就相当于每家基于HTTP协议，自己实现自己的域名解析，自己做一个自己的地址簿，而不使用统一的地址簿。但是默认的域名解析都是走DNS的，因而使用HttpDNS需要绕过默认的DNS路径，就不能使用默认的客户端。使用HttpDNS的，往往是手机应用，需要在手机端嵌入支持HttpDNS的客户端SDK。
通过自己的HttpDNS服务器和自己的SDK，实现了从依赖本地导游，到自己上网查询做旅游攻略，进行自由行，爱怎么玩怎么玩。这样就能够避免依赖导游，而导游又不专业，你还不能把他怎么样的尴尬。
下面我来解析一下 HttpDNS的工作模式。
在客户端的SDK里动态请求服务端，获取HttpDNS服务器的IP列表，缓存到本地。随着不断地解析域名，SDK也会在本地缓存DNS域名解析的结果。
当手机应用要访问一个地址的时候，首先看是否有本地的缓存，如果有就直接返回。这个缓存和本地DNS的缓存不一样的是，这个是手机应用自己做的，而非整个运营商统一做的。如何更新、何时更新，手机应用的客户端可以和服务器协调来做这件事情。
如果本地没有，就需要请求HttpDNS的服务器，在本地HttpDNS服务器的IP列表中，选择一个发出HTTP的请求，会返回一个要访问的网站的IP列表。
请求的方式是这样的。
curl http://106.2.xxx.xxx/d?dn=c.m.163.com {&amp;quot;dns&amp;quot;:[{&amp;quot;host&amp;quot;:&amp;quot;c.m.163.com&amp;quot;,&amp;quot;ips&amp;quot;:[&amp;quot;223.252.199.12&amp;quot;],&amp;quot;ttl&amp;quot;:300,&amp;quot;http2&amp;quot;:0}],&amp;quot;client&amp;quot;:{&amp;quot;ip&amp;quot;:&amp;quot;106.2.81.50&amp;quot;,&amp;quot;line&amp;quot;:269692944}} 手机客户端自然知道手机在哪个运营商、哪个地址。由于是直接的HTTP通信，HttpDNS服务器能够准确知道这些信息，因而可以做精准的全局负载均衡。
当然，当所有这些都不工作的时候，可以切换到传统的LocalDNS来解析，慢也比访问不到好。那HttpDNS是如何解决上面的问题的呢？
其实归结起来就是两大问题。一是解析速度和更新速度的平衡问题，二是智能调度的问题，对应的解决方案是HttpDNS的缓存设计和调度设计。
HttpDNS的缓存设计解析DNS过程复杂，通信次数多，对解析速度造成很大影响。为了加快解析，因而有了缓存，但是这又会产生缓存更新速度不及时的问题。最要命的是，这两个方面都掌握在别人手中，也即本地DNS服务器手中，它不会为你定制，你作为客户端干着急没办法。
而HttpDNS就是将解析速度和更新速度全部掌控在自己手中。一方面，解析的过程，不需要本地DNS服务递归的调用一大圈，一个HTTP的请求直接搞定，要实时更新的时候，马上就能起作用；另一方面为了提高解析速度，本地也有缓存，缓存是在客户端SDK维护的，过期时间、更新时间，都可以自己控制。
HttpDNS的缓存设计策略也是咱们做应用架构中常用的缓存设计模式，也即分为客户端、缓存、数据源三层。
对于应用架构来讲，就是应用、缓存、数据库。常见的是Tomcat、Redis、MySQL。
对于HttpDNS来讲，就是手机客户端、DNS缓存、HttpDNS服务器。
只要是缓存模式，就存在缓存的过期、更新、不一致的问题，解决思路也是很像的。
例如DNS缓存在内存中，也可以持久化到存储上，从而APP重启之后，能够尽快从存储中加载上次累积的经常访问的网站的解析结果，就不需要每次都全部解析一遍，再变成缓存。这有点像Redis是基于内存的缓存，但是同样提供持久化的能力，使得重启或者主备切换的时候，数据不会完全丢失。
SDK中的缓存会严格按照缓存过期时间，如果缓存没有命中，或者已经过期，而且客户端不允许使用过期的记录，则会发起一次解析，保障记录是更新的。
解析可以同步进行，也就是直接调用HttpDNS的接口，返回最新的记录，更新缓存；也可以异步进行，添加一个解析任务到后台，由后台任务调用HttpDNS的接口。
同步更新的优点是实时性好，缺点是如果有多个请求都发现过期的时候，同时会请求HttpDNS多次，其实是一种浪费。
同步更新的方式对应到应用架构中缓存的Cache-Aside机制，也即先读缓存，不命中读数据库，同时将结果写入缓存。
异步更新的优点是，可以将多个请求都发现过期的情况，合并为一个对于HttpDNS的请求任务，只执行一次，减少HttpDNS的压力。同时可以在即将过期的时候，就创建一个任务进行预加载，防止过期之后再刷新，称为预加载。
它的缺点是当前请求拿到过期数据的时候，如果客户端允许使用过期数据，需要冒一次风险。如果过期的数据还能请求，就没问题；如果不能请求，则失败一次，等下次缓存更新后，再请求方能成功。
异步更新的机制对应到应用架构中缓存的Refresh-Ahead机制，即业务仅仅访问缓存，当过期的时候定期刷新。在著名的应用缓存Guava Cache中，有个RefreshAfterWrite机制，对于并发情况下，多个缓存访问不命中从而引发并发回源的情况，可以采取只有一个请求回源的模式。在应用架构的缓存中，也常常用数据预热或者预加载的机制。
HttpDNS的调度设计由于客户端嵌入了SDK，因而就不会因为本地DNS的各种缓存、转发、NAT，让权威DNS服务器误会客户端所在的位置和运营商，而可以拿到第一手资料。
在客户端，可以知道手机是哪个国家、哪个运营商、哪个省，甚至哪个市，HttpDNS服务端可以根据这些信息，选择最佳的服务节点访问。
如果有多个节点，还会考虑错误率、请求时间、服务器压力、网络状况等，进行综合选择，而非仅仅考虑地理位置。当有一个节点宕机或者性能下降的时候，可以尽快进行切换。
要做到这一点，需要客户端使用HttpDNS返回的IP访问业务应用。客户端的SDK会收集网络请求数据，如错误率、请求时间等网络请求质量数据，并发送到统计后台，进行分析、聚合，以此查看不同的IP的服务质量。
在服务端，应用可以通过调用HttpDNS的管理接口，配置不同服务质量的优先级、权重。HttpDNS会根据这些策略综合地理位置和线路状况算出一个排序，优先访问当前那些优质的、时延低的IP地址。
HttpDNS通过智能调度之后返回的结果，也会缓存在客户端。为了不让缓存使得调度失真，客户端可以根据不同的移动网络运营商WIFI的SSID来分维度缓存。不同的运营商或者WIFI解析出来的结果会不同。
小结好了，这节就到这里了，我们来总结一下，你需要记住这两个重点：
传统的DNS有很多问题，例如解析慢、更新不及时。因为缓存、转发、NAT问题导致客户端误会自己所在的位置和运营商，从而影响流量的调度。
HttpDNS通过客户端SDK和服务端，通过HTTP直接调用解析DNS的方式，绕过了传统DNS的这些缺点，实现了智能的调度。
最后，给你留两个思考题。</description></item><item><title>第1讲_为什么要学习网络协议？</title><link>https://artisanbox.github.io/5/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/11/</guid><description>《圣经》中有一个通天塔的故事，大致是说，上帝为了阻止人类联合起来，就让人类说不同的语言。人类没法儿沟通，达不成“协议”，通天塔的计划就失败了。
但是千年以后，有一种叫“程序猿”的物种，敲着一种这个群体通用的语言，连接着全世界所有的人，打造这互联网世界的通天塔。如今的世界，正是因为互联网，才连接在一起。
当"Hello World!"从显示器打印出来的时候，还记得你激动的心情吗？
public class HelloWorld { public static void main(String[] args){ System.out.println(&amp;quot;Hello World!&amp;quot;); } } 如果你是程序员，一定看得懂上面这一段文字。这是每一个程序员向计算机世界说“你好，世界”的方式。但是，你不一定知道，这段文字也是一种协议，是人类和计算机沟通的协议，只有通过这种协议，计算机才知道我们想让它做什么。
协议三要素当然，这种协议还是更接近人类语言，机器不能直接读懂，需要进行翻译，翻译的工作教给编译器，也就是程序员常说的compile。这个过程比较复杂，其中的编译原理非常复杂，我在这里不进行详述。
但是可以看得出，计算机语言作为程序员控制一台计算机工作的协议，具备了协议的三要素。
语法，就是这一段内容要符合一定的规则和格式。例如，括号要成对，结束要使用分号等。
语义，就是这一段内容要代表某种意义。例如数字减去数字是有意义的，数字减去文本一般来说就没有意义。
顺序，就是先干啥，后干啥。例如，可以先加上某个数值，然后再减去某个数值。
会了计算机语言，你就能够教给一台计算机完成你的工作了。恭喜你，入门了！
但是，要想打造互联网世界的通天塔，只教给一台机器做什么是不够的，你需要学会教给一大片机器做什么。这就需要网络协议。只有通过网络协议，才能使一大片机器互相协作、共同完成一件事。
这个时候，你可能会问，网络协议长啥样，这么神奇，能干成啥事？我先拿一个简单的例子，让你尝尝鲜，然后再讲一个大事。
当你想要买一个商品，常规的做法就是打开浏览器，输入购物网站的地址。浏览器就会给你显示一个缤纷多彩的页面。
那你有没有深入思考过，浏览器是如何做到这件事情的？它之所以能够显示缤纷多彩的页面，是因为它收到了一段来自HTTP协议的“东西”。我拿网易考拉来举例，格式就像下面这样：
HTTP/1.1 200 OK Date: Tue, 27 Mar 2018 16:50:26 GMT Content-Type: text/html;charset=UTF-8 Content-Language: zh-CN &amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;base href=&amp;quot;https://pages.kaola.com/&amp;quot; /&amp;gt; &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;/&amp;gt; &amp;lt;title&amp;gt;网易考拉3周年主会场&amp;lt;/title&amp;gt; 这符合协议的三要素吗？我带你来看一下。
首先，符合语法，也就是说，只有按照上面那个格式来，浏览器才认。例如，上来是状态，然后是首部，然后是内容。
第二，符合语义，就是要按照约定的意思来。例如，状态200，表述的意思是网页成功返回。如果不成功，就是我们常见的“404”。
第三，符合顺序，你一点浏览器，就是发送出一个HTTP请求，然后才有上面那一串HTTP返回的东西。
浏览器显然按照协议商定好的做了，最后一个五彩缤纷的页面就出现在你面前了。
我们常用的网络协议有哪些？接下来揭秘我要说的大事情，“双十一”。这和我们要讲的网络协议有什么关系呢？
在经济学领域，有个伦纳德·里德（Leonard E. Read）创作的《铅笔的故事》。这个故事通过一个铅笔的诞生过程，来讲述复杂的经济学理论。这里，我也用一个下单的过程，看看互联网世界的运行过程中，都使用了哪些网络协议。
你先在浏览器里面输入 https://www.</description></item><item><title>第20讲_CDN：你去小卖部取过快递么？</title><link>https://artisanbox.github.io/5/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/12/</guid><description>上一节，我们看到了网站的一般访问模式。
当一个用户想访问一个网站的时候，指定这个网站的域名，DNS就会将这个域名解析为地址，然后用户请求这个地址，返回一个网页。就像你要买个东西，首先要查找商店的位置，然后去商店里面找到自己想要的东西，最后拿着东西回家。
那这里面还有没有可以优化的地方呢？
例如你去电商网站下单买个东西，这个东西一定要从电商总部的中心仓库送过来吗？原来基本是这样的，每一单都是单独配送，所以你可能要很久才能收到你的宝贝。但是后来电商网站的物流系统学聪明了，他们在全国各地建立了很多仓库，而不是只有总部的中心仓库才可以发货。
电商网站根据统计大概知道，北京、上海、广州、深圳、杭州等地，每天能够卖出去多少书籍、卫生纸、包、电器等存放期比较长的物品。这些物品用不着从中心仓库发出，所以平时就可以将它们分布在各地仓库里，客户一下单，就近的仓库发出，第二天就可以收到了。
这样，用户体验大大提高。当然，这里面也有个难点就是，生鲜这类东西保质期太短，如果提前都备好货，但是没有人下单，那肯定就坏了。这个问题，我后文再说。
我们先说，我们的网站访问可以借鉴“就近配送”这个思路。
全球有这么多的数据中心，无论在哪里上网，临近不远的地方基本上都有数据中心。是不是可以在这些数据中心里部署几台机器，形成一个缓存的集群来缓存部分数据，那么用户访问数据的时候，就可以就近访问了呢？
当然是可以的。这些分布在各个地方的各个数据中心的节点，就称为边缘节点。
由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上。有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。
这就是CDN分发系统的架构。CDN系统的缓存，也是一层一层的，能不访问后端真正的源，就不打扰它。这也是电商网站物流系统的思路，北京局找不到，找华北局，华北局找不到，再找北方局。
有了这个分发系统之后，接下来，客户端如何找到相应的边缘节点进行访问呢？
还记得我们讲过的基于DNS的全局负载均衡吗？这个负载均衡主要用来选择一个就近的同样运营商的服务器进行访问。你会发现，CDN分发网络也是一个分布在多个区域、多个运营商的分布式系统，也可以用相同的思路选择最合适的边缘节点。
在没有CDN的情况下，用户向浏览器输入www.web.com这个域名，客户端访问本地DNS服务器的时候，如果本地DNS服务器有缓存，则返回网站的地址；如果没有，递归查询到网站的权威DNS服务器，这个权威DNS服务器是负责web.com的，它会返回网站的IP地址。本地DNS服务器缓存下IP地址，将IP地址返回，然后客户端直接访问这个IP地址，就访问到了这个网站。
然而有了CDN之后，情况发生了变化。在web.com这个权威DNS服务器上，会设置一个CNAME别名，指向另外一个域名 www.web.cdn.com，返回给本地DNS服务器。
当本地DNS服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是web.com的权威DNS服务器了，而是web.cdn.com的权威DNS服务器，这是CDN自己的权威DNS服务器。在这个服务器上，还是会设置一个CNAME，指向另外一个域名，也即CDN网络的全局负载均衡器。
接下来，本地DNS服务器去请求CDN的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括：
根据用户IP地址，判断哪一台服务器距用户最近；
用户所处的运营商；
根据用户所请求的URL中携带的内容名称，判断哪一台服务器上有用户所需的内容；
查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。
基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的IP地址。
本地DNS服务器缓存这个IP地址，然后将IP返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。
CDN可以进行缓存的内容有很多种。
保质期长的日用品比较容易缓存，因为不容易过期，对应到就像电商仓库系统里，就是静态页面、图片等，因为这些东西也不怎么变，所以适合缓存。
还记得这个接入层缓存的架构吗？在进入数据中心的时候，我们希望通过最外层接入层的缓存，将大部分静态资源的访问拦在边缘。而CDN则更进一步，将这些静态资源缓存到离用户更近的数据中心。越接近客户，访问性能越好，时延越低。
但是静态内容中，有一种特殊的内容，也大量使用了CDN，这个就是前面讲过的流媒体。
CDN支持流媒体协议，例如前面讲过的RTMP协议。在很多情况下，这相当于一个代理，从上一级缓存读取内容，转发给用户。由于流媒体往往是连续的，因而可以进行预先缓存的策略，也可以预先推送到用户的客户端。
对于静态页面来讲，内容的分发往往采取拉取的方式，也即当发现未命中的时候，再去上一级进行拉取。但是，流媒体数据量大，如果出现回源，压力会比较大，所以往往采取主动推送的模式，将热点数据主动推送到边缘节点。
对于流媒体来讲，很多CDN还提供预处理服务，也即文件在分发之前，经过一定的处理。例如将视频转换为不同的码流，以适应不同的网络带宽的用户需求；再如对视频进行分片，降低存储压力，也使得客户端可以选择使用不同的码率加载不同的分片。这就是我们常见的，“我要看超清、标清、流畅等”。
对于流媒体CDN来讲，有个关键的问题是防盗链问题。因为视频是要花大价钱买版权的，为了挣点钱，收点广告费，如果流媒体被其他的网站盗走，在人家的网站播放，那损失可就大了。
最常用也最简单的方法就是HTTP头的referer字段， 当浏览器发送请求的时候，一般会带上referer，告诉服务器是从哪个页面链接过来的，服务器基于此可以获得一些信息用于处理。如果refer信息不是来自本站，就阻止访问或者跳到其它链接。
referer的机制相对比较容易破解，所以还需要配合其他的机制。
一种常用的机制是时间戳防盗链。使用CDN的管理员可以在配置界面上，和CDN厂商约定一个加密字符串。
客户端取出当前的时间戳，要访问的资源及其路径，连同加密字符串进行签名算法得到一个字符串，然后生成一个下载链接，带上这个签名字符串和截止时间戳去访问CDN。
在CDN服务端，根据取出过期时间，和当前 CDN 节点时间进行比较，确认请求是否过期。然后CDN服务端有了资源及路径，时间戳，以及约定的加密字符串，根据相同的签名算法计算签名，如果匹配则一致，访问合法，才会将资源返回给客户。
然而比如在电商仓库中，我在前面提过，有关生鲜的缓存就是非常麻烦的事情，这对应着就是动态的数据，比较难以缓存。怎么办呢？现在也有动态CDN，主要有两种模式。
一种为生鲜超市模式，也即边缘计算的模式。既然数据是动态生成的，所以数据的逻辑计算和存储，也相应的放在边缘的节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。就像对生鲜的烹饪是动态的，没办法事先做好缓存，因而将生鲜超市放在你家旁边，既能够送货上门，也能够现场烹饪，也是边缘计算的一种体现。
另一种是冷链运输模式，也即路径优化的模式。数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过CDN的网络，对路径进行优化。因为CDN节点较多，能够找到离源站很近的边缘节点，也能找到离用户很近的边缘节点。中间的链路完全由CDN来规划，选择一个更加可靠的路径，使用类似专线的方式进行访问。
对于常用的TCP连接，在公网上传输的时候经常会丢数据，导致TCP的窗口始终很小，发送速度上不去。根据前面的TCP流量控制和拥塞控制的原理，在CDN加速网络中可以调整TCP的参数，使得TCP可以更加激进地传输数据。
可以通过多个请求复用一个连接，保证每次动态请求到达时。连接都已经建立了，不必临时三次握手或者建立过多的连接，增加服务器的压力。另外，可以通过对传输数据进行压缩，增加传输效率。
所有这些手段就像冷链运输，整个物流优化了，全程冷冻高速运输。不管生鲜是从你旁边的超市送到你家的，还是从产地送的，保证到你家是新鲜的。
小结好了，这节就到这里了。咱们来总结一下，你记住这两个重点就好。
CDN和电商系统的分布式仓储系统一样，分为中心节点、区域节点、边缘节点，而数据缓存在离用户最近的位置。
CDN最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链。它也支持动态数据的缓存，一种是边缘计算的生鲜超市模式，另一种是链路优化的冷链运输模式。
最后，给你留两个思考题：</description></item><item><title>第21讲_数据中心：我是开发商，自己拿地盖别墅</title><link>https://artisanbox.github.io/5/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/13/</guid><description>无论你是看新闻、下订单、看视频、下载文件，最终访问的目的地都在数据中心里面。我们前面学了这么多的网络协议和网络相关的知识，你是不是很好奇，数据中心究竟长啥样呢？
数据中心是一个大杂烩，几乎要用到前面学过的所有知识。
前面讲办公室网络的时候，我们知道办公室里面有很多台电脑。如果要访问外网，需要经过一个叫网关的东西，而网关往往是一个路由器。
数据中心里面也有一大堆的电脑，但是它和咱们办公室里面的笔记本或者台式机不一样。数据中心里面是服务器。服务器被放在一个个叫作机架（Rack）的架子上面。
数据中心的入口和出口也是路由器，由于在数据中心的边界，就像在一个国家的边境，称为边界路由器（Border Router）。为了高可用，边界路由器会有多个。
一般家里只会连接一个运营商的网络，而为了高可用，为了当一个运营商出问题的时候，还可以通过另外一个运营商来提供服务，所以数据中心的边界路由器会连接多个运营商网络。
既然是路由器，就需要跑路由协议，数据中心往往就是路由协议中的自治区域（AS）。数据中心里面的机器要想访问外面的网站，数据中心里面也是有对外提供服务的机器，都可以通过BGP协议，获取内外互通的路由信息。这就是我们常听到的多线BGP的概念。
如果数据中心非常简单，没几台机器，那就像家里或者宿舍一样，所有的服务器都直接连到路由器上就可以了。但是数据中心里面往往有非常多的机器，当塞满一机架的时候，需要有交换机将这些服务器连接起来，可以互相通信。
这些交换机往往是放在机架顶端的，所以经常称为TOR（Top Of Rack）交换机。这一层的交换机常常称为接入层（Access Layer）。注意这个接入层和原来讲过的应用的接入层不是一个概念。
当一个机架放不下的时候，就需要多个机架，还需要有交换机将多个机架连接在一起。这些交换机对性能的要求更高，带宽也更大。这些交换机称为汇聚层交换机（Aggregation Layer）。
数据中心里面的每一个连接都是需要考虑高可用的。这里首先要考虑的是，如果一台机器只有一个网卡，上面连着一个网线，接入到TOR交换机上。如果网卡坏了，或者不小心网线掉了，机器就上不去了。所以，需要至少两个网卡、两个网线插到TOR交换机上，但是两个网卡要工作得像一张网卡一样，这就是常说的网卡绑定（bond）。
这就需要服务器和交换机都支持一种协议LACP（Link Aggregation Control Protocol）。它们互相通信，将多个网卡聚合称为一个网卡，多个网线聚合成一个网线，在网线之间可以进行负载均衡，也可以为了高可用作准备。
网卡有了高可用保证，但交换机还有问题。如果一个机架只有一个交换机，它挂了，那整个机架都不能上网了。因而TOR交换机也需要高可用，同理接入层和汇聚层的连接也需要高可用性，也不能单线连着。
最传统的方法是，部署两个接入交换机、两个汇聚交换机。服务器和两个接入交换机都连接，接入交换机和两个汇聚都连接，当然这样会形成环，所以需要启用STP协议，去除环，但是这样两个汇聚就只能一主一备了。STP协议里我们学过，只有一条路会起作用。
交换机有一种技术叫作堆叠，所以另一种方法是，将多个交换机形成一个逻辑的交换机，服务器通过多根线分配连到多个接入层交换机上，而接入层交换机多根线分别连接到多个交换机上，并且通过堆叠的私有协议，形成双活的连接方式。
由于对带宽要求更大，而且挂了影响也更大，所以两个堆叠可能就不够了，可以就会有更多的，比如四个堆叠为一个逻辑的交换机。
汇聚层将大量的计算节点相互连接在一起，形成一个集群。在这个集群里面，服务器之间通过二层互通，这个区域常称为一个POD（Point Of Delivery），有时候也称为一个可用区（Available Zone）。
当节点数目再多的时候，一个可用区放不下，需要将多个可用区连在一起，连接多个可用区的交换机称为核心交换机。
核心交换机吞吐量更大，高可用要求更高，肯定需要堆叠，但是往往仅仅堆叠，不足以满足吞吐量，因而还是需要部署多组核心交换机。核心和汇聚交换机之间为了高可用，也是全互连模式的。
这个时候还存在一个问题，出现环路怎么办？
一种方式是，不同的可用区在不同的二层网络，需要分配不同的网段。汇聚和核心之间通过三层网络互通的，二层都不在一个广播域里面，不会存在二层环路的问题。三层有环是没有问题的，只要通过路由协议选择最佳的路径就可以了。那为啥二层不能有环路，而三层可以呢？你可以回忆一下二层环路的情况。
如图，核心层和汇聚层之间通过内部的路由协议OSPF，找到最佳的路径进行访问，而且还可以通过ECMP等价路由，在多个路径之间进行负载均衡和高可用。
但是随着数据中心里面的机器越来越多，尤其是有了云计算、大数据，集群规模非常大，而且都要求在一个二层网络里面。这就需要二层互连从汇聚层上升为核心层，也即在核心以下，全部是二层互连，全部在一个广播域里面，这就是常说的大二层。
如果大二层横向流量不大，核心交换机数目不多，可以做堆叠，但是如果横向流量很大，仅仅堆叠满足不了，就需要部署多组核心交换机，而且要和汇聚层进行全互连。由于堆叠只解决一个核心交换机组内的无环问题，而组之间全互连，还需要其他机制进行解决。
如果是STP，那部署多组核心无法扩大横向流量的能力，因为还是只有一组起作用。
于是大二层就引入了TRILL（Transparent Interconnection of Lots of Link），即多链接透明互联协议。它的基本思想是，二层环有问题，三层环没有问题，那就把三层的路由能力模拟在二层实现。
运行TRILL协议的交换机称为RBridge，是具有路由转发特性的网桥设备，只不过这个路由是根据MAC地址来的，不是根据IP来的。
Rbridage之间通过链路状态协议运作。记得这个路由协议吗？通过它可以学习整个大二层的拓扑，知道访问哪个MAC应该从哪个网桥走；还可以计算最短的路径，也可以通过等价的路由进行负载均衡和高可用性。
TRILL协议在原来的MAC头外面加上自己的头，以及外层的MAC头。TRILL头里面的Ingress RBridge，有点像IP头里面的源IP地址，Egress RBridge是目标IP地址，这两个地址是端到端的，在中间路由的时候，不会发生改变。而外层的MAC，可以有下一跳的Bridge，就像路由的下一跳，也是通过MAC地址来呈现的一样。
如图中所示的过程，有一个包要从主机A发送到主机B，中间要经过RBridge 1、RBridge 2、RBridge X等等，直到RBridge 3。在RBridge 2收到的包里面，分内外两层，内层就是传统的主机A和主机B的MAC地址以及内层的VLAN。
在外层首先加上一个TRILL头，里面描述这个包从RBridge 1进来的，要从RBridge 3出去，并且像三层的IP地址一样有跳数。然后再外面，目的MAC是RBridge 2，源MAC是RBridge 1，以及外层的VLAN。
当RBridge 2收到这个包之后，首先看MAC是否是自己的MAC，如果是，要看自己是不是Egress RBridge，也即是不是最后一跳；如果不是，查看跳数是不是大于0，然后通过类似路由查找的方式找到下一跳RBridge X，然后将包发出去。
RBridge 2发出去的包，内层的信息是不变的，外层的TRILL头里面。同样，描述这个包从RBridge 1进来的，要从RBridge 3出去，但是跳数要减1。外层的目标MAC变成RBridge X，源MAC变成RBridge 2。
如此一直转发，直到RBridge 3，将外层解出来，发送内层的包给主机B。</description></item><item><title>第22讲_VPN：朝中有人好做官</title><link>https://artisanbox.github.io/5/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/14/</guid><description>前面我们讲到了数据中心，里面很复杂，但是有的公司有多个数据中心，需要将多个数据中心连接起来，或者需要办公室和数据中心连接起来。这该怎么办呢？
第一种方式是走公网，但是公网太不安全，你的隐私可能会被别人偷窥。
第二种方式是租用专线的方式把它们连起来，这是土豪的做法，需要花很多钱。
第三种方式是用VPN来连接，这种方法比较折中，安全又不贵。
VPN，全名Virtual Private Network，虚拟专用网，就是利用开放的公众网络，建立专用数据传输通道，将远程的分支机构、移动办公人员等连接起来。
VPN是如何工作的？VPN通过隧道技术在公众网络上仿真一条点到点的专线，是通过利用一种协议来传输另外一种协议的技术，这里面涉及三种协议：乘客协议、隧道协议和承载协议。
我们以IPsec协议为例来说明。
你知道如何通过自驾进行海南游吗？这其中，你的车怎么通过琼州海峡呢？这里用到轮渡，其实这就用到隧道协议。
在广州这边开车是有“协议”的，例如靠右行驶、红灯停、绿灯行，这个就相当于“被封装”的乘客协议。当然在海南那面，开车也是同样的协议。这就相当于需要连接在一起的一个公司的两个分部。
但是在海上坐船航行，也有它的协议，例如要看灯塔、要按航道航行等。这就是外层的承载协议。
那我的车如何从广州到海南呢？这就需要你遵循开车的协议，将车开上轮渡，所有通过轮渡的车都关在船舱里面，按照既定的规则排列好，这就是隧道协议。
在大海上，你的车是关在船舱里面的，就像在隧道里面一样，这个时候内部的乘客协议，也即驾驶协议没啥用处，只需要船遵从外层的承载协议，到达海南就可以了。
到达之后，外部承载协议的任务就结束了，打开船舱，将车开出来，就相当于取下承载协议和隧道协议的头。接下来，在海南该怎么开车，就怎么开车，还是内部的乘客协议起作用。
在最前面的时候说了，直接使用公网太不安全，所以接下来我们来看一种十分安全的VPN，IPsec VPN。这是基于IP协议的安全隧道协议，为了保证在公网上面信息的安全，因而采取了一定的机制保证安全性。
机制一：私密性，防止信息泄露给未经授权的个人，通过加密把数据从明文变成无法读懂的密文，从而确保数据的私密性。
前面讲HTTPS的时候，说过加密可以分为对称加密和非对称加密。对称加密速度快一些。而VPN一旦建立，需要传输大量数据，因而我们采取对称加密。但是同样，对称加密还是存在加密密钥如何传输的问题，这里需要用到因特网密钥交换（IKE，Internet Key Exchange）协议。
机制二：完整性，数据没有被非法篡改，通过对数据进行hash运算，产生类似于指纹的数据摘要，以保证数据的完整性。
机制三：真实性，数据确实是由特定的对端发出，通过身份认证可以保证数据的真实性。
那如何保证对方就是真正的那个人呢？
第一种方法就是预共享密钥，也就是双方事先商量好一个暗号，比如“天王盖地虎，宝塔镇河妖”，对上了，就说明是对的。
另外一种方法就是用数字签名来验证。咋签名呢？当然是使用私钥进行签名，私钥只有我自己有，所以如果对方能用我的数字证书里面的公钥解开，就说明我是我。
基于以上三个特性，组成了IPsec VPN的协议簇。这个协议簇内容比较丰富。
在这个协议簇里面，有两种协议，这两种协议的区别在于封装网络包的格式不一样。
一种协议称为AH（Authentication Header），只能进行数据摘要 ，不能实现数据加密。
还有一种ESP（Encapsulating Security Payload），能够进行数据加密和数据摘要。
在这个协议簇里面，还有两类算法，分别是加密算法和摘要算法。
这个协议簇还包含两大组件，一个用于VPN的双方要进行对称密钥的交换的IKE组件，另一个是VPN的双方要对连接进行维护的SA（Security Association）组件。
IPsec VPN的建立过程下面来看IPsec VPN的建立过程，这个过程分两个阶段。
第一个阶段，建立IKE自己的SA。这个SA用来维护一个通过身份认证和安全保护的通道，为第二个阶段提供服务。在这个阶段，通过DH（Diffie-Hellman）算法计算出一个对称密钥K。
DH算法是一个比较巧妙的算法。客户端和服务端约定两个公开的质数p和q，然后客户端随机产生一个数a作为自己的私钥，服务端随机产生一个b作为自己的私钥，客户端可以根据p、q和a计算出公钥A，服务端根据p、q和b计算出公钥B，然后双方交换公钥A和B。
到此客户端和服务端可以根据已有的信息，各自独立算出相同的结果K，就是对称密钥。但是这个过程，对称密钥从来没有在通道上传输过，只传输了生成密钥的材料，通过这些材料，截获的人是无法算出的。
有了这个对称密钥K，接下来是第二个阶段，建立IPsec SA。在这个SA里面，双方会生成一个随机的对称密钥M，由K加密传给对方，然后使用M进行双方接下来通信的数据。对称密钥M是有过期时间的，会过一段时间，重新生成一次，从而防止被破解。</description></item><item><title>第23讲_移动网络：去巴塞罗那，手机也上不了脸书</title><link>https://artisanbox.github.io/5/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/15/</guid><description>前面讲的都是电脑上网的场景，那使用手机上网有什么不同呢？
移动网络的发展历程你一定知道手机上网有2G、3G、4G的说法，究竟这都是什么意思呢？有一个通俗的说法就是：用2G看txt，用3G看jpg，用4G看avi。
2G网络手机本来是用来打电话的，不是用来上网的，所以原来在2G时代，上网使用的不是IP网络，而是电话网络，走模拟信号，专业名称为公共交换电话网（PSTN，Public Switched Telephone Network）。
那手机不连网线，也不连电话线，它是怎么上网的呢？
手机是通过收发无线信号来通信的，专业名称是Mobile Station，简称MS，需要嵌入SIM。手机是客户端，而无线信号的服务端，就是基站子系统（BSS，Base Station SubsystemBSS）。至于什么是基站，你可以回想一下，你在爬山的时候，是不是看到过信号塔？我们平时城市里面的基站比较隐蔽，不容易看到，所以只有在山里才会注意到。正是这个信号塔，通过无线信号，让你的手机可以进行通信。
但是你要知道一点，无论无线通信如何无线，最终还是要连接到有线的网络里。前面讲数据中心的时候我也讲过，电商的应用是放在数据中心的，数据中心的电脑都是插着网线的。
因而，基站子系统分两部分，一部分对外提供无线通信，叫作基站收发信台（BTS，Base Transceiver Station），另一部分对内连接有线网络，叫作基站控制器（BSC，Base Station Controller）。基站收发信台通过无线收到数据后，转发给基站控制器。
这部分属于无线的部分，统称为无线接入网（RAN，Radio Access Network）。
基站控制器通过有线网络，连接到提供手机业务的运营商的数据中心，这部分称为核心网（CN，Core Network）。核心网还没有真的进入互联网，这部分还是主要提供手机业务，是手机业务的有线部分。
首先接待基站来的数据的是移动业务交换中心（MSC，Mobile Service Switching Center），它是进入核心网的入口，但是它不会让你直接连接到互联网上。
因为在让你的手机真正进入互联网之前，提供手机业务的运营商，需要认证是不是合法的手机接入。你别自己造了一张手机卡，就连接上来。鉴权中心（AUC，Authentication Center）和设备识别寄存器（EIR，Equipment Identity Register）主要是负责安全性的。
另外，需要看你是本地的号，还是外地的号，这个牵扯到计费的问题，异地收费还是很贵的。访问位置寄存器（VLR，Visit Location Register）是看你目前在的地方，归属位置寄存器（HLR，Home Location Register）是看你的号码归属地。
当你的手机卡既合法又有钱的时候，才允许你上网，这个时候需要一个网关，连接核心网和真正的互联网。网关移动交换中心（GMSC ，Gateway Mobile Switching Center）就是干这个的，然后是真正的互连网。在2G时代，还是电话网络PSTN。
数据中心里面的这些模块统称为网络子系统（NSS，Network and Switching Subsystem）。
因而2G时代的上网如图所示，我们总结一下，有这几个核心点：
手机通过无线信号连接基站；
基站一面朝前接无线，一面朝后接核心网；
核心网一面朝前接到基站请求，一是判断你是否合法，二是判断你是不是本地号，还有没有钱，一面通过网关连接电话网络。
2.5G网络后来从2G到了2.5G，也即在原来电路交换的基础上，加入了分组交换业务，支持Packet的转发，从而支持IP网络。
在上述网络的基础上，基站一面朝前接无线，一面朝后接核心网。在朝后的组件中，多了一个分组控制单元（PCU，Packet Control Unit），用以提供分组交换通道。
在核心网里面，有个朝前的接待员（SGSN，Service GPRS Supported Node）和朝后连接IP网络的网关型GPRS支持节点（GGSN，Gateway GPRS Supported Node）。
3G网络到了3G时代，主要是无线通信技术有了改进，大大增加了无线的带宽。
以W-CDMA为例，理论最高2M的下行速度，因而基站改变了，一面朝外的是Node B，一面朝内连接核心网的是无线网络控制器（RNC，Radio Network Controller）。核心网以及连接的IP网络没有什么变化。</description></item><item><title>第24讲_云中网络：自己拿地成本高，购买公寓更灵活</title><link>https://artisanbox.github.io/5/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/16/</guid><description>前面我们讲了，数据中心里面堆着一大片一大片的机器，用网络连接起来，机器数目一旦非常多，人们就发现，维护这么一大片机器还挺麻烦的，有好多不灵活的地方。
采购不灵活：如果客户需要一台电脑，那就需要自己采购、上架、插网线、安装操作系统，周期非常长。一旦采购了，一用就N年，不能退货，哪怕业务不做了，机器还在数据中心里留着。
运维不灵活：一旦需要扩容CPU、内存、硬盘，都需要去机房手动弄，非常麻烦。
规格不灵活：采购的机器往往动不动几百G的内存，而每个应用往往可能只需要4核8G，所以很多应用混合部署在上面，端口各种冲突，容易相互影响。
复用不灵活：一台机器，一旦一个用户不用了，给另外一个用户，那就需要重装操作系统。因为原来的操作系统可能遗留很多数据，非常麻烦。
从物理机到虚拟机为了解决这些问题，人们发明了一种叫虚拟机的东西，并基于它产生了云计算技术。
其实在你的个人电脑上，就可以使用虚拟机。如果你对虚拟机没有什么概念，你可以下载一个桌面虚拟化的软件，自己动手尝试一下。它可以让你灵活地指定CPU的数目、内存的大小、硬盘的大小，可以有多个网卡，然后在一台笔记本电脑里面创建一台或者多台虚拟电脑。不用的时候，一点删除就没有了。
在数据中心里面，也有一种类似的开源技术qemu-kvm，能让你在一台巨大的物理机里面，掏出一台台小的机器。这套软件就能解决上面的问题：一点就能创建，一点就能销毁。你想要多大就有多大，每次创建的系统还都是新的。
我们常把物理机比喻为自己拿地盖房子，而虚拟机则相当于购买公寓，更加灵活方面，随时可买可卖。 那这个软件为什么能做到这些事儿呢？
它用的是软件模拟硬件的方式。刚才说了，数据中心里面用的qemu-kvm。从名字上来讲，emu就是Emulator（模拟器）的意思，主要会模拟CPU、内存、网络、硬盘，使得虚拟机感觉自己在使用独立的设备，但是真正使用的时候，当然还是使用物理的设备。
例如，多个虚拟机轮流使用物理CPU，内存也是使用虚拟内存映射的方式，最终映射到物理内存上。硬盘在一块大的文件系统上创建一个N个G的文件，作为虚拟机的硬盘。
简单比喻，虚拟化软件就像一个“骗子”，向上“骗”虚拟机里面的应用，让它们感觉独享资源，其实自己啥都没有，全部向下从物理机里面弄。
虚拟网卡的原理那网络是如何“骗”应用的呢？如何将虚拟机的网络和物理机的网络连接起来？
首先，虚拟机要有一张网卡。对于qemu-kvm来说，这是通过Linux上的一种TUN/TAP技术来实现的。
虚拟机是物理机上跑着的一个软件。这个软件可以像其他应用打开文件一样，打开一个称为TUN/TAP的Char Dev（字符设备文件）。打开了这个字符设备文件之后，在物理机上就能看到一张虚拟TAP网卡。
虚拟化软件作为“骗子”，会将打开的这个文件，在虚拟机里面虚拟出一张网卡，让虚拟机里面的应用觉得它们真有一张网卡。于是，所有的网络包都往这里发。
当然，网络包会到虚拟化软件这里。它会将网络包转换成为文件流，写入字符设备，就像写一个文件一样。内核中TUN/TAP字符设备驱动会收到这个写入的文件流，交给TUN/TAP的虚拟网卡驱动。这个驱动将文件流再次转成网络包，交给TCP/IP协议栈，最终从虚拟TAP网卡发出来，成为标准的网络包。
就这样，几经转手，数据终于从虚拟机里面，发到了虚拟机外面。
虚拟网卡连接到云中我们就这样有了虚拟TAP网卡。接下来就要看，这个卡怎么接入庞大的数据中心网络中。
在接入之前，我们先来看，云计算中的网络都需要注意哪些点。
共享：尽管每个虚拟机都会有一个或者多个虚拟网卡，但是物理机上可能只有有限的网卡。那这么多虚拟网卡如何共享同一个出口？
隔离：分两个方面，一个是安全隔离，两个虚拟机可能属于两个用户，那怎么保证一个用户的数据不被另一个用户窃听？一个是流量隔离，两个虚拟机，如果有一个疯狂下片，会不会导致另外一个上不了网？
互通：分两个方面，一个是如果同一台机器上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？另一个是如果不同物理机上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？
灵活：虚拟机和物理不同，会经常创建、删除，从一个机器漂移到另一台机器，有的互通、有的不通等等，灵活性比物理网络要好得多，需要能够灵活配置。
共享与互通问题这些问题，我们一个个来解决。
首先，一台物理机上有多个虚拟机，有多个虚拟网卡，这些虚拟网卡如何连在一起，进行相互访问，并且可以访问外网呢？
还记得我们在大学宿舍里做的事情吗？你可以想象你的物理机就是你们宿舍，虚拟机就是你的个人电脑，这些电脑应该怎么连接起来呢？当然应该买一个交换机。
在物理机上，应该有一个虚拟的交换机，在Linux上有一个命令叫作brctl，可以创建虚拟的网桥brctl addbr br0。创建出来以后，将两个虚拟机的虚拟网卡，都连接到虚拟网桥brctl addif br0 tap0上，这样将两个虚拟机配置相同的子网网段，两台虚拟机就能够相互通信了。
那这些虚拟机如何连接外网呢？在桌面虚拟化软件上面，我们能看到以下选项。
这里面，host-only的网络对应的，其实就是上面两个虚拟机连到一个br0虚拟网桥上，而且不考虑访问外部的场景，只要虚拟机之间能够相互访问就可以了。
如果要访问外部，往往有两种方式。
一种方式称为桥接。如果在桌面虚拟化软件上选择桥接网络，则在你的笔记本电脑上，就会形成下面的结构。
每个虚拟机都会有虚拟网卡，在你的笔记本电脑上，会发现多了几个网卡，其实是虚拟交换机。这个虚拟交换机将虚拟机连接在一起。在桥接模式下，物理网卡也连接到这个虚拟交换机上，物理网卡在桌面虚拟化软件上，在“界面名称”那里选定。
如果使用桥接网络，当你登录虚拟机里看IP地址的时候会发现，你的虚拟机的地址和你的笔记本电脑的，以及你旁边的同事的电脑的网段是一个网段。这是为什么呢？这其实相当于将物理机和虚拟机放在同一个网桥上，相当于这个网桥上有三台机器，是一个网段的，全部打平了。我将图画成下面的样子你就好理解了。
在数据中心里面，采取的也是类似的技术，只不过都是Linux，在每台机器上都创建网桥br0，虚拟机的网卡都连到br0上，物理网卡也连到br0上，所有的br0都通过物理网卡出来连接到物理交换机上。
同样我们换一个角度看待这个拓扑图。同样是将网络打平，虚拟机会和你的物理网络具有相同的网段。
在这种方式下，不但解决了同一台机器的互通问题，也解决了跨物理机的互通问题，因为都在一个二层网络里面，彼此用相同的网段访问就可以了。但是当规模很大的时候，会存在问题。
你还记得吗？在一个二层网络里面，最大的问题是广播。一个数据中心的物理机已经很多了，广播已经非常严重，需要通过VLAN进行划分。如果使用了虚拟机，假设一台物理机里面创建10台虚拟机，全部在一个二层网络里面，那广播就会很严重，所以除非是你的桌面虚拟机或者数据中心规模非常小，才可以使用这种相对简单的方式。
另外一种方式称为NAT。如果在桌面虚拟化软件中使用NAT模式，在你的笔记本电脑上会出现如下的网络结构。
在这种方式下，你登录到虚拟机里面查看IP地址，会发现虚拟机的网络是虚拟机的，物理机的网络是物理机的，两个不相同。虚拟机要想访问物理机的时候，需要将地址NAT成为物理机的地址。
除此之外，它还会在你的笔记本电脑里内置一个DHCP服务器，为笔记本电脑上的虚拟机动态分配IP地址。因为虚拟机的网络自成体系，需要进行IP管理。为什么桥接方式不需要呢？因为桥接将网络打平了，虚拟机的IP地址应该由物理网络的DHCP服务器分配。
在数据中心里面，也是使用类似的方式。这种方式更像是真的将你宿舍里面的情况，搬到一台物理机上来。
虚拟机是你的电脑，路由器和DHCP Server相当于家用路由器或者寝室长的电脑，物理网卡相当于你们宿舍的外网网口，用于访问互联网。所有电脑都通过内网网口连接到一个网桥br0上，虚拟机要想访问互联网，需要通过br0连到路由器上，然后通过路由器将请求NAT成为物理网络的地址，转发到物理网络。</description></item><item><title>第25讲_软件定义网络：共享基础设施的小区物业管理办法</title><link>https://artisanbox.github.io/5/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/17/</guid><description>上一节我们说到，使用原生的VLAN和Linux网桥的方式来进行云平台的管理，但是这样在灵活性、隔离性方面都显得不足，而且整个网络缺少统一的视图、统一的管理。
可以这样比喻，云计算就像大家一起住公寓，要共享小区里面的基础设施，其中网络就相当于小区里面的电梯、楼道、路、大门等，大家都走，往往会常出现问题，尤其在上班高峰期，出门的人太多，对小区的物业管理就带来了挑战。
物业可以派自己的物业管理人员，到每个单元的楼梯那里，将电梯的上下行速度调快一点，可以派人将隔离健身区、景色区的栅栏门暂时打开，让大家可以横穿小区，直接上地铁，还可以派人将多个小区出入口，改成出口多、入口少等等。等过了十点半，上班高峰过去，再派人都改回来。
软件定义网络（SDN）这种模式就像传统的网络设备和普通的Linux网桥的模式，配置整个云平台的网络通路，你需要登录到这台机器上配置这个，再登录到另外一个设备配置那个，才能成功。
如果物业管理人员有一套智能的控制系统，在物业监控室里就能看到小区里每个单元、每个电梯的人流情况，然后在监控室里面，只要通过远程控制的方式，拨弄一个手柄，电梯的速度就调整了，栅栏门就打开了，某个入口就改出口了。
这就是软件定义网络（SDN）。它主要有以下三个特点。
控制与转发分离：转发平面就是一个个虚拟或者物理的网络设备，就像小区里面的一条条路。控制平面就是统一的控制中心，就像小区物业的监控室。它们原来是一起的，物业管理员要从监控室出来，到路上去管理设备，现在是分离的，路就是走人的，控制都在监控室。
控制平面与转发平面之间的开放接口：控制器向上提供接口，被应用层调用，就像总控室提供按钮，让物业管理员使用。控制器向下调用接口，来控制网络设备，就像总控室会远程控制电梯的速度。这里经常使用两个名词，前面这个接口称为北向接口，后面这个接口称为南向接口，上北下南嘛。
逻辑上的集中控制：逻辑上集中的控制平面可以控制多个转发面设备，也就是控制整个物理网络，因而可以获得全局的网络状态视图，并根据该全局网络状态视图实现对网络的优化控制，就像物业管理员在监控室能够看到整个小区的情况，并根据情况优化出入方案。
OpenFlow和OpenvSwitchSDN有很多种实现方式，我们来看一种开源的实现方式。
OpenFlow是SDN控制器和网络设备之间互通的南向接口协议，OpenvSwitch用于创建软件的虚拟交换机。OpenvSwitch是支持OpenFlow协议的，当然也有一些硬件交换机也支持OpenFlow协议。它们都可以被统一的SDN控制器管理，从而实现物理机和虚拟机的网络连通。
SDN控制器是如何通过OpenFlow协议控制网络的呢？
在OpenvSwitch里面，有一个流表规则，任何通过这个交换机的包，都会经过这些规则进行处理，从而接收、转发、放弃。
那流表长啥样呢？其实就是一个个表格，每个表格好多行，每行都是一条规则。每条规则都有优先级，先看高优先级的规则，再看低优先级的规则。
对于每一条规则，要看是否满足匹配条件。这些条件包括，从哪个端口进来的，网络包头里面有什么等等。满足了条件的网络包，就要执行一个动作，对这个网络包进行处理。可以修改包头里的内容，可以跳到任何一个表格，可以转发到某个网口出去，也可以丢弃。
通过这些表格，可以对收到的网络包随意处理。
具体都能做什么处理呢？通过上面的表格可以看出，简直是想怎么处理怎么处理，可以覆盖TCP/IP协议栈的四层。
对于物理层：
匹配规则包括从哪个口进来；
执行动作包括从哪个口出去。
对于MAC层：
匹配规则包括：源MAC地址是多少？（dl_src），目标MAC是多少？（dl_dst），所属vlan是多少？（dl_vlan）；
执行动作包括：修改源MAC（mod_dl_src），修改目标MAC（mod_dl_dst），修改VLAN（mod_vlan_vid），删除VLAN（strip_vlan），MAC地址学习（learn）。
对于网络层：
匹配规则包括：源IP地址是多少？(nw_src)，目标IP是多少？（nw_dst）。
执行动作包括：修改源IP地址（mod_nw_src），修改目标IP地址（mod_nw_dst）。
对于传输层：
匹配规则包括：源端口是多少？（tp_src），目标端口是多少？（tp_dst）。
执行动作包括：修改源端口（mod_tp_src），修改目标端口（mod_tp_dst）。
总而言之，对于OpenvSwitch来讲，网络包到了我手里，就是一个Buffer，我想怎么改怎么改，想发到哪个端口就发送到哪个端口。
OpenvSwitch有本地的命令行可以进行配置，能够实验咱们前面讲过的一些功能。我们可以通过OpenvSwitch的命令创建一个虚拟交换机。然后可以将多个虚拟端口port添加到这个虚拟交换机上。比如说下面这个add-br命令，就是创建虚拟交换机的。
ovs-vsctl add-br br0 实验一：用OpenvSwitch实现VLAN的功能下面我们实验一下通过OpenvSwitch实现VLAN的功能，在OpenvSwitch中端口port分两种，分别叫做access port和trunk port。
第一类是access port：
这个端口可以配置一个tag，其实就是一个VLAN ID，从这个端口进来的包都会被打上这个tag； 如果网络包本身带有某个VLAN ID并且等于这个tag，则这个包就会从这个port发出去； 从access port发出的包就会把VLAN ID去掉。 第二类是trunk port：</description></item><item><title>第26讲_云中的网络安全：虽然不是土豪，也需要基本安全和保障</title><link>https://artisanbox.github.io/5/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/18/</guid><description>在今天的内容开始之前，我先卖个关子。文章结尾，我会放一个超级彩蛋，所以，今天的内容你一定要看到最后哦！
上一节我们看到，做一个小区物业维护一个大家共享的环境，还是挺不容易的。如果都是自觉遵守规则的住户那还好，如果遇上不自觉的住户就会很麻烦。
就像公有云的环境，其实没有你想的那么纯净，各怀鬼胎的黑客到处都是。扫描你的端口呀，探测一下你启动的什么应用啊，看一看是否有各种漏洞啊。这就像小偷潜入小区后，这儿看看，那儿瞧瞧，窗户有没有关严了啊，窗帘有没有拉上啊，主人睡了没，是不是时机潜入室内啊，等等。
假如你创建了一台虚拟机，里面明明跑了一个电商应用，这是你非常重要的一个应用，你会把它进行安全加固。这台虚拟机的操作系统里，不小心安装了另外一个后台应用，监听着一个端口，而你的警觉性没有这么高。
虚拟机的这个端口是对着公网开放的，碰巧这个后台应用本身是有漏洞的，黑客就可以扫描到这个端口，然后通过这个后台应用的端口侵入你的机器，将你加固好的电商网站黑掉。这就像你买了一个五星级的防盗门，卡车都撞不开，但是厕所窗户的门把手是坏的，小偷从厕所里面就进来了。
所以对于公有云上的虚拟机，我的建议是仅仅开放需要的端口，而将其他的端口一概关闭。这个时候，你只要通过安全措施守护好这个唯一的入口就可以了。采用的方式常常是用ACL（Access Control List，访问控制列表）来控制IP和端口。
设置好了这些规则，只有指定的IP段能够访问指定的开放接口，就算有个有漏洞的后台进程在那里，也会被屏蔽，黑客进不来。在云平台上，这些规则的集合常称为安全组。那安全组怎么实现呢？
我们来复习一下，当一个网络包进入一台机器的时候，都会做什么事情。
首先拿下MAC头看看，是不是我的。如果是，则拿下IP头来。得到目标IP之后呢，就开始进行路由判断。在路由判断之前，这个节点我们称为PREROUTING。如果发现IP是我的，包就应该是我的，就发给上面的传输层，这个节点叫作INPUT。如果发现IP不是我的，就需要转发出去，这个节点称为FORWARD。如果是我的，上层处理完毕后，一般会返回一个处理结果，这个处理结果会发出去，这个节点称为OUTPUT，无论是FORWARD还是OUTPUT，都是路由判断之后发生的，最后一个节点是POSTROUTING。
整个过程如图所示。
整个包的处理过程还是原来的过程，只不过为什么要格外关注这五个节点呢？
是因为在Linux内核中，有一个框架叫Netfilter。它可以在这些节点插入hook函数。这些函数可以截获数据包，对数据包进行干预。例如做一定的修改，然后决策是否接着交给TCP/IP协议栈处理；或者可以交回给协议栈，那就是ACCEPT；或者过滤掉，不再传输，就是DROP；还有就是QUEUE，发送给某个用户态进程处理。
这个比较难理解，经常用在内部负载均衡，就是过来的数据一会儿传给目标地址1，一会儿传给目标地址2，而且目标地址的个数和权重都可能变。协议栈往往处理不了这么复杂的逻辑，需要写一个函数接管这个数据，实现自己的逻辑。
有了这个Netfilter框架就太好了，你可以在IP转发的过程中，随时干预这个过程，只要你能实现这些hook函数。
一个著名的实现，就是内核模块ip_tables。它在这五个节点上埋下函数，从而可以根据规则进行包的处理。按功能可分为四大类：连接跟踪（conntrack）、数据包的过滤（filter）、网络地址转换（nat）和数据包的修改（mangle）。其中连接跟踪是基础功能，被其他功能所依赖。其他三个可以实现包的过滤、修改和网络地址转换。
在用户态，还有一个你肯定知道的客户端程序iptables，用命令行来干预内核的规则。内核的功能对应iptables的命令行来讲，就是表和链的概念。
iptables的表分为四种：raw--&amp;gt;mangle--&amp;gt;nat--&amp;gt;filter。这四个优先级依次降低，raw不常用，所以主要功能都在其他三种表里实现。每个表可以设置多个链。
filter表处理过滤功能，主要包含三个链：
INPUT链：过滤所有目标地址是本机的数据包；
FORWARD链：过滤所有路过本机的数据包；
OUTPUT链：过滤所有由本机产生的数据包。
nat表主要是处理网络地址转换，可以进行Snat（改变数据包的源地址）、Dnat（改变数据包的目标地址），包含三个链：
PREROUTING链：可以在数据包到达防火墙时改变目标地址；
OUTPUT链：可以改变本地产生的数据包的目标地址；
POSTROUTING链：在数据包离开防火墙时改变数据包的源地址。
mangle表主要是修改数据包，包含：
PREROUTING链；
INPUT链；
FORWARD链；
OUTPUT链；
POSTROUTING链。
将iptables的表和链加入到上面的过程图中，就形成了下面的图和过程。
数据包进入的时候，先进mangle表的PREROUTING链。在这里可以根据需要，改变数据包头内容之后，进入nat表的PREROUTING链，在这里可以根据需要做Dnat，也就是目标地址转换。
进入路由判断，要判断是进入本地的还是转发的。
如果是进入本地的，就进入INPUT链，之后按条件过滤限制进入。</description></item><item><title>第27讲_云中的网络QoS：邻居疯狂下电影，我该怎么办？</title><link>https://artisanbox.github.io/5/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/19/</guid><description>在小区里面，是不是经常有住户不自觉就霸占公共通道，如果你找他理论，他的话就像一个相声《楼道曲》说的一样：“公用公用，你用我用，大家都用，我为什么不能用？”。
除此之外，你租房子的时候，有没有碰到这样的情况：本来合租共享WiFi，一个人狂下小电影，从而你网都上不去，是不是很懊恼？
在云平台上，也有这种现象，好在有一种流量控制的技术，可以实现QoS（Quality of Service），从而保障大多数用户的服务质量。
对于控制一台机器的网络的QoS，分两个方向，一个是入方向，一个是出方向。
其实我们能控制的只有出方向，通过Shaping，将出的流量控制成自己想要的模样。而进入的方向是无法控制的，只能通过Policy将包丢弃。
控制网络的QoS有哪些方式？在Linux下，可以通过TC控制网络的QoS，主要就是通过队列的方式。
无类别排队规则第一大类称为无类别排队规则（Classless Queuing Disciplines）。还记得我们讲ip addr的时候讲过的pfifo_fast，这是一种不把网络包分类的技术。
pfifo_fast分为三个先入先出的队列，称为三个Band。根据网络包里面TOS，看这个包到底应该进入哪个队列。TOS总共四位，每一位表示的意思不同，总共十六种类型。
通过命令行tc qdisc show dev eth0，可以输出结果priomap，也是十六个数字。在0到2之间，和TOS的十六种类型对应起来，表示不同的TOS对应的不同的队列。其中Band 0优先级最高，发送完毕后才轮到Band 1发送，最后才是Band 2。
另外一种无类别队列规则叫作随机公平队列（Stochastic Fair Queuing）。
会建立很多的FIFO的队列，TCP Session会计算hash值，通过hash值分配到某个队列。在队列的另一端，网络包会通过轮询策略从各个队列中取出发送。这样不会有一个Session占据所有的流量。
当然如果两个Session的hash是一样的，会共享一个队列，也有可能互相影响。hash函数会经常改变，从而session不会总是相互影响。
还有一种无类别队列规则称为令牌桶规则（TBF，Token Bucket Filte）。
所有的网络包排成队列进行发送，但不是到了队头就能发送，而是需要拿到令牌才能发送。
令牌根据设定的速度生成，所以即便队列很长，也是按照一定的速度进行发送的。
当没有包在队列中的时候，令牌还是以既定的速度生成，但是不是无限累积的，而是放满了桶为止。设置桶的大小为了避免下面的情况：当长时间没有网络包发送的时候，积累了大量的令牌，突然来了大量的网络包，每个都能得到令牌，造成瞬间流量大增。
基于类别的队列规则另外一大类是基于类别的队列规则（Classful Queuing Disciplines），其中典型的为分层令牌桶规则（HTB， Hierarchical Token Bucket）。
HTB往往是一棵树，接下来我举个具体的例子，通过TC如何构建一棵HTB树来带你理解。
使用TC可以为某个网卡eth0创建一个HTB的队列规则，需要付给它一个句柄为（1:）。
这是整棵树的根节点，接下来会有分支。例如图中有三个分支，句柄分别为（:10）、（:11）、（:12）。最后的参数default 12，表示默认发送给1:12，也即发送给第三个分支。
tc qdisc add dev eth0 root handle 1: htb default 12 对于这个网卡，需要规定发送的速度。一般有两个速度可以配置，一个是rate，表示一般情况下的速度；一个是ceil，表示最高情况下的速度。对于根节点来讲，这两个速度是一样的，于是创建一个root class，速度为（rate=100kbps，ceil=100kbps）。
tc class add dev eth0 parent 1: classid 1:1 htb rate 100kbps ceil 100kbps 接下来要创建分支，也即创建几个子class。每个子class统一有两个速度。三个分支分别为（rate=30kbps，ceil=100kbps）、（rate=10kbps，ceil=100kbps）、（rate=60kbps，ceil=100kbps）。</description></item><item><title>第28讲_云中网络的隔离GRE、VXLAN：虽然住一个小区，也要保护隐私</title><link>https://artisanbox.github.io/5/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/20/</guid><description>对于云平台中的隔离问题，前面咱们用的策略一直都是VLAN，但是我们也说过这种策略的问题，VLAN只有12位，共4096个。当时设计的时候，看起来是够了，但是现在绝对不够用，怎么办呢？
一种方式是修改这个协议。这种方法往往不可行，因为当这个协议形成一定标准后，千千万万设备上跑的程序都要按这个规则来。现在说改就放，谁去挨个儿告诉这些程序呢？很显然，这是一项不可能的工程。
另一种方式就是扩展，在原来包的格式的基础上扩展出一个头，里面包含足够用于区分租户的ID，外层的包的格式尽量和传统的一样，依然兼容原来的格式。一旦遇到需要区分用户的地方，我们就用这个特殊的程序，来处理这个特殊的包的格式。
这个概念很像咱们第22讲讲过的隧道理论，还记得自驾游通过摆渡轮到海南岛的那个故事吗？在那一节，我们说过，扩展的包头主要是用于加密的，而我们现在需要的包头是要能够区分用户的。
底层的物理网络设备组成的网络我们称为Underlay网络，而用于虚拟机和云中的这些技术组成的网络称为Overlay网络，这是一种基于物理网络的虚拟化网络实现。这一节我们重点讲两个Overlay的网络技术。
GRE第一个技术是GRE，全称Generic Routing Encapsulation，它是一种IP-over-IP的隧道技术。它将IP包封装在GRE包里，外面加上IP头，在隧道的一端封装数据包，并在通路上进行传输，到另外一端的时候解封装。你可以认为Tunnel是一个虚拟的、点对点的连接。
从这个图中可以看到，在GRE头中，前32位是一定会有的，后面的都是可选的。在前4位标识位里面，有标识后面到底有没有可选项？这里面有个很重要的key字段，是一个32位的字段，里面存放的往往就是用于区分用户的Tunnel ID。32位，够任何云平台喝一壶的了！
下面的格式类型专门用于网络虚拟化的GRE包头格式，称为NVGRE，也给网络ID号24位，也完全够用了。
除此之外，GRE还需要有一个地方来封装和解封装GRE的包，这个地方往往是路由器或者有路由功能的Linux机器。
使用GRE隧道，传输的过程就像下面这张图。这里面有两个网段、两个路由器，中间要通过GRE隧道进行通信。当隧道建立之后，会多出两个Tunnel端口，用于封包、解封包。
主机A在左边的网络，IP地址为192.168.1.102，它想要访问主机B，主机B在右边的网络，IP地址为192.168.2.115。于是发送一个包，源地址为192.168.1.102，目标地址为192.168.2.115。因为要跨网段访问，于是根据默认的default路由表规则，要发给默认的网关192.168.1.1，也即左边的路由器。
根据路由表，从左边的路由器，去192.168.2.0/24这个网段，应该走一条GRE的隧道，从隧道一端的网卡Tunnel0进入隧道。
在Tunnel隧道的端点进行包的封装，在内部的IP头之外加上GRE头。对于NVGRE来讲，是在MAC头之外加上GRE头，然后加上外部的IP地址，也即路由器的外网IP地址。源IP地址为172.17.10.10，目标IP地址为172.16.11.10，然后从E1的物理网卡发送到公共网络里。
在公共网络里面，沿着路由器一跳一跳地走，全部都按照外部的公网IP地址进行。
当网络包到达对端路由器的时候，也要到达对端的Tunnel0，然后开始解封装，将外层的IP头取下来，然后根据里面的网络包，根据路由表，从E3口转发出去到达服务器B。
从GRE的原理可以看出，GRE通过隧道的方式，很好地解决了VLAN ID不足的问题。但是，GRE技术本身还是存在一些不足之处。
首先是Tunnel的数量问题。GRE是一种点对点隧道，如果有三个网络，就需要在每两个网络之间建立一个隧道。如果网络数目增多，这样隧道的数目会呈指数性增长。
其次，GRE不支持组播，因此一个网络中的一个虚机发出一个广播帧后，GRE会将其广播到所有与该节点有隧道连接的节点。
另外一个问题是目前还是有很多防火墙和三层网络设备无法解析GRE，因此它们无法对GRE封装包做合适地过滤和负载均衡。
VXLAN第二种Overlay的技术称为VXLAN。和三层外面再套三层的GRE不同，VXLAN则是从二层外面就套了一个VXLAN的头，这里面包含的VXLAN ID为24位，也够用了。在VXLAN头外面还封装了UDP、IP，以及外层的MAC头。
VXLAN作为扩展性协议，也需要一个地方对VXLAN的包进行封装和解封装，实现这个功能的点称为VTEP（VXLAN Tunnel Endpoint）。
VTEP相当于虚拟机网络的管家。每台物理机上都可以有一个VTEP。每个虚拟机启动的时候，都需要向这个VTEP管家注册，每个VTEP都知道自己上面注册了多少个虚拟机。当虚拟机要跨VTEP进行通信的时候，需要通过VTEP代理进行，由VTEP进行包的封装和解封装。
和GRE端到端的隧道不同，VXLAN不是点对点的，而是支持通过组播的来定位目标机器的，而非一定是这一端发出，另一端接收。
当一个VTEP启动的时候，它们都需要通过IGMP协议。加入一个组播组，就像加入一个邮件列表，或者加入一个微信群一样，所有发到这个邮件列表里面的邮件，或者发送到微信群里面的消息，大家都能收到。而当每个物理机上的虚拟机启动之后，VTEP就知道，有一个新的VM上线了，它归我管。
如图，虚拟机1、2、3属于云中同一个用户的虚拟机，因而需要分配相同的VXLAN ID=101。在云的界面上，就可以知道它们的IP地址，于是可以在虚拟机1上ping虚拟机2。
虚拟机1发现，它不知道虚拟机2的MAC地址，因而包没办法发出去，于是要发送ARP广播。
ARP请求到达VTEP1的时候，VTEP1知道，我这里有一台虚拟机，要访问一台不归我管的虚拟机，需要知道MAC地址，可是我不知道啊，这该咋办呢？
VTEP1想，我不是加入了一个微信群么？可以在里面@all 一下，问问虚拟机2归谁管。于是VTEP1将ARP请求封装在VXLAN里面，组播出去。
当然在群里面，VTEP2和VTEP3都收到了消息，因而都会解开VXLAN包看，里面是一个ARP。
VTEP3在本地广播了半天，没人回，都说虚拟机2不归自己管。
VTEP2在本地广播，虚拟机2回了，说虚拟机2归我管，MAC地址是这个。通过这次通信，VTEP2也学到了，虚拟机1归VTEP1管，以后要找虚拟机1，去找VTEP1就可以了。
VTEP2将ARP的回复封装在VXLAN里面，这次不用组播了，直接发回给VTEP1。
VTEP1解开VXLAN的包，发现是ARP的回复，于是发给虚拟机1。通过这次通信，VTEP1也学到了，虚拟机2归VTEP2管，以后找虚拟机2，去找VTEP2就可以了。
虚拟机1的ARP得到了回复，知道了虚拟机2的MAC地址，于是就可以发送包了。
虚拟机1发给虚拟机2的包到达VTEP1，它当然记得刚才学的东西，要找虚拟机2，就去VTEP2，于是将包封装在VXLAN里面，外层加上VTEP1和VTEP2的IP地址，发送出去。
网络包到达VTEP2之后，VTEP2解开VXLAN封装，将包转发给虚拟机2。
虚拟机2回复的包，到达VTEP2的时候，它当然也记得刚才学的东西，要找虚拟机1，就去VTEP1，于是将包封装在VXLAN里面，外层加上VTEP1和VTEP2的IP地址，也发送出去。
网络包到达VTEP1之后，VTEP1解开VXLAN封装，将包转发给虚拟机1。
有了GRE和VXLAN技术，我们就可以解决云计算中VLAN的限制了。那如何将这个技术融入云平台呢？
还记得将你宿舍里面的情况，所有东西都搬到一台物理机上那个故事吗？
虚拟机是你的电脑，路由器和DHCP Server相当于家用路由器或者寝室长的电脑，外网网口访问互联网，所有的电脑都通过内网网口连接到一个交换机br0上，虚拟机要想访问互联网，需要通过br0连到路由器上，然后通过路由器将请求NAT后转发到公网。
接下来的事情就惨了，你们宿舍闹矛盾了，你们要分成三个宿舍住，对应上面的图，你们寝室长，也即路由器单独在一台物理机上，其他的室友也即VM分别在两台物理机上。这下把一个完整的br0一刀三断，每个宿舍都是单独的一段。
可是只有你的寝室长有公网口可以上网，于是你偷偷在三个宿舍中间打了一个隧道，用网线通过隧道将三个宿舍的两个br0连接起来，让其他室友的电脑和你寝室长的电脑，看起来还是连到同一个br0上，其实中间是通过你隧道中的网线做了转发。
为什么要多一个br1这个虚拟交换机呢？主要通过br1这一层将虚拟机之间的互联和物理机机之间的互联分成两层来设计，中间隧道可以有各种挖法，GRE、VXLAN都可以。
使用了OpenvSwitch之后，br0可以使用OpenvSwitch的Tunnel功能和Flow功能。
OpenvSwitch支持三类隧道：GRE、VXLAN、IPsec_GRE。在使用OpenvSwitch的时候，虚拟交换机就相当于GRE和VXLAN封装的端点。
我们模拟创建一个如下的网络拓扑结构，来看隧道应该如何工作。
三台物理机，每台上都有两台虚拟机，分别属于两个不同的用户，因而VLAN tag都得打地不一样，这样才不能相互通信。但是不同物理机上的相同用户，是可以通过隧道相互通信的，因而通过GRE隧道可以连接到一起。</description></item><item><title>第29讲_容器网络：来去自由的日子，不买公寓去合租</title><link>https://artisanbox.github.io/5/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/21/</guid><description>如果说虚拟机是买公寓，容器则相当于合租，有一定的隔离，但是隔离性没有那么好。云计算解决了基础资源层的弹性伸缩，却没有解决PaaS层应用随基础资源层弹性伸缩而带来的批量、快速部署问题。于是，容器应运而生。
容器就是Container，而Container的另一个意思是集装箱。其实容器的思想就是要变成软件交付的集装箱。集装箱的特点，一是打包，二是标准。
在没有集装箱的时代，假设要将货物从A运到B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，弄得乱七八糟，然后还要再搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能干完活。
有了尺寸全部都一样的集装箱以后，可以把所有的货物都打包在一起，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不用耗费很长时间了。这是集装箱的“打包”“标准”两大特点在生活中的应用。
那么容器如何对应用打包呢？
学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。
封闭的环境主要使用了两种技术，一种是看起来是隔离的技术，称为namespace，也即每个 namespace中的应用看到的是不同的 IP地址、用户空间、程号等。另一种是用起来是隔离的技术，称为cgroup，也即明明整台机器有很多的 CPU、内存，而一个应用只能用其中的一部分。
有了这两项技术，就相当于我们焊好了集装箱。接下来的问题就是如何“将这个集装箱标准化”，并在哪艘船上都能运输。这里的标准首先就是镜像。
所谓镜像，就是将你焊好集装箱的那一刻，将集装箱的状态保存下来，就像孙悟空说：“定！”，集装箱里的状态就被定在了那一刻，然后将这一刻的状态保存成一系列文件。无论从哪里运行这个镜像，都能完整地还原当时的情况。
接下来我们就具体来看看，这两种网络方面的打包技术。
命名空间（namespace）我们首先来看网络namespace。
namespace翻译过来就是命名空间。其实很多面向对象的程序设计语言里面，都有命名空间这个东西。大家一起写代码，难免会起相同的名词，编译就会冲突。而每个功能都有自己的命名空间，在不同的空间里面，类名相同，不会冲突。
在Linux下也是这样的，很多的资源都是全局的。比如进程有全局的进程ID，网络也有全局的路由表。但是，当一台Linux上跑多个进程的时候，如果我们觉得使用不同的路由策略，这些进程可能会冲突，那就需要将这个进程放在一个独立的namespace里面，这样就可以独立配置网络了。
网络的namespace由ip netns命令操作。它可以创建、删除、查询namespace。
我们再来看将你们宿舍放进一台物理机的那个图。你们宿舍长的电脑是一台路由器，你现在应该知道怎么实现这个路由器吧？可以创建一个Router虚拟机来做这件事情，但是还有一个更加简单的办法，就是我在图里画的这条虚线，这个就是通过namespace实现的。
我们创建一个routerns，于是一个独立的网络空间就产生了。你可以在里面尽情设置自己的规则。
ip netns add routerns 既然是路由器，肯定要能转发嘛，因而forward开关要打开。
ip netns exec routerns sysctl -w net.ipv4.ip_forward=1 exec的意思就是进入这个网络空间做点事情。初始化一下iptables，因为这里面要配置NAT规则。
ip netns exec routerns iptables-save -c ip netns exec routerns iptables-restore -c 路由器需要有一张网卡连到br0上，因而要创建一个网卡。
ovs-vsctl -- add-port br0 taprouter -- set Interface taprouter type=internal -- set Interface taprouter external-ids:iface-status=active -- set Interface taprouter external-ids:attached-mac=fa:16:3e:84:6e:cc 这个网络创建完了，但是是在namespace外面的，如何进去呢？可以通过这个命令：
ip link set taprouter netns routerns 要给这个网卡配置一个IP地址，当然应该是虚拟机网络的网关地址。例如虚拟机私网网段为192.</description></item><item><title>第2季回归_这一次，我们一起拿下设计模式！</title><link>https://artisanbox.github.io/2/74/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/74/</guid><description>你好，我是王争。“数据结构与算法之美”在今年2月底全部更新完毕。时隔8个月，我又为你带来了一个新的专栏“设计模式之美”。如果说“数据结构与算法之美”是教你如何写出高效的代码，那“设计模式之美”就是教你如何写出高质量的代码。
在设计“设计模式之美”专栏的时候，我仍然延续“数据结构与算法之美”的讲述方式。在专栏的整体设计上，我希望尽量还原一对一、手把手code review的场景，通过100篇正文和10篇不定期加餐，200多个真实的项目实战代码案例剖析，100多个有深度的课堂讨论、头脑风暴，来为你交付这个“设计模式之美”专栏。
我希望通过这个专栏，一次性把跟编写高质量代码相关的所有知识，都系统、全面地讲清楚，一次性给你讲透彻。让你看完这个专栏，就能搞清楚所有跟写高质量代码相关的知识点。
专栏共100期正文和10期不定期加餐，分为5个模块。下面是专栏的目录：
为了感谢老同学，我为你准备了一个专属福利：
11月4日，专栏上新时，我会送你一张30元专属优惠券，可与限时优惠同享，有效期48小时，建议尽早使用。点击下方图片，立即免费试读新专栏。
一段新的征程，期待与你一起见证成长！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>第2季回归_这次我们来“趣谈Linux操作系统”</title><link>https://artisanbox.github.io/5/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/22/</guid><description>你好，我是你的老朋友刘超。在“趣谈网络协议”结课半年之后，我又给你带来了一个新的基础课程，“趣谈Linux操作系统”。
在咱们“趣谈网络协议”的留言里，我和同学们进行了很多互动，同时，我也和其他做基础知识专栏的作者有了不少交流，我发现，无论是从个人的职业发展角度，还是从公司招聘候选人的角度来看，扎实的基础知识是很多人的诉求。这让我更加坚信，我应该在“趣谈基础知识”这条道路上走下去。
在设计“趣谈Linux操作系统”专栏的时候，我仍然秉承“趣谈”和“故事化”的方式，将枯燥的基础知识结合某个场景，给你生动、具象地讲述出来，帮你加深理解、巩固记忆、夯实基础。
在我看来，操作系统在计算机中承担着“大管家”的角色，这个“大管家”就好比一家公司的老板，我们的目标就是把这家公司做上市，具体的过程，我用了一张图来表示：
Linux操作系统中的概念非常多，数据结构也很多，流程也复杂，一般人在学习的过程中很容易迷路。我希望能够将这些复杂的概念、数据结构、流程通俗地讲解出来，争取每篇文章都用一张图串起这篇的知识点。
最终，整个专栏下来，你如果能把这些图都掌握了，你的知识就会形成体系和连接。在此基础上再进行深入学习，就会如鱼得水、易如反掌。
一段新的征途即将开始，期待与你继续同行。为了感谢老同学，我为你送上一张10元专属优惠券，可以与限时优惠同享，优惠券有效期仅5天，建议你抓紧使用。点击下方图片，可试读专栏最新文章。
我们新专栏见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>第2讲_网络分层的真实含义是什么？</title><link>https://artisanbox.github.io/5/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/23/</guid><description>长时间从事计算机网络相关的工作，我发现，计算机网络有一个显著的特点，就是这是一个不仅需要背诵，而且特别需要将原理烂熟于胸的学科。很多问题看起来懂了，但是就怕往细里问，一问就发现你懂得没有那么透彻。
我们上一节列了之后要讲的网络协议。这些协议本来没什么稀奇，每一本教科书都会讲，并且都要求你背下来。因为考试会考，面试会问。可以这么说，毕业了去找工作还答不出这类题目的，那你的笔试基本上也就挂了。
当你听到什么二层设备、三层设备、四层LB和七层LB中层的时候，是否有点一头雾水，不知道这些所谓的层，对应的各种协议具体要做什么“工作”？
这四个问题你真的懂了吗？ 因为教科书或者老师往往会打一个十分不恰当的比喻：为什么网络要分层呀？因为不同的层次之间有不同的沟通方式，这个叫作协议。例如，一家公司也是分“层次”的，分总经理、经理、组长、员工。总经理之间有他们的沟通方式，经理和经理之间也有沟通方式，同理组长和员工。有没有听过类似的比喻？
那么第一个问题来了。请问经理在握手的时候，员工在干什么？很多人听过TCP建立连接的三次握手协议，也会把它当知识点背诵。同理问你，TCP在进行三次握手的时候，IP层和MAC层对应都有什么操作呢？
除了上面这个不恰当的比喻，教科书还会列出每个层次所包含的协议，然后开始逐层地去讲这些协议。但是这些协议之间的关系呢？却很少有教科书会讲。
学习第三层的时候会提到，IP协议里面包含目标地址和源地址。第三层里往往还会学习路由协议。路由就像中转站，我们从原始地址A到目标地址D，中间经过两个中转站A-&amp;gt;B-&amp;gt;C-&amp;gt;D，是通过路由转发的。
那么第二个问题来了。A知道自己的下一个中转站是B，那从A发出来的包，应该把B的IP地址放在哪里呢？B知道自己的下一个中转站是C，从B发出来的包，应该把C的IP地址放在哪里呢？如果放在IP协议中的目标地址，那包到了中转站，怎么知道最终的目的地址是D呢？
教科书不会通过场景化的例子，将网络包的生命周期讲出来，所以你就会很困惑，不知道这些协议实际的应用场景是什么。
我再问你一个问题。你一定经常听说二层设备、三层设备。二层设备处理的通常是MAC层的东西。那我发送一个HTTP的包，是在第七层工作的，那是不是不需要经过二层设备？或者即便经过了，二层设备也不处理呢？或者换一种问法，二层设备处理的包里，有没有HTTP层的内容呢？
最终，我想问你一个综合的问题。从你的电脑，通过SSH登录到公有云主机里面，都需要经历哪些过程？或者说你打开一个电商网站，都需要经历哪些过程？说得越详细越好。
实际情况可能是，很多人回答不上来。尽管对每一层都很熟悉，但是知识点却串不起来。
上面的这些问题，有的在这一节就会有一个解释，有的则会贯穿我们整个课程。好在后面一节中我会举一个贯穿的例子，将很多层的细节讲过后，你很容易就能把这些知识点串起来。
网络为什么要分层？ 这里我们先探讨第一个问题，网络为什么要分层？因为，是个复杂的程序都要分层。
理解计算机网络中的概念，一个很好的角度是，想象网络包就是一段Buffer，或者一块内存，是有格式的。同时，想象自己是一个处理网络包的程序，而且这个程序可以跑在电脑上，可以跑在服务器上，可以跑在交换机上，也可以跑在路由器上。你想象自己有很多的网口，从某个口拿进一个网络包来，用自己的程序处理一下，再从另一个网口发送出去。
当然网络包的格式很复杂，这个程序也很复杂。复杂的程序都要分层，这是程序设计的要求。比如，复杂的电商还会分数据库层、缓存层、Compose层、Controller层和接入层，每一层专注做本层的事情。
程序是如何工作的？ 我们可以简单地想象“你”这个程序的工作过程。
当一个网络包从一个网口经过的时候，你看到了，首先先看看要不要请进来，处理一把。有的网口配置了混杂模式，凡是经过的，全部拿进来。
拿进来以后，就要交给一段程序来处理。于是，你调用process_layer2(buffer)。当然，这是一个假的函数。但是你明白其中的意思，知道肯定是有这么个函数的。那这个函数是干什么的呢？从Buffer中，摘掉二层的头，看一看，应该根据头里面的内容做什么操作。
假设你发现这个包的MAC地址和你的相符，那说明就是发给你的，于是需要调用process_layer3(buffer)。这个时候，Buffer里面往往就没有二层的头了，因为已经在上一个函数的处理过程中拿掉了，或者将开始的偏移量移动了一下。在这个函数里面，摘掉三层的头，看看到底是发送给自己的，还是希望自己转发出去的。
如何判断呢？如果IP地址不是自己的，那就应该转发出去；如果IP地址是自己的，那就是发给自己的。根据IP头里面的标示，拿掉三层的头，进行下一层的处理，到底是调用process_tcp(buffer)呢，还是调用process_udp(buffer)呢？
假设这个地址是TCP的，则会调用process_tcp(buffer)。这时候，Buffer里面没有三层的头，就需要查看四层的头，看这是一个发起，还是一个应答，又或者是一个正常的数据包，然后分别由不同的逻辑进行处理。如果是发起或者应答，接下来可能要发送一个回复包；如果是一个正常的数据包，就需要交给上层了。交给谁呢？是不是有process_http(buffer)函数呢？
没有的，如果你是一个网络包处理程序，你不需要有process_http(buffer)，而是应该交给应用去处理。交给哪个应用呢？在四层的头里面有端口号，不同的应用监听不同的端口号。如果发现浏览器应用在监听这个端口，那你发给浏览器就行了。至于浏览器怎么处理，和你没有关系。
浏览器自然是解析HTML，显示出页面来。电脑的主人看到页面很开心，就点了鼠标。点击鼠标的动作被浏览器捕获。浏览器知道，又要发起另一个HTTP请求了，于是使用端口号，将请求发给了你。
你应该调用send_tcp(buffer)。不用说，Buffer里面就是HTTP请求的内容。这个函数里面加一个TCP的头，记录下源端口号。浏览器会给你目的端口号，一般为80端口。
然后调用send_layer3(buffer)。Buffer里面已经有了HTTP的头和内容，以及TCP的头。在这个函数里面加一个IP的头，记录下源IP的地址和目标IP的地址。
然后调用send_layer2(buffer)。Buffer里面已经有了HTTP的头和内容、TCP的头，以及IP的头。这个函数里面要加一下MAC的头，记录下源MAC地址，得到的就是本机器的MAC地址和目标的MAC地址。不过，这个还要看当前知道不知道，知道就直接加上；不知道的话，就要通过一定的协议处理过程，找到MAC地址。反正要填一个，不能空着。
万事俱备，只要Buffer里面的内容完整，就可以从网口发出去了，你作为一个程序的任务就算告一段落了。
揭秘层与层之间的关系 知道了这个过程之后，我们再来看一下原来困惑的问题。
首先是分层的比喻。所有不能表示出层层封装含义的比喻，都是不恰当的。总经理握手，不需要员工在吧，总经理之间谈什么，不需要员工参与吧，但是网络世界不是这样的。正确的应该是，总经理之间沟通的时候，经理将总经理放在自己兜里，然后组长把经理放自己兜里，员工把组长放自己兜里，像套娃娃一样。那员工直接沟通，不带上总经理，就不恰当了。
现实生活中，往往是员工说一句，组长补充两句，然后经理补充两句，最后总经理再补充两句。但是在网络世界，应该是总经理说话，经理补充两句，组长补充两句，员工再补充两句。
那TCP在三次握手的时候，IP层和MAC层在做什么呢？当然是TCP发送每一个消息，都会带着IP层和MAC层了。因为，TCP每发送一个消息，IP层和MAC层的所有机制都要运行一遍。而你只看到TCP三次握手了，其实，IP层和MAC层为此也忙活好久了。
这里要记住一点：只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。
所以，对TCP协议来说，三次握手也好，重试也好，只要想发出去包，就要有IP层和MAC层，不然是发不出去的。
经常有人会问这样一个问题，我都知道那台机器的IP地址了，直接发给他消息呗，要MAC地址干啥？这里的关键就是，没有MAC地址消息是发不出去的。
所以如果一个HTTP协议的包跑在网络上，它一定是完整的。无论这个包经过哪些设备，它都是完整的。
所谓的二层设备、三层设备，都是这些设备上跑的程序不同而已。一个HTTP协议的包经过一个二层设备，二层设备收进去的是整个网络包。这里面HTTP、TCP、 IP、 MAC都有。什么叫二层设备呀，就是只把MAC头摘下来，看看到底是丢弃、转发，还是自己留着。那什么叫三层设备呢？就是把MAC头摘下来之后，再把IP头摘下来，看看到底是丢弃、转发，还是自己留着。
小结 总结一下今天的内容，理解网络协议的工作模式，有两个小窍门：
始终想象自己是一个处理网络包的程序：如何拿到网络包，如何根据规则进行处理，如何发出去； 始终牢记一个原则：只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。 最后，给你留两个思考题吧。
如果你也觉得总经理和员工的比喻不恰当，你有更恰当的比喻吗？ 要想学习网络协议，IP这个概念是最最基本的，那你知道如何查看IP地址吗？ 欢迎你留言和我讨论。趣谈网络协议，我们下期见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>第30讲_容器网络之Flannel：每人一亩三分地</title><link>https://artisanbox.github.io/5/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/24/</guid><description>上一节我们讲了容器网络的模型，以及如何通过NAT的方式与物理网络进行互通。
每一台物理机上面安装好了Docker以后，都会默认分配一个172.17.0.0/16的网段。一台机器上新创建的第一个容器，一般都会给172.17.0.2这个地址，当然一台机器这样玩玩倒也没啥问题。但是容器里面是要部署应用的，就像上一节讲过的一样，它既然是集装箱，里面就需要装载货物。
如果这个应用是比较传统的单体应用，自己就一个进程，所有的代码逻辑都在这个进程里面，上面的模式没有任何问题，只要通过NAT就能访问进来。
但是因为无法解决快速迭代和高并发的问题，单体应用越来越跟不上时代发展的需要了。
你可以回想一下，无论是各种网络直播平台，还是共享单车，是不是都是很短时间内就要积累大量用户，否则就会错过风口。所以应用需要在很短的时间内快速迭代，不断调整，满足用户体验；还要在很短的时间内，具有支撑高并发请求的能力。
单体应用作为个人英雄主义的时代已经过去了。如果所有的代码都在一个工程里面，开发的时候必然存在大量冲突，上线的时候，需要开大会进行协调，一个月上线一次就很不错了。而且所有的流量都让一个进程扛，怎么也扛不住啊！
没办法，一个字：拆！拆开了，每个子模块独自变化，减少相互影响。拆开了，原来一个进程扛流量，现在多个进程一起扛。所以，微服务就是从个人英雄主义，变成集团军作战。
容器作为集装箱，可以保证应用在不同的环境中快速迁移，提高迭代的效率。但是如果要形成容器集团军，还需要一个集团军作战的调度平台，这就是Kubernetes。它可以灵活地将一个容器调度到任何一台机器上，并且当某个应用扛不住的时候，只要在Kubernetes上修改容器的副本数，一个应用马上就能变八个，而且都能提供服务。
然而集团军作战有个重要的问题，就是通信。这里面包含两个问题，第一个是集团军的A部队如何实时地知道B部队的位置变化，第二个是两个部队之间如何相互通信。
第一个问题位置变化，往往是通过一个称为注册中心的地方统一管理的，这个是应用自己做的。当一个应用启动的时候，将自己所在环境的IP地址和端口，注册到注册中心指挥部，这样其他的应用请求它的时候，到指挥部问一下它在哪里就好了。当某个应用发生了变化，例如一台机器挂了，容器要迁移到另一台机器，这个时候IP改变了，应用会重新注册，则其他的应用请求它的时候，还是能够从指挥部得到最新的位置。
接下来是如何相互通信的问题。NAT这种模式，在多个主机的场景下，是存在很大问题的。在物理机A上的应用A看到的IP地址是容器A的，是172.17.0.2，在物理机B上的应用B看到的IP地址是容器B的，不巧也是172.17.0.2，当它们都注册到注册中心的时候，注册中心就是这个图里这样子。
这个时候，应用A要访问应用B，当应用A从注册中心将应用B的IP地址读出来的时候，就彻底困惑了，这不是自己访问自己吗？
怎么解决这个问题呢？一种办法是不去注册容器内的IP地址，而是注册所在物理机的IP地址，端口也要是物理机上映射的端口。
这样存在的问题是，应用是在容器里面的，它怎么知道物理机上的IP地址和端口呢？这明明是运维人员配置的，除非应用配合，读取容器平台的接口获得这个IP和端口。一方面，大部分分布式框架都是容器诞生之前就有了，它们不会适配这种场景；另一方面，让容器内的应用意识到容器外的环境，本来就是非常不好的设计。
说好的集装箱，说好的随意迁移呢？难道要让集装箱内的货物意识到自己传的信息？而且本来Tomcat都是监听8080端口的，结果到了物理机上，就不能大家都用这个端口了，否则端口就冲突了，因而就需要随机分配端口，于是在注册中心就出现了各种各样奇怪的端口。无论是注册中心，还是调用方都会觉得很奇怪，而且不是默认的端口，很多情况下也容易出错。
Kubernetes作为集团军作战管理平台，提出指导意见，说网络模型要变平，但是没说怎么实现。于是业界就涌现了大量的方案，Flannel就是其中之一。
对于IP冲突的问题，如果每一个物理机都是网段172.17.0.0/16，肯定会冲突啊，但是这个网段实在太大了，一台物理机上根本启动不了这么多的容器，所以能不能每台物理机在这个大网段里面，抠出一个小的网段，每个物理机网段都不同，自己看好自己的一亩三分地，谁也不和谁冲突。
例如物理机A是网段172.17.8.0/24，物理机B是网段172.17.9.0/24，这样两台机器上启动的容器IP肯定不一样，而且就看IP地址，我们就一下子识别出，这个容器是本机的，还是远程的，如果是远程的，也能从网段一下子就识别出它归哪台物理机管，太方便了。
接下来的问题，就是物理机A上的容器如何访问到物理机B上的容器呢？
你是不是想到了熟悉的场景？虚拟机也需要跨物理机互通，往往通过Overlay的方式，容器是不是也可以这样做呢？
这里我要说Flannel使用UDP实现Overlay网络的方案。
在物理机A上的容器A里面，能看到的容器的IP地址是172.17.8.2/24，里面设置了默认的路由规则default via 172.17.8.1 dev eth0。
如果容器A要访问172.17.9.2，就会发往这个默认的网关172.17.8.1。172.17.8.1就是物理机上面docker0网桥的IP地址，这台物理机上的所有容器都是连接到这个网桥的。
在物理机上面，查看路由策略，会有这样一条172.17.0.0/24 via 172.17.0.0 dev flannel.1，也就是说发往172.17.9.2的网络包会被转发到flannel.1这个网卡。
这个网卡是怎么出来的呢？在每台物理机上，都会跑一个flanneld进程，这个进程打开一个/dev/net/tun字符设备的时候，就出现了这个网卡。
你有没有想起qemu-kvm，打开这个字符设备的时候，物理机上也会出现一个网卡，所有发到这个网卡上的网络包会被qemu-kvm接收进来，变成二进制串。只不过接下来qemu-kvm会模拟一个虚拟机里面的网卡，将二进制的串变成网络包，发给虚拟机里面的网卡。但是flanneld不用这样做，所有发到flannel.1这个网卡的包都会被flanneld进程读进去，接下来flanneld要对网络包进行处理。
物理机A上的flanneld会将网络包封装在UDP包里面，然后外层加上物理机A和物理机B的IP地址，发送给物理机B上的flanneld。
为什么是UDP呢？因为不想在flanneld之间建立两两连接，而UDP没有连接的概念，任何一台机器都能发给另一台。
物理机B上的flanneld收到包之后，解开UDP的包，将里面的网络包拿出来，从物理机B的flannel.1网卡发出去。
在物理机B上，有路由规则172.17.9.0/24 dev docker0 proto kernel scope link src 172.17.9.1。
将包发给docker0，docker0将包转给容器B。通信成功。
上面的过程连通性没有问题，但是由于全部在用户态，所以性能差了一些。
跨物理机的连通性问题，在虚拟机那里有成熟的方案，就是VXLAN，那能不能Flannel也用VXLAN呢？
当然可以了。如果使用VXLAN，就不需要打开一个TUN设备了，而是要建立一个VXLAN的VTEP。如何建立呢？可以通过netlink通知内核建立一个VTEP的网卡flannel.1。在我们讲OpenvSwitch的时候提过，netlink是一种用户态和内核态通信的机制。
当网络包从物理机A上的容器A发送给物理机B上的容器B，在容器A里面通过默认路由到达物理机A上的docker0网卡，然后根据路由规则，在物理机A上，将包转发给flannel.1。这个时候flannel.1就是一个VXLAN的VTEP了，它将网络包进行封装。
内部的MAC地址这样写：源为物理机A的flannel.1的MAC地址，目标为物理机B的flannel.1的MAC地址，在外面加上VXLAN的头。
外层的IP地址这样写：源为物理机A的IP地址，目标为物理机B的IP地址，外面加上物理机的MAC地址。
这样就能通过VXLAN将包转发到另一台机器，从物理机B的flannel.1上解包，变成内部的网络包，通过物理机B上的路由转发到docker0，然后转发到容器B里面。通信成功。
小结好了，今天的内容就到这里，我来总结一下。
基于NAT的容器网络模型在微服务架构下有两个问题，一个是IP重叠，一个是端口冲突，需要通过Overlay网络的机制保持跨节点的连通性。
Flannel是跨节点容器网络方案之一，它提供的Overlay方案主要有两种方式，一种是UDP在用户态封装，一种是VXLAN在内核态封装，而VXLAN的性能更好一些。
最后，给你留两个问题：
通过Flannel的网络模型可以实现容器与容器直接跨主机的互相访问，那你知道如果容器内部访问外部的服务应该怎么融合到这个网络模型中吗？
基于Overlay的网络毕竟做了一次网络虚拟化，有没有更加高性能的方案呢？</description></item><item><title>第31讲_容器网络之Calico：为高效说出善意的谎言</title><link>https://artisanbox.github.io/5/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/25/</guid><description>上一节我们讲了Flannel如何解决容器跨主机互通的问题，这个解决方式其实和虚拟机的网络互通模式是差不多的，都是通过隧道。但是Flannel有一个非常好的模式，就是给不同的物理机设置不同网段，这一点和虚拟机的Overlay的模式完全不一样。
在虚拟机的场景下，整个网段在所有的物理机之间都是可以“飘来飘去”的。网段不同，就给了我们做路由策略的可能。
Calico网络模型的设计思路我们看图中的两台物理机。它们的物理网卡是同一个二层网络里面的。由于两台物理机的容器网段不同，我们完全可以将两台物理机配置成为路由器，并按照容器的网段配置路由表。
例如，在物理机A中，我们可以这样配置：要想访问网段172.17.9.0/24，下一跳是192.168.100.101，也即到物理机B上去。
这样在容器A中访问容器B，当包到达物理机A的时候，就能够匹配到这条路由规则，并将包发给下一跳的路由器，也即发给物理机B。在物理机B上也有路由规则，要访问172.17.9.0/24，从docker0的网卡进去即可。
当容器B返回结果的时候，在物理机B上，可以做类似的配置：要想访问网段172.17.8.0/24，下一跳是192.168.100.100，也即到物理机A上去。
当包到达物理机B的时候，能够匹配到这条路由规则，将包发给下一跳的路由器，也即发给物理机A。在物理机A上也有路由规则，要访问172.17.8.0/24，从docker0的网卡进去即可。
这就是Calico网络的大概思路，即不走Overlay网络，不引入另外的网络性能损耗，而是将转发全部用三层网络的路由转发来实现，只不过具体的实现和上面的过程稍有区别。
首先，如果全部走三层的路由规则，没必要每台机器都用一个docker0，从而浪费了一个IP地址，而是可以直接用路由转发到veth pair在物理机这一端的网卡。同样，在容器内，路由规则也可以这样设定：把容器外面的veth pair网卡算作默认网关，下一跳就是外面的物理机。
于是，整个拓扑结构就变成了这个图中的样子。
Calico网络的转发细节我们来看其中的一些细节。
容器A1的IP地址为172.17.8.2/32，这里注意，不是/24，而是/32，将容器A1作为一个单点的局域网了。
容器A1里面的默认路由，Calico配置得比较有技巧。
default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link 这个IP地址169.254.1.1是默认的网关，但是整个拓扑图中没有一张网卡是这个地址。那如何到达这个地址呢？
前面我们讲网关的原理的时候说过，当一台机器要访问网关的时候，首先会通过ARP获得网关的MAC地址，然后将目标MAC变为网关的MAC，而网关的IP地址不会在任何网络包头里面出现，也就是说，没有人在乎这个地址具体是什么，只要能找到对应的MAC，响应ARP就可以了。
ARP本地有缓存，通过ip neigh命令可以查看。
169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee STALE 这个MAC地址是Calico硬塞进去的，但是没有关系，它能响应ARP，于是发出的包的目标MAC就是这个MAC地址。
在物理机A上查看所有网卡的MAC地址的时候，我们会发现veth1就是这个MAC地址。所以容器A1里发出的网络包，第一跳就是这个veth1这个网卡，也就到达了物理机A这个路由器。
在物理机A上有三条路由规则，分别是去两个本机的容器的路由，以及去172.17.9.0/24，下一跳为物理机B。
172.17.8.2 dev veth1 scope link 172.17.8.3 dev veth2 scope link 172.17.9.0/24 via 192.168.100.101 dev eth0 proto bird onlink 同理，物理机B上也有三条路由规则，分别是去两个本机的容器的路由，以及去172.17.8.0/24，下一跳为物理机A。
172.17.9.2 dev veth1 scope link 172.17.9.3 dev veth2 scope link 172.17.8.0/24 via 192.</description></item><item><title>第32讲_RPC协议综述：远在天边，近在眼前</title><link>https://artisanbox.github.io/5/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/26/</guid><description>前面我们讲了容器网络如何实现跨主机互通，以及微服务之间的相互调用。
网络是打通了，那服务之间的互相调用，该怎么实现呢？你可能说，咱不是学过Socket吗。服务之间分调用方和被调用方，我们就建立一个TCP或者UDP的连接，不就可以通信了？
你仔细想一下，这事儿没这么简单。我们就拿最简单的场景，客户端调用一个加法函数，将两个整数加起来，返回它们的和。
如果放在本地调用，那是简单的不能再简单了，只要稍微学过一种编程语言，三下五除二就搞定了。但是一旦变成了远程调用，门槛一下子就上去了。
首先你要会Socket编程，至少先要把咱们这门网络协议课学一下，然后再看N本砖头厚的Socket程序设计的书，学会咱们学过的几种Socket程序设计的模型。这就使得本来大学毕业就能干的一项工作，变成了一件五年工作经验都不一定干好的工作，而且，搞定了Socket程序设计，才是万里长征的第一步。后面还有很多问题呢！
如何解决这五个问题？问题一：如何规定远程调用的语法？客户端如何告诉服务端，我是一个加法，而另一个是乘法。我是用字符串“add”传给你，还是传给你一个整数，比如1表示加法，2表示乘法？服务端该如何告诉客户端，我的这个加法，目前只能加整数，不能加小数，不能加字符串；而另一个加法“add1”，它能实现小数和整数的混合加法。那返回值是什么？正确的时候返回什么，错误的时候又返回什么？
问题二：如果传递参数？我是先传两个整数，后传一个操作符“add”，还是先传操作符，再传两个整数？是不是像咱们数据结构里一样，如果都是UDP，想要实现一个逆波兰表达式，放在一个报文里面还好，如果是TCP，是一个流，在这个流里面，如何将两次调用进行分界？什么时候是头，什么时候是尾？把这次的参数和上次的参数混了起来，TCP一端发送出去的数据，另外一端不一定能一下子全部读取出来。所以，怎么才算读完呢？
问题三：如何表示数据？在这个简单的例子中，传递的就是一个固定长度的int值，这种情况还好，如果是变长的类型，是一个结构体，甚至是一个类，应该怎么办呢？如果是int，不同的平台上长度也不同，该怎么办呢？
在网络上传输超过一个Byte的类型，还有大端Big Endian和小端Little Endian的问题。
假设我们要在32位四个Byte的一个空间存放整数1，很显然只要一个Byte放1，其他三个Byte放0就可以了。那问题是，最后一个Byte放1呢，还是第一个Byte放1呢？或者说1作为最低位，应该是放在32位的最后一个位置呢，还是放在第一个位置呢？
最低位放在最后一个位置，叫作Little Endian，最低位放在第一个位置，叫作Big Endian。TCP/IP协议栈是按照Big Endian来设计的，而X86机器多按照Little Endian来设计的，因而发出去的时候需要做一个转换。
问题四：如何知道一个服务端都实现了哪些远程调用？从哪个端口可以访问这个远程调用？假设服务端实现了多个远程调用，每个可能实现在不同的进程中，监听的端口也不一样，而且由于服务端都是自己实现的，不可能使用一个大家都公认的端口，而且有可能多个进程部署在一台机器上，大家需要抢占端口，为了防止冲突，往往使用随机端口，那客户端如何找到这些监听的端口呢？
问题五：发生了错误、重传、丢包、性能等问题怎么办？本地调用没有这个问题，但是一旦到网络上，这些问题都需要处理，因为网络是不可靠的，虽然在同一个连接中，我们还可通过TCP协议保证丢包、重传的问题，但是如果服务器崩溃了又重启，当前连接断开了，TCP就保证不了了，需要应用自己进行重新调用，重新传输会不会同样的操作做两遍，远程调用性能会不会受影响呢？
协议约定问题看到这么多问题，你是不是想起了我第一节讲过的这张图。
本地调用函数里有很多问题，比如词法分析、语法分析、语义分析等等，这些编译器本来都能帮你做了。但是在远程调用中，这些问题你都需要重新操心。
很多公司的解决方法是，弄一个核心通信组，里面都是Socket编程的大牛，实现一个统一的库，让其他业务组的人来调用，业务的人不需要知道中间传输的细节。通信双方的语法、语义、格式、端口、错误处理等，都需要调用方和被调用方开会协商，双方达成一致。一旦有一方改变，要及时通知对方，否则通信就会有问题。
可是不是每一个公司都有这种大牛团队，往往只有大公司才配得起，那有没有已经实现好的框架可以使用呢？
当然有。一个大牛Bruce Jay Nelson写了一篇论文Implementing Remote Procedure Calls，定义了RPC的调用标准。后面所有RPC框架，都是按照这个标准模式来的。
当客户端的应用想发起一个远程调用时，它实际是通过本地调用本地调用方的Stub。它负责将调用的接口、方法和参数，通过约定的协议规范进行编码，并通过本地的RPCRuntime进行传输，将调用网络包发送到服务器。
服务器端的RPCRuntime收到请求后，交给提供方Stub进行解码，然后调用服务端的方法，服务端执行方法，返回结果，提供方Stub将返回结果编码后，发送给客户端，客户端的RPCRuntime收到结果，发给调用方Stub解码得到结果，返回给客户端。
这里面分了三个层次，对于用户层和服务端，都像是本地调用一样，专注于业务逻辑的处理就可以了。对于Stub层，处理双方约定好的语法、语义、封装、解封装。对于RPCRuntime，主要处理高性能的传输，以及网络的错误和异常。
最早的RPC的一种实现方式称为Sun RPC或ONC RPC。Sun公司是第一个提供商业化RPC库和 RPC编译器的公司。这个RPC框架是在NFS协议中使用的。
NFS（Network File System）就是网络文件系统。要使NFS成功运行，要启动两个服务端，一个是mountd，用来挂载文件路径；一个是nfsd，用来读写文件。NFS可以在本地mount一个远程的目录到本地的一个目录，从而本地的用户在这个目录里面写入、读出任何文件的时候，其实操作的是远程另一台机器上的文件。
操作远程和远程调用的思路是一样的，就像操作本地一样。所以NFS协议就是基于RPC实现的。当然无论是什么RPC，底层都是Socket编程。
XDR（External Data Representation，外部数据表示法）是一个标准的数据压缩格式，可以表示基本的数据类型，也可以表示结构体。
这里是几种基本的数据类型。
在RPC的调用过程中，所有的数据类型都要封装成类似的格式。而且RPC的调用和结果返回，也有严格的格式。
XID唯一标识一对请求和回复。请求为0，回复为1。
RPC有版本号，两端要匹配RPC协议的版本号。如果不匹配，就会返回Deny，原因就是RPC_MISMATCH。
程序有编号。如果服务端找不到这个程序，就会返回PROG_UNAVAIL。
程序有版本号。如果程序的版本号不匹配，就会返回PROG_MISMATCH。
一个程序可以有多个方法，方法也有编号，如果找不到方法，就会返回PROC_UNAVAIL。
调用需要认证鉴权，如果不通过，则Deny。
最后是参数列表，如果参数无法解析，则返回GABAGE_ARGS。
为了可以成功调用RPC，在客户端和服务端实现RPC的时候，首先要定义一个双方都认可的程序、版本、方法、参数等。
如果还是上面的加法，则双方约定为一个协议定义文件，同理如果是NFS、mount和读写，也会有类似的定义。</description></item><item><title>第33讲_基于XML的SOAP协议：不要说NBA，请说美国职业篮球联赛</title><link>https://artisanbox.github.io/5/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/27/</guid><description>上一节我们讲了RPC的经典模型和设计要点，并用最早期的ONC RPC为例子，详述了具体的实现。
ONC RPC存在哪些问题？ ONC RPC将客户端要发送的参数，以及服务端要发送的回复，都压缩为一个二进制串，这样固然能够解决双方的协议约定问题，但是存在一定的不方便。
首先，需要双方的压缩格式完全一致，一点都不能差。一旦有少许的差错，多一位，少一位或者错一位，都可能造成无法解压缩。当然，我们可以用传输层的可靠性以及加入校验值等方式，来减少传输过程中的差错。
其次，协议修改不灵活。如果不是传输过程中造成的差错，而是客户端因为业务逻辑的改变，添加或者删除了字段，或者服务端添加或者删除了字段，而双方没有及时通知，或者线上系统没有及时升级，就会造成解压缩不成功。
因而，当业务发生改变，需要多传输一些参数或者少传输一些参数的时候，都需要及时通知对方，并且根据约定好的协议文件重新生成双方的Stub程序。自然，这样灵活性比较差。
如果仅仅是沟通的问题也还好解决，其实更难弄的还有版本的问题。比如在服务端提供一个服务，参数的格式是版本一的，已经有50个客户端在线上调用了。现在有一个客户端有个需求，要加一个字段，怎么办呢？这可是一个大工程，所有的客户端都要适配这个，需要重新写程序，加上这个字段，但是传输值是0，不需要这个字段的客户端很“冤”，本来没我啥事儿，为啥让我也忙活？
最后，ONC RPC的设计明显是面向函数的，而非面向对象。而当前面向对象的业务逻辑设计与实现方式已经成为主流。
这一切的根源就在于压缩。这就像平时我们爱用缩略语。如果是篮球爱好者，你直接说NBA，他马上就知道什么意思，但是如果你给一个大妈说NBA，她可能就不知所云。
所以，这种RPC框架只能用于客户端和服务端全由一拨人开发的场景，或者至少客户端和服务端的开发人员要密切沟通，相互合作，有大量的共同语言，才能按照既定的协议顺畅地进行工作。
XML与SOAP 但是，一般情况下，我们做一个服务，都是要提供给陌生人用的，你和客户不会经常沟通，也没有什么共同语言。就像你给别人介绍NBA，你要说美国职业篮球赛，这样不管他是干啥的，都能听得懂。
放到我们的场景中，对应的就是用文本类的方式进行传输。无论哪个客户端获得这个文本，都能够知道它的意义。
一种常见的文本类格式是XML。我们这里举个例子来看。
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt; &amp;lt;geek:purchaseOrder xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; xmlns:geek=&amp;quot;http://www.example.com/geek&amp;quot;&amp;gt; &amp;lt;order&amp;gt; &amp;lt;date&amp;gt;2018-07-01&amp;lt;/date&amp;gt; &amp;lt;className&amp;gt;趣谈网络协议&amp;lt;/className&amp;gt; &amp;lt;Author&amp;gt;刘超&amp;lt;/Author&amp;gt; &amp;lt;price&amp;gt;68&amp;lt;/price&amp;gt; &amp;lt;/order&amp;gt; &amp;lt;/geek:purchaseOrder&amp;gt; 我这里不准备详细讲述XML的语法规则，但是你相信我，看完下面的内容，即便你没有学过XML，也能一看就懂，这段XML描述的是什么，不像全面的二进制，你看到的都是010101，不知所云。
有了这个，刚才我们说的那几个问题就都不是问题了。
首先，格式没必要完全一致。比如如果我们把price和author换个位置，并不影响客户端和服务端解析这个文本，也根本不会误会，说这个作者的名字叫68。
如果有的客户端想增加一个字段，例如添加一个推荐人字段，只需要在上面的文件中加一行：
&amp;lt;recommended&amp;gt; Gary &amp;lt;/recommended&amp;gt; 对于不需要这个字段的客户端，只要不解析这一行就是了。只要用简单的处理，就不会出现错误。
另外，这种表述方式显然是描述一个订单对象的，是一种面向对象的、更加接近用户场景的表示方式。
既然XML这么好，接下来我们来看看怎么把它用在RPC中。
传输协议问题 我们先解决第一个，传输协议的问题。
基于XML的最著名的通信协议就是SOAP了，全称简单对象访问协议（Simple Object Access Protocol）。它使用XML编写简单的请求和回复消息，并用HTTP协议进行传输。
SOAP将请求和回复放在一个信封里面，就像传递一个邮件一样。信封里面的信分抬头和正文。
POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/soap+xml; charset=utf-8 Content-Length: nnn &amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt; &amp;lt;soap:Envelope xmlns:soap=&amp;quot;http://www.w3.org/2001/12/soap-envelope&amp;quot; soap:encodingStyle=&amp;quot;http://www.w3.org/2001/12/soap-encoding&amp;quot;&amp;gt; &amp;lt;soap:Header&amp;gt; &amp;lt;m:Trans xmlns:m=&amp;quot;http://www.w3schools.com/transaction/&amp;quot; soap:mustUnderstand=&amp;quot;1&amp;quot;&amp;gt;1234 &amp;lt;/m:Trans&amp;gt; &amp;lt;/soap:Header&amp;gt; &amp;lt;soap:Body xmlns:m=&amp;quot;http://www.</description></item><item><title>第34讲_基于JSON的RESTful接口协议：我不关心过程，请给我结果</title><link>https://artisanbox.github.io/5/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/28/</guid><description>上一节我们讲了基于XML的SOAP协议，SOAP的S是啥意思来着？是Simple，但是好像一点儿都不简单啊！
你会发现，对于SOAP来讲，无论XML中调用的是什么函数，多是通过HTTP的POST方法发送的。但是咱们原来学HTTP的时候，我们知道HTTP除了POST，还有PUT、DELETE、GET等方法，这些也可以代表一个个动作，而且基本满足增、删、查、改的需求，比如增是POST，删是DELETE，查是GET，改是PUT。
传输协议问题 对于SOAP来讲，比如我创建一个订单，用POST，在XML里面写明动作是CreateOrder；删除一个订单，还是用POST，在XML里面写明了动作是DeleteOrder。其实创建订单完全可以使用POST动作，然后在XML里面放一个订单的信息就可以了，而删除用DELETE动作，然后在XML里面放一个订单的ID就可以了。
于是上面的那个SOAP就变成下面这个简单的模样。
POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/xml; charset=utf-8 Content-Length: nnn &amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt; &amp;lt;order&amp;gt; &amp;lt;date&amp;gt;2018-07-01&amp;lt;/date&amp;gt; &amp;lt;className&amp;gt;趣谈网络协议&amp;lt;/className&amp;gt; &amp;lt;Author&amp;gt;刘超&amp;lt;/Author&amp;gt; &amp;lt;price&amp;gt;68&amp;lt;/price&amp;gt; &amp;lt;/order&amp;gt;
而且XML的格式也可以改成另外一种简单的文本化的对象表示格式JSON。
POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/json; charset=utf-8 Content-Length: nnn { &amp;quot;order&amp;quot;: { &amp;quot;date&amp;quot;: &amp;quot;2018-07-01&amp;quot;, &amp;quot;className&amp;quot;: &amp;quot;趣谈网络协议&amp;quot;, &amp;quot;Author&amp;quot;: &amp;quot;刘超&amp;quot;, &amp;quot;price&amp;quot;: &amp;quot;68&amp;quot; } }
经常写Web应用的应该已经发现，这就是RESTful格式的API的样子。
协议约定问题 然而RESTful可不仅仅是指API，而是一种架构风格，全称Representational State Transfer，表述性状态转移，来自一篇重要的论文《架构风格与基于网络的软件架构设计》（Architectural Styles and the Design of Network-based Software Architectures）。
这篇文章从深层次，更加抽象地论证了一个互联网应用应该有的设计要点，而这些设计要点，成为后来我们能看到的所有高并发应用设计都必须要考虑的问题，再加上REST API比较简单直接，所以后来几乎成为互联网应用的标准接口。
因此，和SOAP不一样，REST不是一种严格规定的标准，它其实是一种设计风格。如果按这种风格进行设计，RESTful接口和SOAP接口都能做到，只不过后面的架构是REST倡导的，而SOAP相对比较关注前面的接口。
而且由于能够通过WSDL生成客户端的Stub，因而SOAP常常被用于类似传统的RPC方式，也即调用远端和调用本地是一样的。
然而本地调用和远程跨网络调用毕竟不一样，这里的不一样还不仅仅是因为有网络而导致的客户端和服务端的分离，从而带来的网络性能问题。更重要的问题是，客户端和服务端谁来维护状态。所谓的状态就是对某个数据当前处理到什么程度了。
这里举几个例子，例如，我浏览到哪个目录了，我看到第几页了，我要买个东西，需要扣减一下库存，这些都是状态。本地调用其实没有人纠结这个问题，因为数据都在本地，谁处理都一样，而且一边处理了，另一边马上就能看到。
当有了RPC之后，我们本来期望对上层透明，就像上一节说的“远在天边，尽在眼前”。于是使用RPC的时候，对于状态的问题也没有太多的考虑。</description></item><item><title>第35讲_二进制类RPC协议：还是叫NBA吧，总说全称多费劲</title><link>https://artisanbox.github.io/5/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/29/</guid><description>前面我们讲了两个常用文本类的RPC协议，对于陌生人之间的沟通，用NBA、CBA这样的缩略语，会使得协议约定非常不方便。
在讲CDN和DNS的时候，我们讲过接入层的设计，对于静态资源或者动态资源静态化的部分都可以做缓存。但是对于下单、支付等交易场景，还是需要调用API。
对于微服务的架构，API需要一个API网关统一的管理。API网关有多种实现方式，用Nginx或者OpenResty结合Lua脚本是常用的方式。在上一节讲过的Spring Cloud体系中，有个组件Zuul也是干这个的。
数据中心内部是如何相互调用的？API网关用来管理API，但是API的实现一般在一个叫作Controller层的地方。这一层对外提供API。由于是让陌生人访问的，我们能看到目前业界主流的，基本都是RESTful的API，是面向大规模互联网应用的。
在Controller之内，就是咱们互联网应用的业务逻辑实现。上节讲RESTful的时候，说过业务逻辑的实现最好是无状态的，从而可以横向扩展，但是资源的状态还需要服务端去维护。资源的状态不应该维护在业务逻辑层，而是在最底层的持久化层，一般会使用分布式数据库和ElasticSearch。
这些服务端的状态，例如订单、库存、商品等，都是重中之重，都需要持久化到硬盘上，数据不能丢，但是由于硬盘读写性能差，因而持久化层往往吞吐量不能达到互联网应用要求的吞吐量，因而前面要有一层缓存层，使用Redis或者memcached将请求拦截一道，不能让所有的请求都进入数据库“中军大营”。
缓存和持久化层之上一般是基础服务层，这里面提供一些原子化的接口。例如，对于用户、商品、订单、库存的增删查改，将缓存和数据库对再上层的业务逻辑屏蔽一道。有了这一层，上层业务逻辑看到的都是接口，而不会调用数据库和缓存。因而对于缓存层的扩容，数据库的分库分表，所有的改变，都截止到这一层，这样有利于将来对于缓存和数据库的运维。
再往上就是组合层。因为基础服务层只是提供简单的接口，实现简单的业务逻辑，而复杂的业务逻辑，比如下单，要扣优惠券，扣减库存等，就要在组合服务层实现。
这样，Controller层、组合服务层、基础服务层就会相互调用，这个调用是在数据中心内部的，量也会比较大，还是使用RPC的机制实现的。
由于服务比较多，需要一个单独的注册中心来做服务发现。服务提供方会将自己提供哪些服务注册到注册中心中去，同时服务消费方订阅这个服务，从而可以对这个服务进行调用。
调用的时候有一个问题，这里的RPC调用，应该用二进制还是文本类？其实文本的最大问题是，占用字节数目比较多。比如数字123，其实本来二进制8位就够了，但是如果变成文本，就成了字符串123。如果是UTF-8编码的话，就是三个字节；如果是UTF-16，就是六个字节。同样的信息，要多费好多的空间，传输起来也更加占带宽，时延也高。
因而对于数据中心内部的相互调用，很多公司选型的时候，还是希望采用更加省空间和带宽的二进制的方案。
这里一个著名的例子就是Dubbo服务化框架二进制的RPC方式。
Dubbo会在客户端的本地启动一个Proxy，其实就是客户端的Stub，对于远程的调用都通过这个Stub进行封装。
接下来，Dubbo会从注册中心获取服务端的列表，根据路由规则和负载均衡规则，在多个服务端中选择一个最合适的服务端进行调用。
调用服务端的时候，首先要进行编码和序列化，形成Dubbo头和序列化的方法和参数。将编码好的数据，交给网络客户端进行发送，网络服务端收到消息后，进行解码。然后将任务分发给某个线程进行处理，在线程中会调用服务端的代码逻辑，然后返回结果。
这个过程和经典的RPC模式何其相似啊！
如何解决协议约定问题？接下来我们还是来看RPC的三大问题，其中注册发现问题已经通过注册中心解决了。下面我们就来看协议约定问题。
Dubbo中默认的RPC协议是Hessian2。为了保证传输的效率，Hessian2将远程调用序列化为二进制进行传输，并且可以进行一定的压缩。这个时候你可能会疑惑，同为二进制的序列化协议，Hessian2和前面的二进制的RPC有什么区别呢？这不绕了一圈又回来了吗？
Hessian2是解决了一些问题的。例如，原来要定义一个协议文件，然后通过这个文件生成客户端和服务端的Stub，才能进行相互调用，这样使得修改就会不方便。Hessian2不需要定义这个协议文件，而是自描述的。什么是自描述呢？
所谓自描述就是，关于调用哪个函数，参数是什么，另一方不需要拿到某个协议文件、拿到二进制，靠它本身根据Hessian2的规则，就能解析出来。
原来有协议文件的场景，有点儿像两个人事先约定好，0表示方法add，然后后面会传两个数。服务端把两个数加起来，这样一方发送012，另一方知道是将1和2加起来，但是不知道协议文件的，当它收到012的时候，完全不知道代表什么意思。
而自描述的场景，就像两个人说的每句话都带前因后果。例如，传递的是“函数：add，第一个参数1，第二个参数2”。这样无论谁拿到这个表述，都知道是什么意思。但是只不过都是以二进制的形式编码的。这其实相当于综合了XML和二进制共同优势的一个协议。
Hessian2是如何做到这一点的呢？这就需要去看Hessian2的序列化的语法描述文件。
看起来很复杂，编译原理里面是有这样的语法规则的。
我们从Top看起，下一层是value，直到形成一棵树。这里面的有个思想，为了防止歧义，每一个类型的起始数字都设置成为独一无二的。这样，解析的时候，看到这个数字，就知道后面跟的是什么了。
这里还是以加法为例子，“add(2,3)”被序列化之后是什么样的呢？
H x02 x00 # Hessian 2.0 C # RPC call x03 add # method &amp;quot;add&amp;quot; x92 # two arguments x92 # 2 - argument 1 x93 # 3 - argument 2 H开头，表示使用的协议是Hession，H的二进制是0x48。
C开头，表示这是一个RPC调用。
0x03，表示方法名是三个字符。
0x92，表示有两个参数。其实这里存的应该是2，之所以加上0x90，就是为了防止歧义，表示这里一定是一个int。</description></item><item><title>第36讲_跨语言类RPC协议：交流之前，双方先来个专业术语表</title><link>https://artisanbox.github.io/5/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/30/</guid><description>到目前为止，咱们讲了四种RPC，分别是ONC RPC、基于XML的SOAP、基于JSON的RESTful和Hessian2。
通过学习，我们知道，二进制的传输性能好，文本类的传输性能差一些；二进制的难以跨语言，文本类的可以跨语言；要写协议文件的严谨一些，不写协议文件的灵活一些。虽然都有服务发现机制，有的可以进行服务治理，有的则没有。
我们也看到了RPC从最初的客户端服务器模式，最终演进到微服务。对于RPC框架的要求越来越多了，具体有哪些要求呢？
首先，传输性能很重要。因为服务之间的调用如此频繁了，还是二进制的越快越好。
其次，跨语言很重要。因为服务多了，什么语言写成的都有，而且不同的场景适宜用不同的语言，不能一个语言走到底。
最好既严谨又灵活，添加个字段不用重新编译和发布程序。
最好既有服务发现，也有服务治理，就像Dubbo和Spring Cloud一样。
Protocol Buffers这是要多快好省地建设社会主义啊。理想还是要有的嘛，这里我就来介绍一个向“理想”迈进的GRPC。
GRPC首先满足二进制和跨语言这两条，二进制说明压缩效率高，跨语言说明更灵活。但是又是二进制，又是跨语言，这就相当于两个人沟通，你不但说方言，还说缩略语，人家怎么听懂呢？所以，最好双方弄一个协议约定文件，里面规定好双方沟通的专业术语，这样沟通就顺畅多了。
对于GRPC来讲，二进制序列化协议是Protocol Buffers。首先，需要定义一个协议文件.proto。
我们还看买极客时间专栏的这个例子。
syntax = “proto3”; package com.geektime.grpc option java_package = “com.geektime.grpc”; message Order { required string date = 1; required string classname = 2; required string author = 3; required int price = 4; } message OrderResponse { required string message = 1; }
service PurchaseOrder { rpc Purchase (Order) returns (OrderResponse) {} } 在这个协议文件中，我们首先指定使用proto3的语法，然后我们使用Protocol Buffers的语法，定义两个消息的类型，一个是发出去的参数，一个是返回的结果。里面的每一个字段，例如date、classname、author、price都有唯一的一个数字标识，这样在压缩的时候，就不用传输字段名称了，只传输这个数字标识就行了，能节省很多空间。</description></item><item><title>第37讲_知识串讲：用双十一的故事串起碎片的网络协议（上）</title><link>https://artisanbox.github.io/5/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/31/</guid><description>基本的网络知识我们都讲完了，还记得最初举的那个“双十一”下单的例子吗？这一节开始，我们详细地讲解这个过程，用这个过程串起我们讲过的网络协议。
我把这个过程分为十个阶段，从云平台中搭建一个电商开始，到BGP路由广播，再到DNS域名解析，从客户看商品图片，到最终下单的整个过程，每一步我都会详细讲解。这节我们先来看前三个阶段。
1.部署一个高可用高并发的电商平台 首先，咱们要有个电商平台。假设我们已经有了一个特别大的电商平台，这个平台应该部署在哪里呢？假设我们用公有云，一般公有云会有多个位置，比如在华东、华北、华南都有。毕竟咱们的电商是要服务全国的，当然到处都要部署了。我们把主站点放在华东。
为了每个点都能“雨露均沾”，也为了高可用性，往往需要有多个机房，形成多个可用区（Available Zone）。由于咱们的应用是分布在两个可用区的，所以假如任何一个可用区挂了，都不会受影响。
我们来回想数据中心那一节，每个可用区里有一片一片的机柜，每个机柜上有一排一排的服务器，每个机柜都有一个接入交换机，有一个汇聚交换机将多个机柜连在一起。
这些服务器里面部署的都是计算节点，每台上面都有Open vSwitch创建的虚拟交换机，将来在这台机器上创建的虚拟机，都会连到Open vSwitch上。
接下来，你在云计算的界面上创建一个VPC（Virtual Private Cloud，虚拟私有网络），指定一个IP段，这样以后你部署的所有应用都会在这个虚拟网络里，使用你分配的这个IP段。为了不同的VPC相互隔离，每个VPC都会被分配一个VXLAN的ID。尽管不同用户的虚拟机有可能在同一个物理机上，但是不同的VPC二层压根儿是不通的。
由于有两个可用区，在这个VPC里面，要为每一个可用区分配一个Subnet，也就是在大的网段里分配两个小的网段。当两个可用区里面网段不同的时候，就可以配置路由策略，访问另外一个可用区，走某一条路由了。
接下来，应该创建数据库持久化层。大部分云平台都会提供PaaS服务，也就是说，不需要你自己搭建数据库，而是采用直接提供数据库的服务，并且单机房的主备切换都是默认做好的，数据库也是部署在虚拟机里面的，只不过从界面上，你看不到数据库所在的虚拟机而已。
云平台会给每个Subnet的数据库实例分配一个域名。创建数据库实例的时候，需要你指定可用区和Subnet，这样创建出来的数据库实例可以通过这个Subnet的私网IP进行访问。
为了分库分表实现高并发的读写，在创建的多个数据库实例之上，会创建一个分布式数据库的实例，也需要指定可用区和Subnet，还会为分布式数据库分配一个私网IP和域名。
对于数据库这种高可用性比较高的，需要进行跨机房高可用，因而两个可用区都要部署一套，但是只有一个是主，另外一个是备，云平台往往会提供数据库同步工具，将应用写入主的数据同步给备数据库集群。
接下来是创建缓存集群。云平台也会提供PaaS服务，也需要每个可用区和Subnet创建一套，缓存的数据在内存中，由于读写性能要求高，一般不要求跨可用区读写。
再往上层就是部署咱们自己写的程序了。基础服务层、组合服务层、Controller层，以及Nginx层、API网关等等，这些都是部署在虚拟机里面的。它们之间通过RPC相互调用，需要到注册中心进行注册。
它们之间的网络通信是虚拟机和虚拟机之间的。如果是同一台物理机，则那台物理机上的OVS就能转发过去；如果是不同的物理机，这台物理机的OVS和另一台物理机的OVS中间有一个VXLAN的隧道，将请求转发过去。
再往外就是负载均衡了，负载均衡也是云平台提供的PaaS服务，也是属于某个VPC的，部署在虚拟机里面的，但是负载均衡有个外网的IP，这个外网的IP地址就是在网关节点的外网网口上的。在网关节点上，会有NAT规则，将外网IP地址转换为VPC里面的私网IP地址，通过这些私网IP地址访问到虚拟机上的负载均衡节点，然后通过负载均衡节点转发到API网关的节点。
网关节点的外网网口是带公网IP地址的，里面有一个虚拟网关转发模块，还会有一个OVS，将私网IP地址放到VXLAN隧道里面，转发到虚拟机上，从而实现外网和虚拟机网络之间的互通。
不同的可用区之间，通过核心交换机连在一起，核心交换机之外是边界路由器。
在华北、华东、华南同样也部署了一整套，每个地区都创建了VPC，这就需要有一种机制将VPC连接到一起。云平台一般会提供硬件的VPC互连的方式，当然也可以使用软件互连的方式，也就是使用VPN网关，通过IPsec VPN将不同地区的不同VPC通过VPN连接起来。
对于不同地区和不同运营商的用户，我们希望他能够就近访问到网站，而且当一个点出了故障之后，我们希望能够在不同的地区之间切换，这就需要有智能DNS，这个也是云平台提供的。
对于一些静态资源，可以保持在对象存储里面，通过CDN下发到边缘节点，这样客户端就能尽快加载出来。
2.大声告诉全世界，可以到我这里买东西 当电商应用搭建完毕之后，接下来需要将如何访问到这个电商网站广播给全网。
刚才那张图画的是一个可用区的情况，对于多个可用区的情况，我们可以隐去计算节点的情况，将外网访问区域放大。
外网IP是放在虚拟网关的外网网口上的，这个IP如何让全世界知道呢？当然是通过BGP路由协议了。
每个可用区都有自己的汇聚交换机，如果机器数目比较多，可以直接用核心交换机，每个Region也有自己的核心交换区域。
在核心交换外面是安全设备，然后就是边界路由器。边界路由器会和多个运营商连接，从而每个运营商都能够访问到这个网站。边界路由器可以通过BGP协议，将自己数据中心里面的外网IP向外广播，也就是告诉全世界，如果要访问这些外网IP，都来我这里。
每个运营商也有很多的路由器、很多的点，于是就可以将如何到达这些IP地址的路由信息，广播到全国乃至全世界。
3.打开手机来上网，域名解析得地址 这个时候，不但你的这个网站的IP地址全世界都知道了，你打的广告可能大家也都看到了，于是有客户下载App来买东西了。
客户的手机开机以后，在附近寻找基站eNodeB，发送请求，申请上网。基站将请求发给MME，MME对手机进行认证和鉴权，还会请求HSS看有没有钱，看看是在哪里上网。
当MME通过了手机的认证之后，开始建立隧道，建设的数据通路分两段路，其实是两个隧道。一段是从eNodeB到SGW，第二段是从SGW到PGW，在PGW之外，就是互联网。
PGW会为手机分配一个IP地址，手机上网都是带着这个IP地址的。
当在手机上面打开一个App的时候，首先要做的事情就是解析这个网站的域名。
在手机运营商所在的互联网区域里，有一个本地的DNS，手机会向这个DNS请求解析DNS。当这个DNS本地有缓存，则直接返回；如果没有缓存，本地DNS才需要递归地从根DNS服务器，查到.com的顶级域名服务器，最终查到权威DNS服务器。
如果你使用云平台的时候，配置了智能DNS和全局负载均衡，在权威DNS服务中，一般是通过配置CNAME的方式，我们可以起一个别名，例如 vip.yourcomany.com ，然后告诉本地DNS服务器，让它请求GSLB解析这个域名，GSLB就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。
GSLB通过查看请求它的本地DNS服务器所在的运营商和地址，就知道用户所在的运营商和地址，然后将距离用户位置比较近的Region里面，三个负载均衡SLB的公网IP地址，返回给本地DNS服务器。本地DNS解析器将结果缓存后，返回给客户端。
对于手机App来说，可以绕过刚才的传统DNS解析机制，直接只要HTTPDNS服务，通过直接调用HTTPDNS服务器，得到这三个SLB的公网IP地址。
看，经过了如此复杂的过程，咱们的万里长征还没迈出第一步，刚刚得到IP地址，包还没发呢？话说手机App拿到了公网IP地址，接下来应该做什么呢？
欢迎你留言和我讨论。趣谈网络协议，我们下期见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>第38讲_知识串讲：用双十一的故事串起碎片的网络协议（中）</title><link>https://artisanbox.github.io/5/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/32/</guid><description>上一节我们讲到，手机App经过了一个复杂的过程，终于拿到了电商网站的SLB的IP地址，是不是该下单了？
别忙，俗话说的好，买东西要货比三家。大部分客户在购物之前要看很多商品图片，比来比去，最后好不容易才下决心，点了下单按钮。下单按钮一按，就要开始建立连接。建立连接这个过程也挺复杂的，最终还要经过层层封装，才构建出一个完整的网络包。今天我们就来看这个过程。
4.购物之前看图片，静态资源CDN客户想要在购物网站买一件东西的时候，一般是先去详情页看看图片，是不是想买的那一款。
我们部署电商应用的时候，一般会把静态资源保存在两个地方，一个是接入层nginx后面的varnish缓存里面，一般是静态页面；对于比较大的、不经常更新的静态图片，会保存在对象存储里面。这两个地方的静态资源都会配置CDN，将资源下发到边缘节点。
配置了CDN之后，权威DNS服务器上，会为静态资源设置一个CNAME别名，指向另外一个域名 cdn.com ，返回给本地DNS服务器。
当本地DNS服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的时候就不是原来的权威DNS服务器了，而是 cdn.com 的权威DNS服务器。这是CDN自己的权威DNS服务器。
在这个服务器上，还是会设置一个CNAME，指向另外一个域名，也即CDN网络的全局负载均衡器。
本地DNS服务器去请求CDN的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，将IP返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。
如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器，将内容拉到本地。
5.看上宝贝点下单，双方开始建连接当你浏览了很多图片，发现实在喜欢某个商品，于是决定下单购买。
电商网站会对下单的情况提供RESTful的下单接口，而对于下单这种需要保密的操作，需要通过HTTPS协议进行请求。
在所有这些操作之前，首先要做的事情是建立连接。
HTTPS协议是基于TCP协议的，因而要先建立TCP的连接。在这个例子中，TCP的连接是在手机上的App和负载均衡器SLB之间的。
尽管中间要经过很多的路由器和交换机，但是TCP的连接是端到端的。TCP这一层和更上层的HTTPS无法看到中间的包的过程。尽管建立连接的时候，所有的包都逃不过在这些路由器和交换机之间的转发，转发的细节我们放到那个下单请求的发送过程中详细解读，这里只看端到端的行为。
对于TCP连接来讲，需要通过三次握手建立连接，为了维护这个连接，双方都需要在TCP层维护一个连接的状态机。
一开始，客户端和服务端都处于CLOSED状态。服务端先是主动监听某个端口，处于LISTEN状态。然后客户端主动发起连接SYN，之后处于SYN-SENT状态。服务端收到发起的连接，返回SYN，并且ACK客户端的SYN，之后处于SYN-RCVD状态。
客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态。这是因为，它一发一收成功了。服务端收到ACK的ACK之后，也会处于ESTABLISHED状态，因为它的一发一收也成功了。
当TCP层的连接建立完毕之后，接下来轮到HTTPS层建立连接了，在HTTPS的交换过程中，TCP层始终处于ESTABLISHED。
对于HTTPS，客户端会发送Client Hello消息到服务器，用明文传输TLS版本信息、加密套件候选列表、压缩算法候选列表等信息。另外，还会有一个随机数，在协商对称密钥的时候使用。
然后，服务器会返回Server Hello消息，告诉客户端，服务器选择使用的协议版本、加密套件、压缩算法等。这也有一个随机数，用于后续的密钥协商。
然后，服务器会给你一个服务器端的证书，然后说：“Server Hello Done，我这里就这些信息了。”
客户端当然不相信这个证书，于是从自己信任的CA仓库中，拿CA的证书里面的公钥去解密电商网站的证书。如果能够成功，则说明电商网站是可信的。这个过程中，你可能会不断往上追溯CA、CA的CA、CA的CA的CA，反正直到一个授信的CA，就可以了。
证书验证完毕之后，觉得这个服务端是可信的，于是客户端计算产生随机数字Pre-master，发送Client Key Exchange，用证书中的公钥加密，再发送给服务器，服务器可以通过私钥解密出来。
接下来，无论是客户端还是服务器，都有了三个随机数，分别是：自己的、对端的，以及刚生成的Pre-Master随机数。通过这三个随机数，可以在客户端和服务器产生相同的对称密钥。
有了对称密钥，客户端就可以说：“Change Cipher Spec，咱们以后都采用协商的通信密钥和加密算法进行加密通信了。”
然后客户端发送一个Encrypted Handshake Message，将已经商定好的参数等，采用协商密钥进行加密，发送给服务器用于数据与握手验证。
同样，服务器也可以发送Change Cipher Spec，说：“没问题，咱们以后都采用协商的通信密钥和加密算法进行加密通信了”，并且也发送Encrypted Handshake Message的消息试试。
当双方握手结束之后，就可以通过对称密钥进行加密传输了。
真正的下单请求封装成网络包的发送过程，我们先放一放，我们来接着讲这个网络包的故事。
6.发送下单请求网络包，西行需要出网关当客户端和服务端之间建立了连接后，接下来就要发送下单请求的网络包了。
在用户层发送的是HTTP的网络包，因为服务端提供的是RESTful API，因而HTTP层发送的就是一个请求。
POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/json; charset=utf-8 Content-Length: nnn { &amp;quot;order&amp;quot;: { &amp;quot;date&amp;quot;: &amp;quot;2018-07-01&amp;quot;, &amp;quot;className&amp;quot;: &amp;quot;趣谈网络协议&amp;quot;, &amp;quot;Author&amp;quot;: &amp;quot;刘超&amp;quot;, &amp;quot;price&amp;quot;: &amp;quot;68&amp;quot; } } HTTP的报文大概分为三大部分。第一部分是请求行，第二部分是请求的首部，第三部分才是请求的正文实体。</description></item><item><title>第39讲_知识串讲：用双十一的故事串起碎片的网络协议（下）</title><link>https://artisanbox.github.io/5/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/33/</guid><description>上一节，我们封装了一个长长的网络包，“大炮”准备完毕，开始发送。
发送的时候可以说是重重关隘，从手机到移动网络、互联网，还要经过多个运营商才能到达数据中心，到了数据中心就进入第二个复杂的过程，从网关到VXLAN隧道，到负载均衡，到Controller层、组合服务层、基础服务层，最终才下单入库。今天，我们就来看这最后一段过程。
7.一座座城池一道道关，流控拥塞与重传网络包已经组合完毕，接下来我们来看，如何经过一道道城关，到达目标公网IP。
对于手机来讲，默认的网关在PGW上。在移动网络里面，从手机到SGW，到PGW是有一条隧道的。在这条隧道里面，会将上面的这个包作为隧道的乘客协议放在里面，外面SGW和PGW在核心网机房的IP地址。网络包直到PGW（PGW是隧道的另一端）才将里面的包解出来，转发到外部网络。
所以，从手机发送出来的时候，网络包的结构为：
源MAC：手机也即UE的MAC；
目标MAC：网关PGW上面的隧道端点的MAC；
源IP：UE的IP地址；
目标IP：SLB的公网IP地址。
进入隧道之后，要封装外层的网络地址，因而网络包的格式为：
外层源MAC：E-NodeB的MAC；
外层目标MAC：SGW的MAC；
外层源IP：E-NodeB的IP；
外层目标IP：SGW的IP；
内层源MAC：手机也即UE的MAC；
内层目标MAC：网关PGW上面的隧道端点的MAC；
内层源IP：UE的IP地址；
内层目标IP：SLB的公网IP地址。
当隧道在SGW的时候，切换了一个隧道，会从SGW到PGW的隧道，因而网络包的格式为：
外层源MAC：SGW的MAC；
外层目标MAC：PGW的MAC；
外层源IP：SGW的IP；
外层目标IP：PGW的IP；
内层源MAC：手机也即UE的MAC；
内层目标MAC：网关PGW上面的隧道端点的MAC；
内层源IP：UE的IP地址；
内层目标IP：SLB的公网IP地址。</description></item><item><title>第3讲_ifconfig：最熟悉又陌生的命令行</title><link>https://artisanbox.github.io/5/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/34/</guid><description>上一节结尾给你留的一个思考题是，你知道怎么查看IP地址吗？
当面试听到这个问题的时候，面试者常常会觉得走错了房间。我面试的是技术岗位啊，怎么问这么简单的问题？
的确，即便没有专业学过计算机的人，只要倒腾过电脑，重装过系统，大多也会知道这个问题的答案：在Windows上是ipconfig，在Linux上是ifconfig。
那你知道在Linux上还有什么其他命令可以查看IP地址吗？答案是ip addr。如果回答不上来这个问题，那你可能没怎么用过Linux。
那你知道ifconfig和ip addr的区别吗？这是一个有关net-tools和iproute2的“历史”故事，你刚来到第三节，暂时不用了解这么细，但这也是一个常考的知识点。
想象一下，你登录进入一个被裁剪过的非常小的Linux系统中，发现既没有ifconfig命令，也没有ip addr命令，你是不是感觉这个系统压根儿没法用？这个时候，你可以自行安装net-tools和iproute2这两个工具。当然，大多数时候这两个命令是系统自带的。
安装好后，我们来运行一下ip addr。不出意外，应该会输出下面的内容。
root@test:~# ip addr 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::f816:3eff:fec7:7975/64 scope link valid_lft forever preferred_lft forever 这个命令显示了这台机器上所有的网卡。大部分的网卡都会有一个IP地址，当然，这不是必须的。在后面的分享中，我们会遇到没有IP地址的情况。</description></item><item><title>第40讲_搭建一个网络实验环境：授人以鱼不如授人以渔</title><link>https://artisanbox.github.io/5/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/35/</guid><description>因为这门课是基础课程，而且配合音频的形式发布，所以我多以理论为主来进行讲解。在专栏更新的过程中，不断有同学让我推荐一些网络方面的书籍，还有同学说能不能配合一些实验来说明理论。
的确，网络是一门实验性很强的学科，就像我在开篇词里面说的一样：一看觉得懂，一问就打鼓，一用就糊涂。 在写专栏的过程中，我自己也深深体会到了。这个时候，我常常会拿一个现实的环境，上手操作一下，抓个包看看，这样心里就会有定论。
《TCP/IP详解》实验环境搭建 对于网络方面的书籍，我当然首推Rechard Stevens的《TCP/IP illustrated》（《TCP/IP详解》）。这本书把理论讲得深入浅出，还配有大量的上手实践和抓包，看到这些抓包，原来不理解的很多理论，一下子就能懂了。
这本书里有个拓扑图，书上的很多实验都是基于这个图的，但是这个拓扑图还是挺复杂的。我这里先不说，一会儿详细讲。
Rechard Stevens，因为工作中有这么一个环境，很方便做实验，最终才写出了这样一本书，而我们一般人学习网络，没有这个环境应该怎么办呢？
时代不同了，咱们现在有更加强大的工具了。例如，这里这么多的机器，我们可以用Docker来实现，多个网络可以用Open vSwitch来实现。你甚至不需要一台物理机，只要一台1核2G的虚拟机，就能将这个环境搭建起来。
搭建这个环境的时候，需要一些脚本。我把脚本都放在了Github里面，你可以自己取用。
1.创建一个Ubuntu虚拟机 在你的笔记本电脑上，用VirtualBox创建就行。1核2G，随便一台电脑都能搭建起来。
首先，我们先下载一个Ubuntu的镜像。我是从Ubuntu官方网站下载的。
然后，在VirtualBox里面安装Ubuntu。安装过程网上一大堆教程，你可以自己去看，我这里就不详细说了。
这里我需要说明的是网络的配置。
对于这个虚拟机，我们创建两个网卡，一个是Host-only，只有你的笔记本电脑上能够登录进去。这个网卡上的IP地址也只有在你的笔记本电脑上管用。这个网卡的配置比较稳定，用于在SSH上做操作。这样你的笔记本电脑就可以搬来搬去，在公司里安装一半，回家接着安装另一半都没问题。
这里有一个虚拟的网桥，这个网络可以在管理&amp;gt;主机网络管理里面进行配置。
在这里可以虚拟网桥的的IP地址，同时启用一个DHCP服务器，为新创建的虚拟机配置IP地址。
另一个网卡配置为NAT网络，用于访问互联网。配置了NAT网络之后，只要你的笔记本电脑能上网，虚拟机就能上网。由于咱们在Ubuntu里面要安装一些东西，因而需要联网。
你可能会问了，这个配置复杂吗？一点儿都不复杂。咱们讲虚拟机网络的时候，讲过这个。
安装完了Ubuntu之后，需要对Ubuntu里面的网卡进行配置。对于Ubuntu来讲，网卡的配置在/etc/network/interfaces这个文件里面。在我的环境里，NAT的网卡名称为enp0s3，Host-only的网卡的名称为enp0s8，都可以配置为自动配置。
auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet dhcp
auto enp0s8 iface enp0s8 inet dhcp
这样，重启之后，IP就配置好了。
2.安装Docker和Open vSwitch 接下来，在Ubuntu里面，以root用户，安装Docker和Open vSwitch。
你可以按照Docker的官方安装文档来做。我这里也贴一下我的安装过程。
apt-get remove docker docker-engine docker.io apt-get -y update apt-get -y install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg &amp;gt; gpg apt-key add gpg apt-key fingerprint 0EBFCD88 add-apt-repository &amp;quot;deb [arch=amd64] https://download.</description></item><item><title>第4讲_DHCP与PXE：IP是怎么来的，又是怎么没的？</title><link>https://artisanbox.github.io/5/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/36/</guid><description>上一节，我们讲了IP的一些基本概念。如果需要和其他机器通讯，我们就需要一个通讯地址，我们需要给网卡配置这么一个地址。
如何配置IP地址？那如何配置呢？如果有相关的知识和积累，你可以用命令行自己配置一个地址。可以使用ifconfig，也可以使用ip addr。设置好了以后，用这两个命令，将网卡up一下，就可以开始工作了。
使用net-tools：
$ sudo ifconfig eth1 10.0.0.1/24 $ sudo ifconfig eth1 up 使用iproute2：
$ sudo ip addr add 10.0.0.1/24 dev eth1 $ sudo ip link set up eth1 你可能会问了，自己配置这个自由度太大了吧，我是不是配置什么都可以？如果配置一个和谁都不搭边的地址呢？例如，旁边的机器都是192.168.1.x，我非得配置一个16.158.23.6，会出现什么现象呢？
不会出现任何现象，就是包发不出去呗。为什么发不出去呢？我来举例说明。
192.168.1.6就在你这台机器的旁边，甚至是在同一个交换机上，而你把机器的地址设为了16.158.23.6。在这台机器上，你企图去ping192.168.1.6，你觉得只要将包发出去，同一个交换机的另一台机器马上就能收到，对不对？
可是Linux系统不是这样的，它没你想的那么智能。你用肉眼看到那台机器就在旁边，它则需要根据自己的逻辑进行处理。
还记得我们在第二节说过的原则吗？只要是在网络上跑的包，都是完整的，可以有下层没上层，绝对不可能有上层没下层。
所以，你看着它有自己的源IP地址16.158.23.6，也有目标IP地址192.168.1.6，但是包发不出去，这是因为MAC层还没填。
自己的MAC地址自己知道，这个容易。但是目标MAC填什么呢？是不是填192.168.1.6这台机器的MAC地址呢？
当然不是。Linux首先会判断，要去的这个地址和我是一个网段的吗，或者和我的一个网卡是同一网段的吗？只有是一个网段的，它才会发送ARP请求，获取MAC地址。如果发现不是呢？
Linux默认的逻辑是，如果这是一个跨网段的调用，它便不会直接将包发送到网络上，而是企图将包发送到网关。
如果你配置了网关的话，Linux会获取网关的MAC地址，然后将包发出去。对于192.168.1.6这台机器来讲，虽然路过它家门的这个包，目标IP是它，但是无奈MAC地址不是它的，所以它的网卡是不会把包收进去的。
如果没有配置网关呢？那包压根就发不出去。
如果将网关配置为192.168.1.6呢？不可能，Linux不会让你配置成功的，因为网关要和当前的网络至少一个网卡是同一个网段的，怎么可能16.158.23.6的网关是192.168.1.6呢？
所以，当你需要手动配置一台机器的网络IP时，一定要好好问问你的网络管理员。如果在机房里面，要去网络管理员那里申请，让他给你分配一段正确的IP地址。当然，真正配置的时候，一定不是直接用命令配置的，而是放在一个配置文件里面。不同系统的配置文件格式不同，但是无非就是CIDR、子网掩码、广播地址和网关地址。
动态主机配置协议（DHCP）原来配置IP有这么多门道儿啊。你可能会问了，配置了IP之后一般不能变的，配置一个服务端的机器还可以，但是如果是客户端的机器呢？我抱着一台笔记本电脑在公司里走来走去，或者白天来晚上走，每次使用都要配置IP地址，那可怎么办？还有人事、行政等非技术人员，如果公司所有的电脑都需要IT人员配置，肯定忙不过来啊。
因此，我们需要有一个自动配置的协议，也就是动态主机配置协议（Dynamic Host Configuration Protocol），简称DHCP。
有了这个协议，网络管理员就轻松多了。他只需要配置一段共享的IP地址。每一台新接入的机器都通过DHCP协议，来这个共享的IP地址里申请，然后自动配置好就可以了。等人走了，或者用完了，还回去，这样其他的机器也能用。
所以说，如果是数据中心里面的服务器，IP一旦配置好，基本不会变，这就相当于买房自己装修。DHCP的方式就相当于租房。你不用装修，都是帮你配置好的。你暂时用一下，用完退租就可以了。
解析DHCP的工作方式当一台机器新加入一个网络的时候，肯定一脸懵，啥情况都不知道，只知道自己的MAC地址。怎么办？先吼一句，我来啦，有人吗？这时候的沟通基本靠“吼”。这一步，我们称为DHCP Discover。
新来的机器使用IP地址0.0.0.0发送了一个广播包，目的IP地址为255.255.255.255。广播包封装了UDP，UDP封装了BOOTP。其实DHCP是BOOTP的增强版，但是如果你去抓包的话，很可能看到的名称还是BOOTP协议。
在这个广播包里面，新人大声喊：我是新来的（Boot request），我的MAC地址是这个，我还没有IP，谁能给租给我个IP地址！
格式就像这样：
如果一个网络管理员在网络里面配置了DHCP Server的话，他就相当于这些IP的管理员。他立刻能知道来了一个“新人”。这个时候，我们可以体会MAC地址唯一的重要性了。当一台机器带着自己的MAC地址加入一个网络的时候，MAC是它唯一的身份，如果连这个都重复了，就没办法配置了。
只有MAC唯一，IP管理员才能知道这是一个新人，需要租给它一个IP地址，这个过程我们称为DHCP Offer。同时，DHCP Server为此客户保留为它提供的IP地址，从而不会为其他DHCP客户分配此IP地址。
DHCP Offer的格式就像这样，里面有给新人分配的地址。
DHCP Server仍然使用广播地址作为目的地址，因为，此时请求分配IP的新人还没有自己的IP。DHCP Server回复说，我分配了一个可用的IP给你，你看如何？除此之外，服务器还发送了子网掩码、网关和IP地址租用期等信息。
新来的机器很开心，它的“吼”得到了回复，并且有人愿意租给它一个IP地址了，这意味着它可以在网络上立足了。当然更令人开心的是，如果有多个DHCP Server，这台新机器会收到多个IP地址，简直受宠若惊。
它会选择其中一个DHCP Offer，一般是最先到达的那个，并且会向网络发送一个DHCP Request广播数据包，包中包含客户端的MAC地址、接受的租约中的IP地址、提供此租约的DHCP服务器地址等，并告诉所有DHCP Server它将接受哪一台服务器提供的IP地址，告诉其他DHCP服务器，谢谢你们的接纳，并请求撤销它们提供的IP地址，以便提供给下一个IP租用请求者。</description></item><item><title>第5讲_从物理层到MAC层：如何在宿舍里自己组网玩联机游戏？</title><link>https://artisanbox.github.io/5/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/37/</guid><description>上一节，我们见证了IP地址的诞生，或者说是整个操作系统的诞生。一旦机器有了IP，就可以在网络的环境里和其他的机器展开沟通了。
故事就从我的大学宿舍开始讲起吧。作为一个八零后，我要暴露年龄了。
我们宿舍四个人，大一的时候学校不让上网，不给开通网络。但是，宿舍有一个人比较有钱，率先买了一台电脑。那买了电脑干什么呢？
首先，有单机游戏可以打，比如说《拳皇》。两个人用一个键盘，照样打得火热。后来有第二个人买了电脑，那两台电脑能不能连接起来呢？你会说，当然能啊，买个路由器不就行了。
现在一台家用路由器非常便宜，一百多块的事情。那时候路由器绝对是奢侈品。一直到大四，我们宿舍都没有买路由器。可能是因为那时候技术没有现在这么发达，导致我对网络技术的认知是逐渐深入的，而且每一层都是实实在在接触到的。
第一层（物理层）使用路由器，是在第三层上。我们先从第一层物理层开始说。
物理层能折腾啥？现在的同学可能想不到，我们当时去学校配电脑的地方买网线，卖网线的师傅都会问，你的网线是要电脑连电脑啊，还是电脑连网口啊？
我们要的是电脑连电脑。这种方式就是一根网线，有两个头。一头插在一台电脑的网卡上，另一头插在另一台电脑的网卡上。但是在当时，普通的网线这样是通不了的，所以水晶头要做交叉线，用的就是所谓的1－3、2－6交叉接法。
水晶头的第1、2和第3、6脚，它们分别起着收、发信号的作用。将一端的1号和3号线、2号和6号线互换一下位置，就能够在物理层实现一端发送的信号，另一端能收到。
当然电脑连电脑，除了网线要交叉，还需要配置这两台电脑的IP地址、子网掩码和默认网关。这三个概念上一节详细描述过了。要想两台电脑能够通信，这三项必须配置成为一个网络，可以一个是192.168.0.1/24，另一个是192.168.0.2/24，否则是不通的。
这里我想问你一个问题，两台电脑之间的网络包，包含MAC层吗？当然包含，要完整。IP层要封装了MAC层才能将包放入物理层。
到此为止，两台电脑已经构成了一个最小的局域网，也即LAN。可以玩联机局域网游戏啦！
等到第三个哥们也买了一台电脑，怎么把三台电脑连在一起呢？
先别说交换机，当时交换机也贵。有一个叫做Hub的东西，也就是集线器。这种设备有多个口，可以将宿舍里的多台电脑连接起来。但是，和交换机不同，集线器没有大脑，它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。
第二层（数据链路层）你可能已经发现问题了。Hub采取的是广播的模式，如果每一台电脑发出的包，宿舍的每个电脑都能收到，那就麻烦了。这就需要解决几个问题：
这个包是发给谁的？谁应该接收？ 大家都在发，会不会产生混乱？有没有谁先发、谁后发的规则？ 如果发送的时候出现了错误，怎么办？ 这几个问题，都是第二层，数据链路层，也即MAC层要解决的问题。MAC的全称是Medium Access Control，即媒体访问控制。控制什么呢？其实就是控制在往媒体上发数据的时候，谁先发、谁后发的问题。防止发生混乱。这解决的是第二个问题。这个问题中的规则，学名叫多路访问。有很多算法可以解决这个问题。就像车管所管束马路上跑的车，能想的办法都想过了。
比如接下来这三种方式：
方式一：分多个车道。每个车一个车道，你走你的，我走我的。这在计算机网络里叫作信道划分；
方式二：今天单号出行，明天双号出行，轮着来。这在计算机网络里叫作轮流协议；
方式三：不管三七二十一，有事儿先出门，发现特堵，就回去。错过高峰再出。我们叫作随机接入协议。著名的以太网，用的就是这个方式。
解决了第二个问题，就是解决了媒体接入控制的问题，MAC的问题也就解决好了。这和MAC地址没什么关系。
接下来要解决第一个问题：发给谁，谁接收？这里用到一个物理地址，叫作链路层地址。但是因为第二层主要解决媒体接入控制的问题，所以它常被称为MAC地址。
解决第一个问题就牵扯到第二层的网络包格式。对于以太网，第二层的最开始，就是目标的MAC地址和源的MAC地址。
接下来是类型，大部分的类型是IP数据包，然后IP里面包含TCP、UDP，以及HTTP等，这都是里层封装的事情。
有了这个目标MAC地址，数据包在链路上广播，MAC的网卡才能发现，这个包是给它的。MAC的网卡把包收进来，然后打开IP包，发现IP地址也是自己的，再打开TCP包，发现端口是自己，也就是80，而nginx就是监听80。
于是将请求提交给nginx，nginx返回一个网页。然后将网页需要发回请求的机器。然后层层封装，最后到MAC层。因为来的时候有源MAC地址，返回的时候，源MAC就变成了目标MAC，再返给请求的机器。
对于以太网，第二层的最后面是CRC，也就是循环冗余检测。通过XOR异或的算法，来计算整个包是否在发送的过程中出现了错误，主要解决第三个问题。
这里还有一个没有解决的问题，当源机器知道目标机器的时候，可以将目标地址放入包里面，如果不知道呢？一个广播的网络里面接入了N台机器，我怎么知道每个MAC地址是谁呢？这就是ARP协议，也就是已知IP地址，求MAC地址的协议。
在一个局域网里面，当知道了IP地址，不知道MAC怎么办呢？靠“吼”。
广而告之，发送一个广播包，谁是这个IP谁来回答。具体询问和回答的报文就像下面这样：
为了避免每次都用ARP请求，机器本地也会进行ARP缓存。当然机器会不断地上线下线，IP也可能会变，所以ARP的MAC地址缓存过一段时间就会过期。
局域网好了，至此我们宿舍四个电脑就组成了一个局域网。用Hub连接起来，就可以玩局域网版的《魔兽争霸》了。
打开游戏，进入“局域网选项”，选择一张地图，点击“创建游戏”，就可以进入这张地图的房间中。等同一个局域网里的其他小伙伴加入后，游戏就可以开始了。
这种组网的方法，对一个宿舍来说没有问题，但是一旦机器数目增多，问题就出现了。因为Hub是广播的，不管某个接口是否需要，所有的Bit都会被发送出去，然后让主机来判断是不是需要。这种方式路上的车少就没问题，车一多，产生冲突的概率就提高了。而且把不需要的包转发过去，纯属浪费。看来Hub这种不管三七二十一都转发的设备是不行了，需要点儿智能的。因为每个口都只连接一台电脑，这台电脑又不怎么换IP和MAC地址，只要记住这台电脑的MAC地址，如果目标MAC地址不是这台电脑的，这个口就不用转发了。
谁能知道目标MAC地址是否就是连接某个口的电脑的MAC地址呢？这就需要一个能把MAC头拿下来，检查一下目标MAC地址，然后根据策略转发的设备，按第二节课中讲过的，这个设备显然是个二层设备，我们称为交换机。
交换机怎么知道每个口的电脑的MAC地址呢？这需要交换机会学习。
一台MAC1电脑将一个包发送给另一台MAC2电脑，当这个包到达交换机的时候，一开始交换机也不知道MAC2的电脑在哪个口，所以没办法，它只能将包转发给除了来的那个口之外的其他所有的口。但是，这个时候，交换机会干一件非常聪明的事情，就是交换机会记住，MAC1是来自一个明确的口。以后有包的目的地址是MAC1的，直接发送到这个口就可以了。
当交换机作为一个关卡一样，过了一段时间之后，就有了整个网络的一个结构了，这个时候，基本上不用广播了，全部可以准确转发。当然，每个机器的IP地址会变，所在的口也会变，因而交换机上的学习的结果，我们称为转发表，是有一个过期时间的。
有了交换机，一般来说，你接个几十台、上百台机器打游戏，应该没啥问题。你可以组个战队了。能上网了，就可以玩网游了。
小结好了，今天的内容差不多了，我们来总结一下，有三个重点需要你记住：
第一，MAC层是用来解决多路访问的堵车问题的；
第二，ARP是通过吼的方式来寻找目标MAC地址的，吼完之后记住一段时间，这个叫作缓存；
第三，交换机是有MAC地址学习能力的，学完了它就知道谁在哪儿了，不用广播了。
最后，给你留两个思考题吧。
在二层中我们讲了ARP协议，即已知IP地址求MAC；还有一种RARP协议，即已知MAC求IP的，你知道它可以用来干什么吗？ 如果一个局域网里面有多个交换机，ARP广播的模式会出现什么问题呢？ 欢迎你留言和我讨论。趣谈网络协议，我们下期见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>第6讲_交换机与VLAN：办公室太复杂，我要回学校</title><link>https://artisanbox.github.io/5/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/38/</guid><description>上一次，我们在宿舍里组建了一个本地的局域网LAN，可以愉快地玩游戏了。这是一个非常简单的场景，因为只有一台交换机，电脑数目很少。今天，让我们切换到一个稍微复杂一点的场景，办公室。
拓扑结构是怎么形成的？我们常见到的办公室大多是一排排的桌子，每个桌子都有网口，一排十几个座位就有十几个网口，一个楼层就会有几十个甚至上百个网口。如果算上所有楼层，这个场景自然比你宿舍里的复杂多了。具体哪里复杂呢？我来给你具体讲解。
首先，这个时候，一个交换机肯定不够用，需要多台交换机，交换机之间连接起来，就形成一个稍微复杂的拓扑结构。
我们先来看两台交换机的情形。两台交换机连接着三个局域网，每个局域网上都有多台机器。如果机器1只知道机器4的IP地址，当它想要访问机器4，把包发出去的时候，它必须要知道机器4的MAC地址。
于是机器1发起广播，机器2收到这个广播，但是这不是找它的，所以没它什么事。交换机A一开始是不知道任何拓扑信息的，在它收到这个广播后，采取的策略是，除了广播包来的方向外，它还要转发给其他所有的网口。于是机器3也收到广播信息了，但是这和它也没什么关系。
当然，交换机B也是能够收到广播信息的，但是这时候它也是不知道任何拓扑信息的，因而也是进行广播的策略，将包转发到局域网三。这个时候，机器4和机器5都收到了广播信息。机器4主动响应说，这是找我的，这是我的MAC地址。于是一个ARP请求就成功完成了。
在上面的过程中，交换机A和交换机B都是能够学习到这样的信息：机器1是在左边这个网口的。当了解到这些拓扑信息之后，情况就好转起来。当机器2要访问机器1的时候，机器2并不知道机器1的MAC地址，所以机器2会发起一个ARP请求。这个广播消息会到达机器1，也同时会到达交换机A。这个时候交换机A已经知道机器1是不可能在右边的网口的，所以这个广播信息就不会广播到局域网二和局域网三。
当机器3要访问机器1的时候，也需要发起一个广播的ARP请求。这个时候交换机A和交换机B都能够收到这个广播请求。交换机A当然知道主机A是在左边这个网口的，所以会把广播消息转发到局域网一。同时，交换机B收到这个广播消息之后，由于它知道机器1是不在右边这个网口的，所以不会将消息广播到局域网三。
如何解决常见的环路问题？这样看起来，两台交换机工作得非常好。随着办公室越来越大，交换机数目肯定越来越多。当整个拓扑结构复杂了，这么多网线，绕过来绕过去，不可避免地会出现一些意料不到的情况。其中常见的问题就是环路问题。
例如这个图，当两个交换机将两个局域网同时连接起来的时候。你可能会觉得，这样反而有了高可用性。但是却不幸地出现了环路。出现了环路会有什么结果呢？
我们来想象一下机器1访问机器2的过程。一开始，机器1并不知道机器2的MAC地址，所以它需要发起一个ARP的广播。广播到达机器2，机器2会把MAC地址返回来，看起来没有这两个交换机什么事情。
但是问题来了，这两个交换机还是都能够收到广播包的。交换机A一开始是不知道机器2在哪个局域网的，所以它会把广播消息放到局域网二，在局域网二广播的时候，交换机B右边这个网口也是能够收到广播消息的。交换机B会将这个广播信息发送到局域网一。局域网一的这个广播消息，又会到达交换机A左边的这个接口。交换机A这个时候还是不知道机器2在哪个局域网，于是将广播包又转发到局域网二。左转左转左转，好像是个圈哦。
可能有人会说，当两台交换机都能够逐渐学习到拓扑结构之后，是不是就可以了？
别想了，压根儿学不会的。机器1的广播包到达交换机A和交换机B的时候，本来两个交换机都学会了机器1是在局域网一的，但是当交换机A将包广播到局域网二之后，交换机B右边的网口收到了来自交换机A的广播包。根据学习机制，这彻底损坏了交换机B的三观，刚才机器1还在左边的网口呢，怎么又出现在右边的网口呢？哦，那肯定是机器1换位置了，于是就误会了，交换机B就学会了，机器1是从右边这个网口来的，把刚才学习的那一条清理掉。同理，交换机A右边的网口，也能收到交换机B转发过来的广播包，同样也误会了，于是也学会了，机器1从右边的网口来，不是从左边的网口来。
然而当广播包从左边的局域网一广播的时候，两个交换机再次刷新三观，原来机器1是在左边的，过一会儿，又发现不对，是在右边的，过一会，又发现不对，是在左边的。
这还是一个包转来转去，每台机器都会发广播包，交换机转发也会复制广播包，当广播包越来越多的时候，按照上一节讲过一个共享道路的算法，也就是路会越来越堵，最后谁也别想走。所以，必须有一个方法解决环路的问题，怎么破除环路呢？
STP协议中那些难以理解的概念在数据结构中，有一个方法叫做最小生成树。有环的我们常称为图。将图中的环破了，就生成了树。在计算机网络中，生成树的算法叫作STP，全称Spanning Tree Protocol。
STP协议比较复杂，一开始很难看懂，但是其实这是一场血雨腥风的武林比武或者华山论剑，最终决出五岳盟主的方式。
在STP协议里面有很多概念，译名就非常拗口，但是我一作比喻，你很容易就明白了。
Root Bridge，也就是根交换机。这个比较容易理解，可以比喻为“掌门”交换机，是某棵树的老大，是掌门，最大的大哥。
Designated Bridges，有的翻译为指定交换机。这个比较难理解，可以想像成一个“小弟”，对于树来说，就是一棵树的树枝。所谓“指定”的意思是，我拜谁做大哥，其他交换机通过这个交换机到达根交换机，也就相当于拜他做了大哥。这里注意是树枝，不是叶子，因为叶子往往是主机。
Bridge Protocol Data Units （BPDU） ，网桥协议数据单元。可以比喻为“相互比较实力”的协议。行走江湖，比的就是武功，拼的就是实力。当两个交换机碰见的时候，也就是相连的时候，就需要互相比一比内力了。BPDU只有掌门能发，已经隶属于某个掌门的交换机只能传达掌门的指示。
Priority Vector，优先级向量。可以比喻为实力 （值越小越牛）。实力是啥？就是一组ID数目，[Root Bridge ID, Root Path Cost, Bridge ID, and Port ID]。为什么这样设计呢？这是因为要看怎么来比实力。先看Root Bridge ID。拿出老大的ID看看，发现掌门一样，那就是师兄弟；再比Root Path Cost，也即我距离我的老大的距离，也就是拿和掌门关系比，看同一个门派内谁和老大关系铁；最后比Bridge ID，比我自己的ID，拿自己的本事比。
STP的工作过程是怎样的？接下来，我们来看STP的工作过程。
一开始，江湖纷争，异常混乱。大家都觉得自己是掌门，谁也不服谁。于是，所有的交换机都认为自己是掌门，每个网桥都被分配了一个ID。这个ID里有管理员分配的优先级，当然网络管理员知道哪些交换机贵，哪些交换机好，就会给它们分配高的优先级。这种交换机生下来武功就很高，起步就是乔峰。
既然都是掌门，互相都连着网线，就互相发送BPDU来比功夫呗。这一比就发现，有人是岳不群，有人是封不平，赢的接着当掌门，输的就只好做小弟了。当掌门的还会继续发BPDU，而输的人就没有机会了。它们只有在收到掌门发的BPDU的时候，转发一下，表示服从命令。
数字表示优先级。就像这个图，5和6碰见了，6的优先级低，所以乖乖做小弟。于是一个小门派形成，5是掌门，6是小弟。其他诸如1-7、2-8、3-4这样的小门派，也诞生了。于是江湖出现了很多小的门派，小的门派，接着合并。
合并的过程会出现以下四种情形，我分别来介绍。
情形一：掌门遇到掌门当5碰到了1，掌门碰见掌门，1觉得自己是掌门，5也刚刚跟别人PK完成为掌门。这俩掌门比较功夫，最终1胜出。于是输掉的掌门5就会率领所有的小弟归顺。结果就是1成为大掌门。
情形二：同门相遇同门相遇可以是掌门与自己的小弟相遇，这说明存在“环”了。这个小弟已经通过其他门路拜在你门下，结果你还不认识，就PK了一把。结果掌门发现这个小弟功夫不错，不应该级别这么低，就把它招到门下亲自带，那这个小弟就相当于升职了。
我们再来看，假如1和6相遇。6原来就拜在1的门下，只不过6的上司是5，5的上司是1。1发现，6距离我才只有2，比从5这里过来的5（=4+1）近多了，那6就直接汇报给我吧。于是，5和6分别汇报给1。
同门相遇还可以是小弟相遇。这个时候就要比较谁和掌门的关系近，当然近的当大哥。刚才5和6同时汇报给1了，后来5和6在比较功夫的时候发现，5你直接汇报给1距离是4，如果5汇报给6再汇报给1，距离只有2+1=3，所以5干脆拜6为上司。
情形三：掌门与其他帮派小弟相遇小弟拿本帮掌门和这个掌门比较，赢了，这个掌门拜入门来。输了，会拜入新掌门，并且逐渐拉拢和自己连接的兄弟，一起弃暗投明。
例如，2和7相遇，虽然7是小弟，2是掌门。就个人武功而言，2比7强，但是7的掌门是1，比2牛，所以没办法，2要拜入7的门派，并且连同自己的小弟都一起拜入。</description></item><item><title>第7讲_ICMP与ping：投石问路的侦察兵</title><link>https://artisanbox.github.io/5/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/39/</guid><description>无论是在宿舍，还是在办公室，或者运维一个数据中心，我们常常会遇到网络不通的问题。那台机器明明就在那里，你甚至都可以通过机器的终端连上去看。它看着好好的，可是就是连不上去，究竟是哪里出了问题呢？
ICMP协议的格式一般情况下，你会想到ping一下。那你知道ping是如何工作的吗？
ping是基于ICMP协议工作的。ICMP全称Internet Control Message Protocol，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢？
网络包在异常复杂的网络环境中传输时，常常会遇到各种各样的问题。当遇到问题的时候，总不能“死个不明不白”，要传出消息来，报告情况，这样才可以调整传输策略。这就相当于我们经常看到的电视剧里，古代行军的时候，为将为帅者需要通过侦察兵、哨探或传令兵等人肉的方式来掌握情况，控制整个战局。
ICMP报文是封装在IP包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。因为作为侦查兵，要轻装上阵，不能携带大量的包袱。
ICMP报文有很多的类型，不同的类型有不同的代码。最常用的类型是主动请求为8，主动请求的应答为0。
查询报文类型我们经常在电视剧里听到这样的话：主帅说，来人哪！前方战事如何，快去派人打探，一有情况，立即通报！
这种是主帅发起的，主动查看敌情，对应ICMP的查询报文类型。例如，常用的ping就是查询报文，是一种主动请求，并且获得主动应答的ICMP协议。所以，ping发的包也是符合ICMP协议格式的，只不过它在后面增加了自己的格式。
对ping的主动请求，进行网络抓包，称为ICMP ECHO REQUEST。同理主动请求的回复，称为ICMP ECHO REPLY。比起原生的ICMP，这里面多了两个字段，一个是标识符。这个很好理解，你派出去两队侦查兵，一队是侦查战况的，一队是去查找水源的，要有个标识才能区分。另一个是序号，你派出去的侦查兵，都要编个号。如果派出去10个，回来10个，就说明前方战况不错；如果派出去10个，回来2个，说明情况可能不妙。
在选项数据中，ping还会存放发送请求的时间值，来计算往返时间，说明路程的长短。
差错报文类型当然也有另外一种方式，就是差错报文。
主帅骑马走着走着，突然来了一匹快马，上面的小兵气喘吁吁的：报告主公，不好啦！张将军遭遇埋伏，全军覆没啦！这种是异常情况发起的，来报告发生了不好的事情，对应ICMP的差错报文类型。
我举几个ICMP差错报文的例子：终点不可达为3，源抑制为4，超时为11，重定向为5。这些都是什么意思呢？我给你具体解释一下。
第一种是终点不可达。小兵：报告主公，您让把粮草送到张将军那里，结果没有送到。
如果你是主公，你肯定会问，为啥送不到？具体的原因在代码中表示就是，网络不可达代码为0，主机不可达代码为1，协议不可达代码为2，端口不可达代码为3，需要进行分片但设置了不分片位代码为4。
具体的场景就像这样：
网络不可达：主公，找不到地方呀？ 主机不可达：主公，找到地方没这个人呀？ 协议不可达：主公，找到地方，找到人，口号没对上，人家天王盖地虎，我说12345！ 端口不可达：主公，找到地方，找到人，对了口号，事儿没对上，我去送粮草，人家说他们在等救兵。 需要进行分片但设置了不分片位：主公，走到一半，山路狭窄，想换小车，但是您的将令，严禁换小车，就没办法送到了。 第二种是源站抑制，也就是让源站放慢发送速度。小兵：报告主公，您粮草送的太多了吃不完。
第三种是时间超时，也就是超过网络包的生存时间还是没到。小兵：报告主公，送粮草的人，自己把粮草吃完了，还没找到地方，已经饿死啦。
第四种是路由重定向，也就是让下次发给另一个路由器。小兵：报告主公，上次送粮草的人本来只要走一站地铁，非得从五环绕，下次别这样了啊。
差错报文的结构相对复杂一些。除了前面还是IP，ICMP的前8字节不变，后面则跟上出错的那个IP包的IP头和IP正文的前8个字节。
而且这类侦查兵特别恪尽职守，不但自己返回来报信，还把一部分遗物也带回来。
侦察兵：报告主公，张将军已经战死沙场，这是张将军的印信和佩剑。 主公：神马？张将军是怎么死的（可以查看ICMP的前8字节）？没错，这是张将军的剑，是他的剑（IP数据包的头及正文前8字节）。 ping：查询报文类型的使用接下来，我们重点来看ping的发送和接收过程。
假定主机A的IP地址是192.168.1.1，主机B的IP地址是192.168.1.2，它们都在同一个子网。那当你在主机A上运行“ping 192.168.1.2”后，会发生什么呢?
ping命令执行的时候，源主机首先会构建一个ICMP请求数据包，ICMP数据包内包含多个字段。最重要的是两个，第一个是类型字段，对于请求数据包而言该字段为 8；另外一个是顺序号，主要用于区分连续ping的时候发出的多个数据包。每发出一个请求数据包，顺序号会自动加1。为了能够计算往返时间RTT，它会在报文的数据部分插入发送时间。
然后，由ICMP协议将这个数据包连同地址192.168.1.2一起交给IP层。IP层将以192.168.1.2作为目的地址，本机IP地址作为源地址，加上一些其他控制信息，构建一个IP数据包。
接下来，需要加入MAC头。如果在本节ARP映射表中查找出IP地址192.168.1.2所对应的MAC地址，则可以直接使用；如果没有，则需要发送ARP协议查询MAC地址，获得MAC地址后，由数据链路层构建一个数据帧，目的地址是IP层传过来的MAC地址，源地址则是本机的MAC地址；还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。
主机B收到这个数据帧后，先检查它的目的MAC地址，并和本机的MAC地址对比，如符合，则接收，否则就丢弃。接收后检查该数据帧，将IP数据包从帧中提取出来，交给本机的IP层。同样，IP层检查后，将有用的信息提取后交给ICMP协议。
主机B会构建一个 ICMP 应答包，应答数据包的类型字段为 0，顺序号为接收到的请求数据包中的顺序号，然后再发送出去给主机A。
在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明目标主机不可达；如果接收到了 ICMP 应答包，则说明目标主机可达。此时，源主机会检查，用当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。
当然这只是最简单的，同一个局域网里面的情况。如果跨网段的话，还会涉及网关的转发、路由器的转发等等。但是对于ICMP的头来讲，是没什么影响的。会影响的是根据目标IP地址，选择路由的下一跳，还有每经过一个路由器到达一个新的局域网，需要换MAC头里面的MAC地址。这个过程后面几节会详细描述，这里暂时不多说。
如果在自己的可控范围之内，当遇到网络不通的问题的时候，除了直接ping目标的IP地址之外，还应该有一个清晰的网络拓扑图。并且从理论上来讲，应该要清楚地知道一个网络包从源地址到目标地址都需要经过哪些设备，然后逐个ping中间的这些设备或者机器。如果可能的话，在这些关键点，通过tcpdump -i eth0 icmp，查看包有没有到达某个点，回复的包到达了哪个点，可以更加容易推断出错的位置。
经常会遇到一个问题，如果不在我们的控制范围内，很多中间设备都是禁止ping的，但是ping不通不代表网络不通。这个时候就要使用telnet，通过其他协议来测试网络是否通，这个就不在本篇的讲述范围了。
说了这么多，你应该可以看出ping这个程序是使用了ICMP里面的ECHO REQUEST和ECHO REPLY类型的。
Traceroute：差错报文类型的使用那其他的类型呢？是不是只有真正遇到错误的时候，才能收到呢？那也不是，有一个程序Traceroute，是个“大骗子”。它会使用ICMP的规则，故意制造一些能够产生错误的场景。
所以，Traceroute的第一个作用就是故意设置特殊的TTL，来追踪去往目的地时沿途经过的路由器。Traceroute的参数指向某个目的IP地址，它会发送一个UDP的数据包。将TTL设置成1，也就是说一旦遇到一个路由器或者一个关卡，就表示它“牺牲”了。
如果中间的路由器不止一个，当然碰到第一个就“牺牲”。于是，返回一个ICMP包，也就是网络差错包，类型是时间超时。那大军前行就带一顿饭，试一试走多远会被饿死，然后找个哨探回来报告，那我就知道大军只带一顿饭能走多远了。
接下来，将TTL设置为2。第一关过了，第二关就“牺牲”了，那我就知道第二关有多远。如此反复，直到到达目的主机。这样，Traceroute就拿到了所有的路由器IP。当然，有的路由器压根不会回这个ICMP。这也是Traceroute一个公网的地址，看不到中间路由的原因。</description></item><item><title>第8讲_世界这么大，我想出网关：欧洲十国游与玄奘西行</title><link>https://artisanbox.github.io/5/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/40/</guid><description>前几节，我主要跟你讲了宿舍里和办公室里用到的网络协议。你已经有了一些基础，是时候去外网逛逛了！
怎么在宿舍上网？还记得咱们在宿舍的时候买了台交换机，几台机器组了一个局域网打游戏吗？可惜啊，只能打局域网的游戏，不能上网啊！盼啊盼啊，终于盼到大二，允许宿舍开通网络了。学校给每个宿舍的网口分配了一个IP地址。这个IP是校园网的IP，完全由网管部门控制。宿舍网的IP地址多为192.168.1.x。校园网的IP地址，假设是10.10.x.x。
这个时候，你要在宿舍上网，有两个办法：
第一个办法，让你们宿舍长再买一个网卡。这个时候，你们宿舍长的电脑里就有两张网卡。一张网卡的线插到你们宿舍的交换机上，另一张网卡的线插到校园网的网口。而且，这张新的网卡的IP地址要按照学校网管部门分配的配置，不然上不了网。这种情况下，如果你们宿舍的人要上网，就需要一直开着宿舍长的电脑。
第二个办法，你们共同出钱买个家庭路由器（反正当时我们买不起）。家庭路由器会有内网网口和外网网口。把外网网口的线插到校园网的网口上，将这个外网网口配置成和网管部的一样。内网网口连上你们宿舍的所有的电脑。这种情况下，如果你们宿舍的人要上网，就需要一直开着路由器。
这两种方法其实是一样的。只不过第一种方式，让你的宿舍长的电脑，变成一个有多个口的路由器而已。而你买的家庭路由器，里面也跑着程序，和你宿舍长电脑里的功能一样，只不过是一个嵌入式的系统。
当你的宿舍长能够上网之后，接下来，就是其他人的电脑怎么上网的问题。这就需要配置你们的网卡。当然DHCP是可以默认配置的。在进行网卡配置的时候，除了IP地址，还需要配置一个Gateway的东西，这个就是网关。
你了解MAC头和IP头的细节吗？一旦配置了IP地址和网关，往往就能够指定目标地址进行访问了。由于在跨网关访问的时候，牵扯到MAC地址和IP地址的变化，这里有必要详细描述一下MAC头和IP头的细节。
在MAC头里面，先是目标MAC地址，然后是源MAC地址，然后有一个协议类型，用来说明里面是IP协议。IP头里面的版本号，目前主流的还是IPv4，服务类型TOS在第三节讲ip addr命令的时候讲过，TTL在第7节讲ICMP协议的时候讲过。另外，还有8位标识协议。这里到了下一层的协议，也就是，是TCP还是UDP。最重要的就是源IP和目标IP。先是源IP地址，然后是目标IP地址。
在任何一台机器上，当要访问另一个IP地址的时候，都会先判断，这个目标IP地址，和当前机器的IP地址，是否在同一个网段。怎么判断同一个网段呢？需要CIDR和子网掩码，这个在第三节的时候也讲过了。
如果是同一个网段，例如，你访问你旁边的兄弟的电脑，那就没网关什么事情，直接将源地址和目标地址放入IP头中，然后通过ARP获得MAC地址，将源MAC和目的MAC放入MAC头中，发出去就可以了。
如果不是同一网段，例如，你要访问你们校园网里面的BBS，该怎么办？这就需要发往默认网关Gateway。Gateway的地址一定是和源IP地址是一个网段的。往往不是第一个，就是第二个。例如192.168.1.0/24这个网段，Gateway往往会是192.168.1.1/24或者192.168.1.2/24。
如何发往默认网关呢？网关不是和源IP地址是一个网段的么？这个过程就和发往同一个网段的其他机器是一样的：将源地址和目标IP地址放入IP头中，通过ARP获得网关的MAC地址，将源MAC和网关的MAC放入MAC头中，发送出去。网关所在的端口，例如192.168.1.1/24将网络包收进来，然后接下来怎么做，就完全看网关的了。
网关往往是一个路由器，是一个三层转发的设备。啥叫三层设备？前面也说过了，就是把MAC头和IP头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。
在你的宿舍里面，网关就是你宿舍长的电脑。一个路由器往往有多个网口，如果是一台服务器做这个事情，则就有多个网卡，其中一个网卡是和源IP同网段的。
很多情况下，人们把网关就叫做路由器。其实不完全准确，而另一种比喻更加恰当：路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的IP地址都和局域网的IP地址相同的网段，每只手都是它握住的那个局域网的网关。
任何一个想发往其他局域网的包，都会到达其中一只手，被拿进来，拿下MAC头和IP头，看看，根据自己的路由算法，选择另一只手，加上IP头和MAC头，然后扔出去。
静态路由是什么？这个时候，问题来了，该选择哪一只手？IP头和MAC头加什么内容，哪些变、哪些不变呢？这个问题比较复杂，大致可以分为两类，一个是静态路由，一个是动态路由。动态路由下一节我们详细地讲。这一节我们先说静态路由。
静态路由，其实就是在路由器上，配置一条一条规则。这些规则包括：想访问BBS站（它肯定有个网段），从2号口出去，下一跳是IP2；想访问教学视频站（它也有个自己的网段），从3号口出去，下一跳是IP3，然后保存在路由器里。
每当要选择从哪只手抛出去的时候，就一条一条的匹配规则，找到符合的规则，就按规则中设置的那样，从某个口抛出去，找下一跳IPX。
IP头和MAC头哪些变、哪些不变？对于IP头和MAC头哪些变、哪些不变的问题，可以分两种类型。我把它们称为“欧洲十国游”型和“玄奘西行”型。
之前我说过，MAC地址是一个局域网内才有效的地址。因而，MAC地址只要过网关，就必定会改变，因为已经换了局域网。两者主要的区别在于IP地址是否改变。不改变IP地址的网关，我们称为转发网关；改变IP地址的网关，我们称为NAT网关。
“欧洲十国游”型结合这个图，我们先来看“欧洲十国游”型。
服务器A要访问服务器B。首先，服务器A会思考，192.168.4.101和我不是一个网段的，因而需要先发给网关。那网关是谁呢？已经静态配置好了，网关是192.168.1.1。网关的MAC地址是多少呢？发送ARP获取网关的MAC地址，然后发送包。包的内容是这样的：
源MAC：服务器A的MAC
目标MAC：192.168.1.1这个网口的MAC
源IP：192.168.1.101
目标IP：192.168.4.101
包到达192.168.1.1这个网口，发现MAC一致，将包收进来，开始思考往哪里转发。
在路由器A中配置了静态路由之后，要想访问192.168.4.0/24，要从192.168.56.1这个口出去，下一跳为192.168.56.2。
于是，路由器A思考的时候，匹配上了这条路由，要从192.168.56.1这个口发出去，发给192.168.56.2，那192.168.56.2的MAC地址是多少呢？路由器A发送ARP获取192.168.56.2的MAC地址，然后发送包。包的内容是这样的：
源MAC：192.168.56.1的MAC地址
目标MAC：192.168.56.2的MAC地址
源IP：192.168.1.101
目标IP：192.168.4.101
包到达192.168.56.2这个网口，发现MAC一致，将包收进来，开始思考往哪里转发。
在路由器B中配置了静态路由，要想访问192.168.4.0/24，要从192.168.4.1这个口出去，没有下一跳了。因为我右手这个网卡，就是这个网段的，我是最后一跳了。
于是，路由器B思考的时候，匹配上了这条路由，要从192.168.4.1这个口发出去，发给192.168.4.101。那192.168.4.101的MAC地址是多少呢？路由器B发送ARP获取192.168.4.101的MAC地址，然后发送包。包的内容是这样的：
源MAC：192.168.4.1的MAC地址
目标MAC：192.168.4.101的MAC地址
源IP：192.168.1.101
目标IP：192.168.4.101</description></item><item><title>第9讲_路由协议：西出网关无故人，敢问路在何方</title><link>https://artisanbox.github.io/5/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/41/</guid><description>俗话说得好，在家千日好，出门一日难。网络包一旦出了网关，就像玄奘西行一样踏上了江湖漂泊的路。
上一节我们描述的是一个相对简单的情形。出了网关之后，只有一条路可以走。但是，网络世界复杂得多，一旦出了网关，会面临着很多路由器，有很多条道路可以选。如何选择一个更快速的道路求取真经呢？这里面还有很多门道可以讲。
如何配置路由？通过上一节的内容，你应该已经知道，路由器就是一台网络设备，它有多张网卡。当一个入口的网络包送到路由器时，它会根据一个本地的转发信息库，来决定如何正确地转发流量。这个转发信息库通常被称为路由表。
一张路由表中会有多条路由规则。每一条规则至少包含这三项信息。
目的网络：这个包想去哪儿？
出口设备：将包从哪个口扔出去？
下一跳网关：下一个路由器的地址。
通过route命令和ip route命令都可以进行查询或者配置。
例如，我们设置ip route add 10.176.48.0/20 via 10.173.32.1 dev eth0，就说明要去10.176.48.0/20这个目标网络，要从eth0端口出去，经过10.173.32.1。
上一节的例子中，网关上的路由策略就是按照这三项配置信息进行配置的。这种配置方式的一个核心思想是：根据目的IP地址来配置路由。
如何配置策略路由？当然，在真实的复杂的网络环境中，除了可以根据目的ip地址配置路由外，还可以根据多个参数来配置路由，这就称为策略路由。
可以配置多个路由表，可以根据源IP地址、入口设备、TOS等选择路由表，然后在路由表中查找路由。这样可以使得来自不同来源的包走不同的路由。
例如，我们设置：
ip rule add from 192.168.1.0/24 table 10 ip rule add from 192.168.2.0/24 table 20 表示从192.168.1.10/24这个网段来的，使用table 10中的路由表，而从192.168.2.0/24网段来的，使用table20的路由表。
在一条路由规则中，也可以走多条路径。例如，在下面的路由规则中：
ip route add default scope global nexthop via 100.100.100.1 weight 1 nexthop via 200.200.200.1 weight 2 下一跳有两个地方，分别是100.100.100.1和200.200.200.1，权重分别为1比2。
在什么情况下会用到如此复杂的配置呢？我来举一个现实中的例子。
我是房东，家里从运营商那儿拉了两根网线。这两根网线分别属于两个运行商。一个带宽大一些，一个带宽小一些。这个时候，我就不能买普通的家用路由器了，得买个高级点的，可以接两个外网的。
家里的网络呢，就是普通的家用网段192.168.1.x/24。家里有两个租户，分别把线连到路由器上。IP地址为192.168.1.101/24和192.168.1.102/24，网关都是192.168.1.1/24，网关在路由器上。
就像上一节说的一样，家里的网段是私有网段，出去的包需要NAT成公网的IP地址，因而路由器是一个NAT路由器。
两个运营商都要为这个网关配置一个公网的IP地址。如果你去查看你们家路由器里的网段，基本就是我图中画的样子。
运行商里面也有一个IP地址，在运营商网络里面的网关。不同的运营商方法不一样，有的是/32的，也即一个一对一连接。
例如，运营商1给路由器分配的地址是183.134.189.34/32，而运营商网络里面的网关是183.134.188.1/32。有的是/30的，也就是分了一个特别小的网段。运营商2给路由器分配的地址是60.190.27.190/30，运营商网络里面的网关是60.190.27.189/30。</description></item><item><title>结束语_放弃完美主义，执行力就是限时限量认真完成</title><link>https://artisanbox.github.io/5/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/45/</guid><description>你好，我是刘超。
从筹备、上线到今天专栏完结，过去了将近半年的时间。200多天，弹指一挥间。
我原本计划写36篇，最后愣是写到了45篇。原本编辑让我一篇写两三千字，结果几乎每篇都是四五千字。这里面涉及图片张数我没具体数过，但是据说多到让编辑上传到吐。编辑一篇我的稿件的工作量相当于别的专栏的两倍。
人常说，有多少付出，就有多少回报。但是，写这个“趣谈网络协议”专栏，我收获的东西远超过我的想象。我希望你的收获也是如此。为什么这么说呢？我们把时间放回到这个专栏最开始的时候，我慢慢跟你讲。
我不是最懂的人，但我想尝试成为这样的人今年年初，极客时间来找我，希望我讲一些偏重基础的知识，比如网络协议。
他们一提到这个主题，我就很兴奋，因为这也触动了我心中长期以来的想法，因为网络这个东西学起来实在是太痛苦。
但是，说实话，接下这个重任，我心里其实是有点“怕”的。我怕自己不够专业，毕竟业内有这么多网络工程师和研究网络理论的教授。我讲这个课会不会贻笑大方啊？
我知道，很多技术人员不敢写博客、写公众号，其实都有这种“怕”的心理：我又不牛，没啥要分享的，要是误导了别人怎么办？
如果你想做出一些成绩，这个心理一定要克服。其实每个人都有自己的相对优势。对于某个东西，你研究的时间不一定是最长的，但是你可能有特殊的角度、表达方式和应用场景。坚定了这个想法之后，我就开始投入热火朝天的专栏写作了。
一旦开始写，我发现，这个事情远没有看上去那么简单。它会花费你非常多的个人时间。写专栏这几个月，晚上两点之后睡，周末全在写专栏，基本成为我的生活常态。但是我想挑战一下自己，我觉得，只要咬牙挺过去，自己的技术就会上升一个层次。
放弃完美主义，执行力就是限时限量认真完成技术人都有完美主义倾向，觉得什么事情都要钻研个底儿朝天，才拿出来见人。我也一样。
我曾经答应某出版社写一本搜索引擎的书。这本书分为原理篇和实践篇。我总觉得我还没把原理篇写完，就不能写实践篇。但是，仅原理篇我就写了一年。搜索引擎就火了一两年，最后时间窗口过了，书稿没有完成，这件事儿也就这么搁浅了。
所以，完美主义虽然是个很好听的词，但是它往往是和拖延症如影随形的，它常常会给拖延症披上一个华丽的外衣，说，我是因为追求完美嘛。但是，最终的结果往往是，理论研究半天还没动手，执行力很差。时间点过了，就心安理得地说，反正现在也不需要了，那就算了吧。久而久之，你就会发现，自己好像陷入了瓶颈。
我慢慢明白过来，我们不是为了做技术而做技术，做技术是为了满足人类需求的。完美主义是好事儿，但是，坚持完美主义的同时要限时限量地完成，才能形成执行力。
写这个专栏之后，我更加深刻地体会到这一点。每周都要写三篇文章，压力很大，根本容不得任何拖延。如果我还是坚持以前完美主义的做法，读完十本书，用三年时间把网络协议都研究透再来写，那现在就没有这个专栏了。
如果我们要强调执行力，时间点这个因素就至关重要。在固定的时间点上，就要把控范围，不能顾虑太多，要勇于放弃。就像给产品做排期，先做最小闭环的功能集合，其他的放在以后再补充。在这个前提下，以自己最大的限度往完美的方向上努力。比如，我觉得每天2点睡是我的身体极限，努力到这个程度，我也就无愧于心了。
所以说，我们做事情的目的并不是完美，而是在固定的时间点，以固定的数量和质量，尽可能认真地满足当时的客户需求，这才是最重要的。
这样做肯定会有不满意的地方，比如很多同学在留言区指出我的错误，甚至有的同学提的问题，我原来都没思考过。但是，我觉得这些都不是事儿。我可以再查资料，再补充、再完善。所以，后来时间宽裕了，我还增加了5期答疑，回答了一下之前没来得及回答地问题。高手在民间，咱们一起来讨论和进步。这个过程已经让我受益良多。
保持饥渴，不怕被“鄙视”，勇于脱离舒适区有人可能会问了，你看你既不是最专业的，还不追求完美，真的不怕被人“鄙视”吗？
被“鄙视”，谁都怕，这也是为什么越大的会议，参加人数越多的演讲，越是没有人提出具体的问题。大家都怕丢人，看上去好像大家都听懂了，就我啥都不懂，我要是问，被大家笑话怎么办？我想很多人都有这样的经历吧？我也来给你讲讲我的亲身经历。
我从Windows开发去做Linux的存储系统开发时，连Linux man都不会看；我在惠普从事OpenStack实施工作的时候，对于网络的了解一塌糊涂，一直被甲方骂；我在华为做云计算，支撑运营商项目的时候，面对一大堆核心网词汇，一脸懵；我在网易云对内支撑考拉的时候，在微服务架构方面也是小白……被“鄙视”了这么多次之后，我不怕了。因为这每一次“鄙视”都可以让我发现自己的短板，然后啃下这些东西，这不就是最大的收获吗？
我就是这样一直被“鄙视”着成长起来的人，我就是常常在别人分享的时候坐第一排问很傻的那种问题的人，我就是常常一知半解还愿意和别人讨论的人……
怕被“鄙视”，说明你还不够饥渴，还没有勇气脱离你的舒适区。 在你熟悉的领域里面，你是最最权威的，但是，天下之大，你真的只满足于眼前这一亩三分地吗？
很多人因为怕被“鄙视”，不敢问、不敢做，因而与很多美好的东西都擦肩而过了。直到有一天你用到了，你才后悔，当时自己怎么没去多问一句。
所以，当你看到一个特别好的、突破自己的学习机会，别犹豫，搭上这辆车。等过了十年，你会发现，当年那些嘲笑、轻视，甚至谩骂，都算不了什么，进步本身才是最最重要的。
今天，咱们没有谈具体的知识，我只表达了一下我的观点。我就是那个你在直播里看到的，那个邋遢、搞笑、不装，同时做事认真，愿意和你一起进步的技术大叔。
脱离舒适区吧，希望我们可以一起成长！
最后，我在这里放了一个毕业调查问卷。如果你对这个专栏或者我本人有什么建议，可以通过这个问卷进行反馈，我一定会认真查看每一封的内容。期待你的反馈！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>结束语_点线网面，一起构建MySQL知识网络</title><link>https://artisanbox.github.io/1/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/46/</guid><description>时光流逝，这是专栏的最后一篇文章。回顾整个过程，如果用一个词来描述，就是“没料到”：
我没料到文章这么难写，似乎每一篇文章都要用尽所学；
我没料到评论这么精彩，以致于我花在评论区的时间并不比正文少；
我没料到收获这么大，每一次被评论区的提问问到盲点，都会带着久违的兴奋去分析代码。
如果让我自己评价这个专栏：
我最满意的部分，是每一篇文章都带上了实践案例，也尽量讲清楚了原理；
我最得意的段落，是在讲事务隔离级别的时候，把文章重写到第三遍，终于能够写上“到这里，我们把一致性读、当前读和行锁就串起来了”；
我最开心的时候，是看到评论区有同学在回答课后思考题时，准确地用上了之前文章介绍的知识点。因为我理解的构建知识网络，就是这么从点到线，从线到网，从网到面的过程，很欣喜能跟大家一起走过这个过程。
当然，我更看重的还是你的评价。所以，当我看到你们在评论区和知乎说“好”的时候，就只会更细致地设计文章内容和课后思考题。
同时，我知道专栏的订阅用户中，有刚刚接触MySQL的新人，也有使用MySQL多年的同学。所以，我始终都在告诫自己，要尽量让大家都能有所收获。
在我的理解里，介绍数据库的文章需要有操作性，每一个操作有相应的原理，每一个原理背后又有它的原理，这是一个链条。能够讲清楚链条中的一个环节，就可能是一篇好文章。但是，每一层都有不同的受众。所以，我给这45篇文章定的目标就是：讲清楚操作和第一层的原理，并适当触及第二层原理。希望这样的设计不会让你觉得太浅。
有同学在问MySQL的学习路径，我在这里就和你谈谈我的理解。
1. 路径千万条，实践第一条如果你问一个DBA“理解得最深刻的知识点”，他很可能告诉你是他踩得最深的那个坑。由此，“实践”的重要性可见一斑。
以前我带新人的时候，第一步就是要求他们手动搭建一套主备复制结构。并且，平时碰到问题的时候，我要求要动手复现。
从专栏评论区的留言可以看出来，有不少同学在跟着专栏中的案例做实验，我觉得这是个非常好的习惯，希望你能继续坚持下去。在阅读其他技术文章、图书的时候，也是同样的道理。如果你觉得自己理解了一个知识点，也一定要尝试设计一个例子来验证它。
同时，在设计案例的时候，我建议你也设计一个对照的反例，从而达到知识融汇贯通的目的。就像我在写这个专栏的过程中，就感觉自己也涨了不少知识，主要就得益于给文章设计案例的过程。
2. 原理说不清，双手白费劲不论是先实践再搞清楚原理去解释，还是先明白原理再通过实践去验证，都不失为一种好的学习方法，因人而异。但是，怎么证明自己是不是真的把原理弄清楚了呢？答案是说出来、写出来。
如果有人请教你某个知识点，那真是太好了，一定要跟他讲明白。不要觉得这是在浪费时间。因为这样做，一来可以帮你验证自己确实搞懂了这个知识点；二来可以提升自己的技术表达能力，毕竟你终究要面临和这样的三类人讲清楚原理的情况，即：老板、晋升答辩的评委、新工作的面试官。
我在带新人的时候，如果这一届的新人不止一个，就会让他们组成学习小组，并定期给他们出一个已经有确定答案的问题。大家分头去研究，之后在小组内进行讨论。如果你能碰到愿意跟你结成学习小组的同学，一定要好好珍惜。
而“写出来”又是一个更高的境界。因为，你在写的过程中，就会发现这个“明白”很可能只是一个假象。所以，在专栏下面写下自己对本章知识点的理解，也是一个不错的夯实学习成果的方法。
3. 知识没体系，转身就忘记把知识点“写下来”，还有一个好处，就是你会发现这个知识点的关联知识点。深究下去，点就连成线，然后再跟别的线找交叉。
比如，我们专栏里面讲到对临时表的操作不记录日志，然后你就可以给自己一个问题，这会不会导致备库同步出错？再比如，了解了临时表在不同的binlog格式下的行为，再追问一句，如果创建表的时候是statement格式，之后再修改为row格式（或者反之），会怎么样呢？
把这些都搞明白以后，你就能够把临时表、日志格式、同步机制，甚至于事务机制都连起来了。
相信你和我一样，在学习过程中最喜欢的就是这种交叉的瞬间。交叉多了，就形成了网络。而有了网络以后，吸收新知识的速度就很快了。
比如，如果你对事务隔离级别弄得很清楚了，在看到第45篇文章讲的max_trx_id超限会导致持续脏读的时候，相信你理解起来就很容易了。
4. 手册补全面，案例扫盲点有同学还问我，要不要一开始就看手册？我的建议是不要。看手册的时机，应该是你的知识网络构建得差不多的时候。
那你可能会问，什么时候算是差不多呢？其实，这没有一个固定的标准。但是，有一些基本实践可以帮你去做一个检验。
能否解释清楚错误日志（error log）、慢查询日志（slow log）中每一行的意思？ 能否快速评估出一个表结构或者一条SQL语句，设计得是否合理？ 能否通过explain的结果，来“脑补”整个执行过程（我们已经在专栏中练习几次了）？ 到网络上找MySQL的实践建议，对于每一条做一次分析： 如果觉得不合理，能否给出自己的意见？ 如果觉得合理，能否给出自己的解释？ 那，怎么判断自己的意见或者解释对不对呢？最快速、有效的途径，就是找有经验的人讨论。比如说，留言到我们专栏的相关文章的评论区，就是一个可行的方法。
这些实践做完后，你就应该对自己比较有信心了。这时候，你可以再去看手册，把知识网络中的盲点补全，进而形成面。而补全的方法就是前两点了，理论加实践。
我希望这45篇文章，可以在你构建MySQL知识体系的过程中，起到一个加速器的作用。
我特意安排在最后一篇文章，和你介绍MySQL里各种自增id达到定义的上限以后的不同行为。“45”就是我们这个专栏的id上限，而这一篇结束语，便是超过上限后的第一个值。这是一个未定义的值，由你来定义：
有的同学可能会像表定义的自增id一样，就让它定格在这里； 有的同学可能会像row_id一样，二刷，然后用新的、更全面的理解去替代之前的理解； 也许最多的情况是会像thread_id一样，将已经彻底掌握的文章标记起来，专门刷那些之前看过、但是已经印象模糊的文章。 不论是哪一种策略，只要这45篇文章中，有那么几个知识点，像Xid或者InnoDB trx_id一样，持久化到了你的知识网络里，你和我在这里花费的时间，就是“极客”的时间，就值了。
这是专栏的最后一篇文章的最后一句话，江湖再见。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>结束语_知也无涯，愿你也享受发现的乐趣</title><link>https://artisanbox.github.io/4/58/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/4/58/</guid><description>你好，我是徐文浩。伴随着无数个不眠之夜，“深入浅出计算机组成原理”专栏终于来到了结束语。
去年11月份，极客时间找到我，我开始构思这个专栏。本以为今年4、5月份就能把专栏写完。结果，一方面因为创业过程中时间总是不够用，另一方面，写出有价值内容的并不是一件容易的事情，直到9月10号的凌晨，我才写完这最后一篇结束语。原本计划的45讲，也在这个过程中变成了近60讲。现在回过去看，写这个“深入浅出计算机组成原理”专栏，是一个远比想象中要困难的挑战，但同时也是一个有趣的发现之旅。
完成比完美更好Facebook的文化里面喜欢用各种小标语，其中有一条我很喜欢：“Done is better than perfect”。翻译成中文就是，“完成比完美更好”。写这个专栏的时候，我对这一点的体会特别深刻。在学习更多深入知识的时候，我希望你也可以抱有这样的态度。
在初期构思专栏的时候，我期望写成一个完美的专栏。不过随着时间的推移，我发现其实并没有什么完美可言。
一方面，组成原理的知识点很多，如果每一个都写下来，没有个一两百讲怕是讲不完。更何况有那么多大师的教科书珠玉在前，只是做解读知识点、覆盖已有的知识点，我觉得价值不大。思来想去，我希望尽可能找到最重要、最核心的知识点，以及能和大多数工程师日常工作有结合的知识点，希望能够从应用中多给你一些启发。
另一方面，写专栏和我们写程序一样，都是有deadline的。无论是在系统发版之后的午夜里，还是去美国出差的飞机上，乃至偶尔忘带了录音笔的时候，总是要打起精神想尽方法，写出一篇让自己满意的文章来。同时，也有不少同学给我挑出了错漏或者不准确的部分，一起把这个专栏打磨地更“完美”。
不知道正在读结束语的你，有没有在过去5个月里坚持学习这个专栏呢？有没有认真阅读我每一节后的推荐阅读呢？有没有尝试去做一做每一讲后面的思考题呢？
如果你能够坚持下来，那首先要恭喜你，我相信能够学完的同学并不太多。如果你还没有学完，也不要紧，先跟着整个课程走一遍，有个大致印象。与其半途而费，不如先囫囵吞枣，硬着头皮看完再说。新的知识第一遍没有百分百看懂，而随着时间的推移，慢慢领悟成长了，这才是人生的常态。而我所见到的优秀的工程师大都会经历这样的成长过程。
我们这个行业，经常喜欢把软件开发和建筑放在一起类比，所以才会有经典的《设计模式》这样的书。甚至有不少人干脆从《建筑的永恒之道》里面去寻找灵感。然而，建筑能够在历史上留下长久的刻印，但是软件却完全不同。无论多么完美的代码都会不断迭代，就好像新陈代谢一样。几年过去之后，最初那些代码的踪影早已经没有了。软件工程师放弃了追求永恒，而是投身在创作的快乐之中。
希望在日后的学习过程中，你也能抱着“日拱一卒、不期速成”的心态坚持下去，不断地学习、反思、练习、再学习，这样的迭代才是最快的成长之路。
知也无涯，愿你享受发现的乐趣说实话，从构思到写作这个专栏，这整个过程对我来说，还是有些忐忑的。组成原理是一门离大部分工程师的日常工作比较远的话题，却又是一个很多经典教材会讲的主题。“到底从什么角度去切入讲解”，我在构思文章的时候常常问自己。
组成原理其实是一门类似于“计算机科学101”的课程，固然我可以在里面讲VHDL这样的硬件编程语言，不过说实话，这样的知识对于大部分的人意义并不大。我期望，能够通过这个专栏，让你体会到计算机科学知识是真的有用的，能够让你把学专栏的过程变成一个发现之旅。
比如，在学习HDD硬盘原理的时候，你能知道为什么用它来记录日志很好，但是拿来作为KV数据库就很糟糕；在学习CPU Cache的时候，你实际用代码体会一下它有多快，为什么Disruptor里面的缓存行填充这样的小技巧，能够把性能发挥到极致。
除此之外，撰写整个专栏的过程，也是我对自己的一个发现之旅。
虽然在过去开发大型系统的时候，已经体会到掌握各种计算机科学基础知识的重要性，但是，这个专栏还是给了我一个系统性地、对基础知识回顾和整理的机会，在忙碌的日常工作之外，在离开学校那么多年后，重新把基础的理论知识和实际的系统开发做了一一印证。
在这个过程中，对我自己是一个温故而知新的过程，我自己新学到不少过去不了解的知识点，也因此重新找到了很多新的技术兴奋点。乃至在专栏写了一半的时候，我特地在出差的空隙跑了一趟计算机历史博物馆，去感受创造新事物的那种激动人心的感觉。
不过，在这整个过程中，我也深深体会到了内容创作的难。
过去这10个月里，持续地写稿、画图、写实验程序，在编辑的反馈下再改稿和录音，对我也是一个全新的体验。没有思路、时间不够、工作和写稿压力太大的时候，抓狂、发脾气、骂人都发生过。如果没有编辑在背后一直督促着，只靠自律，我想我无论如何也不可能写完这样一个规模的专栏。
但是，我相信只有不断地逼迫自己走出习惯的舒适区，去尝试、体验新的挑战，才会进一步的成长。而很多未来的机会，也孕育在其间。就像史蒂夫·乔布斯说的，我们未来生活的可能性就是靠这些点点滴滴串联起来的。
也许你今天只是在学校写简单的课程管理系统，可能会觉得有些无聊。抽一些时间出来，去了解计算机科学的底层知识，可能会让你找到求知的乐趣，无形中，这也为你去解决更有挑战的问题做好了铺垫。就像我自己在过去研究底层的数据系统、写技术博客的时候，也没有想到会有机会写上这样一个20万字以上的专栏。
就像罗素说的那样，“对爱的渴望，对知识的追求，对人类苦难不可遏制的同情，是支配我一生的单纯而强烈的三种情感”。
我希望，在学习成长的过程中，你能够摆脱一些功利性，不用去回避遇到的痛苦和挫败感，多从这个过程中找到获得知识的快乐。
希望这个专栏能够给你带来发现的乐趣，也能够为你在未来的生活里铺垫上那小小的一步。相信这个专栏不是你学习的终点，也也不是我探索和发现新主题的终点。说不定，在不久的未来我们还会有缘再见。
对了，我在文章末尾放了一个毕业调查问卷。在这5个月的学习过程中，如果你对这个专栏或者我本人有什么建议，可以通过这个问卷给我反馈，我一定会认真查看每一封的内容。期待你的反馈！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>结束语_送君千里，终须一别</title><link>https://artisanbox.github.io/2/76/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/76/</guid><description>专栏到今天真的要结束了。在写这篇结束语的时候，我的心情还是蛮复杂的，既有点如释重负，又有点不舍。如释重负，是因为我自己对专栏的整体质量非常满意；不舍，是因为我还想分享更多“压箱底”的东西给你。
专栏是在2018年9月发布的。在发布后的两三天时间里，就有2万多人订阅，同时也引来了很多争议。有人说，我就是随便拿个目录就来“割韭菜”。也有人说，数据结构和算法的书籍那么多，国外还有那么多动画、视频教程，为什么要来学我的专栏？
这些质疑我都非常理解，毕竟大部分基础学科的教材，的确是国外的更全面。实际上，在专栏构思初期，我就意识到了这一点。不夸张地讲，我几乎读过市面上所有有关数据结构和算法的书籍，所以，我也深知市面上的数据结构和算法书籍存在的问题。
尽管有很多书籍讲得通俗易懂，也有很多书籍全面、经典，但是大部分都偏理论，书中的例子也大多脱离真实的软件开发。这些书籍毫无疑问是有用的，但是看完书之后，很多人只是死记硬背了一些知识点而已。这样填鸭式的学习，对于锻炼思维、开拓眼界并没有太多作用。而且，从基础理论到应用实践，有一个非常大的鸿沟要跨越，这是大学教育的普遍不足之处，这也是为什么我们常常觉得大学里学过的很多知识都没用。
我本人是一个追求完美、极致的人，凡事都想做到最好，都想争第一。所以，就我个人而言，我也不允许自己写一个“太普通”“烂大街”的专栏。那时我就给自己立了一个flag：我一定要写一个跟所有国内、国外经典书籍都不一样的专栏，写出一个可以长期影响一些人的专栏。
所以，在这个专栏写作过程中，我力争并非只是单纯地把某个知识点讲清楚，而是结合自己的理解、实践和经验来讲解。我写每篇文章的时候，几乎都是从由来讲起，做到让你知其然、知其所以然，并且列举大量的实际软件开发中的场景，给你展示如何利用数据结构和算法解决真实的问题。
除此之外，课后思考题我也不拿一些现成的LeetCode的题目来应付。这些题目都是我精心设计的、贴合具体实践、非常考验逻辑思维的问题。毫不夸张地讲，只把这些课后思考题做个解答，就可以写成一个有价值、有干货的专栏！
专栏到今天就要结束了。尽管有些内容稍有瑕疵，但我觉得我实现了最初给自己立下的flag。那你又学得怎么样呢？
如果这是你第一次接触数据结构和算法，只是跟着学一遍，你可能不会完全理解所有的内容。关于这个专栏，我从来也不想标榜，我的专栏是易懂到地铁里听听就可以的。因为你要知道，没有难度的学习，也就没有收获。所以，作为初学者，你要想真的拿下数据结构和算法，时间允许的话，建议你再二刷、三刷。
如果你是有一定基础的小伙伴，希望你能够真的做到学以致用。在开发项目、阅读开源代码、理解中间件架构设计方面，多结合数据结构和算法，从本质上理解原理，掌握创新的源头。
如果你是数据结构和算法高手，那我的专栏应该也没有让你失望吧？我个人觉得，专栏里还是有很多可以给你惊喜的地方。对于你来说，哪怕只学到了一个之前没有接触的知识点，我觉得其实已经值得了。
送君千里终须一别。数据结构和算法的学习，我暂时只能陪你到这里了。感谢你订阅我的专栏，感谢这5个月的同行，真心希望我的专栏能对你有所帮助。
我知道，很多小伙伴都是“潜水党”，喜欢默默地学习，在专栏要结束的今天，我希望能听到你的声音，希望听听你学习这个专栏的感受和收获。最后，再次感谢！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 .</description></item><item><title>结束语｜等待你大展身手的那些领域</title><link>https://artisanbox.github.io/3/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/45/</guid><description>你好，我是宫文学。
到今天为止，我们这门课的主要内容就都更新完了。不过，还有一些补充性的内容，我会通过加餐和开源项目的方式，继续和你保持沟通。
今天的结束语，我想跟你探讨一下，学习实现一门语言的相关技术，到底会有什么用途。
我会分成领域编程语言、平台级的软件和通用编程语言这三个话题，分析一下 编程语言技术能帮助你抓住哪些机会，让你有机会从普通的程序员进阶成大神级的程序员，并创造出一些卓越的产品。
首先，我们来谈谈领域编程语言这个话题。
领域编程语言（DSL）对于我们大部分同学来说，其实很难有机会，或者也没有这个意愿，去参与实现一门通用性编程语言。不过，其实在大部分情况下，我们也没有必要追求那么大的目标。有时候，针对我们所在的领域，实现一门领域编程语言，就是很有意义、很有成就感的事情。
我举几个我遇到的DSL的例子，看看能否抛转引玉，让你找到更多可以设计和使用DSL的场景。
MiniZinc：最优化领域的开发工具在2020年的12月，我曾经研究了一下最优化算法相关的技术和工具，看看它能否用于我们的一个产品。
很多同学在大学都学过最优化相关的理论，像线性规划、非线性规划这些，都属于这个领域。你也可能听说过运筹学，它们的意思差不多。最优化理论在实践中有很多用途。比如，我要解决一个应用问题，就是在某个领域，有很多员工，也有很多任务要完成。每个员工的技能是不同的，我需要通过算法来安排这些员工的工作，取得整体最优的效果。
为了实现最优化求解，有人开发了各种求解器，有商业的，也有开源的。但对于我一个新手来说，我一开始并不知道要用哪个工具，有点茫然。
通过某些途径，我了解到了MiniZinc这个工具。这个工具提供了一种DSL，能够描述各种最优化问题，然后调用各种不同的求解器来求解。比如在下图，你能看到菜单栏有一个下拉菜单，里面有多个求解器。
MiniZinc这个工具一下子解决了我的两个需求。首先，这个DSL很友好、很直观。你完全可以按照最优化的理论，描述一个问题的变量、参数、约束条件，然后就可以求解了，非常方便。第二，我暂时也不用关心不同的求解器的差别，可以随便选一个先用着，或者换着用不同的求解器，看看它们在性能和求解结果有哪些差异。
所以我很快就用MiniZinc编写了几个小程序来验证我的想法，并在较短的时间内取得了一些成果。
在使用这个工具的时候，我就在想，上过我这门课的同学，有没有能力做这么一个工具呢？我们来分析一下。
其实，要实现MiniZinc，主要的工作就是实现一个编译器的前端，做词法分析、语法分析和语义分析工作。这个DSL的语法和语义都不是很复杂，所以工作量并不大。
做完前端的工作以后，程序就可以基于AST来解释执行了。解释执行的过程，其实就是调用各个求解器的API，并把结果显示到界面上。
这么一个小工具，会给那些最优化领域的工作者和科研人员带来很大的便利。因为他们通常专注于研究算法，对于通用的计算机编程并不是很熟练。
那在你的领域中，是不是也有这样的情况呢？你是IT专业的人员，而你同事可能是其他专业的专家，比如是工程专家、投资专家、财务专家等等。你能否针对他们的领域，设计出一些DSL，并提供一个开发工具来解决他们的一些痛点问题呢？
我们再看看第二个例子，这个例子是遥感领域的一个二次开发平台。
遥感领域的二次开发平台我硕士的专业是在遥感和GIS领域，我有一个硕士同学在这个领域做出了一个上市公司。他们有一个产品，是一个遥感云平台，也就是把全国各地的很多遥感资料都放到云上管理。这个云平台里有一个开发工具，能够让用户在浏览器里编写程序，调用云平台的API，实现遥感数据分析等功能。
据说这个平台是跟Google Earth对标的，国外很多科学家都会在Google Earth平台上写程序处理遥感数据，并构建自己领域的应用。
如果让你去实现这个开发工具，你会怎么做呢？
首先，你肯定需要一个基于Web的代码编辑器，这方面有好几个开源工具，所以这点并不是难题。
接下来，你仍然要实现编译器前端的工作。这一次，你需要编译的是JavaScript语言，它的语法特性和语义特性都比较多，所以实现的工作量要大一些。你可以把我们现在的词法分析器和语法分析器改一改，来实现JavaScript的解析。如果你想偷懒，还可以直接用antlr和现成的语法规则生成一下解析器。不过，无论如何，语义分析的工作是省不了的。你需要建立符号表、进行符号的消解，但不需要像我们这门课这样做那么多的类型处理。
完成编译器前端以后，还要做些什么呢？我们还要做一些中端的优化工作。因为这个开发工具要调用与遥感有关的API。而如何调用这些API才会让效率最高呢？所以这一点上，我们实际上是要做一些优化的。
做完中端优化后，后续的编译和运行过程，有可能只要交给一个成熟的JavaScript引擎就可以了。
怎么样？使用我们这门课上学过的技术，你可以把很多科研工作者、各个行业的应用开发人员，都聚集到一个平台上，充分释放海量遥感数据的价值，这是不是一件挺酷的事情？
接下来，我再举一个CAD领域的例子，这个产品是OpenCAD。
OpenCAD几个月前，我一时兴起买了一台3D打印机，想自己打印点好玩的东西。但是在打印之前，还需要建立3D模型，所以我就搜了搜建模工具，发现了一个叫做OpenCAD的软件。
这个软件提供了一个编程界面。你可以在这个界面中，通过编程来创建长方体、圆柱体这样的三维对象，也可以通过编程来控制它们的位置、旋转的角度。
另外，你还可以创建模块，把多个基础的对象拼成复杂的对象，比如把一些长方体、圆柱体拼成一辆车。之后，这辆车就可以作为一个整体，用来构架更复杂的场景。
并且，模块还可以带参数，就像一个函数或者一个类那样。你通过调整模块的参数，就可以调整生成的3D对象。比如同样是一辆车，你可以通过调整参数生成一辆很大的车，也可以生成一辆玩具车。
我并不熟悉CAD领域，所以很难把这个软件跟其他CAD软件做客观地对比。不过，我能够看出，OpenCAD通过编程来建模的形式，有几个特别的优势：
第一个优势是精准。在建立某些机械模型的时候，零件的大小、位置等信息必须是精准的。而使用编程语言，你可以用数学公式做出各种精准地计算。比如，上面的例子就使用了三角函数来精确的绘制曲线。
第二个优势是可重用性。可重用性是编程语言的基本特征，模块、函数、类，都是可重用的元素。并且，这种可重用的元素都是参数化的。在不同的场景中，可以通过调整参数来获得想要的功能。
所以，要开发一款优秀的CAD软件，我们需要充分吸收编程语言的技术。类似的领域还有建筑建模软件（BIM）、城市建模软件（CIM）等，现在很多公司的产品，都声称提供了低代码的数字城市平台，那这些产品也应该充分使用计算机语言的技术，并形成特定领域的DSL。
你看，我们现在分析了三个案例，看到了三个领域对计算机语言的要求。其实，这样的领域还有很多，并且它们都可以受益于领域编程语言。我希望你能受到这些例子的启发，看看在自己的领域内还有没有这样的需求，说不定你会也能做出一些开创性的事情。因为这样级别的工作，必须在掌握了我们这门课的知识体系以后，你才有能力驾驭。
上面这些采用了DSL的软件，基本都属于一些平台级的软件。那我们再围绕如何实现平台级的软件这个话题，展开讨论一下编程语言技术的作用，和你当前面对的机会。
实现平台级的软件Lisp语言的重要推广者、《黑客与画家》的作者、创业孵化器Y Combinator的创始人保罗·格雷厄姆曾经说过一段话，大概就是说，每个软件演化到最后，都会内置一个Lisp语言的实现。
他的这段话，其实是说，任何软件，如果想覆盖尽量多的应用场景，都需要提供一定的编程能力。我举几个例子来说明一下这个观点：
为了更高效的管理很多服务器上的操作系统，搞运维的技术人员都会用Python来写脚本。这个时候，我们是在用脚本语言来扩展操作系统的功能； 数据库系统之所以能够满足各种应用的需求，是因为它是通过SQL语言来访问数据库的功能； 微软的各种应用产品，几乎都提供了二次开发的能力，这让你可以基于微软的产品形成各种不同领域的解决方案。比如，如果你能够熟练使用Excel里面的宏和编程技术，你可以完成很多的数据分析需求； 三维游戏引擎提供编程功能，让你能够创建各种三维游戏场景； 各种报表或BI工具，都提供了定义数据源、定义报表公式等功能，方便你设计各种报表； 工作流或BPM系统，需要提供流程设计、公式定义和自定义逻辑的功能，来满足各种不同的流程场景了； 对于一个API网关来说，需要提供一定的编程逻辑，来定义在什么情况下，把API访问路由到哪个微服务，或者进行熔断。 类似的例子还有很多。
如果你的软件只是为某个用户个性化定制的，只需要满足这一个客户的需求，那么你只要弄清楚需求，然后实现出来就行了。但如果你想让更多的用户使用你的软件，那该怎么办呢？
在少量情况下，你可以设计一套标准的软件，并让所有的用户都满意。比如，几乎所有的字处理软件的功能都差不多，你只需要购买一个License就行了，很少会提出个性化的需求。
但这样的通用软件是很少的。更多的情况下，特别是在企业应用领域，我们都需要对软件的标准功能做一定的调整，让它符合某个客户特定的需求。比如，你可能需要调整某些业务规则、某些流程、某些数据项，等等。
而国内大部分软件公司，目前都是通过修改源代码来满足这些个性化的需求。这就导致软件的实施成本很高，版本难以维护，这是很多应用软件开发商所处的困境。你在的公司，也可能存在这样的困境。
这个时候，如果你能把应用软件上升为平台，也就是在不修改原来的源代码的基础上，提供二次开发的能力，二次开发的部分由各个客户自行维护，才可以从根本上打破这种困境。从这个角度看，我们每个同学所在的领域，都存在着大量的潜在机会，采用这门课学习到的编程语言技术，你可以把你所在领域的软件，提升成一个平台级的软件。
还有最后一种情况，就是你本来就要开发平台级的软件，比如数据库系统、表单系统、报表系统、游戏引擎等，那么编程能力就更加是缺省的要求。
不管怎样，只要你想让你的软件变成平台级的软件，具备适应各种不同应用场景的能力，具备扩展功能的能力，你就需要采用这门课教给你的编程语言技术，对这些软件进行改造。在我看来，国内有太多软件产品需要进行这种提升了，这都是你可以施展身手的机会。
好了，聊完了领域编程语言和平台级软件以后，你还有没有其他机会来大显身手呢？有的，这就是实现通用编程语言这个终极大Boss。
通用编程语言像Java、C、Go这些语言，能够用于很多领域，所以它们被叫做通用编程语言。到目前为止，我们国家还没有正式发布的、被广泛接受的通用编程语言。
不过信息灵通的同学可能也知道，我们多个大厂，其实都在内部酝酿和研发这样的语言。我本人也在参与某门语言的内测和评价工作。鉴于保密协议的约束，在这门语言没有正式发布前，我是不能谈论它的名称和技术细节的。
所以，如果你对实现通用编程语言很感兴趣的话，其实现在就有机会进入这些团队，贡献自己的一份力量。
如果你参与了这样的项目团队，那么你就可以选定一个具体的领域，深入研究我们这门课涉及的那些知识点。有人可能变成语法分析的专家，有人可能成为优化技术的专家，有人可能成为后端技术的专家，还有人可能会成为运行时方面的专家。
而我相信，正是你们这些未来的专家，将来必然会让中国的通用编程语言领域大放异彩！
写在最后这门课程的主体内容，到这里就正式结束了，不过我还会跟同学们保持联系。保持联系的方式有几个，一是开源项目PlayScript，一是计划不定期发布的几篇加餐，还有就是这门课的微信群。
我会继续在编程语言的领域探索和实践，希望能够跟你多多交流。也希望你在前进的道路上，能够找到更多的志同道合的朋友，一起砥砺前行，创造出优秀的作品！
另外，我还给你准备了一份毕业问卷，题目不多，希望你能在问卷里聊一聊你对这门课的看法。欢迎你点击下面的图片，用1～2分钟的时间填写一下。当然了，如果你对课程内容还有什么问题，也欢迎你在留言区或交流群继续提问，我会持续回复你的留言。</description></item><item><title>结课测试_这些网络协议你都掌握了吗？</title><link>https://artisanbox.github.io/5/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/5/44/</guid><description>经过三个多月的学习，相信你对网络协议的基础概念和使用场景有了更深入的了解。我从专栏中精心筛选了核心知识点，编成了这20道测试题。希望可以帮助你学习自检，消化吸收，以期获得更好的学习效果。
如果你刚刚打开这个专栏，可以用这20道题，找到自己的薄弱点，对症下药；如果你已经学习了一段时间，可以用这20道题，检测一下学习成果，查漏补缺。
还等什么，点击下面按钮开始测试吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>结课测试｜这些MySQL知识你都掌握了吗？</title><link>https://artisanbox.github.io/1/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/1/49/</guid><description>你好，我是林晓斌。
《MySQL实战45讲》这门课程已经全部结束了。我给你准备了一个结课小测试，来帮助你检验自己的学习效果。
这套测试题共有 20 道题目，包括3道单选题和17道多选题，满分 100 分，系统自动评分。
还等什么，点击下面按钮开始测试吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>结课测试｜这些数据结构与算法，你真的掌握了吗？</title><link>https://artisanbox.github.io/2/75/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/75/</guid><description>你好，我是王争。
《数据结构与算法之美》已经完结一段时间了。在这段时间里，我依然收到了很多用户的留言，很感谢你一直以来的认真学习和支持！
为了帮助你检验自己的学习效果，我特别给你准备了一套结课测试题。这套测试题共有 20 道题目，都是单选题，满分 100 分。
点击下面按钮，马上开始测试吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>课前热身｜开始学习之前我们要准备什么？</title><link>https://artisanbox.github.io/3/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/3/47/</guid><description>你好！我是宫文学，欢迎来到《手把手带你写一门编程语言》的课程。
其实，你从课程题目就可以看出，我们这个课强调动手实践。所以在这一节课，我要给你介绍一下我们这个课程示例代码所采用的计算机语言，以及相关编程环境的搭建。这样，会方便你阅读、运行和修改课程的示例代码。
对于课程里用到的汇编语言、编译原理知识，如果你之前没有相关的经验，也不要担心。我会介绍一下我们这方面的设计思路，保证你通过这个课程会更快、更扎实地掌握它们。
通过这篇导读，你会对课程里用到的语言、工具、技术心里有数，以便更好地开启你的学习之旅。
好，我们先从使用的计算机语言和环境说起。
怎么快速上手TypeScript语言我们这个课程的目标呢，是要实现TypeScript的编译器和各种运行时。既然如此，那么我就尽可能地用TypeScript来实现这个目标。
虽然我们这个课程主体的代码都是用TypeScript写的，但我正式使用TypeScript其实是从2021年5月份开始，也就是我开始准备这个课的时间。
我知道你肯定会问：用几个月的时间，既要了解TypeScript，又要用TypeScript写自己的编译器，是不是太不靠谱了？当然，你可能也是因为要学习这门课程，第一次使用TypeScript，所以我就分享一下自己的一些经验。
第一，使用它！
我一直觉得，真正的语言学习开始于你使用它的那一刻。否则，你就是一直看这门语言的资料，也只能留下个大概印象，而且很快就会忘掉，只有动手使用，才会形成肌肉记忆。比如，现在我一写for循环，手指不自觉地打出“for (let i = …)”或"for (let x of …)"开头，这就是形成肌肉记忆了。
第二，看资料！
说实在的，现在学习计算机语言实在是太方便了，各种资料应有尽有，又有很多热心同学在网上的分享，遇到什么需求一查就有，用过才会记住。
如果你让我推荐一本学习资料，我比较推荐流浪小猫写的开源电子书《TypeScript入门教程》。这个作者可能比较懂我想看什么，提供的内容会到点上。比如，我们做面向对象编程的时候，都关心该语言是否具备运行时的类型判断能力，因为这个功能几乎百分之百会被用到，而这个教程里就专门有类型断言的章节。
第三，靠经验直觉！
其实，很多同学都学过多门语言，那么学一门新语言的速度就会很快。有经验的程序员会建立一些直觉，能够猜到一门语言可能具备什么特性。
在准备课程代码的时候，我有一次要编写一个类，代表汇编指令中的寄存器操作数。而在x86的汇编指令中，不同位数的寄存器的名称是不一样的，所以，我需要用一个成员变量bits，来表示这个寄存器的位数。这个时候，我想当然地写出了下面的代码：
class Register extends Oprand{ bits:32|64 = 32; //寄存器的位数 ... } 也就是位数只能取两个值，要么是32，要么是64，赋其他值给它都是错误的。
在使用这个写法的时候，我其实不是很确定是否可以用两个值的集合来描述变量的类型，因为在教程里提到联合类型（Union Types）的时候，只是说了可以用两个类型的联合。我直觉上觉得也应该支持两个值的联合，因为如果我是TypeScript的作者，我可能不会忽视这种使用场景。我根据自己的猜测试了一下，然后就成功了。
如果上升到类型理论的高度，那么我们可以说，类型本来就是可以取的值的集合。但如果我们不上升到理论高度，仅凭直觉，其实也能去正确地使用类型。
说到老程序员的直觉，其实这门课程的一个重要目标，就是想帮你建立起更多的直觉，建立仅仅通过高级语言的语法表象就能看透其内部实现机制的能力，让计算机语言在你面前成为一个白盒子，从而让你能够更加自如地去支配不同的语言来为自己服务。
好了，了解了怎么上手以后，我们再来看看在这门课程中的TypeScript的环境配置问题。
TypeScript的环境配置我们这个课程关于TypeScript的环境配置主要包括这些：
1.编译和运行环境：Node.js。
首先，我使用Node.js来编译和运行TypeScript，所以你要先在自己的电脑上安装Node.js，配置好相应的环境。这方面的资料很多，我就不提供链接了。
2.安装和配置TypeScript。
使用下面的命令，可以安装TypeScript：
npm install typescript -g 之后，你可以用git命令下载示例代码：
git clone https://gitee.com/richard-gong/craft-a-language.git 在用git下载了示例代码以后，需要你在示例代码的目录中运行下面这个命令，安装示例程序依赖的node.js中的一些包。安装完毕以后，会在craft-a-language目录中建立一个node_modules子目录：
npm i --save-dev @types/node 3.IDE：Visual Studio Code（简称VS Code）。
我在课程里使用了VS Code作为IDE，VS Code缺省就支持TypeScript语言，毕竟这个IDE本身就是用TypeScript编写的。
而且，我们每一节课的代码，都会被放在我们代码库里的一个单独的目录下，比如01、02……在每个目录下，都会有几个json文件，是TypeScript的工程配置文件。你只要在目录下输入tsc，就会编译该工程的所有.ts文件。
打开tscongfig.json，你会看到我提供的一些配置项：
target项是编译目标，这里我用的是es6，因为es6具备了很多高级特性。 module项是模块管理工具，我们选用的是CommonJS。 另外，有“exclude”选项，是排除了一些.</description></item><item><title>课程迭代｜全新交付71讲音频</title><link>https://artisanbox.github.io/2/78/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/2/78/</guid><description>你好，我是极客时间专栏主编李佳。今天是2021年1月8日，距离《数据结构与算法之美》课程结课快2年的时间，我代表极客时间教研团队，在此向你汇报一下这门课程的情况和迭代规划。
2020年12月7日，咱们这门课程的订阅超过了十万，这也就意味着我们成为了极客时间上最大的一个班集体。十万多位同学一起死磕数据结构和算法，这件事想想都振奋人心。
在我们的课程里，我们的文章有45,443次收藏，有438,273处划线，36,684条笔记，22,076条留言。这些学习数据都是我们这个班集体一起学习、一起进步的见证。
特别地，在课程留言里，你不仅分享了自己的学习收获、心得与经验，提出了自己的疑惑和问题，还指出了音频里的错误之处，真心感谢你的每一次批评指正。正是基于大家的反馈，我们决定重新交付课程音频，修改之前音频里的错误内容，同时，也在音频迭代的过程中，重新检查一遍文稿内容。这样，后面新加入的同学可以获得更好的学习体验，已经学完的同学也可以在复习的时候有不一样的感受。
这次音频迭代涉及课程里的71讲内容，我们会分2次全部替换完。音频替换计划如下：
1月8日，替换开篇词、01讲～30讲； 1月29日，替换31讲～56讲，以及加餐和结束语。 如果你还没有学完，或者是刚刚加入，那就跟着这个节奏，重新来学一遍吧。
希望这次全新迭代的音频，能带给你不一样的学习体验。也欢迎你继续提出问题、分享经验，我们一起学习、一起进步。
2021年刚刚开始，世界依然充满了种种不确定性，但是日日学习、点滴精进，这些都是确定的，而能给自己这种确定感的也只有我们自己。还等什么，就今天，就此刻，就从搞定一个算法、一个数据结构开始吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item></channel></rss>