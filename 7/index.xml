<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>编译原理实战课 on Gen 的学习笔记</title><link>https://artisanbox.github.io/7/</link><description>Recent content in 编译原理实战课 on Gen 的学习笔记</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 08 Mar 2022 18:37:53 +0800</lastBuildDate><atom:link href="https://artisanbox.github.io/7/index.xml" rel="self" type="application/rss+xml"/><item><title>01_编译的全过程都悄悄做了哪些事情？</title><link>https://artisanbox.github.io/7/1/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/1/</guid><description>你好，我是宫文学。
正如我在开篇词中所说的，这一季课程的设计，是要带你去考察实际编译器的代码，把你带到编译技术的第一现场，让你以最直观、最接地气的方式理解编译器是怎么做出来的。
但是，毕竟编译领域还是有很多基本概念的。对于编译原理基础不太扎实的同学来说，在跟随我出发探险之前，最好还是做一点准备工作，磨刀不误砍柴工嘛。所以，在正式开始本课程之前，我会先花8讲的时间，用通俗的语言，帮你把编译原理的知识体系梳理一遍。
当然，对于已经学过编译原理的同学来说，这几讲可以帮助你复习以前学过的知识，把相关的知识点从遥远的记忆里再调出来，重温一下，以便更好地进入状态。
今天这一讲，我首先带你从宏观上理解一下整个编译过程。后面几讲中，我再针对编译过程中的每个阶段做细化讲解。
好了，让我们开始吧。
编译，其实就是把源代码变成目标代码的过程。如果源代码编译后要在操作系统上运行，那目标代码就是汇编代码，我们再通过汇编和链接的过程形成可执行文件，然后通过加载器加载到操作系统里执行。如果编译后是在解释器里执行，那目标代码就可以不是汇编代码，而是一种解释器可以理解的中间形式的代码即可。
我举一个很简单的例子。这里有一段C语言的程序，我们一起来看看它的编译过程。
int foo(int a){ int b = a + 3; return b; } 这段源代码，如果把它编译成汇编代码，大致是下面这个样子：
.section __TEXT,__text,regular,pure_instructions .globl _foo ## -- Begin function foo _foo: ## @foo pushq %rbp movq %rsp, %rbp movl %edi, -4(%rbp) movl -4(%rbp), %eax addl $3, %eax movl %eax, -8(%rbp) movl -8(%rbp), %eax popq %rbp retq 你可以看出，源代码和目标代码之间的差异还是很大的。那么，我们怎么实现这个翻译呢？
其实，编译和把英语翻译成汉语的大逻辑是一样的。前提是你要懂这两门语言，这样你看到一篇英语文章，在脑子里理解以后，就可以把它翻译成汉语。编译器也是一样，你首先需要让编译器理解源代码的意思，然后再把它翻译成另一种语言。
表面上看，好像从英语到汉语，一下子就能翻译过去。但实际上，大脑一瞬间做了很多个步骤的处理，包括识别一个个单词，理解语法结构，然后弄明白它的意思。同样，编译器翻译源代码，也需要经过多个处理步骤，如下图所示。
图1：编译的各个阶段我来解释一下各个步骤。
词法分析（Lexical Analysis）首先，编译器要读入源代码。
在编译之前，源代码只是一长串字符而已，这显然不利于编译器理解程序的含义。所以，编译的第一步，就是要像读文章一样，先把里面的单词和标点符号识别出来。程序里面的单词叫做Token，它可以分成关键字、标识符、字面量、操作符号等多个种类。把字符串转换为Token的这个过程，就叫做词法分析。
图2：把字符串转换为Token（注意：其中的空白字符，代表空格、tab、回车和换行符，EOF是文件结束符）语法分析（Syntactic Analysis）识别出Token以后，离编译器明白源代码的含义仍然有很长一段距离。下一步，我们需要让编译器像理解自然语言一样，理解它的语法结构。这就是第二步，语法分析。
上语文课的时候，老师都会让你给一个句子划分语法结构。比如说：“我喜欢又聪明又勇敢的你”，它的语法结构可以表示成下面这样的树状结构。
图3：把一个句子变成语法树那么在编译器里，语法分析阶段也会把Token串，转换成一个体现语法规则的、树状的数据结构，这个数据结构叫做抽象语法树（AST，Abstract Syntax Tree）。我们前面的示例程序转换为AST以后，大概是下面这个样子：</description></item><item><title>02_词法分析：用两种方式构造有限自动机</title><link>https://artisanbox.github.io/7/2/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/2/</guid><description>你好，我是宫文学。
上一讲，我带你把整个编译过程走了一遍。这样，你就知道了编译过程的整体步骤，每一步是做什么的，以及为什么要这么做。
进一步地，你就可以研究一下每个环节具体是如何实现的、有哪些难点、有哪些理论和算法。通过这个过程，你不仅可以了解每个环节的原理，还能熟悉一些专有词汇。这样一来，你在读编译原理领域的相关资料时，就会更加顺畅了。
不过，编译过程中涉及的算法和原理有些枯燥，所以我会用尽量通俗、直观的方式来给你解读，让你更容易接受。
本讲，我主要跟你讨论一下词法分析（Lexical Analysis）这个环节。通过这节课，你可以掌握词法分析这个阶段是如何把字符串识别成一个个Token的。进而，你还会学到如何实现一个正则表达式工具，从而实现任意的词法解析。
词法分析的原理首先，我们来了解一下词法分析的原理。
通过上一讲，你已经很熟悉词法分析的任务了：输入的是字符串，输出的是Token串。所以，词法分析器在英文中一般叫做Tokenizer。
图1：把字符串转换为Token（注意：其中的空白字符，代表空格、tab、回车和换行符，EOF是文件结束符）但具体如何实现呢？这里要有一个计算模型，叫做有限自动机（Finite-state Automaton，FSA），或者叫做有限状态自动机（Finite-state Machine，FSM）。
有限自动机这个名字，听上去可能比较陌生。但大多数程序员，肯定都接触过另一个词：状态机。假设你要做一个电商系统，那么订单状态的迁移，就是一个状态机。
图2：状态机的例子（订单的状态和迁移过程）有限自动机就是这样的状态机，它的状态数量是有限的。当它收到一个新字符的时候，会导致状态的迁移。比如说，下面的这个状态机能够区分标识符和数字字面量。
图3：一个能够识别标识符和数字字面量的有限自动机在这样一个状态机里，我用单线圆圈表示临时状态，双线圆圈表示接受状态。接受状态就是一个合格的Token，比如图3中的状态1（数字字面量）和状态2（标识符）。当这两个状态遇到空白字符的时候，就可以记下一个Token，并回到初始态（状态0），开始识别其他Token。
可以看出，词法分析的过程，其实就是对一个字符串进行模式匹配的过程。说起字符串的模式匹配，你能想到什么工具吗？对的，正则表达式工具。
大多数语言，以及一些操作系统的命令，都带有正则表达式工具，来帮助你匹配合适的字符串。比如下面的这个Linux命令，可以用来匹配所有包含“sa”“sb” … “sh”字符串的进程。
ps -ef | grep 's[a-h]' 在这个命令里，“s[a-h]”是用来描述匹配规则的，我们把它叫做一个正则表达式。
同样地，正则表达式也可以用来描述词法规则。这种描述方法，我们叫做正则文法（Regular Grammar）。比如，数字字面量和标识符的正则文法描述是这样的：
IntLiteral : [0-9]+; //至少有一个数字 Id : [A-Za-z][A-Za-z0-9]*; //以字母开头，后面可以是字符或数字 与普通的正则表达式工具不同的是，词法分析器要用到很多个词法规则，每个词法规则都采用“Token类型: 正则表达式”这样一种格式，用于匹配一种Token。
然而，当我们采用了多条词法规则的时候，有可能会出现词法规则冲突的情况。比如说，int关键字其实也是符合标识符的词法规则的。
Int : int; //int关键字 For : for; //for关键字 Id : [A-Za-z][A-Za-z0-9]*; //以字母开头，后面可以是字符或数字 所以，词法规则里面要有优先级，比如排在前面的词法规则优先级更高。这样的话，我们就能够设计出区分int关键字和标识符的有限自动机了，可以画成下面的样子。其中，状态1、2和3都是标识符，而状态4则是int关键字。
图4：一个能够识别int关键字和标识符的有限自动机从正则表达式生成有限自动机现在，你已经了解了如何构造有限自动机，以及如何处理词法规则的冲突。基本上，你就可以按照上面的思路来手写词法分析器了。但你可能觉得，这样手写词法分析器的步骤太繁琐了，我们能否只写出词法规则，就自动生成相对应的有限自动机呢？
当然是可以的，实际上，正则表达式工具就是这么做的。此外，词法分析器生成工具lex（及GNU版本的flex）也能够基于规则自动生成词法分析器。
它的具体实现思路是这样的：把一个正则表达式翻译成NFA，然后把NFA转换成DFA。对不起，我这里又引入了两个新的术语：NFA和DFA。
先说说DFA，它是“Deterministic Finite Automaton”的缩写，即确定的有限自动机。它的特点是：该状态机在任何一个状态，基于输入的字符，都能做一个确定的状态转换。前面例子中的有限自动机，都属于DFA。
再说说NFA，它是“Nondeterministic Finite Automaton”的缩写，即不确定的有限自动机。它的特点是：该状态机中存在某些状态，针对某些输入，不能做一个确定的转换。
这又细分成两种情况：
对于一个输入，它有两个状态可以转换。 存在ε转换的情况，也就是没有任何字符输入的情况下，NFA也可以从一个状态迁移到另一个状态。 比如，“a[a-zA-Z0-9]*bc”这个正则表达式，对字符串的要求是以a开头，以bc结尾，a和bc之间可以有任意多个字母或数字。可以看到，在图5中，状态1的节点输入b时，这个状态是有两条路径可以选择的：一条是迁移到状态2，另一条是仍然保持在状态1。所以，这个有限自动机是一个NFA。
图5：一个NFA的例子，识别“a[a-zA-Z0-9]*bc”的自动机这个NFA还有引入ε转换的画法，如图6所示，它跟图5的画法是等价的。实际上，图6表示的NFA可以用我们下面马上要讲到的算法，通过正则表达式自动生成出来。
图6：另一个NFA的例子，同样能识别“a[a-zA-Z0-9]*bc”，其中有ε转换需要注意的是，无论是NFA还是DFA，都等价于正则表达式。也就是说，所有的正则表达式都能转换成NFA或DFA；而所有的NFA或DFA，也都能转换成正则表达式。
理解了NFA和DFA以后，接下来我再大致说一下算法。
首先，一个正则表达式可以机械地翻译成一个NFA。它的翻译方法如下：
识别字符i的NFA。 当接受字符i的时候，引发一个转换，状态图的边上标注i。其中，第一个状态（i，initial）是初始状态，第二个状态(f，final)是接受状态。</description></item><item><title>03_语法分析：两个基本功和两种算法思路</title><link>https://artisanbox.github.io/7/3/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/3/</guid><description>你好，我是宫文学。
通过第1讲的学习，现在你已经清楚了语法分析阶段的任务：依据语法规则，把Token串转化成AST。
今天，我就带你来掌握语法分析阶段的核心知识点，也就是两个基本功和两种算法思路。理解了这些重要的知识点，对于语法分析，你就不是外行了。
两个基本功：第一，必须能够阅读和书写语法规则，也就是掌握上下文无关文法；第二，必须要掌握递归下降算法。 两种算法思路：一种是自顶向下的语法分析，另一种则是自底向上的语法分析。 上下文无关文法（Context-Free Grammar）在开始语法分析之前，我们要解决的第一个问题，就是如何表达语法规则。在上一讲中，你已经了解了，我们可以用正则表达式来表达词法规则，语法规则其实也差不多。
我还是以下面这个示例程序为例，里面用到了变量声明语句、加法表达式，我们看看语法规则应该怎么写：
int a = 2; int b = a + 3; return b; 第一种写法是下面这个样子，它看起来跟上一讲的词法规则差不多，都是左边是规则名称，右边是正则表达式。
start：blockStmts ; //起始 block : '{' blockStmts '}' ; //语句块 blockStmts : stmt* ; //语句块中的语句 stmt = varDecl | expStmt | returnStmt | block; //语句 varDecl : type Id varInitializer？ ';' ; //变量声明 type : Int | Long ; //类型 varInitializer : '=' exp ; //变量初始化 expStmt : exp ';' ; //表达式语句 returnStmt : Return exp ';' ; //return语句 exp : add ; //表达式 add : add '+' mul | mul; //加法表达式 mul : mul '*' pri | pri; //乘法表达式 pri : IntLiteral | Id | '(' exp ')' ; //基础表达式 在语法规则里，我们把冒号左边的叫做非终结符（Non-terminal），又叫变元（Variable）。非终结符可以按照右边的正则表达式来逐步展开，直到最后都变成标识符、字面量、运算符这些不可再展开的符号，也就是终结符（Terminal）。终结符其实也是词法分析过程中形成的Token。</description></item><item><title>04_语义分析：让程序符合语义规则</title><link>https://artisanbox.github.io/7/4/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/4/</guid><description>你好，我是宫文学。这一讲，我们进入到语义分析阶段。
对计算机程序语义的研究，是一个专门的学科。要想很简单地把它讲清楚，着实不是太容易的事情。但我们可以退而求其次，只要能直观地去理解什么是语义就可以了。语义，就是程序要表达的意思。
因为计算机最终是用来做计算的，那么理解程序表达的意思，就是要知道让计算机去执行什么计算动作，这样才好翻译成目标代码。
那具体来说，语义分析要做什么工作呢？我们在第1讲中说过，每门计算机语言的标准中，都会定义很多语义规则，比如对加法运算要执行哪些操作。而在语义分析阶段，就是去检查程序是否符合这些语义规则，并为后续的编译工作收集一些语义信息，比如类型信息。
再具体一点，这些语义规则可以分为两大类。
第一类规则与上下文有关。因为我们说了，语法分析只能处理与上下文无关的工作。而与上下文有关的工作呢，自然就放到了语义分析阶段。
第二类规则与类型有关。在计算机语言中，类型是语义的重要载体。所以，语义分析阶段要处理与类型有关的工作。比如，声明新类型、类型检查、类型推断等。在做类型分析的时候，我们会用到一个工具，就是属性计算，也是需要你了解和掌握的。
补充：某些与类型有关的处理工作，还必须到运行期才能去做。比如，在多态的情况，调用一个方法时，到底要采用哪个子类的实现，只有在运行时才会知道。这叫做动态绑定。
在语义分析过程中，会使用两个数据结构。一个还是AST，但我们会把语义分析时获得的一些信息标注在AST上，形成带有标注的AST。另一个是符号表，用来记录程序中声明的各种标识符，并用于后续各个编译阶段。
那今天这一讲，我就会带你看看如何完成与上下文有关的分析、与类型有关的处理，并带你认识符号表和属性计算。
首先，我们来学习如何处理与上下文有关的工作。
上下文相关的分析那什么是与上下文有关的工作呢？在解析一个程序时，会有非常多的分析工作要结合上下文来进行。接下来，我就以控制流检查、闭包分析和引用消解这三个场景和你具体分析下。
场景1：控制流检查
像return、break和continue等语句，都与程序的控制流有关，它们必须符合控制流方面的规则。在Java这样的语言中，语义规则会规定：如果返回值不是void，那么在退出函数体之前，一定要执行一个return语句，那么就要检查所有的控制流分支，是否都以return语句结尾。
场景2：闭包分析
很多语言都支持闭包。而要正确地使用闭包，就必须在编译期知道哪些变量是自由变量。这里的自由变量是指在本函数外面定义的变量，但被这个函数中的代码所使用。这样，在运行期，编译器就会用特殊的内存管理机制来管理这些变量。所以，对闭包的分析，也是上下文敏感的。
场景3：引用消解
我们重点说一下引用消解，以及相关的作用域问题。
引用消解（Reference Resolution），有时也被称作名称消解（Name Resolution）或者标签消解（Label Resolution）。对变量名称、常量名称、函数名称、类型名称、包名称等的消解，都属于引用消解。因此，引用消解是一种非常重要的上下文相关的语义规则，我来重点讲解下。
在高级语言里，我们会做变量、函数（或方法）和类型的声明，然后在其他地方使用它们。这个时候，我们要找到定义和使用之间的正确引用关系。
我们来看一个例子。在语法分析阶段，对于“int b = a + 3”这样一条语句，无论a是否提前声明过，在语法上都是正确的。而在实际的计算机语言中，如果引用某个变量，这个变量就必须是已经声明过的。同时，当前这行代码，要处于变量a的作用域中才行。
图1：变量引用的消解对于变量来说，为了找到正确的引用，就需要用到作用域（Scope）这个概念。在编译技术里面，作用域这个词，有两个稍微有所差异的使用场景。
作用域的第一个使用场景，指的是变量、函数等标识符可以起作用的范围。下图列出了三个变量的作用域，每个变量声明完毕以后，它的下一句就可以引用它。
图2：变量的作用域作用域的第二个使用场景，是词法作用域（Lexical Scope），也就是程序中的不同文本区域。比如，一个语句块、参数列表、类定义的主体、函数（方法）的主体、模块主体、整个程序等。
到这里，咱们来总结下这两个使用场景。标识符和词法的作用域的差异在于：一个本地变量（标识符）的作用域，虽然属于某个词法作用域（如某个函数体），但其作用范围只是在变量声明之后的语句。而类的成员变量（标识符）的作用域，跟词法作用域是一致的，也就是整个类的范围，跟声明的位置无关。如果这个成员变量不是私有的，它的作用域还会覆盖到子类。
那具体到不同的编程语言，它们的作用域规则是不同的。比如，C语言里允许你在一个if语句块里定义一个变量，覆盖外部的变量，而Java语言就不允许这样。所以，在给Java做语义分析时，我们要检查出这种错误。
void foo(){ int a = 2; if (...){ int a = 3; //在C语言里允许，在Java里不允许 ... } } 在做引用消解的时候，为了更好地查找变量、类型等定义信息，编译器会使用一个辅助的数据结构：符号表。
符号表（Symbol Table）在写程序的时候，我们会定义很多标识符，比如常量名称、变量名称、函数名称、类名称，等等。在编译器里，我们又把这些标识符叫做符号（Symbol）。用来保存这些符号的数据结构，就叫做符号表。
比如，对于变量a来说，符号表中的基本信息可以包括：
名称：a 分类：变量 类型：int 作用域：foo函数体 其他必要的信息。 符号表的具体实现，每个编译器可能都不同。比如，它可能是一张线性的表格，也可能是按照作用域形成的一种有层次的表格。以下面这个程序为例，它包含了两个函数，每个函数里面都定义了多个变量：
void foo(){ int a； int b； if (a&amp;gt;0){ int c; int d; } else{ int e; int f; } } void bar(){ int g; { int h; int i; } } 它的符号表可能是下面这样的，分成了多个层次，每个层次对应了一个作用域。在全局作用域，符号表里包含foo和bar两个函数。在foo函数体里，有两个变量a和b，还有两个内部块，每个块里各有两个变量。</description></item><item><title>05_运行时机制：程序如何运行，你有发言权</title><link>https://artisanbox.github.io/7/5/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/5/</guid><description>你好，我是宫文学。在语义分析之后，编译过程就开始进入中后端了。
经过前端阶段的处理分析，编译器已经充分理解了源代码的含义，准备好把前端处理的结果（带有标注信息的AST、符号表）翻译成目标代码了。
我在第1讲也说过，如果想做好翻译工作，编译器必须理解目标代码。而要理解目标代码，它就必须要理解目标代码是如何被执行的。通常情况下，程序有两种执行模式。
第一种执行模式是在物理机上运行。针对的是C、C++、Go这样的语言，编译器直接将源代码编译成汇编代码（或直接生成机器码），然后生成能够在操作系统上运行的可执行程序。为了实现它们的后端，编译器需要理解程序在底层的运行环境，包括CPU、内存、操作系统跟程序的互动关系，并要能理解汇编代码。
第二种执行模式是在虚拟机上运行。针对的是Java、Python、Erlang和Lua等语言，它们能够在虚拟机上解释执行。这时候，编译器要理解该语言的虚拟机的运行机制，并生成能够被执行的IR。
理解了这两种执行模式的特点，我们也就能弄清楚用高级语言编写的程序是如何运行的，进而也就理解了编译器在中后端的任务是什么。接下来，我们就从最基础的物理机模式开始学习吧。
在物理机上运行在计算机发展的早期，科学家们确立了计算机的结构，并一直延续至今，这种结构就是冯·诺依曼结构。它的主要特点是：数据和指令不加区别，混合存储在同一个存储器中（即主存，或叫做内存）；用一个指令指针指向内存中指令的位置，CPU就能自动加载这个位置的指令并执行。
在x86架构下，这个指针是eip寄存器（32位模式）或rip寄存器（64位模式）。一条指令执行完毕，指令指针自动增加，并执行下一条指令。如果遇到跳转指令，则跳转到另一个地址去执行。
图1：计算机的运行机制这其实就是计算机最基本的运行原理。这样，你就可以在大脑中建立起像图1那样的直观结构。
通过图1，你会看到，计算机指令的执行基本上只跟两个硬件相关：一个是CPU，一个是内存。
CPUCPU是计算机的核心。从硬件构成方面，我们需要知道它的三个信息：
第一，CPU上面有寄存器，并且可以直接由指令访问。寄存器的读写速度非常快，大约是内存的100倍。所以我们编译后的代码，要尽量充分利用寄存器，而不是频繁地去访问内存。 第二，CPU有高速缓存，并且可能是多级的。高速缓存也比内存快。CPU在读取指令和数据的时候，不是一次读取一条，而是读取相邻的一批数据，放到高速缓存里。接下来要读取的数据，很可能已经在高速缓存里了，通过这种机制来提高运行性能。因此，编译器要尽量提高缓存的命中率。 第三，CPU内部有多个功能单元，有的负责计算，有的负责解码，等等。所以，一个指令可以被切分成多个执行阶段，每个阶段在不同的功能单元上运行，这为实现指令级并行提供了硬件基础。在第8讲，我还会和你详细解释这个话题。 好了，掌握了这个知识点，我们可以继续往下学习了。我们说，CPU是运行指令的地方，那指令到底是什么样子的呢？
我们知道，CPU有多种不同的架构，比如x86架构、ARM架构等。不同架构的CPU，它的指令是不一样的。不过它们的共性之处在于，指令都是01这样的机器码。为了便于理解，我们通常会用汇编代码来表示机器指令。比如，b=a+2指令对应的汇编码可能是这样的：
movl -4(%rbp), %eax #把%rbp-4内存地址的值拷贝到%eax寄存器 addl $2, %eax #把2加到%eax寄存器 movl %eax, -8(%rbp) #把%eax寄存器的值保存回内存，地址是%rbp-8 上面的汇编代码采用的是GNU汇编器规定的格式。每条指令都包含了两部分：操作码（opcode）和操作数（oprand）。
图2：汇编代码示例操作码是让CPU执行的动作。这段示例代码中，movl、addl是助记符（Assembly Mnemonic），其中的mov和add是指令，l是后缀，表示操作数的位数。
而操作数是指令的操作对象，它可以是常数、寄存器和某个内存地址。图2示例的汇编代码中，“$2”就是个常数，在指令里我们把它叫做立即数；而“%eax”是访问一个寄存器，其中eax是寄存器的名称；而带有括号的“-4(%rbp)”，则是对内存的访问方式，这个内存的地址是在rbp寄存器的值的基础上减去4。
如果你还想对指令、汇编代码有更多的了解，可以再去查阅些资料学习，比如去参考下我的《编译原理之美》中的第22、23、31这几讲。
这里要提一下，虽然程序觉得自己一直在使用CPU，但实际上，背后有操作系统在做调度。操作系统是管理系统资源的，而CPU是计算机的核心资源，操作系统会把CPU的时间划分成多个时间片，分配给不同的程序使用，每个程序实际上都是在“断断续续”地使用CPU，这就是操作系统的分时调度机制。在后面课程里讨论并发的时候，我们会更加深入地探讨这个机制。
内存好了，接下来我说说执行指令相关的另一个硬件：内存。
程序在运行时，操作系统会给它分配一块虚拟的内存空间，让它可以在运行期内使用。内存中的每个位置都有一个地址，地址的长度决定了能够表示多大空间，这叫做寻址空间。我们目前使用的都是64位的机器，理论上，你可以用一个64位的长整型来表示内存地址。
不过，由于我们根本用不了这么大的内存，所以AMD64架构的寻址空间只使用了48位。但这也有256TB，远远超出了一般情况下的需求。所以，像Windows这样的操作系统还会给予进一步的限制，缩小程序的寻址空间。
图3：48位寻址空间有多大但即使是在加了限制的情况下，程序在逻辑上可使用的内存一般也会大于实际的物理内存。不过进程不会一下子使用那么多的内存，只有在向操作系统申请内存的时候，操作系统才会把一块物理内存，映射成进程寻址空间内的一块内存。对应到图4中，中间一条是物理内存，上下两条是两个进程的寻址空间，它们要比物理内存大。
对于有些物理内存的内容，还可以映射进多个进程的地址空间，以减少内存的使用。比如说，如果进程1和进程2运行的是同一个可执行文件，那么程序的代码段是可以在两个进程之间共享的。你在图中可以看到这种情况。
图4：物理内存和逻辑内存的关系另外，对于已经分配给进程的内存，如果进程很长时间不用，操作系统会把它写到磁盘上，以便腾出更多可用的物理内存。在需要的时候，再把这块空间的数据从磁盘中读回来。这就是操作系统的虚拟内存机制。
当然，也存在没有操作系统的情况，这个时候你的程序所使用的内存就是物理内存，我们必须自己做好内存的管理。
那么从程序角度来说，我们应该怎样使用内存呢？
本质上来说，你想怎么用就怎么用，并没有什么特别的限制。一个编译器的作者，可以决定在哪儿放代码，在哪儿放数据。当然了，别的作者也可能采用其他的策略。比如，C语言和Java虚拟机对内存的管理和使用策略就是不同的。
不过尽管如此，大多数语言还是会采用一些通用的内存管理模式。以C语言为例，会把内存划分为代码区、静态数据区、栈和堆，如下所示。
图5：C语言的内存布局方式其中，代码区（也叫做文本段），主要存放编译完成后的机器码，也就是CPU指令；静态数据区会保存程序中的全局变量和常量。这些内存是静态的、固定大小的，在编译完毕以后就能确定清楚所占用空间的大小、代码区每个函数的地址，以及静态数据区每个变量和常量的地址。这些内存在程序运行期间会一直被占用。
而堆和栈，属于程序动态、按需获取的内存。我来和你分析下这两种内存。
我们先看看栈（Stack）。使用栈的一个好处是，操作系统会根据程序使用内存的需求，自动地增加或减少栈的空间。通常来说，操作系统会用一个寄存器保存栈顶的地址，程序可以修改这个寄存器的值，来获取或者释放空间。有的CPU，还有专门的指令来管理栈，比如x86架构，会使用push和pop指令，把数据写入栈或弹出栈，并自动修改栈顶指针。
在程序里使用栈的场景是这样的，程序的运行可以看做是在逐级调用函数（或者叫过程）。像下面的示例程序，存在着main-&amp;gt;bar-&amp;gt;foo的调用结构，这也就是控制流转移的过程。
int main(){ int a = 1; foo(3); bar(); } int foo(int c){ int b = 2; return b+c; }
int bar(){ return foo(4) + 1; 图6：程序逐级调用的过程每次调用函数的过程中，都需要一些空间来保存一些信息，比如参数、需要保护的寄存器的值、返回地址、本地变量等，这些信息叫做这个过程的活动记录（Activation Record）。</description></item><item><title>06_中间代码：不是只有一副面孔</title><link>https://artisanbox.github.io/7/6/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/6/</guid><description>你好，我是宫文学。今天这一讲，我来带你认识一下中间代码（IR）。
IR，也就是中间代码（Intermediate Representation，有时也称Intermediate Code，IC），它是编译器中很重要的一种数据结构。编译器在做完前端工作以后，首先就是生成IR，并在此基础上执行各种优化算法，最后再生成目标代码。
所以说，编译技术的IR非常重要，它是运行各种优化算法、代码生成算法的基础。不过，鉴于IR的设计一般与编译器密切相关，而一些教科书可能更侧重于讲理论，所以对IR的介绍就不那么具体。这就导致我们对IR有非常多的疑问，比如：
IR都有哪些不同的设计，可以分成什么类型？ IR有像高级语言和汇编代码那样的标准书写格式吗？ IR可以采用什么数据结构来实现？ 为了帮助你把对IR的认识从抽象变得具体，我今天就从全局的视角和你一起梳理下IR有关的认知。
首先，我们来了解一下IR的用途，并一起看看由于用途不同导致IR分成的多个层次。
IR的用途和层次设计IR的目的，是要满足编译器中的各种需求。需求的不同，就会导致IR的设计不同。通常情况下，IR有两种用途，一种是用来做分析和变换的，一种是直接用于解释执行的。我们先来看第一种。
编译器中，基于IR的分析和处理工作，一开始可以基于一些抽象层次比较高的语义，这时所需要的IR更接近源代码。而在后面，则会使用低层次的、更加接近目标代码的语义。
基于这种从高到低的抽象层次，IR可以归结为HIR、MIR和LIR三类。
HIR：基于源语言做一些分析和变换假设你要开发一款IDE，那最主要的功能包括：发现语法错误、分析符号之间的依赖关系（以便进行跳转、判断方法的重载等）、根据需要自动生成或修改一些代码（提供重构能力）。
这个时候，你对IR的需求，是能够准确表达源语言的语义就行了。这种类型的IR，可以叫做High IR，简称HIR。
其实，AST和符号表就可以满足这个需求。也就是说，AST也可以算作一种IR。如果你要开发IDE、代码翻译工具（从一门语言翻译到另一门语言）、代码生成工具、代码统计工具等，使用AST（加上符号表）就够了。
当然，有些HIR并不是树状结构（比如可以采用线性结构），但一般会保留诸如条件判断、循环、数组等抽象层次比较高的语法结构。
基于HIR，可以做一些高层次的代码优化，比如常数折叠、内联等。在Java和Go的编译器中，你可以看到不少基于AST做的优化工作。
MIR：独立于源语言和CPU架构做分析和优化大量的优化算法是可以通用的，没有必要依赖源语言的语法和语义，也没有必要依赖具体的CPU架构。
这些优化包括部分算术优化、常量和变量传播、死代码删除等，我会在下一讲和你介绍。实现这类分析和优化功能的IR可以叫做Middle IR，简称MIR。
因为MIR跟源代码和目标代码都无关，所以在讲解优化算法时，通常是基于MIR，比如三地址代码（Three Address Code，TAC）。
TAC的特点是，最多有三个地址（也就是变量），其中赋值符号的左边是用来写入的，而右边最多可以有两个地址和一个操作符，用于读取数据并计算。
我们来看一个例子，示例函数foo：
int foo (int a){ int b = 0; if (a &amp;gt; 10) b = a; else b = 10; return b; } 对应的TAC可能是：
BB1: b := 0 if a&amp;gt;10 goto BB3 //如果t是false(0),转到BB3 BB2: b := 10 goto BB4 BB3: b := a BB4: return b 可以看到，TAC用goto语句取代了if语句、循环语句这种比较高级的语句，当然也不会有类、继承这些高层的语言结构。但是，它又没有涉及数据如何在内存读写等细节，书写格式也不像汇编代码，与具体的目标代码也是独立的。</description></item><item><title>07_代码优化：跟编译器做朋友，让你的代码飞起来</title><link>https://artisanbox.github.io/7/7/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/7/</guid><description>你好，我是宫文学。
一门语言的性能高低，是它能否成功的关键。拿JavaScript来说，十多年来，它的性能多次得到成倍的提升，这也是前端技术栈如此丰富和强大的根本原因。
因此，编译器会无所不用其极地做优化，而优化工作在编译器的运行时间中，也占据了很大的比例。
不过，对编译技术的初学者来说，通常会搞不清楚编译器到底做了哪些优化，这些优化的实现思路又是怎样的。
所以今天这一讲，我就重点给你普及下编译器所做的优化工作，及其工作原理。在这个过程中，你还会弄明白很多似曾相识的术语，比如在前端必须了解的AST、终结符、非终结符等，在中后端必须熟悉的常数折叠、值编号、公共子表达式消除等。只有这样，你才算是入门了。
首先，我带你认识一些常见的代码优化方法。
常见的代码优化方法对代码做优化的方法有很多。如果要把它们分一下类的话，可以按照下面两个维度：
第一个分类维度，是机器无关的优化与机器相关的优化。机器无关的优化与硬件特征无关，比如把常数值在编译期计算出来（常数折叠）。而机器相关的优化则需要利用某硬件特有的特征，比如SIMD指令可以在一条指令里完成多个数据的计算。 第二个分类维度，是优化的范围。本地优化是针对一个基本块中的代码，全局优化是针对整个函数（或过程），过程间优化则能够跨越多个函数（或过程）做优化。 但优化算法很多，仅仅按照这两个维度分类，仍显粗糙。所以，我就按照优化的实现思路再分分类，让你了解起来更轻松一些。
思路1：把常量提前计算出来程序里的有些表达式，肯定能计算出一个常数值，那就不要等到运行时再去计算，干脆在编译期就计算出来。比如 “x=2*3”可以优化成“x=6”。这种优化方法，叫做常数折叠（Constant Folding）。
而如果你一旦知道x的值其实是一个常量，那你就可以把所有用到x的地方，替换成这个常量，这叫做常数传播（Constant Propagation）。如果有“y=x*2”这样一个语句，那么就能计算出来“y=12”。所以说，常数传播会导致更多的常数折叠。
就算不能引起新的常数折叠，比如说“z=a+x”，替换成“z=a+6”以后，计算速度也会更快。因为对于很多CPU来说，“a+x”和“a+6”对应的指令是不一样的。前者可能要生成两条指令（比如先把a放到寄存器上，再把x加上去），而后者用一条指令就行了，因为常数可以作为操作数。
更有用的是，常数传播可能导致分支判断条件是常量，因此导致一个分支的代码不需要被执行。这种优化叫做稀疏有条件的常数传播（Sparse Conditional Constant Propagation）。
a = 2 b = 3 if(a&amp;lt;b){ //判断语句去掉 ... //直接执行这个代码块 } else{ ... //else分支会去掉 } 思路2：用低代价的方法做计算完成相同的计算，可以用代价更低的方法。比如“x=x+0”这行代码，操作前后x没有任何变化，所以这样的代码可以删掉；又比如“x=x*0” 可以简化成“x=0”。这类利用代数运算的规则所做的简化，叫做代数简化（Algebra Simplification）。
对于很多CPU来说，乘法运算改成移位运算，速度会更快。比如，“x*2”等价于“x&amp;lt;&amp;lt;1”，“x*9”等价于“x&amp;lt;&amp;lt;3+x”。这种采用代价更低的运算的方法，也叫做强度折减（Strength Reduction）。
思路3：消除重复的计算下面的示例代码中，第三行可以被替换成“z:=2*x”， 因为y的值就等于x。这个时候，可能x的值已经在寄存器中，所以直接采用x，运算速度会更快。这种优化叫做拷贝传播（Copy Propagation）。
x := a + b y := x z := 2 * y 值编号（Value Numbering）也能减少重复计算。值编号是把相同的值，在系统里给一个相同的编号，并且只计算一次即可。比如，Wikipedia上的这个案例：
w := 3 x := 3 y := x + 4 z := w + 4 其中w和x的值是一样的，因此编号是相同的。这会进一步导致y和z的编号也是相同的。进而，它们可以简化成：</description></item><item><title>08_代码生成：如何实现机器相关的优化？</title><link>https://artisanbox.github.io/7/8/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/8/</guid><description>你好，我是宫文学。我们继续来学习编译器后端的技术。
在编译过程的前几个阶段之后，编译器生成了AST，完成了语义检查，并基于IR运行了各种优化算法。这些工作，基本上都是机器无关的。但编译的最后一步，也就是生成目标代码，则必须是跟特定CPU架构相关的。
这就是编译器的后端。不过，后端不只是简单地生成目标代码，它还要完成与机器相关的一些优化工作，确保生成的目标代码的性能最高。
这一讲，我就从机器相关的优化入手，带你看看编译器是如何通过指令选择、寄存器分配、指令排序和基于机器代码的优化等步骤，完成整个代码生成的任务的。
首先，我们来看看编译器后端的任务：生成针对不同架构的目标代码。
生成针对不同CPU的目标代码我们已经知道，编译器的后端要把IR翻译成目标代码，那么要生成的目标代码是什么样子的呢？
我以foo.c函数为例：
int foo(int a, int b){ return a + b + 10; } 执行“clang -S foo.c -o foo.x86.s”命令，你可以得到对应的x86架构下的汇编代码（为了便于你理解，我进行了简化）：
#序曲 pushq %rbp movq %rsp, %rbp #%rbp是栈底指针 #函数体 movl %edi, -4(%rbp) #把第1个参数写到栈里第一个位置（偏移量为4） movl %esi, -8(%rbp) #把第2个参数写到栈里第二个位置（偏移量为8） movl -4(%rbp), %eax #把第1个参数写到%eax寄存器 addl -8(%rbp), %eax #把第2个参数加到%eax addl $10, %eax #把立即数10加到%eax，%eax同时是放返回值的地方
#尾声 popq %rbp retq 小提示：上述汇编代码采用的是GNU汇编器的代码格式，源操作数在前面，目的操作数在后面。
我在第1讲中说过，要翻译成目标代码，编译器必须要先懂得目标代码，就像做汉译英一样，我们必须要懂得英语。可是，通常情况下，我们会对汇编代码比较畏惧，觉得汇编语言似乎很难学。其实不然。
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;补充说明：有些编译器，是先编译成汇编代码，再通过汇编器把汇编代码转变成机器码。而另一些编译器，是直接生成机器码，并写成目标文件，这样编译速度会更快一些。但这样的编译器一般就要带一个反汇编器，在调试等场合把机器码转化成汇编代码，这样我们看起来比较方便。
因此，在本课程中，我有时会不区分机器码和汇编代码。我可能会说，编译器生成了某机器码，但实际写给你看的是汇编代码，因为文本化的汇编代码才方便阅读。你如果看到这样的表述，不要感到困惑。
那为什么我说汇编代码不难学呢？你可以去查阅下各种不同CPU的指令。然后，你就会发现这些指令其实主要就那么几种，一类是做加减乘除的（如add指令），一类是做内存访问的（如mov、lea指令），一类是控制流程的（如jmp、ret指令），等等。说得夸张一点，这就是个复杂的计算器。
只不过，相比于高级语言，汇编语言涉及的细节比较多。它是啰嗦，但并不复杂。那我再分享一个我学习汇编代码的高效方法：让编译器输出高级语言的汇编代码，多看一些各种情况下汇编代码的写法，自然就会对汇编语言越来越熟悉了。
不过，虽然针对某一种CPU的汇编并不难，但问题是不同架构的CPU，其指令是不同的。编译器的后端每支持一种新的架构，就要有一套新的代码。这对写一个编译器来说，就是很大的工作量了。
我来举个例子。我们使用“clang -S -target armv7a-none-eabi foo.</description></item><item><title>09_Java编译器（一）：手写的编译器有什么优势？</title><link>https://artisanbox.github.io/7/9/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/9/</guid><description>你好，我是宫文学。
从今天开始呢，我会带着你去考察实际编译器的具体实现机制，你可以从中学习和印证编译原理的基础知识，进而加深你对编译原理的理解。
我们探险的第一站，是很多同学都很熟悉的Java语言，我们一起来看看它的编译器里都有什么奥秘。我从97年就开始用它，算是比较早了。当时，我就对它的“一次编译，到处运行”留下了很深的印象，我在Windows下写的程序，编译完毕以后放到Solaris上就能跑。现在看起来这可能不算什么，但在当年，我在Windows和Unix下写程序用的工具可是完全不同的。
到现在，Java已经是一门非常成熟的语言了，而且它也在不断进化，与时俱进，泛型、函数式编程、模块化等特性陆续都增加了进来。在服务端编程领域，它也变得非常普及。
与此同时，Java的编译器和虚拟机中所采用的技术，也比20年前发生了天翻地覆的变化。对于这么一门成熟的、广泛普及的、又不断焕发新生机的语言来说，研究它的编译技术会带来两个好处：一方面，Java编译器所采用的技术肯定是比较成熟的、靠谱的，你在实现自己的编译功能时，完全可以去参考和借鉴；另一方面，你可以借此深入了解Java的编译过程，借此去实现一些高级的功能，比方说，按需生成字节码，就像Spring这类工具一样。
因此，我会花4讲的时间，跟你一起探索Java的前端编译器（javac）。然后再花4讲的时间在Java的JIT编译器上。
那么，针对Java编译器，你可能会提出下面的问题：
Java的编译器是用什么语言编写的？ Java的词法分析器和语法分析器，是工具生成的，还是手工编写的？为什么会这样选择？ 语法分析的算法分为自顶向下和自底向上的。那么Java的选择是什么呢？有什么道理吗？ 如何自己动手修改Java编译器？ 这些问题，在今天的旅程结束后，你都会获得解答。并且，你还会获得一些额外的启发：噢，原来这个功能是可以这样做的呀！这是对你探险精神的奖励。
好吧，让我们开始吧。
第一步，我们先初步了解一下Java的编译器。
初步了解Java的编译器大多数Java工程师是通过javac命令来初次接触Java编译器的。假设你写了一个MyClass类：
public class MyClass { public int a = 2+3; public int foo(){ int b = a + 10; return b; } } 你可以用javac命令把MyClass.java文件编译成字节码文件：
javac MyClass.java 那这个javac的可执行文件就是Java的编译器吗？并不是。javac只是启动了一个Java虚拟机，执行了一个Java程序，跟我们平常用“java”命令运行一个程序是一样的。换句话说，Java编译器本身也是用Java写的。
这就很有趣了。我们知道，计算机语言是用来编写软件的，而编译器也是一种软件。所以，一门语言的编译器，竟然可以用自己来实现。这种现象，叫做“自举”(Bootstrapping)，这就好像一个人抓着自己的头发，要把自己提起来一样，多么神奇！实际上，一门语言的编译器，一开始肯定是要用其他语言来实现的。但等它成熟了以后，就会尝试实现自举。
既然Java编译器是用Java实现的，那意味着你自己也可以写一个程序，来调用Java的编译器。比如，运行下面的示例代码，也同样可以编译MyClass.java文件，生成MyClass.class文件：
import javax.tools.JavaCompiler; import javax.tools.ToolProvider; public class CompileMyClass { public static void main(String[] args) { JavaCompiler compiler = ToolProvider.getSystemJavaCompiler(); int result = compiler.run(null, null, null, &amp;quot;MyClass.java&amp;quot;); System.</description></item><item><title>10_Java编译器（二）：语法分析之后，还要做些什么？</title><link>https://artisanbox.github.io/7/10/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/10/</guid><description>你好，我是宫文学。
上一讲，我带你了解了Java语言编译器的词法分析和语法分析功能，这两项工作是每个编译器都必须要完成的。那么，根据第1讲我对编译过程的介绍，接下来就应该是语义分析和生成IR了。对于javac编译器来说，生成IR，也就是字节码以后，编译器就完成任务了。也就是说，javac编译器基本上都是在实现一些前端的功能。
不过，由于Java的语法特性很丰富，所以即使只是前端，它的编译功能也不少。那么，除了引用消解和类型检查这两项基础工作之外，你是否知道注解是在什么时候处理的呢？泛型呢？还有各种语法糖呢？
所以，今天这一讲，我就带你把Java编译器的总体编译过程了解一遍。然后，我会把重点放在语义分析中的引用消解、符号表的建立和注解的处理上。当你学完以后，你就能真正理解以下这些问题了：
符号表是教科书上提到的一种数据结构，但它在Java编译器里是如何实现的？编译器如何建立符号表？ 引用消解会涉及到作用域，那么作用域在Java编译器里又是怎么实现的？ 在编译期是如何通过注解的方式生成新程序的？ 为了方便你理解Java编译器内部的一些对象结构，我画了一些类图（如果你不习惯看类图的话，可以参考下面的图表说明，比如我用方框表示一个类，用小圆圈表示一个接口，几种线条分别代表继承关系、引用关系和接口实现）。
图1 ：课程中用到的类图的图表说明在课程开始之前，我想提醒几点：建议你在一个良好的学习环境进入今天的学习，因为你需要一步步地，仔细地跟住我的脚步，避免在探索过程中迷路；除此之外，你的手边还需要一个电脑，这样随时可以查看我在文章中提到的源代码。
了解整个编译过程现在，你可以打开jdk.compiler模块中的com.sun.tools.javac.comp包对应的源代码目录。
comp应该是Compile的缩写。这里面有一个com.sun.tools.javac.comp.CompileStates类，它的意思是编译状态。其中有一个枚举类型CompileState，里面列出了所有的编译阶段。
你会看到，词法和语法分析只占了一个环节（PARSE），生成字节码占了一个环节，而剩下的8个环节都可以看作是语义分析工作（建立符号表、处理注解、属性计算、数据流分析、泛型处理、模式匹配处理、Lambda处理和去除其他语法糖）。
public enum CompileState { INIT(0), //初始化 PARSE(1), //词法和语法分析 ENTER(2), //建立符号表 PROCESS(3), //处理注解 ATTR(4), //属性计算 FLOW(5), //数据流分析 TRANSTYPES(6), //去除语法糖：泛型处理 TRANSPATTERNS(7), //去除语法糖：模式匹配处理 UNLAMBDA(8), //去除语法糖：LAMBDA处理(转换成方法) LOWER(9), //去除语法糖：内部类、foreach循环、断言等。 GENERATE(10); //生成字节码 ... } 另外，你还可以打开com.sun.tools.javac.main.JavaCompiler的代码，看看它的compile()方法。去掉一些细节，你会发现这样的代码主干，从中能看出编译处理的步骤：
processAnnotations( //3：处理注解 enterTrees(stopIfError(CompileState.PARSE, //2：建立符号表 initModules(stopIfError(CompileState.PARSE, parseFiles(sourceFileObjects)) //1：词法和语法分析 )) ),classnames); &amp;hellip; case SIMPLE: generate( //10：生成字节码 desugar( //6~9：去除语法糖 flow( //5：数据流分析 attribute(todo)))); //4：属性计算
其中，PARSE阶段的成果就是生成一个AST，后续的语义分析阶段会基于它做进一步的处理：
enterTrees()：对应ENTER，这个阶段的主要工作是建立符号表。 processAnnotations()：对应PROCESS阶段，它的工作是处理注解。 attribute()：对应ATTR阶段，这个阶段是做属性计算，我会在下一讲中给你做详细的介绍。 flow()：对应FLOW阶段，主要是做数据流分析。我在第7讲中就提到过数据流分析，那时候是用它来做代码优化。那么，难道在前端也需要做数据流分析吗？它会起到什么作用？这些问题的答案我也会在下一讲中为你揭晓。 desugar()：去除语法糖，其实这里包括了TRANSTYPES（处理泛型）、TRANSPATTERNS（处理模式匹配）、UNLAMBDA（处理Lambda）和LOWER（处理其他所有的语法糖，比如内部类、foreach循环等）四个阶段，我会在第12讲给你介绍。 generate()：生成字节码，对应了GENERATE阶段，这部分内容我也会在第12讲详细介绍。 在今天这一讲，我会给你介绍前两个阶段的工作：建立符号表和处理注解。</description></item><item><title>11_Java编译器（三）：属性分析和数据流分析</title><link>https://artisanbox.github.io/7/11/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/11/</guid><description>你好，我是宫文学。
在上一讲，我们主要讨论了语义分析中的ENTER和PROCESS阶段。今天我们继续往下探索，看看ATTR和FLOW两个阶段。
ATTR的字面意思是做属性计算。在第4讲中，我已经讲过了属性计算的概念，你应该还记得什么是S属性，什么是I属性。那么，Java编译器会计算哪些属性，又会如何计算呢？
FLOW的字面意思是做数据流分析。通过第7讲，你已经初步了解了数据流分析的算法。但那个时候是把数据流分析用于编译期后端的优化算法，包括删除公共子表达式、变量传播、死代码删除等。而这里说的数据流分析，属于编译器前端的工作。那么，前端的数据流分析会做什么工作呢？
这些问题的答案，我今天都会为你一一揭晓。好了，我们进入正题，首先来看看ATTR阶段的工作：属性分析。
ATTR：属性分析现在，你可以打开com.sun.tools.javac.comp.Attr类的代码。在这个类的头注释里，你会发现原来ATTR做了四件事，分别在4个辅助类里实现：
Check：做类型检查。 Resolve：做名称的消解，也就是对于程序中出现的变量和方法，关联到其定义。 ConstFold：常量折叠，比如对于“2+3”这种在编译期就可以计算出结果的表达式，就直接计算出来。 Infer：用于泛型中的类型参数推导。 我们首先来看Check，也就是类型检查。
类型检查类型检查是语义分析阶段的一项重要工作。静态类型系统的语言，比如Java、C、Kotlin、Swift，都可以通过类型检查，避免很多编译错误。
那么，一个基础的问题是：Java都有哪些类型？
你是不是会觉得这个问题挺幼稚？Java的类型，不就是原始数据类型，再加上类、接口这些吗？
说得对，但是并不全面。你已经看到，Java编译器中每个AST节点都有一个type属性。那么，一个模块或者一个包的类型是什么？一个方法的类型又是什么呢？
在java.compile模块中，定义了Java的语言模型，其中有一个包，是对Java的类型体系做了设计，你可以看一下：
图1：Java的类型体系这样你就能理解了：原来模块和包的类型是NoType，而方法的类型是可执行类型（ExecutableType）。你可以看一下源代码，会发现要刻画一个可执行类型是比较复杂的，竟然需要5个要素：
returnType：返回值类型； parameterTypes：参数类型的列表； receiverType：接收者类型，也就是这个方法是定义在哪个类型（类、接口、枚举）上的； thrownTypes：所抛出异常的类型列表； typeVariables：类型参数的列表。 如果你学过C语言，你应该记得描述一个函数的类型只需要这个列表中的前两项，也就是返回值类型和参数类型就可以了。通过这样的对比，想必你会对Java的可执行类型理解得更清楚。
然而，通过一个接口体系来刻画类型还是不够细致，Java又提供了一个TypeKind的枚举类型，把某些类型做进一步的细化，比如原始数据类型进一步细分为BOOLEAN、BYTE、SHORT等。这种设计方式可以减少接口的数量，使类型体系更简洁。你也可以在编程中借鉴这种设计方式，避免产生过多的、没有什么实际意义的子类型。
同样，在jdk.compiler模块中，有一些具体的类实现了上述类型体系的接口：
图2：类型体系的实现好了，现在你已经了解了Java的类型体系。那么，编译器是如何实现类型检查的呢？
我用一个Java程序的例子，来给你做类型检查的说明。在下面这段代码中，变量a的声明语句是错误的，因为等号右边是一个字符串字面量“Hello”，类型是java.lang.String，跟变量声明语句的类型“int”不相符。在做类型检查的时候，编译器应该检查出这个错误来。
而后面那句“float b = 10”，虽然变量b是float型的，而等号右边是一个整型的字面量，但Java能够自动把整型字面量转化为浮点型，所以这个语句是合法的。
public class TypeCheck{ int a = &amp;quot;Hello&amp;quot;; //等号两边的类型不兼容，编译报错 float b = 10; //整型字面量可以赋值给浮点型变量 } 对于“int a = "hello"”这个语句，它的类型检查过程分了四步，如下图所示：
图3：类型检查的过程第1步，计算vartype子节点的类型。这一步是在把a加入符号表的时候（MemberEnter）就顺便一起做了（调用的是“Attr.attribType()方法”）。计算结果是int型。
第2步，在ATTR阶段正式启动以后，深度优先地遍历整棵AST，自底向上计算每个节点的类型。自底向上是S属性的计算方式。你可以看一下Attr类中的各种attribXXX()方法，大多数都是要返回一个类型值，也就是处理完当前子树后的类型。这个时候，能够知道init部分的类型是字符串型（java.lang.String）。
第3步，检查init部分的类型是否正确。这个时候，比对的就是vartype和init这两棵子树的类型。具体实现是在Check类的checkType()方法，这个方法要用到下面这两个参数。
final Type found：“发现”的类型，也就是“Hello”字面量的类型，这里的值是java.lang.String。这个是自底向上计算出来的，属于S属性。 final Type req：“需要”的类型，这里的值是int。也就是说，a这个变量需要初始化部分的类型是int型的。这个变量是自顶向下传递下来的，属于I属性。 所以你能看出，所谓的类型检查，就是所需类型（I属性）和实际类型（S属性）的比对。
这个时候，你就会发现类型不匹配，从而记录下错误信息。
下面是在做类型检查时整个的调用栈：
JavaCompiler.compile() -&amp;gt;JavaCompiler.attribute() -&amp;gt;Attr.attib() -&amp;gt;Attr.attribClass() //计算TypeCheck的属性 -&amp;gt;Attr.</description></item><item><title>12_Java编译器（四）：去除语法糖和生成字节码</title><link>https://artisanbox.github.io/7/12/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/12/</guid><description>你好，我是宫文学。今天是Java编译器的最后一讲，我们来探讨编译过程最后的两个步骤：去除语法糖和生成字节码。
其实今天要讲的这两个编译步骤，总体上都是为生成字节码服务的。在这一阶段，编译器首先会把语法糖对应的AST，转换成更基础的语法对应的AST，然后基于AST和符号表，来生成字节码。
从AST和符号表，到变成字节码，这可是一个很大的转变，就像把源代码转化成AST一样。那么，这个过程的实现思路是什么？有什么难点呢？
今天这一讲，我们就一起来解决以上这些问题，在这个过程中，你对Java编译器的认识会变得更加完整。
好了，我们首先来看看去除语法糖这一处理步骤。
去除语法糖（Syntactic Sugar）Java里面提供了很多的语法糖，比如泛型、Lambda、自动装箱、自动拆箱、foreach循环、变长参数、内部类、枚举类、断言（assert），等等。
你可以这么理解语法糖：它就是提高我们编程便利性的一些语法设计。既然是提高便利性，那就意味着语法糖能做到的事情，用基础语法也能做到，只不过基础语法可能更啰嗦一点儿而已。
不过，我们最终还是要把语法糖还原成基础语法结构。比如，foreach循环会被还原成更加基础的for循环。那么，问题来了，在编译过程中，究竟是如何去除语法糖的？基础语法和语法糖又有什么区别呢？
在第10讲中，我提到过，在JDK14中，去除语法糖涵盖了编译过程的四个小阶段。
TRANSTYPES：泛型处理，具体实现在TransTypes类中。 TRANSPATTERNS：处理模式匹配，具体实现在TransPattern类中。 UNLAMBDA：把LAMBDA转换成普通方法，具体实现在LambdaToMethod类中。 LOWER：其他所有的语法糖处理，如内部类、foreach循环、断言等，具体实现在Lower类中。 以上去除语法糖的处理逻辑都是相似的，它们的本质都是对AST做修改和变换。所以，接下来我挑选了两个比较有代表性的语法糖，泛型和foreach循环，和你分析它们的处理过程。
首先是对泛型的处理。
Java泛型的实现比较简单，LinkedList&amp;lt;String&amp;gt;和LinkedList对应的字节码其实是一样的。泛型信息&amp;lt;String&amp;gt;，只是用来在语义分析阶段做类型的检查。检查完之后，这些类型信息就会被去掉。
所以，Java的泛型处理，就是把AST中与泛型有关的节点简单地删掉（相关的代码在TransTypes类中）。
对于“ List&amp;lt;String&amp;gt; names = new ArrayList&amp;lt;String&amp;gt;() ”这条语句，它对应的AST的变化过程如下，其中，橙色的节点就是被去掉的泛型。
图1：对泛型的处理然后，我们分析下对foreach循环的处理。
foreach循环的意思是“遍历每一个成员”，它能够以更简洁的方式，遍历集合和数组等数据结构。在下面的示例代码中，foreach循环和基础for循环这两种处理方式的结果是等价的，但你可以看到，foreach循环会更加简洁。
public static void main(String args[]) { List&amp;lt;String&amp;gt; names = new ArrayList&amp;lt;String&amp;gt;(); ... //foreach循环 for (String name:names) System.out.println(name); //基础for循环 for ( Iterator i = names.iterator(); i.hasNext(); ) { String name = (String)i.next(); System.out.println(name); } } Java编译器把foreach循环叫做增强for循环，对应的AST节点是JCEnhancedForLoop。
针对上面的示例代码，我们来对比一下增强for循环的AST和去除语法糖之后的AST，如下图所示：
图2：foreach循环被改造成普通的for循环你可以通过反编译，来获得这些没有语法糖的代码，它跟示例代码中用到的基础for循环语句是一样的。
对foreach循环的处理，是在Lower类的visitForeachLoop方法中。
其实，你在阅读编译技术相关的文献时，应该经常会看到Lower这个词。它的意思是，让代码从对人更友好的状态，变换到对机器更友好的状态。比如说，语法糖对编程人员更友好，而基础的语句则相对更加靠近机器实现的一端，所以去除语法糖的过程是Lower。除了去除语法糖，凡是把代码向着机器代码方向所做的变换，都可以叫做Lower。以后你再见到Lower的时候，是不是就非常清楚它的意思了呢。
好了，通过对泛型和foreach循环的处理方式的探讨，现在你应该已经大致了解了去除语法糖的过程。总体来说，去除语法糖就是把AST做一些变换，让它变成更基础的语法要素，从而离生成字节码靠近了一步。</description></item><item><title>13_JavaJIT编译器（一）：动手修改Graal编译器</title><link>https://artisanbox.github.io/7/13/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/13/</guid><description>你好，我是宫文学。
在前面的4讲当中，我们已经解析了OpenJDK中的Java编译器，它是把Java源代码编译成字节码，然后交给JVM运行。
用过Java的人都知道，在JVM中除了可以解释执行字节码以外，还可以通过即时编译（JIT）技术生成机器码来执行程序，这使得Java的性能很高，甚至跟C++差不多。反之，如果不能达到很高的性能，一定会大大影响一门语言的流行。
但是，对很多同学来说，对于编译器中后端的了解，还是比较模糊的。比如说，你已经了解了中间代码、优化算法、指令选择等理论概念，那这些知识在实际的编译器中是如何落地的呢？
所以从今天开始，我会花4讲的时间，来带你了解Java的JIT编译器的组成部分和工作流程、它的IR的设计、一些重要的优化算法，以及生成目标代码的过程等知识点。在这个过程中，你还可以印证关于编译器中后端的一些知识点。
今天这一讲呢，我首先会带你理解JIT编译的基本原理；然后，我会带你进入Graal编译器的代码内部，一起去修改它、运行它、调试它，让你获得第一手的实践经验，消除你对JIT编译器的神秘感。
认识Java的JIT编译器我们先来探究一下JIT编译器的原理。
在第5讲中，我讲过程序运行的原理：把一个指令指针指向一个内存地址，CPU就可以读取其中的内容，并作为指令来执行。
所以，Java后端的编译器只要生成机器码就行了。如果是在运行前一次性生成，就叫做提前编译（AOT）；如果是在运行时按需生成机器码，就叫做即时编译（JIT）。Java以及基于JVM的语言，都受益于JVM的JIT编译器。
在JDK的源代码中，你能找到src/hotspot目录，这是JVM的运行时，它们都是用C++编写的，其中就包括JIT编译器。标准JDK中的虚拟机呢，就叫做HotSpot。
实际上，HotSpot带了两个JIT编译器，一个叫做C1，又叫做客户端编译器，它的编译速度快，但优化程度低。另一个叫做C2，又叫做服务端编译器，它的编译速度比较慢，但优化程度更高。这两个编译器在实际的编译过程中，是被结合起来使用的。而字节码解释器，我们可以叫做是C0，它的运行速度是最慢的。
在运行过程中，HotSpot首先会用C0解释执行；接着，HotSpot会用C1快速编译，生成机器码，从而让运行效率提升。而对于运行频率高的热点（HotSpot）代码，则用C2深化编译，得到运行效率更高的代码，这叫做分层编译（Tiered Compilation）。
图1：分层编译由于C2会做一些激进优化，比如说，它会根据程序运行的统计信息，认为某些程序分支根本不会被执行，从而根本不为这个分支生成代码。不过，有时做出这种激进优化的假设其实并不成立，那这个时候就要做一个逆优化（Deoptimization），退回到使用C1的代码，或退回到用解释器执行。
触发即时编译，需要检测热点代码。一般是以方法为单位，虚拟机会看看该方法的运行频次是否很高，如果运行特别频繁，那么就会被认定为是热点代码，从而就会被触发即时编译。甚至如果一个方法里，有一个循环块是热点代码（比如循环1.5万次以上），这个时候也会触发编译器去做即时编译，在这个方法还没运行完毕的时候，就被替换成了机器码的版本。由于这个时候，该方法的栈帧还在栈上，所以我们把这个技术叫做栈上替换（On-stack Replacement，OSR）。栈上替换的技术难点，在于让本地变量等数据无缝地迁移，让运行过程可以正确地衔接。
Graal：用Java编写的JIT编译器如果想深入地研究Java所采用的JIT编译技术，我们必须去看它的源码。可是，对于大多数Java程序员来说，如果去阅读C++编写的编译器代码，肯定会有些不适应。
一个好消息是，Oracle公司推出了一个完全用Java语言编写的JIT编译器：Graal，并且也有开放源代码的社区版，你可以下载安装并使用。
用Java开发一款编译器的优点是很明显的。
首先，Java是内存安全的，而C++程序的很多Bug都与内存管理有关，比如可能不当地使用了指针之类的。 第二，与Java配套的各种工具（比如IDE）更友好、更丰富。 第三，Java的性能并不低，所以能够满足对编译速度的需求。 最后，用Java编译甚至还能节省内存的占用，因为Java采用的是动态内存管理技术，一些对象没用了，其内存就会被回收。而用C++编写的话，可能会由于程序员的疏忽，导致一些内存没有被及时释放。 从Java9开始，你就可以用Graal来替换JDK中的JIT编译器。这里有一个JVMCI（JVM Compiler Interface）接口标准，符合这个接口标准的JIT编译器，都可以被用于JVM。
Oracle公司还专门推出了一款JVM，叫做GraalVM。它除了用Graal作为即时编译器以外，还提供了一个很创新的功能：在一个虚拟机上支持多种语言，并且支持它们之间的互操作。你知道，传统的JVM上已经能够支持多种语言，比如Scala、Clojure等。而新的GraalVM会更进一步，它通过一个Truffle框架，可以支持JavaScript、Ruby、R、Python等需要解释执行的语言。
再进一步，它还通过一个Sulong框架支持LLVM IR，从而支持那些能够生成LLVM IR的语言，如C、C++、Rust等。想想看，在Java的虚拟机上运行C语言，还是有点开脑洞的！
图2：GraalVM的架构最后，GraalVM还支持AOT编译，这就让Java可以编译成本地代码，让程序能更快地启动并投入高速运行。我听说最近的一些互联网公司，已经在用Graal做AOT编译，来生成本地镜像，提高应用的启动时间，从而能够更好地符合云原生技术的要求。
修改并运行Graal好，那接下来，我就带你一起动手修改一下Graal编译器，在这个过程中，你就能对Graal的程序结构熟悉起来，消除对它的陌生感，有助于后面深入探索其内部的实现机制。
在本课程中，我采用了Graal的20.0.1版本的源代码。你可以参考Graal中的文档来做编译工作。
首先，下载源代码（指定了代码的分支）：
git clone -b vm-20.0.1 https://github.com/oracle/graal.git 接着，下载GraalVM的构建工具mx，它是用Python2.7编写的，你需要有正确的Python环境：
git clone https://github.com/graalvm/mx.git export PATH=$PWD/mx:$PATH 你需要在自己的机器上设置好JDK8或11的环境。我这里是在macOS上，采用JDK8。
export PATH=&amp;quot;/Library/Java/JavaVirtualMachines/openjdk1.8.0_252-jvmci-20.1-b02-fastdebug/Contents/Home/bin:$PATH&amp;quot; export JAVA_HOME=/Library/Java/JavaVirtualMachines/openjdk1.8.0_252-jvmci-20.1-b02-fastdebug/Contents/Home 好了，现在你就可以编译Graal了。你可以在Graal源代码的compiler子目录中，运行mx build：
mx build 编译完毕以后，你可以写一个小小的测试程序，来测试Graal编译器的功能。
javac Foo.java //编译Foo.java mx vm Foo //运行Foo.java，相当于执行java Foo “mx vm”命令在第一次运行的时候，会打包出一个新的GraalVM，它所需要的HotSpot VM，是从JDK中拷贝过来的，然后它会把Graal编译器等其他模块也添加进去。
Foo.java的源代码如下。在这个示例程序中，main方法会无限次地调用add方法，所以add方法就成为了热点代码，这样会逼迫JIT编译器把add方法做即时编译。
public class Foo{ public static void main(String args[]){ int i = 0; while(true){ if(i%1000==0){ System.</description></item><item><title>14_JavaJIT编译器（二）：SeaofNodes为何如此强大？</title><link>https://artisanbox.github.io/7/14/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/14/</guid><description>你好，我是宫文学。这一讲，我们继续来研究Graal编译器，重点来了解一下它的IR的设计。
在上一讲中，我们发现Graal在执行过程中，创建了一个图的数据结构，这个数据结构就是Graal的IR。之后的很多处理和优化算法，都是基于这个IR的。可以说，这个IR是Graal编译器的核心特性之一。
那么，为什么这个IR采用的是图结构？它有什么特点和优点？编译器的优化算法又是如何基于这个IR来运行的呢？
今天，我就带你一起来攻破以上这些问题。在揭晓问题答案的过程中，你对真实编译器中IR的设计和优化处理过程，也就能获得直观的认识了。
基于图的IRIR对于编译器非常重要，因为它填补了高级语言和机器语言在语义上的巨大差别。比如说，你在高级语言中是使用一个数组，而翻译成最高效的x86机器码，是用间接寻址的方式，去访问一块连续的内存。所以IR的设计必须有利于实现这种转换，并且还要有利于运行优化算法，使得生成的代码更加高效。
在上一讲中，通过跟踪Graal编译器的执行过程，我们会发现它在一开始，就把字节码翻译成了一种新的IR，这个IR是用图的结构来表示的。那这个图长什么样子呢？非常幸运的是，我们可以用工具来直观地看到它的结构。
你可以从Oracle的网站上，下载一个idealgraphvisualizer的工具。下载之后，解压缩，并运行它：
export PATH=&amp;quot;/&amp;lt;上级目录&amp;gt;/idealgraphvisualizer/bin:$PATH&amp;quot; idealgraphvisualizer &amp;amp; 这时，程序会启动一个图形界面，并在4445端口上等待GraalVM发送数据过来。
接着，还是运行Foo示例程序，不过这次你要增加一个参数“-Dgraal.Dump”，这会让GraalVM输出编译过程的一些中间结果。并且在这个示例程序当中，我还增加了一个“-Xcomp”参数，它能让JIT编译器在第一次使用某个方法的时候，就去做编译工作。
mx vm \ -XX:+UnlockExperimentalVMOptions \ -XX:+EnableJVMCI \ -XX:+UseJVMCICompiler \ -XX:-TieredCompilation \ -XX:CompileOnly=Foo \ -Dgraal.Dump \ -Xcomp \ Foo GraalVM会在终端输出“Connected to the IGV on 127.0.0.1:4445”，这表明它连接上了idealgraphvisualizer。接着，在即时编译之后，idealgraphvisualizer就接收到了编译过程中生成的图，你可以点击显示它。
这里我展示了其中两个阶段的图，一个是刚解析完字节码之后（After parsing），一个是在处理完中间层之后（After mid tier）。
图1：After parsing图2：After mid tierGraal IR其实受到了“程序依赖图”的影响。我们在第6讲中提到过程序依赖图（PDG），它是用图来表示程序中的数据依赖和控制依赖。并且你也知道了，这种IR还有一个别名，叫做节点之海（Sea of Nodes）。因为当程序稍微复杂一点以后，图里的节点就会变得非常多，我们用肉眼很难看得清。
基于Sea of Nodes的IR呢，算是后起之秀。在HotSpot的编译器中，就采用了这种IR，而且现在Java的Graal编译器和JavaScript的V8编译器中的IR的设计，都是基于了Sea of Nodes结构，所以我们必须重视它。
这也不禁让我们感到好奇了：Sea of Nodes到底强在哪里？
我们都知道，数据结构的设计对于算法来说至关重要。IR的数据结构，会影响到算法的编写方式。好的IR的设计，会让优化算法的编写和维护都更加容易。
而Sea of Nodes最大的优点，就是能够用一个数据结构同时反映控制流和数据流，并且尽量减少它们之间的互相依赖。
怎么理解这个优点呢？在传统的编译器里，控制流和数据流是分开的。控制流是用控制流图（Control-flow Graph，CFG）来表示的，比如GNU的编译器、LLVM，都是基于控制流图的。而IR本身，则侧重于表达数据流。
以LLVM为例，它采用了SSA格式的IR，这种IR可以很好地体现值的定义和使用关系，从而很好地刻画了数据流。
而问题在于，采用这种比较传统的方式，控制流和数据流会耦合得比较紧，因为IR指令必须归属于某个基本块。
举个例子来说明一下吧。在下面的示例程序中，“int b = a*2;”这个语句，会被放到循环体的基本块中。
int foo(int a){ int sum = 0; for(int i = 0; i&amp;lt; 10; i++){ int b = a*2; //这一句可以提到外面 sum += b; } } 可是，从数据流的角度看，变量b只依赖于a。所以这个语句没必要放在循环体内，而是可以提到外面。在传统的编译器中，这一步是要分析出循环无关的变量，然后再把这条语句提出去。而如果采用Sea of Nodes的数据结构，变量b一开始根本没有归属到特定的基本块，所以也就没有必要专门去做代码的移动了。</description></item><item><title>15_JavaJIT编译器（三）：探究内联和逃逸分析的算法原理</title><link>https://artisanbox.github.io/7/15/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/15/</guid><description>你好，我是宫文学。
基于Graal IR进行的优化处理有很多。但有些优化，针对Java语言的特点，会显得更为重要。
今天这一讲，我就带你来认识两个对Java来说很重要的优化算法。如果没有这两个优化算法，你的程序执行效率会大大下降。而如果你了解了这两个算法的机理，则有可能写出更方便编译器做优化的程序，从而让你在实际工作中受益。这两个算法，分别是内联和逃逸分析。
另外，我还会给你介绍一种JIT编译所特有的优化模式：基于推理的优化。这种优化模式会让某些程序比AOT编译的性能更高。这个知识点，可能会改变你对JIT和AOT的认知，因为通常来说，你可能会认为AOT生成的机器码速度更快，所以通过这一讲的学习，你也会对“全生命周期优化”的概念有所体会。
好，首先，我们来看看内联优化。
内联（Inlining）内联优化是Java JIT编译器非常重要的一种优化策略。简单地说，内联就是把被调用的方法的方法体，在调用的地方展开。这样做最大的好处，就是省去了函数调用的开销。对于频繁调用的函数，内联优化能大大提高程序的性能。
执行内联优化是有一定条件的。第一，被内联的方法要是热点方法；第二，被内联的方法不能太大，否则编译后的目标代码量就会膨胀得比较厉害。
在Java程序里，你经常会发现很多短方法，特别是访问类成员变量的getter和setter方法。你可以看看自己写的程序，是否也充斥着很多对这种短方法的调用？这些调用如果不做优化的话，性能损失是很厉害的。你可以做一个性能对比测试，通过“-XX:-Inlining”参数来阻止JVM做内联优化，看看性能降低得会有多大。
但是这些方法有一个好处：它们往往都特别短，内联之后，实际上并不会显著增加目标代码长度。
比如，针对add2示例方法，我们采用内联选项优化后，方法调用被替换成了LoadField（加载成员变量）。
public int add2(){ return getX() + getY(); } 图1：将getter方法内联在做了Lower处理以后，LoadField会被展开成更底层的操作：根据x和y的地址相对于对象地址的偏移量，获取x和y的值。
图2：计算字段x和y的地址而要想正确地计算字段的偏移量，我们还需要了解Java对象的内存布局。
在64位平台下，每个Java对象头部都有8字节的标记字，里面有对象ID的哈希值、与内存收集有关的标记位、与锁有关的标记位；标记字后面是一个指向类定义的指针，在64位平台下也是8位，不过如果堆不是很大，我们可以采用压缩指针，只占4个字节；在这后面才是x和y字段。因此，x和y的偏移量分别是12和16。
图3：内存中，Java对象头占据的空间在Low Tier编译完毕以后，图2还会进一步被Lower，形成AMD64架构下的地址。这样的话，编译器再进一步翻译成汇编代码就很容易了。
图4：生成AMD64架构的地址计算节点内联优化除了会优化getter、setter这样的短方法，它实际上还起到了另一个重要的作用，即跨过程的优化。一般的优化算法，只会局限在一个方法内部。而启动内联优化后，多个方法会合并成一个方法，所以就带来了更多的优化的可能性。
我们来看看下面这个inlining示例方法。它调用了一个atLeastTen方法，这个方法对于&amp;lt;10的参数，会返回10；对于≥10的参数，会返回该参数本身。所以你用肉眼就可以看出来，inlining方法的返回值应该是10。
public int inliningTest(int a){ return atLeastTen(3); //应该返回10 } //至少返回10 public int atLeastTen(int a){ if (a &amp;lt; 10) return 10; else return a; } 如果不启用编译器的内联选项，那么inliningTest方法对应的IR图，就是常规的方法调用而已：
图5：不启用内联时调用atLeastTen()方法而一旦启用了内联选项，就可以触发一系列的优化。在把字节码解析生成IR的时候，编译器就启动了内联分析过程，从而会发现this参数和常量3对于inliningTest方法根本是无用的，在图里表现成了一些孤岛。在Mid Tier处理完毕之后，inliningTest方法就直接返回常量10了。
图6：启用内联后，调用atLeastTen()方法另外，方法的类型也会影响inlining。如果方法是final的，或者是private的，那么它就不会被子类重载，所以可以大胆地内联。
但如果存在着多重继承的类体系，方法就有可能被重载，这就会导致多态。在运行时，JVM会根据对象的类型来确定到底采用哪个子类的具体实现。这种运行时确定具体方法的过程，叫做虚分派（Virtual Dispatch）。
在存在多态的情况下，JIT编译器做内联就会遇到困难了。因为它不知道把哪个版本的实现内联进来。不过编译器仍然没有放弃。这时候所采用的技术，就叫做“多态内联（Polymorphic inlining）”。
它的具体做法是，在运行时，编译器会统计在调用多态方法的时候，到底用了哪几个实现。然后针对这几个实现，同时实现几个分支的内联，并在一开头根据对象类型判断应该走哪个分支。这个方法的缺陷是生成的代码量会比较大，但它毕竟可以获得内联的好处。最后，如果实际运行中遇到的对象，与提前生成的几个分支都不匹配，那么编译器还可以继续用缺省的虚分派模式来做函数调用，保证程序不出错。
这个案例也表明了，JIT编译器是如何充分利用运行时收集的信息来做优化的。对于AOT模式的编译来说，由于无法收集到这些信息，因此反倒无法做这种优化。
如果你想对多态内联做更深入的研究，还可以参考这一篇经典论文《Inlining of Virtual Methods》。
总结起来，内联优化不仅能降低由于函数调用带来的开销，还能制造出新的优化机会，因此带来的优化效果非常显著。接下来，我们看看另一个能带来显著优化效果的算法：逃逸分析。
逃逸分析（Escape Analysis, EA）逃逸分析是JVM的另一个重要的优化算法，它同样可以起到巨大的性能提升作用。
逃逸分析能够让编译器判断出，一个对象是否能够在创建它的方法或线程之外访问。如果只能在创建它的方法内部访问，我们就说这个对象不是方法逃逸的；如果仅仅逃逸出了方法，但对这个对象的访问肯定都是在同一个线程中，那么我们就说这个对象不是线程逃逸的。</description></item><item><title>16_JavaJIT编译器（四）：Graal的后端是如何工作的？</title><link>https://artisanbox.github.io/7/16/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/16/</guid><description>你好，我是宫文学。
前面两讲中，我介绍了Sea of Nodes类型的HIR，以及基于HIR的各种分析处理，这可以看做是编译器的中端。
可编译器最终还是要生成机器码的。那么，这个过程是怎么实现的呢？与硬件架构相关的LIR是什么样子的呢？指令选择是怎么做的呢？
这一讲，我就带你了解Graal编译器的后端功能，回答以上这些问题，破除你对后端处理过程的神秘感。
首先，我们来直观地了解一下后端处理的流程。
后端的处理流程在第14讲中，我们在运行Java示例程序的时候（比如atLeastTen()方法），使用了“-Dgraal.Dump=:5”的选项，这个选项会dump出整个编译过程最详细的信息。
对于HIR的处理过程，程序会通过网络端口，dump到IdealGraphVisualizer里面。而后端的处理过程，缺省则会dump到工作目录下的一个“graal_dumps”子目录下。你可以用文本编辑器打开查看里面的信息。
//至少返回10 public int atLeastTen(int a){ if (a &amp;lt; 10) return 10; else return a; } 不过，你还可以再偷懒一下，使用一个图形工具c1visualizer来查看。
补充：c1visualizer原本是用于查看Hopspot的C1编译器（也就是客户端编译器）的LIR的工具，这也就是说，Graal的LIR和C1的是一样的。另外，该工具不能用太高版本的JDK运行，我用的是JDK1.8。
图1：atLeatTen()方法对应的LIR在窗口的左侧，你能看到后端的处理流程。
首先是把HIR做最后一次排序（HIR Final Schedule），这个处理会把HIR节点分配到基本块，并且排序； 第二是生成LIR，在这个过程中要做指令选择； 第三，寄存器分配工作，Graal采用的算法是线性扫描（Linear Scan）； 第四，是基于LIR的一些优化工作，比如ControlFlowOptimizer等； 最后一个步骤，是生成目标代码。 接下来，我们来认识一下这个LIR：它是怎样生成的，用什么数据结构保存的，以及都有什么特点。
认识LIR在对HIR的处理过程中，前期（High Tier、Mid Tier）基本上都是与硬件无关。到了后期（Low Tier），你会看到IR中的一些节点逐步开始带有硬件的特征，比如上一讲中，计算AMD64地址的节点。而LIR就更加反映目标硬件的特征了，基本上可以跟机器码一对一地翻译。所以，从HIR生成LIR的过程，就要做指令选择。
我把与LIR相关的包和类整理成了类图，里面划分成了三个包，分别包含了与HIR、LIR和CFG有关的类。你可以重点看看它们之间的相互关系。
图2：HIR、LIR和CFG的关联关系在HIR的最后的处理阶段，程序会通过一个Schedule过程，把HIR节点排序，并放到控制流图中，为生成LIR和目标代码做准备。我之前说过，HIR的一大好处，就是那些浮动节点，可以最大程度地免受控制流的约束。但在最后生成的目标代码中，我们还是要把每行指令归属到某个具体的基本块的。而且，基本块中的HIR节点是按照顺序排列的，在ScheduleResult中保存着这个顺序（blockToNodesMap中顺序保存了每个Block中的节点）。
你要注意，这里所说的Schedule，跟编译器后端的指令排序不是一回事儿。这里是把图变成线性的程序；而编译器后端的指令排序（也叫做Schedule），则是为了实现指令级并行的优化。
当然，把HIR节点划分到不同的基本块，优化程度是不同的。比如，与循环无关的代码，放在循环内部和外部都是可以的，但显然放在循环外部更好一些。把HIR节点排序的Schedule算法，复杂度比较高，所以使用了很多启发式的规则。刚才提到的把循环无关代码放在循环外面，就是一种启发式的规则。
图2中的ControlFlowGraph类和Block类构成了控制流图，控制流图和最后阶段的HIR是互相引用的。这样，你就可以知道HIR中的每个节点属于哪个基本块，也可以知道每个基本块中包含的HIR节点。
做完Schedule以后，接着就会生成LIR。与声明式的HIR不同，LIR是命令式的，由一行行指令构成。
图1显示的是Foo.atLeatTen方法对应的LIR。你会看到一个控制流图（CFG），里面有三个基本块。B0是B1和B2的前序基本块，B0中的最后一个语句是分支语句（基本块中，只有最后一个语句才可以是导致指令跳转的语句）。
LIR中的指令是放到基本块中的，LIR对象的LIRInstructions属性中，保存了每个基本块中的指令列表。
OK，那接下来，我们来看看LIR的指令都有哪些，它们都有什么特点。
LIRInstruction的子类，主要放在三个包中，你可以看看下面的类图。
图3：LIR中的指令类型首先，在org.graalvm.compiler.lir包中，声明了一些与架构无关的指令，比如跳转指令、标签指令等。因为无论什么架构的CPU，一定都会有跳转指令，也一定有作为跳转目标的标签。
然后，在org.graalvm.compiler.lir.amd64包中，声明了几十个AMD64架构的指令，为了降低你的阅读负担，这里我只列出了有代表性的几个。这些指令是LIR代码中的主体。
最后，在org.graalvm.compiler.hotspot.amd64包中，也声明了几个指令。这几个指令是利用HotSpot虚拟机的功能实现的。比如，要获取某个类的定义的地址，只能由虚拟机提供。
好了，通过这样的一个分析，你应该对LIR有更加具体的认识了：LIR中的指令，大多数是与架构相关的。这样才适合运行后端的一些算法，比如指令选择、寄存器分配等。你也可以据此推测，其他编译器的LIR，差不多也是这个特点。
接下来，我们就来了解一下Graal编译器是如何生成LIR，并且在这个过程中，它是如何实现指令选择的。
生成LIR及指令选择我们已经知道了，Graal在生成LIR的过程中，要进行指令选择。
我们先看一下Graal对一个简单的示例程序Foo.add1，是如何生成LIR的。
public static int add1(int x, int y){ return x + y + 10; } 这个示例程序，在转LIR之前，它的HIR是下面这样。其中有两个加法节点，操作数包括了参数（ParameterNode）和常数（ConstantNode）两种类型。最后是一个Return节点。这个例子足够简单。实际上，它简单到只是一棵树，而不是图。</description></item><item><title>17_Python编译器（一）：如何用工具生成编译器？</title><link>https://artisanbox.github.io/7/17/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/17/</guid><description>你好，我是宫文学。
最近几年，Python在中国变得越来越流行，我想可能有几个推动力：第一个是因为人工智能热的兴起，用Python可以很方便地使用流行的AI框架，比如TensorFlow；第二个重要的因素是编程教育，特别是很多面对青少年的编程课程，都是采用的Python语言。
不过，Python之所以变得如此受欢迎，虽然有外在的机遇，但也得益于它内在的一些优点。比如说：
Python的语法比较简单，容易掌握，它强调一件事情只能用一种方法去做。对于老一代的程序员来说，Python就像久远的BASIC语言，很适合作为初学者的第一门计算机语言去学习，去打开计算机编程这个充满魅力的世界。 Python具备丰富的现代语言特性，实现方式又比较简洁。比如，它既支持面向对象特性，也支持函数式编程特性，等等。这对于学习编程很有好处，能够带给初学者比较准确的编程概念。 我个人比较欣赏Python的一个原因，是它能够充分利用开源世界的一些二进制的库，比如说，如果你想研究计算机视觉和多媒体，可以用它调用OpenCV和FFmpeg。Python跟AI框架的整合也是同样的道理，这也是Python经常用于系统运维领域的原因，因为它很容易调用操作系统的一些库。 最后，Python还有便于扩展的优势。如果你觉得Python有哪方面能力的不足，你也可以用C语言来写一些扩展。而且，你不仅仅可以扩展出几个函数，你还能扩展出新的类型，并在Python里使用这些新类型。比如，Python的数学计算库是NumPy，它的核心代码是用C语言编写的，性能很高。 看到这里，你自然会好奇，这么一门简洁有力的语言，是如何实现的呢？吉多·范罗苏姆（Python初始设计者）在编写Python的编译器的时候，脑子里是怎么想的呢？
从这一讲开始，我们就进入到Python语言的编译器内部，去看看它作为一门动态、解释执行语言的代表，是如何做词法分析、语法分析和语义分析的，又是如何解释执行的，以及它的运行时有什么设计特点，让它可以具备这些优势。你在这个过程中，也会对编译技术的应用场景了解得更加全面。这也正是我要花3讲的时间，带领你来解析Python编译器的主要原因。
今天这一讲，我们重点来研究Python的词法分析和语法分析功能，一起来看看它在这两个处理阶段都有什么特点。你会学到一种新的语法分析实现思路，还能够学到CST跟AST的区别。
好了，让我们开始吧。
编译源代码，并跟踪调试首先，你可以从python.org网站下载3.8.1版本的源代码。解压后你可以先自己浏览一下，看看能不能找到它的词法分析器、语法分析器、符号表处理程序、解释器等功能的代码。
Python源代码划分了多个子目录，每个子目录的内容整理如下：
首先，你会发现Python编译器是用C语言编写的。这跟Java、Go的编译器不同，Java和Go语言的编译器是支持自举的编译器，也就是这两门语言的编译器是用这两门语言自身实现的。
实际上，用C语言实现的Python编译器叫做CPython，是Python的几个编译器之一。它的标准库也是由C语言和Python混合编写的。我们课程中所讨论的就是CPython，它是Python语言的参考实现，也是macOS和Linux缺省安装的版本。
不过，Python也有一个编译器是用Python本身编写的，这个编译器是PyPy。它的图标是一条咬着自己尾巴的衔尾蛇，表明这个编译器是自举的。除此之外，还有基于JVM的Jython，这个版本的优势是能够借助成熟的JVM生态，比如可以不用自己写垃圾收集器，还能够调用丰富的Java类库。如果你觉得理解C语言的代码比较困难，你也可以去看看这两个版本的实现。
在Python的“开发者指南”网站上，有不少关于Python内部实现机制的技术资料。请注意，这里的开发者，指的是有兴趣参与Python语言开发的程序员，而不是Python语言的使用者。这就是像Python这种开源项目的优点，它欢迎热爱Python的程序员来修改和增强Python语言，甚至你还可以增加一些自己喜欢的语言特性。
根据开发者指南的指引，你可以编译一下Python的源代码。注意，你要用调试模式来编译，因为接下来我们要跟踪Python编译器的运行过程。这就要使用调试工具GDB。
GDB是GNU的调试工具，做C语言开发的人一般都会使用这个工具。它支持通过命令行调试程序，包括设置断点、单步跟踪、观察变量的值等，这跟你在IDE里调试程序的操作很相似。
开发者指南中有如何用调试模式编译Python，并如何跟GDB配合使用的信息。实际上，GDB现在可以用Python来编写扩展，从而给我们带来更多的便利。比如，我们在调试Python编译器的时候，遇到Python对象的指针（PyObject*），就可以用更友好的方式来显示Python对象的信息。
好了，接下来我们就通过跟踪Python编译器执行过程，看看它在编译过程中都涉及了哪些主要的程序模块。
在tokenizer.c的tok_get()函数中打一个断点，通过GDB观察Python的运行，你会发现下面的调用顺序（用bt命令打印输出后整理的结果）：
这个过程是运行Python并执行到词法分析环节，你可以看到完整的程序执行路径：
首先是python.c，这个文件很短，只是提供了一个main()函数。你运行python命令的时候，就会先进入这里。 接着进入Modules/main.c文件，这个文件里提供了运行环境的初始化等功能，它能执行一个python文件，也能启动REPL提供一个交互式界面。 之后是Python/pythonrun.c文件，这是Python的解释器，它调用词法分析器、语法分析器和字节码生成功能，最后解释执行。 再之后来到Parser目录的parsetok.c文件，这个文件会调度词法分析器和语法分析器，完成语法分析过程，最后生成AST。 最后是toknizer.c，它是词法分析器的具体实现。 拓展：REPL是Read-Evaluate-Print-Loop的缩写，也就是通过一个交互界面接受输入并回显结果。
通过上述的跟踪过程，我们就进入了Python的词法分析功能。下面我们就来看一下它是怎么实现的，再一次对词法分析的原理做一下印证。
Python的词法分析功能首先，你可以看一下tokenizer.c的tok_get()函数。你一阅读源代码，就会发现，这是我们很熟悉的一个结构，它也是通过有限自动机把字符串变成Token。
你还可以用另一种更直接的方法来查看Python词法分析的结果。
./python.exe -m tokenize -e foo.py 补充：其中的python.exe指的是Python的可执行文件，如果是在Linux系统，可执行文件是python。
运行上面的命令会输出所解析出的Token：
其中的第二列是Token的类型，第三列是Token对应的字符串。各种Token类型的定义，你可以在Grammar/Tokens文件中找到。
我们曾在研究Java编译器的时候，探讨过如何解决关键字和标识符的词法规则冲突的问题。那么Python是怎么实现的呢？
原来，Python在词法分析阶段根本没有区分这两者，只是都是作为“NAME”类型的Token来对待。
补充：Python里面有两个词法分析器，一个是用C语言实现的（tokenizer.c），一个是用Python实现的（tokenizer.py）。C语言版本的词法分析器由编译器使用，性能更高。
所以，Python的词法分析功能也比较常规。其实你会发现，每个编译器的词法分析功能都大同小异，你完全可以借鉴一个比较成熟的实现。Python跟Java的编译器稍微不同的一点，就是没有区分关键字和标识符。
接下来，我们来关注下这节课的重点内容：语法分析功能。
Python的语法分析功能在GDB中继续跟踪执行过程，你会在parser.c中找到语法分析的相关逻辑：
那么，Python的语法分析有什么特点呢？它采用的是什么算法呢？是自顶向下的算法，还是自底向上的算法？
首先，我们到Grammar目录，去看一下Grammar文件。这是一个用EBNF语法编写的Python语法规则文件，下面是从中节选的几句，你看是不是很容易读懂呢？
//声明函数 funcdef: 'def' NAME parameters ['-&amp;gt;' test] ':' [TYPE_COMMENT] func_body_suite //语句 simple_stmt: small_stmt (';' small_stmt)* [';'] NEWLINE small_stmt: (expr_stmt | del_stmt | pass_stmt | flow_stmt | import_stmt | global_stmt | nonlocal_stmt | assert_stmt) 通过阅读规则文件，你可以精确地了解Python的语法规则。</description></item><item><title>18_Python编译器（二）：从AST到字节码</title><link>https://artisanbox.github.io/7/18/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/18/</guid><description>你好，我是宫文学。
今天这一讲，我们继续来研究Python的编译器，一起来看看它是如何做语义分析的，以及是如何生成字节码的。学完这一讲以后，你就能回答出下面几个问题了：
像Python这样的动态语言，在语义分析阶段都要做什么事情呢，跟Java这样的静态类型语言有什么不同？ Python的字节码有什么特点？生成字节码的过程跟Java有什么不同？ 好了，让我们开始吧。首先，我们来了解一下从AST到生成字节码的整个过程。
编译过程Python编译器把词法分析和语法分析叫做“解析（Parse）”，并且放在Parser目录下。而从AST到生成字节码的过程，才叫做“编译（Compile）”。当然，这里编译的含义是比较狭义的。你要注意，不仅是Python编译器，其他编译器也是这样来使用这两个词汇，包括我们已经研究过的Java编译器，你要熟悉这两个词汇的用法，以便阅读英文文献。
Python的编译工作的主干代码是在Python/compile.c中，它主要完成5项工作。
第一步，检查future语句。future语句是Python的一个特性，让你可以提前使用未来版本的特性，提前适应语法和语义上的改变。这显然会影响到编译器如何工作。比如，对于“8/7”，用不同版本的语义去处理，得到的结果是不一样的。有的会得到整数“1”，有的会得到浮点数“1.14285…”，编译器内部实际上是调用了不同的除法函数。
第二步，建立符号表。
第三步，为基本块产生指令。
第四步，汇编过程：把所有基本块的代码组装在一起。
第五步，对字节码做窥孔优化。
其中的第一步，它是Python语言的一个特性，但不是我们编译技术关注的重点，所以这里略过。我们从建立符号表开始。
语义分析：建立符号表和引用消解通常来说，在语义分析阶段首先是建立符号表，然后在此基础上做引用消解和类型检查。
而Python是动态类型的语言，类型检查应该是不需要了，但引用消解还是要做的。并且你会发现，Python的引用消解有其独特之处。
首先，我们来看看Python的符号表是一个什么样的数据结构。在Include/symtable.h中定义了两个结构，分别是符号表和符号表的条目：
图1：符号表和符号表条目在编译的过程中，针对每个模块（也就是一个Python文件）会生成一个符号表（symtable）。
Python程序被划分成“块（block）”，块分为三种：模块、类和函数。每种块其实就是一个作用域，而在Python里面还叫做命名空间。每个块对应一个符号表条目（PySTEntryObject），每个符号表条目里存有该块里的所有符号（ste_symbols）。每个块还可以有多个子块（ste_children），构成树状结构。
在符号表里，有一个st_blocks字段，这是个字典，它能通过模块、类和函数的AST节点，查找到Python程序的符号表条目，通过这种方式，就把AST和符号表关联在了一起。
我们来看看，对于下面的示例程序，它对应的符号表是什么样子的。
a = 2 #模块级变量 class myclass: def __init__(self, x): self.x = x def foo(self, b): c = a + self.x + b #引用了外部变量a return c 这个示例程序有模块、类和函数三个级别的块。它们分别对应一条符号表条目。
图2：示例程序对应的符号表你可以看到，每个块里都有ste_symbols字段，它是一个字典，里面保存了本命名空间涉及的符号，以及每个符号的各种标志位（flags）。关于标志位，我下面会给你解释。
然后，我们再看看针对这个示例程序，符号表里的主要字段的取值：
好了，通过这样一个例子，你大概就知道了Python的符号表是怎样设计的了。下面我们来看看符号表的建立过程。
建立符号表的主程序是Python/symtable.c中的PySymtable_BuildObject()函数。
Python建立符号表的过程，需要做两遍处理，如下图所示。
图3：Python建立符号表的过程第一遍，主要做了两件事情。第一件事情是建立一个个的块（也就是符号表条目），并形成树状结构，就像示例程序那样；第二件事情，就是给块中的符号打上一定的标记（flag）。
我们用GDB跟踪一下第一遍处理后生成的结果。你可以参考下图，看一下我在Python的REPL中的输入信息：
我在symtable_add_def_helper()函数中设置了断点，便于调试。当编译器处理到foo函数的时候，我在GDB中打印输出了一些信息：
在这些输出信息中，你能看到前面我给你整理的表格中的信息，比如，符号表中各个字段的取值。
我重点想让你看的，是foo块中各个符号的标志信息：self和b是20，c是2，a是16。这是什么意思呢？
ste_symbols = {'self': 20, 'b': 20, 'c': 2, 'a': 16} 这就需要看一下symtable.h中，对这些标志位的定义：
我给你整理成了一张更容易理解的图，你参考一下：</description></item><item><title>19_Python编译器（三）：运行时机制</title><link>https://artisanbox.github.io/7/19/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/19/</guid><description>你好，我是宫文学。
在前面两讲中，我们已经分析了Python从开始编译到生成字节码的机制。但是，我们对Python只是了解了一半，还有很多问题需要解答。比如：Python字节码是如何运行的呢？它是如何管理程序所用到的数据的？它的类型体系是如何设计的，有什么特点？等等。
所以今天这一讲，我们就来讨论一下Python的运行时机制。其中的核心，是Python对象机制的设计。
我们先来研究一下字节码的运行机制。你会发现，它跟Python的对象机制密切相关。
理解字节码的执行过程我们用GDB跟踪执行一个简单的示例程序，它只有一行：“a=1”。
这行代码对应的字节码如下。其中，前两行指令实现了“a=1”的功能（后两行是根据Python的规定，在执行完一个模块之后，缺省返回一个None值）。
你需要在_PyEval_EvalFrameDefault()函数这里设置一个断点，在这里实际解释指令并执行。
首先是执行第一行指令，LOAD_CONST。
你会看到，解释器做了三件事情：
从常数表里取出0号常数。你知道，编译完毕以后会形成PyCodeObject，而在这个对象里会记录所有的常量、符号名称、本地变量等信息。常量1就是从它的常量表中取出来的。 把对象引用值加1。对象引用跟垃圾收集机制相关。 把这个常数对象入栈。 从这第一行指令的执行过程，你能得到什么信息呢？
第一个信息，常量1在Python内部，它是一个对象。你可以在GDB里显示这个对象的信息：该对象的类型是PyLong_Type型，这是Python的整型在内部的实现。
另外，该对象的引用数是126个，说明这个常量对象其实是被共享的，LOAD_CONST指令会让它的引用数加1。我们用的常数是1，这个值在Python内部也是会经常被用到，所以引用数会这么高。你可以试着选个不那么常见的常数，看看它的引用数是多少，都是在哪里被引用的。
进一步，我们会发现，往栈里放的数据，其实是个对象指针，而不像Java的栈机那样，是放了个整数。
总结上述信息，我其实可以告诉你一个结论：在Python里，程序中的任何符号都是对象，包括整数、浮点数这些基础数据，或者是自定义的类，或者是函数，它们都是对象。在栈机里处理的，是这些对象的引用。
我们再继续往下分析一条指令，也就是STORE_NAME指令，来加深一下对Python运行机制的理解。
执行STORE_NAME指令时，解释器做了5件事情：
根据指令的参数，从名称表里取出变量名称。这个名称表也是来自于PyCodeObject。前面我刚说过了，Python程序中的一切都是对象，那么name也是对象。你可以查看它的类型，是PyUnicode_Type，也就是Unicode的字符串。 从栈顶弹出上一步存进去的常量对象。 获取保存了所有本地变量的字典，这也是来自PyCodeObject。 在字典里，设置a的值为该常量。如果你深入跟踪其执行过程，你会发现在存入字典的时候，name对象和v对象的引用都会加1。这也是可以理解的，因为它们一个作为key，一个作为value，都要被字典所引用。 减少常量对象的引用计数。意思是栈机本身不再引用该常量。 好了，通过详细解读这两条指令的执行过程，我相信你对Python的运行机制摸到一点头绪了，但可能还是会提出很多问题来，比如说：
既然栈里的操作数都是对象指针，那么如何做加减乘除等算术运算？ 如果函数也是对象，那么执行函数的过程又是怎样的？ …… 别着急，我在后面会带你探究清楚这些问题。不过在此之前，我们有必要先加深一下对Python对象的了解。
Python对象的设计Python的对象定义在object.h中。阅读文件头部的注释和对各类数据结构的定义，你就可以理解Python对象的设计思路。
首先是PyObject和PyVarObject两个基础的数据结构，它们分别表示定长的数据和变长的数据。
typedef struct _object { //定长对象 Py_ssize_t ob_refcnt; //对象引用计数 struct _typeobject *ob_type; //对象类型 } PyObject; typedef struct { //变长对象 PyObject ob_base; Py_ssize_t ob_size; //变长部分的项目数量，在申请内存时有确定的值，不再变 } PyVarObject; PyObject是最基础的结构，所有的对象在Python内部都表示为一个PyObject指针。它里面只包含两个成员：对象引用计数（ob_refcnt）和对象类型（ob_type），你在用GDB跟踪执行时也见过它们。可能你会问，为什么只有这两个成员呢？对象的数据（比如一个整数）保存在哪里？
实际上，任何对象都会在一开头包含PyObject，其他数据都跟在PyObject的后面。比如说，Python3的整数的设计是一个变长对象，会用一到多个32位的段，来表示任意位数的整数：
#define PyObject_VAR_HEAD PyVarObject ob_base; struct _longobject { PyObject_VAR_HEAD //PyVarObject digit ob_digit[1]; //数字段的第一个元素 }; typedef struct _longobject PyLongObject; //整型 它在内存中的布局是这样的：</description></item><item><title>20_JavaScript编译器（一）：V8的解析和编译过程</title><link>https://artisanbox.github.io/7/20/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/20/</guid><description>你好，我是宫文学。从这一讲开始，我们就进入另一个非常重要的编译器：V8编译器。
V8是谷歌公司在2008年推出的一款JavaScript编译器，它也可能是世界上使用最广泛的编译器。即使你不是编程人员，你每天也会运行很多次V8，因为JavaScript是Web的语言，我们在电脑和手机上浏览的每个页面，几乎都会运行一点JavaScript脚本。
扩展：V8这个词，原意是8缸的发动机，换算成排量，大约是4.0排量，属于相当强劲的发动机了。它的编译器，叫做Ignition，是点火装置的意思。而它最新的JIT编译器，叫做TurboFan，是涡轮风扇发动机的意思。
在浏览器诞生的早期，就开始支持JavaScript了。但在V8推出以后，它重新定义了Web应用可以胜任的工作。到今天，在浏览器里，我们可以运行很多高度复杂的应用，比如办公套件等，这些都得益于以V8为代表的JavaScript引擎的进步。2008年V8发布时，就已经比当时的竞争对手快10倍了；到目前，它的速度又已经提升了10倍以上。从中你可以看到，编译技术有多大的潜力可挖掘！
对JavaScript编译器来说，它最大的挑战就在于，当我们打开一个页面的时候，源代码的下载、解析（Parse）、编译（Compile）和执行，都要在很短的时间内完成，否则就会影响到用户的体验。
那么，V8是如何做到既编译得快，又要运行得快的呢？所以接下来，我将会花两讲的时间，来带你一起剖析一下V8里面的编译技术。在这个过程中，你能了解到V8是如何完成前端解析、后端优化等功能的，它都有哪些突出的特点；另外，了解了V8的编译原理，对你以后编写更容易优化的程序，也会非常有好处。
今天这一讲，我们先来透彻了解一下V8的编译过程，以及每个编译阶段的工作原理，看看它跟我们已经了解的其他编译器相比，有什么不同。
初步了解V8首先，按照惯例，我们肯定要下载V8的源代码。按照官方文档中的步骤，你可以下载源代码，并在本地编译。注意，你最好把它编译成Debug模式，这样便于用调试工具去跟踪它的执行，所以你要使用下面的命令来进行编译。
tools/dev/gm.py x64.debug 编译完毕以后，进入v8/out/x64.debug目录，你可以运行./d8，这就是编译好的V8的命令行工具。如果你用过Node.js，那么d8的使用方法，其实跟它几乎是完全一样的，因为Node.js就封装了一个V8引擎。你还可以用GDB或LLDB工具来调试d8，这样你就可以知道，它是怎么编译和运行JavaScript程序了。
而v8/src目录下的，就是V8的源代码了。V8是用C++编写的。你可以重点关注这几个目录中的代码，它们是与编译有关的功能，而别的代码主要是运行时功能：
V8的编译器的构成跟Java的编译器很像，它们都有从源代码编译到字节码的编译器，也都有解释器（叫Ignition），也都有JIT编译器（叫TurboFan）。你可以看下V8的编译过程的图例。在这个图中，你能注意到两个陌生的节点：流处理节点（Stream）和预解析器（PreParser），这是V8编译过程中比较有特色的两个处理阶段。
图1：V8的编译过程注意：这是比较新的V8版本的架构。在更早的版本里，有时会用到两个JIT编译器，类似于HotSpot的C1和C2，分别强调编译速度和优化效果。在更早的版本里，还没有字节码解释器。现在的架构，引入了字节码解释器，其速度够快，所以就取消了其中一级的JIT编译器。
下面我们就进入到V8编译过程中的各个阶段，去了解一些编译器的细节。
超级快的解析过程（词法分析和语法分析）首先，我们来了解一下V8解析源代码的过程。我在开头就已经说过，V8解析源代码的速度必须要非常快才行。源代码边下载边解析完毕，在这个过程中，用户几乎感觉不到停顿。那它是如何实现的呢？
有两篇文章就非常好地解释了V8解析速度快的原因。
一个是“optimizing the scanner”这篇文章，它解释了V8在词法分析上做的优化。V8的作者们真是锱铢必较地在每一个可能优化的步骤上去做优化，他们所采用的技术很具备参考价值。
那我就按照我对这篇文章的理解，来给你解释一下V8解析速度快的原因吧：
第一个原因，是V8的整个解析过程是流（Stream）化的，也就是一边从网络下载源代码，一边解析。在下载后，各种不同的编码还被统一转化为UTF-16编码单位，这样词法解析器就不需要处理多种编码了。
第二个原因，是识别标识符时所做的优化，这也让V8的解析速度更快了一点。你应该知道，标识符的第一个字符（ID_START）只允许用字母、下划线和$来表示，而之后的字符（ID_CONTINUE）还可以包括数字。所以，当词法解析器遇到一个字符的时候，我们首先要判断它是否是合法的ID_START。
那么，这样一个逻辑，通常你会怎么写？我一般想也不想，肯定是这样的写法：
if(ch &amp;gt;= 'A' &amp;amp;&amp;amp; ch &amp;lt;= 'Z' || ch &amp;gt;='a' &amp;amp;&amp;amp; ch&amp;lt;='z' || ch == '$' || ch == '_'){ return true; } 但你要注意这里的一个问题，if语句中的判断条件需要做多少个运算？
最坏的情况下，要做6次比较运算和3次逻辑“或”运算。不过，V8的作者们认为这太奢侈了。所以他们通过查表的方法，来识别每个ASCII字符是否是合法的标识符开头字符。
这相当于准备了一张大表，每个字符在里面对应一个位置，标明了该字符是否是合法的标识符开头字符。这是典型的牺牲空间来换效率的方法。虽然你在阅读代码的时候，会发现它调用了几层函数来实现这个功能，但这些函数其实是内联的，并且在编译优化以后，产生的指令要少很多，所以这个方法的性能更高。
第三个原因，是如何从标识符中挑出关键字。
与Java的编译器一样，JavaScript的Scanner，也是把标识符和关键字一起识别出来，然后再从中挑出关键字。所以，你可以认为这是一个最佳实践。那你应该也会想到，识别一个字符串是否是关键字的过程，使用的方法仍然是查表。查表用的技术是“完美哈希（perfect hashing）”，也就是每个关键字对应的哈希值都是不同的，不会发生碰撞。并且，计算哈希值只用了三个元素：前两个字符（ID_START、ID_CONTINUE），以及字符串的长度，不需要把每个字符都考虑进来，进一步降低了计算量。
文章里还有其他细节，比如通过缩窄对Unicode字符的处理范围来进行优化，等等。从中你能体会到V8的作者们在提升性能方面，无所不用其极的设计思路。
除了词法分析，在语法分析方面，V8也做了很多的优化来保证高性能。其中，最重要的是“懒解析”技术（lazy parsing）。
一个页面中包含的代码，并不会马上被程序用到。如果在一开头就把它们全部解析成AST并编译成字节码，就会产生很多开销：占用了太多CPU时间；过早地占用内存；编译后的代码缓存到硬盘上，导致磁盘IO的时间很长，等等。
所以，所有浏览器中的JavaScript编译器，都采用了懒解析技术。在V8里，首先由预解析器，也就是Preparser粗略地解析一遍程序，在正式运行某个函数的时候，编译器才会按需解析这个函数。你要注意，Preparser只检查语法的正确性，而基于上下文的检查则不是这个阶段的任务。你如果感兴趣的话，可以深入阅读一下这篇介绍Preparser的文章，我在这里就不重复了。
你可以在终端测试一下懒解析和完整解析的区别。针对foo.js示例程序，你输入“./d8 – ast-print foo.js”命令。
function add(a,b){ return a + b; } //add(1,2) //一开始，先不调用add函数 得到的输出结果是：</description></item><item><title>21_JavaScript编译器（二）：V8的解释器和优化编译器</title><link>https://artisanbox.github.io/7/21/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/21/</guid><description>你好，我是宫文学。通过前一讲的学习，我们已经了解了V8的整个编译过程，并重点探讨了一个问题，就是V8的编译速度为什么那么快。
V8把解析过程做了重点的优化，解析完毕以后就可以马上通过Ignition解释执行了。这就让JavaScript可以快速运行起来。
今天这一讲呢，我们重点来讨论一下，V8的运行速度为什么也这么快，一起来看看V8所采用的优化技术。
上一讲我也提及过，V8在2008年刚推出的时候，它提供了一个快速编译成机器码的编译器，虽然没做太多优化，但性能已经是当时其他JavaScript引擎的10倍了。而现在，V8的速度又是2008年刚发布时候的10倍。那么，是什么技术造成了这么大的性能优化的呢？
这其中，一方面原因，是TurboFan这个优化编译器，采用了很多的优化技术。那么，它采用了什么优化算法？采用了什么IR？其优化思路跟Java的JIT编译器有什么相同点和不同点？
另一方面，最新的Ignition解释器，虽然只是做解释执行的功能，但竟然也比一个基础的编译器生成的代码慢不了多少。这又是什么原因呢？
所以今天，我们就一起把这些问题都搞清楚，这样你就能全面了解V8所采用的编译技术的特点了，你对动态类型语言的编译，也就能有更深入的了解，并且这也有助于你编写更高效的JavaScript程序。
好，首先，我们来了解一下TurboFan的优化编译技术。
TurboFan的优化编译技术TurboFan是一个优化编译器。不过它跟Java的优化编译器要完成的任务是不太相同的。因为JavaScript是动态类型的语言，所以如果它能够推断出准确的类型再来做优化，就会带来巨大的性能提升。
同时，TurboFan也会像Java的JIT编译器那样，基于IR来运行各种优化算法，以及在后端做指令选择、寄存器分配等优化。所有的这些因素加起来，才使得TurboFan能达到很高的性能。
我们先来看看V8最特别的优化，也就是通过对类型的推理所做的优化。
基于推理的优化（Speculative Optimazition）对于基于推理的优化，我们其实并不陌生。在研究Java的JIT编译器时，你就发现了Graal会针对解释器收集的一些信息，对于代码做一些推断，从而做一些激进的优化，比如说会跳过一些不必要的程序分支。
而JavaScript是动态类型的语言，所以对于V8来说，最重要的优化，就是能够在运行时正确地做出类型推断。举个例子来说，假设示例函数中的add函数，在解释器里多次执行的时候，接受的参数都是整型，那么TurboFan就处理整型加法运算的代码就行了。这也就是上一讲中我们生成的汇编代码。
function add(a,b){ return a+b; } for (i = 0; i&amp;lt;100000; i++){ if (i%1000==0) console.log(i);
add(i, i+1); } 但是，如果不在解释器里执行，直接要求TurboFan做编译，会生成什么样的汇编代码呢？
你可以在运行d8的时候，加上“–always-opt”参数，这样V8在第一次遇到add函数的时候，就会编译成机器码。
./d8 &amp;ndash;trace-opt-verbose &amp;ndash;trace-turbo &amp;ndash;turbo-filter=add &amp;ndash;print-code &amp;ndash;print-opt-code &amp;ndash;code-comments &amp;ndash;always-opt add.js 这一次生成的汇编代码，跟上一讲生成的就不一样了。由于编译器不知道add函数的参数是什么类型的，所以实际上，编译器是去调用实现Add指令的内置函数，来生成了汇编代码。
这个内置函数当然支持所有加法操作的语义，但是它也就无法启动基于推理的优化机制了。这样的代码，跟解释器直接解释执行，性能上没太大的差别，因为它们本质上都是调用一个全功能的内置函数。
而推理式优化的版本则不同，它直接生成了针对整型数字进行处理的汇编代码：
我来给你解释一下这几行指令的意思：
第1行和第3行，是把参数1和参数2分别拷贝到r8和r9寄存器。注意，这里是从物理寄存器里取值，而不是像前一个版本一样，在栈里取值。前一个版本遵循的是更加保守和安全的调用约定。 第2行和第4行，是把r8和r9寄存器的值向右移1位。 第5行，是把r8和r9相加。 看到这里，你可能就发现了一个问题：只是做个简单的加法而已，为什么要做移位操作呢？实际上，如果你熟悉汇编语言的话，要想实现上面的功能，其实只需要下面这两行代码就可以了：
movq rax, rdi #把参数1拷贝到rax寄存器 addq rax, rcx #把参数2加到rax寄存器上，作为返回值 那么，多出来的移位操作是干什么的呢？
这就涉及到了V8的内存管理机制。原来，V8对象都保存在堆中。在栈帧中保存的数值，都是指向堆的指针。垃圾收集器可以通过这些指针，知道哪些内存对象是不再被使用的，从而把它们释放掉。我们前面学过，Java的虚拟机和Python对于对象引用，本质上也是这么处理的。
但是，这种机制对于基础数据类型，比如整型，就不太合适了。因为你没有必要为一个简单的整型数据在堆中申请内存，这样既浪费内存，又降低了访问效率，V8需要访问两次内存才能读到一个整型变量的值（第一次读到地址，第二次根据该地址到堆里读到值）。你记得，Python就是这么访问基础数据的。
V8显然不能忍受这种低效的方式。它采用的优化机制，是一种被广泛采用的技术，叫做标记指针（Tagged Pointer）或者标记值（Tagged Value）。《Pointer Compression in V8》这篇文章，就介绍了V8中采用Tagged Pointer技术的细节。</description></item><item><title>22_Julia编译器（一）：如何让动态语言性能很高？</title><link>https://artisanbox.github.io/7/22/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/22/</guid><description>你好，我是宫文学。
Julia这门语言，其实是最近几年才逐渐获得人们越来越多的关注的。有些人会拿它跟Python进行比较，认为Julia已经成为了Python的劲敌，甚至还有人觉得它在未来可能会取代Python的地位。虽然这样的说法可能是有点夸张了，不过Julia确实是有它的可取之处的。
为什么这么说呢？前面我们已经研究了Java、Python和JavaScript这几门主流语言的编译器，这几门语言都是很有代表性的：Java语言是静态类型的、编译型的语言；Python语言是动态类型的、解释型的语言；JavaScript是动态类型的语言，但可以即时编译成本地代码来执行。
而Julia语言却声称同时兼具了静态编译型和动态解释型语言的优点：一方面它的性能很高，可以跟Java和C语言媲美；而另一方面，它又是动态类型的，编写程序时不需要指定类型。一般来说，我们很难能期望一门语言同时具有动态类型和静态类型语言的优点的，那么Julia又是如何实现这一切的呢？
原来它是充分利用了LLVM来实现即时编译的功能。因为LLVM是Clang、Rust、Swift等很多语言所采用的后端工具，所以我们可以借助Julia语言的编译器，来研究如何恰当地利用LLVM。不过，Julia使用LLVM的方法很有创造性，使得它可以同时具备这两类语言的优点。我将在这一讲中给你揭秘。
此外，Julia编译器的类型系统的设计也很独特，它体现了函数式编程的一些设计哲学，能够帮助你启迪思维。
还有一点，Julia来自MIT，这里也曾经是Lisp的摇篮，所以Julia有一种学术风和极客风相结合的品味，也值得你去仔细体会一下。
所以，接下来的两讲，我会带你来好好探究一下Julia的编译器。你从中能够学习到Julia编译器的处理过程，如何创造性地使用LLVM的即时编译功能、如何使用LLVM的优化功能，以及它的独特的类型系统和方法分派。
那今天这一讲，我会先带你来了解Julia的编译过程，以及它高性能背后的原因。
初步认识JuliaJulia的性能有多高呢？你可以去它的网站上看看与其他编程语言的性能对比：
图1：Julia和各种语言的性能对比可以看出，它的性能是在C、Rust这一个级别的，很多指标甚至比Java还要好，比起那些动态语言（如Python、R和Octave），那更是高了一到两个数量级。
所以，Julia的编译器声称它具备了静态类型语言的性能，确实是不虚此言的。
你可以从Julia的官网下载Julia的二进制版本和源代码。如果你下载的是源代码，那你可以用make debug编译成debug版本，这样比较方便用GDB或LLDB调试。
Julia的设计目的主要是用于科学计算。过去，这一领域的用户主要是使用R语言和Python，但麻省理工（MIT）的研究者们对它们的性能不够满意，同时又想要保留R和Python的友好性，于是就设计出了这门新的语言。目前这门语言受到了很多用户的欢迎，使用者也在持续地上升中。
我个人对它感兴趣的点，正是因为它打破了静态编译和动态编译语言的边界，我认为这体现了未来语言的趋势：编译和优化的过程是全生命周期的，而不局限在某个特定阶段。
好了，让我们先通过一个例子来认识Juia，直观了解一下这门语言的特点：
julia&amp;gt; function countdown(n) if n &amp;lt;= 0 println(&amp;quot;end&amp;quot;) else print(n, &amp;quot; &amp;quot;) countdown(n-1) end end countdown (generic function with 1 method) julia&amp;gt; countdown(10) 10 9 8 7 6 5 4 3 2 1 end 所以从这段示例代码中，可以看出，Julia主要有这样几个特点：
用function关键字来声明一个函数； 用end关键字作为块（函数声明、if语句、for语句等）的结尾； 函数的参数可以不用指定类型（变量声明也不需要），因为它是动态类型的； Julia支持递归函数。 那么Julia的编译器是用什么语言实现的呢？又是如何支持它的这些独特的特性的呢？带着这些好奇，让我们来看一看Julia编译器的源代码。
图2：Julia的源代码目录其实Julia的实现会让人有点困扰，因为它使用了4种语言：C、C++、Lisp和Julia自身。相比而言，CPython的实现只用了两种语言：C语言和Python。这种情况，就对社区的其他技术人员理解这个编译器和参与开发，带来了不小的困难。
Julia的作者用C语言实现了一些运行时的核心功能，包括垃圾收集器。他们是比较偏爱C语言的。C++主要是用来实现跟LLVM衔接的功能，因为LLVM是用C++实现的。
但是，为什么又冒出了一个Lisp语言呢？而且前端部分的主要功能都是用Lisp实现的。
原来，Julia中用到Lisp叫做femtolisp（简称flisp），这是杰夫·贝赞松（Jeff Bezanson）做的一个开源Lisp实现，当时的目标是做一个最小的、编译速度又最快的Lisp版本。后来Jeff Bezanson作为Julia的核心开发人员，又把flisp带进了Julia。
实际上，Julia语言本身也宣称是继承了Lisp语言的精髓。在其核心的设计思想里，在函数式编程风格和元编程功能方面，也确实是如此。Lisp在研究界一直有很多的追随者，Julia这个项目诞生于MIT，同时又主要服务于各种科研工作者，所以它也就带上了这种科学家的味道。它还有其他特性，也能看出这种科研工作者的倾向，比如说：
对于类型系统，Julia的开发者们进行了很好的形式化，是我在所有语言中看到的最像数学家做的类型系统。 在它的语法和语义设计上，带有Metalab和Mathematics这些数学软件的痕迹，科研工作者们应该很熟悉这种感觉。 在很多特性的实现上，都带有很强的前沿探索的特征，锋芒突出，不像我们平常使用的那些商业公司设计的计算机语言一样，追求四平八稳。 以上就是我对Julia的感觉，一种结合了数据家风格的自由不羁的极客风。实际上，Lisp最早的设计者约翰·麦卡锡（John McCarthy）就是一位数学博士，所以数学上的美感是Lisp给人的感受之一。而且，Lisp语言本身也是在MIT发源的，所以Julia可以说是继承了这个传统、这种风格。</description></item><item><title>23_Julia编译器（二）：如何利用LLVM的优化和后端功能？</title><link>https://artisanbox.github.io/7/23/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/23/</guid><description>你好，我是宫文学。
上一讲，我给你概要地介绍了一下Julia这门语言，带你一起解析了它的编译器的编译过程。另外我也讲到，Julia创造性地使用了LLVM，再加上它高效的分派机制，这就让一门脚本语言的运行速度，可以跟C、Java这种语言媲美。更重要的是，你用Julia本身，就可以编写需要高性能的数学函数包，而不用像Python那样，需要用另外的语言来编写（如C语言）高性能的代码。
那么今天这一讲，我就带你来了解一下Julia运用LLVM的一些细节。包括以下几个核心要点：
如何生成LLVM IR？ 如何基于LLVM IR做优化？ 如何利用内建（Intrinsics）函数实现性能优化和语义个性化？ 这样，在深入解读了这些问题和知识点以后，你对如何正确地利用LLVM，就能建立一个直观的认识了，从而为自己使用LLVM打下很好的基础。
好，首先，我们来了解一下Julia做即时编译的过程。
即时编译的过程我们用LLDB来跟踪一下生成IR的过程。
$ lldb #启动lldb (lldb)attach --name julia #附加到julia进程 c #让julia进程继续运行 首先，在Julia的REPL中，输入一个简单的add函数的定义：
julia&amp;gt; function add(a, b) x = a+b x end 接着，在LLDB或GDB中设置一个断点“br emit_funciton”，这个断点是在codegen.cpp中。
(lldb) br emit_function #添加断点 然后在Julia里执行函数add：
julia&amp;gt; add(2,3) 这会触发Julia的编译过程，并且程序会停在断点上。我整理了一下调用栈的信息，你可以看看，即时编译是如何被触发的。
通过跟踪执行和阅读源代码，你会发现Julia中最重要的几个源代码：
gf.c：Julia以方法分派快速而著称。对于类似加法的这种运算，它会有上百个方法的实现，所以在运行时，就必须能迅速定位到准确的方法。分派就是在gf.c里。 interpreter.c：它是Julia的解释器。虽然Julia中的函数都是即时编译的，但在REPL中的简单的交互，靠解释执行就可以了。 codegen.cpp：生成LLVM IR的主要逻辑都在这里。 我希望你能自己动手跟踪执行一下，这样你就会彻底明白Julia的运行机制。
Julia的IR：采用SSA形式在上一讲中，你已经通过@code_lowered和@code_typed宏，查看过了Julia的IR。
Julia的IR也经历了一个发展演化过程，它的IR最早不是SSA的，而是后来才改成了SSA形式。这一方面是因为，SSA真的是有优势，它能简化优化算法的编写；另一方面也能看出，SSA确实是趋势呀，我们目前接触到的Graal、V8和LLVM的IR，都是SSA格式的。
Julia的IR主要承担了两方面的任务。
第一是类型推断，推断出来的类型被保存到IR中，以便于生成正确版本的代码。
第二是基于这个IR做一些优化，其实主要就是实现了内联优化。内联优化是可以发生在比较早的阶段，你在Go的编译器中就会看到类似的现象。
你可以在Julia中写两个短的函数，让其中一个来调用另一个，看看它所生成的LLVM代码和汇编代码是否会被自动内联。
另外，你还可以查看一下传给emit_function函数的Julia IR是什么样子的。在LLDB里，你可以用下面的命令来显示src参数的值（其中，jl_(obj)是Julia为了调试方便提供的一个函数，它能够更好地显示Julia对象的信息，注意显示是在julia窗口中）。src参数里面包含了要编译的Julia代码的信息。
(lldb) expr jl_(src) 为了让你能更容易看懂，我稍微整理了一下输出的信息的格式：
你会发现，这跟用@code_typed(add(2,3))命令打印出来的信息是一致的，只不过宏里显示的信息会更加简洁：
接下来，查看emit_function函数，你就能够看到生成LLVM IR的整个过程。
生成LLVM IRLLVM的IR有几个特点：
第一，它是SSA格式的。 第二，LLVM IR有一个类型系统。类型系统能帮助生成正确的机器码，因为不同的字长对应的机器码指令是不同的。 第三，LLVM的IR不像其他IR，一般只有内存格式，它还有文本格式和二进制格式。你完全可以用文本格式写一个程序，然后让LLVM读取，进行编译和执行。所以，LLVM的IR也可以叫做LLVM汇编。 第四，LLVM的指令有丰富的元数据，这些元数据能够被用于分析和优化工作中。 基本上，生成IR的程序没那么复杂，就是用简单的语法制导的翻译即可，从AST或别的IR生成LLVM的IR，属于那种比较幼稚的翻译方法。</description></item><item><title>24_Go语言编译器：把它当作教科书吧</title><link>https://artisanbox.github.io/7/24/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/24/</guid><description>你好，我是宫文学。今天这一讲，我来带你研究一下Go语言自带的编译器，它可以被简称为gc。
我之所以要来带你研究Go语言的编译器，一方面是因为Go现在确实非常流行，很多云端服务都用Go开发，Docker项目更是巩固了Go语言的地位；另一方面，我希望你能把它当成编译原理的教学参考书来使用。这是因为：
Go语言的编译器完全用Go语言本身来实现，它完全实现了从前端到后端的所有工作，而不像Java要分成多个编译器来实现不同的功能模块，不像Python缺少了后端，也不像Julia用了太多的语言。所以你研究它所采用的编译技术会更方便。 Go编译器里基本上使用的都是经典的算法：经典的递归下降算法、经典的SSA格式的IR和CFG、经典的优化算法、经典的Lower和代码生成，因此你可以通过一个编译器就把这些算法都贯穿起来。 除了编译器，你还可以学习到一门语言的其他构成部分的实现思路，包括运行时（垃圾收集器、并发调度机制等）、标准库和工具链，甚至连链接器都是用Go语言自己实现的，从而对实现一门语言所需要做的工作有更完整的认识。 最后，Go语言的实现继承了从Unix系统以来形成的一些良好的设计哲学，因为Go语言的核心设计者都是为Unix的发展，做出过重要贡献的极客。因此了解了Go语言编译器的实现机制，会提高你的软件设计品味。 扩展：每种语言都有它的个性，而这个个性跟语言设计者的背景密切相关。Go语言的核心设计者，是Unix领域的极客，包括Unix的创始人和C语言的共同发明人之一，Ken Tompson。Rob Pike也是Unix的核心作者。
Go语言的作者们显然希望新的语言体现出他们的设计哲学和口味。比如，致力于像Unix那样的简洁和优雅，并且致力于让Go再次成为一款经典作品。
所以，在已经研究了多个高级语言的编译器之后，我们可以拿Go语言的编译器，把整个编译过程再重新梳理和印证一遍。
好了，现在就开始我们今天探索的旅途吧。
首先，我们来看看Go语言编译器的前端。
重要提示：照例，你要下载Go语言的源代码，本讲采用的是1.14.2版本。并且，你最好使用一个IDE，便于跟踪调试编译器的执行过程。
Go的源代码中附带的介绍编译器的文档，写得很好、很清晰，你可以参考一下。
词法分析和语法分析Go的编译器的词法分析和语法分析功能的实现，是在cmd/compile/internal/syntax目录下。
词法分析器是scanner.go。其实大部分编程语言的词法分析器的算法，都已经很标准了，我们在Java编译器里就曾经分析过。甚至它们处理标识符和关键字的方式也都一致，都是先作为标识符识别出来，然后再查表挑出关键字来。Go的词法分析器并没有像V8那样在不遗余力地压榨性能，它跟你平常编码的方式是很一致的，非常容易阅读。
语法分析器是parser.go。它是一个标准的手写的递归下降算法。在解析二元表达式的时候，Go的语法分析器也是采用了运算符优先级算法，这个已经是我们第N次见到这个算法了，所以你一定要掌握！不过，每个编译器的实现都不大一样，而Go的实现方式相当的简洁，你可以去自己看一下，或者用调试器来跟踪一下它的执行过程。
图1：用IDE工具Goland跟踪调试编译过程Go的AST的节点，是在nodes.go中定义的，它异常简洁，可以说简洁得让你惊讶。你可以欣赏一下。
Go的语法分析器还有一个很有特色的地方，就是对错误的处理。它在处理编译错误时，有一个原则，就是不要遇到一个错误就停止编译，而是要尽可能跳过当前这个出错的地方，继续往下编译，这样可以一次多报几个语法错误。
parser.go的处理方式是，当语法分析器在处理某个产生式的时候，如果发现了错误，那就记录下这个错误，并且往下跳过一些Token，直到找到一个Token是属于这个产生式的Follow集合的。这个时候编译器就认为找到了这个产生式的结尾。这样分析器就可以跳过这个语法单元，继续处理下面的语法单元。
比如，在解析函数声明语句时，如果Go的语法分析器没有找到函数名称，就报错“expecting name or (”，然后往后找到“{”或者“;”，这样就跳过了函数名称的声明部分，继续去编译后面的函数体部分。
在cmd/compile/internal/syntax目录下，还有词法分析器和语法分析器的测试程序，你可以去运行测试一下。
最后，如果你还想对Go语言的语法分析有更加深入地了解，我建议你去阅读一下Go语言的规范，它里面对于每个语法单元，都有EBNF格式的语法规则定义，比如对语句的定义。你通过看代码、看语言规范，积累语法规则的第一手经验，以后再看到一段程序，你的脑子里就能反映出它的语法规则，并且能随手画出AST了，这是你学习编译原理需要建立的硬功夫。比如说，这里我节选了一段Go语言的规范中针对语句的部分语法规则。
Statement = Declaration | LabeledStmt | SimpleStmt | GoStmt | ReturnStmt | BreakStmt | ContinueStmt | GotoStmt | FallthroughStmt | Block | IfStmt | SwitchStmt | SelectStmt | ForStmt | DeferStmt . SimpleStmt = EmptyStmt | ExpressionStmt | SendStmt | IncDecStmt | Assignment | ShortVarDecl .</description></item><item><title>25_MySQL编译器（一）：解析一条SQL语句的执行过程</title><link>https://artisanbox.github.io/7/25/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/25/</guid><description>你好，我是宫文学。现在，就到了我们编译之旅的最后一站了，我们一起来探索一下MySQL编译器。
数据库系统能够接受SQL语句，并返回数据查询的结果，或者对数据库中的数据进行修改，可以说几乎每个程序员都使用过它。
而MySQL又是目前使用最广泛的数据库。所以，解析一下MySQL编译并执行SQL语句的过程，一方面能帮助你加深对数据库领域的编译技术的理解；另一方面，由于SQL是一种最成功的DSL（特定领域语言），所以理解了MySQL编译器的内部运作机制，也能加深你对所有使用数据操作类DSL的理解，比如文档数据库的查询语言。另外，解读SQL与它的运行时的关系，也有助于你在自己的领域成功地使用DSL技术。
那么，数据库系统是如何使用编译技术的呢？接下来，我就会花两讲的时间，带你进入到MySQL的内部，做一次全面的探秘。
今天这一讲，我先带你了解一下如何跟踪MySQL的运行，了解它处理一个SQL语句的过程，以及MySQL在词法分析和语法分析方面的实现机制。
好，让我们开始吧！
编译并调试MySQL按照惯例，你要下载MySQL的源代码。我下载的是8.0版本的分支。
源代码里的主要目录及其作用如下，我们需要分析的代码基本都在sql目录下，它包含了编译器和服务端的核心组件。
图1：MySQL的源代码包含的主要目录MySQL的源代码主要是.cc结尾的，也就是说，MySQL主要是用C++编写的。另外，也有少量几个代码文件是用C语言编写的。
为了跟踪MySQL的执行过程，你要用Debug模式编译MySQL，具体步骤可以参考这篇开发者文档。
如果你用单线程编译，大约需要1个小时。编译好以后，先初始化出一个数据库来：
./mysqld --initialize --user=mysql 这个过程会为root@localhost用户，生成一个缺省的密码。
接着，运行MySQL服务器：
./mysqld &amp;amp; 之后，通过客户端连接数据库服务器，这时我们就可以执行SQL了：
./mysql -uroot -p #连接mysql server 最后，我们把GDB调试工具附加到mysqld进程上，就可以对它进行调试了。
gdb -p `pidof mysqld` #pidof是一个工具，用于获取进程的id，你可以安装一下 提示：这一讲中，我是采用了一个CentOS 8的虚拟机来编译和调试MySQL。我也试过在macOS下编译，并用LLDB进行调试，也一样方便。
注意，你在调试程序的时候，有两个设置断点的好地方：
dispatch_command：在sql/sql_parse.cc文件里。在接受客户端请求的时候（比如一个SQL语句），会在这里集中处理。 my_message_sql：在sql/mysqld.cc文件里。当系统需要输出错误信息的时候，会在这里集中处理。 这个时候，我们在MySQL的客户端输入一个查询命令，就可以从雇员表里查询姓和名了。在这个例子中，我采用的数据库是MySQL的一个示例数据库employees，你可以根据它的文档来生成示例数据库。
mysql&amp;gt; select first_name, last_name from employees; #从mysql库的user表中查询信息 这个命令被mysqld接收到以后，就会触发断点，并停止执行。这个时候，客户端也会老老实实地停在那里，等候从服务端传回数据。即使你在后端跟踪代码的过程会花很长的时间，客户端也不会超时，一直在安静地等待。给我的感觉就是，MySQL对于调试程序还是很友好的。
在GDB中输入bt命令，会打印出调用栈，这样你就能了解一个SQL语句，在MySQL中执行的完整过程。为了方便你理解和复习，这里我整理成了一个表格：
我也把MySQL执行SQL语句时的一些重要程序入口记录了下来，这也需要你重点关注。它反映了执行SQL过程中的一些重要的处理阶段，包括语法分析、处理上下文、引用消解、优化和执行。你在这些地方都可以设置断点。
图2：MySQL执行SQL语句时的部分重要程序入口好了，现在你就已经做好准备，能够分析MySQL的内部实现机制了。不过，由于MySQL执行的是SQL语言，它跟我们前面分析的高级语言有所不同。所以，我们先稍微回顾一下SQL语言的特点。
SQL语言：数据库领域的DSLSQL是结构化查询语言（Structural Query Language）的英文缩写。举个例子，这是一个很简单的SQL语句：
select emp_no, first_name, last_name from employees; 其实在大部分情况下，SQL都是这样一个一个来做语句执行的。这些语句又分为DML（数据操纵语言）和DDL（数据定义语言）两类。前者是对数据的查询、修改和删除等操作，而后者是用来定义数据库和表的结构（又叫模式）。
我们平常最多使用的是DML。而DML中，执行起来最复杂的是select语句。所以，在本讲，我都是用select语句来给你举例子。
那么，SQL跟我们前面分析的高级语言相比有什么不同呢？
第一个特点：SQL是声明式（Declarative）的。这是什么意思呢？其实就是说，SQL语句能够表达它的计算逻辑，但它不需要描述控制流。
高级语言一般都有控制流，也就是详细规定了实现一个功能的流程：先调用什么功能，再调用什么功能，比如if语句、循环语句等等。这种方式叫做命令式（imperative）编程。
更深入一点，声明式编程说的是“要什么”，它不关心实现的过程；而命令式编程强调的是“如何做”。前者更接近人类社会的领域问题，而后者更接近计算机实现。
第二个特点：SQL是一种特定领域语言（DSL，Domain Specific Language），专门针对关系数据库这个领域的。SQL中的各个元素能够映射成关系代数中的操作术语，比如选择、投影、连接、笛卡尔积、交集、并集等操作。它采用的是表、字段、连接等要素，而不需要使用常见的高级语言的变量、类、函数等要素。
所以，SQL就给其他DSL的设计提供了一个很好的参考：
采用声明式，更加贴近领域需求。比如，你可以设计一个报表的DSL，这个DSL只需要描述报表的特征，而不需要描述其实现过程。 采用特定领域的模型、术语，甚至是数学理论。比如，针对人工智能领域，你完全就可以用张量计算（力学概念）的术语来定义DSL。 好了，现在我们分析了SQL的特点，从而也让你了解了DSL的一些共性特点。那么接下来，顺着MySQL运行的脉络，我们先来了解一下MySQL是如何做词法分析和语法分析的。</description></item><item><title>26_MySQL编译器（二）：编译技术如何帮你提升数据库性能？</title><link>https://artisanbox.github.io/7/26/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/26/</guid><description>你好，我是宫文学。今天这一讲，我们继续来探究MySQL编译器。
通过上一讲的学习，你已经了解了MySQL编译器是怎么做词法和语法分析的了。那么在做完语法分析以后，MySQL编译器又继续做了哪些处理，才能成功地执行这个SQL语句呢？
所以今天，我就带你来探索一下MySQL的实现机制，我会把重点放在SQL的语义分析和优化机制上。当你学完以后，你就能真正理解以下这些问题了：
高级语言的编译器具有语义分析功能，那么MySQL编译器也会做语义分析吗？它有没有引用消解问题？有没有作用域？有没有类型检查？ MySQL有没有类似高级语言的那种优化功能呢？ 好，让我们开始今天的探究吧。不过，在讨论MySQL的编译过程之前，我想先带你了解一下MySQL会用到的一些重要的数据结构，因为你在解读代码的过程中经常会见到它们。
认识MySQL编译器的一些重要的数据结构第一组数据结构，是下图中的几个重要的类或结构体，包括线程、保存编译上下文信息的LEX，以及保存编译结果SELECT_LEX_UNIT和SELECT_LEX。
图1：MySQL编译器的中的几个重要的类和结构体首先是THD，也就是线程对象。对于每一个客户端的连接，MySQL编译器都会启动一个线程来处理它的查询请求。
THD中的一个重要数据成员是LEX对象。你可以把LEX对象想象成是编译SQL语句的工作区，保存了SQL语句编译过程中的上下文信息，编译器会把编译的成果放在这里，而编译过程中所需要的信息也是从这里查找。
在把SQL语句解析完毕以后，编译器会形成一些结构化的对象来表示一个查询。其中SELECT_LEX_UNIT结构体，就代表了一个查询表达式（Query Expression）。一个查询表达式可能包含了多个查询块，比如使用UNION的情况。
而SELECT_LEX则代表一个基本的查询块（Query Block），它里面的信息包括了所有的列和表达式、查询用到的表、where条件等。在SELECT_LEX中会保存查询块中涉及的表、字段和表达式等，它们也都有对应的数据结构。
第二组需要了解的数据结构，是表示表、字段等信息的对象。Table_ident对象保存了表的信息，包括数据库名、表名和所在的查询语句（SELECT_LEX_UNIT对象）。
图2：Table_indent对象，代表一个表而字段和表达式等表示一个值的对象，用Item及其子类来表示。SQL语句中的每个字段、每个计算字段，最后都对应一个Item。where条件，其实也是用一个Item就能表示。具体包括：
字段（Item_field）。 各种常数，包括数字、字符和null等（Item_basic_constant）。 能够产生出值的运算（Item_result_field），包括算术表达式（Item_num_op）、存储过程（Item_func_sp）、子查询（Item_subselect）等。 在语法分析过程中产生的Item（Parse_tree_item）。它们是一些占位符，因为在语法分析阶段，不容易一下子创建出真正的Item，这些Parse_tree_item需要在上下文分析阶段，被替换成真正的Item。 图3：Item及其子类好了，上面这些就是MySQL会用到的最核心的一些数据结构了。接下来的编译工作，就会生成和处理上述的数据结构。
上下文分析我们先来看一下MySQL编译器的上下文分析工作。
你已经知道，语法分析仅仅完成的是上下文无关的分析，还有很多的工作，需要基于上下文来做处理。这些工作，就属于语义分析。
MySQL编译器中，每个AST节点，都会有一个contextualize()方法。从这个方法的名称来看，你就能知道它是做上下文处理的（contextualize，置于上下文中）。
对一个Select语句来说，编译器会调用其根节点PT_select_stmt的contextualize()方法，从而深度遍历整个AST，并调用每个节点的contextualize()方法。
那么，MySQL编译器的上下文处理，都完成了什么工作呢？
首先，是检查数据库名、表名和字段名是否符合格式要求（在table.cc中实现）。
比如，MySQL会规定表名、字段名等名称不能超过64个字符，字段名不能包含ASCII值为255的字符，等等。这些规则在词法分析阶段是不检查的，要留在语义分析阶段检查。
然后，创建并填充SELECT_LEX_UNIT和SELECT_LEX对象。
前面我提到了，SELECT_LEX_UNIT和SELECT_LEX中，保存了查询表达式和查询块所需的所有信息，依据这些信息，MySQL就可以执行实际的数据库查询操作。
那么，在contextualize的过程中，编译器就会生成上述对象，并填充它们的成员信息。
比如，对于查询中用到的表，在语法分析阶段就会生成Table_ident对象。但其中的数据库名称可能是缺失的，那么在上下文的分析处理当中，就会被编译器设置成当前连接所采用的默认数据库。这个信息可以从线程对象（THD）中获得，因为每个线程对应了一个数据库连接，而每个数据库连接是针对一个具体的数据库的。
好了，经过上下文分析的编译阶段以后，我们就拥有了可以执行查询的SELECT_LEX_UNIT和SELECT_LEX对象。可是，你可能会注意到一个问题：为什么在语义分析阶段，MySQL没有做引用的消解呢？不要着急，接下来我就给你揭晓这个答案。
MySQL是如何做引用消解的？我们在SQL语句中，会用到数据库名、表名、列名、表的别名、列的别名等信息，编译器肯定也需要检查它们是不是正确的。这就是引用消解（或名称消解）的过程。一般编译器是在语义分析阶段来做这项工作的，而MySQL是在执行SQL命令的时候才做引用消解。
引用消解的入口是在SQL命令的的prepare()方法中，它会去检查表名、列名都对不对。
通过GDB调试工具，我们可以跟踪编译器做引用消解的过程。你可以在my_message_sql()函数处设个断点，然后写个SQL语句，故意使用错误的表名或者列名，来看看MySQL是在什么地方检查出这些错误的。
比如说，你可以执行“select * from fake_table”，其中的fake_table这个表，在数据库中其实并不存在。
下面是打印出的调用栈。你会注意到，MySQL在准备执行SQL语句的过程中，会试图去打开fake_table表，这个时候编译器就会发现这个表不存在。
你还可以再试一下“select fake_column from departments”这个语句，也一样会查出，fake_column并不是departments表中的一列。
那么，MySQL是如何知道哪些表和字段合法，哪些不合法的呢？
原来，它是通过查表的定义，也就是数据库模式信息，或者可以称为数据字典、元数据。MySQL在一个专门的库中，保存了所有的模式信息，包括库、表、字段、存储过程等定义。
你可以跟高级语言做一下类比。高级语言，比如说Java也会定义一些类型，类型中包含了成员变量。那么，MySQL中的表，就相当于高级语言的类型；而表的字段（或列）就相当于高级语言的类型中的成员变量。所以，在这个方面，MySQL和高级语言做引用消解的思路其实是一样的。
但是，高级语言在做引用消解的时候有作用域的概念，那么MySQL有没有类似的概念呢？
有的。举个例子，假设一个SQL语句带了子查询，那么子查询中既可以引用本查询块中的表和字段，也可以引用父查询中的表和字段。这个时候就存在了两个作用域，比如下面这个查询语句：
select dept_name from departments where dept_no in (select dept_no from dept_emp where dept_name != 'Sales' #引用了上一级作用域中的字段 group by dept_no having count(*)&amp;gt; 20000) 其中的dept_name字段是dept_emp表中所没有的，它其实是上一级作用域中departments表中的字段。</description></item><item><title>27_课前导读：学习现代语言设计的正确姿势</title><link>https://artisanbox.github.io/7/27/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/27/</guid><description>你好，我是宫文学。
到目前为止，你就已经学完了这门课程中前两个模块的所有内容了。在第一个模块“预备知识篇”，我带你梳理了编译原理的关键概念、算法等核心知识点，帮你建立了一个直观的编译原理基础知识体系；在第二个模块“真实编译器解析篇”，我带你探究了7个真实世界的编译器，让你对编译器所实际采用的各种编译技术都有所涉猎。那么在接下来的第三个模块，我会继续带你朝着提高编译原理实战能力的目标前进。这一次，我们从计算机语言设计的高度，来印证一下编译原理的核心知识点。
对于一门完整的语言来说，编译器只是其中的一部分。它通常还有两个重要的组成部分：一个是运行时，包括内存管理、并发机制、解释器等模块；还有一个是标准库，包含了一些标准的功能，如算术计算、字符串处理、文件读写，等等。
再进一步来看，我们在实现一门语言的时候，首先要做的，就是确定这门语言所要解决的问题是什么，也就是需求问题；其次，针对需要解决的问题，我们要选择合适的技术方案，而这些技术方案正是分别由编译器、运行时和标准库去实现的。
所以，从计算机语言设计的高度来印证编译原理知识，我们也能更容易理解编译器的任务，更容易理解它是如何跟运行时环境去做配合的，这也会让你进一步掌握编译技术。
好了，那接下来就一起来看看，到底用什么样的方式，我们才能真正理解计算机语言的设计思路。
首先，我们来聊一聊实现一门计算机语言的关键因素：需求和设计。
如何实现一门计算机语言？我们学习编译原理的一个重要的目标，就是能够实现一门计算机语言。这种语言可能是你熟悉的某些高级语言，也可能是某个领域、为了解决某个具体问题而设计的DSL。就像我们在第二个模块中见到的SQL，以及编译器在内部实现时用到的一些DSL，如Graal生成LIR时的模式匹配规则、Python编译器中的ASDL解析器，还有Go语言编译器中IR的重写规则等。
那么要如何实现一门优秀的语言呢？我们都知道，要实现一个软件，有两个因素是最重要的，一个是需求，一个是设计。计算机语言作为一种软件，具有清晰的需求和良好的设计，当然也是至关重要的。
我先来说说需求问题，也就是计算机语言要解决的问题。
这里你要先明确一件事，如果需求不清晰、目标不明确，那么想要实现这门语言其实是很难成功的。通常来说，我们不能指望任何一种语言是全能的，让它擅长解决所有类型的问题。所以，每一门语言都有其所要解决的针对性问题。
举个例子，JavaScript如果单从设计的角度来看，有很多细节值得推敲，有不少的“坑”，比如null、undefined和NaN几个值就很令人困惑，你知道“null==undefined”的值是true还是false吗？但是它所能解决的问题也非常清晰，就是作为浏览器的脚本语言，提供Web的交互功能。在这个方面，它比同时期诞生的其他竞争技术，如ActiveX和Java Applet，都更具优势，所以它才能胜出。
历史上的计算机语言，都是像JavaScript那样，在满足了那个时代的某个需求以后而流行起来的。其中，根据“硅谷创业之父”保罗·格雷厄姆（Paul Graham）在《黑客与画家》中的说法，这些语言往往是一个流行的系统的脚本。比如说，C语言是Unix系统的脚本，COBOL是大型机的脚本，SQL是数据库系统的脚本，JavaScript、Java和C#都是浏览器的脚本，Swift和Objective-C是苹果系统的脚本，Kotlin是Android的脚本。让一门语言成为某个流行的技术系统的脚本，为这个生态提供编程支持，就是一种定位很清晰的需求。
好，明确了语言的需求以后，我们再来说说设计问题。
设计是实现计算机语言这种软件所要做的技术选择。你已经看到，我们研究的不同语言，其实现技术都各有特点，分别服务于该语言的需求问题，或者说设计目标。
我还是拿JavaScript来举例子。JavaScript被设计成了一门解释执行的语言，这样的话，它就能很方便地嵌入到HTML文本中，随着HTML下载和执行，并且支持不同的客户端操作系统和浏览器平台。而如果是需要静态编译的语言，就没有这么方便。
再进一步，由于HTML下载完毕后，JavaScript就要马上执行，从而也对JavaScript的编译速度有了更高的要求，所以我们才会看到V8里面的那些有利于快速解析的技术，比如通过查表做词法分析、懒解析等。
另外，因为JavaScript早期只是在浏览器里做一些比较简单的工作，所以它一开始没有设计并发计算的能力。还有，由于每个页面运行的JavaScript都是单独的，并且在页面退出时就可以收回内存，因此JavaScript的垃圾收集功能也不需要太复杂。
作为对比，Go语言的设计主要是用来编写服务端程序的，那么它的关键特性也是与这个定位相适应。
并发：服务端的软件最重要的一项能力就是支持大量并发任务。Go在语言设计上把并发能力作为第一等级的语言要素。 垃圾收集：由于垃圾收集会造成整个应用程序停下来，所以Go语言特别重视降低由于垃圾收集而产生的停顿。 那么总结起来，我们要想成功地实现一门语言，要把握两个要点：第一，要弄清楚该语言的需求，也就是它要去解决的问题；第二，要确定合适的技术方案，来更好地解决它要面对的问题。
计算机语言的设计会涉及到比较多的内容，为了防止你在学习时抓不到重点，我在第三个模块里，挑了一些重点的内容来做讲解，比如前面提到的垃圾收集的特性等。我会以第二个模块所研究的多门语言和编译器作为素材，一起探讨一下，各门语言都是采用了什么样的技术方案来满足各自的设计目标的，从而让你对计算机语言设计所考虑的因素、编译技术如何跟其他相关技术打配合，形成一个宏观的认识。
“现代语言设计篇”都会讲哪些内容？这个模块的内容，我根据计算机语言的组成和设计中的关键点，将其分成了三个部分。
第一部分，是对各门语言的编译器的前端、中端和后端技术做一下对比和总结。
这样，通过梳理和总结，我们就可以找出各种编译器之间的异同点。对于其共同的部分，我们可以看作是一些最佳实践，你在自己的项目中可以大胆地采用；而有差异的部分，则往往是某种编译器为了实现该语言的设计目标而采用的技术策略，你可以去体会各门语言是如何做取舍的，这样也能变成你自己的经验储备。
第二部分，主要是对语言的运行时和标准库的实现技术做一个解析。
我们说过，一门语言要包括编译器、运行时和标准库。在学习第二个模块的时候，你应该已经有了一些体会，你能发现编译器的很多特性是跟语言的运行时密切相关的。比如，Python有自己独特的对象体系的设计，那么Python的字节码就体现了对这些对象的操作，字节码中的操作数都是对象的引用。
那么在这一部分，我就分为了几个话题来进行讲解：
第一，是对语言的运行时和标准库的宏观探讨。我们一起来看看不同的语言的运行时和它的编译器之间是如何互相影响的。另外，我还会和你探讨语言的基础功能和标准库的实现策略，这是非常值得探讨的知识点，它让一门语言具备了真正的实用价值。 第二，是垃圾收集机制。本课程分析、涉及的几种语言，它们所采用的垃圾收集机制都各不相同。那么，为什么一门语言会选择这个机制，而另一种语言会选择另一种机制呢？带着这样的问题所做的分析，会让你把垃圾收集方面的原理落到实践中去。 第三，是并发模型。对并发的支持，对现代语言来说也越来越重要。在后面的课程中，我会带你了解线程、协程、Actor三种并发模式，理解它们的优缺点，同时你也会了解到，如何在编译器和运行时中支持这些并发特性。 第三部分，是计算机语言设计上的4个高级话题。
第一，是元编程技术。元编程技术是一种对语言做扩展的技术，相当于能够定制一门语言，从而更好地解决特定领域的问题。Java语言的注解功能、Python的对象体系的设计，都体现了元编程功能。而Julia语言，更是集成了Lisp语言在元编程方面的强大能力。因此我会带你了解一下这些元编程技术的具体实现机制和特点，便于你去采纳和做好取舍。
第二，是泛型编程技术。泛型，或者说参数化类型，大大增强了现代语言的类型体系，使得很多计算逻辑的表达变得更简洁。最典型的应用就是容器类型，比如列表、树什么的，采用泛型技术实现的容器类型，能够方便地保存各种数据类型。像Java、C++和Julia等语言都支持泛型机制，但它们各自实现的技术又有所不同。我会带你了解这些不同实现技术背后的原因，以及各自的特点。
第三，是面向对象语言的实现机制。面向对象特性是当前很多主流语言都支持的特性。那么要在编译器和运行时上做哪些工作，来支持面向对象的特性呢？对象在内存里的表示都有哪些不同的方式？如何实现继承和多态的特性？为什么Java支持基础数据类型和对象类型，而有些语言中所有的数据都是对象？要在编译技术上做哪些工作来支持纯面向对象特性？这些问题，我会花一讲的时间来带你分析清楚，让你理解面向对象语言的底层机制。
第四，是函数式编程语言的实现机制。函数式编程这个范式出现得很早，不少人可能不太了解或者不太关注它，但最近几年出现了复兴的趋势。像Java等面向对象语言，也开始加入对函数式编程机制的支持。在第三个模块中，我会带你分析函数式编程的关键特征，比如函数作为一等公民、不变性等，并会一起探讨函数式编程语言实现上的一些关键技术，比如函数类型的内部表示、针对函数式编程特有的优化算法等，让你真正理解函数式编程语言的底层机制。
该模块的最后一讲，也是本课程的最后一讲，是对我们所学知识的一个综合检验。这个检验的题目，就是解析方舟编译器。
方舟编译器，应该是第一个引起国内IT界广泛关注的编译器。俗话说，外行看热闹，内行看门道。做一个编译器，到底有哪些关键的技术点？它们在方舟编译器里是如何体现的？我们在学习了编译原理的核心基础知识，在考察了多个编译器之后，应该能够有一定的能力去考察方舟编译器了。这也是学以致用、紧密结合实际的表现。通过这样的分析，你能了解到中国编译技术崛起的趋势，甚至还可能会思考如何参与到这个趋势中来。这一讲，我希望同学们都能发表自己的看法，而我的看法呢，只是一家之言，你作为参考就好了。
小结总结一下。咱们课程的名称是《编译原理实战课》，而最体现实战精神的，莫过于去实现一门计算机语言了。而在第三个模块，我就会带你解析实现一门计算机语言所要考虑的那些关键技术，并且通过学习，你也能够根据语言的设计目标来选择合适的技术方案。
从计算机语言设计的高度出发，这个模块会带你对编译原理形成更全面的认知，从而提高你把编译原理用于实战的能力。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>28_前端总结：语言设计也有人机工程学</title><link>https://artisanbox.github.io/7/28/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/28/</guid><description>你好，我是宫文学。
正如我在上一讲的“课程导读”中所提到的，在“现代语言设计篇”，我们会开始探讨现代语言设计中的一些典型特性，包括前端、中后端、运行时的特性等，并会研究它们与编译技术的关系。
今天这一讲，我先以前面的“真实编译器解析篇”所分析的7种编译器作为基础，来总结一下它们的前端技术的特征，为你以后的前端工作做好清晰的指引。
在此基础上，我们还会进一步讨论语言设计方面的问题。近些年，各种新语言都涌现出了一个显著特征，那就是越来越考虑对程序员的友好性，运用了人机工程的思维。比如说，自动类型推导、Null安全性等。那么在这里，我们就一起来分析一下，要支持这些友好的语法特征，在编译技术上都要做一些什么工作。
好，首先，我们就来总结一下各个编译器的前端技术特征。
前端编译技术总结通过前面课程中对7个编译器的解读分析，我们现在已经知道了，编译器的前端有一些共性的特征，包括：手写的词法分析器、自顶向下分析为主的语法分析器和差异化的语义分析功能。
手写的词法分析器我们分析的这几个编译器，全部都采用了手写的词法分析器。主要原因有几个：
第一，手写的词法分析实现起来比较简单，再加上每种语言的词法规则实际上是大同小异的，所以实现起来也都差不多。 第二，手写词法分析器便于做一些优化。典型的优化是把关键字作为标识符的子集来识别，而不用为识别每个关键字创建自动机。V8的词法分析器还在性能上做了调优，比如判断一个字符是否是合法的标识符字符，是采用了查表的方法，以空间换性能，提高了解析速度。 第三，手写词法分析器便于处理一些特殊的情况。在 MySQL的词法分析器中，我们会发现，它需要根据当前字符集来确定某个字符串是否是合法的Token。如果采用工具自动生成词法分析器，则不容易处理这种情况。 结论：如果你要实现词法分析器，可以参考这些编译器，来实现你自己手写的版本。
自顶向下分析为主的语法分析器在“解析篇”中，我们还见到了多个语法分析器。
手写 vs 工具生成
在前面解析的编译器当中，大部分都是手写的语法分析器，只有Python和MySQL这两个是用工具生成的。
一方面，手写实现能够在某些地方做一些优化的实现，比如在Java语言里，我们可以根据需要，预读一到多个Token。另外，手写实现也有利于编译错误的处理，这样可以尽量给用户提供更友好的编译错误信息，并且当一个地方发生错误以后，也能尽量不影响对后面的语句的解析。手写的语法分析器在这些方面都能提供更好的灵活性。
另一方面，Python和MySQL的编译器也证明了，用工具生成的语法分析器，也是完全可以用于高要求的产品之中的。所以，如果你的项目时间和资源有限，你要优先考虑用工具生成语法分析器。
自顶向下 vs 自底向上
我们知道，语法分析有两大算法体系。一是自顶向下，二是自底向上。
从我们分析过的7种编译器里可以发现，自顶向下的算法体系占了绝对的主流，只有MySQL的语法分析器，采用的是自底向上的LALR算法。
而在自顶向下的算法中，又几乎全是采用了递归下降算法，Java、JavaScript和Go三大语言的编译器都是如此。并且对于左递归这个技术点，我们用标准的改写方法就可以解决。
不过，我们还看到了自顶向下算法和自底向上算法的融合。Java语言和Go语言在处理二元表达式时，引入了运算符优先级解析器，从而避免了左递归问题，并且在处理优先级和结合性的问题上，也会更加容易。而运算符优先级解析器，实际上采用的是一种LR算法。
差异化的语义分析功能不同编译器的语义分析功能有其共性，那就是都要建立符号表、做引用消解。对于静态类型的语言来说，还一定要做类型检查。
语义分析最大的特点是上下文相关，AST加上这些上下文相关的关系，就从树变成了图。由于处理图的算法一般比较复杂，这就给引用消解带来了困难，因此我们在算法上必须采用一定的启发式规则，让算法简化。
比如，我们可以先把类型加入符号表，再去消解用到这些类型的地方：变量声明、方法声明、类继承的声明，等等。你还需要注意的是，在消解本地变量的时候，还必须一边消解，一边把本地变量加入符号表，这样才能避免形成错误的引用关系。
不过，在建立符号表，并做完引用消解以后，上下文相关导致的复杂性就被消除了。所以，后续的语义分析算法，我们仍然可以通过简单地遍历AST来实现。所以，你会看到这些编译器当中，大量的算法都是实现了Visitor模式。
另外，除了建立符号表、做引用消解和类型检查等语义分析功能，不同的编译器还要去处理自己特有的语义。比如说，Java编译器花了很多的工作量在处理语法糖上，还有对注解的处理上；Julia的编译器会去做类型推断；Python的编译器会去识别变量的作用域范围，等等。
这其中，很多的语义处理功能，都是为了支持更加友好的语言特性，比如Java的语法糖。在现代语言中，还增加了很多的特性，能够让程序员的编程工作更加容易。接下来，我就挑几个共性的特性，跟你一起探讨一下它们的实现。
支持友好的语言特性自动类型推导、Null安全性、通过语法糖提高语法的友好性，以及提供一些友好的词法规则，等等。这些都是现代语言努力提高其友好性的表现。
自动类型推导自动类型推导可以减少编程时与类型声明有关的工作量。我们来看看下面这几门语言，都是如何声明变量的。
C++语言是一门不断与时俱进的语言。在C++ 11中，采用了auto关键字做类型推导。比如：
int a = 10; auto b = a; //能够自动推导b的类型是int cout &amp;lt;&amp;lt; typeid(b).name() &amp;lt;&amp;lt; endl; //输出int 你可能会觉得，这看上去似乎也没啥呀，把int换成了auto好像并没有省多少事儿。但在下面这个例子中，你会发现用于枚举的变量的类型很长（std::vector&amp;lt;std::string&amp;gt;::iterator），那么你就大可以直接用一个auto来代替，省了很多事，代码也更加整洁。所以实际上，auto关键字也成为了在C++中使用枚举器的标准用法：
std::vector&amp;lt;std::string&amp;gt; vs; for(std::vector&amp;lt;std::string&amp;gt;::iterator i=vs.begin(); i!=vs.end();i++){ //... } //使用auto以后，简化为： fora(auto i=vs.begin(); i!=vs.end();i++){ //... } 我们接着来看看其他的语言，都是如何做类型推导的。
Kotlin中用var声明变量，也支持显式类型声明和类型推导两种方式。
var a : Int = 10; //显式声明 var b = 10; //类型推导 Go语言，会用“:=” 让编译器去做类型推导：</description></item><item><title>29_中端总结：不遗余力地进行代码优化</title><link>https://artisanbox.github.io/7/29/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/29/</guid><description>你好，我是宫文学。
今天这一讲，我继续带你来总结前面解析的7种真实的编译器中，中端部分的特征和编译技术。
在课程的第1讲，我也给你总结过编译器的中端的主要作用，就是实现各种优化。并且在中端实现的优化，基本上都是机器无关的。而优化是在IR上进行的。
所以，今天这一讲，我们主要来总结以下这两方面的问题：
第一，是对IR的总结。我在第6讲中曾经讲过，IR分为HIR、MIR和LIR三个层次，可以采用线性结构、图、树等多种数据结构。那么基于我们对实际编译器的研究，再一起来总结一下它们的IR的特点。 第二，是对优化算法的总结。在第7讲，我们把各种优化算法做了一个总体的梳理。现在就是时候，来总结一下编译器中的实际实现了。 通过今天的总结，你能够对中端的两大主题IR和优化，形成更加深入的理解，从而更有利于你熟练运用编译技术。
好了，我们先来把前面学到的IR的相关知识，来系统地梳理和印证一下吧。
对IR的总结通过对前面几个真实编译器的分析，我们会发现IR方面的几个重要特征：SSA已经成为主流；Sea of Nodes展现出令人瞩目的优势；另外，一个编译器中的IR，既要能表示抽象度比较高的操作，也要能表示抽象度比较低的、接近机器码的操作。
SSA成为主流通过学习前面的课程，我们会发现，符合SSA格式的IR成为了主流。Java和JavaScript的Sea of Nodes，是符合SSA的；Golang是符合SSA的；Julia自己的IR，虽然最早不是SSA格式的，但后来也改成了SSA；而Julia所采用的LLVM工具，其IR也是SSA格式的。
SSA意味着什么呢？源代码中的一个变量，会变成多个版本，每次赋值都形成一个新版本。在SSA中，它们都叫做一个值（Value），对变量的赋值就是对值的定义（def）。这个值定义出来之后，就可以在定义其他值的时候被使用（use），因此就形成了清晰的“使用-定义”链（use-def）。
这种清晰的use-def链会给优化算法提供很多的便利：
如果一个值定义了，但没有被使用，那就可以做死代码删除。 如果对某个值实现了常数折叠，那么顺着def-use链，我们就可以马上把该值替换成常数，从而实现常数传播。 如果两个值的定义是一样的，那么这两个值也一定是一样的，因此就可以去掉一个，从而实现公共子表达式消除；而如果不采取SSA，实现CSE（公共子表达式消除）需要做一个数据流分析，来确定表达式的变量值并没有发生变化。 针对最后一种情况，也就是公共子表达式消除，我再给你展开讲解一下，让你充分体会SSA和传统IR的区别。
我们知道，基于传统的IR，要做公共子表达式消除，就需要专门做一个“可用表达式”的分析。像下图展示的那样，每扫描一遍代码，就要往一个集合里增加一个可用的表达式。
为什么叫做可用表达式呢？因为变量可能被二次赋值，就像图中的变量c那样。在二次赋值以后，之前的表达式“c:=a+b”就不可用了。
图1：变量c二次赋值后，各个位置的可用表达式集合在后面，当替换公共子表达式的时候，我们可以把“e:=a+b”替换成“e:=d”，这样就可以少做一次计算，实现了优化的目标。
而如果采用SSA格式，上面这几行语句就可以改写为下图中的样子：
图2：用SSA格式的IR改写的程序可以看到，原来的变量c被替换成了c1和c2两个变量，而c1、d和e右边的表达式都是一样的，并且它们的值不会再发生变化。所以，我们可以马上消除掉这些公共子表达式，从而减少了两次计算，这就比采用SSA之前的优化效果更好了。最重要的是，整个过程根本不需要做数据流分析。
图3：把公共子表达式a+b消除掉好，在掌握了SSA格式的特点以后，我们还可以注意到，Java和JavaScript的两大编译器，在做优化时，竟然都不约而同地用到了Sea Of Nodes这种数据结构。它看起来非常重要，所以，我们再接着来总结一下，符合SSA格式的Sea of Nodes，都有什么特点。
Sea of Nodes的特点总结其实在解析Graal编译器的时候，我就提到过，Sea of Nodes的特点是把数据流图和控制流图合二为一，从而更容易实现全局优化。因为采用这种IR，代码并没有一开始就被限制在一个个的基本块中。直到最后生成LIR的环节，才会把图节点Schedule到各个基本块。作为对比，采用基于CFG的IR，优化算法需要让代码在基本块内部和基本块之间移动，处理起来会比较复杂。
在这里，我再带你把生成IR的过程推导一遍，你能从中体会到生成Sea of Nodes的思路，并且还会有一些惊喜的发现。
示例函数或方法是下面这样：
int foo(int b){ a = b; c = a + b; c = b; d = a + b; e = a + b; return e; } 那么，为它生成IR图的过程是怎么样的呢？</description></item><item><title>30_后端总结：充分发挥硬件的能力</title><link>https://artisanbox.github.io/7/30/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/30/</guid><description>你好，我是宫文学。
后端的工作，主要是针对各种不同架构的CPU来生成机器码。在第8讲，我已经对编译器在生成代码的过程中，所做的主要工作进行了简单的概述，你现在应该对编译器的后端工作有了一个大致的了解，也知道了后端工作中的关键算法包括指令选择、寄存器分配和指令排序（又叫做指令调度）。
那么今天这一讲，我们就借助在第二个模块中解析过的真实编译器，来总结、梳理一下各种编译器的后端技术，再来迭代提升一下原有的认知，并加深对以下这些问题的理解：
首先，在第8讲中，我只讲了指令选择的必要性，但对于如何实现指令选择等步骤，我并没有展开介绍。今天这一讲，我就会带你探索一下指令选择的相关算法。 其次，关于寄存器分配算法，我们探索过的好几个编译器，比如Graal、gc编译器等，采用的都是线性扫描算法，那么这个算法的原理是什么呢？我们一起来探究一下。 最后，我们再回到计算机语言设计的主线上来，一起分析一下不同编译器的后端设计，是如何跟该语言的设计目标相匹配的。 OK，我们先来了解一下指令选择的算法。
指令选择算法回顾一下，我们主要是在Graal和Go语言的编译器中，分析了与指令选择有关的算法。它们都采用了一种模式匹配的DSL，只要找到了符合模式的指令组合，编译器就生成一条低端的、对应于机器码的指令。
那为什么这种算法是有效的呢？这种算法的原理是什么呢？都有哪些不同的算法实现？接下来，我就给你揭晓一下答案。
我先给你举个例子。针对表达式“a[i]=b”，它是对数组a的第i个元素赋值。假设a是一个整数数组，那么地址的偏移量就是a+4*i，所以，这个赋值表达式用C语言可以写成“*(a+4*i)=b”，把它表达成AST的话，就是下图所示的样子。其中，赋值表达式的左子树的计算结果，是一个内存地址。
图1：a[i]=b的AST那么，我们要如何给这个表达式生成指令呢？
如果你熟悉x86汇编，你就会知道，上述语句可以非常简单地表达出来，因为x86的指令对数组寻址做了优化（参见第8讲的内容）。
不过，这里为了让你更容易理解算法的原理，我设计了一个新的指令集。这个指令集中的每条指令，都对应了一棵AST的子树，我们把它叫做模式树（Pattern Tree）。在有的算法里，它们也被叫做瓦片（Tiling）。对一个AST生成指令，就是用这样的模式树或瓦片来覆盖整个AST的过程。所以，这样的算法也叫做基于模式匹配的指令生成算法。
图2：指令集中的指令和对应的模式树你可以看到，在图2中，对于每棵模式树，它的根节点是这个指令产生的结果的存放位置。比如，Load_Const指令执行完毕以后，常数会被保存到一个寄存器里。这个寄存器，又可以作为上一级AST节点的操作数来使用。
图2中的指令包含：把常数和内存中的值加载到寄存器、加法运算、乘法运算等。其中有两个指令是特殊设计的，目的就是为了让你更容易理解接下来要探究的各种算法。
第一个指令是#4（Store_Offset），它把值保存到内存的时候，可以在目的地址上加一个偏移量。你可以认为这是为某些场景做的一个优化，比如你在对象地址上加一个偏移量，就能获得成员变量的地址，并把数值保存到这个地址上。
第二个指令是#9（Lea），它相当于x86指令集中的Lea指令，能够计算一个地址值，特别是能够利用间接寻址模式，计算出一个数组元素的地址。它能通过一条指令完成一个乘法计算和一个加法计算。如果你忘记了Lea指令，可以重新看看第8讲的内容。
基于上述的指令和模式树，我们就可以尝试来做一下模式匹配，从而选择出合适的指令。那么都可以采用什么样的算法呢？
第一个算法，是一种比较幼稚的算法。我们采取深度优先的后序遍历，也就是按照“左子节点-&amp;gt;右子节点-&amp;gt;父节点”的顺序遍历，针对每个节点去匹配上面的模式。
第1步，采用模式#2，把内存中a的值，也就是数组的地址，加载到寄存器。因为无论加减乘除等任何运算，都是可以拿寄存器作为操作数的，所以做这个决策是很安全的。 第2步，同上，采用模式#1，把常量4加载到寄存器。 第3步，采用模式#2，把内存中i的值加载到寄存器。 第4步，采用模式#8，把两个寄存器的值相乘，得到（4*i）的值。 第5步，采用模式#5，把两个寄存器的值相加，得到a+4*i的值，也就是a[i]的地址。 第6步，采用模式#2，把内存中b的值加载到寄存器。 第7步，采用模式#3，把寄存器中b的值写入a[i]的地址。 图3：用比较幼稚的算法做模式匹配最后形成的汇编代码是这样的：
Load_Mem a, R1 Load_Const 4, R2 Load_Mem i, R3 Mul_Reg R2, R3 Add_Reg R3, R1 Load_Mem b, R2 Store R2, (R1) 这种方法，是自底向上的做树的重写。它的优点是特别简单，缺点是性能比较差。它一共生成了7条指令，代价是19（3+1+3+4+1+3+4）。
在上述步骤中，我们能看到很多可以优化的地方。比如，4*i这个子表达式，我们是用了3条指令来实现的，总的Cost是1+3+4=8，而如果改成两条指令，也就是使用Mul_mem指令，就不用先把i加载到寄存器，Cost可以是1+6=7。
Load_Const 4, R1 Mul_Mem i, R1 第二种方法，是类似Graal编译器所采用的方法，自顶向下的做模式匹配。比如，当我们处理赋值节点的时候，算法会尽量匹配更多的子节点。因为一条指令包含的子节点越多，那么通过一条指令完成的操作就越多，从而总的Cost就更低。
所以，算法的大致步骤是这样的：
第1步，在#3和#4两个模式中做选择的话，选中了#4号。 第2步，沿着AST继续所深度遍历，其中+号节点第1步被处理掉了，所以现在处理变量a，采用了模式#2，把变量加载到寄存器。 第3步，处理*节点。这个时候要在#7和#8之间做对比，最后选择了#7，因为它可以包含更多的节点。 第4步，处理常量4。因为上级节点在这里需要一个寄存器作为操作数，所以我们采用了模式#1，把常量加载到寄存器。 第5步，处理变量b。这里也要把它加载到寄存器，因此采用了模式#2。 图4：Maximal Munch算法的匹配结果到此为止，我们用了5条指令就做完了所有的运算，生成的汇编代码是：</description></item><item><title>31_运行时（一）：从0到语言级的虚拟化</title><link>https://artisanbox.github.io/7/31/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/31/</guid><description>你好，我是宫文学。今天，我会带你去考察现代语言设计中的运行时特性，并讨论一下与标准库有关的话题。
你可能要问了，咱们这门课是要讲编译原理啊，为什么要学运行时呢。其实，对于一门语言来说，除了要提供编译器外，还必须提供运行时功能和标准库：一是，编译器生成的目标代码，需要运行时的帮助才能顺利运行；二是，我们写代码的时候，有一些标准的功能，像是读写文件的功能，自己实现起来太麻烦，或者根本不可能用这门语言本身来实现，这时就需要标准库的支持。
其实，我们也经常会接触到运行时和库，但可能只是停留在使用层面上，并不太会关注它们的原理等。如果真要细究起来、真要对编译原理有更透彻的理解的话，你可能就会有下面这些问题了：
到底什么是运行时？任何语言都有运行时吗？运行时和编译器是什么关系？ 什么是标准库？标准库和运行时库又是什么关系？库一般都包含什么功能？ 今天，我们就来探讨一下这些与运行时和标准库有关的话题。这样，你能更加充分地理解设计一门语言要完成哪些工作，以及这些工作跟编译技术又有什么关系，也就能对编译原理有更深一层的理解。
首先，我们来了解一下运行时，以及它和编译技术的关系。
什么是运行时（Runtime）？我们在第5讲说过，每种语言都有一个特定的执行模型（Execution Model）。而这个执行模型就需要运行时系统（Runtime System）的支持。我们把这种可以支撑程序运行的运行时系统，简称为运行时。
那运行时都包含什么功能呢？通常，我们最关心的是三方面的功能：程序运行机制、内存管理机制和并发机制。接下来，我就分别以Java、Python以及C、C++、Go语言的运行时机制为例，做一下运行时的分析，因为它们的使用者比较多，并且体现了一些有代表性的运行时特征。
Java的运行时我们先看看Java语言的运行时系统，也就是JVM。
其实，JVM不仅为Java提供了运行时环境，还为其他所有基于JVM的语言提供了支撑，包括Scala、Clojure、Groovy等。我们可以通过JVM的规范来学习一下它的主要特点。
第一，JVM规定了一套程序的运行机制。JVM支持基于字节码的解释执行机制，还包括了即时编译成机器码并执行的机制。
针对基于字节码的解释执行机制，JVM规范定义下面这些内容：
定义了一套字节码来运行程序。这些字节码能支持一些基本的运算。超出这些基本运算逻辑的，就要自己去实现。比如，idiv指令用于做整数的除法，当除数为零的时候，虚拟缺省的操作是抛出异常。如果你自己的语言是专注于数学计算的，想让整数除以零的结果为无穷大，那么你需要自己去实现这个逻辑。 规定了一套类型系统，包括基础数据类型、数组、引用类型等。所以说，任何运行在JVM上的语言，不管它设计的类型系统是什么样子，编译以后都会变成字节码规定的基础类型。 定义了class文件的结构。class文件规定了把某个类的符号表放在哪里、把字节码放在哪里，所以写编译器的时候要遵守这个规范才能生成正确的class文件。JVM在运行时会加载class文件并执行。 提供了一个基于栈的解释器，来解释执行字节码。编译器要根据这个执行模型来生成正确的字节码。 除了解释执行字节码的机制，JVM还支持即时编译成机器码并执行的机制。它可以调度多个编译器，生成不同优化级别的机器码，这就是分层编译机制。在需要的时候，还可以做逆优化，在不同版本的机器码以及解释执行模式之间做切换。
最后，Java程序之间的互相调用，需要遵循一定的调用约定或二进制标准，包括如何传参数等等。这也是运行机制的一部分。
总体来说，JVM代表了一种比较复杂的运行机制，既可以解释执行，又可以编译成机器码执行。V8的运行时机制也跟JVM也很类似。
第二，JVM对内存做了统一的管理。它把内存划分为程序计数器、虚拟机栈、堆、方法区、运行时常量池和本地方法栈等不同的区域。
对于栈来说，它的栈桢既可以服务于解释执行，又可以用于执行机器码，并且还可以在两种模式之间转换。在解释执行的时候，栈桢里会有一个操作数栈，服务于解释器。我们提到过OSR，也就是在运行一个方法的时候，把这个方法做即时编译，并且把它的栈桢从解释执行的状态切换成运行机器码的状态。而如果遇到逆优化的场景，栈桢又会从运行机器码的状态，切换成解释执行的状态。
对于堆来说，Java提供了垃圾收集器帮助进行内存的自动管理。减少整体的停顿时间，是垃圾收集器设计的重要目标。
第三，JVM封装了操作系统的线程模型，为应用程序提供了并发处理的机制。我会在讲并发机制的时候再展开。
以上就是JVM为运行在其上的任何程序提供的支撑了。在提供这些支撑的同时，运行时系统也给程序运行带来了一些限制。
第一，JVM实际上提供了一个基础的对象模型，JVM上的各种语言必须遵守。所以，虽然Clojure是一个函数式编程语言，但它在底层却不得不使用JVM规定的对象模型。
第二，基于JVM的语言程序要去调用C语言等生成的机器码的库，会比较难。不过，对于同样基于JVM的语言，则很容易实现相互之间的调用，因为它们底层都是类和字节码。
第三，在内存管理上，程序不能直接访问内存地址，也不能手动释放内存。
第四，在并发方面，JVM只提供了线程机制。如果你要使用其他并发模型，比如我们会在34讲中讲到的协程模型和35讲中的Actor模型，需要语言的实现者绕着弯去做，增加一些自己的运行时机制（我会在第34讲来具体介绍）。
好了，以上就是我要通过JVM的例子带你学习的Java的运行时，以及其编译器的影响了。我们再来看看Python的运行时。
Python的运行时在解析Python语言的时候，已经讲了Python的字节码和解释器，以及Python对象模型和程序调用的机制。这里，我再从程序运行机制、内存管理机制、并发机制这三个方面，给你梳理下。
第一，Python也提供了一套字节码，以及运行该字节码的解释器。这套字节码，也是跟Python的类型体系互相配合的。字节码中操作的那些标识符，都是Python的对象引用。
第二，在内存管理方面，Python也提供了自己的机制，包括对栈和堆的管理。
首先，我们看看栈。Python运行程序的时候，有些时候是运行机器码，比如内置函数，而有些时候是解释执行字节码。
运行机器码的时候，栈帧跟C语言程序的栈帧是没啥区别的。而在解释执行字节码的时候，栈帧里会包含一个操作数栈，这点跟JVM的栈机是一样的。如果你再进一步，去看看操作数栈的实现，会发现解释器本身主要就是一个C语言实现的函数，而操作数栈就是这个函数里用到的本地变量。因此操作数栈也会像其他本地变量一样，被优化成尽量使用物理寄存器，从而提高运行效率。这个知识点你要掌握，也就是说，栈桢中的操作数栈，其实是有可能基于物理寄存器的。
然后，Python还提供了对堆的管理机制。程序从堆里申请内存的时候，不是直接从操作系统申请，而是通过Python提供的一个Arena机制，使得内存的申请和释放更加高效、灵活。Python还提供了基于引用的垃圾收集机制（我会在下一讲为你总结垃圾收集机制）。
第三，是并发机制。Python把操作系统的线程进行了封装，让Python程序能支持基于线程的并发。同时，它也实现了协程机制（我会在34讲详细展开）。
好了，我们再继续看看第三类语言，也就是C、C++、Go这样的直接编译成二进制文件执行的语言的运行时。
C、C++、Go的运行时一个有意思的问题是，C语言有没有运行时呢？我们对C语言的印象，是一旦编译完成以后，就是一段完全可以自主运行的二进制代码了，你也可以看到输出的完整的汇编代码。除此之外没有其他，C语言似乎不需要运行时的支持。
所以，C语言最主要的运行时，实际上就是操作系统。C语言和现代的各种操作系统可以说是伴生关系，就像Java和JVM是伴生关系一样。所以，如果我们要深入使用C语言，某种意义上就是要深入了解操作系统的运行机制。
在程序执行机制方面，C语言编译完毕的程序是完全按照操作系统的运行机制来执行的。
在内存管理方面，C语言使用了操作系统提供的线程栈，操作系统能够自动帮助程序管理内存。程序也可以从堆里申请内存，但必须自己负责释放，没有自动内存管理机制。
在并发机制方面，当然也是直接用操作系统提供的线程机制。因为操作系统没有提供协程和Actor机制，所以C语言也没有提供这种并发机制。
不过有一个程序crt0.o，有时被称作是C语言的运行时。它是一段汇编代码（crt0.s），由链接器自动插入到程序里面，主要功能是在调用main函数之前做一些初始化工作，比如设置main函数的参数（argc和argv）、环境变量的地址、调用main函数、设置一些中断向量用于处理程序异常等。所以，这个所谓的运行时所做的工作也特别简单。
不同系统的crt0.s会不太一样，因为CPU架构和ABI是不同的。下面是一个crt0.s的示例代码：
.text .globl _start _start: # _start是链接器需要用到的入口 xor %ebp, %ebp # 让ebp置为0，标记栈帧的底部 mov (%rsp), %edi # 从栈里获得argc的值 lea 8(%rsp), %rsi # 从栈里获得argv的地址 lea 16(%rsp,%rdi,8), %rdx # 从栈里获得envp的地址 xor %eax, %eax # 按照ABI的要求把eax置为0，并与icc兼容 call main # 调用main函数，%edi, %rsi, %rdx是传给main函数的三个参数 mov %eax, %edi # 把main函数的返回值提供给_exit作为第一个参数 xor %eax, %eax # 按照ABI的要求把eax置为0，并与icc兼容 call _exit # 终止程序 可以说，C语言的运行时是一个极端，提供了最少的功能。反过来呢，这也就是给了程序员最大的自由度。C++语言的跟C是类似的，我就不再展开了。总的来说，它们都没有Java和Python那种意义上的运行时。</description></item><item><title>32_运行时（二）：垃圾收集与语言的特性有关吗？</title><link>https://artisanbox.github.io/7/32/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/32/</guid><description>你好，我是宫文学。今天，我们继续一起学习垃圾收集的实现机制以及与编译器的关系。
对于一门语言来说，垃圾收集机制能够自动管理从堆中申请的内存，从而大大降低程序员的负担。在这门课的第二大模块“真实编译器解析篇”中，我们学习Java、Python、Go、Julia和JavaScript这几门语言，都有垃圾收集机制。那在今天这一讲，我们就来学习一下，这些语言的垃圾收集机制到底有什么不同，跟语言特性的设计又是什么关系，以及编译器又是如何配合垃圾收集机制的。
这样如果我们以后要设计一门语言的话，也能清楚如何选择合适的垃圾收集机制，以及如何让编译器来配合选定的垃圾收集机制。
在讨论不同语言的垃圾收集机制之前，我们还是需要先了解一下，通常我们都会用到哪些垃圾收集算法，以及它们都有什么特点。这样，我们才能深入探讨应该在什么时候采用什么算法。如果你对各种垃圾收集算法已经很熟悉了，也可以从这一讲的“Python与引用计数算法”开始学习；如果你还想理解垃圾收集算法的更多细节，也可以去看看我的第一季课程《编译原理之美》的第33讲的内容。
垃圾收集算法概述垃圾收集主要有标记-清除（Mark and Sweep）、标记-整理（Mark and Compact）、停止-拷贝（Stop and Copy）、引用计数、分代收集、增量收集和并发收集等不同的算法，在这里我简要地和你介绍一下。
首先，我们先学习一下什么是内存垃圾。内存垃圾，其实就是一些保存在堆里的、已经无法从程序里访问的对象。
我们看一个具体的例子。
在堆中申请一块内存时（比如Java中的对象实例），我们会用一个变量指向这块内存。但是，如果给变量赋予一个新的地址，或者当栈桢弹出时，该栈桢的变量全部失效，这时，变量所指向的内存就没用了（如图中的灰色块）。
图1：A是内存垃圾另外，如果A对象有一个成员变量指向C对象，那么A不可达，C也会不可达，也就失效了。但D对象除了被A引用，还被B引用，仍然是可达的。
图2：A和C是内存垃圾那么，所有不可达的内存就是垃圾。所以，垃圾收集的重点就是找到并清除这些垃圾。接下来，我们就看看不同的算法是怎么完成这个任务的。
标记-清除标记-清除算法，是从GC根节点出发，顺着对象的引用关系，依次标记可达的对象。这里说的GC根节点，包括全局变量、常量、栈里的本地变量、寄存器里的本地变量等。从它们出发，就可以找到所有有用的对象。那么剩下的对象，就是内存垃圾，可以清除掉。
标记-整理采用标记-清除算法，运行时间长了以后，会形成内存碎片。这样在申请内存的时候，可能会失败。
图3：内存碎片导致内存申请失败为了避免内存碎片，你可以采用变化后的算法，也就是标记-整理算法：在做完标记以后，做一下内存的整理，让存活的对象都移动到一边，消除掉内存碎片。
图4：内存整理以后，可以更有效地利用内存停止-拷贝停止和拷贝算法把内存分为新旧空间两部分。你需要保持一个堆指针，指向自由空间开始的位置。申请内存时，把堆指针往右移动就行了。
图5：在旧空间中申请内存当旧空间内存不够了以后，就会触发垃圾收集。在收集时，会把可达的对象拷贝到新空间，然后把新旧空间互换。
图6：新旧空间互换停止-拷贝算法，在分配内存的时候，不需要去查找一块合适的空闲内存；在垃圾收集完毕以后，也不需要做内存整理，因此速度是最快的。但它也有缺点，就是总有一半内存是闲置的。
引用计数引用计数方法，是在对象里保存该对象被引用的数量。一旦这个引用数为零，那么就可以作为垃圾被收集走。
有时候，我们会把引用计数叫做自动引用计数（ARC），并把它作为跟垃圾收集（GC）相对立的一个概念。所以，如果你读到相关的文章，它把ARC和GC做了对比，也不要吃惊。
引用计数实现起来简单，并且可以边运行边做垃圾收集，不需要为了垃圾收集而专门停下程序。可是，它也有缺陷，就是不能处理循环引用（Reference Cycles）的情况。在下图中，四个对象循环引用，但没有GC根指向它们。它们已经是垃圾，但计数却都为1。
图7：循环引用另外，由于在程序引用一个对象的前后，都要修改引用计数，并且还有多线程竞争的可能性，所以引用计数法的性能开销比较大。
分代收集在程序中，新创建的对象往往会很快死去，比如，你在一个方法中，使用临时变量指向一些新创建的对象，这些对象大多数在退出方法时，就没用了。这些数据叫做新生代。而如果一个对象被扫描多次，发现它还没有成为垃圾，那就会标记它为比较老的时代。这些对象可能Java里的静态数据成员，或者调用栈里比较靠近根部的方法所引用的，不会很快成为垃圾。
对于新生代对象，可以更频繁地回收。而对于老一代的对象，则回收频率可以低一些。并且，对于不同世代的对象，还可以用不同的回收方法。比如，新生代比较适合复制式收集算法，因为大部分对象会被收集掉，剩下来的不多；而老一代的对象生存周期比较长，拷贝的话代价太大，比较适合标记-清除算法，或者标记-整理算法。
增量收集和并发收集垃圾收集算法在运行时，通常会把程序停下。因为在垃圾收集的过程中，如果程序继续运行，可能会出错。这种停下整个程序的现象，被形象地称作“停下整个世界（STW）”。
可是让程序停下来，会导致系统卡顿，用户的体验感会很不好。一些对实时性要求比较高的系统，根本不可能忍受这种停顿。
所以，在自动内存管理领域的一个研究的重点，就是如何缩短这种停顿时间。增量收集和并发收集算法，就是在这方面的有益探索：
增量收集可以每次只完成部分收集工作，没必要一次把活干完，从而减少停顿。 并发收集就是在不影响程序执行的情况下，并发地执行垃圾收集工作。 好了，理解了垃圾收集算法的核心原理以后，我们就可以继续去探索各门语言是怎么运用这些算法的了。
首先，我们从Python的垃圾收集算法学起。
Python与引用计数算法Python语言选择的是引用计数的算法。除此之外，Swift语言和方舟编译器，采用的也是引用计数，所以值得我们重视。
Python的内存管理和垃圾收集机制首先我们来复习一下Python内存管理的特征。在Python里，每个数据都是对象，而这些对象又都是在堆上申请的。对比一下，在C和Java这样的语言里，很多计算可以用本地变量实现，而本地变量是在栈上申请的。这样，你用到一个整数的时候，只占用4个字节，而不像Python那样有一个对象头的固定开销。栈的优势还包括：不会产生内存碎片，数据的局部性又好，申请和释放的速度又超快。而在堆里申请太多的小对象，则会走向完全的反面：太多次系统调用，性能开销大；产生内存碎片；数据的局部性也比较差。
所以说，Python的内存管理方案，就决定了它的内存占用大、性能低。这是Python内存管理的短板。而为了稍微改善一下这个短板，Python采用了一套基于区域（Region-based）的内存管理方法，能够让小块的内存管理更高效。简单地说，就是Python每次都申请一大块内存，这一大块内存叫做Arena。当需要较小的内存的时候，直接从Arena里划拨就好了，不用一次次地去操作系统申请。当用垃圾回收算法回收内存时，也不一定马上归还给操作系统，而是归还到Arena里，然后被循环使用。这个策略能在一定程度上提高内存申请的效率，并且减少内存碎片化。
接下来，我们就看看Python是如何做垃圾回收的。回忆一下，在第19讲分析Python的运行时机制时，其中提到了一些垃圾回收的线索。Python里每个对象都是一个PyObject，每个PyObject都有一个ob_refcnt字段用于记录被引用的数量。
在解释器执行字节码的时候，会根据不同的指令自动增加或者减少ob_refcnt的值。当一个PyObject对象的ob_refcnt的值为0的时候，意味着没有任何一个变量引用它，可以立即释放掉，回收对象所占用的内存。
现在你已经知道，采用引用计数方法，需要解决循环引用的问题。那Python是如何实现的呢？
Python在gc模块里提供了一个循环检测算法。接下来我们通过一个示例，来看看这个算法的原理。在这个例子中，有一个变量指向对象A。你能用肉眼看出，对象A、B、C不是垃圾，而D和E是垃圾。
图8：把容器对象加入待扫描列表在循环检测算法里，gc使用了两个列表。一个列表保存所有待扫描的对象，另一个列表保存可能的垃圾对象。注意，这个算法只检测容器对象，比如列表、用户自定义的类的实例等。而像整数对象这样的，就不用检测了，因为它们不可能持有对其他对象的引用，也就不会造成循环引用。
在这个算法里，我们首先让一个gc_ref变量等于对象的引用数。接着，算法假装去掉对象之间的引用。比如，去掉从A到B的引用，这使得B对象的gc_ref值变为了0。在遍历完整个列表以后，除了A对象以外，其他对象的gc_ref都变成了0。
图9：扫描列表，修改gc_ref的值gc_ref等于零的对象，有的可能是垃圾对象，比如D和E；但也有些可能不是，比如B和C。那要怎么区分呢？我们先把这些对象都挪到另一个列表中，怀疑它们可能是垃圾。
图10：认为gc_ref为0的对象可能是垃圾这个时候，待扫描对象区只剩下了对象A。它的gc_ref是大于零的，也就是从gc根是可到达的，因此肯定不是垃圾对象。那么顺着这个对象所直接引用和间接引用到的对象，也都不是垃圾。而剩下的对象，都是从gc根不可到达的，也就是真正的内存垃圾。
图11：去除其中可达的对象，剩下的是真正的垃圾另外，基于循环检测的垃圾回收算法是定期执行的，这会跟Java等语言的垃圾收集器一样，导致系统的停顿。所以，它也会像Java等语言的垃圾收集器一样，采用分代收集的策略来减少垃圾收集的工作量，以及由于垃圾收集导致的停顿。
好了，以上就是Python的垃圾收集算法。我们前面提过，除了Python以外，Swift和方舟编译器也使用了引用计数算法。另外，还有些分代的垃圾收集器，在处理老一代的对象时，也会采用引用计数的方法，这样就可以在引用计数为零的时候收回内存，而不需要一遍遍地扫描。
编译器如何配合引用计数算法？对于Python来说，引用计数的增加和减少，是由运行时来负责的，编译器并不需要做额外的工作。它只需要生成字节码就行了。而对于Python的解释器来说，在把一个对象赋值给一个变量的时候，要把对象的引用数加1；而当该变量超出作用域的时候，要把对象的引用数减1。
不过，对于编译成机器码的语言来说，就要由编译器介入了。它要负责生成相应的指令，来做引用数的增减。
不过，这只是高度简化的描述。实际实现时，还要解决很多细致的问题。比如，在多线程的环境下，对引用数的改变，必须要用到锁，防止超过一个线程同时修改引用数。这种频繁地对锁的使用，会导致性能的降低。这时候，我们之前学过的一些优化算法就可以派上用场了。比如，编译器可以做一下逃逸分析，对于没有逃逸或者只是参数逃逸的对象，就可以不使用锁，因为这些对象不可能被多个线程访问。这样就可以提高程序的性能。
除了通过逃逸分析优化对锁的使用，编译器还可以进一步优化。比如，在一段程序中，一个对象指针被调用者通过参数传递给某个函数使用。在函数调用期间，由于调用者是引用着这个对象的，所以这个对象不会成为垃圾。而这个时候，就可以省略掉进入和退出函数时给对象引用数做增减的代码。
还有不少类似上面的情况，需要编译器配合垃圾收集机制，生成高效的、正确的代码。你在研究Swift和方舟编译器时，可以多关注一下它们对引用计数做了哪些优化。
接下来，我们再看看其他语言是怎么做垃圾收集的。
其他语言是怎么做垃圾收集的？除了Python以外，我们在第二个模块研究的其他几门语言，包括Java、JavaScript（V8）和Julia，都没有采用引用计数算法（除了在分代算法中针对老一代的对象），它们基本都采用了分代收集的策略。针对新生代，通常是采用标记-清除或者停止拷贝算法。
它们不采用引用计数的原因，其实我们可以先猜测一下，那就是因为引用计数的缺点。比如增减引用计数所导致的计算量比较多，在多线程的情况下要用到锁，就更是如此；再比如会导致内存碎片化、局部性差等。
而采用像停止-拷贝这样的算法，在总的计算开销上会比引用计数的方法低。Java和Go语言主要是用于服务端程序开发的。尽量减少内存收集带来的性能损耗，当然是语言的设计者重点考虑的问题。
再进一步看，采用像停止-拷贝这样的算法，其实是用空间换时间，以更大的内存消耗换来性能的提升。如果你的程序需要100M内存，那么虚拟机需要给它准备200M内存，因为有一半空间是空着的。这其实也是为什么Android手机比iPhone更加消耗内存的原因之一。
在为iPhone开发程序的时候，无论是采用Objective C还是Swift，都是采用引用计数的技术。并且，程序员还负责采用弱引用等技术，来避免循环引用，从而进一步消除了在运行时进行循环引用检测的开销。
通过上面的分析，我们能发现移动端应用和服务端应用有不同的特点，因此也会导致采用不同的垃圾收集算法。那么方舟编译器采用引用计数的方法，来编译原来的Android应用，是否也是借鉴了iPhone的经验呢？我没有去求证过，所以不得而知。但我们可以根据自己的知识去做一些合理的猜测。
好，回过头来，我们继续分析一下用Java和Go语言来写服务端程序对垃圾收集的需求。对于服务器端程序来说，垃圾收集导致的停顿，是一个令程序员们头痛的问题。有时候，一次垃圾收集会让整个程序停顿一段非常可观的时间（比如上百毫秒，甚至达到秒级），这对于实时性要求较高或并发量较大的系统来说，就会引起很大的问题。也因此，一些很关键的系统很长时间内无法采用Java和Go语言编写。
所以，Java和Go语言一直在致力于减少由于垃圾收集而产生的停顿。最新的垃圾收集器，已经使得垃圾收集导致的停顿降低到了几毫秒内。
在这里，你需要理解的要点，是为什么在垃圾收集时，要停下整个程序？又有什么办法可以减少停顿的时间？
为什么在垃圾收集时，要停下整个程序？其实，对于引用计数算法来说，是不需要停下整个程序的，每个对象的内存在计数为零的时候就可以收回。
而采用标记-清除算法时，你就必须要停下程序：首先做标记，然后做清除。在做标记的时候，你必须从所有的GC根出发，去找到所有存活的对象，剩下的才是垃圾。所以，看上去，这是一项完整的工作，程序要一直停顿到这项完整的工作做完。
让事情更棘手的是，你不仅要停下当前的线程，扫描栈里的所有GC根，你还要停下其他的线程，因为其他线程栈里的对象，也可能引用了相同的对象。最后的结果，就是你停下了整个世界。</description></item><item><title>33_并发中的编译技术（一）：如何从语言层面支持线程？</title><link>https://artisanbox.github.io/7/33/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/33/</guid><description>你好，我是宫文学。
现代的编程语言，开始越来越多地采用并发计算的模式。这也对语言的设计和编译技术提出了要求，需要能够更方便地利用计算机的多核处理能力。
并发计算需求的增长跟两个趋势有关：一是，CPU在制程上的挑战越来越大，逼近物理极限，主频提升也越来越慢，计算能力的提升主要靠核数的增加，比如现在的手机，核数越来越多，动不动就8核、12核，用于服务器的CPU核数则更多；二是，现代应用对并发处理的需求越来越高，云计算、人工智能、大数据和5G都会吃掉大量的计算量。
因此，在现代语言中，友好的并发处理能力是一项重要特性，也就需要编译技术进行相应的配合。现代计算机语言采用了多种并发技术，包括线程、协程、Actor模式等。我会用三讲来带你了解它们，从而理解编译技术要如何与这些并发计算模式相配合。
这一讲，我们重点探讨线程模式，它是现代计算机语言中支持并发的基础模式。它也是讨论协程和Actor等其他话题的基础。
不过在此之前，我们需要先了解一下并发计算的一点底层机制：并行与并发、进程和线程。
并发的底层机制：并行与并发、进程与线程我们先来学习一下硬件层面对并行计算的支持。
假设你的计算机有两颗CPU，每颗CPU有两个内核，那么在同一时间，至少可以有4个程序同时运行。
后来CPU厂商又发明了超线程（Hyper Threading）技术，让一个内核可以同时执行两个线程，增加对CPU内部功能单元的利用率，这有点像我们之前讲过的流水线技术。这样一来，在操作系统里就可以虚拟出8个内核（或者叫做操作系统线程），在同一时间可以有8个程序同时运行。这种真正的同时运行，我们叫做并行（parallelism）。
图1：虚拟内核与CPU真实内核的对应关系可是仅仅8路并行，也不够用呀。如果你去查看一下自己电脑里的进程数，会发现运行着几十个进程，而线程数就更多了。
所以，操作系统会用分时技术，让一个程序执行一段时间，停下来，再让另一个程序运行。由于时间片切得很短，对于每一个程序来说，感觉上似乎一直在运行。这种“同时”能处理多个任务，但实际上并不一定是真正同时执行的，就叫做并发（Concurrency）。
实际上，哪怕我们的计算机只有一个内核，我们也可以实现多个任务的并发执行。这通常是由操作系统的一个调度程序（Scheduler）来实现的。但是有一点，操作系统在调度多个任务的时候，是有一定开销的：
一开始是以进程为单位来做调度，开销比较大。 在切换进程的时候，要保存当前进程的上下文，加载下一个进程的上下文，也会有一定的开销。由于进程是一个比较大的单位，其上下文的信息也比较多，包括用户级上下文（程序代码、静态数据、用户堆栈等）、寄存器上下文（各种寄存器的值）和系统级上下文（操作系统中与该进程有关的信息，包括进程控制块、内存管理信息、内核栈等）。 相比于进程，线程技术就要轻量级一些。在一个进程内部，可以有多个线程，每个线程都共享进程的资源，包括内存资源（代码、静态数据、堆）、操作系统资源（如文件描述符、网络连接等）和安全属性（用户ID等），但拥有自己的栈和寄存器资源。这样一来，线程的上下文包含的信息比较少，所以切换起来开销就比较小，可以把宝贵的CPU时间用于执行用户的任务。
总结起来，线程是操作系统做并发调度的基本单位，并且可以跟同一个进程内的其他线程共享内存等资源。操作系统会让一个线程运行一段时间，然后把它停下来，把它所使用的寄存器保存起来，接着让另一个线程运行，这就是线程调度原理。你要在大脑里记下这个场景，这样对理解后面所探讨的所有并发技术都很有帮助。
图2：进程的共享资源和线程私有的资源我们通常把进程作为资源分配的基本单元，而把线程作为并发执行的基本单元。不过，有的时候，用进程作为并发的单元也是比较好的，比如谷歌浏览器每打开一个Tab页，就新启动一个进程。这是因为，浏览器中多个进程之间不需要有互动。并且，由于各个进程所使用的资源是独立的，所以一个进程崩溃也不会影响到另一个。
而如果采用线程模型的话，由于它比较轻量级，消耗的资源比较少，所以你可以在一个操作系统上启动几千个线程，这样就能执行更多的并发任务。所以，在一般的网络编程模型中，我们可以针对每个网络连接，都启动一条线程来处理该网络连接上的请求。在第二个模块中我们分析过的MySQL就是这样做的。你每次跟MySQL建立连接，它就会启动一条线程来响应你的查询请求。
采用线程模型的话，程序就可以在不同线程之间共享数据。比如，在数据库系统中，如果一个客户端提交了一条SQL，那么这个SQL的编译结果可以被缓存起来。如果另一个用户恰好也执行了同一个SQL，那么就可以不用再编译一遍，因为两条线程可以访问共享的内存。
但是共享内存也会带来一些问题。当多个线程访问同样的数据的时候，会出现数据处理的错误。如果使用并发程序会造成错误，那当然不是我们所希望的。所以，我们就要采用一定的技术去消除这些错误。
Java语言内置的并发模型就是线程模型，并且在语法层面为线程模型提供了一些原生的支持。所以接下来，我们先借助Java语言去了解一下，如何用编译技术来配合线程模型。
Java的并发机制Java从语言层面上对并发编程提供了支持，简化了程序的开发。
Java对操作系统的线程进行了封装，程序员使用Thread类或者让一个类实现Runnable接口，就可以作为一个线程运行。Thread类提供了一些方法，能够控制线程的运行，并能够在多个线程之间协作。
从语法角度，与并发有关的关键字有synchronized和volatile。它们就是用于解决多个线程访问共享内存的难题。
synchronized关键字：保证操作的原子性我们通过一个例子，来看看多个线程访问共享数据的时候，为什么会导致数据错误。
public class TestThread { public static void main(String[] args) { Num num = new Num(); for (int i = 0; i &amp;amp;lt; 3; i++) { new NewThread(num).start(); } } }
//线程类NewThread 对数字进行操作 class NewThread extends Thread { private Num num;</description></item><item><title>34_并发中的编译技术（二）：如何从语言层面支持协程？</title><link>https://artisanbox.github.io/7/34/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/34/</guid><description>你好，我是宫文学。
上一讲我们提到了线程模式是当前计算机语言支持并发的主要方式。
不过，在有些情况下，线程模式并不能满足要求。当需要运行大量并发任务的时候，线程消耗的内存、线程上下文切换的开销都太大。这就限制了程序所能支持的并发任务的数量。
在这个背景下，一个很“古老”的技术重新焕发了青春，这就是协程（Coroutine）。它能以非常低的代价、友好的编程方式支持大量的并发任务。像Go、Python、Kotlin、C#等语言都提供了对协程的支持。
今天这一讲，我们就来探究一下如何在计算机语言中支持协程的奇妙功能，它与编译技术又是怎样衔接的。
首先，我们来认识一下协程。
协程（Coroutine）的特点与使用场景我说协程“古老”，是因为这个概念是在1958年被马尔文 · 康威（Melvin Conway）提出来、在20世纪60年代又被高德纳（Donald Ervin Knuth）总结为两种子过程（Subroutine）的模式之一。一种是我们常见的函数调用的方式，而另一种就是协程。在当时，计算机的性能很低，完全没有现代的多核计算机。而采用协程就能够在这样低的配置上实现并发计算，可见它是多么的轻量级。
有的时候，协程又可能被称作绿色线程、纤程等，所采用的技术也各有不同。但总的来说，它们都有一些共同点。
首先，协程占用的资源非常少。你可以在自己的笔记本电脑上随意启动几十万个协程，而如果你启动的是几十万个线程，那结果就是不可想象的。比如，在JVM中，缺省会为每个线程分配1MB的内存，用于线程栈等。这样的话，几千个线程就要消耗掉几个GB的内存，而几十万个线程理论上需要消耗几百GB的内存，这还没算程序在堆中需要申请的内存。当然，由于底层操作系统和Java应用服务器的限制，你也无法启动这么多线程。
其次，协程是用户自己的程序所控制的并发。也就是说，协程模式，一般是程序交出运行权，之后又被另外的程序唤起继续执行，整个过程完全是由用户程序自己控制的。而线程模式就完全不同了，它是由操作系统中的调度器（Scheduler）来控制的。
我们看个Python的例子：
def running_avg(): total = 0.0 count = 0 avg = 0 while True: num = yield avg total += num count += 1 avg = total/count #生成协程，不会有任何输出 ra = running_avg() #运行到yield next(ra)
print(ra.send(2))
print(ra.send(3))
print(ra.send(4)) print(ra.send(7))
print(ra.send(9))
print(ra.send(11))
#关掉协程 ra.close 可以看到，使用协程跟我们平常使用函数几乎没啥差别，对编程人员很友好。实际上，它可以认为是跟函数并列的一种子程序形式。和函数的区别是，函数调用时，调用者跟被调用者之间像是一种上下级的关系；而在协程中，调用者跟被调用者更像是互相协作的关系，比如一个是生产者，一个是消费者。这也是“协程”这个名字直观反映出来的含义。
我们用一张图来对比下函数和协程中的调用关系。
图1：函数（左）与协程（右）的控制流细想一下，编程的时候，这种需要子程序之间互相协作的场景有很多，我们一起看两种比较常见的场景。
第一种比较典型的场景，就是生产者和消费者模式。如果你用过Unix管道或者消息队列编程的话，会非常熟悉这种模式。但那是在多个进程之间的协作。如果用协程的话，在一个进程内部就能实现这种协作，非常轻量级。
就拿编译器前端来说，词法分析器（Tokenizer）和语法分析器（Parser）就可以是这样的协作关系。也就是说，为了更好的性能，没有必要一次把词法分析完毕，而是语法分析器消费一个，就让词法分析器生产一个。因此，这个过程就没有必要做成两个线程了，否则就太重量级了。这种场景，我们可以叫做生成器（Generator）场景：主程序调用生成器，给自己提供数据。
特别适合使用协程的第二种场景是IO密集型的应用。比如做一个网络爬虫同时执行很多下载任务，或者做一个服务器同时响应很多客户端的请求，这样的任务大部分时间是在等待网络传输。
如果用同步阻塞的方式来做，一个下载任务在等待的时候就会把整个线程阻塞掉。而用异步的方式，协程在发起请求之后就把控制权交出，调度程序接收到数据之后再重新激活协程，这样就能高效地完成IO操作，同时看上去又是用同步的方式编程，不需要像异步编程那样写一大堆难以阅读的回调逻辑。
这样的场景在微服务架构的应用中很常见，我们来简化一个实际应用场景，分析下如何使用协程。
在下面的示例中，应用A从客户端接收大量的并发请求，而应用A需要访问应用B的服务接口，从中获得一些信息，然后返回给客户端。
图2：应用间通讯的场景要满足这样的场景，我们最容易想到的就是，编写同步通讯的程序，其实就是同步调用。
假设应用A对于每一个客户端的请求，都会起一个线程做处理。而你呢，则在这个线程里发起一个针对应用B的请求。在等待网络返回结果的时候，当前线程会被阻塞住。
图3：采用同步编程实现应用间的通讯这个架构是最简单的，你如果采用Java的Servlet容器来编写程序的话，很可能会采用这个结构。但它有一些缺陷：</description></item><item><title>35_并发中的编译技术（三）：Erlang语言厉害在哪里？</title><link>https://artisanbox.github.io/7/35/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/35/</guid><description>你好，我是宫文学。
在前面两讲，我们讨论了各门语言支持的并发计算的模型。线程比进程更加轻量级，上下文切换成本更低；协程则比线程更加轻量级，在一台计算机中可以轻易启动几十万、上百万个并发任务。
但不论是线程模型、还是协程模型，当涉及到多个线程访问共享数据的时候，都会出现竞争问题，从而需要用到锁。锁会让其他需要访问该数据的线程等待，从而导致系统整体处理能力的降低。
并且，编程人员还要特别注意，避免出现死锁。比如，线程A持有了锁x，并且想要获得锁y；而线程B持有了锁y，想要获得锁x，结果这两个线程就会互相等待，谁也进行不下去。像数据库这样的系统，检测和消除死锁是一项重要的功能，以防止互相等待的线程越来越多，对数据库操作不响应，并最终崩溃掉。
既然使用锁这么麻烦，那在并发计算中，能否不使用锁呢？这就出现了Actor模型。那么，什么是Actor模型？为什么它可以不用锁就实现并发？这个并发模型有什么特点？需要编译技术做什么配合？
今天这一讲，我们就从这几个问题出发，一起学习并理解Actor模型。借此，我们也可以把用编译技术支持不同的并发模型的机制，理解得更深刻。
首先，我们看一下什么是Actor模型。
什么是Actor模型？在线程和协程模型中，之所以用到锁，是因为两个线程共享了内存，而它们会去修改同一个变量的值。那，如果避免共享内存，是不是就可以消除这个问题了呢？
没错，这就是Actor模型的特点。Actor模型是1973年由Carl Hewitt提出的。在Actor模型中，并发的程序之间是不共享内存的。它们通过互相发消息来实现协作，很多个一起协作的Actor就构成了一个支持并发计算的系统。
我们看一个有三个Actor的例子。
图1：三个Actor的例子你会注意到，每个Actor都有一个邮箱，用来接收其他Actor发来的消息；每个Actor也都可以给其他Actor发送消息。这就是Actor之间交互的方式。Actor A给Actor B发完消息后就返回，并不会等着Actor B处理完毕，所以它们之间的交互是异步的。如果Actor B要把结果返回给A，也是通过发送消息的方式。
这就是Actor大致的工作原理了。因为Actor之间只是互发消息，没有共享的变量，当然也就不需要用到锁了。
但是，你可能会问：如果不共享内存，能解决传统上需要对资源做竞争性访问的需求吗？比如，卖电影票、卖火车票、秒杀或者转账的场景。我们以卖电影票为例讲解一下。
在用传统的线程或者协程来实现卖电影票功能的时候，对票的状态进行修改，需要用锁的机制实现同步互斥，以保证同一个时间段只有一个线程可以去修改票的状态、把它分配给某个用户，从而避免多个线程同时访问而出现一张票卖给多个人的情况。这种情况下，多个程序是串行执行的，所以系统的性能就很差。
如果用Actor模式会怎样呢？
你可以把电影院的前半个场地和后半个场地的票分别由Actor B和 C负责销售：Actor A在接收到定前半场座位的请求的时候，就发送给Actor B，后半场的就发送给Actor C，Actor B和C依次处理这些请求；如果Actor B或C接收到的两个信息都想要某个座位，那么针对第二个请求会返回订票失败的消息。
图2：Actor用于订票场景你发现没有？在这个场景中，Actor B和C仍然是顺序处理各个请求。但因为是两个Actor并发地处理请求，所以系统整体的性能会提升到原来的两倍。
甚至，你可以让每排座位、每个座位都由一个Actor负责，使得系统的性能更高。因为在系统中创建一个Actor的成本是很低的。Actor跟协程类似，很轻量级，一台服务器里创建几十万、上百万个Actor也没有问题。如果每个Actor负责一个座位，那一台服务器也能负责几十万、上百万个座位的销售，也是可以接受的。
当然，实际的场景要比这个复杂，比如一次购买多张相邻的票等，但原理是一样的。用这种架构，可以大大提高并发能力，处理海量订票、秒杀等场景不在话下。
其实，我个人比较喜欢Actor这种模式，因为它跟现实世界里的分工协作很相似。比如，餐厅里不同岗位的员工，他们通过互相发信息来实现协作，从而并发地服务很多就餐的顾客。
分析到这里，我再把Actor模式跟你非常熟悉的一个概念，面向对象编程（Object Oriented Programming，OOP）关联起来。你可能会问：Actor和面向对象怎么还有关联？
是的。面向对象语言之父阿伦 · 凯伊（Alan Kay），Smalltalk的发明人，在谈到面向对象时是这样说的：对象应该像生物的细胞，或者是网络上的计算机，它们只能通过消息互相通讯。对我来说OOP仅仅意味着消息传递、本地保留和保护以及隐藏状态过程，并且尽量推迟万物之间的绑定关系。
I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages (so messaging came at the very beginning – it took a while to see how to do messaging in a programming language efficiently enough to be useful)</description></item><item><title>36_高级特性（一）：揭秘元编程的实现机制</title><link>https://artisanbox.github.io/7/36/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/36/</guid><description>你好，我是宫文学。
作为一名技术人员，我想你肯定知道什么是编程，那你有没有听说过“元编程（Meta-Programming）”这个概念呢？
元编程是计算机语言提供的一项重要能力。这么说吧，如果你要编写一些比较厉害的程序，像是Java世界里的Spring、Hibernate这样的库，以及C++的STL库等这样级别的程序，也就是那些通用性很强、功能强大的库，元编程功能通常会给予你巨大的帮助。
我还可以从另一个角度来评价元编程功能。那就是善用计算机语言的元编程功能，某种意义上能让你修改这门语言，让它更满足你的个性化需求，为你量身打造！
是不是觉得元编程还挺有意思的？今天这一讲，我就带你来理解元编程的原理，并一起探讨如何用编译技术来支持元编程功能的实现。
首先，我们需要透彻地了解一下什么是元编程。
什么是元编程（Meta-Programming）？元编程是一种把程序当做数据来处理的技术。因此，采用元编程技术，你可以把一个程序变换成另一个程序。
图1：元编程处理的对象是程序那你可能要问了，既然把程序作为处理对象的技术就是元编程技术，那么编译器不就是把程序作为处理对象的吗？经过处理，编译器会把源代码转换成目标代码。类似的还有对源代码的静态分析工具、代码生成工具等，都算是采用了元编程技术。
不过，我们在计算机语言里说的元编程技术，通常是指用这门语言本身提供的功能，就能处理它自己的程序。
比如说，在C语言中，你可以用宏功能。经过C语言的预处理器处理以后，那些宏就被转换成了另外的代码。下面的MUL宏，用起来像一个函数，但其实它只是做了一些字符串的替换工作。它可以说是最原始的元编程功能了。你在阅读像Python和Julia的编译器时，就会发现有不少地方采用了宏的功能，能让代码更简洁、可读性更好。
#define MUL(a,b) (a*b) MUL(2,3) //预处理后变成(2*3) 再拿Java语言举个例子。Java语言对元编程提供了多重支持，其中之一是注解功能。我们在解析Java编译器的时候已经发现，Java编译器会把所编译的程序表示成一个对象模型。而注解程序可以通过这个对象模型访问被注解的程序，并进行一些处理，比如生成新的程序。所以，这也是把程序作为数据来处理。
除了注解以外，Java还提供了反射机制。通过反射机制，Java程序可以在运行时获取某个类有哪些方法、哪些属性等信息，并可以动态地运行该程序。你看，这同样是把程序作为数据来处理。
像Python和JavaScript这样的脚本语言，其元编程能力就更强了。比如说，你用程序可以很容易地查询出某个对象都有哪些属性和方法，甚至可以给它们添加新的属性和方法。换句话说，你可以很容易地把程序作为数据进行各种变换，从而轻松地实现一些灵活的功能。这种灵活性，是很多程序员特别喜欢Python和JavaScript这样的语言的原因。
图2：各种不同的元编程技术起作用的时机好了，到现在为止，你已经了解了元编程的基本特征：把程序当做数据去处理。接下来，我再带你更深入地了解一下元编程，并把不同的元编程技术做做分类。
理解Meta的含义、层次以及作用首先，我们来注意一下Meta这个词缀的意思。维基百科中的解释是，Meta来自希腊文，意思是“在……之后（after）”和“超越……（beyond）”。加上这个词缀后，Meta-Somthing所形成的新概念就会比原来的Somthing的概念的抽象度上升一层。
举例来说，Physics是物理学的意思，表示看得见摸得着的物理现象。而Metaphysics就代表超越了物理现象的学问，也就是形而上学。Data是数据，而Metadata是元数据，是指对数据特性的描述，比如它是什么数据类型、取值范围是什么，等等。
还有，一门语言我们叫做Language，而语法规则（Grammar）是对一门语言的特点的描述，所以语法规则可以看做是Metalanguage。
其次，在理解了Meta的概念以后，我再进一步告诉你，Meta是可以分层次的。你可以对Meta再超越一层、抽象一层，就是Meta-Meta。理解Meta的层次，对于你深入理解元编程的概念非常重要。
拿你很熟悉的关系数据库来举个例子吧，看看不同的Meta层次都是什么意思。
首先是M0层，也就是关系数据库中的数据。比如一条人员数据，编号是“001”，姓名是“宫文学”等。一个数据库的使用者，从数据库中查出了这条数据，我们说这个人是工作在M0层的。
比M0抽象一层的是M1层，也就是Metadata，它描述了数据库中表的结构。比如，它定义了一张人员表，并且规定里面有编号、姓名等字段，以及每个字段的数据类型等信息。这样看来，元数据实际上是描述了一个数据模型，所以它也被叫做Model。一个工程师设计了这个数据库表的结构，我们说这个工程师是工作在M1层的。基于该工程师设计的数据库表，你可以保存很多M0层的人员数据：张三、李四、王五，等等。
比M1再抽象一层的是M2层。因为M1层可以叫做Model，所以M2层可以叫做Metamodel，也就是元模型。在这个例子中，Metamodel描述的是关系数据模型：它是由一张张的表（Table）构成的；而每张表，都是由字段构成的；每个字段，都可以有数据类型、是否可空等信息。发明关系数据模型，以及基于这个模型设计出关系数据库的大师，是工作在M2层的。基于关系模型，你可以设计出很多M1层的数据库表：人员表、订单表、商品表，等等。
那么有没有比Metamodel更抽象的层次呢？有的。这就是M3层，叫做Meta-Metamodel。这一层要解决的问题是，如何去描述关系数据模型和其他的元模型？在UML标准中，有一个MOF（Meta Object Facility）的规范，可以用来描述关系数据库、数据仓库等元模型。它用类、关联、数据类型和包这些基本要素来描述一个元模型。
好，通过关系数据库这个例子，现在你应该理解了不同的Meta层次是什么概念。那我们再把这个概念应用到计算机语言领域，也是一样的。
假设你使用一门面向对象的语言写了一个程序。这个程序运行时，在内存里创建了一个Person对象。那这个对象属于M0层。
而为了创建这个Person对象，你需要用程序设计一个Person类。从这个意义上来看，我们平常写的程序属于M1层，也就是相当于建立了一个模型来描述现实世界。你编写的订票程序就是对真实世界中的购票行为建立了一个模型，而你编写的游戏当然也是建立了一个逼真的游戏模型。
那么，你要如何才能设计一个Person类，以及一个完整的程序呢？这就需要用到计算机语言。计算机语言对应着M2层。它提供了类、成员变量、方法、数据类型、本地变量等元素，用于设计你的程序。我们对一门计算机语言的词法规则、语法规则和语义规则等方面的描述，就属于M2层，也就是一门计算机语言的元模型。而编译器就是工作在M2层的程序，它会根据元模型，也就是词法规则、语法规则等，来做程序的翻译工作。
我们在描述词法规则、语法规则的时候，曾经用到产生式、EBNF这些工具。这些工具是属于M3层的。你可以用我们前面说过的一个词，也就是Metalanguage来称呼这一层次。
这里我用了一个表格，来给你展示下关系数据模型与Java程序中不同的Meta层次。
元编程技术的分类理解了Meta层次的概念以后，我们再来总结一下元编程技术都有哪些分类。
第一，元编程可以通过生成语义层对象来生成程序。
当我们操纵M1层的程序时，我们通常需要透过M2层的对象来完成，比如读取类和方法的定义信息。类和方法就是M2层的对象。Java的注解功能和反射机制，就是通过读取和操纵M2层的对象来完成的。
在学习编译原理的过程中，你知道了类、方法这些都是语义层次的概念，编译器保证了编译后的程序在语义上的正确性，所以你可以大胆地使用这些信息，不容易出错。如果你要在运行时动态地调用方法，运行时也会提供一定的检查机制，减少出错的可能性。
第二，元编程可以通过生成AST来生成程序。
你同样知道，一个程序也可以用AST来表达。所以，我们能不能让程序直接读取、修改和生成AST呢？这样对AST的操纵，就等价于对程序的操纵。
答案是可以的。所有Lisp家族的语言都采用了这种元数据技术，Julia就是其中之一。Lisp语言可以用S表达式来表示程序。S表达式是那种括号嵌套括号的数据结构，其实就是一棵AST。你可以用宏来生成S表达式，也就是生成AST。
不过，让程序直接操作比较底层的数据结构，其代价是可能生成的AST不符合语义规则。毕竟，AST只表达了语法规则。所以，用这种方式做元编程需要小心一些，不要生成错误的程序。同时，这种元编程技术对程序员来说，学习的成本也更高，因为他们要在比较低的概念层次上工作。
第三，元编程可以通过文本字符串来生成程序。
当然，你还可以把程序表达成更加低端的格式，就是一些文本字符串而已。我们前面说过，C语言的宏，其实就是做字符串的替换。而一些脚本语言，通常也能接受一个文本字符串作为程序来运行，比如JavaScript的eval()函数就可以接受一个字符串作为参数，然后把字符串作为程序来运行。所以，在JavaScript里的一项非常灵活的功能，就是用程序生成一些字符串，然后用eval()函数来运行。当然你也能预料到，用越原始的模型来表示程序，出错的可能性就越大。所以有经验的程序员，都会很谨慎地使用类似eval()这样的功能。但无论如何，这也确实是一种元编程技术。
第四，元编程可以通过字节码操纵技术来生成字节码。
那么，除了通过生成语义层对象、AST和文本来生成程序以外，对于Java这种能够运行字节码的语言来说，你还可以通过字节码操纵技术来生成字节码。这种技术一般不是由语言本身提供的能力，而是由第三方工具来实现的，典型的就是Spring。
好，到这里，我们就探讨完了通过元编程技术由程序生成程序的各种方式。下面我们再通过另一个维度来讨论一下元编程技术。这个维度是元编程技术起作用的时机，我们可以据此分为静态元编程和动态元编程。
静态元编程技术只在编译期起作用。比如C++的模板技术和把Java注解技术用在编译期的情况（在下面会具体介绍这两种技术）。一旦编译完毕以后，元程序跟普通程序一样，都会变成机器码。
动态元编程技术会在运行期起作用。这方面的例子是Java的反射机制。你可以在运行期加载一个类，来查看它的名称、都有哪些方法，然后打印出来。而为了实现这种功能，Java程序必须在class文件里保存这个类的Model，比如符号表，并通过M2层的接口，来查询类的信息。Java程序能在运行期进行类型判断，也是基于同样的原理。
好，通过上面的介绍，我想你对元编程的概念应该有比较清晰的理解了。那接下来，我们就来看看不同语言具体实现元编程的方式，并且一起探讨下在这个过程中应该如何运用编译技术。
不同语言的元编程技术我们讨论的语言包括几大类，首先是Java，接着是Python和JavaScript这样的脚本语言，然后是Julia这样的Lisp语言，最后是C++的模板技术等一些很值得探讨的元编程技术。
Java的元编程技术在分析Java的编译器的时候，我们已经解析了它是如何处理注解的，注解就是一种元编程技术。在我们举的例子中，注解是在编译期就被处理掉了。
@Retention(RetentionPolicy.SOURCE) //注解用于编译期处理 @Target(ElementType.TYPE) //注解是针对类型的 public @interface HelloWorld { } 当时我们写了一个简单的注解处理程序，这个程序，能够获取被注解的代码的元数据（M1层的信息），比如类名称、方法名称等。这些元数据是由编译器提供的。然后，注解处理程序会基于这些元数据生成一个新的Java源代码，紧接着该源代码就会被编译器发现并编译掉。
通过这个分析，你会发现注解处理过程自始至终都借助了编译器提供的能力：先是通过编译器查询被注解的程序的元数据，然后生成的新程序也会被编译器编译掉。所以你能得出一个结论：所谓元编程，某种意义上就是由程序来调用编译器提供的能力。
刚刚我们探究的是在编译期使用元编程技术。那么在运行期，Java提供了反射机制，来动态地获取程序的元数据，并操纵程序的执行。
举个例子。假设你写了一个简单的ORM（Object-Relational Mapping）程序，能够把Java对象自动保存到数据库中。那么你就可以通过反射机制，来获取这个对象都有哪些属性，然后读取这些属性的值，并生成一个正确的SQL语句来完成对象的保存动作。比如，对于一个Person对象，ORM程序通过反射机制会得知它有name和country两个字段，再从对象里读取name和字段的值，就会生成类似"Insert into Person (name, age), values(“Richard”, “China”)"</description></item><item><title>37_高级特性（二）：揭秘泛型编程的实现机制</title><link>https://artisanbox.github.io/7/37/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/37/</guid><description>你好，我是宫文学。
对泛型的支持，是现代语言中的一个重要特性。它能有效地降低程序员编程的工作量，避免重复造轮子，写很多雷同的代码。像C++、Java、Scala、Kotlin、Swift和Julia这些语言都支持泛型。至于Go语言，它的开发团队也对泛型技术方案讨论了很久，并可能会在2021年的版本中正式支持泛型。可见，泛型真的是成为各种强类型语言的必备特性了。
那么，泛型有哪些特点？在设计和实现上有哪些不同的方案？编译器应该进行什么样的配合呢？今天这一讲，我就带你一起探讨泛型的实现原理，借此加深你对编译原理相关知识点的认知，让你能够在自己的编程中更好地使用泛型技术。
首先，我们来了解一下什么是泛型。
什么是泛型？在日常编程中，我们经常会遇到一些代码逻辑，它们除了类型不同，其他逻辑是完全一样的。你可以看一下这段示例代码，里面有两个类，其中一个类是保存Integer的列表，另一个类是保存Student对象的列表。
public class IntegerList{ List data = new ArrayList(); public void add(Integer elem){ data.add(elem); } public Integer get(int index){ return (Integer) data.get(index); } } public class StudentList{ List data = new ArrayList(); public void add(Student elem){ data.add(elem); } public Student get(int index){ return (Student) data.get(index); } } 我们都知道，程序员是很不喜欢重复的代码的。像上面这样的代码，如果要为每种类型都重新写一遍，简直会把人逼疯！
泛型的典型用途是针对集合类型，能够更简单地保存各种类型的数据，比如List、Map这些。在Java语言里，如果用通用的集合类来保存特定类型的对象，就要做很多强制转换工作。而且，我们还要小心地做类型检查。比如：
&amp;lt;!&amp;ndash; [[[read_end]]] &amp;ndash;&amp;gt;List strList = new ArrayList(); //字符串列表 strList.add(&amp;quot;Richard&amp;quot;); String name = (String)strList.get(i); //类型转换 for (Object obj in strList){ String str = (String)obj; //类型转换 &amp;hellip; }</description></item><item><title>38_综合实现（一）：如何实现面向对象编程？</title><link>https://artisanbox.github.io/7/38/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/38/</guid><description>你好，我是宫文学。
从20世纪90年代起，面向对象编程的范式逐渐成为了主流。目前流行度比较高的几种语言，比如Java、JavaScript、Go、C++和Python等，都支持面向对象编程。
那么，为了支持面向对象编程，我们需要在语言的设计上，以及编译器和运行时的实现上，考虑到哪些问题呢？
这一讲，我就带你来探讨一下如何在一门语言里支持面向对象特性。这是一个很综合的话题，会涉及很多的知识点，所以很有助于帮你梳理和贯通与编译原理有关的知识。
那么，我们就先来分析一下，面向对象特性都包括哪些内容。
面向对象语言的特性日常中，虽然我们经常会使用面向对象的语言，但如果要问，到底什么才是面向对象？我们通常会说得含含糊糊。最常见的情况，就是会拿自己所熟悉的某种语言的面向对象特性，想当然地认为这就是面向对象语言的全部特性。
不过，在我们的课程里，我想从计算机语言设计的角度，带你重新梳理和认识一下面向对象的编程语言，把面向对象按照清晰的逻辑解构，这样也便于讨论它的实现策略。在这个过程中，你可能会对面向对象产生新的认识。
特征1：对象面向对象编程语言的核心，是把世界看成了一个个的对象，比如汽车、动物等。这些对象包含了数据和代码。数据被叫做字段或属性，而代码通常又被叫做是方法。
此外，这些对象之间还会有一定的关系。比如，汽车是由轮子、发动机等构成的，这叫做聚合关系。而某个班级会有一个班主任，那么班级和作为班主任的老师之间，会有一种引用关系。
对象之间还可以互相发送消息。比如，司机会“通知”汽车，让它加速或者减速。在面向对象的语言中，这通常是通过方法调用来实现的。但也并不局限于这种方式，比如对象之间还可以通过异步的消息进行互相通讯，不过一般的编程语言都没有原生支持这种通讯方式。我们在讨论Actor模式的时候，曾经提到过Actor之间互相通讯的方式，就有点像对象之间互发消息。
特征2：类和类型体系很多面向对象的语言都是基于类（class）的，并且类也是一种自定义的类型。这个类型是对象的模板。而对象呢，则是类的实例。我们还可以再印证一下，前面在探究元编程的实现机制时，学过的Meta层次的概念。对象属于M0层，而类属于M1层，它为对象制定了一个标准，也就是对象中都包含了什么数据和方法。
其实，面向对象的语言并不一定需要类这个概念，这个概念更多是来自于类型理论，而非面向对象的语言一样可以支持类型和子类型。类型的好处主要是针对静态编译的语言的，因为这样就可以通过类型，来限制可以访问的对象属性和方法，从而减少程序的错误。
而有些面向对象的语言，比如JavaScript并没有类的概念。也有的像Python，虽然有类的概念，但你可以随时修改对象的属性和方法。
特征3：重用–继承（Inheritance）和组合（Composition）在软件工程里，我们总是希望能重用已有的功能。像Java、C++这样的语言，能够让子类型重用父类型的一些数据和逻辑，这叫做继承。比如Animal有speak()方法，Cat是Animal的子类，那么Cat就可以继承这个speak()方法。Cat也可以重新写一个方法，把父类的方法覆盖掉，让叫声更像猫叫。
不过，并不是所有的面向对象编程语言都喜欢通过继承的方式来实现重用。你在网上可以找到很多文章，都在分析继承模式的缺陷。像Go语言，采用的是组合方式来实现重用。在这里，我引用了一篇文章中的例子。在这个例子中，作者首先定义了一个author的结构体，并给这个结构体定义了一些方法：
type author struct { //结构体：author(作者) firstName string //作者的名称 lastName string bio string //作者简介 } func (a author) fullName() string { //author的方法：获取全名 return fmt.Sprintf(&amp;quot;%s %s&amp;quot;, a.firstName, a.lastName) }
type post struct { //结构体：文章 title string //文章标题 content string //文章内容 author //文章作者 }
func (p post) details() { //文章的方法：获取文章的详细内容。 fmt.Println(&amp;quot;Title: &amp;quot;, p.title) fmt.Println(&amp;quot;Content: &amp;quot;, p.content) fmt.Println(&amp;quot;Author: &amp;quot;, p.</description></item><item><title>39_综合实现（二）：如何实现函数式编程？</title><link>https://artisanbox.github.io/7/39/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/39/</guid><description>你好，我是宫文学。
近些年，函数式编程正在复兴。除了一些纯函数式编程语言，比如Lisp、Clojure、Erlang等，众多的主流编程语言，如Python、JavaScript、Go甚至Java，它们都有对函数式编程的支持。
你应该会发现，现在人们对于函数式编程的讨论有很多，比如争论函数式编程和面向对象编程到底哪个更强，在语言里提供混合的编程模式到底对不对等等。
这些论战一时半会儿很难停息。不过我们的这一讲，不会涉及这些有争议的话题，而是试图从编译技术的角度，来探讨如何支持函数式编程，包括如何让函数作为一等公民、如何针对函数式编程的特点做优化、如何处理不变性，等等。通过函数式编程这个综合的主题，我们也再一次看看，如何在实现一门语言时综合运用编译原理的各种知识点，同时在这个探究的过程中，也会加深你对函数式编程语言的理解。
好，我们先来简单了解一下函数式编程的特点。
函数式编程的特点我想，你心里可能多多少少都会有一点疑问，为什么函数式编程开始变得流行了呢？为什么我在开篇的时候，说函数式编程正在“复兴”，而没有说正在兴起？为什么围绕函数式编程会有那么多的争论？
要回答这几个问题，我会建议你先去了解一点历史。
我们都知道，计算机发展历史上有一个重要的人物是阿兰 · 图灵（Alan Turing）。他在1936年提出了一种叫做图灵机的抽象模型，用来表达所有的计算。图灵机有一个无限长的纸带，还有一个读写头，能够读写数据并根据规则左右移动。这种计算过程跟我们在现代的计算机中，用一条条指令驱动计算机运行的方式很相似。
不过，计算模型其实不仅仅可以用图灵机来表达。早在图灵机出现之前，阿隆佐 · 邱奇（Alonzo Church）就提出了一套Lambda演算的模型。并且，计算机科学领域中的很多人，其实都认为用Lambda演算来分析可计算性、计算复杂性，以及用来编程，会比采用图灵机模型更加简洁。而Lambda演算，就是函数式编程的数学基础。
补充：实际上，邱奇是图灵的导师。当年图灵发表他的论文的时候，编辑看不懂，所以找邱奇帮忙，并推荐图灵成为他的学生，图灵机这个词也是邱奇起的。所以师生二人，对计算机科学的发展都做出了很大的贡献。
因为有Lambda演算的数学背景，所以函数式编程范式的历史很早。上世纪50年代出现的Lisp语言，就是函数式编程语言。Lisp的发明人约翰 · 麦卡锡（John McCarthy）博士，是一位数学博士。所以你用Lisp语言和其他函数式编程语言的时候，都会感觉到有一种数学思维的味道。
也正因如此，与函数式编程有关的理论和术语其实是有点抽象的，比如函子（Functor）、单子（Monad）、柯里化（Currying）等。当然，对它们的深入研究不是我们这门课的任务。这里我想带你先绕过这些理论和术语，从我们日常的编程经验出发，来回顾一下函数式编程的特点，反倒更容易一些。
我前面也说过，目前流行的很多语言，虽然不是纯粹的函数式编程语言，但多多少少都提供了对函数式编程的一些支持，比如JavaScript、Python和Go等。就连Java语言，也在Java8中加入了对函数式编程的支持，很多同学可能已经尝试过了。
我们使用函数式编程最多的场景，恐怕是对集合的处理了。举个例子，假设你有一个JavaScript的数组a，你想基于这个数组计算出另一个数组b，其中b的每个元素是a中对应元素的平方。如果用普通的方式写程序，你可能会用一个循环语句，遍历数组a，然后针对每个数组元素做处理：
var b = []; for (var i = 0; i&amp;lt; a.length; i++){ //遍历数组a b.push(a[i]*a[i]); //把计算结果加到数组b中 } 不过你也可以采用更简单的实现方法。
这次我们使用了map方法，并给它传了一个回调函数。map方法会针对数组的每个元素执行这个回调函数，并把计算结果组合成一个新的数组。
function sq(item){ //计算平方值的函数 return item*item; } var b = a.map(sq); //把函数作为参数传递 它还可以写成一种更简化的方式，也就是Lambda表达式的格式：
var b = a.map(item=&amp;gt;item*item); 通过这个简单的例子，我们可以体会出函数式编程的几个特点：
1.函数作为一等公民也就是说，函数可以像一个数值一样，被赋给变量，也可以作为函数参数。如果一个函数能够接受其他函数作为参数，或者能够把一个函数作为返回值，那么它就是高阶函数。像示例程序中的map就是高阶函数。
那函数式编程语言的优势来自于哪里呢？就在于它可以像数学那样使用函数和变量，这会让软件的结构变得特别简单、清晰，运行结果可预测，不容易出错。
根据这个特点，我们先来看看函数式编程语言中的函数，跟其他编程语言中的函数有什么不同。
2.纯函数（Pure Function）在函数式编程里面，有一个概念叫做纯函数。纯函数是这样一种函数，即相同的输入，永远会得到相同的输出。
其实你对纯函数应该并不陌生。你在中学时学到的函数，就是纯函数。比如对于f(x)=ax+b，对于同样的x，所得到的函数值肯定是一样的。所以说，纯函数不应该算是个新概念，而是可以回归到你在学习计算机语言之前的那个旧概念。
在C语言、Java等语言当中，由于函数或方法里面可以引用外面的变量，比如全局变量、对象的成员变量，使得其返回值与这些变量有关。因此，如果有其他软件模块修改了这些变量的值，那么该函数或方法的返回值也会受到影响。这就会让多个模块之间基于共享的变量耦合在一起，这种耦合也使得软件模块的依赖关系变得复杂、隐秘，容易出错，牵一发而动全身。这也是像面向对象语言这些命令式编程语言最令人诟病的一点。
而对于纯函数来说，它不依赖外部的变量，这个叫做引用透明（Reference Transparency）。纯函数的这种“靠谱”、可预测的特征，就给我们的编程工作带来了很多的好处。
举个例子。既然函数的值只依赖输入，那么就跟调用时间无关了。假设有一个函数式g(f(x))，如果按照传统的求值习惯，我们应该先把f(x)的值求出来，再传递给g()。但如果f(x)是纯函数，那么早求值和晚求值其实是无所谓的，所以我们可以延迟求值（Lazy Evaluation）。</description></item><item><title>40_成果检验：方舟编译器的优势在哪里？</title><link>https://artisanbox.github.io/7/40/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/40/</guid><description>你好，我是宫文学。到这里，咱们的课程就已经进入尾声了。在这门课程里，通过查看真实的编译器，你应该已经积累了不少对编译器的直观认识。前面我们研究的各种编译器，都是国外的产品或项目。而这一讲呢，我们则要看看一个有中国血统的编译器：方舟编译器。
通过阅读方舟编译器已经公开的代码和文档，在解析它的过程中，你可以检验一下自己的所学，谈谈你对它的认识。比如，跟你了解的其他编译器相比，它有什么特点？先进性如何？你是否有兴趣利用方舟编译器做点实际项目？等等。
不过，到目前为止，由于方舟编译器开源的部分仍然比较有限，所以这一讲我们只根据已经掌握的信息做一些分析。其中涉及两个大的话题，一是对方舟编译器的定位和设计思路的分析，二是对方舟编译器所使用的Maple IR的介绍。
好，首先，我借助Android对应用开发支持的缺陷，来谈一下为什么方舟编译器是必要的。
Android的不足为什么要研发一款自己的编译器？对于一个大的技术生态而言，语言的编译和运行体系非常重要。它处在上层应用和下层硬件之间，直接决定了应用软件能否充分地发挥出硬件的性能。对于移动应用生态而言，我国拥有体量最大的移动用户和领先的移动应用，也有着最大的手机制造量。可是，对于让上层应用和底层硬件得以发挥最大能力的编译器和运行时，我们却缺少话语权。
实际上，我认为Android对应用开发的支持并不够好。我猜测，掌控Android生态的谷歌公司，对于移动应用开发和手机制造都没有关系到切身利益，因此创新的动力不足。
我之所以说Android对应用开发的支持不够好，这其实跟苹果的系统稍加对比就很清楚了。同样的应用，在苹果手机上会运行得更流畅，且消耗的内存也更低。所以Android手机只好增加更多的CPU内核和更多的内存。
你可能会问，谷歌不是也有自己的应用吗？对应用的支持也关系到谷歌自己的利益呀。那我这里其实要补充一下，我说的应用开发，指的是用Java和Kotlin开发的应用，这也是大部分Android平台上的应用开发者所采用的语言。而像谷歌这样拥有强大技术力量的互联网巨头们，通常对于性能要求比较高的代码，是用C开发的。比如微信的关键逻辑就是用C编写的；像手机游戏这种对性能要求比较高的应用，底层的游戏引擎也是基于C/C++实现的。
这些开发者们不采用Java的原因，是因为Java在Android平台上的编译和运行方式有待提高。Android为了提升应用的运行速度，一直在尝试升级其应用运行机制。从最早的仅仅解释执行字节码，到引入JIT编译机制，到当前版本的ART（Android Runtime）支持AOT、JIT和基于画像的编译机制。尽管如此，Android对应用的支持仍然存在明显的短板。
第一个短板，是垃圾收集机制。我们知道，Java基于标记-拷贝算法的垃圾收集机制有两个缺陷。一是要占据更多的内存，二是在垃圾收集的时候会有停顿，导致应用不流畅。在系统资源紧张的时候，更是会强制做内存收集，引起整个系统的卡顿。
实际上，Java的内存管理机制使得它一直不太适合编写客户端应用。就算在台式机上，用Java编写的客户端应用同样会占用很大的内存，并且时不时会有卡顿。你如果使用过Eclipse和IDEA，应该就会有这样的体会。
第二个短板，是不同语言的融合问题。Android系统中大量的底层功能都是C/C++实现，而Java应用只是去调用它们。比如，图形界面的绘制和刷新，是由一个叫做Skia的库来实现的，这个库是用C/C++编写的，各种窗口控件都是在Skia的基础上封装出来的。所以，用户在界面上的操作，背后就有大量的JNI调用。
问题是，Java通过JNI调用C语言的库的时候，实现成本是很高的，因为两种不同语言的数据类型、调用约定完全不同，又牵涉到跨语言的异常传播和内存管理，所以Java不得不通过虚拟机进行昂贵的处理，效率十分低下。
据调查，95%的顶级移动应用都是用Java和C、C++等混合开发的。所以，让不同语言开发的功能能够更好地互相调用，是一个具有普遍意义的问题。
第三个短板，就是Android的运行时一直还是受Java虚拟机思路的影响，一直摆脱不了虚拟机。虚拟机本身要占据内存资源和CPU资源。在做即时编译的时候，也要消耗额外的资源。
那么如何解决这些问题呢？我们来看看方舟编译器的解决方案。
方舟编译器的解决方案方舟编译器的目标并不仅仅是为了替代Android上的应用开发和运行环境。但我们可以通过方舟是如何解决Android应用开发的问题，来深入了解一下方舟编译器。
我们先来看看，方舟编译器是怎么解决垃圾收集的问题的。
不过，在讨论方舟的方案之前，我们不妨先参考一下苹果的方案做个对照。苹果采用的开发语言，无论是Objective-C，还是后来的Swift，都是采用引用计数技术。引用计数可以实时回收内存垃圾，所以没有卡顿。并且它也不用像标记-拷贝算法那样，需要保留额外的内存。而方舟编译器，采用的是跟苹果一样的思路，同样采用了引用计数技术。
当然，这里肯定会有孰优孰劣的争论。我们之前也讲过，采用引用计数法，每次在变量引用对象的时候都要增加引用计数，而在退出变量的作用域或者变量不再指向该对象时，又要减少引用计数，这会导致一些额外的性能开销。当对象在多个线程之间共享的时候，增减引用计数的操作还要加锁，从而进一步导致了性能的降低。
不过，针对引用计数对性能的损耗，我们可以在编译器中通过多种优化算法得到改善，尽量减少不必要的增减计数的操作，也减少不必要的锁操作。另外，有些语言在设计上也会做一些限制，比如引入弱引用机制，从而降低垃圾收集的负担。
无论如何，在全面考察了引用计数方法的优缺点以后，你仍然会发现它其实更适合开发客户端应用。
关于第二个问题，也就是不同语言的融合问题。华为采取的方法是，让Java语言的程序和基于C、C++等语言的程序按照同一套框架做编译。无论前端是什么语言，都统一编译成机器码，同时不同语言的程序互相调用的时候，也没有额外的开销。
下图是方舟编译器的文档中所使用的架构图。你能看到它的设计目标是支持多种语言，都统一转换成方舟IR，然后进行统一的优化处理，再生成机器码的可执行文件。
方舟编译器架构示意图这个技术方案其实非常大胆。它不仅解决了不同语言之间的互相调用问题，也彻底抛弃了植根于JVM的虚拟机思路。方舟编译器新的思路是不要虚拟机，最大程度地以机器码的方式运行，再加上一个非常小的运行时。
我说这个技术方案大胆，是因为方舟编译器彻底抛弃了Java原有的运行方案，包括内存布局、调用约定、对象结构、分层编译机制等。我们在第二个模块讲过Graal，在仍然基于JVM运行的情况下，JIT只是尽力做改良，它随时都有一个退路，就是退到用字节码解释器去执行。就算采用AOT以后，运行时可以变得小一些，但Java运行机制的大框架仍然是不变的。
我也介绍过，GraalVM支持对多种语言做统一编译，其中也包含了对C语言的支持，并且也支持语言之间的互相调用。但即便如此，它仍是改良主义，它不会抛弃Java原来的技术积累。
而方舟编译器不是在做改良，而是在做革命。它对Java的编译更像是对C/C++等语言的编译，抛弃了JVM的那一套思路。
这个方案不仅大胆，而且难度更高。因为这样就不再像分层编译那样有退路，方舟编译器需要把所有的Java语义都静态编译成机器码。而对于那些比较动态的语义，比如运行时的动态绑定、Reflection机制等，是挑战比较大的。
那方舟编译器目前的成果如何呢？根据华为官方的介绍，方舟编译器可以使安卓系统的操作流畅度提升24%，响应速度提升44%，第三方应用操作流畅度提升高达60%。这就是方舟编译器的厉害之处，这也证明方舟编译器的大胆革新之路是走对了的。
我们目前只讨论了方舟编译器对Android平台的改进。其实，方舟编译器的目标操作系统不仅仅是Android平台，它本质上可移植所有的操作系统，也包括华为自己的鸿蒙操作系统。对于硬件平台也一样，它可以支持从手机到物联网设备的各种硬件架构。
所以，你能看出，方舟编译器真的是志存高远。它不是为了解决某一个小问题，而是致力于打造一套新的应用开发生态。
好了，通过上面的介绍，你应该对方舟编译器的定位有了一个了解。接下来的问题是，方舟编译器的内部到底是怎样的呢？
方舟编译器的开源项目要深入了解方舟编译器，还是必须要从它的源代码入手。从去年9月份开源以来，方舟编译器吸引了很多人的目光。不过方舟编译器是逐步开源的，由于开放出来的源代码必须在知识产权等方面能够经得起严格的审查，因此到现在为止，我们能看到的开源版本号还只是0.2版，开放出来的功能并不多。
我参照方舟的环境配置文档，在Ubuntu 16.04上做好了环境配置。
注意：请尽量完全按照文档的要求来配置环境，避免出现不必要的错误。不要嫌某些软件的版本不够新。
接着，你可以继续根据开发者指南来编译方舟编译器本身。方舟编译器本身的代码是用C++写的，需要用LLVM加Clang编译，这说明它到目前还没有实现自举。然后，你可以编译一下示例程序。比如，用下面的四个命令，可以编译出HelloWorld样例。
source build/envsetup.sh; make; cd samples/helloworld/; make 这个“hellowold”目录原来只有一个HelloWorld.java源代码，经过编译后，形成了下面的文件：
如果你跟踪查看编译过程，你会发现中间有几步的操作：
第一步，执行java2jar，这一步是调用Java的编译器，把Java文件先编译成class文件，然后打包成jar文件。
补充：java2jar实际上是一个简单的脚本文件，你可以查看里面的内容。
第二步，执行jbc2mpl，也就是把Java字节码转换成Maple IR。Maple IR是方舟编译器的IR，我下面会展开介绍。编译后生成的Maple IR保存到了HelloWorld.mpl中。
第三步，通过maple命令，执行mpl2mpl和mplme这两项对Maple IR做分析和优化的工作。这其中，很重要的一个步骤，就是把Java方法的动态绑定，用vtable做了实现，并生成了一个新的Maple IR文件：HelloWorld.VtableImpl.mpl。
最后一步，调用mplcg命令，将Maple IR转换成汇编代码，保存到一个以.s结尾的文件里面。
注意，我们目前还没有办法编译成直接可以执行的文件。当前开源的版本，既没有编译器前端部分的代码，也没有后端部分的代码，甚至基于Maple IR的一些常见的优化，比如内联、公共子表达式消除、常量传播等等，都是没有的。目前开源的版本主要展现了Maple IR，以及对Maple IR做的一些变换，比如转换成SSA格式，以便进行后续的分析处理。
到这里，你可能会有一点失望，因为当前开放出来的东西确实有点少。但是不要紧，方舟编译器既然选择首先开放Maple IR的设计，这说明Maple IR在整个方舟编译器的体系中是很重要的。
事实也确实如此。方舟编译器的首席科学家，Fred Chow（周志德）先生，曾发表过一篇论文：The increasing significance of intermediate representations in compilers。他指出，IR的设计会影响优化的效果；IR的调整，会导致编译器实现的重大调整。他还提出：如果不同体系的IR可以实现转换的话，就可以加深编译器之间的合作。</description></item><item><title>不定期加餐1_远程办公，需要你我具备什么样的素质？</title><link>https://artisanbox.github.io/7/41/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/41/</guid><description>你好，我是宫文学。到这里，咱们课程的第一模块“预备知识篇”就已经更新完了。通过这么多讲的学习，这些编译技术的核心基础知识，你掌握得怎么样了呢？是不是感觉自己已经构建了一个编译原理的知识框架了？
不过我也知道，要理解编译技术的这些核心概念和算法，可能不是一件很容易的事儿，在跟随我一起探索编译之旅的过程中，估计也耗费了你不少的脑细胞，那咱们是时候来轻松一下了。
今天，我就想借着这个加餐的环节，跟你聊一聊一个很有意思的话题：远程办公。
之所以选择这个话题，主要有两方面的原因。
首先，由于疫情的影响，春节之后，很多公司都采取了远程办公的方式。所以，如何在远程办公的情况下做好工作，对于员工和公司来说，都是一个挑战。
第二个原因，是我个人一直对于远程办公这种工作模式很感兴趣，这些年来也一直在做这方面的思考，关注这方面的实践，所以有了一些心得体会，想跟你分享一下。
不过，要想把远程办公这个话题聊清楚，确实不容易，分歧也比较大。有一些朋友会比较悲观，觉得远程办公根本不切实际；而另一些朋友则会很乐观，觉得远程办公马上就会普及。
今天，我就来分享一下我看待远程办公的一些视角。我会从公司和员工这两个角度，来分析远程办公带来的机遇和挑战，希望能给你带来一些启发，让你以更积极和务实的姿态，迎接远程办公的浪潮，甚至在这种工作模式转换的趋势下，抓住更多的发展机遇。
首先，我来聊一聊远程办公的那些吸引人的地方。
远程办公的好处我对远程办公的了解，最早是透过开源圈的朋友了解了一些故事，后来自己也接触了一些酷酷的公司。很多做开源软件产品和技术服务的公司都是远程办公的，他们的员工可能来自世界各地。我曾经接触过一个芬兰公司的CEO，他们主要做嵌入式Linux的技术服务。一百多人的公司，平常办公室是没什么人的。据说他们公司有的员工，可以一边上班，一边去全世界旅游。
我当时认为，一百多人的公司，全部都能远程办公，并且管理良好，就已经很不错了。但后来看了一篇文章，讲到WordPress的母公司Automattic有上千名员工，分布在全球75个国家，全部都是远程办公。这就有点令人吃惊了！我才意识到，在互联网时代，原来远程办公可以适用于任何规模的企业。
这次疫情中，IT领域的很多公司都大量地采用了远程办公模式，包括谷歌、Facebook、微软等大型企业。
现在新闻上说，疫情之后，世界再也回不到过去了。其实我觉得，在很多领域，我倒是宁愿它回不去了。比如，远程教育；再比如，远程工作。
因为远程，意味着你获得了一个难得的自由：位置自由。
现代社会，我们苦“位置”久已！因为很多资源都是跟位置绑定在一起的，比如说，教育资源与学区房。
我在北京的很多朋友，他们在孩子上学期间，一直都是租房子住的，因为要住得离孩子学校近，而自己的房子会租出去。这种状态要持续到孩子上大学为止。
而对于若干都市白领来说，在大城市上班，就意味着要把整个肉体在办公室和家之间移动，所以每天可能要在路上花两三个小时，很多时候还会在路上堵个半天。
如果我们真的获得了位置自由，那么整个生活的幸福指数会提高一大截吧！
对于远程教育来说，我比较希望见到的现象，是在偏远的乡村，一样能够通过线上教育获得最优质的知识资源。至于线下的老师，更多的是关注孩子的健康成长，多带着孩子亲近大自然，扮演“育”的角色，而不是“教”的角色。
工作也是一样。现在越来越多的工作，都可以在网上进行了。互联网电商的发展，虽然让一些线下店铺的营业状况受到了影响，但只要能通网络，很多人在网上就可以卖东西了呀。另外，随着外卖的兴起，很多餐饮企业也不再需要临街的店面了。
所以，通过远程办公，我们可能就不需要北漂、深漂等各种漂了，可以选择离自己的亲人更近一些，或者可以反过来，四海为家。并且，你还可能获得更多、更好的工作机会，你可以从全世界的公司里选择你喜欢的那份工作，并且也不需要离开你喜欢居住的地方。
并且，伴随着位置自由，往往也会给我们带来时间自由。因为远程后不再需要按时上班打卡了（很多在全球都招揽人才的公司，大家的作息时间都不一样，当然不可能统一打卡），所以管理体系会更加面向绩效，而不会管你到底是在什么时间来完成这些工作的（通常也没法管理）。这就意味着，你可以在家人需要你的时候，出现在他们身边（比如接孩子），然后选择自己合适的时间段来工作。
上面说的是远程办公对员工的好处。从企业的角度来看，远程办公其实也会带来一些潜在的好处。
首先，有些员工可能会在工作上做更多的投入（这跟某些员工会偷懒恰恰相反，所以可能出现两极分化）。这些人很享受自己的工作，每天上下班反倒降低了他可能做出的贡献。如果公司有一套良好的管理体系，那就可能会因此筛选出更适合自己的员工，而避开那些混日子、划水的员工，整个团队的素质反倒会得到提高。
我曾经跟MySQL的前CEO Mårten Mickos聊天。我问他，管理远程办公的员工，需要注意些什么？
他思考了一下，说要建议员工跟家人一起住，至少要养条狗什么的。因为家人能帮助管理这些极客们的作息。不然由着这些极客们的性子，他们会昏天黑地、毫无规律地作息，不利于健康。就算养条狗，你也会因为要照料它们，而让自己的生活节奏健康一点。
他的回答其实出乎我的意料，我原本以为他会说什么公司的管理措施之类的。你体会一下，如果你是公司老板，你是不是会因为拥有这样热爱工作的员工而感到欣慰呢？
第二，因为没有了地域限制，公司也就可以充分任用全球各地的人才。这个方面在很多做开源软件的公司那里，得到了很好的体现。如果你喜欢某个开源产品，在社区里贡献自己的力量，那你很可能就会被邀请加入到该公司。
在互联网时代，企业的组织方式也正在重构。滴滴打车、美团外卖这些采用新雇佣方式的公司，不但可以更好地利用各地的人力资源，TA们也提供了一些自由工作的机会。
第三，没有了地域的限制，公司也可能更容易拓展自己的市场。这个好处也是显而易见的，就不用我多说了。
远程办公的挑战上面是我对远程的一些美好的憧憬。还是回到现实吧，因为更改现有的教育体制，可能是很难的。而让企业老板们改变公司的管理方式，难度也不低。
老板们都是理性的。真金白银投入做企业，是要见到效益的。可是，如何能保证采用远程办公模式，不会让企业变成一团散沙，纪律涣散、效率低下呢？
你可以问问，在春节后不得已实行远程办公的企业，对经营有没有产生影响。
说实话，在没做好充分的准备之前，仓促地采用远程办公，肯定会产生各种不适。
因为远程办公，对于管理体系，有着更高的要求。很多工作是难以直接度量绩效的，比如说研发工作就比销售工作更难衡量绩效。
而没有良好的管理体系，仅凭员工的自觉，是不可能产生良好的效果的。其实，硅谷有一些IT公司很早就实行过远程办公，但后来又取消了，原因就是绩效不理想，混日子的员工太多。
反过来，站在员工的角度，你真能做好自己的工作管理吗？在办公室工作的时候，迫于同事们的目光，你总得做点事情吧。可是，如果脱离了直接的监督，有多少人能够真正管好自己呢？
好，你说你的自我管理能力强，那么请问，有多少人能控制住自己每天刷手机的时间呢？据说，超过50%的成年人，都有手机上瘾症。在办公室的时候，尚且见缝插针地刷手机。如果在家办公，又会怎样呢？
有过远程工作经历的人，都会经历这么一个时期。即使是你很有责任心、很有事业心，但也要每天花费很多的精力来管理自己的行为。我认识的一个朋友，她在IT行业，主要做售前支持工作。之前跟她闲聊的时候，她说自己花了3年的时间跟自己搏斗，才养成了良好的居家工作习惯。而管理自己这件事情，也是消耗注意力的。注意力本身，又是个稀缺资源。所以在初期，你会觉得，对比在办公室里，居家办公会更累，在公司你不需要花精力来控制自己的行为，因为环境和同事帮忙做了这件事情，实际上节省了你的注意力。
我也听说，有的工程师会在网上直播自己编码的过程。这样做的一个原因，就是为了帮助管理自己的行为，因为这时候你必须更加集中注意力在自己的工作上。
还有一个是办公环境的因素。我们中国人的居住状态比较拥挤，在自己家里开辟出一个安静的、不被打搅的书房并不容易，这可能还跟中国的文化有关。而西方的文化，可能会更尊重个人的空间。
再说了，我们跟外国人的居住条件也确实不同。西方发达国家很早就开始了郊区化发展，大部分人会住在郊区和小城镇，自然环境比较好。而我们中国呢，大部分住在小区的楼房里。
不过，如果真的远程工作了，你也可以不住在大城市呀。网上有些视频经常会吸引我，某夫妇在乡村翻新出一栋漂亮的别墅，还拥有美丽的花园，等等。其实我目前就住在一个自然环境良好的山上，只不过从这个村子去办公室也很方便就是了。
远程办公还会产生心理上的挑战：白天晚上都在家里，会容易心理疲劳。而换个环境，反倒会让人兴奋起来。我就有个感觉，在家里工作久了，效率就会降低。而这时候再回公司工作的话，反倒更容易集中注意力。
而且，远程办公肯定也会降低沟通的效率。一些互联网公司，在设计办公室的时候，会故意设计一些空间，方便大家偶遇，闲聊几句。而做研发工作的同学都知道，这种看似随意的交流，有时候能激发出很多创新的思维。而如果总是自己苦思冥想，往往很快就会走入死胡同。这种线下偶遇式的沟通，往往见到了就会聊个几句。但在远程办公时，如果大家互相见不到面，还真就不聊了。
面对远程办公，我们要做好什么准备？所以，我们需要实行一些积极的操作，来更好地应对远程办公给我们带来的挑战，这样也能更好地抓住远程办公给我们带来的机遇。
从公司的角度出发那首先，我们来看看，对于企业来说，都需要做好什么准备。
第一，我觉得企业管理者要建立一个意识：远程办公是企业必须要面对的管理考验。
其实只要企业做大了，几乎都要面对远程管理的问题。比如你有了分公司，或者在各个城市设门店，甚至把生意做到国外。那么，突破地域的限制拓展业务，本来就是对企业能力的考验，是企业发展中必须踏过的门槛。
所以说，企业也一样需要获得位置自由。这些分公司、派出机构工作的人员，对于总部来说，本来就是远程工作的。有了这个意识，管理者就会开始放弃旧的思维，拥抱远程办公。
第二，从看住人，转换到管绩效。
很多比较传统的企业，他们的绩效标准都比较模糊，所以在远程办公的形势下，我们需要把绩效标准的清晰化、准确化放到第一位。像滴滴、外卖这些新职业，之所以能够迅速扩展规模，充分利用社会化人力资源，就是因为他们的工作绩效的标准是清晰而准确的。
第三，建立拥抱远程办公的文化，给员工授权和赋能。
像软件研发类的工作，它是知识密集型的，对员工的绩效评估比较难，人员更换的成本也相对较高。那么对于这类工作，我们可以多向那些开源软件公司学习，建立一个拥抱远程办公的公司文化，去吸引那些对工作充满兴趣和热爱的人参与进来。这些人，也会更加珍视公司给予的授权和自由。
第四，充分利用IT技术。
管理，一定要落实在工具上。我接触的那家芬兰公司，就花了很多年的时间，积累了一套成熟的内部管理系统。比如说，作为软件公司，你肯定要对项目进度、代码量、Bug数等基础指标有所管理才行吧？
信息技术成本的降低，也大大降低了远程管理的开销。这次疫情，促进了视频会议在全世界的普及。对于中国的中小企业来说，甚至可以0成本享受高品质的远程会议服务，这真是一个了不起的福利！
从员工的角度出发OK，说完了公司，那我们再来看看，从员工的角度出发，我们都要具备什么素质，才能更好地迎接远程办公模式。
第一，员工也要建立一个意识：无论是否远程办公，都要向绩效负责，管理好自己的工作。
即使你仍然在传统的办公模式下工作，如果你能像一个远程工作者那样对绩效负责，管理好自己的注意力，我想你很快就会获得领导的注意，从而赋予你更大的工作自由。你有没有听说过，张小龙经常睡懒觉迟到，而马化腾从来不管他？因为马化腾需要的是一个能做出微信来的张小龙，而不是一个每天按时打卡的张小龙。
第二，正视远程办公对自我管理的高要求，养成良好的工作习惯。
在办公室工作，会有环境来约束你。而当真的给了你位置自由以后，你其实要珍视这种自由，给自己定一些规矩，甚至给自己找一些监督（就像前面说的在网上直播），从而养成良好的工作习惯。
第三，建立激进的协作习惯。
由于远程工作对于协作的挑战，你必须建立激进的协作习惯，而不是仅仅停留在我们目前使用即时通讯工具和视频会议工具的习惯上。比如，你可以全时间视频在线、主动找人线上闲聊一小会儿、主动创造一些与人沟通的机会，等等。
第四，可能是最重要的：为兴趣而工作，为自己而工作。
人在没有很多督促的情况下，真正能驱动自己前行的动力，就是兴趣了。这个时候，你会把工作看作是促进自己成长的必要因素，从工作中成长，从成长中获得快乐。这个时候，你已经不是在为公司工作，而是为自己而工作。这样的人，才算获得了真正的自由。
小结今天，我们讨论了远程办公对公司和员工的好处、挑战，以及我们需要做好的准备。我讲了两个主要的观点。第一个观点是对企业的：远程办公管理能力是企业未来必须具备的能力。第二个观点是对个人的：只有能够管理好自己的人，才能抓住远程办公带来的机遇。</description></item><item><title>不定期加餐2_学习技术的过程，其实是训练心理素质的过程</title><link>https://artisanbox.github.io/7/42/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/42/</guid><description>你好，我是宫文学。
最近，高考刚刚结束。每年一度的高考都牵动了很多人的心，学生和家长们都把高考看作是人生的一大关键关口。可是，等上了大学以后呢？很多同学也会感到不适应，因为缺少了一个像高考那样明确的学习目标，也没有老师和家长在旁边不停地鞭策和关注。到了工作以后，就更是如此了。
对于进入计算机领域的人而言呢，很多人迫于找一份好工作的压力，会刻苦学习一段时间，包括参加各种学习班。而一旦获得了一份工作，融入职场以后，很容易就进入舒适区。反正当前的工作也能应付过去，为什么还要费力再去学呢？毕竟，工作已经够辛苦了。
在这种情况下，人生的第二次转折点就出现了。
有的人，能够管理好自己，充分利用各种时间和机会，不断地加深自己对技术的理解。虽然短时间看上去进步并不大，但成年累月地积累下来，效果就逐渐出现了，TA们开始能够胜任一些关键岗位，成了技术领头人。而另一些人，则只能掌握那些比较容易掌握的技术，时间一长就会显得平庸，等年轻人赶上来的时候，就更加没有竞争优势了。虽然这不是像高考一样，能马上分出重点大学和普通大学的差别来，但在进入职场5年、10年以后，这两类人在发展上的差别并不比高考带来的差别小。
我说这些，不是在贩卖焦虑，而是想引出我们今天要讨论的话题：从心理的角度看待学习技术的过程。特别是自己管理自己学习的过程、跟自己相处的过程。
学习没有轻松的。尤其是学习像编译原理这样的基础技术，就会显得挑战更大。想要学好它，调整和保持一个良好的心态是非常重要的。而通常，我们在心理上会面对三大问题：
第一，我为什么要学习这么难的技术？学一些比较容易的、应用层面的技术不就行了吗？这是学习的目的和动力问题。 第二，以我的能力，能学会这么难的技术吗？这是自信心和勇气的问题。 第三，如何看待学习中经常遇到的挫折？总是找不到时间怎么办？等等。这是学习过程中的心态调节和习惯养成问题。 如果对这三方面的问题，你都获得了清晰的答案，那么你应该就能保持好持续学习、终生学习的心态，从而对自己的人生有更好的掌控力。
那接下来，我就给你解读一下，我对于这三类问题的理解。
首先，我们来说说学习目的问题。
为什么要学这么难的技术？在做课程设计的时候，我和编辑同学都会尽量想着如何让这样的基础技术和你的日常工作关联起来，让你觉得它不是屠龙之术，而是能够在日常工作中发挥实际效用的。这确实是学习基础技术的收获之一。
不过，如果想长期坚持下去，我会建议你把心态调整成一种更高级的模式。用中国文化中的一句话来形容，就是“用出世的态度，做入世的事情”。如果一件事情你觉得应该去做，那就去做，不要太斤斤计较一些功利层面的东西。
那么对于学计算机而言，什么是我们应该去做的呢？那当然是要了解计算机领域的那些最基础的原理呀。如果搞了一辈子IT技术，却不明白其中的道理，那岂不是一辈子活得稀里糊涂的？
我知道，大部分人不注重基础性知识的原因，可能是觉得它们不会马上发挥作用。可是，那些最重要的知识、那些构成你知识结构的底蕴的东西，往往就是那些看上去不会马上有用的东西。
我个人非常欣赏复旦大学做教育的一种态度，就是教给学生无用之学。哲学、艺术、写作、演讲、逻辑学、历史等知识，在西方教育中被称作Liberal Arts，我们有时候翻译成通识教育，或者博雅教育。这些教育对于我们从事任何专业的工作，都是有用的。
比如说，美学素养。一个设计良好的系统架构，一定是优美的。新东方的元老之一王强，在美国学习计算机的时候，会把写完的程序拉开一定的距离看。如果看上去不够美观，那一定是程序设计得不够好。
你乍一听上去，可能会感觉是无稽之谈，但有经验的程序员一定会认同这个看法。那些写得有问题的程序，往往本身就是又臭又长、非常难读；而高质量的程序，往往是模块划分清晰、简洁易读的。做不出好的系统设计的人，肯定美学素养也不够高。像爱因斯坦等大科学家，往往驱动他们做出某个研究成果的动力，就是想去发现一条更加简洁、更具优美感的公式，因为真理往往是简洁的、优美的。
我之前公司的一名股东，他以前是一位很厉害的软件工程师，后来被一个外企挖走，担任了多年的销售副总。挖他去外企的原因，就是因为当时该外企刚开始在中国推广中间件的概念，他听了介绍以后就说，那不就跟我写的某软件的原理是一样的吗？并且一下子就说出了这类软件的关键技术点。于是，该外企下定决心要把他挖过去，并且是去负责销售。去年，他突然又写了一套科幻小说，名称是《云球》。我这里不是为他打广告，我是想说，做一个优秀的软件工程师、担任销售副总和小说家，这三个职业从表面上看相差很大，但其实背后依赖的基础素质都是一样的，都是那些乍一看上去没用的基础知识、基础素质。
所以，从这个角度，我是同意素质教育的理念的。一个缺乏美学素养、哲学素养和沟通能力等素质的软件工程师，潜力可能是有限的。
说到基础素养，我补充一个例子。有一次，我和前面说到的这位朋友在一起聊天，结果一个软件公司的老总给我们吹嘘他们公司开发的某软件平台。在说到一些特性的时候，听得我俩目瞪口呆。后来我们告诉这位老总，他声称的东西违背了基本的物理学和信息学的规律。在违背科学的底层规律的方向上做事情，那就相当于去造永动机，根本是虚妄的。这是很多项目失败的根本原因。
而另一些人，却具备抓住事情本质的能力。众所周知，马云并不懂技术。但就是不懂技术的马云，在懂技术的马化腾、李彦宏都认为云计算不是趋势，只不过是新瓶装旧酒的时候，果断拍板做云计算技术。期间，来自内部的反对声一直很强，大家都不愿意在内部使用尚不成熟的云计算技术。然而时间证明，马云的眼光更准。并且，力主开发云计算技术的王坚博士，他自己的专业也不是计算机专业。那么，为什么一拨非科班人士会比科班的技术大佬们看问题还准呢？我想可能是他们的无用之学学得更好，基础素质更全面吧。
所以，这就是我对于像编译原理、操作系统、算法等基础知识的态度。你就把它们看做是无用之学好了。我不仅鼓励你把这些基础知识学明白，并且我也希望你可以尽量再往深里挖一挖。比如，像图灵那样去思考一下，计算的本质到底是什么；编译原理用到的形式语言，也可以被继续深挖，从而跟整个西方科学体系底层的形式逻辑体系挂钩，以此去深入地理解希尔伯特猜想和哥德尔定理；了解面向对象、函数式编程这样的编程范式，跟人类的认知和思维模式的关系，跟Lamda计算、范畴论等数学工具的关系；你还可以去了解复杂科学领域的成果，并用这样的思维去看待大型复杂的信息系统。
如果你觉得编译原理这样的技术没啥用，那你一定会觉得我刚才说的那些更加没用。但我知道，一个优秀的软件工程师，其实一定是对我说的那些话题有所涉猎、有兴趣的。
总结起来，一个人的基础素质，决定了他的思维方式、思维质量和眼光，那些看上去没用的基础知识、基础原理，其实是真正做大事、承担重任所需要的素质。那，你到底要不要去学习呢？
好，如果你认可我的观点，那么我们接下来再探讨第二个话题，关于学习的信心问题。
我能学得会吗？很多人都会有一个担心，说某些基础技术、基础原理太难，自己的基础又不够好，那么能学得会吗？如果学了半天又学不会，那不是白费力气吗？
从能力角度，我必须承认，我们每个人都是有天赋的差异的。你让一个普通人去跟姚明比赛打篮球，那不是难为人吗？
学习这件事情也一样有天赋的问题。
我本人当年在高考的时候，是省里的前几名，但是等我到了北大，看到周围的同学通常也都是身手不凡；在记忆力方面，我也比不过很多同学，有的同学对普通的词汇书根本不感兴趣，会去背词典，甚至背专业领域的词典；在数学等需要逻辑思维的领域，我又比不过另一些同学，直到今天，对于一些涉及数学的问题，我都会去咨询这些同学的意见。
但从另一个角度讲，一些基础知识、基础原理，一定要有很强的天赋才能学会吗？
不是的。在人类知识的殿堂中，你要想增加一点新的理论、新的原理，那是非常难的。所以我们必须对那些大科学家们，那些计算机领域的先驱们顶礼膜拜。那些顶尖的工作，确实需要天赋，再加上努力和机缘。
不过，即使狭义相对论和广义相对论发明起来那么困难，但一般的理工科学生只要想学，都是可以弄明白的。这就证明了，发现知识和学习知识所需要的能力，是极大的不对称的。在高考季，经常会出现妈妈级、奶奶级的考生，从陪考到变成跟儿孙辈一起上大学的故事。人家奶奶都能考上大学，我们年轻大学生学不会本专业的一些基础原理，这个道理说得通吗？
同理，你常常会听到的一个理由也是不成立的，这个理由就是：我不是科班出身。这个我就不认真去反驳了。你想想看吧，费马的本职是律师，而他“业余”是个大数学家；数学家罗素却获得过诺贝尔文学奖；比尔·盖茨进的是哈佛大学商学院；我前面说的王坚博士是学心理的；罗永浩的专业也肯定跟IT没关系；刘慈欣是业余写小说的。
所以，那些所谓的困难，只是你给自己设的玻璃天花板。这不是个能力问题，而是个心理问题。儒家提倡“智、仁、勇”三种最高的道德标准，勇气是其中之一，它也是我们应该训练的一种品质呀。
好，如果你又一次认同了我的观点，那么我们再来讨论第三个问题，如何克服学习过程中的困难。
如何持之以恒？在我看来，如果理顺了前两个问题，也就是为什么要学，以及信心和勇气的问题，那么你最大的心魔其实就破除了。
但毕竟，学习贵在持之以恒的坚持。在这个过程中，我们可能会遇到很多的困难。但对于这些困难，我们也要用正确的心法来对待。所以，接下来我就针对如何面对学习中的困难、如何保证学习时间、如何找到学习的乐趣等常见问题，谈谈我的看法。
困难是必须的首先你得明白，有价值的东西，一定是要克服困难才能得到的，这是公平的。所以不要指望学知识而不需要付出努力，再好的教程和老师，也只是起到辅助作用。这里你得注意一个问题，就是不要被某些书籍和课程收了智商税，比如说，“7天学会XXX”，“学英语其实不用背单词”，等等。这种标题，就是违背学习的基本规律的。
所以，当你知道了苦难不可避免这个道理，那你剩下的就只有面对这些苦难。在学习中，你可能经常会被一个难点阻碍住，这很正常。你正确的心态应该是这样的：
没有我拿不下的山头，正面拿不下从侧面，侧面不行走背面。多换几个角度，多几次尝试，多看点参考资料，总会成功； 那么多人都能学会，我没有道理学不会，一定有更好的方法； 这个问题既然有难度，那价值一定也大，所以一定不要放弃。 有了这样的心态，其实再苦再难的事儿都好说了。
在旅途中发现乐趣我一个朋友最近正在从新疆骑行到西藏，全程3000公里，中间需要穿越无人区。这是他第三次做这样的骑行，之前已经骑过川藏线、青藏线。虽然过程很艰苦，但沿途美丽的风景，和跟自己相处的过程，就是这个旅途给他的回报。
我自己也喜欢户外。我家人有时不理解我，问我为什么要开着一辆大房车去那么远，累不累呀。我说，这就是旅行的意义呀。如果直接飞机过去，那有什么意思。
我用这两个例子作类比，是想告诉你：当我们学习那些有难度的知识的时候，其实肯定能发现出其中的乐趣来。比如，在学编译原理的时候，你去动手实现几个小例子，哪怕还不到实用的程度，但是好玩呀！当你找到了其中的乐趣，那么别人看你是在艰苦地学习，但其实你是乐在其中呢。就好像，别人看着一个人是在顶风冒雪一个人骑行，但他也是乐在其中呢！
另外呢，在互联网时代，各种不需要动脑的娱乐方式层出不穷。普通的人会在这种廉价的快乐中流连忘返。而如果你的目标是持续进步，那要培养自己另一种习惯，就是习惯于获得那些艰难的乐趣，这种乐趣是真正的充实的乐趣。
跟自己相处我前面举的朋友骑行的例子，他是自己一个人。我也喜欢自己开车出去，因为没有了其他人，也就避免了因为人际关系而导致的分神，你只需要关注大自然和你自己。你能感受到自己跟自己对话的过程，自己跟大自然对话的过程。
学习在大多数情况下也是一个人前行的过程，学到的知识也只属于你一个人。在这个时候，就只剩下了你要去攻克的知识，和你自己。你能感受到自己跟自己对话的过程，自己跟知识对话的过程。当遇到困难了，你能发现自己的苦闷和焦虑；当解决问题了，你能感受到自己的欣喜。
真正有价值的成绩，都是在这样的跟自己独处、跟自己对话的过程中做出来的。这是一种值得追求的素质。
跟志同道合者相伴独行难，众行易。除了那些内心特别强大的、从来都不屑于与普通人同行的天才，我们大部分普通人还是愿意有一些同伴一起结伴而行的，这样会大大降低驱动自己所需的努力。
我在读研时曾报过GRE的培训班。我感觉报班的最大作用，其实不是跟着老师学到多少知识，而是培训班乌泱乌泱的一大堆的同学，给我提供了一种气场，让我每天不想别的，赶紧学习就是了。
这样的群体还会有效改变自己的学习标准。在学GRE之前，我觉得一天背几十个单词已经挺辛苦的了。但到了GRE班，我很快就接受了每天背200个的新标准，因为其他人也是采用这个标准的。关键是，就算每天背200个，我也没觉得有多困难。所以你看，人的潜力有多大的弹性，而一个好的群体就是能无形中给人提供这种心理上的能量。
而且那时的同学都会有这种体会，就是每天如果不背单词就不舒服，上瘾。那段时间，随便看到一个单词，脑子里就会出现几个近义词和反义词，这种感觉很奇妙。再次印证了我前面说到的那种奋斗中的乐趣。
在软件领域，有很多技术社区，这些社区也能起到对人的心理加持作用，你可以善加利用。
最后，如果有要好的朋友和导师，能够鞭策你，那也非常难得。有管理经验的人都知道，虽然我们希望每个员工都有自我驱动的能力，但合适的外部驱动能降低员工驱动自己所需要消耗的努力。毕竟，我们大部分人其实是愿意工作在“低功耗模式”，能节省能量就节省能量。
使用运营思维在互联网时代，各种App在功能设计和运营上，充满了心理学的套路，以便培养用户的习惯。游戏公司更是会雇佣心理学专家，来设计各种套路。
那么，与其让别人套路你，不如自己套路自己，同样利用心理学的知识来培养自己的学习习惯，把自己的时间、自己的命运把握在自己手里，不是更好吗？
心理学的基础原理并不难，你自己就能从各种App的使用套路里体会到一些。比如说对取得的成绩即时给予奖励。从心理学的角度、从各种App背后的运营者的角度来看，我们每个人其实就是巴甫洛夫实验室里的动物而已。通过这样的自我训练，你可以达到一些很好的效果：
建立良好的学习流程，有明确的开始和结束时间；确认一下每天的学习目标和学习成果，或者可以建立学习过程的仪式感；给自己一个良好的环境。 没有学习的时间？那是不可能的。这是因为你没有给学习安排出专门的时间来。 以输出带动输入。很多同学有写技术博客的习惯，这个习惯非常好。因为你要写出东西来，所以会逼迫自己把思路理清楚。 激进一点的：直播自己的学习过程，给自己提供外部监督和激励机制。 小结今天这一讲，我聊了聊对于学习比较难的、比较基础的知识的心法的理解。总结起来，主要有三点：</description></item><item><title>不定期加餐3_这几年，打动我的两本好书</title><link>https://artisanbox.github.io/7/43/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/43/</guid><description>你好，我是宫文学。
在互联网时代，读书好像变成了一件挺奢侈的事情。因为我们现在获取信息的主要渠道，已经变成了网络。不过，在互联网统治力如此强劲的今天，我偶尔仍能发现一些好书，让我可以放下电脑和手机，对着厚厚的一大本，从头看到尾，甚至还会看很多遍。可见这些书确实是真正打动了我，让我这个理科背景的人，能以新的视角来看待世界，理解这个世界背后的运行规律。
我觉得一本书籍能达到这个阅读境界就很值得推荐了，因为这相当于是在帮助我们树立世界观、沉淀方法论。所以今天的加餐环节，我想给你分享两本打动我的好书，或者说是以其为代表的两类好书，跟你聊聊我读这些书的感受和收获，希望也能给你一些启迪。
那第一本书呢，就是《失控》。
《失控》失控这本书的作者是《连线》杂志的主编凯文 · 凯利，于1994年出版。这本书被很多人推崇，据说张小龙就曾说过，谁能看懂《失控》这本书，谁就可以到他那里工作。
这本书的神奇之处，在于它虽然成书于上个世纪90年代初，但准确预测了后来互联网时代的一系列的创新，更厉害的是它揭示了互联网时代背后蕴藏的道理。就如这本书的副标题所说的，它揭示了“全人类的最终命运和结局”。
我自己呢，是在读过这本书后，对其中的内容感觉极为惊讶。我甚至怀疑，凯文 · 凯利到底是何方神圣，为何他能够写出这样的惊世之作。
我就拿《失控》中第二章的内容，跟你一起分享一下，做一次管中窥豹。
第二章的标题是“蜂群思维”。蜜蜂是一种社会性昆虫，它们总是一大群一起生活。在研究蜂群的时候，你会发现，一群蜜蜂相当于构成了一个单独的生命体，这个生命体比单只的蜜蜂更加高级。举个例子，单只蜜蜂只有6天的记忆力，而一个蜂群却拥有三个月的记忆时间（这是个体记忆与群体记忆的区别之处）。另外这个生命体会比单只蜜蜂拥有更长的寿命，且具有繁殖能力，能分化出新的蜂群。
这样看起来，它似乎符合一个生命体的所有特征。而这种把很多个体连接起来，构成一个更高级的存在的现象，就叫做涌现（Emergence）。
另一个能很好地解释涌现的例子，就是人类的大脑。大脑中的神经元，实际上就是一个很简单的个体，它们只知道在接收到信号的时候，对其他神经元发送信号。而基于很多亿的神经元所涌现出来的大脑，却具备记忆、推理、情感等很高级的能力。试想，如果你是一个神经元，你其实是根本无法理解，以亿万个你构成的一个超级生命体，竟然会拥有情感、逻辑推理这种东西。因为在一个神经元的世界里，它只有接收信号和发送信号这两件事情。
你再往下思考，就会发现人类社会正是由亿万个你我构成的，那人类社会是不是一个超级生命体呢？这个生命体在思考什么，我们作为一个神经元级别的存在，如何能理解呢？或者说，思考仅仅是我们这个级别的个体所能理解的事情。而这个超级生命体所做的事情，可能已经根本不是人类的思考这种层面的事情了呢？早期人类的宗教，以及宗教中的神，也是高于单个的人类个体的。那么，它们是不是这个超级生命体在人类历史中早期的一种呈现方式呢？
我们再来假设一下，当前的互联网时代，连接网络的计算机、各种智能手机、智能设备越来越多，甚至已经开始接近大脑神经元的数量了。那么，它们会不会涌现出一个超级生命体？这个生命体是否会具备自己难以撼动的意志，而我们必须屈服于这种意志呢？
怎么样？这本书里的观点，是否也能同样给你带来启发，开一个大大的脑洞？是不是也引起了你去一读的兴趣呢？
这个级别的内容，在《失控》里还有很多。再举一个例子：活系统的特征是“摇摇晃晃的平衡”，而处于稳定的系统就进入了死寂。从这个角度看，如果我们的生活中问题不断，其实正是活系统的特征，因为我们要谋求持续的不均衡，这样我们才有机会去改变它，这总好过稳定的、死寂的生活。你看，这样的结论都已经带有了哲学的特征，让我们在面对生活中的挫折时，会采取更加积极的心态。
于是，还是回到我前头提到的那个疑惑：为什么凯文 · 凯利会有这么深刻的洞察力，远远超越我们这些普通人呢？
经过研究，我发现原来书中的很多观点，其实是对从上个世纪中叶以来，各学科的科研成果的总结，然后通过一个资深科技编辑的叙述普及给大众。看到这里，我才放心了：原来并不是出了一个多么逆天的天才，而是我自己对科技发展的新成果，以及其中蕴含的新思想缺少了解。这些思想或理论呢，包括了很多同学都知道的系统论、控制论和信息论三大论，以及后来的协同学、博弈论、突变论、混沌理论、分型理论、耗散结构和复杂性理论，等等。它们在过去的几十年间不断地发展，并正在形成一个宏大的、崭新的世界观和方法论体系。现在的一个新兴学科——复杂科学，似乎就是这些元素的集大成者。
我以前对复杂科学了解得不多，但我觉得其实也不能怪我。因为我们在中学、大学学的那些知识，大部分都是用来描述简单系统的。比如在大多数情况下，天体的运行就是一个简单系统，我们用相对论这样的理论就能准确地预测天体的行为。
而复杂系统，其构成部分之间的相互作用关系比较复杂，难以预测。我还是拿天体来说，三颗星体的相互作用，就变得难以预测了，这就是著名的三体现象，也是刘慈欣小说名称的来源。蝴蝶效应、混沌系统，等等，说的也是这个现象。
可以说，复杂系统破除了对还原论的迷信。也就是，即使你把一个系统分割成一个个的“零件”，并且完全理解了每个“零件”，你也可能无法理解整体系统。因为整体不等于部分的简单相加，比如说，就算你理解了一个社会的经济体中的每个企业和消费者的行为，你也无法准确掌控宏观经济。
可是，了解这些，对你我有什么意义呢？
我先讲一个小的用途。作为软件架构师，你其实应该知道，当一个软件系统复杂到一定程度的时候，你要把它看成一个动态演化的有机体。你对系统做的任何改动，都可能会引起一些你完全预料不到的结果。这就是为什么，你可以花一点儿钱甚至是免费就能搭建一套简单的电商系统，但是像淘宝这样的大型系统，则需要几千人来建设和维护它。
再举个例子。我们现在都非常熟悉的微服务架构，它的理念是，一个大型软件系统是从一个个分布式的、自治的单元中涌现出来的。流媒体巨头NetFlix，他们也是微服务架构的首批推动者之一。在NetFlix，软件工程师们会设计一些叫做Monkey的程序，随机地杀死一些服务，看看系统能否正常地自动修复。发现了吗？像微服务这样的复杂系统，它的冗余和自愈的能力已经像是一个生命体了，即使出现了一些突发的故障，比如某些服务的宕机，它也不会一下子全部瘫痪。
除了软件领域，与人类社会密切相关的系统，包括天气系统、生态系统、经济系统、社会系统，甚至包括人体本身，它们也都是复杂系统，所以现在的很多学科都在采用复杂系统的思维做研究。比如，采用演化的思维做心理学的研究，就形成了进化心理学的分支（其实更恰当的翻译方法是演化心理学，因为演化是没有方向性的）。这个学科的基本逻辑，就是现在人类具有的某种心理特质（比如为什么恋爱中男人更主动，女人更矜持），都是在进化中形成的。因为凡是没有这种心理特质的人类，都已经在进化过程中被淘汰了。
再进一步，其实你根据上面的信息可以得出一个结论：原来文科生研究的对象都是复杂系统。你一旦意识到这一点，你就可以通过复杂系统的研究成果，去研究原来属于文科生的研究范畴，比如说社会学、经济学、文学和哲学，从而拥有方法论上的优势。
给你简单举个例子，经济学中的宏观经济学部分，就是针对复杂系统的。这也是为什么大家总是说经济学家都是事后诸葛亮的原因：复杂系统是很难被简单地驾驭的。
甚至，你也可以用复杂科学的视角来重新审视哲学，特别是一些古代的哲学思维。因为基本上这些古老的哲学思想都是复杂系统的描述，是让你能够更好地适应自然系统和人类社会这两个复杂系统的一些解。比如说，儒家的思想，是理顺人际间的互动关系，从而缔造一个稳定的社会系统；而道家的思想，则是描述了包含人类社会和自然界的一个更大的系统规律。
有意思的是，凯文 · 凯利在《失控》的最后一讲，总结了复杂系统的特征，有很多地方跟道家的思想非常契合。比如说，“世界是从无中创造出来的”“从无数神经元的连接中，涌现出了大脑；而分子层面的连接，则涌现出了细胞”。
可以说，从《失控》这本书开始，就引起了我对复杂科学的兴趣，这个主题下的其他书籍，比如《复杂》，也非常值得你一读。
好，接下来，我再给你分享另一类好书，是关于文化的。而且它跟复杂科学这个主题，也是存在联系的。
文化与地方志我从大学起，就对“文化”这个主题非常感兴趣，跟东西方文化有关的东西我都乱看了一气。大学时我读过一本书，是房龙的《人类的故事》，非常喜欢，因为它不但描述了历史事实，还描述了推动历史发展背后的原因和规律。我当时想，如果历史都这么写，那么大家学历史的时候肯定不会觉得枯燥。
因为我的思维特点是非常理科生的，我很难记住那些相互之间没有逻辑关系的事实，我也很难接受强加过来的一套体系，除非我能弄清楚它背后的逻辑。而如果一本书，它能讲清楚事实背后的因果关系的脉络，就比较令人愉悦了。
而我前面所说的复杂系统的一些研究成果，就可以用来理解这些文化背后的逻辑规律。我挺喜欢的一个独立学者王东岳，他写了一本书叫做《物演通论》。王东岳很喜欢解读东西方文化背后的脉络，看他写的书就让人有一下子把厚厚的书读薄的感受，非常过瘾。
不过我想，如果我没有读过《失控》及其相关理论，我可能又会对王东岳此人惊为天人，对其著作惊为天书。但在有了前面的知识积累以后，我就不会那么惊讶了。因为王东岳先生的思考，也是建立在大半个世纪以来的科研成果的基础上的。物演的“演”字，就是演化思维的体现。当然，他能够进行提炼和再创造，构造一个完整的知识体系，也相当值得敬佩。
其实说了这么多，我的意思是，文化可以用复杂科学的思维来解构。这个方法，特别适合像我这样的、擅长逻辑思维的理科生们。每当你观察到一个文化现象，你都能解构出这背后的原因，岂不是很有意思呢？
作为一个北方人，我这几年大部分的时间都在厦门，对这里的闽南文化做了饶有兴趣的观察。去过厦门旅游的同学，应该都知道厦门的文艺气氛还挺浓厚的。那为什么厦门会有这种调调呢？还有，你在旅游的时候，应该会发现厦门的一种小吃，叫做沙茶面。那为什么沙茶面会在厦门文化中涌现出来呢？
这就需要结合闽南这个地方的地理、历史等各个要素及其互动关系来做分析。不过，我准备在课后的留言里，再分享我对这几个问题的看法。你有兴趣的话，也可以发表你的观点。
类似的文化方面的问题，还能提出很多来，比如：
为什么泉州会成为海上丝绸之路的起点？ 为什么孔圣人出在山东，而历代出状元最多的省份，却都在南方？ 中国有很多古镇，每个古镇在历史上肯定都是富甲一方的地方，那究竟是什么因素才促使它们兴盛起来的？ 如果某个地方有一个地理标识产品，是某种柿子，你能推测出那里的地质特点吗？ …… 去年的时候，我因为一个项目，翻阅了某县的县志，结果没想到我会对县志如此感兴趣，读得津津有味。我才发现，通过县志我能了解一个地方的地理、历史、经济、文化、重要人物等各种信息。通过这些信息，我基本上就能看到一个由很多要素相互作用构造出来的一个复杂系统，就能读懂当地各种文化的成因，这非常有意思。
中国的很多文化积淀很丰富。如果有机会能够一点一点地解读过去，那该多好。我估计我会一直保持阅读并解读地方志的兴趣。最近回老家，家人又给了我一本我们县在民国时代的县志。看着这些书籍，我有一种强烈的感觉：即使你是在这里生、这里长的，你也不一定真的了解本地的文化。
我为什么会推荐你去读地方志和其他讲解文化现象的书，读懂自己的本地文化呢？
第一层原因，是呼应我在加餐2“学习技术的过程，其实是训练心理素质的过程”中，提倡你多学点“无用之学”的观点。哪怕只是让你的灵魂更有趣，不是也挺好的吗？
第二层原因，是我作为一个理科生的思维方式。把自己所处的社会系统理解清楚，能够透过现象看到后面的逻辑，不是很有意思吗？
第三层原因，如果你能够运用复杂科学的思维，来理解现在的社会系统，其实是有实际意义的。举个例子，如果你要撰写一个商业计划，或者想给一个企业写一套软件，这就需要你理解其当前的商业系统、理解一个企业组织具体是如何运行的。而你之前的这些阅读积累，就会成为你的底蕴，成为你的智慧源泉呀！
小结今天这一讲，我推荐了两本书，或者说是两类书。一类书，是以《失控》为代表，讲述与复杂性相关的话题。另一类书，是以地方志为代表的文化载体。之所以给你推荐这两类书，是因为它们给我如何观察和理解这个世界开启了一扇窗户，并且给我这样一个严谨的理科生，提供了一条去打开文史哲的大门的独特的、有优势的途径，希望能对你有所启发。
思考一下 你有没有阅读过《失控》？你对复杂科学有什么了解？复杂科学在你的领域里有什么应用？ 你对自己出生地的文化了解吗？你有没有曾经发现一个文化现象背后的逻辑脉络？你觉得多研究点文化现象对于自己的职业生涯有好处吗？ 欢迎在留言区发表你的观点。如果今天的加餐让你有所启发，也欢迎把它分享给你的朋友。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>不定期加餐4_从身边的牛人身上，我学到的一些优秀品质</title><link>https://artisanbox.github.io/7/44/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/44/</guid><description>你好，我是宫文学。
今天的加餐环节，我想跟你分享一下让我很敬佩的那些牛人，以及从他们身上我所能学到的优秀品质。我希望你也能从这些人的故事上得到启发。这里为了叙述方便，我就不提具体的名字了。你只需要了解他的故事，从中有所感悟就好了。
我把这些牛人分为了两类，一类是搞技术的，一类是创业的。由于我自己也身兼两重身份，所以我很关注这两类人中能带给我启发的人。
首先来说说第一类人，搞技术的，也就是我们常说的极客们。
我所理解的极客我曾经在技术圈子里参加过比较多的活动，特别是开源圈子的活动，因此也接触了不少技术大牛，国内国外的都有。
早在2000年的时候，我就听过理查德·斯托曼（Richard Stallman）的讲座，听他布道自由软件。Stallman是GNU这个组织的创始人，他也发起了GPL开源协议。更重要的是，他是GCC编译器的主要作者，所以跟我们这门课也是很有渊源的。记得当时他给我们放一个幻灯片的时候，用的是Linux上的一个软件，界面没有微软的PowerPoint那么酷炫。但你能想到，Stallman是绝对不会用PowerPoint的。
后来在参加和组织开源技术活动的过程中，我也接触了不少国内国外的技术团队，他们在很多方面刷新了我的认知、拓宽了我的视野，也让我更理解极客都是一些什么样的人。
在我看来，这些人应该就是合格的极客。那么，怎样才能被称为极客？是技术水平高吗？我想不是的。技术水平高，其实只是一个结果。真正让极客显得与众不同的，其实是他们对待技术的态度，乃至是对待人生的态度。这些特质，也能给所有做技术的人一些启发。
首先，是热爱技术。
跟普通人只是把技术作为一个谋生的手段不同，极客们是真心喜欢技术，热衷于钻研和探讨各种技术细节。他们在对待工作的时候，绝不会把某项工作做到能交差就行，他们想要做到完美。
我之前公司的一位股东，他在做程序员的时候，曾经接到领导的一项任务，给了他一块语音板子，让他研究一下能否做呼叫中心的功能。两个星期以后，再问他，技术上是否可行？他说，已经做完了。不仅做完了，他还考虑了各种扩展性。比如，给他的板子只有八个语音口，但他写的程序考虑了用不同的板子，有不同的口的情况。以至于后来很多年，基于他的程序做的呼叫中心系统，底层都不用做很大的改动。
我这位朋友，我在加餐2中也提到过。他因为对于底层软件的深刻理解力，被挖到中间件公司做老总。后来又在创业什么的，最近又写了一套科幻小说。不管什么时候，我总能从他身上吸取到一些东西。
另一个例子，是我一个在苹果工作的同学提到的。这位同学负责苹果的文件系统的开发，我下面还会给你讲他的故事。这里是他讲的另一件事情。一次，一位博士分配到他们组，一时没有合适的工作给他干，就先让他做一阵测试。结果这位老兄，彻底升级了测试系统，对于大量的工作都实现了自动化测试，给整个团队带来了巨大的价值。
这个故事也让我更新了看待测试工作的视角。我现在基本上不会去招聘那些因为对自己的技术能力没有信心，而选择去做测试工作的人。我认为测试工作需要极大的技术热情才能做好。
我想，不管是从事什么岗位，能够热爱自己所做的事情，都是非常值得庆幸的。反过来，如果不喜欢自己所做的事情，为什么要去凑合呢？
换句话说，能够做自己喜欢的事情，其实是有所取舍、有所牺牲的。林纳斯·托瓦兹（Linus Torvalds）就喜欢领着一拨人折腾Linux。如果他去做某个大公司的CTO甚至是创业合伙人，也无不可。但他选择的是自己喜欢的生活方式。他没有太去想自己因此损失了多少发财的机会。
这就涉及到了第二点，就是极客们洒脱的生活态度。
极客们所展现出来的这个特质，从某种意义上来看是更具魅力的。很多极客，都是不愿意以“生活所迫”为借口，选择自己不喜欢的工作和生活方式。
我在加餐1分享远程办公话题的时候，就提到过一家芬兰公司。这家公司都是远程办公的，其中有的员工，是一边全球旅游，一边工作的。这些技术型的公司，正是以这种方式，吸引那些真正的极客加入。
还有一次我参加一个技术活动，我的朋友C指着一个老外说，这家伙在泰国买了一个小岛自己住，还弄了个度假村什么的。说实话，这样的归园田居的生活方式，对像我们这样浸染在中华文化中的人来说，是有很大的吸引力的。但我们有多少人敢于不从众，去选择自己喜欢的生活方式呢？
我还有的朋友是依托自己的技术创业的。创业这件事当然很不容易，但他们通常都会保持积极乐观的态度，并没有因为自己的项目没有及时被社会认可，就变得垂头丧气。
那第三点，就是极客们看待这个世界的方式：用代码说话。
极客这群人，是不大讲政治的。他们一般只认真实的本事。Linus就有一句名言“Talk is cheap, show me the code.”，这也代表了极客们的精神。一个人的水平如何，看看他写的代码，或者至少看看他发表的文章，其实差不多就有数了，这个是伪装不了的。
早在智能手机流行前，有一次聚会，我一个搞Linux的朋友F，就拿出了一台手机，里面装着Linux、图形界面、App什么的，看着都还行。这都是他鼓捣出来的。其实再加把劲，比如支持用Java开发应用，这就是一个Android系统了。而Android的创始人安迪·鲁宾（Andy Rubin），差不多也是这样一个极客。前一阵，我跟一个公司的老总聊天。他问我，为什么中国搞不出安卓来？我给他解释了原因。其实不是我们没有这样的技术，在极客们的眼里，最早的那个安卓版本也没什么大不了的。只不过我们没有掌握技术生态而已。
极客们一般对系统底层的技术都比较熟悉。像安卓系统这种看似很高大上的东西，不会让他们心生畏惧。这些人在互相交流的时候，也会谈论一些底层技术。几句话下来，心里已经有数。
然后呢？他们之间会缔结惺惺相惜的友谊。两个极客之间的交往可以极其简单，他们甚至不需要见过面，只需要见过对方的代码，或者读过对方的文章，就会认可彼此。如果有事情，直接打招呼就行。
某互联网大厂是如何把自己的底层技术搞扎实的呢？据我了解，就是找到一个开源圈的大牛。这位大牛进去了以后，又给技术社区的其他人打招呼，说这里有什么技术难题需要解决，过来吧。于是就聚集了一个小组的牛人，搞出了非常好的成绩。这就是极客们的特殊的社交方式：他们知道who is who，并且志同道合的人愿意聚到一起。如果光靠HR部门和猎头公司来做，要想达成这样的结果是很难的。
Github在某种意义上也是把极客们的这种倾向给充分地引导了出来。它从一个代码托管工具，几乎已经变成了程序员的社交网站。
这里我是想说明一个观点，那就是技术人并没有怀才不遇这一说。把真本事亮出来，所有的事情会变得简单很多。
好了，这就是我总结的极客们给我的三点印象：热爱技术、生活洒脱、凭本事说话。这些特质，都是我很欣赏的，也常常作为参照来调整自己。
比如说，我觉得自己也挺热爱技术的，但是在前些年，我觉得自己不够洒脱，做不好取舍，总是想各方面都兼顾，结果哪方面都顾不好。所以还不如在自己喜欢的事情上全情投入，不去计较太多得失，反倒会更加心情愉快，做事情的结果也更好。
你可能会问，那这些极客都发展得怎么样呢？
我所认识的极客，有的是在小公司工作，有的是在大公司工作，还有的是在创业。不过，不管从事什么岗位，似乎都发展得不错。我想，这是因为他们从底层上选择了一个更好的发展逻辑：首先是做好取舍，让自己能够专注技术；在拥有了比较好的技术底蕴以后，他们也有更好的施展自己才华的平台；在专注于技术价值的同时，他们的生活也变得简单和健康。
OK，讲完了搞技术的，我再讲讲搞创业的朋友的故事，以及他们给我的启迪。
创业者这个物种我周围的朋友有不少是搞创业的。这些人往往都有一些很特别的点，让我欣赏、赞叹乃至仰慕。
首先一点，是坚韧不拔的意志力。
我们都知道，创业肯定不是简单的事情。而让一个企业能够穿越惊涛骇浪，最重要的就是创始人坚韧不拔的意志品质。
我本科的同班同学中，就有一个创业者，他公司的主营业务是户外运动用品，目前已经做到上市了。他的性格就很坚韧，我给你讲两个故事。
第一个，在他成为我北大的同学之前，其实曾经考上了一所技术类的军校。但后来他发现自己并不喜欢那里，于是就想退学。可是，军校岂可以当作儿戏，想来就来，想走就走？为了能够退学，他想了很多办法，包括自己注射大剂量的抗生素，产生精神恍惚的效果，以便让校医诊断为精神疾病；此法不成，又从3楼的阳台上滚下来，想把胳膊摔断，以此理由退学……后来校领导实在看他态度坚决，也就同意了他退学。他又重新参加了高考，选择了他喜欢的学校和专业。
第二个，大约在2006年，我们一些同学因为毕业十周年又聚到了一起，去内蒙古草原上玩，其中一项活动就是骑马。我的这位同学骑术很好，在草原上策马狂奔。不过，在一个地方，突然马失前蹄，他从马背上摔了下来。这真是很惊险的一个意外，我们在场一群人看了都心惊肉跳。不过，他休息了一会以后，又要了一匹马，上马继续策马狂奔。晚上，我们问他，为什么刚摔了又骑？他说，如果今天不骑，以后就不敢骑了。
说到这，我想再多讲一个例子。这是我同一级的另一个同学的故事，他是社会学系的。如果我说他的名字，很多同学应该都会知道。他从2000年开始做一个与汽车有关的网站，结果后来互联网泡沫破裂，然后投资人撤资。他就自己筹了2000万买下了投资人的股份，坚持做了下去，直到2011年上市。想想看，那个年头的2000万，是多大的压力。但他就是咬着牙挺过来了。
我不知道有多少人能拥有像他们这样钢铁般的意志力。并且，令我沮丧的是，我怀疑这种个性可能主要是天生的？反正我是万万难以做到的。所以，创业这件事情，其实不是每个人都适合去做。而我这两个同学能做到上市，也绝不是偶然。
不过，为了不让自己的希望完全破灭，我还是倾向于相信意志和勇气这样的事情，至少在部分上是可以后天磨炼的。我在大学的时候练习过拳击，因为我觉得拳击可以锻炼人的勇气。来拳的时候不能眨眼，是拳击运动员的基本素质。那么在创业中，如果我们每次都去积极地面对挑战，那面对困难的能力也会越来越强。
我认识的其他几个创业者，虽然不像这两位那么夸张，但在意志力方面，也都属于罕见的。比如，某个技术社区，其创始人能够做到天天更新内容，十年如一日，这就是常人所不及的。最近我通过写编译原理的课程，也对内容编写这件事有了一定的体会。这样的事情，做一个星期、一个月、一个季度，是凭着兴趣和热情就可以做到的。而长年累月地去做，你要说没有意志力的因素，那是不可能的。
说完了强大的意志力，我再来说说我钦佩的这些人的第二点品质，就是有主见，不从众。
我观察，这些创业成功的人，往往判断事情都有自己的标准，这些标准往往与大众是不一致的。
还是说我同班的那个同学。在学校读书的时候，他经常就会消失不见了，过一阵再重新出现，他告诉我们，这次去陕北了，有什么感受，怎样怎样。过一段时间又会消失，回来后，说自己在新疆沙漠里独自走了几天，遇到被人追赶，差点殒命，等等等等。
等到快期末考试的时候，他拉着我在未名湖边给他补习高数，说能及格就行。几年以后，在创业的过程中，他还读了个清华的MBA班，也是连毕业证都没要。按他的意思来说，就是：学到知识就行了。证书什么的，不重要。
而我们这些俗人，天天使劲读书。等到毕业以后，又根据习惯和潮流，很多人又去出国，虽然我敢说，大部分同学那时候都想不清楚出国到底要干嘛。
所以从某种意义上来讲，比尔·盖茨、马克·扎克伯格等人敢于辍学创业，本身就意味着他们不是一般人。
而作为对比，还有一些人，都不管自己什么年纪了还在花高价去混文凭，不停地想往自己身上贴一些标签，来为自己壮胆。我觉得，这些人不要说创业了，给他一个重要的职位都是一件很冒险的事情。
前面我也提到了有些极客，会基于自己的兴趣爱好来创业。他们喜欢的技术和产品，往往在很长的时间内都不会得到社会的认可，不能变成有经济价值的商品。然而他们就是会坚持自己的方向。这些人，也是我学习的榜样。
这些技术创业者，有的发展比较顺利，但似乎也不是刻意为之。比如上海的小X，我跟他在技术活动上有几面之缘。他搞了一个用于物联网的小小的OS，搞了很多年了，前两年突然听说融了很多资，估值不错。我觉得资本投在这些人身上是投对了。
也有的朋友，会经历一些坎坷。但是他们总是按照自己的方式去折腾，保持对科技发展趋势的敏锐观察。每隔一段时间，我总能从他们那里听到一些新的思想和动态。就拿我一个做移动端底层平台朋友来说，他做这个方向已经很多年了。我相信他肯定会做成。不过先不说未来结果如何，至少我觉得他的生活状态是洒脱的、阳光的、不纠结的。
小结今天的加餐，我给你分享了周围搞技术的和做创业的一些朋友的故事。这些故事跟你有什么关联呢？
首先，你选择了编译原理这门课程，基本上已经说明你有成为一名极客的潜质，否则也不会给自己这个挑战。但是在这个过程中呢，你可能会遇到很多的困难和心理上的纠结。我希望通过我分享的故事，能够帮助你做好取舍，丢掉包袱，健康阳光地拥抱作为一个技术从业者的职业生涯。
而如果你不小心选择了创业这条路，我也希望你能够像故事中的人物一样，去磨炼自己的意志力，以及坚持自己的主见。成功不成功不敢保证，至少你的生活会是很有价值的，不会后悔的。
以上也是对我自己的勉励，希望能跟你共勉。如果你或你身边也有类似的故事，欢迎在留言区分享出来。同样，也非常欢迎你把这一讲分享出去。</description></item><item><title>不定期加餐5_借助实例，探究C++编译器的内部机制</title><link>https://artisanbox.github.io/7/45/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/45/</guid><description>你好，我是宫文学。欢迎来到编译原理实战课的加餐环节，今天我们来探讨一下C++的编译器。
在前面的课程中，我们已经一起解析了很多语言的编译器了，但一直没有讨论C和C++的编译器。并不是因为它们不重要，而是因为C语言家族的编译器实现起来要更复杂一些，阅读代码的难度也更高一些，会对初学者造成比较大的挑战。
不过，没有解析C和C++语言的特性及其编译器的实现，其实在我心里也多多少少有点遗憾，因为C和C++是很经典的语言。至今为止，我们仍然有一些编程任务是很难用其他语言来代替的，比如，针对响应时间和内存访问量，需要做精确控制的高性能的服务端程序，以及一些系统级的编程任务，等等。
C和C++有很多个编译器，今天我们要研究的是Clang编译器。其实它只是前端编译器，而后端用的是LLVM。之所以选择Clang，是因为它的模块划分更清晰，更便于理解，并且还可以跟课程里介绍过的LLVM后端工具串联起来学习。
另外，因为C++语言的特性比较多，编译器实现起来也比较复杂一些，下手阅读编译器的源代码会让人觉得有点挑战。所以今天这一讲，我的主要目的，就是给你展示如何借助调试工具，深入到Clang的内部，去理解它的运行机制。
我们会具体探究哪个特性呢？我选择了C++的模板技术。这个技术是很多人学习C++时感觉有困难的一个技术点。通过探究它在编译器中的实现过程，你不仅会加深了解编译器是如何支持元编程的，也能够加深对C++模板技术本身的了解。
那么下面，我们就先来认识一下Clang这个前端。
认识ClangClang是LLVM的一个子项目，它是C、C++和Objective-C的前端。在llvm.org的官方网站上，你可以下载Clang+LLVM的源代码，这次我用的是10.0.1版本。为了省事，你可以下载带有全部子项目的代码，这样就同时包含了LLVM和Clang。然后你可以参考官网的文档，用Cmake编译一下。
我使用的命令如下，你可以参考：
cd llvm-project-10.0.1 #创建用于编译的目录 mkdir build cd build
#生成用于编译的文件 cmake -DCMAKE_BUILD_TYPE=Debug -DLLVM_TARGETS_TO_BUILD=&amp;quot;X86&amp;quot; -DLLVM_BUILD_EXAMPLES=ON ../llvm
#调用底层的build工具去执行具体的build cmake &amp;ndash;build . 这里你要注意的地方，是我为Cmake提供的一些变量的值。我让Cmake只为x86架构生成代码，这样可以大大降低编译工作量，也减少了对磁盘空间的占用；并且我是编译成了debug的版本，这样的话，我就可以用LLDB或其他调试工具，来跟踪Clang编译C++代码的过程。
编译完毕以后，你要把llvm-project-10.0.1 /build/bin目录加到PATH中，以便在命令行使用Clang和LLVM的各种工具。你可以写一个简单的C++程序，比如说foo.cpp，然后就可以用“clang++ foo.cpp”来编译这个程序。
补充：如果你像我一样，是在macOS上编译C++程序，并且使用了像iostream这样的标准库，那么可能编译器会报找不到头文件的错误。这是我们经常会遇到的一个问题。
&amp;nbsp;
这个时候，你需要安装Xcode的命令行工具。甚至还要像我一样，在.zshrc文件中设置两个环境变量：
export CPLUS_INCLUDE_PATH=&amp;quot;/Library/Developer/CommandLineTools/usr/include/c++/v1:$CPLUS_INCLUDE_PATH&amp;quot; export SDKROOT=&amp;quot;/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk&amp;quot; 好，到目前为止，你就把Clang的环境配置好了。那回过头来，你可以先去看看Clang的源代码结构。
你会看到，Clang的源代码主要分为两个部分：头文件（.h文件）全部放在include目录下，而.cpp文件则都放在了lib目录下。这两个目录下的子目录结构是一致的，每个子目录代表了一个模块，模块的划分还是很清晰的。比如：
AST目录：包含了AST的数据结构，以及对AST进行遍历处理的功能。 Lex目录：词法分析功能。 Parse目录：语法分析功能。 Sema目录：语义分析功能（Sema是Sematic Analysis的缩写）。 接下来，你可以进入这些目录，去寻找一下词法分析、语法分析、语义分析等功能的实现。由于Clang的代码组织很清晰，你可以很轻松地根据源代码的名称猜到它的功能，从而找到语法分析等功能的具体实现。
现在，你可以先猜测一下，Clang的词法分析和语法分析都是如何实现的呢？
如果你已经学过了第二个模块中几个编译器的实现，可能就会猜测得非常准确，因为你已经在Java编译器、Go的编译器、V8的编译器中多次见到了这种实现思路：
词法分析：手写的词法分析器，也就是用手工的方法构造有限自动机。 语法分析：总体上，采用了手写的递归下降解析器；在表达式解析部分，采用的是运算符优先级解析器。 所以，针对词法分析和语法分析的内容，我们就不多展开了。
那么，Clang的语义分析有什么特点呢？
通过前面课程的学习，现在你已经知道，语义分析首先要做的是建立符号表，并做引用消解。C和C++在这方面的实现比较简单。简单在哪里呢？因为它要求必须声明在前，使用在后，这就让引用消解变得很简单。
而更现代一些的语言，在声明和使用的顺序上可以更加自由，比如Java类中，方法中可以引用类成员变量和其他方法，而被引用的成员变量和方法可以在该方法之后声明。这种情况，对引用消解算法的要求就要更高一些。
然后，现在你也知道，在语义分析阶段，编译器还要做类型检查、类型推导和其他很多的语义检查。这些方面Clang实现得也很清晰，你可以去看它的StaticAnalysis模块。
最后，在语义分析阶段，Clang还会做一些更加复杂的工作，比如C++的模板元编程机制。
我在探究元编程的那一讲中，介绍过C++的模板机制，它能有效地提高代码的复用能力。比如，你可以实现一个树的容器类，用来保存整型、浮点型等各种不同类型的数据，并且它不会像Java的泛型那样浪费额外的存储空间。因为C++的模板机制，会根据不同的模板类型生成不同的代码。
那么，C++具体是如何实现这一套机制的呢？接下来我就带你一起去深入了解一下，从而让你对模板元编程技术的理解也更加深入。
揭秘模板的实现机制首先，我们通过一个示例程序，来观察一下Clang是如何编译模板程序的。假设，你写了一个简单的函数min，用来比较两个参数的大小，并返回比较小的那个参数。
int min(float a, float b){ return a&amp;lt;b ? a : b; } 你可以用clang++命令带上“-ast-dump”参数来编译这个示例程序，并显示编译后产生的AST。</description></item><item><title>划重点_7种编译器的核心概念与算法</title><link>https://artisanbox.github.io/7/46/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/46/</guid><description>你好，我是编辑王惠。
阶段性的总结复习和验证成果是非常重要的。所以，在8月7日到8月12日这为期一周的期中复习时间里，我们先来巩固一下“真实编译器解析篇”中的重点知识。你可以通过学习委员朱英达总结梳理的划重点内容，以及涵盖了关键知识点的7张思维导图，来回顾7种语言编译器的核心概念与算法。
另外，宫老师还精心策划了10道考试题，让你能在行至半程之时，做好自检，及时发现知识漏洞，到时候一起来挑战一下吧！
在期中复习周的最后，我还会邀请一位优秀的同学来做一次学习分享。通过他的学习故事，你也可以借此对照一下自己的编译原理学习之路。
好，下面我们就一起来复习这些核心的编译原理概念与算法知识吧。
Java编译器（javac）Java是一种广泛使用的计算机编程语言，主要应用于企业级Web应用开发、大型分布式系统以及移动应用开发（Android）。到现在，Java已经是一门非常成熟的语言了，而且它也在不断进化、与时俱进，泛型、函数式编程、模块化等特性陆续都增加了进来。与此同时，Java的编译器和虚拟机中所采用的技术，也比 20 年前发生了天翻地覆的变化。
Java的字节码编译器（javac）是用Java编写的，它实现了自举。启动Java编译器需要Java虚拟机（默认是HotSpot虚拟机，使用C++编写）作为宿主环境。
javac编译器的编译过程，主要涉及到了这样一些关键概念和核心算法：
词法分析阶段：基于有限自动机的理论实现。在处理标识符与关键字重叠的问题上，采用了先都作为标识符识别出来，然后再把其中的关键词挑出来的方式。 语法分析阶段：使用了自顶向下的递归下降算法、LL(k)方式以及多Token预读；处理左递归问题时，采用了标准的改写文法的方法；处理二元表达式时，采用了自底向上的运算符优先级解析器。 语义分析阶段：会分为多个小的阶段，且并不是顺序执行的，而是各阶段交织在一起。 语义分析阶段主要包含：ENTER（建立符号表）、PROCESS（处理注解）、ATTR（属性分析）、FLOW（数据流分析）、TRANSTYPES（处理泛型）、TRANSPATTERNS（处理模式匹配）、UNLAMBDA（处理 Lambda）和 LOWER（处理其他所有的语法糖，比如内部类、foreach 循环等）、GENERATE 阶段（生成字节码）等。在ATTR和FLOW这两个阶段，编译器完成了主要的语义检查工作。 注意：生成字节码是一个比较机械的过程，编译器只需要对 AST 进行深度优先的遍历即可。在这个过程中会用到前几个阶段形成的属性信息，特别是类型信息。 参考资料：
关于注解的官方教程，参考这个链接。 关于数据流分析的理论性内容，参考龙书（Compilers Principles, Techniques and Tools）第二版的9.2和9.3节。也可以参考《编译原理之美》 的第27、28讲，那里进行了比较直观的讲述。 关于半格这个数学工具，可以参考龙书第二版的9.3.1部分，也可以参考《编译原理之美》的第28讲。 Java语言规范第六章，参考Java虚拟机指令集。 Java JIT编译器（Graal）对于编译目标为机器码的Java后端的编译器来说，主要可以分AOT和JIT两类：如果是在运行前一次性生成，就叫做提前编译（AOT）；如果是在运行时按需生成机器码，就叫做即时编译（JIT）。Java以及基于JVM的语言，都受益于JVM的JIT编译器。
在JDK的源代码中，你能找到src/hotspot目录，这是 JVM 的运行时：HotSpot虚拟机，它是用C++编写的，其中就包括JIT编译器。
Graal是Oracle公司推出的一个完全用Java语言编写的JIT编译器。Graal编译器有两个特点：内存安全（相比C++实现的Java JIT编译器而言）；与Java配套的各种工具（比如ID）更友好、更丰富。
Java JIT编译器的编译过程，主要涉及到了这样一些关键概念和核心算法：
分层编译：C0（解释器）、C1（客户端编译器）、C2（服务端编译器）。不同阶段的代码优化激进的程度不同，且存在C2降级回C1的逆优化。 IR采用了“节点之海（Sea of Nodes）”，整合了控制流图与数据流图，符合 SSA 格式，有利于优化算法的编写和维护。 两个重要的优化算法：内联优化和逃逸分析。 几个重要的数据结构：HIR（硬件无关的IR）、LIR（硬件相关的IR）、CFG（控制流图）。 寄存器分配算法：LinearScan。 金句摘录：“编译器开发的真正的工作量，都在中后端。”
参考资料：
GraalVM项目的官方网站；Graal的Github地址；Graal项目的出版物。 基于图的IR的必读论文：程序依赖图-J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program dependence graph and its use in optimization.</description></item><item><title>学习指南_如何学习这门编译原理实战课？</title><link>https://artisanbox.github.io/7/53/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/53/</guid><description>你好，欢迎来到《编译原理实战课》，我是专栏编辑王惠，很高兴认识你。
我们都知道，“编译原理”是一门特别硬核的计算机基础专业课。你是不是也觉得编译原理知识就像是一片望不到头的大海，任自己在里面怎么扑腾、怎么挣扎都游不到学成的对岸。但是没关系，现在我们可以跟着宫老师的脚步一起探索编译的旅程了。
不过在正式开始学习这门课程之前，我想先和你聊聊这门课程的一些设计思路和特设板块，帮你找到最适合自己的学习方式，让你后面的学习能达到事半功倍的效果。
我们有“学习委员”了首先来说个好消息，咱这门课呢有学习委员陪伴我们一起学。担当学委的是我们的资深用户朱英达同学，他曾就职于百度，履任资深研发工程师，擅长Web前后端相关领域技术，对编译技术在业务场景下的应用也有自己的理解。
他的经历可能和你很相似：作为一名计算机科班出身的程序员，在大学课堂中学习过编译原理这门课，但面对教科书上庞杂的知识体系、晦涩的抽象概念、陈旧的代码用例，无奈只学了个一知半解；工作以后，作为一名一线的Coder，在大厂的环境里，看惯了层出不穷的“造轮子怪象”，最终才发现只有掌握像编译原理这样的底层技术，才是真正的精进之道。所以，他想把编译原理这门课重新捡起来，再学一次。
然而，目前市面上编译方面的技术资料却非常匮乏，被学界奉为经典的“龙书”“虎书”“鲸书”，对初学者来说又不够友好。后来，他遇到了宫老师的《编译原理之美》，跟着老师的思路重走了一遭编译之旅，发现自己之前对于编译技术的很多困惑点都迎刃而解了。比如说，宫老师在阐释虚拟机架构时，谈到了栈机和寄存器架构的优劣，这就对他理解V8引擎在虚拟机架构选型上提供了非常好的参考。
朱学委最终也发现，编译技术的学习绝对不能纸上谈兵，只有把学到的理论知识与自己从事的相关技术领域结合起来，才会真正有所感悟。
你看，编译原理或者说所有的技术，都有这么一个反复学习、反复印证的过程。所以在这门课程中，学委将会基于积累的编译原理基础，以及对这门新课程内容的学习，不定期地分享他学习编译原理的方法和思路，和你一起探讨课程要掌握的要点和难点。当然了，学委也会在留言区督促你交作业，和你一起交流讨论。
有了学委的陪伴，相信你再学习这门课，一定可以事半功倍。
如何学习预备知识模块？接下来，我来说说怎么利用好预备知识模块。
那我先来交代下为什么要特别设计这个模块。就像老师在开篇词中所说的，这门课程会带你一起阅读真实语言编译器的源码，跟踪它们的运行过程，分析编译过程的每一步是如何实现的，并会对有特点的编译技术点加以分析和点评。
但在解析编译器的过程中，一定会涉及到很多编译原理的基础概念、理论和算法，如果你从来没有接触过或者不够了解这些编译原理知识，那必然会在一定程度上影响你后面的学习效果。所以，预备知识模块就是帮你先建立起一个初步的编译原理知识体系，打好基础，为后面的学习做好准备。
如果你已经学过老师的第一季课程《编译原理之美》，预备篇的内容也建议你不要跳过。和第一季课程相比，在这个模块里宫老师会以更加高屋建瓴的方式，来重新交付编译基础知识。所以，你一定要利用这个模块来查漏补缺。
那具体怎么做呢？建议你先看每一讲的标题，然后回顾自己已经学过的、掌握了的知识要点，写下来，写好后再开始学习，学完后对比总结心得。千万不要错过这个再学一次的机会。我们都知道重复是学习的关键一环，相信通过这个模块，你一定能在编译技术的理解上更上层楼。
你可以把预备知识理解为编译基础的一个串讲，涉及到的概念会比较多。所以学习这个模块的时候，我建议你每学完一讲都要自己动手画一下这一讲的知识地图。等8篇结束后，学习委员也会总结一张编译原理的核心基础知识大地图。到时候你可以对比来看，给自己一个直接的反馈。然后一定要利用这张图，在脑子里构建起编译原理的知识框架。这样，你就做好了进入下个模块的学习准备啦。
解析7种语言编译器的过程中，你需要做什么？下面我来说说课程的重头戏，也就是解析7种语言的编译器，包括Java编译器（javac）、Java的JIT编译器（Graal）、Python编译器（CPython）、JavaScript编译器（V8）、Julia语言的编译器、Go语言的编译器（gc），以及MySQL的编译器。
这些编译器都是宫老师精选出来的，具有一定的代表性、采用了不同的编译技术，而且其中某一门语言也非常可能就是你在使用的。我们的课程就是从实战的角度切入，用你最擅长的方式（写代码、读代码）带你分析这些编译器。所以学好这门课的关键就是要动手实践，跟随老师的脚步来亲身体验不同编译器的实现机制。
我建议你最好在学习的过程中手边备着一台电脑，或者是一台能查看到源代码的其他设备，工具不重要，趁手最有效。你在自己上手修改源码的时候，就会发现对编译原理的概念理解得更加深入了。
期中复习周，停下来是为了跑得更快接着来说说期中复习周。这一周安排在“真实编译器解析篇”之后，也就是建立在你已经学习并理解了7种不同语言编译器的运行机制之后。设置复习周的目的，就是想要让你能及时、系统地了解自己前半段课程内容的掌握情况，发现学习上的漏洞，并及时弥补。
在这一周，学委首先会帮你划出复习的重点，给你总结前面解析的7种语言编译器所涉及到的核心知识。总结复习的过程，也就是你在提高编译技术能力的过程。
接下来，老师会给你出一套考试题。通过这次测试，你可以验证一下自己的学习方式是否有效，希望你能够及时调整学习心态和方法，更有效率地进行下一阶段的学习。
另外，在消化知识的同时，你还可以通过其他同学分享的心得，去看看他是如何学习、掌握编译原理知识的，毕竟通过借鉴别人来完善自己也是一种很好的学习方法嘛。
Learning by Sharing，分享了才知道自己那么优秀再接下来，我必须得说说“一课一思”这个学习环节了。
一课一思是每一讲最后的固定模块，具体内容呢，要么是给你留了一道动手实践的作业，要么就是抛出一个开放性的问题，引导你发散思考。如果你对这些问题都有自己的见解或者看法，那就不妨在留言区分享出来。这样渐渐地，你会发现自己就能解答一些同学的问题了，这是非常好的自检学习成果的方式。
另外别忘记了，极客时间还有一个社区交流的版块“部落”。在日常工作中，你一定会经常接触各种代码，也一定有自己非常熟悉的一门或多门编程语言。那么在解析了不同语言的编译器以后，你可以在部落里分享自己对于熟悉的或不熟悉的语言编译器的理解。
比如说，你原来深耕在Java领域，那么在学完了javac编译器和Graal编译器以后，你对Java是不是就有更深刻的理解了？在学完了Python的编译器以后，你是不是对这两门语言之间的共性和特性都更加清晰了？这些思考你都可以分享在部落里，通过分享自己所习得的知识，你会获得更好的成长。
如何验收学习成果？最后，在课程的收尾阶段呢，老师还会跟你一起关注一个热点话题，那就是华为的方舟编译器。相信很多同学对于国产的编译器，一直都是翘首以盼的。华为已经公开了一部分源代码，虽然资料仍然很缺乏，但是通过我们课程的学习，你是否有能力看懂华为的编译器呢？从掌握书本上的原理，到读懂流行的语言，再到理解方舟编译器的实现思路，这会是你能力一步步提升的过程。最终，你甚至可以参与到一款严肃的编译器的研发当中了。
好了，以上就是我想让你重点关注的课程设计和特设板块内容。编译原理是个难啃的硬骨头，但是我相信，只要你保有这份一定要吃透编译技术核心知识的决心，有计划、有重点，结合实践进行学习，就没有什么是看不懂、学不会的了。加油吧，祝你学有所成！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.</description></item><item><title>开篇词_在真实世界的编译器中游历</title><link>https://artisanbox.github.io/7/48/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/48/</guid><description>你好，我是宫文学，一名技术创业者，现在是北京物演科技的CEO，很高兴在这里跟你见面。
我在IT领域里已经工作有20多年了。这其中，我个人比较感兴趣的，也是倾注时间精力最多的，是做基础平台类的软件，比如国内最早一批的BPM平台、BI平台，以及低代码/无代码开发平台（那时还没有这个名字）等。这些软件之所以会被称为平台，很重要的原因就是拥有很强的定制能力，比如流程定制、界面定制、业务逻辑定制，等等。而这些定制能力，依托的就是编译技术。
在前几年，我参与了一些与微服务有关的项目。我发现，前些年大家普遍关注那些技术问题，比如有状态的服务（Stateful Service）的横向扩展问题，在云原生、Serverless、FaaS等新技术满天飞的时代，不但没能被很好地解决，反而更恶化了。究其原因就是，状态管理还是被简单地交给数据库，而云计算的场景使得数据库的压力更大了，数据库原来在性能和扩展能力上的短板，就更加显著了。
而比较好的解决思路之一，就是大胆采用新的计算范式，发明新的计算机语言，所以我也有意想自己动手搞一下。
我从去年开始做设计，已经鼓捣了一阵了，采用了一些很前卫的理念，比如云原生的并发调度、基于Actor的数据管理等。总的目标，是要让开发云原生的、有状态的应用，像开发一个简单的单机应用一样容易。那我们就最好能把云架构和状态管理的细节给抽象掉，从而极大地降低成本、减少错误。而为编程提供更高的抽象层次，从来就是编译技术的职责。
Serverless和FaaS已经把无状态服务的架构细节透明掉了。但针对有状态的服务，目前还没有答案。对我而言，这是个有趣的课题。
在我比较熟悉的企业应用领域，ERP的鼻祖SAP、SaaS的鼻祖SalesForce，都用自己的语言开发应用，很可惜国内的企业软件厂商还没有做到这一点。而在云计算时代，设计这样一门语言绕不过去的一个问题，就是解决有状态服务的云化问题。我希望能为解决这个问题提供一个新工具。当然，这个工具必须是开源的。
正是因为给自己挖了这么大一个坑，也促使我更关心编译技术的各种前沿动态，也非常想把这些前沿的动态、理念，以及自己的一些实战经验都分享出来。
所以去年呢，我在极客时间上开了一门课程《编译原理之美》，帮你系统梳理了编译技术最核心的概念、理论和算法。不过在做第一季的过程中呢，我发现很多同学都跟我反馈：我确实理解了编译技术的相关原理、概念、算法等，但是有没有更直接的方式，能让我更加深入地把知识与实践相结合呢？
为什么要解析真实编译器？说到把编译技术的知识与实践相结合，无外乎就是解决以下问题：
我已经知道，语法分析有自顶向下的方法和自底向上的方法，但要自己动手实现的话，到底该选择哪个方法呢？是应该自己手写，还是用工具生成呢？ 我已经知道，在语义分析的过程中要做引用消解、类型检查，并且会用到符号表。那具体到自己熟悉的语言，这些工作是如何完成的呢？有什么难点和实现技巧呢？符号表又被设计成什么样子呢？ 我已经知道，编译器中会使用IR，但实际使用中的IR到底是什么样子的呢？使用什么数据结构呢？完成不同的处理任务，是否需要不同的IR呢？ 我已经知道，编译器要做很多优化工作，但针对自己熟悉的语言，这些优化是如何发生的？哪些优化最重要？又要如何写出便于编译器优化的代码呢？ 类似的问题还有很多，但总结起来其实就是：真实世界的编译器，到底是怎么写出来的？
那弄明白了这个问题，到底对我们有什么帮助呢？
第一，研究这些语言的编译机制，能直接提高我们的技术水平。
一方面，深入了解自己使用的语言的编译器，会有助于你吃透这门语言的核心特性，更好地运用它，从而让自己向着专家级别的工程师进军。举个例子，国内某互联网公司的员工，就曾经向Oracle公司提交了HotSpot的高质量补丁，因为他们在工作中发现了JVM编译器的一些不足。那么，你是不是也有可能把一门语言吃得这么透呢？
另一方面，IT技术的进化速度是很快的，作为技术人，我们需要迅速跟上技术更迭的速度。而这些现代语言的编译器，往往就是整合了最前沿的技术。比如，Java的JIT编译器和JavaScript的V8编译器，它们都不约而同地采用了“Sea of Nodes”的IR来做优化，这是为什么呢？这种IR有什么优势呢？这些问题我们都需要迅速弄清楚。
第二，阅读语言编译器的源码，是高效学习编译原理的重要路径。
传统上，我们学习编译原理，总是要先学一大堆的理论和算法，理解起来非常困难，让人望而生畏。
这个方法本身没有错，因为我们学习任何知识，都要掌握其中的原理。不过，这样可能离实现一款实用的编译器还有相当的距离。
那么根据我的经验，学习编译原理的一个有效途径，就是阅读真实世界中编译器的源代码，跟踪它的执行过程，弄懂它的运行机制。因为只要你会写程序，就能读懂代码。既然能读懂代码，那为什么不直接去阅读编译器的源代码呢？在开源的时代，源代码就是一个巨大的知识宝库。面对这个宝库，我们为什么不进去尽情搜刮呢？想带走多少就带走多少，没人拦着。
当然，你可能会犯嘀咕：编译器的代码一般都比较难吧？以我的水平，能看懂吗？
是会有这个问题。当我们面对一大堆代码的时候，很容易迷路，抓不住其中的重点和核心逻辑。不过没关系，有我呢。在本课程中，我会给你带路，并把地图准备好，带你走完这次探险之旅。而当你确实把握了编译器的脉络以后，你对自己的技术自信心会提升一大截。这些计算机语言，就被你摘掉了神秘的面纱。
俗话说“读万卷书，行万里路”。如果说了解编译原理的基础理论和算法是读书的过程，那么探索真实世界里的编译器是什么样子，就是行路的过程了。根据我的体会，当你真正了解了身边的语言的编译器是怎样编写的之后，那些抽象的理论就会变得生动和具体，你也就会在编译技术领域里往前跨出一大步了。
我们可以解析哪些语言的编译器？那你可能要问了，在本课程中，我都选择了哪些语言的编译器呢？选择这些编译器的原因又是什么呢？
这次，我要带你解析的编译器还真不少，包括了Java编译器（javac）、Java的JIT编译器（Graal）、Python编译器（CPython）、JavaScript编译器（V8）、Julia语言的编译器、Go语言的编译器（gc），以及MySQL的编译器，并且在讲并行的时候，还涉及了Erlang的编译器。
我选择剖析这些语言的编译器，有三方面的原因：
第一，它们足够有代表性，是你在平时很可能会用到的。这些语言中，除了Julia比较小众外，都比较流行。而且，虽然Julia没那么有名，但它使用的LLVM工具很重要。因为LLVM为Swift、Rust、C++、C等多种语言提供了优化和后端的支持，所以Julia也不缺乏代表性。 第二，它们采用了各种不同的编译技术。这些编译器，有的是编译静态类型的语言，有的是动态类型的语言；有的是即时编译（JIT），有的是提前编译（AOT）；有高级语言，也有DSL（SQL）；解释执行的话，有的是用栈机（Stack Machine），有的是用寄存器机，等等。不同的语言特性，就导致了编译器采用的技术会存在各种差异，从而更加有利于你开阔视野。 第三，通过研究多种编译器，你可以多次迭代对编译器的认知过程，并通过分析对比，发现这些编译器之间的异同点，探究其中的原因，激发出更多的思考，从而得到更全面的、更深入的认知。 看到这里，你可能会有所疑虑：有些语言我没用过，不怎么了解，怎么办？其实没关系。因为现代的高级语言，其实相似度很高。
一方面，对于不熟悉的语言，虽然你不能熟练地用它们来做项目，但是写一些基本的、试验性的程序，研究它的实现机制，是没有什么问题的。
另一方面，学习编译原理的人会练就一项基本功，那就是更容易掌握一门语言的本质。特别是我这一季的课程，就是要帮你成为钻到了铁扇公主肚子里的孙悟空。研究某一种语言的编译器，当然有助于你通过“捷径”去深入地理解它。
我是如何规划课程模块的？这门课程的目标，是要让你对现代语言的编译器的结构、所采用的算法以及设计上的权衡，都获得比较真切的认识。其最终结果是，如果要你使用编译技术来完成一个项目，你会心里非常有数，知道应该在什么地方使用什么技术。因为你不仅懂得原理，更有很多实际编译器的设计和实现的思路作为你的决策依据。
为了达到本课程的目标，我仔细规划了课程的内容，将其划分为预备知识篇、真实编译器解析篇和现代语言设计篇三部分。
在预备知识篇，我会简明扼要地帮你重温一下编译原理的知识体系，让你对这些关键概念的理解变得更清晰。磨刀不误砍柴工，你学完预备知识篇后，再去看各种语言编译器的源代码和相关文档时，至少不会被各种名词、术语搞晕，也能更好地建立具体实现跟原理之间的关联，能互相印证它们。
在真实编译器解析篇，我会带你研究语言编译器的源代码，跟踪它们的运行过程，分析编译过程的每一步是如何实现的，并对有特点的编译技术点加以分析和点评。这样，我们在研究了Java、Java JIT、Python、JavaScript、Julia、Go、MySQL这7个编译器以后，就相当于把编译原理印证了7遍。
在现代语言设计篇，我会带你分析和总结前面已经研究过的编译器，进一步提升你对相关编译技术的认知高度。学完这一模块以后，你对于如何设计编译器的前端、中端、后端、运行时，都会有比较全面的了解，知道如何在不同的技术路线之间做取舍。
好了，以上就是这一季课程的模块划分思路了。你会发现，这次的课程设计，除了以研究真实编译器为主要手段外，会更加致力于扩大你的知识版图、增加你的见识，达到“行万里路”的目的。
可以说，我在设计和组织这一季课程时，花了大量的时间准备。因此这一季课程的内容，不说是独一无二的，也差不多了。你在市面上很少能找到解析实际编译器的书籍和资料，这里面的很多内容，都是在我自己阅读源代码、跟踪源代码执行过程的基础上梳理出来的。
写在最后近些年，编译技术在全球范围内的进步速度很快。比如，你在学习Graal编译器的时候，你可以先去看看，市面上有多少篇围绕它的高质量论文。所以呢，作为老师，我觉得我有责任引导你去看到、理解并抓住这些技术前沿。
我也有一个感觉，在未来10年左右，中国在编译技术领域，也会逐步有拿得出手的作品出来，甚至会有我们独特的创新之处，就像我们当前在互联网、5G等领域中做到的一样。
虽然这个课程不可能涵盖编译技术领域所有的创新点，但我相信，你在其中投入的时间和精力是值得的。你通过我课程中教给你的方法，可以对你所使用的语言产生更加深入的认知，对编译器的内部结构和原理有清晰理解。最重要的是，对于如何采用编译技术来解决实际问题，你也会有能力做出正确的决策。
这样，这个课程就能起到抛砖引玉的作用，让我们能够成为大胆探索、勇于创新的群体的一份子。未来中国在编译技术的进步，就很可能有来自我们的贡献。我们一起加油！
最后，我还想正式认识一下你。你可以在留言区里做个自我介绍，和我聊聊，你目前学习编译原理的最大难点在哪？或者，你也可以聊聊你对编译原理都有哪些独特的思考和体验，欢迎在留言区和我交流讨论。
好了，让我们正式开始编译之旅吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>期中考试_这些编译原理知识，你都掌握了吗？</title><link>https://artisanbox.github.io/7/51/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/51/</guid><description>你好，我是宫文学。
到这里，我们的课程就已经更新一半了，今天，我们来进行一场期中考试。我出了一套测试题，共有5道判断题、5道多选题，满分100，核心考点都出自前面的“预备知识”模块和“真实编译器解析”模块。
我建议你来认真地做一下这套题目，检验一下自己的学习效果。答完题之后，你也可以回顾试卷的内容，对不太理解或答错的问题，进行深入思考和学习。如果有不明白的，欢迎随时在留言区提问，我会知无不言。还等什么？一起来做下这套题吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>期末答疑与总结_再次审视学习编译原理的作用</title><link>https://artisanbox.github.io/7/49/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/49/</guid><description>你好，我是宫文学。到这里，咱们这门课程的主要内容就要结束了。有的同学在学习课程的过程中呢，提出了他感兴趣的一些话题，而我自己也会有一些想讲的话题，这个我也会在后面，以加餐等方式再做一些补充。接下来，我还会给你出一套期末测试题，帮你检测自己在整个学习过程中的所学所得。
那么，在今天这一讲，我们就来做个期末答疑与总结。在这里，我挑选了同学们提出的几个有代表性的问题，给你解答一下，帮助你更好地了解和掌握本课程的知识内容。
问题1：学习了编译原理，对于我学习算法有什么帮助？ @无缘消受人间富贵：老师，想通过编译器学算法，单独学算法总是不知道有什么意义，每次都放弃，老师有什么建议吗？但是看到评论说用到的都是简单的数据结构，编译器用不到复杂的数据结构和算法？
针对这位同学提出的问题，我想谈一谈我对算法学习的感受。
前一阵，我在跟同事聊天时，提到了一个观点。我说，大部分的程序员，其实从来都没写过一个像样的算法。他们写的程序，都是把业务逻辑简单地翻译成代码。那么，如果一个公司写出来的软件全是这样的代码，就没有什么技术壁垒了，很容易被复制。
反之，一些优秀的软件，往往都是有几个核心的算法的。比如，对于项目管理软件，那么网络优化算法就很关键；对于字处理软件，那么字体渲染算法就很关键，当年方正的激光照排系统，就是以此为基础的；对于电子表格软件，公式功能和自动计算的算法很关键；对于视频会议系统，也必须掌握与音视频有关的核心算法。这样，因为有了算法的技术壁垒，很多软件就算是摆在你的面前，你也很难克隆它。
所以说，作为一名软件工程师，你就必须要有一定的算法素养，这样才能去挑战那些有难度的软件功能。而作为一个软件公司，其实要看看自己在算法上有多少积淀，这样才能构筑自己的技术壁垒。
那么，编译原理对于提升你的算法素养，能带来什么帮助呢？我给你梳理一下。
编译原理之所以硬核，也是因为它涉及了很多的算法。
在编译器前端，主要涉及到的算法有3个：
有限自动机构造算法：这是在讲词法分析时提到的。这个算法可以根据正则文法，自动生成有限自动机。它是正则表达式工具的基础，也是像grep等强大的Linux命令能够对字符串进行模式识别的核心技术。 LL算法：这是在讲自顶向下的语法分析时涉及的。根据上下文无关文法，LL算法能够自动生成自顶向下的语法分析器，中间还涉及对First和Follow集合的计算。 LR算法：这是在讲自底向上的语法分析时涉及的。根据上下文无关文法，LR算法能自动生成自底向上的语法分析器，中间还涉及到有限自动机的构造算法。 总的来说，编译器前端的算法都是判断某个文本是否符合某个文法规则，它对于各种文本处理工作都很有效。有些同学也在留言区里分享，他在做全文检索系统时就会用到上述算法，使得搜索引擎能更容易地理解用户的搜索请求。
在编译器后端，主要涉及到的算法也有3个：
指令选择算法； 寄存器分配算法； 指令重排序（指令调度）算法。 这三个算法也有共同点，它们都是寻找较优解或最优解，而且它们都是NP Complete（NP完全）的。简单地说，就是这类问题能够很容易验证一个解对不对（多项式时间内），但求解过程的效率却可能很低。对这类问题会采用各种方法求解。在讲解指令选择算法时，我介绍了贪婪策略和动态规划这两种不同的求解思路；而寄存器选择算法的图染色算法，则采用了一种启发式算法，这些都是求解NP完全问题的具体实践。
在日常工作中，我们其实也会有很多需要求较优解或最优解的需求。比如，在文本编辑软件中，需要把一个段落的文字分成多行。而如何分行，就需要用到一个这样的算法。再比如，当做一个报表软件，并且需要分页打印的时候，如何分页也是同类型的问题。
其他类似的需求还有很多。如果你没有求较优解或最优解的算法思路，对这样的问题就会束手无策。
而在编译器的中端部分，涉及的算法数量就更多了，但是由于这些算法都是对IR的各种分析和变换，所以IR采用不同的数据结构的时候，算法的实现也会不同。它不像前端和后端算法那样，在不同的编译器里都具有很高的一致性。
不过，IR基本上就是三种数据结构：树结构、图结构和基于CFG的指令列表。所以，这些算法会训练你处理树和图的能力，比如你可以在树和图中发现一些模式，以此对树和图进行变换，等等。这在你日常的很多编程工作中也是非常重要的，因为这两种数据结构是编程中最常使用的数据结构。
那么，总结起来，认真学好编译原理，一定会给你的算法素养带来不小的提升。
问题2：现代编程语言这么多，我们真的需要一门新语言吗？ @蓝士钦：前不久看到所谓的国产编程语言“木兰”被扒皮后，发现是Python套层壳，真的是很气愤。想要掌握编译原理设计一门自己的语言，但同时又有点迷茫，现代编程语言这么多，真的需要再多一门新语言吗？从人机交互的角度来看，任何语言都是语法糖。
关于是否需要一门新语言的话题，我也想跟你聊聊我自己的看法，主要有三个方面。当然，你也可以在此过程中思考一下，看看有没有什么跟我不同的见解，欢迎与我交流讨论。
第一，编程语言其实比我们日常看到的要多，很多的细分领域都需要自己的语言。
我们平常了解的都是一些广泛流行的通用编程语言，而进入到每个细分领域，其实都需要各自领域的语言。比如SaaS的鼻祖Salesforce，就设计了自己的Apex语言，用于开发商业应用。华为的实验室在研发方舟编译器之前，也曾经研发了一门语言Cm，服务于DSP芯片的研发。
第二，中国技术生态的健康发展，都需要有自己的语言。
每当出现一个新的技术生态的时候，总是有一门语言会成为这个技术生态的“脚本”，服务于这个技术生态。比如，C语言就是Unix系统的脚本语言；JavaScript、Java、PHP等等，本质上都是Web的脚本语言；而Objective-C和Swift显然是苹果设备的脚本语言；Android虽然一开始用了Java，但最近也在转成Kotlin，这样Google更容易掌控。
那么，从这个角度看，当中国逐步发展起自己的技术生态的时候，也一定会孕育出自己的语言。以移动计算生态而言，我们有全球最大的移动互联网用户群和最丰富的应用，手机的制造量也是全球最高的。而位于应用和硬件之间的应用开发平台，我们却没有话语权，这会使中国的移动互联网技术生态受到很大的掣肘。
我在第40讲，也已经分析过了，Android系统经过了很多年的演化，但技术上仍然有明显的短板，使得Android平台的使用体验始终赶不上苹果系统。为了弥补这些短板，各个互联网公司都付出了很大的成本，比如一些头部应用的核心功能采用了C/C++开发。
并且，Android系统的编译器，在支持新的硬件上也颇为保守和封闭，让中国厂商难以参与。这也是华为之所以要做方舟编译器的另一个原因。因为华为现在自研的芯片越来越多，要想充分发挥这些芯片的能力，就必须要对编译器有更大的话语权。方舟编译器的问世，也证明了我们其实是有技术能力的，可以比国外的厂商做得更好。既然如此，我们为什么要受别人的制约？华为方舟编译器团队其实也很渴望，在方舟编译器之后推出自己的语言。至于华为内部是否已经立项，这就不太清楚了，但我觉得这是顺理成章的事情。
另外，除了在移动端的开发上会受到很多掣肘，在云端其实也一样。比如说，Java是被大量后端开发的工程师们所掌握的语言，但现在Java是被Oracle掌控的。你现在使用Java的时候，可能已经多多少少感受到了一种不愉快。先不说Java8之后的收费政策，就说我们渴望的特性（如协程、泛型中支持基础数据类型等），一直没有被满足，就会感觉不爽。
我在讲到协程的时候，就指出Java语言目前支持协程其实是很别扭的一种状态，它都是一些第三方的实现，并没有官方的支持。而如果Java的技术生态是由我们主导，可能就不是这样了。因为我国互联网的并发用户数如此之多，我们对更好的并发特性其实是更关切的。到目前为止，像微信团队解决高并发的问题，是用C++加上自己开发的协程库才实现的。而对于很多没有如此强大的技术能力的公司来说，就只能凑合了。
第三，实现一款优秀的软件，一定会用到编译技术。
每一款软件，当发展到极致的时候，都会变得像一个开发平台。这也是《黑客与画家》的作者保罗·格雷厄姆（Paul Graham）表达的思维。他原来的意思是，每个软件写到最后，都会包含一个Lisp的变种。实际上，他所要表达的意思就跟我说的一样。
我前一段时间，在北京跟某公司的老总探讨一个优秀的行业应用软件。这个软件在上世纪90年代就被开发出来了，也被我国广泛采用。一方面它是一个应用软件，另一方面它本身也是一个开发平台。所以它可以经过定制，满足不同行业的需求。
但是，我们国内的软件行业的情况是，在去客户那里实施的时候，几乎总是要修改源代码，否则就不能满足用户的个性化需求。
很多软件公司想去克隆一下我刚才说的那套软件，结果都放弃了。除了有对领域模型理解的困难以外，缺少把一个应用软件做成软件开发平台的能力，是其中很大的一个障碍。
实际上，目前在很多领域都是这样。国外的软件就是摆在那里，但中国的工程师就是做不出自己的来。而对于编译技术的掌握和运用，就是能够提升国内软件水平的重要途径。
我在开头跟同事交流的时候，也提出了软件工程师技术水平修养提升的几个境界。其中一个境界，就是要能够利用编译技术，做出在更大范围内具有通用性的软件。如果你能达到这个境界，那么也一定有更大的发展空间。
问题3：如何判断某门语言是否适合利用LLVM作为后端？ @ヾ(◍°∇°◍)ﾉﾞ：老师，很多语言都声称使用LLVM提升性能，但是在Lua领域好像一直是LuaJIT无法超越？
这个问题涉及到了如何利用后端工具的问题，比较有代表性。
LLVM是一个通用的后端工具。在它诞生之初，首先是用于支持C/C++语言的。所以一门语言，在运行机制上越接近C/C++语言，用LLVM来做后端就越合适。
比如Rust用LLVM就很成功，因为Rust语言跟C/C++一样，它们的目标都是编写系统级的程序，支持各种丰富的基础数据类型，并且也都不需要有垃圾收集机制。
那么，如果换成Python呢？你应该记得，Python不会对基础数据类型进行细粒度的控制，不需要把整型区分成8位、16位、32位和64位的，它的整型计算可以支持任意长度。这种语义就跟C/C++的相差比较远，所以采用LLVM的收益相对就会小一些。
而对于JavaScript语言来说，浏览器的应用场景要求了编译速度要尽量地快，但在这方面LLVM并没有优势。像我们讲过的隐藏类（Shapes）和内联缓存（Inline Caching）这样的对JavaScript很重要的机制，LLVM也帮不上忙。所以，如果在项目时间比较紧张的情况下，你可以暂时拿LLVM顶一顶，Safari浏览器中的JavaScript引擎之前就这么干过。但是，要想达到最好的效果，你还是编写自己的后端更好一些。
那对于Lua语言，其实你也可以用这个思路来分析一下，是采用LLVM，还是自己写后端会更好一些。不过，由于Lua语言比较简单，所以实现后端的工作量应该也相对较小。
小结这一讲，我主要回答了几个比较宏观的问题，它们都涉及到了编译原理这门课的作用。
第一个问题，我是从提升算法素养的角度来展开介绍的。编译原理知识里面涉及了大量的算法，我把它总结成了三大类，每类都有自己的特点，希望能对你宏观把握它们有所帮助。
第二个问题，其实是这门课程的一条暗线。我并没有在课程里去情绪化地鼓吹，一定要有自己的编译器、自己的语言。我的方式其实是想做一点具体的事情，所以在第二个模块中，我带着你一起探究了现有语言的编译器都是怎么实现的，破除你对编译器的神秘感、距离感；在第三个模块，我们又一起探讨了一下实现一门语言中的那些关键技术点，比如垃圾收集、并行等，它们都是如何实现的。
在课程最后呢，我又带你了解了一下具有中国血统的方舟编译器。我想说的是，其实我们不但能做出编译器和语言来，而且可能会做得更好。虽然我们对方舟编译器的分析还没有做完，但通过分析它的技术思路，你应该或多或少地感受到了它的优秀。所以，针对“我们真的需要一门新语言吗”这个问题，我的回答是确定的。并且，即使你不去参与实现一门通用的语言，在实现自己领域的语言，以及把自己的软件做得更具通用性这点上，编译原理仍然能发挥巨大的作用，对你的职业生涯也会有切实的帮助。
好，请你继续给我留言吧，我们一起交流讨论。同时我也希望你能多多地分享，做一个知识的传播者。感谢你的阅读，我们下一讲再见。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>期末考试_“编译原理实战课”100分试卷等你来挑战！</title><link>https://artisanbox.github.io/7/50/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/50/</guid><description>你好，我是宫文学。
咱们课程到这里就算正式更新完了，在临近告别前，我还给你准备了一份期末考试题，这套试卷共有8道单选题和12道多选题，满分100，核心考点都出自前面讲到的所有重要知识，希望可以帮助你进行一场自测。
除此之外，我也很想知道你对这门课的建议，所以我也给你准备了一份问卷。欢迎你在问卷里聊一聊你的想法，也许就有机会获得礼物或者是课程阅码哦。
好了，话不多说，请你来做一做这套期末测试题吧！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } ._2sjJGcOH_0 { list-style-position: inside; width: 100%; display: -webkit-box; display: -ms-flexbox; display: flex; -webkit-box-orient: horizontal; -webkit-box-direction: normal; -ms-flex-direction: row; flex-direction: row; margin-top: 26px; border-bottom: 1px solid rgba(233,233,233,0.6); } ._2sjJGcOH_0 ._3FLYR4bF_0 { width: 34px; height: 34px; -ms-flex-negative: 0; flex-shrink: 0; border-radius: 50%; } .</description></item><item><title>热点问题答疑_如何吃透7种真实的编译器？</title><link>https://artisanbox.github.io/7/52/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/52/</guid><description>你好，我是宫文学。
到这里，咱们就已经解析完7个编译器了。在这个过程中，你可能也积累了不少问题。所以今天这一讲，我就把其中有代表性的问题，给你具体分析一下。这样，能帮助你更好地掌握本课程的学习思路。
问题1：如何真正吃透课程中讲到的7种编译器？在课程中，我们是从解析实际编译器入手的。而每一个真实的编译器里面都包含了大量的实战技术和知识点，所以你在学习的时候，很容易在某个点被卡住。那第一个问题，我想先给你解答一下，“真实编译器解析篇”这个模块的学习方法。
我们知道，学习知识最好能找到一个比较缓的坡，让自己可以慢慢爬上去，而不是一下子面对一面高墙。那么对于研究真实编译器，这个缓坡是什么呢？
我的建议是，你可以把掌握一个具体的编译器的目标，分解成四个级别的任务，逐步提高难度，直到最后吃透。
第一个级别，就是听一听文稿，看一看文稿中给出的示例程序和源代码的链接就可以了。
这个级别最重要的目标是什么？是掌握我给你梳理出来的这个编译器的技术主线，掌握一张地图，这样你就能有一个宏观且直观的把握，并且能增强你对编译原理的核心基础知识点的认知，就可以了。
小提示：关于编译器的技术主线和知识地图，你可以期待一下在期中复习周中，即将发布的“划重点：7种编译器的核心概念和算法”内容。
在这个基础上，如果你还想再进一步，那么就可以挑战第二级的任务。
第二个级别，是要动手做实验。
你可以运行一下我给出的那些使用编译器的命令，打印输出调试信息，或者使用一下课程中提到的图形化工具。
比如，在Graal和V8编译器，你可以通过修改命令行的参数，观察生成的IR是什么样子。这样你就可以了解到，什么情况下才会触发即时编译、什么时候才会触发内联优化、生成的汇编代码是什么样子的，等等。
这样，通过动手做练习，你对这些编译器的认识就会更加具体，并且会有一种自己可以驾驭的感觉，赢得信心。
第三个级别，是看源代码，并跟踪源代码的运行过程，从而进入到编译器的内部，去解析一个编译器的真相。
完成这一级的任务，对你动手能力的要求更高。你最容易遇到的问题，是搭建一个调试环境。比如，调试Graal编译器要采用远程调试的模式，跟你调试一个普通应用还是不大一样的。而采用GDB、LLDB这样的工具，对很多同学来说可能也是一个挑战。
而且，你在编译源代码和调试的过程中也会遇到很多与配置有关的问题。比如，我用GDB来调试Julia和MySQL的时候，就发现最好是使用一个Linux虚拟机，因为macOS对GDB的支持不够好。
不过，上述困难都不是说真的有多难，而是需要你的耐心。遇到问题就解决问题，最终搭建出一个你能驾驭的环境，这个过程也会大大提升你的动手实践能力。
环境搭建好了，在跟踪程序执行的过程中，一样要需要耐心。你可能要跟踪执行很多步，才能梳理出程序的执行脉络和实现思路。我在课程中建议的那些断点的位置和梳理出来程序的入口，可以给你提供一些帮助。
可以说，只要你能做好第三级的工作，终归是能吃透编译器的运行机制的。这个时候，你其实已经差不多进入了高手的行列。比如，在实际编程工作中，当遇到一个特别棘手的问题的时候，你可以跟踪到编译器、虚拟机的内部实现机制上去定位和解决问题。
而我前面说了，掌握一个具体的编译器的目标，是有四个级别的任务。那你可能要问，都能剖析源代码了，还要进一步挑战什么呢？
这第四个级别呢，就是把代码跟编译原理和算法结合起来，实现认识的升华。
在第三级，当你阅读和跟踪程序执行的时候，会遇到一个认知上的挑战。对于某些程序，你每行代码都能看懂，但为什么这么写，你其实不明白。
像编译器这样的软件，在解决每一个关键问题的时候，肯定都是有理论和算法支撑的。这跟我们平常写一些应用程序不大一样，这些应用程序很少会涉及到比较深入的原理和算法。
我举个例子，在讲Java编译器中的语法分析器的时候，我提到几点。第一，它是用递归下降算法的；第二，它在避免左递归时，采用了经典的文法改写的方法；第三，在处理二元表达式时，采用了运算符优先级算法，它是一种简单的LR算法。
我提到的这三点中的每一点，都是一个编译原理的知识点或算法。如果对这些理论没有具体的了解，那你看代码的时候就看不出门道来。类似的例子还有很多。
所以，如果你其实在编译原理的基础理论和算法上都有不错的素养的话，你会直接带着自己的假设去代码里进行印证，这样你就会发现每段程序，其实都是有一个算法去对应的，这样你就真的做到融会贯通了。
那如何才能达到第四级的境界，如何才能理论和实践兼修且互相联系呢？
第一，你要掌握“预备知识”模块中的编译原理核心基础知识和算法。 第二，你要阅读相关的论文和设计文档。有一些论文是一些经典的、奠基性的论文。比如，在讲Sea of Nodes类型的IR的时候，我介绍了三篇重要的论文，需要你去看。还有一些论文或设计文档是针对某个编译器的具体的技术点的，这些论文对于你掌握该编译器的设计思路也很有帮助。 达到第四级的境界，你其实已经可以参与编译器的开发，并能成为该领域的技术专家了。针对某个具体的技术点加以研究和钻研，你也可以写出很有见地的论文。
当然，我不会要求每个同学都成为一个编译器的专家，因为这真的要投入大量的精力和实践。你可以根据自己的技术领域和发展规划，设定自己的目标。
我的建议是：
首先，每个同学肯定要完成第一级的目标。这一级目标的要求是能理解主线，有时候要多读几遍才行。 对于第二级目标，我建议你针对2~3门你感兴趣的语言，上手做一做实验。 对于第三级目标，我希望你能够针对1门语言，去做一下深入探索，找一找跟踪调试一个编译器、甚至修改编译器的源代码的感觉。 对于第四级目标，我希望你能够针对那些常见的编译原理算法，比如前端的词法分析、语法分析，能够在编译器里找到并理解它们的实现。至于那些更加深入的算法，可以作为延伸任务。 总的来说呢，“真实编译器”这个模块的课程内容，为你的学习提供了开放式的各种可能性。
好，接下来，我就针对同学们的提问和课程的思考题，来做一下解析。
问题2：多重分派是泛型实现吗？ @d：“多重分派能够根据方法参数的类型，确定其分派到哪个实现。它的优点是容易让同一个操作，扩展到支持不同的数据类型。”宫老师，多重分派是泛型实现吗？
由于大多数同学目前使用的语言，采用的都是面向对象的编程范式，所以会比较熟悉像这样的一种函数或方法派发的方式：
Mammal mammal = new Cow(); //Cow是Mammal的一个子类 mammal.speak(); 这是调用了mammal的一个方法：speak。那这个speak方法具体指的是哪个实现呢？根据面向对象的继承规则，这个方法可以是在Cow上定义的。如果Cow本身没有定义，就去它的父类中去逐级查找。所以，speak()具体采用哪个实现，是完全由mammal对象的类型来确定的。这就是单一分派。
我们认为，mammal对象实际上是speak方法的第一个参数，虽然在语法上，它并没有出现在参数列表中。而Java的运行时机制，也确实是这么实现的。你可以通过查看编译生成的字节码或汇编代码来验证一下。你如果在方法中使用“this”对象，那么实际上访问的是方法的0号参数来获取对象的引用或地址。
在采用单一分派的情况下，对于二元（或者更多元）的运算的实现是比较别扭的，比如下面的整型和浮点型相加的方法，你需要在整型和浮点型的对象中，分别去实现整型加浮点型，以及浮点型加整型的计算：
Integer a = 2; Float b = 3.1; a.add(b); //采用整型对象的add方法。 b.add(a); //采用浮点型对象的add方法。 但如果再增加新的类型怎么办呢？那么所有原有的类都要去修改，以便支持新的加法运算吗？
多重分派的情况，就不是仅仅由第一个参数来确定函数的实现了，而是会依赖多个参数的组合。这就能很优雅地解决上述问题。在增加新的数据类型的时候，你只需要增加新的函数即可。</description></item><item><title>用户故事_易昊：程序员不止有Bug和加班，还有诗和远方</title><link>https://artisanbox.github.io/7/54/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/54/</guid><description>你好，我是编辑王惠。在处理这门课的留言时，我注意到易昊同学一直在跟随着宫老师的脚步，学习和实践编译原理的相关知识，留言的内容十分有见地、提出的问题也能看出是经过了他深入的思考。同时，咱们这门课也具有很强的互动性，所以我邀请他来和我们分享一下他的心得体会。
Hi，我是易昊，目前在武汉做Android开发，已经工作12年了。很高兴能在这里跟你分享，关于我学习编译原理的一些心得体会。
为什么我要再学编译原理？首先，我想给你解释一下，我为什么会起“程序员不止有Bug和加班，还有诗和远方”这样一个标题呢？
这是因为，作为一名应用开发者，我经常会觉得，自己只是在和源源不断的Bug以及项目进度作斗争，日常工作好像已经无法给我带来技术上的成就感了。但我能肯定的是，我对于技术的情怀并没有消失。我也认为，我不应该只满足于完成日常的普通开发任务，而是应该去做点更有挑战性的事情，来满足自己精神上的追求。
那么，我为什么会选择学习编译技术呢？
首要的原因，是这门课的内容不像编程语言、数据结构那样清晰直观。加上在大学时期，学校安排的课时较短，只有半个学期，自己又没有对它足够重视起来，导致这门课只学了个一知半解，从而造成自己对计算机底层工作原理没有掌握透彻，留下了遗憾。
《程序员的自我修养–链接、装载与库》里，有句话让我印象深刻：“真正了不起的程序员对自己程序的每一个字节都了如指掌。”虽然说得有点夸张，但一个优秀的程序员确实应该理解自己的程序为什么能在计算机上运行起来。而不是在写完代码后，忐忑不安地看着IDE在进行编译，等终于能运行起来时，大喊一声：“哇，能编译运行了，好神奇”。所以工作了几年之后，我一直想找机会能够弥补一下大学时期的遗憾，把编译知识学扎实。
另外，编译技术的巨大挑战性，也是我想重拾学习的重要原因之一。
你可能听过一个段子：程序员的三大浪漫，是自己实现编译器、操作系统和渲染引擎。
其实一开始，我并不理解为什么编译器会在这其中，我猜大概是因为特别困难吧。
我刚接触编程的时候，觉得能学好C、Java这样的编程语言，已经很不容易了，更何况要自己去用编译器实现一门语言。
而且编译原理这门课中，还有相当多比较深奥、晦涩的理论，想要掌握起来非常困难，它完全不像学习普通技术那样，几行代码就能运行一个hello world。同时你光有理论又完全不够，编译器中包含了很多巧妙的工程化处理。比如，会用抽象语法树来表示整个代码的语法结构；在函数调用时，参数和返回值在栈帧中的内存地址顺序等。
所以，不得不说，编译器是一个非常复杂的工程，编译原理是一个非常不容易消化、掌握的基础技术，让人望而生畏。
但是一旦掌握了之后，可以说就打通了计算机技术的任督二脉，因为你已经掌握了计算机运行中相当底层部分的原理，那么再去看其他技术就不会有什么大的障碍了。
而且在工作中，即使没有什么机会需要自己去创造语言或者写编译器，编译原理对我也很有帮助。举个例子吧，Android有很多动态化的技术，像ReactNative、Weex都会使用JavaScript作为脚本语言来调用原生接口，那么为了实现原生语言（如Java、Objective-C）和JavaScript的通信，ReactNative和Weex都在框架中嵌入了JavaScriptCore这个组件，因为JavaSciptCore包含了一个JavaScript的解释器，可以在运行时执行JavaScript。
那么，如果你也想在自己的项目中使用类似的动态化技术，但又不愿意被ReactNative和Weex技术绑死，就需要自己去剖析JavaScriptCore的工作原理，甚至要去实现一个类似的脚本语言执行框架。
而真的要自己去下载JavaScriptCore的源码来看，你就会发现，如果没有一定的编译原理知识作为基础，是很难看懂的。但是如果你具备了编译原理知识基础，其实会发现，这些源码里面也不外乎就是词法分析、语法分析、IR等，这些都是编译原理中的常用概念和算法。
还有就是日常工作中会碰到的某些比较棘手的问题，如果你不理解编译技术，可能就无法找到出现问题的根源。
比如我早期的工作中，在开发C++代码的时候，经常会遇到链接时找不到符号的问题，那个时候我对“符号是怎么产生的”并不理解，所以遇到这类问题，我就会在IDE或者代码里盲目地尝试修改。
后来重新学习了编译技术之后，我就理解了编译器在编译过程中会产生符号表，每个符号包含了符号的名称、类型、地址等信息。之所以出现这类问题，可能是依赖的静态库没有包含这些符号，或者是类型不正确，再或者是这些符号的地址无法被正确解析，所以我用相应的工具去检查一下静态库文件的符号表，一般就可以解决问题了。
你看，编译技术总能帮我解决一些，我之前挠破头都想不出解决方案的问题。到现在我工作了十几年以后，就会有一个越来越强烈的感悟：最重要的还是底层知识。以前我也曾经感慨过计算机技术发展之快，各种技术层出不穷，就拿Android技术来说，各种什么插件化、组件化、动态化技术让人眼花缭乱，要不就是今天谷歌在提倡用Kotlin，明天又开始推Flutter了。
所以有一段时间我比较迷茫，“我究竟该学什么呢”，但我后来发现，虽然那些新技术层出不穷，但万变不离其宗，计算机核心的部分还是编译原理、操作系统那些底层技术。如果你对底层技术真正吃透了，再来看这些时髦的新技术，就不会感到那么神秘了，甚至是可以马上就弄明白它背后的技术原理。原理都搞懂了，那掌握起来也就非常快了。
我是怎么学习专栏的？我开始意识到自己需要重新学习编译技术是在2015年，当时为了能够在Android的原生代码里运行JavaScript脚本，我依次尝试了WebView、JavaScriptCore、Rhino等方案，觉得这种多语言混合开发的技术挺强大，也许能够改变主流的应用开发模式，于是就想继续研究这些框架是怎么工作的。但是我发现，自己对编译原理知识掌握得不牢，导致学习这些技术的时候有点无从下手。
那个时候还没有极客时间这样针对性较强的学习平台，我是自己买了些书，有权威的龙书和虎书，也有《两周自制脚本语言》这样的速成书籍，但总是不得要领。
像龙书、虎书这样的，主要花了大量的篇幅在讲理论。但面对工作和家庭的压力，又不允许我有那么大片的时间去学习理论，而且没有实践来练习的话，也很容易忘掉。
像速成书籍这样的，虽然实现了一个很简单的编译器，但它大量的篇幅是在讲代码的一些细节，原理又讲得太少，知其然而不知其所以然。后来我就没有继续深入地学下去了。
直到偶然地在极客时间上看到了《编译原理之美》这门课，我简单看了下目录之后，就立马买了下来，因为感觉非常实用，既对理论部分的重点内容有深入浅出地讲解，也有与实际问题的紧密联系。学完这门课后，我感觉有信心去尝试写一点东西了，正好临近春节，就计划着在春节期间自己写些代码来验证学习的成果。结果不成想遇到了疫情封城，因为没法复工，就索性在家自己照着龙书和课程中给出的思路，看看能否自己实现个编译器出来。
结果我花了快两个月的时间，真的写出来了一个简单的编译器，能够把类似C语言风格的代码编译成汇编执行。
回想那段时间，虽然疫情很让人焦虑，但当我全身心地投入到对编译技术的钻研时，可以暂时忘记疫情带来的困扰。通过写这个小项目，我算是对编译器的工作过程有了个切身的体会，还把多年未碰的汇编又重新拾起来投入使用，可以说是收获颇丰，疫情带给我的回忆也没有那么痛苦了，从另一个角度看，甚至还有一定的成就感。
这个简单的编译器项目完成了之后，就激发了我更大的兴趣。因为我毕竟只是实现了一个玩具型的编译器，那么工业界的编译器是如何工作的呢？
这个编译器，我是用LL算法来实现的语法分析，但龙书上还大篇幅地讲了LR、SLR、LALR，那么实际中到底是使用LL算法还是LR算法呢？
还有，我的编译器没有什么优化的功能，真实的编译器都有哪些优化措施呢？
这些问题吸引着我要去寻找答案。结果正巧，宫老师又推出了《编译原理实战课》，深入浅出地讲解各大编译器的工作原理，这可正好对我的胃口，于是又毫不犹豫地买下了。
上了几节课之后，觉得收获很大，特别是讲解javac和Graal编译器的部分。这两部分都给出了如何基于源码去剖析编译器的原理，实际操作性很强，我跟着宫老师给出的步骤，下载和一步步地调试、跟踪源码，印象十分深刻。特别是宫老师介绍的javac在处理运算符优先级时，引入了LR算法，从而避免了教科书上介绍的：当使用LL方式时，为了消除左递归，要写多级Tail函数的问题，这些都让我对编译技术的实际应用理解得更加深刻了。在学习Graal时，我也是花了不少的时间去配置Windows环境，包括下载安装Kali Linux、OpenJDK等，才终于把Graal跑起来。通过这样的实际操作，体验到了“折腾”的乐趣，对动手能力也是一种锻炼。
除此之外，课程中还有丰富的流程图、类图和思维导图，因此我可以按图索骥，去研究自己感兴趣的知识点，不用再苦苦地从海量的源码中去大海捞针，学习效率得到了很大提升。
如何更好地学习编译原理？通过这段时间的学习后，我发现，编译原理其实并没有想象中的那么困难。我觉得计算机技术的一个特点就是像在搭积木，有时候只是知识点多、层次关系复杂而已。
编译技术尤其如此，从前端到后端，从词法分析到语法分析到语义分析、IR，最后到优化。这些地方包含着各种知识点，但这些知识也不是凭空变出来的，而是环环相扣、层层叠加出来的。因此你在学习编译技术时一定要静下心来，一点一点地去吃透里面的技术细节，并且还需要不断地总结和提炼，否则对知识的理解可能就不够透彻，浮于表面。
另外，你需要多去动手实践，特别是和算法相关的部分。比如，在编译器的语法分析阶段，有个内容是计算First集合和Follow集合，其实理解起来并不困难，但它包含的细节不少，容易算错，所以需要在课后多去练习。
我就是在课下，把宫老师布置的习题和龙书上相关的题目都算了一遍，还写了计算First集合和Follow集合的程序。不过就算是做到了这些，我感觉对这部分的理解还不够透彻，但是我对编译技术的恐惧感已经消除了，对后续进一步的深入挖掘也打下了基础。所以说，静下心来学，勤动手去练，对学习编译原理这门课程来说非常重要。
我还有一个体会就是，学习编译原理没有捷径可走。因为编译技术是一个复杂而精密的工程，它的知识是环环相扣的。比如说，如果你对词法分析和语法分析的知识掌握得不够牢固，不熟悉一些常用语法规则的推导过程，那后面的IR、三地址代码、CFG等，你就会学得一头雾水。
所以，我们只能够一步一个脚印地去学习。
当然了，如果你和我一样是一个有家有口的上班族，只利用碎片时间可能不好做到连续学习，那我建议你在学习课程的时候，可以稍微花点时间去参考一下宫老师介绍的开源编译器代码，比如JavaCompiler的源码、Graal的源码。这样就可以和课程中的内容结合起来。因为看代码和调试代码会更加直观，也更容易理解。
同时，你也可以看看别人的代码设计，有哪些地方是可以做成组件的，哪些函数是可以自己去实现来练手的。然后，你可以先给自己定一个小目标，就是利用业余时间去完成这些组件或者函数的开发，因为单个组件的工作量并不是太大，因此还是可以尝试完成的。这样在巩固理论知识的同时，还能锻炼自己的动手能力，我想热爱编程的你，应该可以从中得到不少快乐。
我相信，只要最终坚持下来，你和我，都可以掌握好编译这门“屠龙”技术。
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item><item><title>知识地图_一起来复习编译技术核心概念与算法</title><link>https://artisanbox.github.io/7/55/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/55/</guid><description>你好，我是学习委员朱英达。
在“预备知识篇”这个模块，宫老师系统地梳理了编译过程中各个阶段的核心要点，目的就是让我们建立一个编译原理的基础知识体系。那到今天为止，我们就学完了这部分内容，迈出了编译之旅中扎实的第一步。不知道你对这些知识掌握得怎样了？
为了复习，也为了检测我们的学习成果，我根据自己的知识积累和学习情况，整理了一张知识大地图，你可以根据这张地图中标记的七大编译阶段，随时速查常用的编译原理概念和关键算法。
如果你也总结了知识地图，那你可以对照着我这个，给自己一个反馈，看看它们之间有哪些异同点，我们可以在留言区中一起交流和讨论。
不过知识地图的形式，虽然便于你保存、携带、速查，但考虑到图中涉及的概念等内容较多，不方便查看和检索。所以，我还把地图上的知识点，用文字的形式帮你梳理出来了。你可以对照着它，来复习和回顾编译技术的核心概念和算法的知识点，构建自己的知识框架。
你在学习这些预备知识的过程中，可能会发现，宫老师并没有非常深入地讲解编译原理的具体概念、理论和算法。所以，如果你想继续深入学习这些基础知识，可以根据宫老师在每讲最后给出的参考资料，去学习龙书、虎书、鲸书等经典编译原理书籍。当然，你也可以去看看宫老师的第一季专栏课《编译原理之美》。
在我看来，相较于编译方面的教科书而言，《编译原理之美》这门课的优势在于，更加通俗易懂、与时俱进，既可以作为新手的起步指导，也能够帮助已经熟悉编译技术的工程师扩展视野，我很推荐你去学习这门课。所以，我邀请编辑添加了相应的知识点到《编译原理之美》的文章链接，如果你有深入学习的需要，你会很方便地找到它。
好了，一起开始复习吧！
一、词法分析：根据词法规则，把字符串转换为Token核心概念：正则文法 正则文法：词法分析工作的主要文法，它涉及的重要概念是正则表达式。 正则表达式：正则文法的一种直观描述，它是按照指定模式匹配字符串的一种字符组合。 正则表达式工具：字符串的模式匹配工具。大多数程序语言都内置了正则表达式的匹配方法，也可以借助一些编译工具，自动化根据正则表达式生成字符串匹配程序，例如C++的Lex/Yacc以及Java的ANTLR。 具体实现：手工构造词法分析器、自动生成词法分析器手工构造词法分析器
构造词法分析器使用的计算模型：有限自动机（FSA）。它是用于识别正则文法的一种程序实现方案。 其组成的词法单元是Token，也就是指程序中标记出来的单词和标点符号，它可以分成关键字、标识符、字面量、操作符号等多个种类。 在实际的编译器中，词法分析器一般都是手写的。 自动生成词法分析器
具体实现思路：把一个正则表达式翻译成NFA，然后把NFA转换成DFA。 DFA：确定的有限自动机。它的特点是：该状态机在任何一个状态，基于输入的字符，都能做一个确定的状态转换。 NFA：不确定的有限自动机。它的特点是：该状态机中存在某些状态，针对某些输入，不能做一个确定的转换。这里可以细分成两种情况：一种是对于一个输入，它有两个状态可以转换；另一种是存在ε转换的情况，也就是没有任何字符输入的情况下，NFA也可以从一个状态迁移到另一个状态。 技术难点首先，你需要注意，NFA和DFA都有各自的优缺点，以及不同的适用场景。
NFA：优点是在设计上更简单直观，缺点是它无法避免回溯问题，在某些极端的情况下可能会造成编译器运行的性能低下。主要适用于状态较为简单，且不存在回溯的场景。 DFA：优点是它可以避免回溯问题，运行性能较高，缺点是DFA通常不容易直接设计出来，需要通过一系列方案，基于NFA的转换而得到，并且需要占用额外的空间。主要适用于状态较为复杂，或者对时间复杂度要求较为严苛的工业级词法分析器。 其次，你需要了解基于正则表达式构造NFA，再去进行模式匹配的算法思路。
从正则表达式到NFA：这是自动生成词法分析器的一种算法思路。它的翻译方法是，匹配一个字符i —&amp;gt;匹配“或”模式s|t —&amp;gt; 匹配“与”模式st —&amp;gt; 重复模式，如“?”“*”和“+”等符号，它们的意思是可以重复0次、0到多次、1到多次，注意在转换时要增加额外的状态和边。 从NFA到DFA：NFA的运行可能导致大量的回溯，所以我们可以把NFA转换成DFA，让字符串的匹配过程更简单。从NFA转换成DFA的算法是子集构造法，具体的算法思路你可以参考第16讲。 二、语法分析：依据语法规则，编写语法分析程序，把 Token 串转化成 AST核心概念：上下文无关文法 上下文无关的意思：在任何情况下，文法的推导规则都是一样的。 语法规则由4个部分组成：一个有穷的非终结符（或变元）的集合、一个有穷的终结符的集合、一个有穷的产生式集合、一个起始非终结符（变元）。符合这四个特点的文法规则就是上下文无关文法。 两种描述形式：一种是巴科斯范式（BNF），另一种是巴科斯范式的一种扩展形式（EBNF），它更利于自动化生成语法分析器。其中，产生式、终结符、非终结符、开始符号是巴科斯范式的基本组成要素。 上下文无关文法与正则文法的区别：上下文无关文法允许递归调用，而正则文法不允许。上下文无关文法比正则文法的表达能力更强，正则文法是上下文无关文法的一个子集。 具体实现：自顶向下、自底向上一种是自顶向下的算法思路，它是指从根节点逐层往下分解，形成最后的AST。
递归下降算法：它的算法思路是按照语法规则去匹配Token串。优点：程序结构基本上是跟文法规则同构的。缺点：会造成左递归和回溯问题。注意，递归下降是深度优先（DFS）的，只有最左边的子树都生成完了，才会往右生成它的兄弟节点。 LL算法：对于一些比较复杂的语法规则来说，这个算法可以自动计算出选择不同产生式的依据。方法：从左到右地消化掉 Token。要点：计算 First 和 Follow 集合。 另一种是自底向上的算法思路，它是指从底下先拼凑出AST的一些局部拼图，并逐步组装成一棵完整的AST。
自底向上的语法分析思路：移进，把token加入工作区；规约，在工作区内组装AST的片段。 LR算法和 LL 算法一样，也是从左到右地消化掉 Token。 技术难点首先，你需要掌握LL算法的要点，也就是计算First和Follow集合。
其次，你要了解LL算法与LR算法的异同点。
LL算法：优点是较为直观、容易实现，缺点是在一些情况下不得不处理左递归消除和提取左因子问题。 LR算法：优点是不怕左递归，缺点是缺少完整的上下文信息，编译错误显示不友好。 三、语义分析：检查程序是否符合语义规则，并为后续的编译工作收集语义信息核心概念：上下文相关文法 属性文法：上下文相关文法对EBNF进行了扩充，在上下文无关的推导过程中，辅助性解决一些上下文相关的问题。 注意：上下文相关文法没有像状态图、BNF那样直观的分析范式。 应用场景：控制流检查、闭包分析、引用消解等。 场景案例1.控制流检查</description></item><item><title>结束语_实战是唯一标准！</title><link>https://artisanbox.github.io/7/47/</link><pubDate>Tue, 08 Mar 2022 18:37:53 +0800</pubDate><guid>https://artisanbox.github.io/7/47/</guid><description>你好，我是宫文学。
转眼之间，“编译原理实战课”计划中的内容已经发布完毕了。在这季课程中，你的感受如何？有了哪些收获？遇到了哪些困难？
很多同学可能会觉得这一季的课程比上一季的“编译原理之美”要难一些。不过为什么一定要推出这么一门课，来研究实际编译器的实现呢？这是因为我相信，实战是检验你是否掌握了编译原理的唯一标准，也是学习编译原理的真正目标。
计算机领域的工程性很强。这决定了我们学习编译原理，不仅仅是掌握理论，而是要把它付诸实践。在我们学习编译原理的过程中，如果遇到内心有疑惑的地方，那不妨把实战作为决策的标准。
这些疑惑可能包括很多，比如：
词法分析和语法分析工具，应该手写，还是用工具生成？ 应该用LL算法，还是LR算法？ 后端应该用工具，还是自己手写？ 我是否应该学习后端？ IR应该用什么数据结构？ 寄存器分配采用什么算法比较好？ …… 上述问题，如果想在教科书里找到答案，哪怕是“读万卷书”，也是比较难的。而换一个思路，“行万里路”，那就很容易了。你会发现每种语言，因为其适用的领域和设计的目标不同，对于上述问题会采用不同的技术策略，而每种技术策略都有它的道理。从中，你不仅仅可以为上述问题找到答案，更重要的是学会权衡不同技术方案的考虑因素，从而对知识点活学活用起来。
我们说实战是标准。那你可能会反问，难道掌握基础理论和原理就不重要了吗？这是在很多领域都在争论的一个话题：理论重要，还是实践重要。
理论重要，还是实践重要？理论和原理当然重要，在编译原理中也是如此。形式语言有关的理论，以及前端、中端和后端的各个经典算法，构筑了编译原理这门课坚实的理论基础。
但是，在出现编译原理这门课之前，在出现龙书虎书之前，工程师们已经在写编译器了。
你在工作中，有时候就会遇到理论派和实践派之争。举例来说，有时候从理论角度，某一个方案会“看上去很美”。那到底是否采用该方案呢？这个时候，就需要拿实践来说话了。
我拿Linux内核的发展举个例子。当年Linus推出Linux内核的时候，并没有采用学术界推崇的微内核架构，为此Linus还跟Minix的作者有一场著名的辩论。而实践证明，Linux内核发展得很成功，而GNU的另一个采用微架构的内核Hurd发展了20多年还没落地。
客观地说，Linux内核后来也吸收了很多微内核的设计理念。而声称采用微内核架构的Windows系统和macOS系统，其实在很多地方也已经违背了微内核的原则，而具备Linux那样的单内核的特征。之所以有上述的融合，其实都是一个原因，就是为了得到更好的实用效果。所以，实践会为很多历史上的争论划上句号。
在编译技术和计算机语言设计领域，也存在着很多的理论与实践之争。比如，理论上，似乎函数式编程更简洁、更强大，学术界也很偏爱它，但是纯函数的编程语言，至今没有成为主流，这是为什么呢？
再比如，是否一定要把龙书虎书都读明白，才算学会了编译原理呢？
再进一步，如果你使用编译技术的时候，遇到一个实际的问题，是跟着龙书、虎书还有各种课本走，还是拿出一个能解决问题的方案就行？
在课程里，我鼓励你抛弃一些传统上学习编译原理的困扰。如果龙书、虎书看不明白，那也不用过于纠结，这并不会挡住你学习的道路。多看实际的编译器，多自己动手实践，在这个过程中，你自然会明白课本里原来不知所云的知识点。
那么如何以实践为指导，从而具备更好的技术方案鉴别力呢？在本课程里，我们有三个重点。包括研究常用语言的编译器、从计算机语言设计的高度来理解编译原理，以及从运行时的实现来理解编译原理。
对于你所使用的语言，应该把它的编译器研究透这门课程的主张是，你最好把自己所使用语言的编译器研究透。这个建议有几个理由。
第一，因为这门语言是你所熟悉的，所以你研究起来速度会更快。比如，可以更快地写出测试用的程序。并且，由于很多语言的编译器都已经实现了自举，比如说Go语言和Java语言的编译器，所以你可以更快地理解源代码，以及对编译器本身做调试。
第二，这门语言的编译器所采用的实现技术，一定是体现了该语言的特性的。比如V8会强调解析速度快，Java编译器要支持注解特性等，值得你去仔细体会。
第三，研究透编译器，会加深你对这门语言的理解。比如说，你了解清楚了Java的编译器是如何处理泛型的，那你就会彻底理解Java泛型机制的优缺点。而C++的模板机制，对于学习C++的同学是有一定挑战的。但一旦你把它在编译期的实现机制弄明白，就会彻底掌握模板机制。我也计划在后续用一篇加餐，把C++的模板机制给你拆解一下。
那么，既然编译器是为具体语言服务的，所以，我们也在课程里介绍了计算机语言设计所考虑的那些关键因素，以及它们对编译技术的影响。
从计算机语言设计的高度，去理解编译技术在课程里你已经体会到了，语言设计的不同，必然导致采用编译技术不同。
其实，从计算机语言设计的高度上看，编译器只是实现计算机语言的一块底层基石。计算机语言设计本身有很多的研究课题，比如类型系统、所采用的编程范式、泛型特性、元编程特性等等，我们在课程里有所涉猎，但并没有在理论层面深挖。有些学校会从这个方向上来培养博士生，他们会在理论层面做更深入的研究。
什么样的计算机语言是一个好的设计？这是一个充满争议的话题，我们这门课程尽量不参与这个话题的讨论。我们的任务，是要知道当采用不同的语言设计时，应该如何运用编译技术来落地。特别是，要了解自己所使用的语言的实现机制。
如果说计算机语言设计，是一种偏理论性的视角，那么程序具体的运行机制，则是更加注重落地的一种视角。
从程序运行机制的角度，去理解编译技术学习编译原理的一个挑战，就在于你必须真正理解程序是如何运行的，以及程序都可以有哪几种运行方式。这样，你才能理解如何生成服务于这种运行机制的目标代码。
最最基础的，你需要了解像C语言这样的编译成机器码直接运行的语言，它的运行机制是怎样的。代码放在哪里，又是如何一步步被执行的。在执行过程中，栈是怎么变化的。函数调用的过程中，都发生了些什么事情。什么数据是放在栈里的，什么数据是放在堆里的，等等。
在此基础上，如果从C语言换成C++呢？C++多了个对象机制，那对象在内存里是一个什么结构？多重继承的时候是一个什么结构？在存在多态的时候，如何实现方法的正确绑定？这些C++比C语言多出来的语义，你也要能够在运行时机制中把它弄清楚。
再进一步，到了Go语言，仍然是编译成机器码运行的，但跟C和C++又有本质区别。因为Go语言的运行时里包含了垃圾收集机制和并发调度机制，这两个机制要跟你的程序编译成的代码互相配合，所以编译器生成的目标代码里要体现内存管理和并发这两大机制。像Go语言这种特殊的运行机制，还导致了跨语言调用的难度。用Go语言调用C语言的库，要有一定的转换和开销。
然后呢，语言运行时的抽象度进一步增加。到了Java语言，就用到一个虚拟机。字节码也正式登台亮相。你需要知道栈机和寄存器机这两种不同的运行字节码的解释器，也要知道它们对应的字节码的差别。而为了提升运行速度，JIT、分层编译和逆优化机制又登场，栈上替换（OSR）技术也产生。这个时候，你需要理解解释执行和运行JIT生成的本地代码，是如何无缝衔接的。这个时候的栈桢，又有何不同。
然后是JavaScript的运行时机制，就更加复杂了。V8不仅具备JVM的那些能力，在编译时还要去推断变量的类型，并且通过隐藏类的方式安排对象的内存布局，以及通过内联缓存的技术去加快对象属性的访问速度。
这样从最简单的运行时，到最复杂的虚拟机，你都能理解其运行机制的话，你其实不仅知道在不同场景下如何使用编译技术，甚至可以参与虚拟机等底层软件的研发了。
不再是谈论，来参与实战吧！今天，我们学习编译原理，目标不能放在考试考多少分上。中国的技术生态，使得我们已经能够孕育自己的编译器、自己的语言、自己的虚拟机。方舟编译器已经带了个头。我想，中国不会只有方舟编译器孤军奋战的！
就算是开发普通的应用软件，我们也要运用编译技术，让它们平台化，让中国制造的软件，具有更高的技术含量，颠覆世界对于“中国软件”的品牌认知。这样的颠覆，在手机、家电等制造业已经发生了，也应该轮到软件业了。
而经验告诉我们，一旦中国的厂商和工程师开始动起来，那么速度会是非常快的。编译技术并没有多么难。我相信，只要短短几年时间，中国软件界就会在这个领域崭露头角！
这就是我们这门课程的目的。不是为了学习而学习，而是为了实战而学习。
当然，课程虽然看似结束了，但也代表着你学习的重新开始。后面我计划再写几篇加餐，会针对C++、Rust等编译器再做一些解析，拓展你的学习地图。并且，针对方舟编译器，我还会进一步跟你分享我的一些研究成果，希望我们可以形成一个持续不断地对编译器进行研究的社群，让学习和研究不断深入下去，不断走向实用。
另外，我还给你准备了一份毕业问卷，题目不多，希望你能在问卷里聊一聊你对这门课的看法。欢迎你点击下面的图片，用1～2分钟时间填写一下，期待你畅所欲言。当然了，如果你对课程内容还有什么问题，也欢迎你在留言区继续提问，我会持续回复你的留言。
我们江湖再见！
ul { list-style: none; display: block; list-style-type: disc; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; padding-inline-start: 40px; } li { display: list-item; text-align: -webkit-match-parent; } .</description></item></channel></rss>